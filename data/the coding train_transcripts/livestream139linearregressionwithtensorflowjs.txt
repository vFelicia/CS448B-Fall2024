00:04 - [Laughter]
00:13 - my event has started which means that it
00:16 - is me Dan ship and coming to you again
00:19 - for the second time today for the third
00:22 - time in two days this is probably really
00:25 - not so advisable there's a lot of
00:27 - reasons why I shouldn't be here but I am
00:29 - here and I am going to I have about one
00:32 - hour so I'm really gonna jump right in
00:35 - I have I think I might actually even
00:37 - call this dare I say a coding challenge
00:39 - think about this it's okay I don't know
00:43 - this is not the important thing to talk
00:45 - about this is the topic for today linear
00:49 - regression using tensor flow test I'll
00:53 - start going and running away which would
00:55 - be completely understandable I'm really
00:59 - doing this for a couple of reasons one
01:02 - number one is I have started a series of
01:04 - tensorflow chess tutorials just looking
01:07 - at some of the basic features of the API
01:08 - and I want to at least like gets into an
01:11 - example we're actually using this stuff
01:13 - for some purpose now what kind of you
01:17 - know this is really foundational
01:20 - knowledge it's kind of like a classic
01:22 - example of like the beginning stages of
01:24 - machine learning I have actually made
01:26 - some videos about this already in
01:28 - notably
01:30 - this linear regression with gradient
01:32 - descent video that I made just with
01:34 - plain JavaScript I also made this
01:36 - ill-advised video about the mathematics
01:39 - of Oh underlying audio is not working
01:43 - all right well I won't worry about that
01:45 - we just checked the chat here okay this
01:49 - a video about the sort of mathematics
01:51 - trades it's and so these would be two
01:52 - pieces of background you're watching
01:55 - this live so you probably don't want to
01:56 - stop and go watch those but you could
01:58 - okay and so I think this is going to be
02:03 - helpful in term it's a vertical thank
02:05 - you I'm gonna be saying all this stuff
02:07 - again in a second because I'm gonna this
02:09 - but just bear with me please thank you
02:12 - you're very good
02:14 - thank you to old paddock whole pod car
02:18 - on Twitter who posted this code for an
02:24 - interactive simulation of linear
02:25 - aggression with aggression with
02:27 - tensorflow and p5.js this gave me the
02:29 - idea you can read in this thread oh
02:30 - great work I wonder if a video tutorial
02:32 - redoing my linear regression example in
02:35 - a similar fashion would be useful and
02:36 - then the reply was thanks yes and some
02:39 - other people replied and thought this
02:40 - would be useful as well so that's what
02:41 - I'm going to do this is foundational
02:43 - knowledge let me see we're we're aware
02:46 - does this appear we're we're aware under
02:51 - here this is all my neural networks and
02:54 - machine learning playlists we're looking
02:56 - for this one since session 6 so this is
03:01 - what I've done so far and the the thing
03:04 - that I really want to get to which is
03:06 - gonna come next week is the layers API
03:08 - and this is where I'm going to actually
03:10 - begin building some neural network
03:11 - examples and remaking some previous
03:13 - things I've done on this channel without
03:14 - tensorflow TAS but I think before we get
03:17 - there I want to just look at some other
03:20 - features of the attention largest
03:23 - library in the context of linear
03:25 - regression so that's the plan for today
03:31 - ok so I think I just want to get going
03:42 - so I need to reference all these things
03:45 - I guess I will just tab my way through
03:48 - them I'm gonna move this here and I
03:54 - haven't haven't implemented this I
03:55 - scanned over this github users code so I
04:01 - do have that a little bit in the back of
04:03 - my head but I have not tried to
04:04 - implement this myself I've got the chat
04:06 - going the slack channel going here for
04:08 - patrons and YouTube sponsors thank you
04:11 - so I'm gonna you'll be keeping me in
04:14 - check and I've got until about 5:30 p.m.
04:18 - I'm just trying to decide does this sit
04:21 - as 6.5 linear regression with gradient
04:25 - descent or
04:30 - or does this is this a coding challenge
04:33 - I'm seeing in the chat that a chat user
04:35 - says if you want to do interactive stuff
04:38 - with tfj s you should check out TF next
04:40 - frame okay hold on
04:42 - I was not aware of this so let me have a
04:46 - look at next frame returns a promise
04:53 - that resolves when a request animation
04:56 - frame has completed Oh fascinating
04:59 - Oh interesting so that's really
05:03 - interesting I think I'm gonna do this is
05:05 - right so what I'm doing is so incredibly
05:10 - well it's is comparable simple and I I
05:15 - can probably do it with pulling the yeah
05:22 - let's see how this goes let me try to
05:26 - build it and thank you for this note I'm
05:29 - gonna try to do it probably in a less
05:30 - correct way just get it kind of working
05:32 - let the threads kind of like and the
05:34 - amount of computation that I'm gonna do
05:36 - for linear regression is so simple that
05:38 - it even if it locks up for a second that
05:40 - it's gonna be fine - let the animation
05:42 - keep going but this is absolutely
05:43 - something that I will need to pay
05:44 - attention to more later for sure okay
05:53 - does anyone can anyone help me with the
05:55 - pronunciation of this name cows tube old
05:58 - pod car calss tube old pop car any
06:01 - chance that that was kind of close
06:23 - okay
06:25 - you will be getting going in just a
06:28 - second I'm waiting for the live chat to
06:31 - maybe catch up to me somebody might help
06:34 - me pronounce this name correctly all
06:45 - right close enough I think well that's
06:49 - what its gonna have to be where's my
06:50 - marker here we go and I'm gonna get
06:54 - started I wish I could just call and
06:58 - pronounce it
06:59 - ah that's a good point yeah you could
07:02 - you know yeah let's move on I probably
07:04 - some fancy technological way we could
07:06 - make that happen
07:08 - everybody should stretch stretch is
07:12 - really this extra live stream towards
07:15 - the end of the day trying to do
07:16 - something with machine learning and
07:17 - since the flow Jess this really needs
07:20 - some space Mellon but I don't have any
07:23 - space Mellon with me so I'm just gonna
07:24 - have to try my best
07:28 - couse tub thank you coos tub
07:30 - ku stub old pot car okay thank you
07:35 - cows tub old pod ker okay um now people
07:42 - are just trolling me in the chat by
07:44 - saying it's pronounced with the actual
07:46 - spelling of the name I will do my best
07:50 - hello you are here watching another
07:53 - coding challenge and I you know this
07:55 - coding challenge maybe this should just
07:58 - fit in being one of my tutorial videos
08:00 - but I'm gonna make it a coding challenge
08:02 - because I bet his attempt to do it in
08:03 - one video and what I'm doing is
08:05 - recreating something that I've done
08:06 - before and some of my machine learning
08:08 - tutorials and it was it suggested er
08:12 - know if it was suggested exactly but a
08:13 - twitter user a cow's tube old pod car
08:16 - apologies if i pressing the name
08:18 - incorrectly created this interactive
08:21 - simulation of linear regret
08:24 - using tensorflow Jas and so this is very
08:28 - similar to something that I've done
08:29 - previously right I have this video
08:30 - linear regression with gradient descent
08:32 - where I just did this with plain
08:34 - JavaScript and then you could also look
08:36 - at this other video which I go through
08:38 - the mathematics of gradient descent a
08:40 - little bit but here's the thing going
08:42 - through the mathematics making this
08:44 - video where I implement the mathematics
08:46 - in JavaScript while useful and perhaps
08:48 - background for this video one of the
08:50 - exciting things about doing this with
08:52 - tensorflow Dutch ass is tensor photo
08:54 - chess has a nice API for optimizing loss
08:59 - functions with the gradient descent
09:02 - algorithm built into it so I could just
09:05 - do things so let's make a light to come
09:06 - back here but let's make a list oh hold
09:09 - on hold on
09:14 - [Music]
09:29 - the things that I'm going to need right
09:32 - I need to have okay so what's going on
09:35 - here what is linear regression first of
09:38 - all and of course you can compute the I
09:44 - was gonna do this as a coding challenge
09:46 - with no editing but so much for that
09:49 - alright so first of all what is linear
09:53 - regression anyway so let's say we have a
09:57 - space and I drew this as like a canvas
09:59 - but really I should be talking just
10:01 - about a generic kind of two-dimensional
10:04 - Cartesian plane and in that plane there
10:07 - are a lot of points and I don't like the
10:09 - way I just drew that in that plane there
10:18 - are a lot of they're a bunch of points
10:20 - the idea of linear regression is to
10:24 - figure out can we fit oh this is a time
10:27 - for another colored marker can we fit a
10:31 - line into this two-dimensional space
10:34 - that approximates all of these points as
10:37 - best we can and I can visually just kind
10:40 - of make myself do this like this so I
10:43 - can eyeball and say this line kind of
10:45 - gets close what we're trying to do is
10:47 - minimize all of these this is the most
10:53 - beautiful diagram I've ever made all of
10:54 - these distances of all of the points to
10:57 - the line the idea here then is that we
10:59 - can make some predictions right if this
11:01 - data if this x-axis you know represents
11:06 - height we might predict on the y-axis
11:13 - weight right you can think of kinds of
11:18 - data sets simple 2d data sets where
11:21 - there's a linear relationship between
11:23 - the 1:1 field of data and another field
11:27 - of data so if we pick a new height we
11:30 - can kind of make a guess approximately
11:32 - what that weight is going to be that's
11:33 - the idea of linear regression it's
11:35 - incredibly simple a lot of data isn't
11:37 - two-dimensional a lot of
11:39 - data doesn't fit a line you know maybe a
11:41 - curve fits it better and this is more
11:43 - complex scenarios will come as we move
11:46 - forward and make more scenarios with
11:49 - complex polynomial equations our neural
11:51 - network based learning and other types
11:53 - of machine learning algorithms but this
11:54 - is a good place for us to start so what
11:57 - do we need we need a data set so we need
12:01 - a set of X's and Y's this is the data
12:04 - set right we need X's and Y's and I'm
12:09 - gonna create that data set through
12:11 - interactive clicking interactive
12:14 - clicking is the way I'm going to create
12:15 - that data set every with the mouse I
12:18 - need to have something called a loss
12:21 - function the loss function is a way of
12:25 - computing the error and there are a
12:27 - bunch of different loss functions and
12:29 - we'll see these as we as I use
12:31 - tensorflow DHS in more tutorials I can
12:33 - select different kinds of loss functions
12:35 - for different scenarios but in this
12:37 - scenario I'm going to use a simple basic
12:38 - one which I believe is called root mean
12:42 - squared error did I say that correctly
12:46 - is that the right name of it but the
12:48 - idea is that I want to look at all of
12:50 - those distances and I want to minimize
12:53 - them so that number if I add if I find
12:56 - all these distances the difference and
12:58 - by the way I totally uh I botched this
13:01 - time out vertical distance it's even in
13:07 - the chat so the good news is you can
13:12 - barely see it I'm looking over here on
13:14 - my monitor and I can't see it you can
13:17 - barely see it but I really botched what
13:19 - I was drawing there so let me fix that I
13:21 - don't think I'm gonna go back and redo
13:24 - this video I think I'm just gonna get a
13:26 - different pen color since you can't see
13:27 - it anyway and just fix that so a little
13:33 - editing point here
13:41 - okay I'm back as I started talking about
13:43 - the lost function I realized I really
13:45 - didn't draw I'm not actually looking for
13:48 - the the distance from the point to that
13:50 - line which would be perpendicular I'm
13:52 - looking for this vertical distance which
13:55 - is so this is what I'm trying to
13:57 - minimize right I'm trying to minimize
14:00 - and get a line that has and this that
14:03 - that is the least dilute the sum of all
14:07 - these distances is the smallest number
14:09 - minimizing the loss so I have a loss
14:12 - function I need that I also need
14:16 - I also need in tensorflow da chasse
14:24 - something called an optimizer and the
14:28 - optimizer is the thing that allows me to
14:31 - minimize the loss function and in order
14:37 - to do that I also need to have a
14:39 - learning rate so these are all I've
14:43 - actually missed something very important
14:44 - here but these are all the pieces I need
14:46 - the data I need to define a loss
14:48 - function I need to define an optimizer I
14:51 - say hey optimizer minimize the loss
14:53 - function with this learning rate so keep
14:55 - tweaking the pair parameters tweaking
14:57 - the parameters so that's the thing I
14:59 - forgot what are those parameters well
15:00 - the formula for a line is y equals MX
15:04 - plus B M is often referred to as the
15:08 - slope B as the offset what's the name
15:13 - for that what's the name of that formula
15:27 - for a line
15:30 - because why shouldn't do these live
15:32 - streams at the end of the day I was so
15:35 - so with it when I was talking about
15:36 - promises this morning wasn't I
15:38 - y-intercept there we go y-intercept ok
15:51 - I'm back I looked it up
15:53 - M is the slope and B referred to as the
15:57 - y-intercept kind of like bias by the way
16:02 - if you've watched some other neural
16:03 - network tutorials this is like the thing
16:05 - we're doing with all the neurons
16:06 - oh it's also connected but we're just
16:08 - living in this very simple place so I
16:11 - need these parameters I need these
16:14 - variables because that's what's going to
16:18 - allow me to create the predictions that
16:20 - are on the line to compare with the
16:22 - actual points and compute the loss
16:25 - minimize it tweaking these values so
16:27 - tweak these values minimizing the loss
16:29 - this is what we're doing and I've done
16:31 - this before in great detail this is
16:34 - gonna be in less detail because tension
16:36 - flow dot yes it's gonna do a lot of this
16:37 - for us the thing that's a little extra
16:39 - complicated is we can't just work with
16:42 - arrays of numbers and variables in the
16:44 - way that we're used to in JavaScript and
16:46 - so this is what brings me to if you
16:48 - haven't looked at these particular
16:51 - videos that I've made already
16:52 - what's a tensor flow tensor what's a
16:56 - variable what's in operation how does
16:57 - the memory management stuff this is
16:59 - stuff we're gonna have to lean on while
17:01 - I build this example and this should be
17:03 - by the way an actual practical example
17:05 - of where I need a TF variable so I kind
17:08 - of in this video explained what a TF
17:09 - variable is we'll just kind of moved on
17:11 - and didn't use it for anything so
17:12 - hopefully this will show us that all
17:14 - right how about we write some code now
17:17 - I'm pausing for a second taking a look
17:21 - at the chat the route shouldn't be there
17:25 - I don't know what that means offset it's
17:27 - fine why intersect or intercept what's
17:31 - the route oh so what's the let me look
17:36 - this up on the what's the wire mess Oh
17:39 - in the in the back of my
17:43 - yeah let me fix that what's the why did
17:59 - I say root let me look at loss square Oh
18:13 - cuz I don't have to square root it mean
18:15 - squared yeah so I'm gonna just use mean
18:18 - squared error thank you Thank You Simon
18:21 - in the chat in the slack group makes a
18:23 - good point
18:24 - so maybe actually correct that really
18:25 - quickly I am NOT gonna be doing partial
18:30 - derivatives again and definitely not
18:32 - asked Kenneth
18:34 - so maybe I should go back and define
18:36 - that better root mean squared is oh okay
18:46 - so actually I match eclis appeared over
18:49 - here for a second because instead of I
18:52 - did make a little mistake here I mean
18:53 - root mean squared is a perfectly
18:56 - legitimate loss function but most linear
19:00 - regression with gradient descent
19:01 - examples will not bother with the root
19:04 - and the root refers to square root we
19:06 - just want the mean squared error which
19:08 - means like if I say that this value is y
19:12 - and this value is like the guests right
19:16 - I the error is guess minus y and squared
19:22 - and if I do that for all of these that's
19:25 - the mean an average them it's the mean
19:27 - so I can really sum them or mean them
19:29 - well it's gonna take care of that for us
19:31 - so we're just gonna use the mean squared
19:34 - error but that's the idea we take the
19:36 - differences there the reason why we have
19:37 - to square it well for a variety of
19:39 - reasons one it has to do with the
19:40 - derivative stuff that's in my other
19:42 - videos but also just because positive or
19:45 - negative with the it's the distance that
19:48 - the size of the error whether it's up or
19:50 - down which is key all right so I need to
19:54 - start the coding but the microphone is
19:55 - making me crazy
19:56 - I need to tape it to my ear so hold on I
20:04 - just need to get a little piece of tape
20:06 - it it really distracts me when the
20:08 - microphone starts yeah by the way who's
20:13 - Ricardo and the chat is mentioning the
20:15 - coding train hoodie this is a new coding
20:16 - train hoody that will be available later
20:18 - today
20:24 - all right am I good to start coding any
20:27 - last comments before I start coding this
20:33 - feels it's staying on my ear now okay
20:38 - and maybe next day next year next time
20:40 - I'll get the lapel mic back how are we
20:42 - for 51 we're doing pretty well too check
20:50 - mine I'm just gonna keep going all right
20:54 - I think I'm good I think we're ready to
20:58 - start writing some code all right
21:03 - where's my marker in case I need it
21:06 - over here all right so here's the kind
21:15 - of code we're gonna start with I'm using
21:17 - p5 so that I can draw stuff I'm making a
21:20 - canvas and the background is zero which
21:23 - means it's black and this is what I have
21:24 - so far so let's look at our list over
21:26 - here and let's first add the data set
21:29 - the X's and Y's so this is going to be
21:31 - easy because I just want to have X's be
21:37 - an array wise also be an array so and
21:40 - then whenever I click the mouse I want
21:46 - to say oh you know what I could make
21:47 - those vectors let's make them separate
21:50 - arrays I think we're gonna actually I
21:51 - know we're gonna we're don't want to do
21:52 - that for a variety of reasons we're
21:54 - gonna keep those at separate arrays so
21:55 - every time I click the mouse I'm going
21:57 - to say X's push Mouse X wise push Mouse
22:02 - Y ah okay here's this here's a little
22:06 - thing so this
22:09 - is our canvas right this is my drawing
22:11 - of the canvas I know I'm having like a
22:13 - DejaVu thing I totally talked about this
22:15 - in the other video the width is like 400
22:18 - the height is 400 but I really want to
22:20 - think of this as I really want to think
22:24 - of zero zero down here and maybe one
22:26 - being over here oh and normalize
22:28 - everything between zero and one
22:29 - everything's just gonna work better if
22:31 - we do that so with Y pointing up so I'm
22:34 - gonna do a mapping so every Y value
22:36 - that's pixel value between zero and
22:38 - height I'm gonna map between 1 and 0 and
22:41 - every x value that's between 0 and width
22:43 - I'm going to map between 0 and 1 so
22:44 - let's do that
22:46 - so I'm going to say let X equal map
22:50 - mouse X which goes between 0 and width 2
22:55 - between 0 and 1 let Y which is Mouse y
23:01 - between 0 and height and have that go
23:04 - between 1 and 0 and then push X and push
23:08 - Y so the other thing I want to do is I
23:12 - just want to UM you know I'm gonna add a
23:15 - draw loop and I want to draw all those
23:17 - points so I'm also now gonna say stroke
23:23 - 255 stroke wait for 4 let I equal what
23:30 - huh let a equal 0 is less than X dot
23:33 - link I plus plus and those are actually
23:37 - called X's X's dot length and what I'm
23:43 - gonna do is I'm going to say let pixel X
23:45 - equal map I really should make just like
23:47 - I probably have do this a lot so I
23:48 - should probably make a function that
23:50 - just like normalize and unnormal eyes or
23:52 - denormalize pix px equals map X's index
23:57 - I which goes between 0 and 1 back to 0
23:59 - and width so this is the reverse py
24:02 - which maps Y which goes between 0 & 1 to
24:07 - height comma 0 and then I want to say a
24:13 - point P X py so I haven't done any I'm
24:16 - even I'm not even using tensorflow Jass
24:18 - yet I'm just kind of doing the stuff
24:20 - with p5 to draw things so let's see if
24:23 - getting the results that I want which is
24:25 - whatever I click I get the points
24:27 - they're perfect and I kind of want to
24:29 - see them a bit more that's like really
24:31 - make it bigger
24:32 - great that's like too big okay great so
24:36 - we can see those those are the points
24:38 - I'm clicking on okay pause pause edit
24:49 - edit pause pause at it I'm like drunk
24:51 - with power with the editing I just take
24:53 - a break
24:54 - it'll get edited later I used to say
24:56 - that and it didn't now it does I think
25:00 - was probably better when I didn't stop
25:01 - in the middle all right all right so
25:16 - what's next
25:17 - I need a loss function I need an
25:19 - optimizer ah oh I need these let's make
25:23 - these so cuz I'm looking for somewhere
25:26 - where I need to get some tensor flow Jas
25:28 - stuff working so what I need is I need
25:31 - to have M and B so let's figure that out
25:34 - so I'm going to create an M and a B and
25:41 - I'm not going to initialize them in
25:42 - setup here and I probably should be
25:46 - using Const in various places here just
25:48 - protect myself from reassigning
25:49 - something by accident but I'm gonna be
25:51 - loosey-goosey and just use let you know
25:56 - these could be constant but anyway I'm
25:57 - not going to get into the whole let vs.
25:59 - constant it makes me crazy I'm gonna say
26:01 - up here M equals TF scalar random one so
26:07 - I'm going to use the p5 random function
26:10 - to give me a random number between 0 1
26:11 - because I gotta start somewhere so this
26:13 - is kind of like initializing the weights
26:15 - of a neural network there is no neural
26:16 - network I'm just doing I'm just kind of
26:19 - optimizing this function y equals MX
26:20 - plus B but those are like weights and
26:22 - then B so I'm gonna initialize them
26:24 - randomly and scaler because it's a
26:25 - single number so go back to my tensor
26:28 - flow j ass intro videos and you'll see
26:29 - then B I'm gonna say the same thing
26:32 - ah but
26:34 - these are the things that have to change
26:36 - right the data never changes it's sort
26:39 - of fixed M and B change over time I need
26:42 - to tweak those which means they have to
26:45 - be variable they have to be able to
26:46 - change which means when I overhear I
26:49 - think what I write is TF variable I
26:51 - wrapped them in the TF variable so now I
26:55 - have M and B as TF variable right isn't
27:00 - it crazy like you see this kind of code
27:01 - you're like that looks like the craziest
27:03 - scariest thing but you realize like it's
27:05 - just like make a number and because
27:06 - we're in this like kind of lower level
27:09 - working on the GPU land I've got to be
27:11 - very like specific like this is a single
27:13 - number and it's going to be variable but
27:15 - really it's just a random number okay
27:18 - now what do we need to do we need to
27:21 - write I don't think I actually said this
27:24 - but I need to write a function called
27:27 - predict maybe which takes in all of the
27:33 - X's just the X's and gives me the Y
27:37 - predictions based on where the line is
27:38 - right because I need to compare the Y
27:40 - predictions to the actual Y values to
27:42 - get the mean squared error so let's
27:45 - write that function so I'm just gonna
27:47 - I'm putting these in like arbitrary
27:49 - places but I'm gonna write a function
27:50 - called predict and they're what I need
27:53 - to do is I need to have some X's and I
28:00 - need to return some wise I think that's
28:05 - the idea right yes
28:06 - so I don't want to just predict one
28:08 - value I want to predict a bunch so the
28:11 - X's here's the thing so if I call this
28:13 - function the X is if they're just a
28:16 - plain array I need to make it into a
28:18 - tensor so I'm gonna call it Const TF X's
28:23 - that might be a bad is tensor and this
28:27 - is a one D tensor tensor 1 d OT f tensor
28:36 - 1 D I think this will do it X's right
28:40 - and you turn it into a tensor and then I
28:46 - need to
28:48 - have the formula for a line so I need to
28:51 - say what which is y equals MX plus B so
28:57 - what I would be doing is I would be
28:58 - saying TF X's multiplied is it mu L or
29:05 - nu l T multiplied by M plus B right this
29:12 - is the idea if I'm getting just a plain
29:16 - array of numbers I turn them into a
29:18 - tensor then I apply the formula for a
29:21 - line and these are the predictions the
29:23 - wise I guess if I don't like my naming
29:25 - here I'm just gonna call this X and
29:31 - maybe I'll call this X's I don't know I
29:34 - have to think about I'll come back to
29:37 - this later okay so I have that now let's
29:48 - go back and look at the things that I
29:49 - need so I have this predict function I
29:52 - have a data set ah I need a loss
29:55 - function you need a loss function and I
29:58 - need a let's before we do the loss
29:59 - function let's create the optimizer and
30:02 - the learning rate so this is what's
30:03 - wonderful about working with tensorflow
30:06 - j/s when i say make the optimizer
30:08 - I just mean make it TF it exists it'll
30:12 - do this math for us so let's go to the
30:15 - this is not something that I covered in
30:16 - my other videos so let's go look for
30:18 - optimizer and I want an optimizer now
30:22 - there's all these different kinds of
30:24 - optimizers SGD stochastic a gradient
30:28 - descent
30:29 - this means the idea of slowly adjusting
30:34 - mnb toward two to minimize the loss
30:39 - function and I've covered this in more
30:40 - detail in the other videos so so I'm
30:44 - gonna click on that and I'm gonna look
30:46 - here and this is basically what I need
30:47 - to do i we got the code right here look
30:49 - there's even look at this oh my goodness
30:51 - there's like some stuff here we could
30:52 - really use so I'm gonna grab this and
30:58 - I'm gonna put this up here so I want to
31:01 - learning rate
31:01 - I'm gonna I'm gonna have a much bigger
31:03 - learning rate to start with and I want
31:04 - an optimizer so now I have a learning
31:06 - rate and an optimizer and the optimizer
31:08 - is doing stochastic gradient descent so
31:11 - I have learning rate optimizer now I
31:13 - need that loss function I need the loss
31:17 - function okay the loss function is
31:21 - something I'm going to write loss and
31:24 - actually let's go back to here so look
31:26 - at this so this is the fancy es6 way of
31:29 - writing a function but I'm gonna write
31:32 - it in a less fancy way and I'm gonna do
31:36 - this so what I want is I need the loss
31:39 - function I have some predictions and I
31:41 - have some labels so these are the
31:44 - predictions are the Y values I'm getting
31:48 - from the predict function the labels are
31:51 - the actual Y values that are part of
31:54 - this and by the way I'm gonna have to do
31:56 - memory management don't worry I'm if
31:57 - you're screaming at me that I haven't
31:58 - worked about memory management I'm gonna
32:00 - do that
32:01 - just gonna do that later so what I want
32:03 - to do is say return to predictions minus
32:08 - the labels that makes sense right
32:10 - because I said here when I said mean
32:12 - squared error is the predictions like
32:15 - the guests minus the labels which is the
32:17 - actual Y squared and so predictions
32:23 - minus the labels squared and then take
32:31 - the mean of them look at this all of
32:33 - these mathematical operations are inside
32:36 - of tensorflow digest and you can chain
32:38 - them
32:39 - so predictions is a tensor labels is a
32:44 - tensor all of these remember they're
32:46 - just gonna keep producing new tensors
32:48 - and I'm gonna have to tidy and clean all
32:50 - this stuff up or memory management but
32:52 - again I'll worry about that later so now
32:53 - I have the loss function all right well
32:56 - what is it that I want to do every time
32:58 - so let's say I think I'm actually like I
33:00 - have everything I have the loss function
33:03 - I have the data I have the optimizer I
33:07 - have a predict function I have a
33:08 - learning rate I can minimize Y oh this I
33:10 - haven't done so the training the actual
33:13 - training what does it mean to Train
33:15 - to train it means minimize the loss
33:18 - function with the optimizer and
33:20 - adjusting M and B based on that all
33:23 - right so let's see if we can make that I
33:25 - have a feeling that was in that page
33:27 - that I went to so maybe I could just
33:29 - copy it from there I'm kind of this is
33:30 - like very totally is I'm gonna happily
33:34 - cheat here who was seeing that example
33:36 - thank you very much thank you to float
33:37 - IGS documentation and so I'm gonna just
33:42 - put this in draw like every time through
33:44 - draw I'm gonna minimize so let's look at
33:45 - this oh look at this okay so this is a
33:47 - little different so first of all this is
33:49 - using nice fancy es6 arrow notation
33:52 - which I'm somewhat happy about but let
33:54 - me just write a function here called
33:56 - train and the idea of the Train function
34:00 - is to execute the loss with the
34:04 - predictions and the and the actual wise
34:10 - okay so here what I'm really doing is
34:13 - minimizing the Train um
34:16 - that's weird this isn't really no
34:18 - training would be doing this so this is
34:19 - a terrible name for this and actually
34:21 - this is silly for me to even name this
34:24 - it really makes sense for me to just
34:26 - make this an anonymous function and that
34:28 - what I'm minimizing is this right this
34:32 - is what I want to minimize the loss
34:35 - function but if I want to be nice and
34:37 - es6 like with my arrow notation which I
34:39 - think by the fact I'm using tensorflow
34:41 - j/s and you can watch my arrow notation
34:43 - function if this is if this is I can
34:46 - kind of get rid of a lot of the extra
34:48 - stuff here and this should be good so I
34:52 - just want to minimize the loss function
34:54 - now here's the thing these have to be
34:58 - tensors right the loss function requires
35:02 - predictions and labels they have to be
35:04 - tensors and if you remember my X's and
35:06 - Y's aren't tensors when I call the
35:09 - predict function with the X's it gives
35:11 - me back a tensor so that I can't believe
35:16 - I haven't run this code yet this is a
35:17 - terrible thing usually I try to run my
35:19 - code incrementally all the time I guess
35:21 - I forgotten to do that so probably
35:23 - people in the chatter telling me about
35:24 - mistakes I'm making so this is a tensor
35:27 - but this is still a plain array so what
35:30 - I need to do is say constant and I got a
35:33 - rethink to naming maybe something the
35:34 - chat has an idea for me I think what I
35:38 - actually should do I have an idea
35:40 - permit me a moment of refactoring X
35:43 - valve Y valve so I think when it's not a
35:46 - tensor I'm just gonna call it like X
35:48 - underscore valve because that's gonna
35:52 - help me remember so X valve Y valve and
35:58 - then whenever I say and this should be X
36:02 - whenever I say X s are YS that's really
36:04 - a tensor I guess I could have done T X s
36:06 - so here what I'm doing is predicting
36:09 - from the X valve and then Y s is TF
36:13 - tensor shoot somehow I went past a half
36:19 - an hour YS is TF dot tensor one D Y
36:27 - valve so I need to create that tensor
36:31 - and now I can minimize the loss with
36:33 - predicting from the x-files and the Y
36:35 - boughs okay so this is good let's just
36:40 - run this see what errors we get okay
36:47 - predict dotsub dot squared is not a
36:49 - function at loss at optimizer minimize
36:52 - so what do I have wrong here in my loss
36:56 - function sub labels dot squared dot mean
37:02 - hmm
37:04 - and so let me let me put no loop in here
37:13 - because I just want to run this once and
37:16 - let me say I just want to like
37:18 - console.log oh actually I can just do YS
37:22 - dot print oh you know what it is
37:27 - there's nothing in the arrays at the
37:30 - beginning they have zero things in them
37:31 - so a couple things one is I could put
37:33 - something in it
37:33 - but I think I probably should just say
37:35 - I'm not I shouldn't do only if X dot
37:38 - length is greater than zero so this is
37:42 - definitely do I want to bother with do I
37:45 - want to bother with doing any of this if
37:46 - there's no values in there like calling
37:48 - predicting stuff with an empty array I
37:50 - think it's gonna cause problems that
37:52 - makes sense all right let's try this X
37:55 - is not to find X valve my naming okay Oh
38:00 - sketch 45 ah this is X valve and this is
38:08 - X valve Y valve okay that should be good
38:13 - all right let's try this
38:20 - mm-hmm all right I think I have frozen
38:27 - the world up this is not square I being
38:36 - told in the chat breaking news due to
38:38 - deduce that this is actually dot square
38:41 - not dot squared that's not right
38:45 - operations square whoa this I might be
38:50 - able to use computes the mean squared
38:53 - error between two tensors so I could
38:55 - also use this probably but that's not
38:57 - what I'm looking for
38:59 - yeah oh it's square a square okay
39:08 - hold on so what's going on let's comment
39:12 - this out Oh like I put no loop I put no
39:14 - loop in there silly me
39:15 - okay everything's fine I put no loop
39:17 - when I was trying to console.log stuff
39:20 - there we go okay so things are going and
39:23 - there's no I don't have any like I could
39:25 - look at that's M m m dot print right so
39:33 - you can see it's changing it's actually
39:35 - like training it like the value of M is
39:37 - changing so everything's going and
39:39 - working the problem is I'm not seeing
39:41 - the results let's just check B and I
39:48 - haven't done any memory cleanup so if I
39:50 - say memory dot num tensors is that what
39:53 - it is now what is it again so let's look
39:56 - under memory management memory Oh memory
40:02 - num - oh this TF so I know you can't see
40:04 - this but this is what I want I want to
40:06 - check how much I want to check and see
40:08 - like how if I have to have cleaned up
40:11 - stuff you can see I have 1147 tensor so
40:14 - I need to do the memory cleanup I don't
40:16 - know probably better practice would be
40:17 - for me to clean up as I'm going but I'm
40:19 - kind of gonna clean up at the end all
40:21 - right so let's I just want to click back
40:24 - here for a second oops no I'm just gonna
40:27 - click no loop to shut this off and let's
40:30 - go here so what do I need to do ah I
40:32 - need to visualize what's going on all
40:34 - right so how do I do that so I need to
40:37 - draw a line so the way that I would draw
40:41 - the line is first what I would do is all
40:44 - I really need is to give myself the x
40:46 - value of 0 and the x value of 1 get the
40:50 - two Y values and draw a line between
40:52 - those two points so if I were to say let
40:57 - X equal TF scalar this is silly for me
41:04 - to use the predict function why not why
41:07 - not let's use the predict function TF
41:09 - scales are zero so x 1.is TF scalar 0 y1
41:14 - equals T F equals predict at
41:19 - 1 X 2 equals TF scalar 1 y 2 equals
41:27 - predict X 2 right so this should give me
41:31 - I mean it's a little bit silly for me to
41:32 - not just do this keep an extra copy of
41:34 - like M and B he has regular numbers but
41:37 - let's keep going with this will this
41:39 - work is it gonna be able to take a
41:41 - scalar and make a 1 D tensor I think so
41:45 - so let me just see here so let me do X 1
41:48 - dot print y 1 dot print so let's see
41:52 - that tensor 1 D requires values to be a
41:56 - flat typed array hmm
41:58 - so I could y1 I could reshape yes good
42:08 - point
42:09 - minimize sorry sorry I started looking
42:14 - at the chat which I shouldn't do I'm
42:15 - gonna pause for a second for a little
42:16 - edit point Alex Pratt's Pharaoh writes
42:27 - you need to average the squared error I
42:28 - don't think I need to average it because
42:30 - I have the dot mean function that takes
42:32 - the average automatically I think so let
42:36 - me think what might be better is for me
42:38 - to just should I just I should just bite
42:41 - the bullet and get the M and B values
42:43 - back right yeah I could reshape this way
42:52 - oh no this is silly I can just do this
42:55 - hold on
43:00 - all right so one thing I could do is
43:01 - instead of making it a instead of making
43:05 - it a scalar tent I can make it a one D
43:07 - tensor that's what it wants and do the
43:10 - same thing here and I have to put it in
43:11 - as an array then but it's just one value
43:13 - oh this is so silly
43:15 - why am i doing x1 oh I could just do
43:18 - this right X is once again can I use X's
43:22 - yeah yeah yeah so I could do I could
43:26 - just do it with zero and one constant
43:29 - X's and then constant y z-- equals
43:32 - predict X's right so I could have both
43:36 - these points now then let's say X is dot
43:41 - print wise dot print let's look at that
43:44 - let's see if this works
43:47 - predict is not defined because my e key
43:50 - doesn't work I have to type it several
43:52 - times tensor one D requires values to be
43:56 - a flat typed array Oh
43:59 - silly silly me predict doesn't want a
44:03 - tensor oh it wants this so line X equal
44:13 - let me just this is a little bit silly
44:16 - but I'm gonna do this so I'm gonna make
44:19 - the oh oh but I don't need to know the
44:22 - X's tensor because yeah sorry everybody
44:29 - there we go my predict function I
44:34 - totally forgot already see this is just
44:36 - there's so many different ways you could
44:37 - do this like I could have I could
44:39 - enforce you to convert to a tensor
44:41 - before you pass it in to predict but
44:43 - I've just a lot of these decisions are
44:44 - completely arbitrary so you might have a
44:46 - better way of doing it but so I'm gonna
44:48 - do this so now I have the XS and the Y's
44:51 - and I don't even need to say XS print so
44:55 - we can see okay great so I'm getting
44:57 - these points k week mon in the chat
45:01 - makes an excellent point which is that i
45:06 - should think about actually mapping it
45:09 - between negative one and one with zero
45:11 - zero in the center that's not such a bad
45:13 - eye
45:13 - eeeh uh let me just keep going with this
45:16 - and then I'll maybe I'll change that
45:18 - after the fact because this should work
45:19 - anyway so now here's the thing here's
45:21 - the awkward thing in order for me all I
45:25 - need to do now is basically say this let
45:29 - x1 equal map X is 0 which goes between 0
45:33 - & 1 1 between 0 & with and this is kind
45:36 - of silly could just multiply it times
45:38 - width but I'm gonna just go with the
45:42 - normalizing though all the full
45:44 - normalizing y1 equals map X sorry X 2
45:49 - which map X's index 1 so this gives me X
45:53 - 1 which is just 0 and with now y1 and y2
46:04 - I want to map YS the Y value is between
46:11 - and between height and zero because I'm
46:16 - flipping it the problem is and then I
46:22 - just want to say line x1 y1 x2 y2 so
46:28 - this is really all I need to do right I
46:30 - just want to get this sort of two points
46:34 - on the line and then draw a line between
46:36 - them this is fine because my X's are not
46:40 - tensors and I can use plain numbers
46:42 - right here x1 x2 but my y's and and here
46:47 - but my y's are tensors so for me to be
46:50 - able to I really need to get the values
46:53 - back and a way to do that is is with a
46:56 - function called data so I'm gonna say
46:59 - let line y equal wise dot data and I'm
47:08 - just gonna say Data Sync right now and
47:10 - let me comment all this out and let me
47:14 - console.log that and see if this comes
47:18 - so there's this is kind of a bad idea
47:20 - for a variety of reasons but I think
47:22 - it's gonna work ok so you can see I'm
47:26 - getting those
47:27 - numbers back as a float array so here's
47:30 - the thing this really requires not a
47:32 - callback but a promise and I'm so happy
47:35 - I just did a whole video series on
47:36 - promises I really should be saying data
47:38 - then and there's even something called
47:41 - TF next frame which allows me to sort of
47:44 - think about the asynchronous nature of
47:46 - pulling the data out of a tensor into a
47:48 - number that I can use in an animation
47:50 - these are key things I'm definitely
47:52 - gonna have to get to them but here's the
47:53 - thing this is just two numbers I think
47:56 - my animation can handle using data sync
47:59 - and maybe somebody from the tensorflow
48:01 - Jazz team is gonna want to say like
48:03 - actually this was not just a bad idea
48:05 - but like a really bad idea I'm not so
48:07 - sure but I think it's gonna let's just
48:08 - get it to work and see if this
48:09 - demonstrates the idea so now I'm gonna
48:12 - call this line why I should be able to
48:15 - say y1 y2 and I should get 0 & 1 from
48:22 - line Y and now we should see we should
48:28 - be done up line Y is not defined
48:33 - we're sketch that J's line 61
48:48 - I think I just didn't hit save yeah
48:51 - oh I haven't clicked any points hey look
48:56 - at that
48:56 - oh hey look it's working alright for a
49:03 - couple things number one is let's say
49:06 - strokeweight - and by the way we can now
49:12 - start to play with the learning rate III
49:14 - don't have to clean up the memory stuff
49:16 - I have to low can play with the learning
49:19 - rate like let's make this point zero one
49:23 - so you can see what happens with this
49:26 - lower learning rate I don't know if it's
49:32 - let's see is it really working well I
49:35 - shouldn't use such a low learning rate
49:36 - let's make it point five yeah it's
49:40 - definitely it's definitely happy okay so
49:43 - this is working linear regression
49:45 - created to say but I have a severe
49:47 - problem I am just filling the GPU with
49:52 - tensors and tensors and tensors and
49:53 - tensors and never cleaning them up so
49:56 - now it's my job to go through and find
49:59 - every place I'm creating a tensor and
50:01 - dispose of it so I can use TF tidy to do
50:04 - that automatically or it can just use
50:05 - the actual dispose function which I
50:07 - might be inclined to do it first all
50:09 - right so let's go through so here these
50:11 - I do not I always want to keep them m
50:14 - and B your variables that I need to keep
50:15 - throughout the course of this program
50:18 - loss do I just put tidy in here or
50:23 - should I let's predict so do I put tidy
50:27 - in here do I wrap tidy here what if I
50:30 - just put tidy here like what if I say TF
50:34 - dot tidy and put all of this will this
50:41 - do it and then here I also need - well
50:47 - I'm here maybe what I'm gonna do is just
50:49 - dispose these there's no logic to what
50:51 - I'm doing but I'm just gonna dispose
50:53 - these manually oh and that's just the
50:56 - wise right line y is just y s is the own
51:01 - that the tents are here so this should
51:04 - tidy everything but hopefully not the
51:06 - variables that I need to keep rather
51:09 - than individually figuring out what to
51:11 - dispose of and down here I kind of know
51:14 - that this is my only tensor this by the
51:17 - way I should call this like line X just
51:20 - to be consistent with my variable naming
51:24 - you know I'm only using why that YS and
51:28 - XS notate variable name when I have
51:30 - something that's actually a tensor which
51:32 - helps me remember what I need to clean
51:33 - up and not let's see if this goes okay
51:37 - it's still running 221 no okay
51:43 - so I better there's fewer tensors but I
51:46 - haven't cleaned up everything so what
51:48 - could I be missing maybe the call to
52:00 - predict wouldn't tidy clean that up you
52:04 - know we need a I'm have to think about
52:08 - this for a little bit
52:14 - Ferenc asked just joined in as it being
52:16 - recorded and posted later on YouTube yes
52:20 - anybody see what tensors I think this is
52:23 - like the laziest thing ever there's got
52:28 - to be a good way to debug this
52:35 - so this definitely gets cleaned up the
52:37 - question does everything that's made
52:39 - inside of the loss the loss and the
52:43 - predict function get cleaned up
52:45 - it would seem not I'll hold on let's
52:49 - comment this out it's not working but no
52:56 - okay that's good to know
52:57 - okay hold on alright need to debug this
53:02 - somehow one thing I could do is trucks
53:04 - are commenting stuff out to see like
53:06 - where is the memory leak so one worry I
53:08 - have I really think Lawson predict those
53:11 - functions generate a lot of tensors I
53:13 - believe TF tidy should clean up anything
53:16 - but let's just for the sake of argument
53:19 - comment this out and now of course the
53:24 - learning is no longer happening and what
53:27 - I might as well do is console.log the
53:32 - amount of tensors not have to like ask
53:35 - for it whoops
53:37 - um what did I do wrong TF memory num
53:42 - tensors what is it how come I can't
53:43 - remember what it is num tensors no yes
53:48 - it's not a function it's just num
53:50 - tensors sorry everybody okay so and I
53:54 - need another parentheses here a little
53:55 - digression there all right all right so
53:58 - we can see it's growing so let's keep
53:59 - commenting stuff out to see like what's
54:02 - causing the memory leak let's comment
54:05 - out this whole area down here ah good
54:10 - news everybody the memory leak is in
54:12 - that part let's put this back just to be
54:16 - sure okay ah so the memory leak is
54:24 - definitely down here and I probably
54:27 - created oh my goodness oh my goodness
54:37 - no I'm not sure well let's put this back
54:39 - in I thought I saw it but then I didn't
54:41 - again so this is a tensor and I'm
54:45 - disposing it Oh predict aha
54:52 - the predict function makes other tensors
54:54 - and predict got cleaned up because it
54:57 - wasn't ID but I'm just manually
54:59 - disposing the Y's down there that's what
55:02 - it is so let me use tidy
55:04 - I guess so let me do this let me put
55:12 - this up here so this is really what I
55:16 - need to tidy so instead of disposing
55:21 - manually I kind of like the specific
55:25 - things manually the tidy thing kind of
55:27 - freaks me out but the problem with this
55:29 - is I have a scope issue which is that
55:33 - line Y right no matter what I do if I
55:37 - take this out here like this is going to
55:38 - tidy everything so I guess it's not the
55:41 - biggest deal at the moment at least for
55:43 - me to just put everything inside the
55:47 - tidy well that's the let's put
55:51 - everything inside the tidy for right now
55:53 - there's probably a way I could simplify
55:55 - that but this should work let's give
55:57 - this a try there we go there's only ever
56:03 - five tensors all the time so there's no
56:06 - more memory leak five tensors linear
56:09 - regression with gradient descent tensor
56:11 - flow das interactive here it is so
56:14 - what's what's left here so number one
56:16 - things that could be improved pause
56:25 - oh so I could say off all right
56:34 - so line why why I asked so I could
56:39 - actually just put a tidy here and have
56:41 - it returned okay so I wanted to talk
56:46 - through some things that could be
56:47 - improved but already me I am so me in
56:49 - the chat made a very good suggestion
56:50 - this is very awkward how I put
56:52 - everything in tidy so unnecessary let me
56:56 - take that out because predict returns
56:58 - something I can actually just put the
57:02 - tidy right here I don't know why I
57:03 - didn't think of that
57:04 - like I can actually just it's only this
57:06 - predict function that so I can actually
57:09 - put the tidy right here and I can use my
57:16 - fancy es6 arrow syntax right and the
57:19 - return is now assumed and then I can
57:22 - just say why is stuff disposed so this
57:24 - should work right tidies not going to
57:26 - tidy up the this y value but I can I can
57:30 - dispose that manually once I have the
57:32 - values so I think this will do the trick
57:34 - let me just take a look at this yeah
57:37 - so this I like better and there's
57:39 - probably other styles or ways you can do
57:41 - it the point is you've got to keep track
57:43 - of all the tensors you're making and
57:44 - dispose that okay pause edit point let
57:48 - me think what are some other things
57:50 - right I could put that lot let's use the
57:52 - data sink in there but I'm gonna keep
57:54 - that as a separate line all right
57:56 - ah interesting
58:07 - alright let's add a few more things Risa
58:10 - in the chat asks can you print the loss
58:13 - that would be really useful for us to
58:15 - see the loss I could even graph it so
58:17 - I'm sure there's a way for me to do that
58:19 - well I need to grab the loss somewhere
58:23 - so the loss I think the loss would come
58:27 - out here right I wonder if it would get
58:33 - returned
58:38 - let's just take a look at that oops
58:42 - Oh No
58:46 - hold on I mean I could call the loss
58:50 - function but maybe let's not the return
59:01 - is implied sounds better than assumed
59:03 - yes thank you that's a good point
59:07 - hello easily coding train sponsor
59:16 - outliers hold on what are some things
59:19 - that I can do to improve this though
59:22 - adding the loss I'm gonna leave that as
59:24 - an exercise so Matt you were at the Edit
59:28 - point where I was about to like add some
59:30 - improvements anything else
59:34 - tensor board to visualize the training
59:36 - graph is it
59:37 - it's 5:30 so I also have to go I do
59:40 - think I completed this I just want to
59:43 - check I just want to just check the
59:50 - reason why I have to go is I have to be
59:52 - home at a certain time and I just want
59:55 - to make sure it's not like an emergency
59:56 - for me to be home okay yeah all right so
60:00 - I've got like five or ten minutes I
60:02 - think I'm good try another optimizer all
60:05 - right so these are things but these are
60:07 - things I want I think I want this video
60:08 - to be over so you get the loss with a
60:13 - callback ah interesting
60:17 - all right print mean squared error all
60:21 - right all right so let me why don't you
60:27 - tidy the tensors inside the predict yeah
60:31 - that would have been another way to do
60:32 - it there's just so many different ways
60:34 - all right so here okay okay so I think
60:37 - we're gonna wrap this video up I'm
60:38 - getting all these great suggestions from
60:40 - the chat right you know I could have
60:42 - tidied this the tensors here and predict
60:44 - you know so number one is I'm um this
60:48 - code is gonna get posted to the coding
60:49 - train web
60:50 - encoding challenges make your
60:51 - improvements and add them as community
60:53 - contributions some things that I would
60:55 - love to see visualized graph the lost
60:58 - value I think there's a way to get the
61:00 - loss of sure there's a way to get the
61:01 - lost value out of that function that's
61:03 - one idea
61:08 - oh yes K week Mon suggested maybe trying
61:13 - some of the other optimizers so what
61:15 - happens if I go to the tensorflow chess
61:17 - documentation and like just use some of
61:20 - these other optimizers what are they
61:21 - what will they do do you get better or
61:23 - worse results can you make the learning
61:25 - rate somehow interactive adjust the
61:27 - learning rate you know I don't know if
61:29 - you could come up with any really clever
61:31 - visual ideas with this but anyway so but
61:39 - I think I'm good I think I have the
61:41 - basic idea so if you really want to dive
61:44 - as deeply as you can into linear
61:46 - regression with gradient descent you can
61:48 - go back and watch my other videos where
61:49 - I did this with just JavaScript in p5.js
61:52 - now you've seen it with JavaScript p5
61:55 - yes and tensorflow yes so I look forward
61:57 - to your feedback and hearing more about
61:59 - it a more tensorflow digest videos to
62:01 - come and I want to get to some actual
62:03 - more practical things that you might
62:04 - want to do for interactive creative arts
62:06 - projects but I'm still in the weeds here
62:08 - of just trying to understand the nuts
62:09 - and bolts of how the library works so I
62:11 - hope you're enjoying that and I look
62:12 - forward to seeing you in future videos
62:18 - alright put on hold I am reading your
62:23 - comment all right so I am done for the
62:33 - day I have done a lot of live-streaming
62:36 - this week pat myself on the back at
62:39 - least 5 hours of live streaming this
62:41 - week which is pretty good hopefully that
62:43 - makes up for that week's that I've
62:44 - missed next week I think I might
62:47 - actually not be live-streaming on
62:48 - Fridays this summer I think I'm gonna be
62:50 - doing like Wednesdays and Thursdays
62:52 - during the day which is my New York time
62:55 - but I'm kind of gonna it's gonna be kind
62:57 - of gametime decision each week stay
62:59 - tuned so what's the what's the stuff you
63:02 - need to know about
63:03 - so if you go to the coding train comm
63:05 - and you click here to subscribe on
63:08 - YouTube it will take you to the channel
63:11 - hey yeah I totally want to subscribe
63:14 - I may not why not
63:19 - I'm so close to five hundred thousand
63:22 - which is nuts you can get fifty thousand
63:25 - subscribers in the next like ten minutes
63:27 - that would be exciting so good look it
63:30 - says I'm live now which is this but when
63:33 - I schedule a live stream okay I'm
63:36 - getting some important messages I really
63:37 - gotta go putting on my device try asking
63:52 - it now I missed it show us the hoodie
63:58 - you want so this hoodie I don't know if
64:00 - so it's the hoodie available yet
64:02 - oh the music's not working sorry about
64:07 - that everybody I forgot it turned it off
64:10 - the hoodie will soon be available on the
64:14 - store is a coating train that store envy
64:20 - calm so this is the URL for the coding
64:24 - train store for those of you who are
64:26 - interested but right now if you go here
64:30 - hoodie this is oh that's a different
64:32 - hoodie zip up hoodie I think is this the
64:36 - new one or not
64:39 - interesting question I think this I have
64:42 - to see let's click on this it's
64:46 - expensive yeah no this is the old one so
64:49 - you could get this but I'm going to
64:51 - change it this is a new American Apparel
64:53 - hoodie I like it better and it also has
64:54 - the coding train on the back although
64:57 - this was the sample so I'm gonna we're
65:00 - moving I'm gonna move it down to about
65:01 - here if anybody has any suggestions or
65:04 - ideas by the way all of this is
65:06 - fulfilled through a website called
65:08 - printful comm so in theory I have the
65:11 - capability to produce any like
65:14 - ton of like other things but it's just
65:17 - not my priority merchandise I don't
65:19 - really know what I'm doing with that but
65:20 - people were asking me about the hoodie
65:21 - all right yeah printful ships
65:24 - internationally it's not me who's
65:25 - sending it out it's done through a print
65:27 - on demand service it is marked up so
65:30 - there's like a shipping and a slight
65:31 - markup so if you if you buy the hoodie I
65:32 - probably make like four or five dollars
65:34 - or something I don't know what the exact
65:35 - amount is it sort of depends on what the
65:38 - there's always like all these weird
65:39 - taxes and shipping things but I don't
65:41 - you yeah it's actually no the patreon
65:44 - stuff I actually mail myself if you're
65:47 - funding through patreon or ever but the
65:50 - merchandise stuff just gets fulfilled
65:51 - through the printable store I don't have
65:52 - to do anything do you know uh then there
65:57 - the the ner engineer question do you
65:59 - know any reference how to link
66:01 - processing - Java - Adam yes so what you
66:04 - want to look at is commander processing
66:08 - command line so you want to look at
66:19 - command hold on don't where I know the
66:21 - camera went off everybody there we go I
66:25 - think somebody's already done this with
66:28 - like an atom plug-in but if you go here
66:30 - to this wiki command line processing
66:34 - there is a command line tool that you
66:37 - can install and then you can have Adam
66:44 - like execute the sketch via a command
66:46 - line so you have to like configure Adam
66:47 - in a goofy way but this has been done I
66:49 - know it's done with sublime Adam editor
66:51 - processing gorg yeah so like yeah so I
66:57 - have a feeling yeah so you can see
67:00 - somebody's already done this already and
67:02 - figured it out so you have to install
67:04 - processing - Java and this is so these
67:07 - are the instructions and then you have
67:08 - to add stuff to your path and that sort
67:09 - of thing okay let's look at the chat
67:17 - dan why do you know everything ask melon
67:20 - goggles it is absolutely an illusion I
67:22 - definitely do not know everything and I
67:25 - just know I just know like just enough
67:27 - to like get through the tutorial and
67:28 - then like if I were to go two steps
67:30 - further there'd be like a hundred things
67:31 - I don't know and you view all of you I'm
67:34 - sure seen my disastrous live streams
67:36 - where I get completely lost you if any
67:39 - of you will try to watch some of my
67:40 - tutorials where I tried to explain
67:41 - calculus stuff you would see that I
67:43 - don't know everything okay all right so
67:53 - um thank you for all the questions in
67:55 - the chat oh it's for the last question
68:00 - Justine asks what's up with the camera
68:07 - going off every 30 minutes have you ever
68:09 - watched the TV show Lost basically this
68:12 - is the what's the thing called the hatch
68:14 - I'm in the like Swan station and if I
68:17 - don't press the button every 30 minutes
68:19 - the world will end that's why I press
68:21 - the button every 30 minutes no the
68:23 - cameras have a these cameras are set to
68:26 - go to sleep and there are I've been many
68:28 - people have given me many suggestions to
68:30 - get them not to do that and I had not
68:31 - successfully been able to implement any
68:33 - of them so I that's that's kind of a
68:36 - goal I have for the summer for sure oh
68:38 - did I just spoil loft for people I'm
68:40 - terribly sorry the thing is that didn't
68:43 - really spoil anything a little bit loss
68:49 - was one of my favorite television shows
68:50 - I need some what am I trying to watch
68:52 - some stuff this summer but anyway
68:58 - watching I don't think you're gonna find
69:00 - me doing any blockchain tutorials sorry
69:02 - to dissapoint would you possibly do a
69:07 - time series prediction with lsdm
69:08 - recurrent neural networks the example of
69:10 - I don't know everything
69:11 - yeah but I gotta figure that out alright
69:14 - thanks everybody thanks for watching
69:15 - well well I guess my fingers are like a
69:19 - chef I don't know it's a sort of
69:20 - expression that I've adopted from
69:23 - somebody else if anybody knows what that
69:24 - reference is that's gonna that's crazy
69:26 - to me
69:28 - and did I like the lost ending not so
69:36 - much to be honest but it when I think
69:38 - back about that show it gives me fills
69:40 - me with such like amazing memories I was
69:43 - really interested this is the podcast so
69:45 - the ending I forgive you lost for that
69:47 - ending it was fine you did your best
69:49 - I couldn't have done any better that's
69:52 - for sure
69:54 - that was not my favorite season of loss
69:56 - sentiment analysis I definitely want to
69:57 - do yeah alright so there's going to be a
70:02 - lot of edited tutorials coming out just
70:04 - to recap there's I think five or six
70:06 - tutorials on promises async and await
70:08 - and now this coding challenge of linear
70:11 - regression with gradient descent I will
70:13 - see you all next week sometime stay
70:15 - tuned
70:16 - - oh hi Cody garden
70:20 - Cody garden was live streaming at the
70:22 - same time as me earlier today Cody
70:24 - garden seems to live stream like 12
70:27 - hours a day I don't know how that's
70:28 - possible is probably not that much but
70:30 - there's a lot of Cody Gardens live
70:31 - streams so everybody check out cutting
70:33 - garden a sponsor - Cody drain oh I can't
70:37 - put on my music and do my goofy sponsor
70:39 - advertising again see everybody I'm
70:41 - gonna I'm gonna play out the outro now
70:43 - why not cuz it's the end of the day see
70:45 - you all later
70:48 - [Music]
70:52 - my microphone
70:59 - [Music]
71:10 - [Music]
71:23 - imagination creation
71:38 - [Music]
71:44 - [Music]
71:47 - you