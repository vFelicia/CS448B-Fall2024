00:00 - hello this is the first video in a
00:03 - series of videos I'm making about the
00:05 - Microsoft Kinect this thing so what is
00:09 - this thing how does it work and how do
00:10 - you write your own software that makes
00:12 - use of this thing how can you do all
00:13 - sorts of creative coding projects now
00:15 - there's a lot of different programming
00:17 - languages environments and frameworks
00:18 - and libraries for how you might make use
00:20 - the Kinect I'm going to use this thing
00:23 - called processing processing 3 the third
00:25 - edition version of processing which is a
00:29 - Java based programming environment open
00:30 - source environment that there is a
00:33 - connect several different connect
00:34 - libraries for eventually I will hope to
00:37 - make a video where I look at p5.js which
00:39 - is a JavaScript framework for doing
00:42 - creative coding in the browser and how
00:43 - might you get the stuff from the Kinect
00:47 - what is this thing called the Kinect in
00:49 - the browser itself which I think will be
00:51 - an exciting thing to see as well but in
00:53 - this first video what I want to do when
00:56 - I get into the code really in the next
00:57 - video and what I want to do in this
00:58 - video is give you an overview so what
01:01 - are the different editions of the Kinect
01:04 - there's a bunch of different ones that
01:05 - you could buy what are the pieces that
01:06 - you need how do you get the library to
01:09 - make use of the Kinect that sort of
01:10 - stuff and you can see I have a basic
01:12 - example that's running behind me with
01:14 - the Kinect version 2 and I will talk
01:16 - through the pieces of this code so first
01:19 - let's think about the different versions
01:21 - of the Kinect so this is this one here
01:23 - Refik glasses I'm gonna this one is the
01:26 - 14 model 14 14 this one is the and I'm
01:30 - going to I'm going to come over here and
01:32 - try not to trip over to myself and I got
01:36 - to grab this eraser for a second so
01:40 - let's make a list here so the two key
01:43 - pieces of information for you or you
01:45 - need to decide are you using the Kinect
01:48 - version one or the Kinect version two
01:53 - I'm probably gonna get lots of stuff
01:55 - wrong here that you can write in the
01:57 - continent so I'll put little annotations
01:59 - on the YouTube video that fix them but
02:00 - hopefully I'll get things loosely right
02:02 - so the original Kinect version one model
02:05 - 1414 is the one that came out I think I
02:07 - was new
02:08 - number 2011-12 somewhere around there I
02:12 - remember the weekend it came out people
02:13 - people were quote-unquote hacking it but
02:15 - really just making but about hacking at
02:17 - I need making open-source drivers to to
02:19 - read the data driver being a thing that
02:22 - your computer runs to talk to the
02:23 - hardware device and so when that came
02:27 - out a library I worked on the library
02:30 - called open connect for processing and
02:34 - the reason why it's called open connect
02:38 - is because it's making use of the open
02:40 - connect an open connect an open source a
02:43 - driver for for connecting to the connect
02:46 - which is also known as a Lib free net so
02:52 - this is sort of the genesis of all of
02:55 - this the thing that I built for
02:57 - processing is just a thin layer on top
02:59 - of work that lots of lots of other
03:01 - people did which allows you to get the
03:03 - data from the connect now let's come
03:05 - back to the connect like what is it's
03:07 - over here and then I'll get to the other
03:09 - editions in a second like what is this
03:11 - thing so this is the original connect
03:13 - and you can see here that there are
03:15 - three little circles on here it's like a
03:17 - little nice little friend with three
03:19 - eyeballs and what do each of these
03:21 - eyeballs do so one of them if we a
03:25 - camera oh oh
03:26 - Schiffman you i suck at making these
03:28 - videos okay I'm just going to go anyway
03:31 - okay so this is the connect uh and it
03:35 - has three little eyeballs one of which
03:38 - is an infrared projector so this is this
03:44 - is what the 14:14 does and I'll talk a
03:46 - little bit about any a different marker
03:48 - here what happens once you get to the
03:50 - connect version to how that works
03:52 - differently it has an infrared projector
03:55 - which sends out infrared light into the
03:57 - room then it has what I would call you
04:01 - know could call it a sensor or a camera
04:02 - but it has an infrared camera to read
04:05 - the infrared light Pacific in the room
04:08 - what is infrared light it's you know
04:10 - light that's all around us but is it
04:11 - visible somebody with a physics degree
04:13 - could explain that better but this is
04:15 - blasting out infrared light this
04:17 - infrared camera is reading it so what
04:20 - what is the value of doing this so that
04:22 - interesting
04:22 - is the kind of light that it's passing
04:24 - out is actually a whole lot of infrared
04:27 - dots it's projecting a lot of infrared
04:29 - dots into the room it looks like this
04:31 - and it's a very specific pattern of dots
04:34 - and the Kinect itself it knows what that
04:37 - pattern of dots is supposed to look like
04:38 - so it's that pattern of dots if I have
04:41 - the Kinect here it's blasting infrared
04:43 - light lands on a flat surface the
04:46 - infrared camera that's reading where
04:48 - those dots landed that's seeing those
04:49 - dots reflecting back is going to see
04:52 - like oh it matches exactly the pattern
04:54 - of dots that I know that's a flat
04:56 - surface but if this surface was curved
04:58 - those dots will appear distorted by
05:00 - analyzing that distortion the Kinect can
05:03 - recognize what things are closer and
05:05 - what things are further away so the
05:08 - value of this is it is often referred to
05:11 - you can think of it as a depth camera or
05:13 - a depth sensor this is what this
05:15 - infrared projector and infrared camera
05:16 - are doing they're measuring the depth in
05:19 - of each pixel in the room so while a
05:22 - regular web camera says here's a 640 by
05:25 - 480 image each pixel has a red green and
05:27 - blue value and it's beautiful isn't it
05:30 - the colors of the rainbow or the hair in
05:32 - this image the Kinect is saying I see I
05:36 - don't see RGB what I see is I see a
05:39 - pixel and instead of telling you what
05:40 - color that pixel is I'm going to tell
05:42 - you how far is that pixel away from the
05:45 - sensor and this is incredibly valuable
05:47 - in computer vision you know one of the
05:48 - classic computer vision problems that
05:51 - people try to solve is background
05:52 - removal you know that's why I have this
05:54 - green screen okay I have to go
05:56 - underneath this here okay I have the
05:59 - obstacle course in my office I'm going
06:00 - underneath this to turn this camera back
06:02 - on and I'm coming back underneath I have
06:05 - this hello I have this I have this green
06:08 - screen here behind me you can see and so
06:10 - the camera is saying every green pixel
06:13 - remove it and put the stuff from the
06:15 - computer behind it but if I had a Kinect
06:17 - I don't have to say look for the green
06:19 - pixels behind me I can just say look for
06:21 - any pixel that's farther than two feet
06:24 - or something out of centimeters I'm
06:28 - trying to be a metric I'm trying to be
06:29 - metric I want to be a metric person but
06:31 - I'm not so you could remove you could
06:34 - you can analyze things
06:35 - makes it really easy to find a human
06:37 - being in the room because the human
06:39 - being has a certain kind of shape it
06:41 - makes it really easy to do quick and
06:42 - dirty 3d scanning there's lots and lots
06:45 - of possibilities of what you can do once
06:46 - you have access to the depth now there
06:49 - was this third little eye here and this
06:51 - by the way is just an RGB camera so one
06:55 - of the things the Kinect can also do is
06:57 - just see the colors in the room so in
07:00 - addition to having this infrared camera
07:01 - it has an RGB camera now there's a bit
07:03 - of a problem here which is that notice
07:07 - how both of these things are not in the
07:10 - same place so the infrared camera sees
07:14 - the depth of a given pixel at a
07:16 - different place that the RGB camera sees
07:18 - that color so this is an alignment
07:20 - problem a calibration problem where the
07:23 - the color pixels don't necessarily line
07:25 - up exactly with the depth pixels and
07:27 - there are lots of strategies for solving
07:30 - this problem and lots of frameworks and
07:33 - libraries in particular the official
07:35 - Microsoft SDK which has on things baked
07:38 - into it that do this for you but one of
07:40 - the nice things that we'll see once we
07:42 - get to the Kinect Veet version 2 is it
07:44 - has something called a registered image
07:46 - which is an image that aligns the depth
07:50 - pixels with the color pixels okay so
07:52 - this is what the Kinect does and I
07:55 - really described here what the Kinect
07:57 - version 1 does there was also a model
08:00 - 1473 that came out I don't know a year
08:03 - or two later um this one has some
08:05 - problems in particular there's a little
08:07 - bit of a bug with currently with running
08:09 - it with the processing library although
08:10 - it does work kind of only will work
08:12 - every other time can't figure it out for
08:14 - the life of me so but both of these will
08:17 - work with the library what you need to
08:19 - look for the library is the version 1
08:21 - examples so that's that now in between
08:25 - here there was like this Kinect for
08:27 - Windows and I think this was like a
08:30 - version of the Kinect that the Microsoft
08:31 - made to plug into like Windows computers
08:34 - originally this was designed for use
08:35 - with the Xbox for a game for games that
08:38 - you would play by you know dancing I'm
08:41 - kicking my leg by the way if you can't
08:42 - see that and but then Microsoft realized
08:47 - there's I don't know what my
08:48 - what's in Microsoft's cases but I've
08:50 - speculating here but that to make a
08:52 - version that's designed to work with
08:53 - just regular old laptops and computers
08:56 - I'm not sure if this one works with the
08:58 - processing library but more recently and
09:01 - I have this one plugged in and like
09:03 - mounted on the wall over there so I
09:04 - can't hold it up and show it to you the
09:06 - Kinect version 2 is a newer and quite
09:09 - significant upgrade from the first
09:11 - Kinect and it actually uses a completely
09:14 - different technique it uses infrared
09:17 - light but it uses a technique called
09:18 - time-of-flight so it sends the infrared
09:20 - light out measures how long it takes for
09:22 - it to bounce back and that how long that
09:25 - takes
09:25 - let's the sensor know how far away
09:27 - things are kind of like a bat maybe does
09:29 - stuff with sound to see I don't know
09:31 - yeah dolphins do stuff like that but all
09:33 - the sound so with light bouncing it back
09:35 - and forth the new Kinect does that and I
09:38 - suppose it's a bit more accurate it's
09:41 - faster and the RGB camera is also in the
09:45 - new Kinect is higher resolution okay so
09:47 - that's the basics overview and if I come
09:50 - back over here you can see now now I'm
09:53 - running an example I have the Kinect
09:54 - right over here you can't see it I could
09:56 - I could maybe like turn like kind of
09:59 - like if I hold it up over here can there
10:00 - you go there it is this is the new one
10:03 - I'm gonna put it back that felt like a
10:05 - little scary like everything was going
10:06 - to fall over and you can see now that
10:09 - what you're seeing in this particular
10:12 - image is an example of a processing
10:14 - sketch which is rendering all of the
10:17 - pieces of what the library what the
10:19 - Kinect offers now oh but I have
10:21 - something more to mention about this but
10:23 - I'll get to that in a second so up top
10:25 - you can see that's just the RGB image so
10:28 - it's like I have a webcam over here I
10:29 - have a webcam over here high webcam that
10:33 - sort of thing right so that's the webcam
10:34 - that's the RGB camera and it's actually
10:37 - I believe I didn't actually check but
10:39 - it's a pretty high resolution image down
10:42 - below this is the raw feed of what the
10:44 - infrared camera is seeing so this is
10:46 - what the infrared camera is seeing and
10:48 - it uses that to extrapolate depth so
10:50 - mostly just looks like this creepy thing
10:52 - but you can make use of that you can get
10:54 - that image as well up top at the top
10:57 - right this is what's known as the depth
10:59 - image so the what the Kinect is
11:01 - measuring
11:02 - is in millimeters it's measuring a value
11:05 - between 0 and 4500 how far is the thing
11:08 - away from the camera and then often a
11:10 - depth image is used to visualize that
11:13 - data so in this case you can see as I
11:15 - start to go further and further back I
11:17 - get brighter as I start to come closer
11:19 - and closer I get darker so it's mapping
11:21 - the the color of every pixel to how far
11:25 - away it is and you can see just from a
11:27 - standpoint now how much easier that
11:29 - might be to pick out my hand right
11:31 - because my hand is the only thing that
11:32 - has this very very dark color as opposed
11:35 - to other things now there is something
11:37 - funny in the back what's up there up oh
11:40 - that's a window I was like what about
11:42 - black square up there that's how window
11:44 - that's the door you can see in all sorts
11:46 - of things inside inside this room that
11:48 - you may not have seen before
11:49 - and then down in the bottom right this
11:51 - is the registered image so this is not
11:54 - part of you use the version one connect
11:58 - with the open connected library this is
12:00 - not part of that however it with version
12:03 - 2 this is the image that aligns all of
12:06 - the RGB values from the webcam with the
12:09 - depth value so if you wanted to
12:11 - hopefully it's something I might be able
12:12 - to demonstrate it in some video is just
12:14 - do background removal where you see only
12:15 - me and I take I get rid of all the
12:17 - pixels that are behind me um that might
12:20 - be something that I could do here with
12:21 - that particular image oh did the oh the
12:23 - laptop went to sleep come back wake up
12:25 - okay so a couple more things so what I
12:30 - want to show you now is how do you get
12:31 - this library to run this like this
12:33 - particular example so a couple things
12:35 - one is here's the I put all this in the
12:38 - description of the video this is the URL
12:39 - the the library is at github.com slash
12:43 - Schiffman open connect for processing
12:45 - you don't need to go to that URL but
12:46 - that's where the source code is there's
12:48 - a little bit of documentation there I
12:50 - want to make give a big THANK YOU to
12:52 - Tomas Sanchez lending I might not have
12:55 - pronounced his last name correctly he
12:57 - wrote all the code for making this
12:58 - library work with the connect version
13:00 - two so I worked on to the version one
13:02 - number of years ago and sort of
13:04 - floundered and Thomas came back and
13:05 - revived this and really helped over the
13:07 - summer and there is also I have a little
13:10 - page that has some additional
13:12 - documentation it's Schiffman dotnet
13:14 - slash P 5
13:15 - / connect and you know this is some text
13:18 - that kind of goes through the different
13:20 - versions and some of these examples as
13:22 - well that I'm going to cover in the
13:24 - videos now in order to get the actual
13:26 - library itself what you need to do is go
13:29 - to one first you need to download
13:31 - processing if you don't have that
13:32 - already that's at processing org then
13:35 - what you'll need to do is once you have
13:36 - processing it might look just like this
13:39 - to you something empty you're going to
13:41 - go to sketch import library add library
13:45 - now you can see that I have already
13:46 - knife three libraries here I already
13:48 - have that library but I'm going to
13:50 - pretend that I don't for a second I'm
13:51 - going to go to add library which opens
13:53 - up this contributions manager I can type
13:56 - in connect right here and this is
13:57 - something really quite important now to
13:59 - bring up so there are several different
14:03 - libraries there is by the way something
14:05 - called simple open and I which is an
14:07 - older library open and I was an open
14:09 - source platform open source framework
14:11 - for doing skeleton tracking meaning
14:14 - finding of human form where the hands
14:16 - are where the head is which is very very
14:18 - powerful and things that you can do with
14:19 - the Kinect I'm starting with just the
14:21 - raw depth data but open and I think was
14:24 - purchased by Apple have been kind of
14:25 - like shut down as an open thing but
14:27 - there are some efforts to revive it and
14:29 - so you could Google around and that's
14:31 - something that you could possibly use
14:32 - I'll try to include some links but you
14:33 - can see that it's currently this simple
14:35 - open eye it's no longer compatible with
14:37 - processing 3 that's why it's grayed out
14:39 - Connect v2 for processing this is a
14:42 - library that makes use of the Microsoft
14:44 - official SDK and I'm going to
14:46 - demonstrate that using a PC in a later
14:48 - video this is a key this is a really
14:53 - great thing to use if you want to get
14:55 - all of the magic that Microsoft has
14:57 - spent all this time developing so what
15:00 - the Kinect just gives you is raw depth
15:02 - theta raw RGB data but what the
15:04 - Microsoft SDK does is it pulls that data
15:07 - in and on and it analyzes it and finds
15:10 - where's the human being like what kind
15:11 - of muscle are they making like where's
15:13 - their head like is their hand open or
15:16 - closed and it's so much through a layer
15:18 - of analysis on the raw def data that
15:20 - will give you a ton of information so
15:22 - but for that you do need to use a
15:24 - Windows machine
15:24 - of course there are some strategies for
15:25 - like sending the data from windows
15:27 - chained to another machine to like a
15:28 - WebSocket what's a WebSocket all sorts
15:31 - of stuff but we'll come to that in a
15:33 - later video and so those would be the
15:36 - two libraries that but the library I'm
15:38 - using today which you know is already
15:40 - installed you can see by the green
15:41 - checkmark you would just need to click
15:43 - it and click this install button and it
15:46 - would download and install that this is
15:49 - the library I'm using today it uses open
15:51 - source drivers it only looks at the raw
15:54 - depth data so this is good for a bunch
15:56 - of different kind of creative
15:57 - applications that I hope to show you in
15:59 - the next set of videos so this was a
16:02 - long rambling 16 minute explanation
16:04 - about the connect that you may or may
16:06 - not have found useful or interesting but
16:08 - I imagine you've already turned it off
16:10 - if you didn't and in the next video what
16:12 - I will demonstrate is is just how to
16:14 - write a program that gets that depth
16:16 - image and once it I mean bring that back
16:19 - oh it's not running here get gets that
16:21 - depth image and maybe visualizes that
16:23 - depth image in some way so that's where
16:25 - we'll start I'm going to look at a
16:26 - couple other scenarios along the way as
16:27 - well okay and so thanks for being here
16:30 - and watching and talk to you soon