00:00 - (bell rings)
00:00 - - Hello, welcome to a new
session from, I don't know,
00:03 - is it the Machine Learning Course,
00:05 - is it the Programming with
Text Course, I don't know?
00:07 - I'm just here.
00:08 - I'm just a person who's here.
00:10 - And this session which will
be a whole bunch of videos,
00:13 - is about a topic, word2vec.
00:16 - (bell rings)
00:18 - I'm ringing the bell way too much.
00:19 - So, first of all, I want to
mention something very important.
00:22 - I've known about word2vec.
00:24 - And I've used it in
projects for a little while,
00:27 - but I don't think I ever
really understood it.
00:30 - (laughs)
00:31 - And I don't even know that
I really do understand it.
00:32 - But I definitely improved my
understanding of it vastly,
00:35 - after reading this amazing
tutorial by Allison Parrish.
00:39 - It's posted as a Gist on Github.
00:42 - It's a Python Notebook,
Understanding Word Vectors,
00:45 - by Allison Parrish.
00:47 - You know, honestly, if I'm being truthful,
00:49 - you should just stop this video right now
00:51 - and read this instead.
00:52 - But, you know if you, some people seem
00:54 - to like listening to me prattle on.
00:55 - Which is fine, you could keep
watching if you so choose.
00:57 - Read this after then, at the very least.
01:00 - And so this tutorial is
released under Creative Commons
01:04 - by 4.0 license.
01:05 - The code itself is the
Creative Commons Zero license.
01:08 - So you can re-use this material,
01:10 - which is what I'm doing right now.
01:12 - I don't usually do this.
01:13 - I mean my stuff is always
based on other people's stuff,
01:15 - but this first video I'm
really going to like,
01:17 - talk through what's in this
tutorial in my own words.
01:21 - But if you do the same please reference
01:24 - with attribution according to the license.
01:26 - Okay, so I also want to mention
01:28 - that Allison Parrish has a wonderful Talk,
01:31 - it's on YouTube, I will link to it,
01:32 - called Experimental Creative Writing
01:34 - with the Vectorized Word.
01:35 - From the Strange Loop Conference.
01:37 - So I also encourage you to take a look
01:39 - at that as inspiration and background
01:41 - for what it is I want to show you.
01:43 - My end goal with this tutorial
is to get to the point
01:47 - where I have a P5 JavaScript
sketch in the browser,
01:50 - where I can do stuff with word2vec.
01:53 - What is word2vec?
01:54 - The point of this video that
you're watching right now,
01:56 - which I'm taking a very
long time to start,
01:58 - is just to answer the
question what is word2vec?
02:01 - By the end of it, I want to
use word2vec in the projects
02:04 - to make weird stuff happen
with text on a web page.
02:08 - All right.
02:09 - How are we feeling?
02:10 - So, all right.
02:11 - So let me come over here for a second.
02:14 - Because I've written word2vec up here,
02:15 - and that's going to help me.
02:18 - The idea of word2vec and others,
02:22 - this is a machine learning process,
02:24 - similar to other things that I've done
02:26 - that looked at like, classification.
02:28 - Is this image a cat or a dog?
02:30 - Or a regression analysis.
02:32 - What's the, what's, can you
predict the price of this house
02:35 - based on certain properties of that house?
02:37 - These are classic machine
learning examples.
02:38 - Word2vec is a particular
machine learning model
02:43 - that produces something
called a word embedding.
02:46 - Now, and that's a very, very fancy term.
02:49 - And what it means is that any given word,
02:52 - like apple, can be associated
02:57 - with numbers, a vector.
02:58 - This we can basically somehow come up
03:01 - with this sort of like
numeric mathematical
03:04 - essence of this word, as
some array of numbers.
03:09 - Like 0.7 and 1.2
03:12 - and -0.345, et cetera, et cetera.
03:16 - And there's going to be some
amount of numbers in here.
03:18 - This seems like a crazy thing.
03:20 - Why would I ever want to have a word
03:24 - associated with an array of numbers?
03:27 - Well one of the things that one can do
03:29 - with arrays of numbers is math.
03:32 - Linear algebra, multiplying,
03:35 - subtracting, averaging, adding.
03:38 - So, we know we can do that
with arrays of numbers.
03:41 - And this is the kind of thing that happens
03:43 - in lots of my other tutorials
with programming graphics,
03:45 - and pixel processing and machine learning.
03:49 - But one thing we wouldn't know how to do
03:51 - is how would we say, you know,
03:55 - apple plus, I was going to say
plus orange, but that could be.
03:59 - I was trying to like come up
with something, a good example.
04:01 - This is what happens when you don't plan
04:02 - these tutorials in advance.
04:04 - I could come up with
an example on the fly.
04:06 - Apple plus purple,
04:10 - could this equal plum, maybe, right?
04:13 - Like, in other words,
like, I'm trying to come up
04:15 - with some like pseudo math.
04:17 - Like, let's take these two
words and add them together.
04:20 - like cat plus cute maybe
that equals kitten.
04:24 - And can I take, like,
we're not talking about
04:27 - concatenation, apple-purple.
04:28 - We're taking apple plus purple.
04:29 - Could I get that sort
of mathematical essence
04:31 - of these words, add them
together and get a new word?
04:34 - Well, the theory, the
prompt, the idea here,
04:38 - the argument that I am making to you
04:40 - is that word2vec is a mechanism.
04:42 - By which you can do stuff like this.
04:44 - Right there in your code.
04:47 - If I could quantify the word apple
04:49 - as a series of numbers
and I could quantify
04:51 - the word purple as a series of numbers,
04:56 - then couldn't I just add
all those numbers together?
04:59 - I would get a new series of numbers.
05:01 - And then I might look and find which word,
05:04 - or has a set of numbers that is most close
05:06 - to this set of numbers.
05:08 - How could I find the similarity?
05:11 - I could calculate a similarity score
05:12 - between any two sets of numbers.
05:13 - I could find the word
that is the most similar
05:15 - to this plus this and
maybe it would be plum.
05:18 - Why would it be plum?
05:20 - Is that magic?
05:21 - Is it because of what
data that's word2vec model
05:23 - was trained on?
05:24 - Well, yes, it's the latter.
05:26 - But, and so I want to get to all of that.
05:28 - Okay, this is my sort
of like zoomed out view
05:30 - of why we're doing this.
05:31 - Let's come over and look at what Allison
05:32 - has in her particular tutorial here.
05:35 - Which is a really nice example.
05:38 - If I look at this, we can say like,
05:40 - well imagine like a
really simple case, right?
05:42 - I was sort of saying, over here, each word
05:46 - gets a list of maybe 100
numbers, maybe its 300 numbers,
05:49 - maybe its 1000 numbers.
05:51 - This is up to us to sort
of figure out and decide
05:52 - based on what we're trying to do.
05:54 - But what if we simplify that.
05:55 - And here's Alison's example.
05:57 - Where each word gets
essentially two numbers.
06:01 - And those numbers are data
properties of that word.
06:04 - Like a cuteness score from zero to 100,
06:06 - and a size from zero to 100.
06:08 - So you could say kitten is 95, 15.
06:11 - A hamster is 80 comma eight, right?
06:13 - There are these numbers that sort of like,
06:15 - the label is tied to a
set of data properties.
06:18 - So if that's the case, then we can look,
06:21 - we could graph all of those and
we could say something like,
06:23 - oh, you know, like a horse and a dolphin
06:26 - are kind of like similar in
terms of like size and cuteness.
06:31 - And then we could start to do things by,
06:33 - but actually like, we could
do a mathematical analysis.
06:35 - Like what is the actual
Euclidean distance.
06:37 - Euclidean distance means the number of,
06:40 - well in this case pixels or units,
06:42 - between these two words right here.
06:45 - These are very similar because they're
06:46 - physically close to each other.
06:48 - And we can also do things,
you can think of those as,
06:51 - and this is a nice
demonstration of this idea.
06:53 - This is why we talk about
it as vectors, right?
06:56 - I have a whole set of
tutorials about vectors,
06:59 - describing as, describing points in space.
07:03 - So, for example, a
vector, a velocity vector,
07:07 - if I have a particle in a particle system,
07:09 - and I want it to go from here to here,
07:12 - this is its velocity.
07:14 - Its change in location.
07:17 - In essence, this is
basically what I'm doing
07:20 - with an operation like this.
07:21 - For example what if I said,
07:23 - okay, well apple is over here.
07:26 - And then I'm going to add purple to it.
07:31 - I'm going to move by purple's numbers,
07:32 - and over here I now find plum.
07:35 - So when we look at this in two dimensions,
07:38 - it kind of makes, we can sort of like,
07:40 - our brains can understand that.
07:42 - Two dimensions is like
the easiest dimension.
07:44 - I mean I actually find two dimensions
07:45 - to be easier than one dimension.
07:47 - One dimension is weird sometimes.
07:48 - So, what Allison is showing here
07:50 - is by moving from let's say,
07:52 - one word to another word
physically in space,
07:55 - we can establish this idea
of word relationships.
07:58 - Chicken is to kitten as
tarantula is to hamster.
08:02 - Now this is all very arbitrary,
08:04 - with like hard-coded word vectors.
08:07 - So, but this is just for
demonstration purposes
08:10 - in two dimensions so that our brains
08:12 - can kind of process it.
08:13 - Ultimately, if we have a lot
more information, somehow,
08:18 - about all of these words in
higher dimensional space,
08:22 - in vectors that have 100
dimensions, 100 numbers,
08:26 - we can't visualize that so easily.
08:28 - There are interesting techniques
08:29 - called dimension reductionality.
08:33 - Reducing the dimensionality
that we could then draw,
08:36 - like word clusters and stuff.
08:37 - And maybe I'll get to that later.
08:40 - But what I'm trying to say here
08:42 - is that we can establish sophisticated
08:46 - complex relationships between words
08:49 - in higher dimensional space.
08:51 - But in order to do that it's useful
08:53 - to look at a single
example that ties words
08:57 - to numbers in a low dimensional space,
09:00 - that we can either
visualize or if we like,
09:02 - put into our brains.
09:04 - And so, I've kind of described to you
09:06 - what word2vec is.
09:08 - What the model looks
like when it's complete.
09:10 - I haven't looked at all about
the training process, right?
09:13 - The animals example is hard-coded.
09:15 - I'm going to show you.
09:16 - I'm going to do a port of
one of Allison's examples
09:19 - of words associated with colors associated
09:21 - with numbers, right?
09:23 - A word red is 255,0,0
that's a word to a vector.
09:27 - And that's going to be from a dataset.
09:30 - And then the third thing
that I'm going to do
09:32 - is look at what is traditionally thought
09:34 - of as word2vec.
09:36 - These higher dimensional
large dictionaries
09:40 - of words and their associated vectors.
09:43 - Those word embeddings.
09:44 - So that's going to be the journey here.
09:46 - I don't know how many
videos it's going to be.
09:48 - Three, four, five, 471.
09:50 - Something like that.
09:51 - And then at some point I'll try to also,
09:53 - do some projects with that.
09:54 - So in the next video
I'm going to do a port
09:56 - of Allison's project which
you can find all in Python,
09:59 - all of the code in Python on that tutorial
10:01 - that's linked in the description.
10:02 - And I'm going to do a
JavaScript port of it.
10:05 - Okay, so I'll see you there.
10:05 - Maybe, maybe not.
10:07 - Go read that page, its excellent.
10:09 - Okay, goodbye.
10:10 - (upbeat music)