00:01 - [Music]
00:05 - hello good try good good day intranets
00:10 - it's Friday which means it's time for
00:13 - the coating train Here I am again my
00:19 - name is Dan today is Friday June 2nd the
00:26 - year is 2017 and I'm back for another
00:32 - weekly episode live stream of this
00:35 - internet youtube thing that i do which
00:37 - is something to do with coding hi here
00:40 - that's what I was told as I was
00:41 - wandering down the hallway and somebody
00:43 - said what happens in this room where I
00:45 - am okay so uh I've got a bunch of things
00:50 - to talk about I see that the chat is
00:54 - going strong and lots of people saying
00:57 - hi and choo-choo and all sorts of other
01:00 - train related greetings in a variety of
01:02 - languages which is super nice okay I've
01:04 - got some stuff to talk with everybody
01:06 - about today so I don't know if any of
01:10 - you were watching last week if you
01:12 - weren't that's probably for the best
01:16 - but I got a little Mia culpa here which
01:19 - is that I attempted to you know I've got
01:23 - this summer project my summer project I
01:26 - don't know how I ended up doing this but
01:28 - this is my summer project is to teach
01:32 - and create a set of video tutorials
01:34 - about artificial intelligence and
01:37 - machine learning and I'm kind of on the
01:39 - tracks I'd like the train gets derailed
01:41 - quite often but I've been making a
01:43 - variety of tutorials and things and I'm
01:45 - right now I'm in the weeds of building
01:47 - up and trying to set a foundation for
01:50 - neural network based machine learning
01:52 - and who I hear people talking they're
01:56 - talking about me and so one of the
02:00 - things that if I suppose I'm just going
02:02 - to let's let's have some resources here
02:05 - to talk about I'm going to go to this
02:08 - YouTube channel thing and you can see
02:12 - that most recently right
02:14 - now I'm in session three so I'm trying
02:19 - to make a set of videos for a course and
02:21 - I'm not really kind of doing them
02:23 - haphazardly over time in an arbitrary
02:25 - schedule but I'm looking at the chat now
02:31 - and I'm on session three so let me let
02:33 - me get myself centered here if I go to
02:36 - the syllabus session one was all about
02:40 - algorithms and search and graph systems
02:43 - and I have a variety of videos that I
02:45 - made about binary trees and
02:46 - breadth-first search and a star search
02:48 - in traffic salesperson section two was
02:51 - all about genetic algorithms and I did a
02:54 - variety of videos about kind of walking
02:57 - through what a genetic algorithm is and
02:58 - a few different scenarios and
03:00 - applications and so I'm instill in I'm
03:04 - I'm working this week three session
03:07 - three is designed to set a foundation
03:09 - for session four so the idea session
03:12 - three is to talk about machine learning
03:17 - what was she learning is what are
03:20 - typical tasks performed by machine
03:23 - learning namely classification looking
03:27 - at data and trying to label that data
03:29 - it's hot it's cold it's a name a picture
03:32 - of a cat it's a picture of a dog there
03:34 - are a variety of ways you can think
03:36 - about labelled data as well as
03:38 - regression which is making a prediction
03:41 - based on data that has a continuous
03:43 - output so a regression might be let me
03:48 - try to predict the weather based on
03:50 - ice-cream sales or the other way around
03:53 - who knows or you know the classic
03:56 - scenario is let me try to guess they
03:58 - have a machine learning algorithm
03:59 - predict or guess the appropriate price
04:01 - for a house based on the properties of
04:03 - that house so this is where I've been
04:06 - okay last week or two weeks ago or at
04:10 - some point I looked at something called
04:13 - linear regression with gradient descent
04:15 - and the idea here
04:18 - oops let me turn this camera on
04:24 - the idea here is to is to look at a very
04:28 - simple scenario okay
04:30 - so ultimately ultimately eventually the
04:35 - idea of deep learning the reason why
04:40 - machine learning based systems can be
04:43 - powerful and interesting and
04:46 - experimental and playful or useful all
04:49 - of these types of things is because we
04:52 - might have a some data that has many
04:55 - many many many many many many many many
04:57 - many many many many many many many many
04:59 - many many many many many I don't know if
05:02 - you could see that yeah you can all my
05:03 - little lines per it many dimensions so
05:09 - if you think about a house you know
05:13 - there's the number of bedrooms and
05:14 - there's the number there's the square
05:16 - footage and there's the zip code and
05:18 - there's maybe the the average
05:21 - temperature there's it something to do
05:23 - this is there's all sorts of nukid you
05:25 - can come up with a very very very very
05:26 - very long list let's try to think of and
05:30 - so something like trying to learn
05:34 - something about an image or a piece of
05:37 - text these are also high dimensional
05:41 - examples of data with many dimensions
05:44 - like an image has like all these
05:45 - millions of pixels in it so have we make
05:47 - sense of so so much data but in order to
05:51 - get a file in order to to understand how
05:53 - the systems work that can approximate a
05:56 - function that will perform a
05:57 - classification or regression with a lot
05:59 - of high dimensional data coming in it's
06:02 - often easiest to start with well I'm
06:06 - just off in the weeds here it's often
06:08 - easiest to start with it and try to
06:09 - understand well what if there was just
06:11 - one data input so the only thing is you
06:15 - know temperature and from temperature
06:19 - you can think of that as X we're trying
06:22 - to predict sales Y and so we might have
06:27 - an own data set which we could plot on
06:32 - this graph of
06:34 - temperature as related to sales and then
06:39 - we might be able to perform what's known
06:41 - as a linear regression which is trying
06:44 - to fit a line of fit a line to this data
06:52 - that approximates it so that if for any
06:56 - given temperature I can make a
06:58 - prediction as to what I think the sales
07:01 - would be okay so this is what I've done
07:06 - so far this is the case that I'm making
07:08 - I want to be able to build and work with
07:11 - systems with high dimensional data so
07:13 - it's not just the temperature I'm using
07:16 - to predict sales it's so much more it's
07:19 - all about the temperature and the
07:21 - humidity and the population of a city
07:23 - and the you know the list goes on and on
07:26 - and on and on that's what I'm trying to
07:29 - do so okay coming back over to here
07:31 - let's see no matter what I talk about
07:34 - anytime I glint whenever I glance at the
07:36 - chat almost all the time the discussion
07:38 - is I think Python is better than Java
07:41 - actually really should program in C++
07:42 - but Lisp Lisp is the answer to
07:45 - everything but I like a program with
07:47 - JavaScript because it's the best
07:48 - language ever that's always what the
07:50 - discussion is I was able to stop that
07:54 - from being the discussion when by
07:56 - accident I started talking about
07:57 - temperature in Fahrenheit I mentioned I
07:59 - really should use Celsius and then all
08:00 - of a sudden the chat Fahrenheit versus
08:03 - Kelvin versus Celsius nobody was talking
08:05 - about which programming language is
08:06 - better than the other one was the
08:07 - glorious day okay I've only made it
08:12 - worse okay so now so you can perform a
08:20 - linear regression using a statistical
08:24 - approach so with simple data like this
08:27 - and I can even see that it's very small
08:30 - I'm gonna have to write bigger make
08:31 - larger drawings you can perform a linear
08:35 - regression with simple data like this
08:39 - just using a statistical approach so
08:41 - there's a formula to actually figure out
08:43 - where that line goes
08:44 - nicely but if we're looking to make this
08:48 - prediction in this impossible to
08:49 - visualize our parser understand high
08:52 - dimensional space with a lot more than
08:54 - just a single piece of data we can't
08:56 - actually just solve it exactly we don't
08:59 - have the computational power to do that
09:02 - but we could take the approach if let's
09:05 - make a guess let me guess where the line
09:11 - goes and then I can see like well how
09:17 - poorly did that line do let me do some
09:19 - type of evaluation of the error based on
09:22 - the existing data and that line and
09:24 - maybe I'll just like kind of push the
09:26 - line in the direction where I try to
09:28 - like minimize that error and I'll do
09:30 - that over and over and over and over
09:31 - again and this is known as gradient
09:33 - descent okay so this is why are we doing
09:38 - this I want to make sense of high
09:41 - dimensional data that I cannot make
09:42 - sense of easily in order to do that I
09:47 - want to build up some skills to make
09:49 - sense of low dimensional one dimensional
09:51 - data I can do that with distal approach
09:55 - when I get to the high dimensional data
09:57 - I'm going to need to just do a sort of
09:59 - knob twisting trial and error more like
10:01 - approach so why not do the low
10:04 - dimensional data with that and that is
10:08 - the most recent video that I made linear
10:11 - regression with gradient descent okay so
10:16 - I then I did this example without
10:22 - digging into the actual derivation of
10:26 - the formulas for how to add rank those
10:30 - adjustments so you know there's a the
10:35 - formula for a line is y equals MX plus B
10:41 - M being the slope B being the
10:46 - y-intercept and so these are the values
10:48 - that I'm tweaking to try to get the line
10:51 - in the right place so when I did the
10:55 - video on a gradient descent I
10:58 - just used the formulas to do those
11:01 - tweaks and then I said I'm going to make
11:03 - some follow-up videos to derive those
11:05 - formulas and I logged in to my YouTube
11:08 - account Here I am NOT so I but I think
11:10 - if I go under playlists and under
11:13 - session 3 so I have now unlisted these
11:17 - because the derivations of those
11:19 - formulas require a few rules from
11:23 - calculus they require the power rule the
11:26 - chain rule and partial derivatives and I
11:29 - made three videos on those topics those
11:36 - videos weren't very good they're not
11:37 - very good what them um yes some people
11:42 - like them I use 'fl for me in that it
11:46 - forced me to kind of revisit a topic
11:51 - that I had once studied but haven't
11:53 - spent a lot of time working with in my
11:55 - day to day coding life but I did
11:59 - discover something in the feedback and
12:00 - comments that I got which was that I
12:02 - think while watching my channel really
12:05 - enjoy the winging it approach when it
12:08 - comes to coding the making mistakes are
12:10 - trying to debug that sort of part of
12:12 - something that everybody can relate to
12:14 - perhaps people learn the most when they
12:17 - see me trying to figure out an error and
12:18 - finally fix it even if I can't fix it
12:20 - all but getting a suggestion from the
12:21 - chat but I think that it wasn't as use
12:25 - I'm learning that maybe it's not as
12:27 - useful for me to try to walk through a
12:30 - topic from mathematics while winging it
12:33 - and not really knowing what I'm doing or
12:35 - where I'm going so there's not as much
12:37 - of a sort of like live debugging aspect
12:39 - to trying to explain the chain rule in
12:42 - calculus so I think these videos didn't
12:45 - turn out very well now thankfully with
12:48 - some of the feedback I was going to read
12:49 - some of the comments but I guess I don't
12:51 - need to that there is a YouTube channel
12:52 - called I think three blue one brown and
12:57 - three blue one Brown is a YouTube
12:58 - channel that has lots of
13:02 - mathematics and other related tutorials
13:04 - in particular there is a void this
13:10 - essence the Lydian or algebra one I
13:11 - definitely need that one to which I'm
13:13 - going to get to there's this essence of
13:15 - calculus so uh this what I'm going to do
13:21 - now I think is my approach is now if I
13:25 - go back to my playlist so uh right now
13:29 - these are still in the playlist or
13:31 - unlisted I might remove them from the
13:32 - playlist I'm not sure but I think what
13:35 - I'm going to do now I'm still going to
13:36 - have a three point five where I go
13:38 - through and drive the formula but um
13:43 - because I think that's useful because
13:45 - it's a specific formula I'm using in my
13:47 - code and it kind of will relate and tie
13:49 - together I'm gonna use it again and
13:50 - again in future neural network tutorials
13:52 - but I think of what I might do instead
13:54 - is instead of referring people for
13:57 - background calculus background that I'm
13:59 - using to my videos I'll refer people to
14:02 - the three blue one Brown videos which
14:05 - are better and more comprehensive so
14:08 - that's my current plan so for today I
14:11 - really would love if I can today to get
14:14 - to session four and I think the thing to
14:17 - think that I get requested the most is
14:19 - really like do a whole multi-layered
14:20 - feed-forward fancy super-powered all
14:24 - together magical neural network thing in
14:26 - you know whatever your favorite
14:28 - languages of choice I definitely will
14:30 - not be getting to that today but I would
14:31 - like to build a perceptron today a
14:34 - perceptron being a model of the simplest
14:37 - neural network possible a network that
14:39 - really is in a network at all but a
14:41 - network of one neuron the neuron
14:43 - receives inputs the neuron fires an
14:46 - output and what what how does that
14:48 - process work and how is that the
14:49 - building block for larger multi-layered
14:52 - more complex neural network system so
14:55 - that I would really like to do today
14:56 - that's my goal before I get there I
15:01 - think I'm going to attempt to do one
15:06 - follow-up to the gradient descent video
15:08 - where I derive the formula for how I
15:12 - change the weights
15:15 - do you hear that I feel like there's
15:17 - always these like and then and head and
15:23 - then up and then go into the perceptron
15:28 - how's that sound to you guys I'm looking
15:32 - I'm looking at the slot okay so let's
15:36 - see first well let me just see if
15:37 - anybody's watching me today 500 people
15:41 - are watching well so that's kind of
15:44 - amazing and great I appreciate that
15:46 - i I've been feeling kind of a little
15:48 - blue down the dumps a little bit over
15:50 - the last week cuz I really feel like I
15:51 - botched it up and I kind of lost a
15:53 - little bit of confidence here you know
15:58 - this is all a learning experience for me
16:01 - and uh you know I think one of the
16:06 - things that I try to do is not be afraid
16:09 - to just try to explain something or try
16:12 - something live in a youtube channel and
16:14 - it doesn't always go so well and it can
16:17 - be difficult to read the criticism but I
16:20 - get to say it was really wonderful to
16:22 - read the comments from the last week
16:25 - it's on these calculus videos because
16:26 - they were written in such a thoughtful
16:28 - and constructive way and they were
16:31 - negative but they were helpful and they
16:33 - were kind and so that I really really
16:37 - appreciated the Kota Gabe's so I'm
16:40 - getting the chat Dakota game to switch
16:42 - gears for a little bit and I did um I
16:46 - was thinking all let me take a break
16:48 - from the machine learning stuff today
16:49 - and do something different but I feel
16:51 - like I gotta just keep pushing through
16:54 - it so I think I'm gonna push through it
16:57 - I wanted to do two live streams this
16:59 - week and I fail to make that happen I
17:03 - also wanted to do uh ADA from the slack
17:07 - group had an excellent suggestion to do
17:09 - something for Pride Month and with
17:11 - rainbow colors since you know I have an
17:13 - affinity for rainbow based imagery as
17:16 - you might have noticed so that could be
17:20 - sort of a fun generative graphics things
17:23 - to do
17:24 - today but I think I'm going to stick
17:25 - with my machine learning stuff because
17:28 - now and and and Julian right don't
17:30 - listen to the haters on YouTube so I get
17:32 - a lot of you know you can't be on a
17:34 - YouTube channel without getting a lot of
17:35 - like comments about how terrible you are
17:38 - with making your videos and those uh you
17:40 - know I read them sometimes it so it's a
17:42 - little odd and to read them sometimes
17:43 - they just kind of go right over uh you
17:46 - know I'm pretty lucky to not be harassed
17:48 - and I think that that is definitely an
17:49 - issue that other youtubers have to deal
17:54 - with and it's a big problem with the
17:56 - online community so I want to get to go
17:58 - down that road right now but I you know
17:59 - I think I'm pretty privileged here in
18:01 - terms of the the the kind of audience
18:08 - reaction that I get but I forgot what I
18:12 - was saying but ya know either it wasn't
18:14 - a volume up please if you look at let me
18:16 - know if there's an issue with my volume
18:22 - okay I look at the chat and completely
18:24 - lose my train of thought it's kind of I
18:26 - well this is one thing I've never been
18:27 - able to figure out how to do effectively
18:29 - which is kind of like read a chat and
18:30 - talk and like have a program in a
18:32 - whiteboard all that sort of stuff but um
18:35 - I want to be clear I didn't feel like
18:37 - people were unreasonably commenting
18:40 - negatively about my videos they were
18:42 - explaining what that made sense to them
18:43 - what didn't make sense that they had
18:44 - knowledge about it they were pointing
18:46 - out what could be more helpful less
18:48 - helpful oh that was great
18:49 - okay uh yeah
18:53 - Zerg rush Joe in the slack channel is a
18:56 - great point which is and I always say
18:58 - this to students which is that you know
19:00 - I can be stuck on a problem for so long
19:03 - and I could just go and do something
19:05 - else for a while and somehow this like
19:07 - subconscious thinking about it without
19:09 - the pressure of having to solve it when
19:11 - you come back to it it's much easier to
19:12 - figure out and do and keep your brain
19:15 - thinking about the subject even when you
19:17 - do something else and it helps you get
19:18 - through a wall that's excellent
19:19 - excellent point okay now so what I'm
19:26 - going to do fir okay couple things so I
19:30 - keep mentioning the slack channel I will
19:32 - just do my quick plug for various things
19:35 - if you're interested in supporting the
19:36 - work that
19:37 - doing even if you don't like even if I
19:39 - think supporting me make trying to make
19:41 - videos that are bad and failing at them
19:42 - sometimes you can go to
19:45 - patreon.com/scishow to
19:48 - rewards and things that you can get a
19:50 - slack channel that you can join for by
19:52 - signing up and supporting this
19:54 - crowdfunding platform you can also go to
19:57 - coding train Storenvy comm if you would
20:00 - like to get some coding trade
20:02 - merchandise and i also always like to
20:05 - mention a nonprofit organization that I
20:09 - help to administer called the processing
20:11 - foundation which maintains the open
20:14 - source projects processing p5.js and
20:17 - processing Python and a lot of other
20:20 - community and education based
20:22 - initiatives and if you're interested
20:24 - please join the processing membership
20:27 - program and if you're a you if you're in
20:28 - the United States those donations are
20:30 - tax-deductible okay so those are my
20:33 - three quick plugs let me close this what
20:38 - I really want to do I kind of I'm not
20:41 - going to actually do this but I have
20:42 - this very strong desire oops
20:46 - - let me skip this I'm like by accident
20:49 - playing an ad here - just play three
20:52 - blue brown videos and like watch them
20:54 - like Mystery Science Theater and like
20:56 - kind of like comment on them as they're
20:58 - going because they're really really good
20:59 - ok alright so now ok but let's see so
21:12 - I'm just now I'm just taking a moment to
21:14 - breathe
21:17 - [Music]
21:24 - music has driven me mad yet visible man
21:29 - okay alright so let's see where am i
21:37 - timewise twenty-one minutes so okay so I
22:04 - think what I'm going to do is I'm going
22:08 - to take away the pressure of right now
22:12 - this is going to be a recorded edited
22:15 - video tutorial and I'm going to talk
22:18 - through my understanding or going to a
22:20 - practice I'm going to talk through my
22:23 - understanding of the math behind
22:26 - gradient descent and then I'm going to
22:29 - listen to your feedback and look at
22:31 - reading the chat see what makes sense so
22:34 - what notation is Right wrong and then I
22:36 - will try to do it again as a video
22:39 - tutorial and if for some reason whatever
22:41 - I do right now is which it never will
22:45 - which it won't be then maybe I won't
22:47 - even bother to do it again but um let's
22:49 - just see how that goes okay so I'm
22:57 - having a little bit of a runny nose
22:59 - today should I need to mute myself
23:00 - hopefully this won't blow out your ears
23:03 - okay
23:07 - okay feel like I want some kind of
23:10 - ridiculous whiteboard market puttan
23:13 - marker pocket protector thing okay so
23:21 - I'm starting to think about just trying
23:23 - to program Frogger by the way which is a
23:26 - good idea okay
23:28 - so let's let let's think about this okay
23:38 - so to recap I have a bunch of data
23:46 - points in 2d space I have a line in that
23:51 - 2d space the formula for that line is y
23:56 - equals MX plus B now when I try to make
24:05 - a prediction right I get a piece an
24:09 - input of data input X and from there
24:14 - I try to make a guess and I in addition
24:20 - to the guess I have the known Y so this
24:24 - is the correct data that goes with X my
24:29 - machine learning system makes a guess
24:32 - the error is the difference between
24:36 - those two things
24:38 - the error is y the correct answer - the
24:45 - guests but what happens when we look at
24:50 - this error right we don't care so much
24:52 - right okay so so that's the error so
24:55 - there's an error okay so this relates to
24:59 - the idea of a cost function loss
25:04 - function so if we want to evaluate how
25:07 - is our machine learning algorithm
25:09 - performing we have this large data set
25:13 - maybe it has n elements so what we want
25:17 - to do is from 1 to n for all n elements
25:23 - we want to minimize that error so the
25:29 - cost function cost equals the sum
25:33 - of Y sub I every known answer - the
25:40 - guests sub I squared is that something
25:46 - you can actually see yes it is so this
25:49 - is the formula this is a cost known as a
25:52 - cost function this is the total error
25:56 - for the particular in the particular
26:00 - model being the current M and B values
26:03 - that describe this particular line this
26:06 - is the error now this error hold on I'm
26:14 - thinking here for a second
26:17 - that's rad see people like I like
26:19 - glanced over at the chat and I see
26:21 - Tetris like please none but you know I
26:25 - was
26:26 - everybody wants machine learning man
26:28 - it's like the thing I gotta do it I got
26:32 - it to some machine you gotta teach
26:37 - machine learning it's all I was hearing
26:39 - and so I said I'll try that I could
26:42 - learn that I can understand it I could
26:43 - teach that it's hard I'm having a hard
26:46 - time with it but I'm gonna keep going
26:49 - ok so perhaps we can agree we can agree
26:55 - that our goal is to minimize this cost
26:59 - also known as maybe a loss we want to
27:04 - minimize that loss we want to have the
27:06 - lowest error we want the M and B values
27:09 - for the lowest error so we want to
27:12 - minimize this function now what does it
27:15 - mean to minimize a function I'm trying
27:20 - to find the space on the whiteboard
27:21 - where I want to do this next piece
27:29 - okay I just like to go glance at the
27:31 - chat okay okay so now let me think this
27:37 - might actually make it to be a video
27:39 - just different pieces will get edited in
27:41 - different places okay
27:43 - I'm tempted to okay so I'm gonna yep so
27:46 - I'm just got I'm gonna draw Singh here
27:48 - then I'm going to race it so let's say I
27:52 - have a function like y equals x squared
27:55 - see what's confusing about this what's
27:57 - so confusing I've noticed in reading and
28:00 - doing a lot of reading and reading blog
28:02 - posts or watching other video tutorials
28:03 - and this is something that I think I
28:05 - would be able to improve if I were
28:06 - writing this in a book because it would
28:08 - give you a lot of time to kind of
28:09 - rethink rethink re-edit but figuring out
28:12 - an appropriate and clear notation is
28:14 - very difficult because I've got like you
28:19 - know this is a function it's like lime
28:21 - and now I'm going to talk about another
28:22 - function because this is a function
28:23 - there's a y here y equals x squared so
28:25 - actually I think what I want to do I'm
28:33 - gonna erase that for a second or say
28:34 - okay so this function is something
28:39 - equals something squared which is not
28:42 - that different from like me saying just
28:45 - for a moment like y equals x squared so
28:48 - if I were to take a Cartesian coordinate
28:51 - system and graph y equals x squared it
28:56 - would look something like this I'm
28:59 - drawing in purple now because I've
29:01 - stepped away from this notation and
29:04 - syntax for this particular scenario and
29:07 - I'm just talking about a function in
29:08 - general y equals x squared you're going
29:11 - to also write this like f of X equals x
29:14 - squared
29:15 - but I'm graphing y equals x squared so
29:18 - what does it mean to find to minimize
29:22 - this function right I said I want to
29:24 - minimize the loss I want the smallest
29:26 - error
29:27 - I want the whatever line has the
29:29 - smallest error well what it means to
29:32 - minimize a function is that actually
29:34 - find the x value that produces the
29:38 - lowest Y this
29:41 - like the easiest thing in the world that
29:42 - we could ever possibly do right now you
29:45 - don't need any calculus fancy math or
29:47 - anything too fun to minimize this
29:49 - function there it is it's at the bottom
29:51 - it's the lowest point zero its I could I
29:55 - could see it it's quite obvious so this
29:59 - is the thing eventually we're going to
30:03 - in in the machine learning systems that
30:06 - I'm going to get further into neural
30:08 - network based systems with many
30:09 - dimensions of data you know there might
30:11 - be some much more hard to describe crazy
30:14 - function that we're trying to
30:16 - approximate that it's much harder I mean
30:18 - of course we could eyeball this as well
30:20 - but as much harder to sort of
30:21 - mathematically just compute exactly
30:24 - where the minimum is especially if you
30:25 - imagine this as instead of a single line
30:29 - but a bowl and then what happens we can
30:31 - get into three dimensions in four
30:32 - dimensions and five dimensions things
30:34 - get kind of wonky but there is if we
30:39 - know the formula for this function there
30:42 - is another way that you can find that
30:45 - minimum that minima minima minimum I
30:48 - don't know which it is and that is what
30:54 - I keep talking about gradient descent so
30:59 - let's think about what gradient descent
31:01 - means let's say the current error sorry
31:16 - let's say we're looking at this point
31:19 - here and I'm I'm gonna I'm gonna I'm oh
31:24 - I'm gonna walk along this function and
31:28 - I'm like I'm right here I'm like hello
31:29 - I'm looking for the minimum is it over
31:32 - there over there could you help me
31:34 - please can you please provide me can I
31:35 - use my like GPS Google Maps thing to
31:38 - find the minimum how would I find the
31:40 - minimum well if I'm right here I've got
31:43 - two options I could go this way
31:46 - or I could go this way and if I knew
31:49 - which direction I could go I could also
31:51 - say like I should take a big step or I
31:54 - should take a little step right there
31:56 - are all sorts of options so I need to
31:58 - know which way to go and how big of a
32:01 - step to take and there's a way to figure
32:03 - out how to do that and it's known as the
32:07 - derivative so the derivative is a term
32:11 - that comes from calculus and I would
32:14 - refer you to three blue one Brown's
32:17 - calculus series or you can get a bit
32:19 - more background on how what the meaning
32:22 - of derivative is and how it works and
32:24 - how you can sort of think about these
32:25 - concepts from calculus but for us right
32:28 - now what we can think of is it's just
32:30 - the slope of the graph at this
32:34 - particular point and a way to describe
32:37 - that is like a tangent line to that
32:40 - graph so if I'm able to compute this
32:49 - line then I could say well this
32:53 - direction if I go this direction it's
32:55 - going up and I'm going away from the
32:58 - minimum if I go this direction I'm going
33:00 - down and I'm going towards the minimum
33:02 - so I want to go down and you can see
33:05 - like over here the slope is less extreme
33:09 - if I'm right here so maybe I don't need
33:11 - to go very far anymore but if I'm
33:13 - further up that slope is going to point
33:16 - much more this way oh I should take a
33:17 - bigger step down so this idea of being
33:20 - able to compute this slope this
33:23 - derivative of this function tells me how
33:25 - to search and find the bottom
33:29 - pause for a second minima is plural
33:34 - uh yeah guess - why do it I would love
33:41 - to see four occur uh I'm sorry I'm not
33:47 - in the wrong screen now oops
33:49 - this camera went off alright so how am i
33:56 - doing so far I'm going to pause for a
33:58 - second you like me to me I'm not going
34:02 - to do Tetris today okay I'm not in the
34:04 - right I'm going to push through this I
34:11 - don't understand this because I'm in the
34:13 - 10th grade says thon okay
34:19 - yeah um okay um all right thumbs up okay
34:28 - okay so I'm going to keep going here
34:39 - okay so I'm gonna come back to here oh
34:43 - look at this
34:45 - this camera is pointing in a weird did I
34:48 - bump it or was it always doing that
34:50 - Green is there there we go all right
34:56 - remember how I said I was going to
34:58 - practice now I just am sort of doing
35:00 - this I might I'm going to should
35:05 - probably record an intro to it okay okay
35:11 - so now now that we have that established
35:14 - that this idea of finding the derivative
35:16 - or the slope the direction is a way of
35:20 - finding the minimum so if this is the
35:23 - function then what I I want to minimize
35:27 - this function if I could somehow find
35:30 - the derivative of this function I could
35:33 - find the okay okay time out
35:39 - we must establish a few more things to
35:42 - get a little further along here okay
35:47 - okay so I want to say a bit more about
35:51 - this function so about about this I'm
35:56 - pulling I'm thinking I'm thinking
35:56 - pausing I'm thinking hold on
35:58 - I'm always by the way completely
36:00 - standing outside of the frame so it
36:02 - should stand over here if I'm thinking
36:04 - I'm thinking about where I'm going next
36:06 - here so I need to talk about okay okay
36:16 - okay so if you think back to the
36:18 - previous video actually don't even think
36:20 - back let's go look in the code do I hold
36:25 - on I don't have this open so I code the
36:42 - point of doing this is because we're
36:43 - programming
36:54 - okay
37:00 - okay so this is the landscape of the
37:04 - puzzle we're trying to solve and pieces
37:06 - of that puzzle but what is the full
37:09 - what's the what's the actual part of the
37:10 - code that I'm trying to give you more
37:11 - background on the actual part of the
37:14 - code that I'm trying to give you more
37:16 - background on is right over here so this
37:20 - is the gradient descent algorithm that I
37:23 - programmed in the previous video where
37:25 - what I did is we looked at every data
37:28 - point we made a guess we got the error
37:31 - the difference between the known output
37:34 - and the guess and then we adjusted the M
37:36 - and B values M equals so the idea here
37:40 - is that we want to say every for as
37:43 - we're training the I don't know which
37:45 - color I'm using right now as we're
37:47 - training this system I want to say M
37:50 - equals M plus plus Delta n some change
37:56 - in M B equals B plus Delta B so I want
38:03 - to know what is a way that I could
38:06 - change the value of M in y equals MX
38:09 - plus B in order to make the error less I
38:13 - would have minimized the error how do I
38:18 - do that how should I change B in order
38:21 - to minimize the error so the answer
38:25 - which is in the code is this just equals
38:29 - a you know why - guess times X times the
38:37 - learning rate if we can forget about the
38:38 - learning rate right now right that's the
38:46 - answer that you can see in the code
38:50 - right here the Delta is at error times
38:54 - x-4 be the Delta B is just the error so
38:59 - how do I get that okay so I want to try
39:02 - to prove that this why this works now so
39:08 - I need to I want to rewrite the
39:12 - function here in a slightly different
39:14 - way this function which are calling the
39:17 - cost function let's call it J J is a
39:20 - function based on M and B so I want to
39:24 - get the error for Emma B I'm also going
39:26 - to remove the sum and I'm just going to
39:30 - say what's the error for any M and B
39:33 - equals and I'm going to call this I'm
39:39 - going to say this is where I this is
39:40 - where I'm stuck I might need some help
39:42 - with notation I kind of want to call
39:46 - this like error error of x squared
39:58 - hold on I'm stopping to think maybe I
40:07 - should give this a mathematical notation
40:09 - I give that H of X square okay let me
40:27 - start over here
40:33 - okay to do this I'm gonna attempt I'm
40:36 - going to unfortunately start to use some
40:38 - more mathematical looking notation and
40:40 - I'm gonna try to describe it as I go
40:43 - and hopefully we won't get too lost so
40:47 - this cost function I'm going to call j j
40:52 - and it's really a function of M and B
40:54 - right I want to minimize the error for
40:58 - certain M and B values that equals I'm
41:02 - going to get rid of the sum to simplify
41:04 - I could keep the sum in there going to
41:05 - get rid of the sum to simplify that
41:08 - equals and I'm gonna say I'm going to
41:13 - I'm going to take what's written here
41:16 - and call that H a function H of X right
41:23 - what this actually is is y - is this
41:28 - something you can see y minus MX plus B
41:33 - right that's the actual and should I
41:38 - should I have reverse that should it be
41:39 - guess - why I think I saw that it should
41:44 - be guess - why it doesn't really matter
41:47 - because I'm squaring it but maybe I
41:51 - should fix that and this red marker is
41:53 - hard to see so let me go back to purple
42:03 - you
42:11 - this is like continuity all right let's
42:18 - see III guess I'm going back on my like
42:21 - I'm going to do this twice I'm actually
42:22 - just going to do this once but really
42:24 - slowly and back up here and there and
42:25 - match is going to have to do some
42:28 - magical editing I feel like why am I
42:29 - always out of the camera shot - I need
42:31 - to turn this more this way yeah like
42:35 - standing in the wrong place okay that's
42:38 - better okay so let me get the purple pen
42:46 - okay you know I realize I kind of messed
42:50 - up its in some ways it doesn't matter
42:52 - because we're squaring this but I think
42:55 - textbook wise I should really be
42:57 - thinking of the error as the guess - the
43:03 - noob so let me just switch that for a
43:04 - second okay
43:05 - so notation wise I've now called the
43:08 - cost function J and I want to call I
43:12 - want to take so I have this in my trying
43:16 - to do multiple colors here I want to
43:18 - take this right here guess - why and I
43:24 - want to call them to call that H that's
43:27 - a function of X actually it seems weird
43:30 - yeah no no no sorry it's not a function
43:38 - of X what is it a function of I just it
43:42 - doesn't really matter
43:44 - X is wrong I'm going to use the chain
43:51 - rule I know where I'm going with this
43:53 - but I'm trying to think of a notation
44:02 - hold on and now I'm going to have to
44:04 - work this out for a second because what
44:05 - I want to do is if this is equal to H of
44:10 - X then D of J according to M that this
44:17 - will be is going to be the derivative of
44:20 - H of X I'm I lost here H of X times the
44:32 - derivative of H of X right because then
44:36 - I'm going to get Oh - but - right no no
44:41 - I'm sorry wait of course using a chain
44:44 - rule but is the X misleading in here
44:47 - using the chain rule I'm going to if
44:49 - this is H of x squared then I must say
44:53 - two of that same two times that same
44:56 - function times its derivative you know
44:59 - relative you know d h d h relative to m
45:04 - but does the X make sense there or is
45:09 - that confusing I'm going to look in the
45:13 - check because then this is going to give
45:14 - me this is where this is where I'm going
45:15 - then this is going to get it - I can get
45:17 - rid of because the learning rate is
45:18 - going to cancel it out I could have just
45:19 - added a 1/2 but so then this is going to
45:22 - be why this function which is what which
45:25 - is a guess - why guess - y times the
45:33 - derivative of guess - y relative to M
45:37 - which is M X plus B minus y and all X&Y
45:46 - it's--we're cos x and y are constants
45:47 - here I'm doing the derivative relative
45:49 - to M so what I end up getting here is
45:51 - just X so that's why I have the error
45:54 - times the input X that's where I'm going
45:58 - my question is what is the proper what's
46:01 - a good notation for me to use for the
46:04 - function that I'm going to have B this
46:09 - like error function
46:10 - let me go look at the chat now and see
46:13 - if anybody has any suggestions
46:17 - that's what's usually used J as a
46:20 - function of M and B it's the guest that
46:22 - you're differentiating yeah I know it's
46:29 - a function of Y all right it's a
46:32 - function of Y in x2 function so if I I
46:43 - mean usually the notation I see is just
46:48 - right it's a function of Y I mean it's
46:55 - just a function maybe I don't you know
46:58 - what maybe I shouldn't use the maybe it
47:04 - actually what I should do is not even
47:10 - like you know I could go to any let's
47:14 - look at like if I go to let me let me
47:16 - just google something like gradient
47:18 - descent derived am i doing timewise
47:22 - three o'clock like here okay so you can
47:28 - see here the H like these are the
47:34 - weights of X yeah the those are the
47:42 - weights so this is the the theta here
47:45 - the weight is equivalent of my like m
47:46 - and B yeah the x and y's are constants
47:49 - so it's not really
47:55 - [Music]
48:01 - I think I should just call that like you
48:03 - can see here it's really also just a
48:06 - function of this is funny this is
48:08 - exactly what I'm doing this was a good
48:10 - this is a good one to pull up see that
48:13 - this I'm trying to stay away from this
48:15 - notation that gets really really
48:16 - difficult to read see what I mean I've
48:19 - already lost my y guess I know is a
48:25 - function of X let me let me come back to
48:32 - the whiteboard here which might help me
48:34 - here
48:43 - I think what might actually be most
48:45 - useful is if I just call this H if I
48:55 - just call that H or just call that error
49:01 - I'm gonna just call that error that's an
49:04 - individual error that's what I'm gonna
49:07 - call that okay let me try this okay I
49:30 - forgot where I was but try this again
49:54 - all right you guys III I almost want to
49:58 - go look and see I'm sure I've lost at
50:00 - least like 100 to 200 viewers let's do
50:02 - buddy I will get back to making things
50:04 - like Tetris and Frogger and that type of
50:06 - thing eventually okay well I don't know
50:08 - why I just erased that okay so when we
50:13 - come back to this point here okay so in
50:28 - order to figure out what these Delta
50:31 - values are I didn't want to rewrite this
50:33 - function again for the millionth time I
50:38 - think I want to use some mmm I'm gonna
50:42 - rewrite this with some from different
50:44 - notation to try to help keep things
50:46 - going here so this cost function I'm
50:48 - going to call J J is a function of M and
50:52 - B so this loss right there are some
50:56 - values there's a value of M and B and
51:00 - when I have a particular value of m and
51:02 - B I can look at all of the errors across
51:05 - all the points sum them up and I want to
51:07 - find the mm to be with the lowest value
51:09 - now
51:11 - another way of writing this up here is I
51:16 - could just take this guess this guess
51:18 - minus y and I could call that error so
51:27 - I'm going to say that this is really a
51:32 - function J is a function of the error
51:36 - squared I want to minimize I want
51:38 - minimize the error squared I want the
51:40 - least squared error so here's the thing
51:46 - what again what value of M how can I
51:50 - change m to minimize what this is I can
51:55 - figure out which way to go along the I
51:58 - could
52:03 - I'm just sorry I'm taking a guess it'll
52:06 - be fun I get to back-propagation okay I
52:11 - think I've got this down but I realized
52:15 - I'm kind of like have a continuity
52:17 - problem so I've got to come back and I
52:23 - got to go back to where there was a time
52:27 - in my life where I made these videos
52:28 - without doing them live okay
52:33 - okay here we go here we go everybody
52:38 - this is take four hundred and eight
52:42 - thousand and twenty million the next
52:51 - step that I want to do is find the
52:54 - minimum cost I want to minimize this
52:57 - function for a particular I want to find
52:59 - the M and B values for the with the
53:01 - lowest error so to do that we've
53:04 - established that gradient descent says
53:06 - if I could find the derivative of a
53:07 - function I know which way to move to
53:09 - minimize it so somehow I need to find
53:10 - the derivative of this function to know
53:13 - which way to move okay so in order to do
53:15 - that though I'm gonna have to rewrite
53:16 - this function in a different way so a
53:18 - couple things one is I think I made a
53:20 - mistake earlier where this should
53:23 - actually be done it's sort of doesn't
53:25 - matter but this should be guess - why we
53:30 - were squaring it so in a way the
53:32 - positive negative doesn't matter but I
53:33 - think this is important for later so
53:35 - this should be guess - why that's
53:36 - technically the error is a guess - why
53:40 - not why - guess okay so I'm going to
53:43 - call the error function J and J is a
53:46 - function of M and B so I get something
53:48 - I'm sorry not the error function because
53:50 - I'm about to call something else the
53:51 - error function the loss function the
53:52 - cost function J then I'm actually what
53:56 - I'm going to do is I'm going to say I'm
53:57 - going to just simplify this guess - why
53:59 - and I'm going to call that error I'm
54:04 - also going to take out the summation the
54:05 - summation is kind of important it but it
54:07 - this has to do with that stochastic
54:09 - versus batch graded to sent that I
54:11 - talked about in the previous video where
54:13 - I could I want to get the error over
54:14 - everything I just want to look at each
54:16 - error one
54:17 - time so let's simplify things and say
54:18 - we're looking at each error one at a
54:19 - time
54:20 - so I'm going to now say this equals
54:25 - error squared so I have essentially
54:31 - rewritten this function and simplified
54:33 - it the cost J is equal to this error the
54:37 - guess - y squared so what I want to do
54:41 - is I want to find the derivative of J
54:47 - relative to M I want to know how do I
54:52 - minimize J how how does J change when M
54:58 - changes dfj relative to M okay
55:02 - so in uh you know again I recommend that
55:06 - you go and check out some of the three
55:08 - blue one Brown calculus videos which
55:10 - will help give you more background here
55:12 - but what I'm actually going to need to
55:17 - do here is you use a use two rules from
55:21 - calculus I'm looking for another pen
55:22 - color for no reason I need to use the
55:25 - power rule that is one rule and I need
55:32 - to use the chain rule these are two
55:38 - rules um so there are two rules that I
55:40 - need to do the power rule for a
55:42 - derivative is actually just take the
55:44 - exponent to take the derivative of
55:47 - something squared I take the exponent I
55:50 - subtract one from it and then I multiply
55:52 - take the X ah let's just do it let's let
55:55 - me let me just put a background okay
56:00 - look I really tried to like somehow I
56:05 - thought like using multiple colored
56:07 - markers would solve all my problems
56:08 - clearly doesn't okay let me let me
56:12 - establish what the power rule is really
56:13 - quickly if I have a function like f of x
56:16 - equals x to the N the power rule says
56:19 - that the derivative is
56:21 - n times X to the N minus 1 so that's the
56:26 - power rule so I'm going to now apply
56:31 - that here and I'm going to say I don't
56:34 - know why I'm in purple now but I am two
56:37 - times error to the first power
56:42 - the other thing however is though I want
56:46 - to know how J changes not according to
56:49 - error but according to M and the error
56:54 - is a function of M error is actually a
56:56 - function of M and B as well so the chain
57:02 - rule says what I can do is now say I can
57:06 - take this error function I could get the
57:07 - derivative which is just 2 times error
57:09 - and then I can multiply it by the
57:13 - derivative of that error itself relative
57:19 - to n so the cost the derivative of this
57:25 - cost function that I'm trying to
57:27 - minimize relative to say M is I really
57:35 - want to swear I never swear how to
57:37 - gentleman I realized like I've got a
57:39 - lost I got you know I there's just so
57:41 - many pieces here that I'm like glossing
57:43 - over and skipping but I'm going to just
57:45 - keep going with this for a second
57:46 - double back I guess if I have to I just
57:50 - this is not an adequate the chain rule
57:52 - is like I'm kind of glossing over this
57:54 - detail so like is there really any point
57:56 - of me driving this if I haven't really
57:57 - explained the chain rule I guess I have
57:59 - that other video where I explain the
58:00 - chain rule but it's like the video
58:02 - really really struggling with what to do
58:04 - here okay
58:15 - but I'm going to keep going two times
58:19 - error tines okay let me maybe I'm going
58:23 - to try to explain the chain rule really
58:25 - quickly okay let me back up for a second
58:31 - I did the power rule okay I'm gonna let
58:45 - me try to okay so the power rule says
58:53 - now two times error okay but I also need
58:57 - the chain rule I'm not done
58:58 - why do I need the chain rule well the
59:00 - chain rule is a rule I'm going to erase
59:02 - this over here use another marker
59:05 - because somehow if I just use multiple
59:08 - colored markers all this will make sense
59:10 - the chain rule states who okay let's say
59:15 - I have a function like why can you reach
59:18 - you see this orange y equals x squared
59:23 - and I have a function like X equals Z
59:30 - squared so Y depends on X X depends on Z
59:37 - well what the chain rule says is if I
59:41 - want to get the derivative of Y relative
59:44 - to Z what I can do is I can get the
59:48 - derivative of Y relative to X to X and
59:54 - then multiply that by the derivative of
59:58 - X relative to Z which is then x to Z so
60:05 - that's the chain rule I can kind of
60:07 - chain these derivative functions I have
60:09 - two functions and I could get the
60:12 - derivative of one but I should stock
60:15 - quit while I was ahead I can change I
60:20 - riveters I could get the derivative of
60:22 - one relative to something
60:24 - time's the derivative of that something
60:26 - relative to something else and that's
60:27 - actually weirdly what's going on here it
60:31 - may not be immediately apparent to you
60:36 - but J is a function of error error is
60:42 - actually a function of M and B right
60:45 - because the gas is some so so whenever I
60:50 - sometimes I just have to like say to
60:52 - statement and move on J is a function of
60:58 - error and error is a function of N and B
61:01 - because I'm computing the error as the
61:04 - guest MX plus B minus a known Y so here
61:08 - I could then say get this derivative 2
61:10 - times error and multiply that by the
61:15 - derivative of that error function itself
61:22 - relative to M because if I'm trying to
61:26 - get Delta n now I could also also do it
61:29 - relative to B when I want Delta B and
61:32 - this has to do with a partial derivative
61:34 - well see there's so many concepts baked
61:36 - into this that are a lot maybe add again
61:41 - I'm sitting here being like this was all
61:42 - just a bad idea okay but what is this
61:46 - this is actually quite simple to work
61:48 - out and I'm going to do that for you
61:50 - right now I'm gonna get the black marker
61:51 - and what I'm going to do is now I want
61:54 - the derivative of error relative to M
61:55 - okay well what is this actual if I
61:57 - unpack this function guess is M X plus B
62:04 - minus y error equals this so when I say
62:14 - partial derivative means like the
62:16 - derivative relative to M what I mean is
62:20 - everything else is a constant X is a
62:22 - constant B is a constant Y is a constant
62:25 - I mean x and y are actually already
62:27 - constants because those are the things
62:28 - that X is the input data Y is the known
62:32 - output result so this really I should
62:36 - write this as like
62:37 - x times M plus B minus y so this the
62:43 - derivative of this right the power rule
62:46 - says 1 times x times m to the 0 power
62:52 - which means x and the derivative of a
62:56 - constant is 0 because the constant
62:58 - doesn't change right derivative
63:00 - describing how something changes the
63:02 - derivative of this is there so guess
63:04 - what it's just X meaning this whole
63:07 - thing turns out to just be x equals 2
63:11 - times the error times X and guess what
63:16 - this two we're gonna the whole point is
63:19 - if you watch the previous video is we're
63:20 - going to take this and multiply it by
63:22 - something called a learning rate because
63:25 - we wanted to we want to like we know the
63:27 - direction to go this is giving us the
63:29 - direction to go to minimize that error
63:31 - minimize that cost but do I want to take
63:34 - a big step or a little step well if I'm
63:36 - going to multiply it by a learning rate
63:38 - anyway it's sort of like this too as no
63:40 - point because I can have a learning rate
63:42 - that's twice as big or half as big so
63:44 - ultimately this is all it is error times
63:47 - X all of this math and craziness with
63:50 - power rule and chain rule and partial
63:53 - derivative this it all boils down to
63:55 - just finally we get this error times X
63:58 - that's what should go here in Delta M
64:02 - guess what let's go back over to our
64:04 - code edit point
64:15 - oh yeah yeah okay week Mon thank you for
64:21 - a good okay but where am I here but why
64:25 - did i whoops
64:41 - we go back to our code and we can see
64:44 - there it is
64:47 - error times X air times X there we go
64:52 - that's it that's why that says error
64:55 - times X no look that was a lot that's
64:58 - why it says it I feel so happy that we
65:00 - kind of even though it was not the best
65:02 - explanation and there's lots of
65:03 - confusing this in pieces I feel very
65:05 - happy to have arrived there this was
65:07 - useful for me just making this video
65:08 - makes me feel like something happened
65:10 - today okay so um I just want to make
65:15 - sure I had a comment in the chat and
65:17 - maybe I have yes yes yes yes this should
65:21 - say ah this is a big mistake here wait
65:27 - hold on no it's not
65:29 - hold on yeah that's the chain rule okay
65:37 - okay okay okay
65:38 - that's oh that's not a mistake but okay
65:41 - so okay where's my purple marker I put
65:44 - the wrong cap and put the wrong cap of
65:48 - it
65:48 - I'm putting the caps on the wrong
65:50 - markers my kids do this all the time
65:54 - they put the like they put all like none
65:57 - of the markers have the correct caps on
65:59 - them drives me crazy okay hold on okay
66:07 - two things I want to mention a couple
66:09 - things I want to mention here a a way
66:12 - that I can make this make a little bit
66:14 - more sense here a little just to clarify
66:16 - this chain rule thing a little bit
66:17 - better thank you to K week bond in the
66:20 - slack channel is that I could just to
66:24 - see here on what I'm looking for is the
66:26 - derivative of the cost function
66:29 - you know relative to M what happens when
66:31 - I change the M value what does that do
66:33 - to the cost and the chain rule says that
66:36 - if I look at the derivative of that
66:38 - function start relative to the error I
66:42 - can multiply that by the derivative of
66:45 - the
66:46 - our relative to em right so this is
66:50 - actually the chain rule so I can get
66:53 - this by doing the derivative of relative
66:55 - to error the derivative of error
66:57 - relative to M and that's what's going on
66:59 - here two times error times this and
67:02 - that's where I'm getting all this stuff
67:04 - okay so this is one way of looking at
67:06 - this and you can see like oh yeah it's
67:07 - kind of like the numerator and
67:08 - denominator cancel each other out so
67:09 - that makes sense the other thing is if I
67:12 - get this whole thing again but did the
67:14 - derivative down here relative to B right
67:18 - B instead of M what do I get here well I
67:23 - get this is now a constant so this
67:25 - becomes zero this is a constant this
67:27 - becomes zero and what is this this I
67:29 - take the power rule so I take one times
67:32 - B to the zero I just get one so this
67:35 - becomes a error times rather than times
67:40 - points look at this nest that I wrote
67:42 - here can we please end this video with
67:44 - this at least written is the very nice
67:46 - handwriting so when it's relative to M
67:51 - this was two times error times X but
67:59 - when it's relative to B that's when it's
68:01 - relative to M but when it's relative to
68:03 - B it's two times error times one and
68:07 - again we could get rid of the two so
68:08 - it's really just error times X or error
68:10 - times one and then if I come back over
68:14 - here again there you go
68:16 - error times X M changes by error times
68:19 - xB changes by just error
68:26 - Oh so um that hopefully gives you some
68:32 - more background as to why these formulas
68:36 - exists this way and we'll give a chat
68:40 - and as I go forward in session for what
68:45 - comes after this is now session four
68:46 - where I'm going to build a neural
68:49 - network model for learning you're going
68:51 - to see this formula over and over again
68:53 - change the weight instead of saying m
68:56 - and B I'm going to say the weight well
68:58 - the weight changes based on the input
69:01 - multiplied by the error and then there's
69:03 - going to be a lot of some other pieces
69:05 - of but this formula is going to be
69:06 - everywhere so I hope this was another
69:10 - attempt again you know there's a lot of
69:13 - things I've glossed over here in terms
69:15 - of a lot of the background in terms of
69:18 - you know what really is a derivative
69:20 - why does calculus exist why is the chain
69:22 - rule work the way it works why is the
69:24 - power rule work the way it works what's
69:25 - why what what that partial derivative
69:28 - hon did you things like that partial
69:29 - derivative and so again take a look at
69:31 - this video's description and I'm going
69:32 - to point you towards resources and
69:34 - tutorials that kind of dive into each of
69:36 - those components a bit more deeply but
69:38 - hopefully this gives you some semblance
69:40 - of that overall picture okay thanks for
69:43 - watching and uh I don't know maybe maybe
69:46 - you want to hit like or subscribe but
69:48 - honestly totally totally understand
69:50 - totally totally understand don't you get
69:52 - the thumbs down I get it again it okay
69:55 - I'll see you in a future video maybe
69:56 - okay goodbye people of the chatter oh I
70:07 - can't go if you were actually worried
70:08 - about me or if you just like are tired
70:11 - of watching this because lots of people
70:12 - are saying please take a break from his
70:13 - math stuff the thing is I'm just going
70:20 - to feel so happy once I'm done with all
70:22 - this math stuff and honestly I maybe I
70:27 - should just do this on my own time but
70:28 - I'm kind of figuring this stuff out and
70:32 - okay
70:33 - so alright both I know what both means
70:37 - okay so one thing that I need to do
70:39 - is I need to do a quick since somehow
70:44 - instead of doing what I said I was going
70:45 - to which I was just going to work out
70:46 - the math and then kind of like once I
70:49 - figured it out make a video tutorial I
70:51 - kind of just worked it out and stopped
70:53 - and started and I made a big giant
70:55 - editing puzzle again before machiya and
71:00 - this might be another video that I post
71:02 - and then decide to like tick off the
71:04 - internet but you know anyway when I say
71:08 - take off the internet I'm never going to
71:09 - delete this so anybody wants to watch
71:10 - this and offer some feedback maybe learn
71:13 - something maybe not learn to like it
71:14 - will always be there it's a matter of
71:15 - whether I include it publicly in the
71:17 - playlist as part of the course materials
71:18 - and that's sort of the question I'm
71:21 - wrestling with but I do think I need to
71:24 - now that I've done this I do think I
71:32 - need to now that I've done this come
71:34 - back and just do like a one or two
71:41 - minute spiel for the beginning of this
71:43 - video okay so did I get it am I getting
71:53 - this why - guess and guess - why thing
71:57 - all wrong all over the place in multiple
71:58 - places I don't know okay
72:03 - so here we go
72:14 - hello it's me coming to you again from
72:16 - the future you might recognize me some
72:19 - from my failed videos as a calculus
72:22 - partial derivative anyway I made some
72:25 - videos with some calculus stuff they
72:26 - didn't turn out very well you can find
72:27 - them if you want they're kind of
72:28 - unlisted now but I just I tried again
72:31 - and so this video if you're watching it
72:33 - this is a follow-up to my linear
72:35 - regression with gradient descent video
72:37 - that video stands alone it's a
72:39 - programming video where all I do is walk
72:41 - through with the code for how to build
72:44 - an example that demonstrates linear
72:46 - regression with gradient descent and
72:47 - this is a puzzle piece in my machine
72:49 - learning series that will hopefully act
72:51 - as a foundation a building block to your
72:53 - understanding of hopefully some more
72:54 - creative or practical examples that will
72:56 - come later this video that if you're
72:58 - watching it's totally optional to watch
73:00 - as part of this series because you just
73:02 - applied the formula but what I try to do
73:04 - in this video is give some background
73:06 - and I kind of worked it all out here
73:07 - this is the end this is what's on the
73:09 - white board I thought - now if I use
73:10 - multiple colored markers would somehow
73:12 - make a better video I don't think I
73:13 - really seems did that but um so I kind
73:16 - of walked through and trying to describe
73:17 - than that I should say that you know
73:19 - this involves topics from calculus and
73:22 - there's a great video series by three
73:25 - blue one brown on YouTube that gives you
73:27 - great background and more depth in
73:29 - calculus so I'll put links to those
73:31 - videos in this video's description
73:33 - honestly if you're really interested in
73:35 - kind of soaking up as much as this as
73:36 - you can I would go and watch those
73:38 - videos first and then come back here
73:40 - it'll give you that background for
73:42 - understanding the pieces that I've done
73:44 - here so I look forward to your feedback
73:46 - positive and negative constructive
73:49 - feedback into whether this was helpful
73:51 - if it made sense and if you then go on
73:54 - and keep watching there'll be some
73:56 - future videos where I'll be getting back
73:57 - into the code but there's no code in
73:58 - this video just the math stuff maths max
74:01 - okay
74:02 - I enjoy okay it's backwards in the code
74:08 - that's why the plus works there oh I see
74:12 - so it should be - got it got it got it
74:14 - okay you know I always just try it one
74:17 - way or the other
74:17 - Here I am okay alright so we did it
74:23 - everybody
74:24 - we did it we did
74:26 - we did it we did it whoo whoo we can
74:29 - party time excellent kind of please be
74:33 - done now I just like never make a video
74:36 - again I guess I will keep going all
74:38 - right first I need some water cuz I have
74:40 - to admit I didn't really stay very
74:43 - hydrated throughout that I think is a
74:45 - bit of a problem I saw somebody in the
74:50 - patron group on slack pointed me towards
74:52 - the dance your own PhD thesis
74:56 - competition which is really wonderful I
74:58 - feel like I should do a dance version a
75:03 - gradient descent to be maybe something
75:06 - like this okay so now I think then the
75:12 - next thing there's two things I could do
75:14 - right now there's so many things are
75:16 - good you right now I have I have a fair
75:18 - amount of time I step over an hour
75:19 - before I have to go so there's a good
75:21 - amount of time let's try our little slip
75:25 - I don't know if I should do this drop
75:26 - whole thing it's always a terrible idea
75:27 - but let's whoops so let me talk you
75:34 - through sorry a guy still some water on
75:38 - my trackpad so I'm fixing that okay let
75:41 - me um there's nothing running there
75:56 - I just want to I'm going to show you two
76:00 - things that I could do I could so one
76:07 - thing is there's still one example which
76:13 - is basically this so this is an example
76:16 - that uses a JavaScript library to do
76:19 - regression and you can see here it's
76:22 - exactly what you imagined in that it's
76:25 - just the same as my ordinary
76:26 - least-squares video but this is just
76:28 - using a library that does the
76:29 - computation behind the scenes and I can
76:31 - also do polynomial regression which
76:33 - means I can try to find a function
76:35 - that's a curve that fits this and you
76:39 - can see this is actually the function
76:41 - that's fitting it best you see this is a
76:43 - width so I could do this which I think
76:47 - has a lot of interesting applications
76:48 - just in terms of visuals and graphics
76:51 - even so that's one example I could build
76:55 - and demonstrate next that says like a
76:57 - one last piece in this week of
76:59 - classification regression the session
77:00 - three or I could go straight to working
77:06 - through the perceptron which actually
77:09 - you know what I have a have some slides
77:12 - of some diagrams so
77:24 - so I could this is the simplest model of
77:27 - a neural network a single perceptron
77:30 - that receives two inputs and as an
77:32 - output and I could walk through the code
77:35 - and the particular algorithm for how it
77:38 - works and build an example the
77:40 - perceptron I want to do both of those
77:41 - today so the question is should I skip
77:45 - the regression and go straight to the
77:48 - perceptron or should I work on the
77:49 - regression with the and I'm trying to
77:53 - decide and should I do it in processing
77:55 - sure do it in JavaScript I don't know
77:58 - like so let me let me strawpoll this
78:02 - because I'm so indecisive and somehow I
78:05 - think this is going to which goes
78:08 - straight to perceptron do poly do do
78:17 - other regression do regression library
78:23 - example first even even if that limits
78:29 - even if that pushes perceptron to next
78:33 - week like which one should I guarantee
78:37 - which one should I guarantee that I do I
78:40 - don't know that I have time for both so
78:45 - I'm going to create this poll I think I
78:48 - should go straight to perceptron I vote
78:50 - this is the poll strawpoll comm slash
78:53 - III gr8 one somebody who's a moderator
78:58 - it could maybe put that in the chat and
79:04 - I'm going to just go to view results
79:06 - there's no results yet I guess you guys
79:10 - are behind Oh mention the dangers of
79:13 - overfitting yeah go straight to
79:21 - perceptron good to see what all right
79:27 - all right that looks pretty clear to me
79:31 - all right so I'm leaning towards go I'm
79:34 - going to do everything that I have on my
79:35 - list at some point it's just sort of
79:37 - like
79:37 - what order do I do those things in I
79:39 - kind of I would be very glad to go
79:40 - straight to the perceptron to be fun to
79:42 - be honest okay that's that's pretty
79:45 - clear so now you know even I I don't see
79:50 - how I don't see how anything that's
79:53 - going to come back okay go straight to
79:54 - the perceptron okay so next I'm going to
79:58 - I'm just curious I'm going to another
80:00 - strawpoll perceptron processing Java
80:11 - p5.js JavaScript so I'm always curious
80:15 - about creep pull okay so now here's the
80:20 - next one
80:21 - five nine three ybe six and I'll take a
80:28 - look at these results in a little bit
80:30 - I suppose I could still look at that
80:34 - other poll but okay
80:39 - [Music]
80:48 - Oh fascinating
81:05 - close one everybody tie goes to
81:11 - processing by the way oh yeah that's
81:36 - amazing
81:37 - [Music]
81:39 - I'm senior profit gets like 40% of the
81:42 - votes I feel gonna do it prize
81:44 - [Music]
81:49 - I have the power to do whatever I want
82:17 - [Music]
82:19 - come on folks
82:21 - let's get some more votes for frosting
82:22 - and now let's do it
82:25 - [Music]
82:32 - [Music]
82:46 - that amazed how many people are voting
82:48 - how many people are actually watching ah
83:00 - good work everybody
83:02 - it's coming back I'll post the link to
83:06 - the whole straw poll comm 5 9 3 decoding
83:13 - trade brought to you by drop hold up do
83:20 - you think if I just like baked random
83:21 - endorsement as people start sending me
83:22 - checks in the mail
83:23 - [Music]
83:28 - look at this this is amazing
83:33 - [Music]
83:34 - this is like much too exciting okay I
83:37 - gotta give this I was gonna wait till
83:39 - this song is over
83:40 - and yet this song actually is 2 minutes
83:42 - left Python Python is coming I'm gonna
83:49 - do except lights on this summer
83:50 - [Music]
83:53 - every day how come why is life so hard
83:57 - there's so many other things I have to
83:58 - do think of how much bang videos I could
84:01 - make it was just you every day Wow look
84:12 - at that you guys are just the opposite
84:15 - of trolling me you're giving me a
84:17 - nightmare okay all right I'm gonna give
84:21 - this here's the thing I have the example
84:30 - I'll make it invoke just about in order
84:33 - to do this okay I'm gonna fade out the
84:40 - music you have one more minute to vote
84:48 - 15 seconds
84:51 - fifteen seconds
84:56 - their goal
85:02 - [Music]
85:08 - all right and final tally is processing
85:13 - with 124 votes p5 Jess with 104 boats
85:17 - and I totally didn't influence that at
85:19 - all I mean I was and I was like totally
85:21 - not influencing that at all
85:24 - all right it's nice let's do some
85:26 - processing today it's a good it'll be
85:27 - good it'll be good
85:30 - I will happily do it twice if necessary
85:34 - I will port the code this is not I'm not
85:37 - writing new code what I'm going to do is
85:41 - program and talk through a particular
85:44 - example that is already written about in
85:46 - chapter 10 of nature of code book with a
85:51 - little bit of background and the code on
85:55 - the website is all in processing but I
86:00 - also have if I go to nature of code
86:05 - examples p5.js and I go to chapter 10
86:08 - there is a JavaScript version of it
86:12 - right here so rest assured that there is
86:14 - a JavaScript and processing version out
86:17 - there for you and what matters here is
86:19 - the concepts the algorithms not the
86:21 - language okay perceptron poka I love
86:26 - that okay
86:27 - so now I need to get myself ready let's
86:32 - see what version of I'm gonna close this
86:34 - I think this might actually might
86:38 - consider this to be a coding challenge
86:39 - video and I should do like a session for
86:46 - introduction okay so there is a newer
86:49 - version of processing let me get that
86:59 - download that by the way once again I
87:03 - will just mention while in downloading
87:05 - processing ah so many thinks I want to
87:07 - do that I'm so behind my to-do list is
87:09 - really out of control but anyway please
87:13 - consider becoming a member of processing
87:16 - I know that I often mentioned my patreon
87:19 - and I'm thrilled and honored and for the
87:22 - support that people give me personally
87:24 - via patreon but I think that it's also
87:27 - important and perhaps moment and
87:29 - actually definitely more important to
87:30 - support nonprofit open-source
87:33 - development and you can do so through
87:35 - the processing foundation if you're
87:37 - interested ok now let me get the most
87:45 - recent version of processing which is
87:47 - three point three point three copy that
87:50 - to the desktop here and replace that and
87:53 - I can't it's in use close this close
87:56 - this here we go okay so now let me open
88:08 - up processing let me open up my guess
88:15 - what I have slides can you believe that
88:19 - I have slides it's only because I have
88:22 - the diagrams for the nature of code book
88:23 - and I put them into a PDF so I can use
88:25 - those so let me get this going here how
88:36 - is this font size font size okay maybe
88:41 - it should be a tiny bit bigger I guess
88:43 - maybe it's fine
88:46 - perceptron oh you know what I don't want
88:52 - to call that
88:57 - [Music]
89:00 - what do I want to call this hold on what
89:06 - did I do in nature of code I want to
89:10 - have a class the perceptron class so so
89:16 - what did I do we look at the code here
89:19 - simple oh I just I see I called it like
89:23 - simple perceptron and had an example
89:25 - number so yeah when I do that so this is
89:28 - a coding challenge simple perceptron and
89:38 - I will make a new tab when I get to it
89:41 - I'm going to add a set up and draw in
89:44 - advance how's that font size people are
89:52 - I somebody in a slack could tell me if
89:55 - the font size is okay it's too hard for
89:57 - me to follow the oh hi Simon Simon is
90:02 - watching Simon his wonderful viewer
90:03 - hello I love watching your videos make a
90:08 - p5.js version Simon has actually been
90:10 - hoarding a lot of my examples back and
90:14 - forth and so I will definitely make a
90:17 - p5.js version absolutely and if anybody
90:20 - wants to contribute that I welcome that
90:21 - too
90:22 - okay okay let's see looks fine size is
90:35 - good you can see things kind of in 120p
90:37 - okay sorry I lost my train of thought
90:41 - all right
90:46 - you guys okay it's okay for me to erase
90:49 - this beautiful diagram it's looks so
90:53 - dark by the way looking over at my
90:55 - monitor and everything seems so dark I
90:57 - need better lighting okay so I'm going
91:01 - to erase this everybody's okay with that
91:04 - speak now as always I always forget the
91:13 - dis stop this stop this stop this duck
91:15 - and then beating this stop this stop I
91:17 - believe dis dis stock the stock is this
91:38 - stock this stock this done and indeed
91:40 - this stop to stop and you lose this fish
91:42 - stock the stock is done and you do this
91:45 - stock will stop and is not this stop
91:47 - this stock get stuck underneath this
91:49 - stock is stock this stock one of these
92:03 - days I will get one of those glass
92:06 - panels that goes in front of you you
92:08 - could draw on it okay Vista Vista Vista
92:14 - Vista song okay so I think what I'm
92:22 - going to do in this particular so at
92:25 - some point I need to make an intro to
92:27 - session for video which will kind of
92:30 - give a little bit of background about
92:32 - neural network based learning some
92:35 - references and resources to historical
92:37 - references and resources but I'm going
92:41 - to put that aside for right now and so
92:43 - then the first video will simply be a
92:45 - coding challenge where I just launch
92:46 - right into the perceptron so what I need
92:49 - to do in this video is kind of explain
92:51 - what a perceptron is
92:55 - um what perceptron is and define a
93:00 - simple scenario to to solve with a
93:04 - perceptron right all the code for it to
93:07 - implement that and then towards the end
93:10 - I mean kind of reference why the single
93:13 - neuron the simple perceptron model isn't
93:16 - particularly useful but can serve as a
93:18 - building block for larger and
93:23 - multi-layered perceptron based systems
93:27 - that can actually do all sorts of things
93:28 - that's kind of my overview of what I
93:30 - meant going to attempt to do and then so
93:38 - that's my overview one thing that I want
93:41 - to reference is I'm in week four okay
94:02 - so I think that this is probably let's
94:06 - look on Wikipedia to see if I'm missing
94:11 - anything really sort of key here in
94:14 - terms of historical background actually
94:24 - okay right 1957 right right right right
94:35 - right okay
94:36 - I mean you know there's a lot of soup
94:40 - okay so I think this is a single I think
94:54 - this is a single video with not very
94:57 - much editing hopefully yeah
95:10 - look at all this interesting sorry I'm
95:17 - like looking at all this stuff on the
95:19 - Wikipedia page okay alright so sorry I'm
95:32 - thinking about what is the best scenario
95:36 - here I'm just gonna get started okay
95:45 - good
95:46 - I'll ollie in the chat ask whether
95:48 - you're going to make it tutorial about
95:49 - Chrome extensions
95:52 - uh soon I hope I mean I do like that's
95:58 - something I'm going to teach in the fall
95:59 - for sure so it's a point I will make
96:04 - those tutorials I should really go the
96:05 - sooner than later because I won't have
96:06 - to do a lot of like math stuff okay yeah
96:11 - yeah yeah okay all right here we go this
96:20 - is going to be coding challenge number
96:24 - what number am I on here what number
96:29 - coding challenge am I on coding
96:34 - challenge number 71 it looks like oh
96:38 - load more nope
96:41 - 72 coding challenge number 72 for a
96:47 - second I thought I forgot that I
96:48 - forgotten to record this to disk but I'm
96:50 - okay okay
96:56 - oh yeah here it is okay let me
97:18 - okay
97:22 - all right all right all right I'm sorry
97:24 - I'm ready I'm ready
97:29 - I'm not really but I'm it's four o'clock
97:37 - so I haven't out this is the last thing
97:39 - I'm going to do today well if there's
97:41 - time I'll answer some questions but I
97:43 - have to be done by 5:00 which is an hour
97:45 - from now and I can only assume that this
97:48 - won't take an hour but there's a lot of
97:52 - pieces to it this is not the simplest
97:54 - thing to mean that perceptron itself is
97:56 - incredibly simple but in terms of
97:58 - designing a whole system to demonstrate
98:00 - it that can be a little bit trickier so
98:04 - but we're gonna go for it anyway okay
98:07 - here we go No hello it's time for the
98:18 - perceptron okay
98:20 - how's bad idea hello welcome to another
98:28 - coding challenge this coding challenge
98:30 - is part of my intelligence and learning
98:33 - series and in this coding challenge I am
98:36 - going to attempt to make something
98:37 - called a perceptron and know this is a
98:40 - piece of the puzzle I'm working towards
98:42 - this path of eventually can I please
98:46 - start over I have one Mulligan here this
98:50 - is this is this is what happens to be oh
98:52 - it starts to get warm in here okay oh
98:54 - wow
98:55 - sorry hold on I think I'm going to lose
98:57 - the sweatshirt because it's getting a
98:59 - little bit warm here so we will change
99:02 - to just t-shirt oops
99:12 - I do have a slight problem here which is
99:14 - that I need to fix the mic yes okay this
99:25 - okay this sound okay still can you
99:27 - everybody hear me okay sound good
99:33 - okay move some stuff over okay this is
99:46 - this coding challenge is really gonna
99:48 - happen now okay here we go hopefully the
99:55 - sound everyone saying yes it sounds okay
99:57 - okay hello welcome to a coding challenge
100:00 - in this coding challenge I am going to
100:01 - attempt to make a perceptron this is
100:04 - part of a whole bunch of videos that I'm
100:05 - doing about neural network based
100:07 - learning and but in this video I'm not
100:10 - kind of getting into too much about the
100:11 - whole broader landscape of everything
100:13 - history-wise and future wise about
100:16 - what's happening with neural network
100:17 - based learning in this example I just
100:18 - want to build with code the simplest
100:21 - model of an artificial neural network
100:23 - possible known as a perceptron
100:25 - perceptron the concept of perceptron was
100:29 - invented by Frank Rosenblatt in 1957 at
100:32 - the Cornell aeronautical library
100:34 - laboratory I there's a link here to the
100:37 - original paper which I will include in
100:40 - this video's description if you want to
100:41 - take a look at that and of course you
100:44 - can always find more on the perceptron
100:46 - Wikipedia page but what I'm going to do
100:49 - now is talk about what it is okay so
100:54 - what is a perceptron now I'm going to go
101:00 - back over here I forgot that I had
101:02 - diagrams okay what is a perceptron mmm
101:09 - failure okay whoops hope this can be old
101:13 - ah okay
101:14 - oh yeah okay what is a perceptron so
101:21 - ultimately what would do what I'm doing
101:24 - in this coding challenge is building an
101:27 - artificial neural network now it's using
101:30 - the word okay okay do I need this
101:37 - diagram this is yeah okay okay by the
101:44 - way there will be editing in this coding
101:46 - challenge here by announce I got just I
101:49 - gotta get really in my own head here I
101:52 - just gotta shake it all off and let it
101:56 - go here we go okay what is the
101:59 - perceptron why are we making a
102:00 - perceptron so why what is the deal with
102:05 - this uh okay there we go what does it
102:13 - forcep Tron why are we here why are we
102:15 - making this so the idea here is to be
102:17 - inspired by the way the actual brain
102:20 - works the idea of the brain you know
102:22 - this is not a video on the brain and
102:25 - neuroscience but the idea is that the
102:28 - brain has neurons in them those neurons
102:31 - receive - I really am I really really
102:35 - explaining the driver I try really is
102:37 - this diagram really a part of okay okay
102:40 - I wouldn't get some momentum here in a
102:43 - second
102:52 - okay but what does the perceptron why am
102:59 - I even here talking about this so
103:01 - ultimately I want to look at artificial
103:04 - intelligence machine neural network
103:07 - based machine learning systems that are
103:09 - inspired by and modeled loosely off of
103:13 - the idea of the actual brain the actual
103:14 - brain being this thing with like neurons
103:16 - and axons that connect other neurons and
103:19 - dendrites that receive inputs I actually
103:20 - don't know what I'm talking about just
103:22 - reading what's ever on this diet but a
103:25 - perceptron and ultimately where I'm
103:28 - going to start building examples and
103:30 - show you examples of lots of these these
103:33 - components that are all interconnected
103:35 - and inputs are coming in and and outputs
103:37 - are flowing out but I want to start with
103:40 - this idea of the perceptron being a
103:43 - model of a single neuron the simplest
103:46 - possible artificial neural network that
103:48 - we could build and this will serve as
103:50 - the building block for future examples
103:53 - that it will make in future videos okay
103:56 - so let's come over to the whiteboard for
103:59 - a second so if the idea of a perceptron
104:01 - is that there is a single neuron call
104:05 - this a neuron that neuron could have
104:08 - inputs let's say there are two inputs
104:11 - I'm going to call them X 0 X index 1 so
104:19 - inputs 0 and 1 they come into the neuron
104:23 - some type of mathematical process
104:26 - happens in the neuron and then there is
104:30 - an output which I'll call Y or output so
104:36 - these are inputs and again this diagram
104:42 - it might look familiar to you if you've
104:44 - watched some of my other videos because
104:46 - I often talk about this idea of there is
104:49 - some amount of input so long list of
104:51 - inputs that go into some machine
104:54 - learning recipe that processes all those
104:56 - inputs and performs a task maybe it
105:00 - tries to classify
105:01 - or perform a regression but make some
105:03 - sort of output some sort of prediction
105:05 - so this is exactly what this perceptron
105:08 - is designed to do okay here's the thing
105:12 - in order for us to understand and look
105:16 - at all of the pieces of what's happening
105:18 - in here we need some scenario I have
105:21 - these I never have like pre-made
105:23 - diagrams but since I have these I'm
105:24 - going to go back and forth and use them
105:26 - so this is the pre-made scenario that
105:29 - I'm going to use what I want to do is
105:32 - clap I want to perform classification I
105:35 - want to find out if some points are on
105:38 - one side of a line or another side of a
105:40 - line and so does this make sense what
105:44 - I'm saying here let me add one maybe I
105:46 - won't I don't need this diagram I'll
105:50 - just draw that and go back
105:56 - oh I'm on the wrong camera
106:08 - so let's come up with a scenario that we
106:11 - can use so let's say I have a two
106:14 - dimensional space you could think of
106:16 - this this will be my canvas my window
106:18 - and what I'm going to do is I'm going to
106:23 - arbitrarily divide the space with some
106:26 - line some points will be on one side of
106:31 - the line and other points will be on
106:34 - another side of the line so essentially
106:38 - I what I want to do is use this
106:39 - perceptron for a classification problem
106:41 - I want to say that these belong to you
106:46 - know Class A and these belong to Class B
106:49 - so and I'm going to use a supervised
106:53 - learning strategy so for background on
106:57 - some of this you could go and watch
106:59 - session three of my intelligence and
107:01 - learning series where I do some other
107:03 - videos about classification and
107:04 - regression using a linear regression
107:06 - model there's a lot of crossover here
107:08 - but anyway you can stay here if you want
107:10 - I'm going to kind of talk through
107:11 - everything but the idea is I want to
107:13 - classify these so I want these inputs to
107:16 - be so x0 at any point right here like
107:19 - this X comma Y is a little bit confusing
107:24 - the way I'm using it's I don't I hate I
107:26 - don't love this because I should really
107:29 - think of the the X is input 0 the Y is
107:32 - input 1 right the X is input 0 the Y is
107:36 - input 1 the output is Class A or Class B
107:40 - the Y so I'm using x and y in two
107:43 - different places in slightly different
107:44 - ways which is a bit confusing but
107:46 - hopefully will make sense as we continue
107:48 - to go along here okay so but I actually
107:50 - want to say instead of instead of a and
107:54 - B what I actually want to say is plus
107:56 - one and negative one so the idea is that
107:59 - my perceptron is going to output a plus
108:04 - one if the point belongs to group a and
108:07 - it's going to output a negative one if
108:10 - the point belongs to group B so how does
108:14 - and we're going to use the technique
108:16 - known as supervised learning
108:21 - and what supervised learning involves is
108:24 - I am going to ask the perceptron to say
108:29 - here is some input give me a guess but I
108:31 - know the correct answer I'm going to
108:33 - give the perceptron a point that I know
108:36 - should be in a and it's going to guess
108:39 - either a and B if it guess is a I'm
108:41 - going to say great job perceptron you
108:44 - keep on going if it guesses B I'm going
108:46 - to say perceptron you made a mistake
108:49 - let's tweak something about your
108:52 - algorithm to try to get you to the
108:54 - correct answer and this tweaking is a
108:58 - process known as gradient descent and
109:00 - it's something I've also covered in a
109:02 - couple different videos that I will also
109:04 - link to down here we're going to go
109:05 - through it as I get through here okay so
109:06 - that's the supervised learning process
109:09 - okay so what is the actual algorithm
109:12 - what happens here inside the neuron so
109:15 - here there's here's the missing piece
109:17 - there's a there's a few different
109:19 - missing pieces that I'm get to over time
109:21 - I was stepping on something enormous
109:23 - these you can think of as connections
109:26 - the input flow into this neuron but our
109:31 - waited waited waited as they flow in so
109:35 - each one of these connections has a
109:37 - weight I'm going to say W 0 W in W 1 so
109:43 - these inputs are weighted and what the
109:47 - perceptron does its algorithm is to
109:50 - create a sum of all of the inputs
109:53 - multiplied by the weights that sum is X
109:58 - input times sorry input 0 times weight 0
110:03 - plus input 1 times weight 1 now in this
110:09 - case the perceptron only has two inputs
110:12 - so this is a very easy formula to write
110:14 - as you're going to see as I get into
110:16 - future videos you might realize like oh
110:19 - there's a hundred inputs or a thousand
110:21 - inputs or a hundred thousand inputs but
110:23 - this same formula is always going to
110:25 - apply a weighted sum of all the inputs
110:28 - input zero times weight zero input 1
110:31 - times
110:31 - one add them all up together so that's
110:34 - step one is the some step two before so
110:39 - you could say like okay we'll take that
110:41 - and that's the output but this isn't the
110:43 - output step two is something called an
110:46 - activation function activation function
110:51 - and this is a key concept in neural
110:56 - network based machine learning as I get
110:58 - into future videos we're going to see
111:00 - there's a variety of different kinds of
111:01 - activation functions and wide let you
111:03 - use this one or this one and what do
111:05 - they do and why but typically what an
111:08 - activation function does is it allows
111:11 - you to conform the output to some
111:14 - desired range and do thing and another
111:19 - way another way to think about it is if
111:20 - you think about that idea of the brain
111:22 - like you can think if does the does the
111:25 - neuron fire and continue to send its
111:27 - data along or does it not so what
111:28 - happens as the data comes out of that
111:31 - neuron we're going to use in this
111:33 - particular example were going to use a
111:34 - very very simple activation function you
111:36 - could think okay well I only want two
111:38 - outputs
111:38 - I want a plus one or a negative one how
111:41 - can I take any number I can take any
111:43 - number and convert that number into plus
111:46 - one or negative one
111:47 - how would I do that how about a function
111:50 - called sine take the sine of any number
111:54 - n if that number n is positive then I
111:59 - get a plus one if that number n is
112:03 - negative then I get a negative one okay
112:07 - so that's the activation function this
112:09 - is the whole process it's off it's often
112:12 - referred to as feed-forward the inputs
112:16 - come in they get multiplied by the
112:19 - weights they get added together and then
112:21 - that weighted sum gets passed through an
112:24 - activation function and then that
112:26 - activation function gives us a plus one
112:28 - or negative one should the neuron fire
112:30 - or not fire and that gets sent out and
112:32 - that's the output okay pause for a
112:36 - second so there's a lot more to this but
112:40 - I think what I'm going to do is go and
112:42 - write the code for this now I'm going to
112:45 - just check it to see if there's any
112:46 - questions audio sounds fine what about
112:50 - zero yeah that's a good point I can't
112:56 - see what that chat message was I'm
112:58 - reading in the chat there's like a
112:59 - message that says I feel bad for anyone
113:01 - older than like 25 but I've got I've got
113:05 - some serious problems bed is this is the
113:07 - focus on the whiteboard okay can you see
113:09 - it looks like it's fine to me okay
113:12 - okay so there's good questions here okay
113:16 - okay I'm back from check I was just
113:19 - checking a live chat that's going on if
113:20 - you're watching this is an archive of a
113:21 - live chat doesn't exist but there are
113:23 - two important questions before I move on
113:24 - number one is what do you do is 0 I
113:27 - don't know we could just pick we could
113:30 - just arbitrarily right now let's just
113:31 - say this is greater than or equal so 0
113:33 - will will consider +1 I mean in the case
113:36 - this is just like a toy example just to
113:38 - demonstrate the idea it's a building
113:40 - block you know I don't know how
113:41 - meaningful it is to be able to like
113:43 - classified points into these place by
113:44 - sort of lying in your at but but so if
113:47 - this pointed to make an arbitrary
113:48 - determination for that what if it's on
113:50 - the line you know is it is it above or
113:51 - below I'll pick one the other question
113:53 - that was asked is well how do you pick
113:55 - these weights so this is the essential
113:57 - question and this is where I have to get
114:00 - to so the idea is that what we through
114:03 - the supervised learning process we want
114:06 - to search for we're basically doing the
114:08 - search to find the optimal weights the
114:10 - optimal weight values that will get the
114:12 - best results there's a results with the
114:14 - least amount of errors and so to start
114:17 - we have to pick something to start in
114:19 - this case we could pick random values we
114:21 - could just start with the weight at 0
114:23 - that could be problematic so there's
114:25 - different ways this is a big topic in
114:27 - the field of machine learning when you
114:28 - start a neural network based system how
114:31 - do you how do you initialize the weights
114:33 - randomly what kind of distribution of
114:36 - random numbers do you do some other kind
114:37 - of like learning process that gets do
114:39 - like a good starting point for the
114:41 - weights that's a big topic of discussion
114:42 - and research but for us I'm going to
114:45 - pick random weights and start to tweak
114:47 - that ok so there's a lot more pieces of
114:48 - this still but I think I'm going to go
114:50 - and start writing some code and the
114:51 - come back two pieces that we're missing
114:57 - okay so I'm going to do this in whoops
115:04 - I'm going to do this in processing which
115:08 - is a Java based programming language and
115:12 - environment you can find out more about
115:14 - it at processing org
115:15 - I will also release a JavaScript version
115:18 - of this that you can run in the browser
115:19 - so check this video's description for
115:21 - links to both source codes after it's
115:24 - over okay so what I want to do is I'm
115:27 - going to create a perceptron class so
115:34 - what is it the what is it that a
115:36 - perceptron needs a perceptron needs
115:40 - weights so actually hold on I think I
115:44 - have some yeah uh do I want to look at
115:49 - this hold on all right let me hold on
115:55 - yeah I want to create a perceptron class
116:00 - so we can see here by the way I have
116:02 - this slide here this is the same
116:04 - algorithm I just talked through let's
116:05 - just make sure we have it right the
116:06 - algorithm is for every input multiply
116:09 - input by the corresponding weight sum
116:11 - all the weighting it weighted inputs and
116:13 - then compute the output based on that
116:15 - sum pass through an activation function
116:17 - the sine of the Sun so we can see we can
116:20 - think of like this could be the point at
116:22 - 12 comma 4 and these could be the
116:24 - weights of the perceptron so what I'm
116:26 - going to do for this particular
116:27 - perceptron is I am going to create an
116:31 - array to store all of those weights and
116:34 - I'm going to say it's an array with two
116:36 - elements in it and in the perceptron
116:42 - constructor I'm going to loop through
116:46 - all of the weights and give them a
116:51 - random value between negative 1 and 1 so
116:55 - but whoops
116:57 - you don't say void with a constructor I
116:58 - don't remember how to program in Java
117:01 - based language
117:02 - okay so this is the constructor and what
117:07 - I want to do in the constructor is
117:09 - initialize the weights randomly okay now
117:15 - what are some things the perceptron
117:17 - should do well I one of one of the
117:20 - things that should do is it should be
117:21 - able to receive inputs and then compute
117:25 - a guess and output we'll call that a
117:27 - guess okay so let's write a function I'm
117:32 - going to call it a guess and it should
117:34 - return an integer plus 1 or negative 1
117:37 - and it should receive inputs which could
117:41 - also be in the form of an array now I
117:44 - could if I wanted to because the
117:46 - simplicity of the example I could have
117:47 - done something like float W 0 and float
117:50 - W 1 I could just sort of have individual
117:53 - variables for the weights instead of an
117:54 - array but the nice thing about doing
117:55 - this way is this is more flexible that
117:57 - this we could have if we reuse this code
118:01 - later with what we could adjust the
118:03 - number of inputs and that sort of things
118:04 - within our array okay so first thing I
118:07 - need to do is compute that weighted sum
118:09 - so I'm going to create a variable called
118:11 - sum and initialize it at 0 then I'm
118:14 - going to loop through all of the weights
118:20 - and I'm going to say sum plus equal what
118:24 - do I want to do the sum of all the
118:26 - inputs multiplied by their corresponding
118:28 - weights so inputs index I times weights
118:34 - index I so this is now that weighted sum
118:41 - I say that second step start with the
118:45 - summit 0 loop through and multiply all
118:48 - the inputs by the weights then what I'm
118:51 - going to do is I am going to return up I
118:55 - need to get the up so then I'm going to
118:57 - say the output is sine of the sum so it
119:02 - doesn't know what sine is there probably
119:04 - is a like java-based function I could
119:07 - call automatically but let's just write
119:08 - our own up at the top of this code here
119:10 - I'm going to write a function I'm going
119:12 - to say int sine and it gets I guess I
119:15 - could say it gets any
119:16 - I'm going to say if n is greater than or
119:19 - equal to zero return a positive one
119:23 - otherwise return a negative one so this
119:27 - is just that this is the so I could I
119:29 - could write here as a comment this is
119:31 - the activation function I could call it
119:34 - activate or something instead the
119:35 - activation function is a function that
119:37 - receives some value if it's greater than
119:39 - zero positive one if it's less than zero
119:42 - negative one so no matter what number
119:44 - goes in to the what other inputs come in
119:47 - whatever that's weighted sum is no
119:48 - matter what the only thing this
119:50 - perceptron can ever output is a 1 or
119:52 - negative one so and then I can say
119:54 - return output so now I have basically if
120:01 - I kind of give myself hope I really want
120:06 - I guess this is it
120:07 - I don't want to see the whole thing this
120:08 - is if I I have all the code for the
120:11 - almost all the coach of the perceptron a
120:12 - perceptron has a bunch of weights it
120:14 - initialize the weights randomly and it
120:16 - can perform a guess by receiving all the
120:18 - inputs doing a weighted sum passing it
120:20 - through the activation function ok so
120:22 - now if I were to just create something
120:25 - arbitrary just to sort of test if this
120:27 - is working I'm going to say float inputs
120:33 - equals I'm going to just create some
120:36 - random values like negative one zero
120:38 - point five and I'm going to say print
120:41 - line o first I'm going to say I'm going
120:43 - to have a perceptron I'll just call it P
120:48 - for perceptron P equals a new a new
120:51 - perceptron and then I can say P guess
120:55 - inputs and I can say output I can say
121:04 - guess and I can say what's wrong here
121:10 - what doesn't it not like oh oh this
121:13 - should be sorry
121:14 - that should syntax wise that should
121:16 - these are the inputs that's an array and
121:18 - I should say sorry a print line guess so
121:25 - if I run this how come I can't
121:28 - here we go if I run this we should see
121:32 - up I hey I'll put it a 1 let's run it
121:36 - again I got a 1 run it again eventually
121:39 - I should be able to run it a bunch of
121:40 - times and I got a negative 1 negative 1
121:47 - okay so this I believe is working the
121:50 - system works I have a perceptron object
121:53 - I can feed in inputs and I can get make
121:59 - a guess now here's it so time out for a
122:02 - second let's looking at the chat whoops
122:12 - I broke my bail I just broke my bail I
122:18 - mean I didn't break it I can just
122:20 - reattach it I didn't break it
122:24 - topher J in the chat rightz I believe
122:26 - you can change the color scheme of the
122:28 - processing editor I forget how to do
122:30 - that but I think in that in the sort of
122:31 - like deeper preferences you can change
122:32 - that okay no no it's fine
122:41 - it sounds different yeah I think good
122:44 - alright I'm sorry I just needed a like
122:46 - little mental break for a second it's
122:48 - for 23 for 23
122:53 - settle down everybody okay
122:57 - alright so let me keep going here okay
123:07 - okay so we have the overall structure
123:09 - now for the perceptron and it works but
123:14 - we need some we need to do more so
123:18 - here's the thing we need to create a if
123:20 - I had an actual data set if I were to
123:23 - try to classify flowers and these were
123:26 - sunflowers and these were daisies and
123:28 - this x-axis was like petal length and
123:30 - this was sepal length or something I
123:32 - could use a real data set here I'm gonna
123:34 - do something really phony-baloney I
123:37 - think I'm going to like kind of almost
123:39 - be really ridiculous about it which is
123:42 - that I'm going to say I'm going to
123:44 - actually just say that anything do I do
123:48 - I really want to do this I'm going to do
123:49 - it this way let's do anything that is
123:53 - I'm going to cut I'm going to use the
123:58 - line but I'm going to use the line that
124:01 - goes across the middle right so if this
124:03 - is X and this is y these are all the
124:06 - points where X is greater than Y well
124:11 - I'm so lost I'm trying to think of
124:15 - what's like a really simple thing I
124:17 - could do hope the camera went off
124:34 - um
124:38 - yeah let's just consider the line y
124:41 - equals x so anything that's above y
124:46 - equals x anything that's above y equals
124:55 - x is a plus 1 anything that's below y
124:58 - equals x should be a negative 1 so I
125:01 - want to create a known data set a known
125:04 - data set that I can use to train the
125:06 - perceptron so let's do that really
125:09 - quickly what I'm going to do is we think
125:16 - about this I'm going to there's so many
125:20 - different ways I could do this whoops
125:21 - sorry I got to thinking my eye good uh
125:25 - so many different ways I could do this
125:26 - I'm gonna do I'm gonna make a tab called
125:28 - training and I'm going to make a class
125:33 - called point and the point is actually
125:39 - just an input array that has three
125:46 - values in it no no let me think about
125:51 - this let me actually have the point have
125:56 - an X and a Y and also a I don't want to
126:03 - call it class a label I'll just call it
126:06 - a label okay so if I make a new point
126:12 - when I make a new point I'm going to say
126:16 - x equals a random with y equals random
126:20 - height am I going to run into trouble
126:22 - without nap just using the pixel
126:23 - coordinates let's try just using the
126:24 - pixel coordinates I don't know if that's
126:26 - going to be a problem and then the label
126:28 - I could say if X is greater than Y the
126:37 - label is one else the label is negative
126:45 - one right that should give me that
126:49 - should give me everything about I don't
126:50 - know what's above what's below
126:52 - let's do that and then let's do let's
126:54 - write a little function called like show
126:57 - where I'm going to say stroke zero and
127:01 - I'm going to say if label equals one
127:07 - fill 255 else fill zero and then I'm
127:14 - going to draw a little ellipse at X
127:17 - comma Y a small lips okay so what I'm
127:22 - going to do is I'm going to make an
127:27 - array of points I'm going to make a
127:33 - hundred of them and I'm going to
127:39 - initialize them and I'm going to say
127:49 - points index I is a new point and then I
127:54 - let's make the size a little bit bigger
127:57 - 500 comma 300 then I'm going to do a
128:00 - background 255 and for every point in
128:06 - points this is an enhanced loop in Java
128:09 - for every point in points I'm going to
128:12 - say pho so if I run this we can see uh
128:22 - yeah I mean let's make this a square so
128:28 - it looks a little less weird
128:30 - and I can also draw just to sort of like
128:33 - see correctly I'll draw a line from 0 0
128:37 - to with comma height so you can see I
128:42 - picked all these points half of them are
128:45 - on this side and half of them are on
128:47 - this side so I'm reading the chat which
128:54 - I know I shouldn't do I shouldn't I just
128:56 - didn't read the chat sorry ok yeah
129:00 - topher the check points out the reason
129:03 - why I don't use the for
129:04 - each loop in JavaScript I guess there's
129:06 - the four in before each loop in
129:08 - JavaScript
129:08 - it's like asynchronous and like weird
129:10 - things can happen I got to get into that
129:12 - I know I got a bit of a prior five some
129:14 - weird quirks that don't make any sense
129:16 - okay all right now what I'm going to do
129:26 - now is I am going to do it
129:35 - I'm thinking I'm thinking I'm thinking
129:36 - like should be X is greater than equal
129:39 - to Y to be consistent should it be yeah
129:46 - probably
129:47 - that's a good point um oh I'm thinking
129:51 - I'm thinking you going to pause for a
129:53 - second um all right what do I need to do
129:56 - next I got to get to the learning part
129:58 - okay okay all right so now that I have
130:09 - all these points and I can see them
130:12 - correctly categorized this is my known
130:15 - training data so what I need to do
130:18 - process wise is I need to take all of my
130:22 - known training data one at a time I need
130:25 - to pass it in ask the perceptron to give
130:29 - me a guess is it in one is it in one or
130:32 - is it in negative one and then I need to
130:34 - do something based on whether it's
130:36 - correct or incorrect what is it that I
130:40 - need to do okay
130:43 - let's establish something okay actually
130:48 - I've got a problem
130:56 - we've got a problem what time is it
131:01 - we've got a problem that I'm going to
131:03 - run out of time it's 4:30 I'm trying to
131:10 - figure out when I want to introduce the
131:12 - fact that I need the bias white these
131:16 - are hard things to just like completely
131:17 - talk through all right maybe I don't
131:22 - want to talk about the bias just yet
131:24 - I'll get to that in a little bit maybe I
131:36 - should get to that now thinking here
131:43 - okay
131:50 - all right
131:56 - okay uh-huh all right I'm gonna keep
131:59 - going I mean I'm going to come back to
132:01 - the bias in a little bit okay there are
132:08 - still some missing pieces here I need to
132:10 - that I need to add but I'm going to keep
132:12 - going let's talk about training let's
132:13 - talk about supervised learning here's
132:17 - what I need to do I need to take all of
132:20 - that known data I'm going to take each
132:26 - and every piece of known data I'm going
132:28 - to take the x and y pointer to pass it
132:30 - in it's going to do the weighted sum
132:32 - it's going to it's going to passing the
132:33 - activation function and it's going to
132:34 - guess plus 1 or negative fun 1 plus fond
132:38 - or negative fun are you having we're
132:40 - having some negative fun right now I'm
132:42 - pretty sure so this is going to give me
132:45 - a guess but I also have the answer right
132:51 - so I have both the perceptrons guess and
132:55 - I have the answer if I have both of
132:59 - those things I can compute something
133:04 - known as the error the error I can think
133:07 - of as let's say the answer I always get
133:11 - this wrong backwards one way or the
133:13 - other but it's the difference between
133:15 - the correct value and the incorrect
133:18 - sorry between the correct value with the
133:20 - guests right because if it's if the
133:23 - guest is a plus-one and the answer is a
133:26 - plus-one
133:27 - what's that error 1 minus 1 that error
133:29 - is 0 and I actually have a little bit of
133:31 - a diagram here to illustrate this is a
133:34 - pretty simple scenario where these are
133:39 - the open the wrong sorry I didn't switch
133:45 - the camera I have a diagram over here
133:52 - where you know you can sort of see these
133:54 - are the only possible correct answers
133:57 - negative 1 or +1 so there's only four
134:00 - possibilities if it's supposed to be a
134:02 - negative one I could guess a negative
134:04 - one
134:04 - or a plus one if it's supposed to be a
134:05 - plus one I could guess the negative one
134:07 - or a plus one so these are all the
134:10 - possible errors the error is either zero
134:12 - or the error is negative 2 or positive 2
134:14 - or 0 so this is a this is a good
134:17 - starting point I need to have this error
134:19 - the idea here oh come back let me come
134:22 - back to the whiteboard ok so remember
134:25 - what I'm trying to do is find the
134:28 - optimal weights so ultimately what I
134:33 - want to do is I want to figure out if I
134:36 - want to say well the weight should equal
134:39 - itself plus some change in weight I want
134:45 - to adjust the weight if there's a
134:47 - mistake I want to make a tweak I want to
134:49 - like make the weight a little bit higher
134:50 - or a little bit lower right because
134:52 - maybe my weighted sum got me below
134:54 - negative 1 but it should be a plus 1 if
134:57 - I make that weight higher maybe that'll
134:59 - push the output up to positive so the
135:02 - issue becomes how do I calculate that
135:04 - Delta weight how should this weight
135:07 - change based on how should the weight
135:11 - change for weight 0 for weight 1 and if
135:14 - there were many many many more weights
135:16 - so the way that this is calculated with
135:19 - is with a process called gradient
135:21 - descent and I have a couple videos where
135:23 - I go through this in in pretty large
135:26 - detail one way of thinking about this
135:29 - which I'll kind of duplicate here in
135:30 - this video is this really relates back
135:32 - to a lot of my steering examples so I
135:35 - have all these steering examples where I
135:37 - have a vehicle that has a given velocity
135:39 - and it's seeking a given target so this
135:44 - vehicle has a velocity and it also has a
135:46 - desired velocity right because if it
135:49 - should be going towards that target its
135:51 - desired velocity is to go towards that
135:52 - target so you can think of this steering
135:58 - formula if you go back to that Craig
135:59 - Reynolds steering formula the steering
136:03 - formula equals the difference between
136:05 - the desired velocity the way that I want
136:07 - to go and the current velocity which is
136:09 - kind of like my guess and if I get this
136:12 - a steering formula if I get the steering
136:14 - if I get this steering vector and I add
136:18 - it to the velocity it's going to cause
136:20 - me to turn and go towards that target so
136:23 - essentially that's what I want to do
136:25 - here this steering vector is the error
136:27 - the desired velocity is the answer
136:30 - that's where I want to go the velocity
136:31 - is my guess that's where I'm going right
136:33 - now
136:33 - I want to steer in the direction of the
136:36 - error so Delta wait the Delta wait is
136:39 - actually equal to the error multiplied
136:43 - by the input so it's filtered through
136:46 - the input what's that error filtered
136:48 - through the input that's how I change
136:49 - the weight itself so that's the process
136:52 - that I'm going to do over and over again
136:54 - and I have a slide here that I think
136:57 - will walk through that in up so I'm in
136:59 - the wrong keyboard here so this is the
137:02 - process this is the supervised learning
137:04 - algorithm provide the perceptron with
137:07 - inputs for which there is a known answer
137:09 - and ask the perceptron to guess okay the
137:12 - perceptron guessed what's the error is
137:15 - it right or wrong is the error zero is
137:17 - it - is it negative to adjust the
137:19 - weights according to the error and go
137:21 - back and do it again and again and again
137:23 - and this is the formula the weight is
137:27 - changed according to the air multiplied
137:30 - by the input and there something called
137:31 - learning rate which I'll get to in a
137:32 - second so let's see now I've kind of
137:34 - explained that in pieces let me see if I
137:36 - can now add that to the code so I'm
137:40 - going to here whoops I'm going to now
137:48 - create a function in the perceptron and
137:53 - I'm not going to call it guess I'm going
137:58 - to call it a train so this is going to
138:04 - receive some inputs and a target right
138:08 - the difference between the guests is the
138:11 - guest is something like oh I just want
138:13 - to receive these inputs and provide a
138:15 - guess with training I want to receive
138:17 - the inputs in the known answer so I can
138:19 - adjust the weights accordingly okay so
138:22 - the first thing actually that I should
138:24 - do is just get the guess which is
138:27 - actually
138:28 - with those inputs so since I already
138:30 - have a function that does the guess I
138:33 - can ask for the guess from that guess
138:35 - then what I want to do is get the error
138:38 - the error equals the error equals the
138:45 - target - the guess that's the error so
138:50 - now that I've done that what I can do is
138:53 - I can go through all of the weights and
139:00 - say each weight should change according
139:05 - to that error multiplied by its
139:09 - corresponding input so this is that
139:12 - particular algorithm this is tuned all
139:15 - the weights okay so this is like
139:22 - basically supervised learning says put
139:25 - the data in get the result if the result
139:28 - is right just move on move along nothing
139:31 - to see here if the result is wrong twist
139:33 - some dials in here to try to get it
139:34 - closer to the correct answer and do it
139:36 - again and again and over and over again
139:38 - here keep twisting dials eventually to
139:40 - find that optimal result now there's
139:42 - something important here though if I go
139:45 - back to this steering example you could
139:47 - think about okay so this is the vehicle
139:48 - it's going in this direction it should
139:50 - seek the target it knows what the error
139:53 - is the error is the difference between
139:55 - the way it should go and its current
139:57 - velocity how much should it steer if I
140:00 - steer a lot I could actually like
140:02 - overshoot and start going the wrong way
140:04 - in the other direction but if I steer
140:06 - just a little bit maybe I'm going closer
140:09 - but I'm but any reson we're going to do
140:12 - this with lots and lots of data so one
140:14 - thing that's actually optimal here is
140:15 - not to steer the full amount all the way
140:18 - according to the error but just some
140:20 - percentage of the way and that
140:21 - percentage is referred to it's a key
140:23 - concept here and it's called where's a
140:26 - learning rate so what I would actually
140:28 - do here is say that Delta wait what this
140:32 - plus one here is the error times the
140:35 - input over x a
140:39 - turning rate so that's a key concept
140:43 - here so let's add that into our code if
140:47 - I come back over here I'm at the
140:49 - perceptron is going to have a variable
140:51 - call it LR for learning rate I'm just
140:53 - going to say zero point one so now I'm
140:55 - going to say also x learning rate so
140:59 - there we go
141:00 - now this is going to adjust all of the
141:02 - weights so we should be able to if I go
141:07 - back to here I should be able to now
141:10 - train the perceptron so let's go here
141:17 - and say for all of the points oh I did
141:24 - something terrible I'm doing some awful
141:26 - stuff here
141:27 - which is that I used P for perceptron
141:29 - and then I'm using a local variable P
141:31 - for points so let's call this a PT for
141:33 - point let's actually just call this I'm
141:35 - going to call perceptron I'm going to
141:38 - call this like the brain it's probably
141:41 - like a bad variable name but at least
141:43 - till it says something more than P so
141:47 - for every point what I want to do is I
141:50 - want to say brain train the point the
141:57 - inputs associated with that point and
142:00 - the target pass in the target associated
142:04 - with that point right the point oh it
142:08 - has an X and a Y and a label so ah so
142:13 - the target is actually the label and can
142:17 - I do this I want to make something
142:19 - called inputs which is an array which
142:25 - just has point X and point out why is
142:28 - Java going to let me write that I think
142:33 - so so this is what I want to do I want
142:35 - to train the perceptron I want to send
142:38 - in every point as inputs so X the X and
142:42 - the y make the zero on the one input and
142:44 - then I want to send it into the train
142:46 - function with the label which is the
142:48 - known answer so if I do that
142:53 - okay so in theory it's trading and it's
142:55 - doing this I can't see anything so now I
142:57 - need some sort of way of tracking how
143:01 - well it's doing this is going to be
143:03 - tricky but I have an idea for how to do
143:05 - this so I think what I want to do
143:09 - pause edit for a second just see Steve
143:13 - hey
143:29 - so many the chat just pointed out that
143:32 - you know why am i doing the why am i
143:36 - doing the same loop twice yeah that's
143:38 - what kind of unnecessary here but I'm
143:39 - really just trying to separate out
143:40 - different parts of the code ultimately I
143:43 - probably don't need this original loop
143:44 - but so there's a couple things I do
143:47 - one is I could actually calculate the
143:49 - overall error like I could actually look
143:51 - and this wouldn't be such a bad idea and
143:52 - just sort of print that out I could I
143:54 - could calculate the total squared error
143:57 - and kind of evaluate how well it's doing
143:59 - but I want to just actually look at it
144:00 - visually for a second so I think what I
144:03 - want to do is let me see here what's a
144:09 - good way so I'm going to just actually
144:10 - say guess equals inputs a brain guess
144:16 - inputs and then I'm going to say if
144:20 - guess equals a point label let's just
144:26 - call this let's say target equals points
144:29 - points label just to have these in
144:33 - separate variables if guess equals
144:37 - target what I'm going to do is I'm going
144:41 - to draw a I'm going to say I'm going to
144:45 - draw something that's green I'm going to
144:49 - say no stroke I'm going to draw an
144:52 - ellipse at X like I just keep typing p5
144:58 - why that's like a smaller size else fill
145:08 - red well everything became green
145:15 - instantly I can't be right
145:24 - all right let's not train it okay
145:33 - so we can see I guess it's just working
145:36 - better than I had imagined
145:42 - I'm trying to think visually if there's
145:44 - a better this is like okay so let me let
145:49 - me make the window bigger and let me
145:51 - make all of the points let me make all
145:55 - the points much bigger so this is a
145:58 - little easier to see and let's run this
146:04 - again okay
146:07 - so you can see without any training
146:10 - everything is wrong so I'm not if I add
146:14 - the train function back in everything is
146:20 - correct now I guess it just like this is
146:22 - such a simple scenario it's like trained
146:26 - and worked so felt so quickly in like a
146:29 - matter of like one or two iterations so
146:32 - I think you could probably if you're
146:34 - watching this be a bit more clever in
146:36 - terms of how you how you visualize this
146:41 - and I'm going to say a few more things
146:43 - about this okay so I'm coming towards
146:46 - the end of this particular video and I
146:49 - think I need to do I'm going to give you
146:50 - some exercises and I think I'm going to
146:52 - do a follow-up one that kind of add I'm
146:55 - gonna give you some exercises and then
146:57 - I'm going to come back and do another
146:58 - video where I add this these answers in
147:00 - but there's a couple things here what is
147:02 - I've got a serious problem a very
147:05 - serious problem with everything that
147:07 - I've done here let's just say I'm going
147:10 - to let's say that this space that I'm in
147:19 - is actually this zero zero forget about
147:25 - pixel space or anything there's a
147:27 - coordinate space where zero zero is in
147:29 - the middle right here and I want to
147:32 - categorize data as above or below this
147:35 - line or maybe I you know I want to use
147:39 - the same system but I want to categorize
147:45 - above or below this other line so I want
147:48 - to use a perceptron to categorize the
147:50 - same exact code to categorize both of
147:52 - these two scenarios well in one case in
147:55 - the orange case 0-0 should be a plus 1
147:58 - in the black case 0-0 should be a
148:02 - negative 1 but notice something here if
148:05 - I'm actually feeding in 0 0 into this
148:09 - perceptron system no matter what the
148:13 - weights are the only thing I can ever
148:16 - get out of here is a 0 this is a problem
148:21 - because sometimes 0 0 is going to be
148:24 - above or below so it's going to be a
148:25 - Class A so this would be a Class B that
148:27 - can't possibly be right and this is
148:30 - where this idea of a third I'm going to
148:33 - go and get another color here this
148:36 - perceptron will actually not work or
148:38 - perform correctly with this Genet
148:41 - generic scenario other than with having
148:45 - something called a bias so I need
148:51 - another input a third input into the
148:56 - perceptron that is always going to be a
149:00 - 1 input 0 is X input 1 is y and the bias
149:07 - is 1 actually if you think about this
149:09 - this is really I'm really working
149:11 - through the same problem that's from my
149:14 - linear regression with gradient descent
149:15 - videos or what I'm trying to do is solve
149:19 - the Funt formula
149:21 - I'm trying your neural networks are
149:24 - designed to generalize a photic function
149:27 - to solve a function and in this case
149:30 - this very simplistic scenario is
149:32 - actually just looking to solve the
149:34 - formula for this line y equals MX plus B
149:38 - and M being the slope is kind of like
149:40 - rise over run and B being the
149:43 - y-intercept so this weight the bias
149:47 - weight is really there to solve that
149:51 - y-intercept and these weights are really
149:54 - solving the slope the rise over run
149:57 - so that's what we're doing we're
149:59 - essentially doing linear regression with
150:01 - gradient descent again but just through
150:03 - this perceptron model so if you didn't
150:07 - watch those other videos you could go
150:08 - back and watch them just kind of connect
150:10 - all these systems so what I want you to
150:13 - do an edit point here for a second I'm
150:24 - just going to have this open okay is I
150:32 - have an example here from the nature of
150:35 - codebook this is essentially the same
150:39 - code that I've been writing but it does
150:41 - two different things
150:42 - one is it visualizes what the brain
150:47 - thinks the current line is and also it
150:52 - adds that bias so if you're watching
150:55 - this video I'm going to release the code
150:57 - for this video this I guess is part one
150:59 - of the perceptron coding challenge if
151:01 - you're watching this video you could go
151:03 - right to the next one and I'm going to
151:04 - add the bias in and maybe do a bit more
151:06 - with sort of visualizing what's going on
151:08 - here and have a generic formula for a
151:10 - line but what I might suggest you do is
151:14 - see if you can add those things yourself
151:16 - to this particular perceptron and then
151:18 - when I get to the end of the next video
151:19 - I'm going to talk about why this is such
151:21 - a limited system that can barely
151:22 - actually do anything meaningful in
151:24 - machine learning but it can be a
151:27 - building block for a much more complex
151:29 - system that can do a lot more
151:30 - interesting and powerful things so I
151:32 - hope you've got a sense of what a
151:33 - perceptron is what the algorithm for how
151:36 - a perceptron works is and how the
151:39 - feed-forward supervised learning
151:40 - training process of a model perceptron
151:44 - works because this exact building block
151:47 - this scenario is what I'm going to use
151:50 - in the future videos where we start to
151:52 - build more complex neural networks okay
151:54 - thanks for watching and I look forward
151:55 - to your feedback and thoughts in the
151:57 - comments
152:00 - um okay I am going to so I have just
152:08 - kind of like cut that short a little bit
152:10 - because I have to go in about five
152:12 - minutes and you know I sort of felt like
152:14 - that video was getting a little bit long
152:15 - anyway so hopefully that was like I feel
152:22 - like there's some missing pieces here
152:23 - I'm going to come back next week and
152:24 - finish that off it does anyone have any
152:27 - questions or Corrections or things you
152:28 - want to mention I would love to hear
152:33 - them
152:37 - I'm really looking forward to getting
152:39 - out of this machine learning stuff but I
152:40 - am hopefully I'm doing an okay job I'm
152:48 - kind of amazed that this worked and oh
152:52 - yeah oh me I am so me you said try
152:56 - training on mouse click I see what you
152:59 - mean
153:00 - so like run the training once that's a
153:02 - very good so yes I'll hold on let me try
153:10 - that I like this idea so I'm going to
153:15 - put all of this here I should have done
153:18 - this in the video and I'm going to say
153:21 - void a mouse pressed this should just be
153:27 - doing the training but I still need to
153:33 - do this
153:38 - so this should be right now if I do this
153:42 - this is its guess for everything and
153:48 - then if I click train train train oh
153:51 - yeah look at that oh can I go back you
153:54 - think we can insert this match into the
153:56 - middle somewhere that's a really it was
153:58 - such a good suggestion I'm gonna try
154:02 - this might be impossible to do but that
154:06 - was so useful so okay I'm just going to
154:14 - record this and this could get inserted
154:15 - in that'll be wonderful but I don't know
154:18 - if that'll work and I really got to go
154:19 - soon okay me I am so me and the chat had
154:24 - an excellent suggestion where maybe what
154:26 - I should do is actually uh sale me I am
154:36 - so me in the chat had an excellent
154:37 - suggestion which I could demonstrate the
154:39 - training process with a mouse click so
154:41 - I'm going to quickly do that so what I'm
154:43 - actually going to do is I'm going to
154:44 - take this training out here I'm going to
154:47 - take this whole loop and just delete
154:51 - this line of code that actually does the
154:53 - training and I'm going to run it so now
154:55 - it's not training and you can see it's
154:57 - got every time it you know when I run
155:00 - this it's all wrong
155:02 - right but what I can do now is I can say
155:04 - void mousepressed and I can just run the
155:14 - training algorithm so now what I did is
155:16 - only when I click the mouse is it going
155:18 - to run through all the data and actually
155:19 - adjust the weights so now if I run this
155:22 - you can see you can see look at this
155:23 - it's got most wrong but it's got some
155:27 - right it's at most right but some wrong
155:29 - and now if I click the mouse ah a few
155:32 - more correct but some more wrong click
155:34 - the mouse again okay click the mouse
155:35 - again click the mouse again click the
155:37 - mouse again so you can see it's like
155:39 - changing and eventually now it's got
155:40 - everything right and so you can see how
155:43 - that learning process happens over time
155:45 - it only took five or six cycles and
155:48 - maybe this will get that will be the end
155:52 - will gets
155:52 - since amount so I you know there's I
155:54 - think there's a lot more creative ways
155:56 - you could visualize the training you
155:58 - could also visualize the perceptron
155:59 - itself and we can visualize the weights
156:02 - I guess I'm going to come back to this
156:05 - in the that will only make it take a
156:10 - while it's not animating while learns he
156:12 - need to only train a few points at a
156:13 - time oh yeah I could also just train
156:16 - with one point at a time I want to add
156:21 - that in so many good suggestions uh
156:30 - they're all so I can't see who that was
156:32 - in the YouTube chat one other thing that
156:34 - I could do is just train one point at a
156:36 - time so I'm going to draw everything but
156:39 - I could here I could say I could say
156:43 - into training index equals zero so what
156:47 - I'm going to do in draw now is I'm going
156:50 - to say point training equals points
156:55 - training index so I'm just going to take
156:59 - one point and train it off that one
157:03 - point okay and then what I'm going to do
157:08 - is I'm going to say training index plus
157:12 - plus and if training training index
157:16 - equals points length like if I get to
157:19 - the end of the array I will just reset
157:22 - training index back equal to zero so
157:25 - this now in the draw loop what did I get
157:28 - wrong here I'm just spawn training wrong
157:29 - Oh double equal sorry so now in the draw
157:33 - loop it should be training one point at
157:35 - a time and you can see like it's kind of
157:38 - weird all sorts of weird flashy stuff is
157:40 - happening but over time it should
157:42 - eventually settle into at getting
157:44 - everything correct but as it makes these
157:46 - little adjustments it's going to get
157:47 - some things wrong and some things right
157:49 - and you can see now so again there's so
157:51 - many other ways you could think about
157:52 - visualizing the training and you could
157:53 - actually visualize you know one thing
157:55 - you might try is actually visualize the
157:56 - perceptron itself like visualize the
157:58 - weights visualize the connection
158:00 - visualize the data flowing through it so
158:02 - I'll leave that to you creative people
158:03 - watching this video
158:07 - okay so that I've really made the most
158:11 - complicated editing puzzle ever here
158:13 - I've got to go I am going to take I'm
158:15 - going to play my goodbye song I'll see
158:20 - if I can or two questions in the chat
158:22 - thank you guys for thank you guys for
158:26 - tolerating this journey until like some
158:30 - of these esoteric and very highly
158:32 - technical and not so practical make a
158:33 - line object
158:35 - Simon Tiger mentions as well that's a
158:37 - cricket idea so I'm going to leave the
158:39 - more creative ways of doing this stuff
158:42 - to to you guys to all of you watching
158:48 - I've got a run I'm gonna be back next
158:50 - Friday
158:51 - unfortunately I want to be able to try
158:52 - to do additional live streams get
158:54 - further along also take breaks from the
158:57 - machine learning stuff and do some other
158:58 - creative coding challenges challenges
159:00 - but so hopefully I'll another week I'll
159:05 - be able to but I I'm almost sure I won't
159:07 - be back till next Friday so next Friday
159:10 - maybe I'll try to start getting into the
159:13 - neural network stuff maybe I'll just
159:14 - take a break and do some other game
159:16 - coding challenge we will see I'm gonna
159:19 - see if there's any questions in the chat
159:20 - that I can answer somebody posted
159:25 - volcano emoji which I don't know what
159:27 - that means
159:28 - poof what's the next episode I don't
159:33 - know to be determined
159:34 - I'm not feeling super - perceptron to
159:39 - calculate the optimal edit points that's
159:40 - a great suggestion
159:41 - I'm not feeling I'm you know I'm
159:45 - definitely like questioning everything
159:47 - I've ever done in my entire life until
159:49 - now what we're good for everything I'm
159:50 - doing good or useful or helpful but
159:54 - doing them but the salsa musics need the
159:57 - salsa music I don't know if that's
159:58 - really salsa music that music kind of
160:00 - thought I was playing is by I'm Adam Lau
160:02 - who is a film and television composer
160:03 - based in LA who is actually working on a
160:05 - new coding train theme song that will be
160:07 - coming
160:08 - I'm sorry shoe doctor this archive will
160:11 - be available as soon as I hit stop of
160:13 - the livestream you can go back and watch
160:14 - the whole thing I've got to be got to be
160:18 - somewhere by 6 o'clock and it's going to
160:20 - take me some time to unplug everything
160:22 - and do everything and get get out of
160:24 - here so uh so um thank you all again do
160:36 - I look I don't think I look frazzled
160:38 - enough I should go over here and you get
160:42 - this oops this is this is how I feel
160:47 - right now
160:51 - good screenshot that or get fit or
160:54 - whatever I'm a little bit weird how
160:56 - obsessed I am with the vanity of people
160:58 - making gifts but um we're am I going I
161:02 - have to pick up my daughter from a
161:03 - playdate so that's where I'm going
161:07 - ah so thanks thanks about the songs over
161:11 - I've got to go I'll be back next week
161:13 - please send me your questions and
161:15 - comments and feedback in in comments in
161:19 - on Twitter at Schiffman
161:21 - I appreciate it if you can encourage if
161:23 - you could subscribe and like and all
161:25 - that stuff and watch and encourage other
161:26 - people to watch all that stuff helps me
161:28 - out the more people find the channel the
161:30 - more watch time I have all that stuff
161:31 - helps so spread the word if you like the
161:35 - stuff if you don't - please don't feel
161:36 - don't feel anything - of course the only
161:38 - genuine whatever I have really got to go
161:40 - thank you guys and I should get one of
161:44 - Suraj's videos and go slowly over it
161:46 - step by step that's a good idea
161:48 - okay so I will see you guys later
161:51 - good bye