00:00 - all right welcome to tend to flow dais
00:04 - the layers API tutorial part 2 so
00:07 - previous previously on its flow test
00:10 - layers API part 1 I created this model
00:13 - using TF dot sequential TF layers and
00:16 - model dot compile with with a training
00:19 - without with a loss function and an
00:21 - optimizer now let's just review that
00:25 - really quickly before I go on to the
00:26 - next step which is looking at fit and
00:28 - predict so I'm gonna add some comments
00:30 - thank you to the chat just suggested
00:32 - that so this is the model create the
00:38 - hidden layer add the layer and the
00:47 - hidden layer has a number of nodes input
00:55 - shape and an activation function which i
01:00 - think is probably pretty
01:01 - self-explanatory
01:02 - then the output layer is a dense layer
01:04 - so dense is a fully connected layer then
01:12 - the out create an output layer create
01:15 - another layer and here I'm gonna write
01:21 - here the input shape is inferred from
01:25 - the previous layer then an optimizer
01:31 - using gradient descent I must have a
01:36 - video somewhere that talks about what
01:38 - gradient descent is if that's not
01:39 - familiar to you and then I'm done
01:43 - configuring the model so compile it I
01:47 - don't know if this tutorial should be
01:49 - clear to be writing all these comments
01:50 - but it did that's where we are
01:52 - so enum standard are sort of classic
01:56 - machine learning process I would
01:57 - configure my model well actually before
02:00 - I do any of this I should have like
02:02 - collected my data I'd been really
02:03 - thoughtful about that and thought about
02:06 - the ethics of it and why am I doing this
02:07 - saying in the first place is this gonna
02:09 - help people or hurt people I should have
02:11 - been doing all of that
02:12 - but here I'm just looking at potential
02:14 - endogenous it's an API so I I'm kind of
02:16 - skipping those really fundamental the
02:18 - most important parts doing things
02:20 - backwards in a way and I'm just gonna
02:22 - make up data so what I want to do is
02:25 - there's two things one is I want to
02:26 - train the model I want it to adjust all
02:28 - of its weights to fit my training data I
02:31 - have these inputs with these known
02:32 - outputs maybe I'm doing image
02:33 - classification of all these labeled
02:35 - images cats dogs Turtles
02:37 - and I want the model to output cats dogs
02:40 - or Charles some probability value is
02:42 - based on which things that thinks they
02:43 - are and then so that's what this fit
02:46 - function does I could also ask it to do
02:48 - predict which means just take this data
02:50 - I don't know what it is this is not part
02:52 - of my training data and just give me the
02:55 - output so let's do something weird I
02:57 - mean weirder then of what I'm already
02:59 - doing which is talking to myself in a
03:01 - room with the camera and some lights I
03:03 - have like an iPad weird stupid sound
03:05 - effects on it I don't know what's what's
03:07 - happened to me in my life anyway let's
03:12 - let's run predictive not having trained
03:14 - the model will it actually just does it
03:15 - what does it start with it must have a
03:17 - whole bunch of randomly configured
03:18 - weights right must be configure itself
03:20 - randomly let's see if we can get some
03:23 - some output so what do I need to do to
03:26 - call predict let's go look at the API
03:28 - and let's look at again TF sequential so
03:32 - I'm looking for the predict function now
03:33 - here's the thing the pretty odd the
03:35 - predict function is there so it
03:37 - generates output predictions for the
03:38 - input samples model dot predict
03:42 - okay so config there are some
03:44 - configuration options like batch size
03:48 - and how verbose I want it to be but
03:50 - really all I need are the exits the X's
03:52 - are the inputs and I need two of them so
03:55 - what I'm going to do is I'm going to
03:56 - create a tensor I am going to say let
04:01 - input are obvious a Const inputs equals
04:05 - TF tensor one D and let's just give it
04:09 - some numbers like 0.5 0.25 sorry
04:16 - 0.25 0.93 it's a 2 so I just made up
04:22 - some inputs these and look all fake not
04:24 - real data at all I'm just trying to
04:26 - how tensorflow digestible layers API
04:28 - works so now I should be able to say
04:31 - model dot predict inputs now let's go
04:37 - back and look at this model dot predict
04:41 - TF one so interesting dip dip guess I'm
04:48 - good okay so hold on so let's say let
04:51 - outputs equal model dot predict inputs
04:54 - and outputs dot print I don't know could
05:00 - it really be as simple as that
05:01 - let's see
05:04 - and so now I'm gonna go back here
05:07 - uncaught expected dense input to have
05:10 - two dimensions but got array with shape
05:13 - too oh boy I think I might have a big
05:16 - major mistake here alright so what is
05:18 - this error hope oh boy this is the kind
05:22 - of error you're gonna get a lot which is
05:24 - something is wrong with my like shape
05:26 - shape errors error when checking
05:28 - expected dance tense want input to have
05:30 - two dimensions but got array was shaped
05:32 - to what is wrong here I mean after all
05:35 - there are just two inputs I said very
05:40 - specifically that the input shape is one
05:44 - dimensional with two things in it so why
05:46 - is this wrong well it turns out I forgot
05:48 - about this idea of batching so it is
05:51 - quite uncommon but it's possible that
05:54 - you just want to send in a single data
05:56 - point like these two numbers in and get
05:59 - the prediction and so even though this
06:01 - is the array of the inputs that needs to
06:05 - live in an array itself because I might
06:07 - want to send in multiple sets of them
06:10 - and so actually the correct way for me
06:12 - to put this here is to actually have
06:14 - this be the first element in an array
06:17 - and actually this is not a 1d tensor
06:19 - then it's a 2d tensor so now if I run
06:23 - this we should see there we go this
06:25 - these are the outputs and we can see
06:28 - here every time I run it I'm gonna get
06:30 - different outputs because this
06:32 - particular model is initialized randomly
06:35 - now there's probably a way with the
06:37 - layers API
06:38 - I could configure how the weights are
06:39 - initial initialize but it's using some
06:41 - default probably random maybe it's a
06:43 - normalized distribution of random
06:45 - numbers who knows we could look up the
06:47 - documentation and somebody in the
06:48 - comments will tell me okay so but what's
06:51 - interesting about this is now right I
06:56 - could do I could have right I could now
07:07 - send in four inputs and what will I get
07:10 - out all of those results so these are
07:14 - the three output values for the first
07:17 - input the second input at third fourth
07:18 - so this is how the predict function
07:20 - works so I can create a model and I can
07:22 - start predictions now they're useless
07:24 - and pointless and random without me
07:25 - actually trading that model so that's
07:28 - what I need to do next
07:29 - fit so let's now create a scenario where
07:33 - we have some training data right this is
07:35 - my and and by the way there is this
07:37 - really important piece of working with
07:40 - machine learning where you have both
07:41 - training data testing data you can even
07:44 - have something called validation data
07:45 - and then you have the new data the stuff
07:48 - that you're making guesses and
07:49 - predictions with so I'm not getting that
07:50 - far into it yet but let's just in this
07:53 - case I'm just gonna have some training
07:54 - data I'm not gonna have any testing data
07:56 - although we'll see I mean we're gonna
07:59 - look at fit we're gonna see see how this
08:00 - all works fit here we go so now I want
08:04 - to look at fit so here I look at this oh
08:08 - wait oh wait oh wait oh wait oh wait oh
08:13 - wait it just made the video tutorials
08:17 - about the weight so one of the reasons
08:19 - why so working with tensorflow digests
08:21 - natively you're one gonna feel somewhat
08:24 - comfortable with the idea of a
08:25 - JavaScript promise and these new es 8
08:28 - keywords a weight and a sink so I will
08:33 - I'm gonna use those concepts you might
08:35 - want to check my promises playlist if
08:36 - that's new to you okay
08:40 - so let's figure out how we're gonna do
08:41 - this so model dot fit the parameters are
08:45 - X's X and y so here's the thing unlike
08:49 - here and I really should call these X's
08:51 - and this is really the wise right this
08:55 - is really what I'm doing these are the
08:57 - exes and now I'm getting the wise to fit
08:59 - I want to do the same thing the
09:02 - difference is I'm gonna have some known
09:04 - outputs so in this case I might have
09:07 - like this is my training data these are
09:10 - the X's and now the Y's are I need to
09:19 - make you know dreamily this would come
09:21 - from a spreadsheet or some type of
09:22 - actual database that I fossil Eagle I
09:25 - thought about how it to collect and the
09:27 - data and what I'm doing with it but
09:29 - right now I'm just making up dummy data
09:31 - so each one of these I probably just put
09:34 - in random numbers like weirdly sort of
09:39 - lazy about this I like to you just sort
09:41 - of see it so let's just pretend my
09:42 - training dataset just has instead of
09:45 - four let's just have three things in it
09:46 - and let's let's just make some arbitrary
09:55 - okay so this is now my training data I
09:58 - have the X's right the X's are there are
10:04 - only two of them again if I were doing
10:05 - on my like image classification example
10:08 - that I did previously in a in the with a
10:10 - toy narrow Network library I might have
10:12 - 784 inputs for seven or 84 pixels but
10:15 - here in my made-up scenario there's two
10:16 - inputs and there's three outputs so the
10:19 - X's have two and the Y's have three and
10:23 - you can see that reflected here so now I
10:26 - should be saying model dot fit the XS
10:30 - and the Y's let's look here now here's
10:37 - the thing there are some there's this
10:40 - variable called history that's kind of
10:42 - interesting so let's say Const history
10:47 - equals model dot fit and I got to get
10:49 - into the weight and all that in a second
10:50 - so what the history is is that's an
10:53 - object that's returned that has lots of
10:56 - information about how the training is
10:58 - going like how accurate are things what
11:00 - happened there if I want to start like
11:01 - looking at the properties of the
11:03 - training what's the current
11:04 - that type of thing oh right these
11:08 - mismatched thank you so I just started
11:10 - about three and three these have to
11:11 - match thank you to the chat for
11:15 - mentioning that to me okay so then I
11:17 - have this idea back
11:19 - sighs so batch size is let's look at
11:24 - that number of samples per gradient
11:26 - update if unspecified it will default to
11:28 - 32 so this has to do with the inner
11:31 - workings of how the gradient descent
11:33 - algorithm works right at some point
11:35 - gradient descent is going to look at the
11:37 - error and it's going to make all these
11:40 - adjustments to the weights and so does
11:42 - it does do that well does it do that
11:44 - after ten data points after 20 after
11:46 - thirty two so I'm gonna ignore that and
11:49 - then epochs or epochs or I could never
11:53 - know how to know what to know how to
11:59 - pronounce that word is the number of
12:02 - times to iterate over the training data
12:04 - arrays it's optional I think the default
12:06 - is one so here's the thing I'm actually
12:07 - just gonna let this go without setting
12:10 - any of those those things are going to
12:12 - be certainly important hopefully as I
12:14 - get into future examples or as you have
12:16 - specific scenarios but basically I could
12:19 - add an object here that has things like
12:23 - that has various configuration
12:25 - properties and this would need to be a
12:26 - comma here but I'm going to skip that
12:29 - right now
12:29 - because I believe according to
12:32 - documentation config is completely
12:35 - optional so let's just run this and then
12:37 - ever have to start talking about the
12:38 - asynchronous nature of this alright what
12:45 - happens if I say console dot log history
12:50 - let's just look at that so look at that
12:54 - I got a promise oh it's promising me
12:57 - something so what this means is fit is a
13:00 - function that executes asynchronously
13:03 - now it's possible I can do things with a
13:06 - kind of older style of JavaScript using
13:08 - callbacks because it looks like in the
13:10 - documentation here one of the one of the
13:14 - options I can specify as a callback but
13:17 - I'm going to use promises so what I'm
13:19 - gonna do is I'm gonna say model dot fit
13:21 - then and what I want to look at is I
13:29 - don't know what I'm gonna just sort of
13:31 - see and in this case I think actually
13:35 - the way that I'm doing it I'm not I'm
13:37 - gonna look at what's sent into the
13:39 - promise so this should now this should
13:45 - be a way this is if it returns a promise
13:47 - I can figure out I can get the result of
13:51 - how its what it's done as an argument to
13:53 - a function that's executed when the
13:55 - promise is resolved so this is how this
13:58 - could look just to look at that history
13:59 - and you'll want to look at my promises
14:02 - videos if this syntax doesn't make sense
14:04 - to you and we can see here looks like oh
14:08 - I have a history object and I have a
14:09 - loss there we go so this is actually the
14:12 - response and what I want to look at is
14:16 - the response history dot loss index zero
14:27 - so looking at this there we go now what
14:32 - happens if I give it a lot of now what
14:36 - happens if I start to add in a
14:37 - configuration like because I'm really
14:40 - curious what happens if I say so let me
14:44 - let me actually use a variable call
14:49 - obviously the call it config and I'm
14:51 - gonna say it two things I want to add is
14:52 - I want to say verbose is true and I want
14:56 - to say epi pox is five so because I
15:01 - don't see if it's verbose am I gonna get
15:02 - a lot of stuff in the console that's
15:04 - gonna tell me about what's going on so I
15:05 - want to add in some of those parameters
15:09 - verbose mode is not implemented yet
15:11 - oh okay so I guess I can't use it for
15:16 - Bost mode but I can add five and so this
15:20 - is just the loss after five let's add
15:25 - 100
15:28 - Oh point to negative point to alright
15:33 - actually something weird was bothering
15:34 - me there for a second which is why do I
15:36 - have a negative loss like you can't have
15:37 - a negative mean squared error right mean
15:40 - squared error has to be positive it's
15:42 - the difference squared averaged over all
15:47 - the data points so I forgot that I still
15:50 - had cosine distance so let me put it
15:52 - back to mean squared error and now we
15:57 - can see that I'm getting something that
16:00 - looks like it could be mean squared
16:01 - error yes so one thing about this okay
16:04 - so I'm gonna get rid of this config and
16:07 - but what I really want to do is I want
16:09 - to see the lost values over time like as
16:13 - I'm fitting the model I'm running the
16:14 - training data in multiple times maybe
16:16 - I'm shuffling the order there's all
16:18 - sorts of things that I've done in
16:19 - previous videos that I want to do with
16:21 - tensorflow da Chasse
16:22 - so how do I have this model dot fit
16:24 - multiple times I mean I could just call
16:26 - it twice right
16:27 - ooh oh it did not like that whoa
16:32 - fascinating so I can't do that I don't
16:34 - know what's wrong with that but that's
16:35 - not really what I want to do anyway I
16:38 - could try to add a loop by the way the
16:41 - chat is really upset that I have these
16:42 - extra commas here so I'm gonna remove
16:44 - them JavaScript doesn't care it's like
16:46 - put your commas wherever you want like I
16:47 - have a lot of commas right oh no no no
16:51 - it doesn't want extra commas because it
16:52 - has blank entries but okay so so what if
16:56 - what if I were to put a loop here say
16:59 - like oh I want to do this two times so I
17:04 - want to fit the model twice again I got
17:09 - an error oh it really doesn't like this
17:11 - so here's the thing I I'm not exactly
17:13 - sure what this error is but uh maybe
17:16 - I'll somebody will tell me in the
17:17 - comments but I was going down the road I
17:18 - didn't want to go down this is where
17:20 - using wherever it was here in the
17:26 - tensorflow dot yes
17:29 - the a weight keyword is crucial so what
17:33 - I want to be able to do is I want to be
17:35 - able to call this fit inside a loop but
17:39 - promises just like everything's happen
17:41 - a synchronously because it's very hard
17:43 - to follow so actually what I really want
17:46 - to do is I want to go back to that
17:48 - syntax of saying Const history equals
17:53 - model dot fit and then this is what I
17:59 - want to do I want to say then console
18:02 - dot log history plus okay this is what I
18:07 - want this is like think Rinna's code
18:10 - blocking code this is what I want I want
18:12 - to fit the model and then see the
18:13 - results
18:14 - it won't do this because that's not how
18:16 - JavaScript works it works asynchronously
18:18 - so there is the await keyword which is
18:20 - new and ESA which says hey wait for this
18:22 - and then to the next line of code right
18:24 - shouldn't that work whoops and a weight
18:28 - is only valid in an async function so
18:30 - again if you watch my async and await
18:32 - tutorials you would know this but I
18:34 - cannot just do this anywhere in my code
18:36 - I have to create a function a function
18:40 - with the keyword async I'm gonna call it
18:43 - train and I have to do this inside that
18:46 - so this is now the correct syntax this
18:51 - actually is valid but it's in an
18:53 - asynchronous function I have to call
18:54 - that function so I could just call train
18:56 - let's just run this now and oh cannot
19:02 - reap repartee zero of undefined and
19:04 - train so what do I have wrong let's put
19:09 - the X's and Y's in that function
19:19 - well that's not the problem
19:22 - history let's just let's just look at
19:25 - the history
19:26 - oh yeah it's their history Oh dot
19:29 - history right I forget you forgetting
19:31 - this is the response so it was fine
19:34 - actually
19:35 - history dot loss index zero so I just
19:40 - want to look at just the loss so these
19:42 - don't need to be in here these I don't
19:43 - want these in here so now we run this
19:47 - again there's the loss now here's the
19:51 - thing I also just want I'm just gonna
19:53 - put a van in here because I also want to
19:59 - have some sort of event for when the
20:01 - training is complete
20:03 - so this function now kind of magically
20:08 - returns a promise because of the way
20:10 - that await and async work so I want to
20:13 - make sure this is working great and now
20:14 - training is complete now guess what I
20:16 - can now do this I can do this ten times
20:25 - so I can now the loop can go in here
20:28 - because of the beauty of this await
20:30 - syntax this is now work functions as if
20:33 - it's blocking code when it's all
20:35 - complete it will return a promise there
20:39 - we go we can see this is what we should
20:41 - have the loss should be going down right
20:43 - let's change that learning rate
20:45 - somewhere I set up a learning rate let's
20:49 - make the learning rate 0.5 you can see
20:54 - the loss is going down and maybe let's
20:57 - go back to 21
20:58 - let's give it like a thousand basically
21:01 - a thousand iterations and we can see the
21:05 - loss is going down over time so and now
21:08 - what I could do I mean a thousand is a
21:12 - lot the training is now complete now
21:14 - what I could do is I can call remember
21:17 - this I can now call predict now again
21:23 - I'm only working with one data set my
21:26 - training data is my testing data is my
21:28 - validation data is
21:30 - by out by regular data that I'm using in
21:32 - the future once I'm done training the
21:33 - model so but we now see the full process
21:37 - I have X's and Y's I can call model dot
21:41 - fit again this is really hairy stuff not
21:43 - for the faint of heart I'm in the weeds
21:44 - of those sort of like lower-level
21:46 - tensorflow dot j s stuff I mean even
21:48 - though I'm in the layers API but using
21:50 - ES 8 syntax but now we should be able to
21:53 - see let's take a look at
21:57 - oh and it's I'm gonna have an issue here
21:59 - where I have these Y's so I'm gonna call
22:02 - this I'm gonna call this outputs so
22:06 - let's take a look let's train it just a
22:07 - hundred times whoops oh whoa
22:12 - silly me look what happened sighs we
22:14 - totally forgot about the asynchronous
22:15 - nature of all this stuff and I got the
22:17 - predictions before the training finished
22:19 - right because this is happening
22:21 - asynchronously which means what I want
22:25 - to do is after the training is complete
22:28 - then then I want to do by prediction so
22:33 - I want to Train using the asynchronous
22:34 - training a hundred times
22:36 - then I want to do my prediction all
22:39 - right so this is running we can see that
22:40 - the loss is going down it's training
22:42 - over time and when it gets to the end
22:45 - I've got the results of my predictions
22:49 - now you can see these don't look very
22:51 - good like first of all these don't
22:55 - resemble these outputs at all the main
22:57 - issue here is that nothing here makes
22:59 - sense I just have the skeleton of the
23:01 - story of data I can fit I can create the
23:06 - model I can fit it with some data and I
23:08 - can ask it to run a prediction with that
23:09 - same data but I'm ignoring like really
23:12 - important considerations and you can see
23:13 - that machine learning is just magic it
23:14 - doesn't just do the right I don't care
23:16 - that was the right answer so let me at
23:18 - least before I go add a couple things to
23:21 - so that we can actually see that it got
23:22 - somewhere so one thing that I want to do
23:24 - is I'm gonna just simplify what's going
23:27 - on here I'm going to make this data
23:30 - something really obvious like zero zero
23:33 - one one point five point five that's my
23:38 - X's and then my Y's
23:40 - I'm gonna I want to get I want to get
23:42 - one I'm gonna just change that to just
23:44 - output 0.5 and 0 whoops what I miss here
23:54 - yeah
23:55 - oh boy
24:00 - syntax there we go so what I'm doing
24:02 - here is I'm just trying to like come up
24:05 - with some ridiculous scheme that's like
24:06 - super simple to learn like they're sort
24:08 - of like an inverse linear relationship
24:10 - here zero goes to one it's like the map
24:12 - basically let's see if there's a neural
24:13 - network can learn the map function but
24:15 - and why using two numbers and one
24:17 - numbers here but whatever so now I am
24:20 - going to I have to change the output to
24:23 - have just one output and now let me run
24:27 - this so we can see look at this the loss
24:32 - function is still pretty high so there
24:34 - is something that's kind of important
24:35 - here you can see it's going down and
24:38 - actually it like a thousand iterations
24:40 - that it kind of got something it got
24:43 - something pretty close so I would just
24:44 - need to give it more time to train to
24:46 - kind of reproduce the known results but
24:49 - I just want to show a couple important
24:52 - things here one thing that's important
24:53 - is I probably should always shuffle the
24:58 - training data so there is actually a
25:00 - kind of important one of the
25:01 - configuration parameters here for the
25:03 - fit function like if I shuffle is true
25:09 - what this will do is whatever you eat
25:11 - X's and Y's are each time it puts it
25:14 - through the training function it'll do
25:16 - it in a different order and this will
25:17 - really help things as you're training
25:19 - with the same data over and over and
25:21 - over again so let me add that in and
25:24 - just see what happens there so I'm gonna
25:25 - add this config here to here so this is
25:30 - shuffle - that's the only thing I've
25:31 - turned on doing one epoch at a time
25:34 - still so let's also while I'm here just
25:38 - to make things go a little faster let's
25:40 - add let's do like 10 a pox at a time
25:43 - once again total here a thousand times
25:46 - 10 is 10,000 epochs right but because I
25:50 - want to see it over time I'm not just
25:52 - putting not just doing this once with
25:53 - the number 10,000 there I'm doing it 10
25:56 - I'm doing it a thousand times with ten
25:57 - but this what I think will make things
25:59 - move a little faster and we could say it
26:00 - got even better here and so now we can
26:03 - see this over time we can see that the
26:08 - the the loss is really going quite far
26:11 - down now I'm gonna have to sit here and
26:12 - wait for a minute which I'm gonna do but
26:14 - you're not gonna have to this will video
26:16 - will now speed up alright I'm back so
26:25 - look at this I reproduce those results
26:27 - right at reproduce the same results that
26:29 - my training data produces should have
26:31 - produced and I got the lost down pretty
26:33 - low so you can again completely
26:36 - meaningless trivial nonsense nothing but
26:38 - hope I'm just I'm hopefully this is
26:41 - helper to you it's helpful to me because
26:42 - I'm trying to figure out just how the
26:43 - library actually works itself so let's
26:46 - review for a second okay let's review
26:48 - everything I have shown you now these
26:53 - two videos that I can create an empty
26:56 - model right the diagram of my neural
27:00 - network architecture is an empty model
27:02 - then what I could do is I can create
27:07 - layers there can be dense layers or
27:09 - other kinds of layers that I might look
27:11 - at in another time in the future and I
27:13 - can add them the order that I add them
27:16 - is very important because it is a
27:18 - feed-forward sequential model so this is
27:22 - first in this a second beef you know I'm
27:24 - thinking this is hittin an output but
27:26 - they're really just layer 1 layer 2 ok
27:29 - once I've done that I have to define how
27:33 - what sort of mathematics are going to be
27:37 - used to train the model and there's a
27:40 - loss function mean squared errors what
27:42 - I'm choosing to use and opt an
27:44 - optimization function stochastic
27:46 - gradient descent with a learning rate of
27:47 - 0.1 that's everything from Part 1 and
27:51 - now what I've done here is I've shown
27:52 - you okay if I have some data presumably
27:54 - coming in from a spreadsheet I turn
27:56 - those into tensors I can use the fit
28:00 - function to train the model I can say
28:03 - fit all of the weights with these X's in
28:05 - these Y's once that's complete I can
28:08 - then ask it to predict with presumably
28:10 - new data this skeleton has not used any
28:13 - meaningful data I'm also not really
28:16 - being thoughtful about like well what
28:17 - activation function makes sense or what
28:19 - optimization function makes sense
28:21 - different things work for different
28:22 - scenarios so hopefully we'll see that
28:24 - more in the future as I make more videos
28:27 - I would encourage you to just try to do
28:29 - this baby with some actual data play
28:31 - around with it try different input
28:33 - shapes activation functions play around
28:35 - a little bit see what you get let me
28:37 - know about those results in the comments
28:38 - let me know if this was helpful to you
28:39 - and I will see you next next video I'm
28:42 - gonna do I'm gonna look at that X or a
28:43 - problem again in the context of tensor
28:45 - for about yes okay good bye
28:51 - [Music]