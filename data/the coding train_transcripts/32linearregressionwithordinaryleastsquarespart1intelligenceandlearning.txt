00:00 - hello welcome to another video in my
00:05 - machine learning series and in this
00:07 - video I am going to talk about something
00:09 - called linear regression so I'm just
00:11 - actually going to move straight over to
00:12 - the whiteboard it along here so why are
00:15 - we talking about linear regression so
00:18 - what I'm leaning towards and when I may
00:20 - get to if you keep watching these videos
00:22 - they don't exist yet but I'm going to
00:23 - keep making them so eventually you might
00:24 - keep watching them is I'm going to get
00:26 - to neural networks and neural networks
00:29 - are useful and powerful in the case of
00:32 - large datasets with many many variables
00:35 - many many inputs parameters that we
00:38 - almost can't figure out mathematically
00:39 - how to make sense of it may be a neural
00:41 - network can do that in some almost
00:43 - magical way we're going to get into all
00:45 - the details of that but there are
00:47 - machine learning scenarios where we can
00:49 - actually just calculate precisely using
00:52 - a statistical method the relationship
00:54 - between inputs and outputs right so if
00:56 - we were to review we have this idea of a
00:59 - machine learning recipe previously I
01:03 - looked at K nearest neighbor as a
01:05 - possible algorithm to make sense of
01:08 - input data and predict some sort of
01:09 - output whether it's classifying or
01:11 - predicting a price now type of thing so
01:14 - we have some sort of input we get some
01:18 - sort of output so let's take the
01:21 - simplest scenario of inputs related to
01:25 - outputs and a simple scenario for this
01:28 - would be something like a 2-dimensional
01:30 - data set okay so we could graph using
01:35 - something called a scatter plot a data
01:37 - set and we're going to make the data set
01:39 - there with me here a temperature the
01:46 - x-axis I want to think of as temperature
01:49 - so maybe I'm really sorry I'm going to
01:52 - do this in Fahrenheit to apologize for
01:54 - that I'll happily say max so anytime you
01:57 - want zero degrees to 100 degrees I guess
02:04 - it could be negative so in Fahrenheit
02:06 - and then the y-axis will be ice cream
02:10 - sales and
02:12 - this was suggested in the chat but I'd
02:16 - like to add to this sorbet in quotes for
02:19 - no reason really about the frame about
02:22 - the frame I back ok uh but you can see
02:25 - this right yes sir I just thought if
02:27 - I've got my head in front of it okay so
02:29 - uh yeah just because you know Gary
02:30 - doesn't really agree with me just in
02:32 - case you were wondering okay so I like
02:33 - serving okay so uh we can say like oh
02:36 - when it's you know 24 degrees they're
02:40 - only three ice creams are sold per day
02:43 - and then on another day it was 90
02:46 - degrees and there were 18 and you know
02:49 - in another day there was 90 degrees
02:50 - there were this many and then you know
02:52 - you could imagine if you were the owner
02:55 - the purveyor of an ice cream shop II
02:58 - that you could keep track of your sales
03:01 - as it relates to temperature and then
03:04 - what you could do is that you have all
03:05 - this data now somebody comes into your
03:09 - place of business and says okay tomorrow
03:13 - the weather is going to be 50 degrees
03:15 - what could you like could you make a
03:17 - guess as to how many ice creams you're
03:20 - going to sell and so we could look and
03:22 - say here's you know 50 degrees well
03:25 - there was some other day where I sold
03:26 - this much was 50 some other day some of
03:28 - the day how could we make a prediction
03:30 - well this is a scenario where it appears
03:34 - there is a linear relationship a linear
03:38 - relationship between temperature and
03:41 - sales the higher the temperature the
03:45 - more the sales the lower the temperature
03:47 - the less the fewer the fewer the sales
03:51 - so the idea of a linear regression is to
03:55 - figure out how can we fit a line best
03:59 - fit a line to this data and I could look
04:02 - at this and hold on I'm going to be back
04:05 - in a second for some magic I got
04:07 - business if I waited like a historic
04:09 - moment not really because that's be
04:11 - ridiculous this is the first time I've
04:12 - ever using a marker with a different
04:14 - color YouTube channel maybe suddenly
04:16 - I'll just get so many subscribers
04:18 - because if you're like I heard there's
04:19 - this channel with tutorials in a white
04:22 - board where you
04:23 - colors okay so we could make a guess and
04:26 - I could say like look that looks like a
04:29 - line that kind of fits the data that's
04:33 - me just as a human being kind of
04:35 - eyeballing it so now if I wanted to say
04:38 - you know when the temperature is 95
04:40 - degrees I could just look at 95 degrees
04:44 - find this and find the corresponding you
04:47 - know you know 200 ice creams or whatever
04:50 - sold so this is the idea of linear
04:53 - regression looking at a data set and
04:56 - fitting a line to that data set now how
04:58 - do you do this there are many different
05:00 - methods and we're I'm going to look at
05:03 - multiple methods in different videos in
05:04 - this video I would like to discuss the
05:09 - method called ordinary least-squares
05:13 - remember I'm going to write this down
05:14 - ordinary oh boy I got a little dizzy
05:16 - everything's going to be okay least
05:18 - squares what does that mean okay so if
05:24 - we look at this line we can compare
05:28 - every single this is the line that we've
05:31 - fit to the data you know but as a human
05:33 - being eyeballing it one thing that I can
05:36 - do is I could say look how different is
05:39 - each one of these data points from the
05:42 - line and I could see like I could
05:44 - essentially like look at its distance
05:46 - from the line and the idea of ordinary
05:51 - least squares is the least squares
05:54 - method is we want to find the line that
05:57 - minimizes all of these distances so what
06:01 - if we okay so if we could think of all
06:03 - of these as data points like you know X
06:08 - 0 X 1 X 2 X 3 X X 4 right we could think
06:15 - of all of these distances as like d0 d1
06:20 - d2 d3 d4 so if we took all of these
06:25 - distances and squared them d0 squared
06:29 - plus v1 squared plus D 2 square
06:33 - etc etc etc and added them all up
06:36 - together that's the sum of all the
06:40 - squares of all differences we want to
06:42 - minimize this value so how do we
06:45 - calculate the bill how do we find a line
06:48 - that minimizes all those so you might be
06:50 - asking well why are you squirtle why you
06:52 - squaring the values well this is a
06:54 - common technique you know you'll notice
06:56 - that some points are below the line and
06:58 - some points are above the line so the
06:59 - difference could be positive or negative
07:02 - squaring it gets rid of that difference
07:05 - okay so how do we do this like I said
07:09 - there are a variety of methods and what
07:13 - I'm going to do is I'm going to show you
07:17 - a formula for this which I have written
07:19 - down another historic moment I prepared
07:23 - for today's video this was by
07:26 - preparation I wrote down the formula I
07:29 - click what N that I memorized it by
07:31 - editing this out if this is still in
07:34 - this video then I did not pretend okay
07:37 - so let's look at so first of all how do
07:41 - we represent mathematically this pinkish
07:44 - reddish line so the formula for a line
07:47 - is typically written as y equals MX plus
07:52 - B I will point out however that if you
07:55 - look in the statistics textbook you
07:58 - might see something like y equals b0
08:03 - plus b1 times X this is the same exact
08:08 - formula M refers to our b1 here as the
08:11 - slope and B 0 or B here which is the
08:16 - quote-unquote y-intercept which is the
08:21 - value where the line intersects the
08:24 - y-axis so the slope this M value
08:27 - determines like which way to the line
08:29 - point and then B y-intercept is how high
08:32 - or low it you know where is that line
08:36 - relative to the x axis so all we need to
08:40 - do is we need to both calculate n and B
08:43 - so here's the thing this is a
08:45 - you while we're looking at this you know
08:47 - most data sets that you might work with
08:50 - are just simple 2d data sets there might
08:53 - be you know there's temperature there's
08:55 - you know population of the city that the
08:58 - stores in maybe you know there's the
09:00 - hours that it's open I don't know you
09:02 - could think of like all sorts of other
09:04 - data inputs that might relate to the
09:09 - sale of ice cream and this can actually
09:11 - be generalized much of you know this
09:15 - this could be y equals B 0 plus B 1
09:18 - times X 1 plus B 2 times X 2 so there
09:22 - could actually be multiple linear
09:24 - written is referred to as multiple
09:26 - linear regression and generally you know
09:28 - the same math that I'm going to show you
09:30 - applies to this scenario but it
09:33 - typically involves a matrix based
09:35 - calculations maybe I'll do that in a
09:37 - different video for it's simpler to look
09:39 - at in just this context with just one
09:42 - input but we can extrapolate that and
09:44 - you could think about it instead of you
09:46 - know if instead of a line you're fitting
09:49 - this to a plane right if you had this as
09:51 - you know if there was just simply one
09:53 - other to two data to input pieces of
09:55 - data okay how we do it so far it's still
09:59 - here okay so now let's look at this
10:05 - formula look you can see I wrote it down
10:08 - and this piece of paper maybe I can
10:10 - auction this off on ebay or me nobody
10:13 - cool want it okay I'm going to need that
10:15 - piece of paper okay so here's how we
10:17 - calculate M the slope we calculate it as
10:23 - the sum so I'm going to use this Greek
10:27 - letter Sigma looks like an e but it's
10:30 - not an e it's a Sigma which means sum of
10:33 - X minus X with a line over it and I'll
10:39 - talk about what that means time Y minus
10:43 - y with a line over it you could call
10:46 - that Y bar I suppose
10:50 - divided by
10:53 - the sum of X minus X with a bar squared
11:04 - okay
11:05 - so let's think about what this means so
11:08 - first of all X bar or Y bar this means
11:12 - the mean or the average so what this
11:14 - really is is it's all of the X via
11:17 - values added up together divided by how
11:19 - many there are so you could think of X
11:21 - bar as being the sum and Sigma by the
11:27 - way mean um so I've got to kind of
11:28 - unpack that a little bit but the sum of
11:32 - every single X so X index I so x0 x1 x2
11:37 - where I goes from 0 to N and being the
11:41 - total so this is kind of mathematical
11:43 - notation to say add up all the X you can
11:46 - think of it as an array right an array
11:48 - of data points add them all up and then
11:50 - divide it by the total number there is
11:53 - divided by n so this is really what X
11:56 - bar is it's just the average of all the
11:57 - X's Y bar is the average of all the Y's
12:01 - so this means get that average and then
12:04 - take each X minus the average times each
12:08 - Y minus the average and so this really
12:10 - is also these this Sigma should really
12:13 - also have I goes from 0 to n and this is
12:16 - X index I this is y index I we have this
12:20 - from 0 to n this is X index I as well so
12:24 - I'm not going to derive or approve this
12:27 - formula in this video although if I
12:29 - could find some supplemental information
12:30 - I'll link to it in this video's
12:32 - description or maybe in the comments you
12:33 - can offer a suggestion but you can kind
12:36 - of get a an intuitive kind of sense of
12:40 - why this formula works so first of all
12:42 - imagine if x equals y if the formula for
12:45 - the line we're just y equals x that
12:48 - would mean the slope would be 1 right
12:50 - the slope would be 1 if y equals x well
12:53 - look at this if y equals x then X minus
12:56 - X bar times X minus X bar would be like
12:59 - that squared so you can see how M 1
13:01 - equal 1 if y equals x
13:03 - and then you could sort see the
13:04 - numerator is essentially the correlation
13:06 - Thank You Takei we've bought in the chat
13:08 - for typing it out like that cuz I think
13:10 - that's a good way of thinking about it
13:11 - you could see that you know if Y grow
13:15 - you know if it is y growing more as X
13:18 - grows or as X as Y growing less as X
13:22 - grows you can sort of see how this
13:23 - relationship is going to between the
13:26 - numerator and the denominator is going
13:28 - to give you a fraction that describes
13:31 - the slope of this line so think about
13:33 - that hopefully it has some intuitive
13:36 - sense and I'm sure people in the
13:38 - comments will write some nice
13:39 - explanations that help with that
13:41 - understanding so this is really it so
13:43 - what I want to do now in the neck I
13:45 - don't do this in the next video is I
13:47 - want to program this so I think what
13:50 - I'll do is I'll program it in such a way
13:52 - where a user can click and add data
13:55 - points and each time the user clicks I
13:58 - will I will implement this formula and
14:01 - draw the line of best fit according to
14:05 - the ordinary least squares method in
14:08 - canvas in the browser and after we do
14:11 - that I will talk about well what are
14:14 - some reasons why linear regression does
14:17 - it might not make sense for your data
14:19 - but this is the idea this is just just
14:22 - sort of trying to recap for a second the
14:23 - reason we're doing this is this is a
14:25 - model right the idea of a model the idea
14:28 - of a model is you try to fit it to the
14:30 - data we have known data training data
14:33 - temperature with actual sales we want to
14:37 - fit our model or model just as to
14:39 - parameters the slope of the line and the
14:42 - y-intercept and once we solve for those
14:45 - parameters we can make new predictions
14:47 - and even though it's kind of overly
14:49 - simplistic here this is the exact same
14:52 - process that we will that I'll employ
14:54 - again and again once we look at a simple
14:57 - perceptron then a multi-layer perceptron
15:00 - and then things like convolutional
15:02 - networks or recurrent net all of this
15:04 - all this is laying the foundation for
15:06 - more sophisticated robust machine
15:09 - learning based systems oops I forgot so
15:14 - for the formula of a line y equals M
15:17 - X plus B the slope is the more complex
15:20 - calculation actually once we have that
15:22 - slope it's pretty easy to calculate the
15:25 - y-intercept where is B and the formula
15:27 - for that is B equals y bar minus and
15:32 - times X bar and you can kind of see why
15:35 - this is the case right because remember
15:37 - y equals MX plus B so all I need to do
15:41 - is say B equals y minus MX right and we
15:46 - could just use the average of all the X
15:48 - and all the Y to figure out where should
15:50 - that line be shifted so this is a this
15:53 - is the formula to calculate that
15:55 - y-intercept so we can do this with a
15:57 - statistical method we run through all
15:59 - the data we calculate the slope we
16:01 - calculate the y-intercept and we have
16:03 - that line and formula for the line for
16:05 - which you can make predictions for new
16:06 - data okay so I hope this was helpful and
16:09 - made some sense to you and you have a I
16:13 - do of what linear regression is what the
16:16 - least squares method is and if you're
16:18 - inclined to continue just keep following
16:21 - to the next video and I will program
16:23 - this particular algorithm from scratch
16:26 - ok thanks very much see you there
16:28 - maybe
16:29 - [Music]
16:35 - you
16:40 - you