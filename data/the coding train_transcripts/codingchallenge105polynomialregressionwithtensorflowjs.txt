00:00 - hello coding challenge the last one part
00:03 - 2 of the last one something maybe I
00:05 - don't know this is actually my second
00:08 - attempt I started this earlier today and
00:10 - I'm now gonna try it again you shouldn't
00:14 - watch this video go do something else
00:16 - but I'm making it because I want to make
00:18 - this video they've got to feel like I
00:21 - did something today so in the previous
00:23 - video I did a demonstration of linear
00:26 - regression with ten approaches and the
00:31 - idea of linear regression is I have some
00:33 - data set right my data set or a bunch
00:35 - I'm making up a data set just by
00:36 - clicking points but you could imagine
00:38 - the x-axis representing something in the
00:40 - y-axis representing something and then
00:42 - I'm trying to fit a line I want my line
00:44 - to fit to that data set and I'm doing
00:48 - that by creating intent to vote yes
00:52 - these variables that represent the end
00:55 - and the B of the formula for a line and
00:57 - then I create an optimizer and I I try
01:00 - to like figure out the loss like well if
01:02 - I had a line there what's all the
01:03 - differences between all the actual data
01:05 - points and where the line is and
01:06 - minimize that all that stuff that I did
01:08 - in the previous video if you watch that
01:10 - but linear regression can only ever fit
01:15 - a line so what if for example my data
01:19 - looks something like this well you can
01:23 - see the best line you can figure out to
01:26 - fit that data is this line which is it
01:28 - really accurate but I could see pretty
01:30 - easily like oh I could create a
01:31 - polynomial function probably a quadratic
01:34 - function that is a curve that looks like
01:37 - this that fits all those points so this
01:39 - brings me to the topic of what everybody
01:48 - wants to do with their lives so what is
01:51 - a polynomial equation well in some sense
01:54 - this is a polynomial equation it's a
01:56 - degree one I could also have a
01:59 - polynomial equation that's like a
02:01 - constant of degree 0 y equals 5 if I had
02:05 - one of degree 2 what I have is y equals
02:09 - a x squared plus B
02:12 - Plus see quadratic cubic would be y
02:16 - equals ax cubed plus BX squared plus C
02:19 - now what are these values what is this
02:22 - what does this mean so the degree has to
02:24 - do with the number of solutions whether
02:26 - they're real solutions or complex
02:27 - solutions and there are little bunch of
02:29 - solutions across that hello is math
02:31 - stuff which is not my forte
02:33 - I'll try to find a good resource for you
02:35 - know the fundamental theory of algebra
02:37 - or whatever you could read about but the
02:38 - point is if I just want to now adjust my
02:42 - code to use a polynomial equation
02:44 - instead of a linear equation
02:47 - alright so that's not going to be too
02:48 - hard there's actually very little I need
02:50 - to change into code so let's go back
02:51 - here and let's go to the code and let's
02:56 - look so here's the thing now I'm not
02:59 - gonna do this but really what I want to
03:01 - I'll make sure that's at the end I have
03:02 - an exercise for you to do if you make it
03:04 - all the way into this video I've got an
03:05 - exercise for you to do which would be
03:06 - totally fun again in the sense in the
03:10 - world we're polynomial regression is fun
03:12 - alright so I had em in B as variables
03:15 - now I need a B and C okay so now I need
03:20 - instead of m and V a B and C a B you
03:28 - know one thing I don't like about this
03:29 - you know I'm gonna fix something first
03:31 - that I never liked about this example
03:34 - which is that my space my
03:37 - two-dimensional space goes between 0 & 1
03:39 - I think it'd be better just for the
03:42 - world if it went between negative 1 and
03:44 - 1 so let me look at everywhere where I
03:46 - map I mean it's really gonna be the same
03:51 - negative 1 1 negative 1 and 1 probably
03:54 - gonna miss something negative 1 or 1
03:56 - negative 1 and 1 just think that I
03:59 - should have like a Cartesian plane where
04:01 - zero zeros in the middle as my data
04:03 - space just for what I'm doing here so I
04:05 - think and then this should be this so
04:09 - let's look at this whoops okay I missed
04:13 - something
04:14 - yes no there's the mistake that you all
04:19 - saw negative 1 and 1 there we go ok ok
04:23 - so we're
04:24 - my linear regression is working just to
04:26 - be really sure up into a point here oh
04:28 - no look at this is the quickness down
04:31 - there I have it backwards
04:34 - zero too high ah what a negative one all
04:37 - right
04:38 - we're really gonna get this there we go
04:40 - okay there we go all right now
04:42 - everything's fine I can now go to
04:45 - changing these too sorry for that little
04:47 - digression a b and c a b c okay then
04:55 - what's the other thing that i need to
04:57 - change
04:58 - so clearly the thing that i need to
05:00 - change is my predict function wherever
05:03 - that is there it is
05:04 - right so this is the formula y equals MX
05:08 - plus B expressed with tensorflow j/s now
05:13 - I just need to express this formula also
05:17 - we tend to float is this I feel
05:20 - confident I know how to do cuz I did it
05:21 - earlier today y equals ax carrot or I
05:26 - like to say hat look at x squared plus
05:29 - BX plus C so I'm gonna say constant wise
05:33 - equals so X's squared right x squared
05:39 - multiplied all the stuff in tensorflow
05:41 - J's can be chained the mathematical
05:43 - operation so the X is squared multiplied
05:45 - by a adding the X is multiplied by B
05:52 - that's B X and then finally adding C so
05:58 - this you know takes some getting used to
06:00 - how to like put all this stuff together
06:01 - and I could put it in multiple steps to
06:03 - make it more clear but this is the kind
06:04 - of thing you want to if you want to get
06:06 - into little low-level tension flow
06:07 - digest so if you want to practice so now
06:10 - I can get rid of I'm just gonna get rid
06:11 - of this this is the predict function now
06:14 - I have another really significant issue
06:16 - here so I'm not gonna let me just run
06:18 - the code to make sure there's no syntax
06:19 - errors but it's obviously not going to
06:21 - do anything that makes any sense because
06:23 - it is what it's that this code down here
06:27 - is designed to just draw a lot by the
06:31 - way
06:31 - good see I was doing something earlier
06:34 - in other words I'm picking this point
06:37 - and this point and just drawing a line
06:40 - but in order for me to draw a curve I
06:43 - can't you know I have to sample a lot of
06:45 - points along the x-axis so I need to
06:47 - make a loop and I'm now going between
06:48 - negative 1 and 1 to sample all these
06:50 - points I need to say begin shape
06:52 - vertex to your vertex to your vertex
06:53 - here for protection bobba bobba and
06:56 - shape i'll see that line so instead of
07:00 - having X 1 and X 2 I want an array of X
07:03 - values and I'm going to call that
07:07 - instead of line X curve X and that's
07:10 - going to be an array and I am going to
07:14 - start X at negative 1 go all the way up
07:18 - to 1 say X + equals and let's use some
07:21 - increment like 0.05 and then I'm going
07:25 - to say curve X dot push X so I'm just
07:30 - trying to make let me just show you what
07:31 - I'm trying to do here console.log curve
07:34 - X and there's gonna be all sorts of
07:36 - errors here but let's see you can see
07:38 - here that I'm just trying to make an
07:40 - array that has lots of X values but
07:44 - between negative 1 all the way up to 1
07:46 - so I can now I need to get the Y values
07:49 - that go with that so I can draw this
07:51 - curve and I have the predict function
07:52 - does that so predict curve X get all
07:58 - those wise then curve Y is then again
08:04 - maybe I shouldn't be using data sync
08:05 - here that's kind of gonna be an
08:07 - animation slow down a problem but
08:09 - hopefully it'll be fine curve Y is now
08:14 - the regular number the floating point
08:16 - numbers not the tensor anymore version
08:18 - of the Y's then I can get rid of that
08:20 - tensor I'm done with it and instead of
08:22 - drawing a line what I'm gonna do is say
08:26 - now ok now I just need to go through and
08:29 - look at all of the X points and say the
08:36 - x value is map curve X index I which
08:41 - goes between negative 1 and 1 to 0 to
08:44 - width and Y map curve y between negative
08:50 - 1 0 to
08:51 - and then I want to say before this begin
08:54 - shape no fill stroke 255 stroke wait for
09:02 - two or whatever I had it and then n
09:05 - shape oh I need to actually set the
09:08 - vertex vertex X comma Y so let's see
09:13 - let's see what happens here look at that
09:18 - so this by the way is the random curve
09:20 - that the coefficients and I should get
09:22 - rid of this console.log so what's
09:27 - interesting about this is that every
09:29 - time i refresh this i'm gonna get a new
09:31 - polynomial a quadratic equation because
09:35 - it's picking random a b and c and you
09:37 - know what i didn't pick a random any
09:39 - negative numbers so this would also
09:41 - probably make more sense for it to write
09:43 - couldn't technically these be anywhere
09:47 - like I should start between like
09:48 - negative one and one does that make more
09:50 - sense right yes that makes more sense so
09:53 - you could see I'm getting all these
09:54 - random curves that's what it's starting
09:56 - with now when I click you can see it's
10:00 - trying to approximate the curve whoa
10:03 - weird huh that looks exactly right
10:07 - but backwards why because I made this
10:11 - mistake again height is flipped negative
10:14 - one to one
10:15 - oh no no not there this is it's hard to
10:19 - keep track of these pixel mappings
10:20 - between height and zero let's run this
10:26 - one more time there we go
10:30 - you can see now it's you can see it's
10:33 - taking a lot let me let me be more
10:34 - methodical about this I'm gonna click
10:36 - like a bunch like this it's actually
10:40 - finding it quite nicely but you can see
10:42 - it's good that it's quite lovely let me
10:45 - do it the other way
10:48 - you know what's bothering me is that you
10:51 - can see how this is not getting all the
10:52 - way to the edge it's because when I
10:54 - created those points I really want to
10:57 - say less than or equal to I want to get
10:59 - the last point in there as well Oh
11:05 - like do this that's a little bad trick
11:09 - there there we go let me let me write
11:15 - draw these now there we go so you can
11:18 - see this is working
11:19 - hooray so now we have polynomial
11:21 - regression now one thing I probably
11:23 - would want to do do is this you know now
11:26 - that I'm getting past just sort of like
11:29 - basic linear regression the optimizers
11:33 - the optimizer that I'm using if we look
11:35 - at the tensorflow code is train dot s
11:39 - GED stochastic gradient descent let's
11:43 - just try for a second
11:45 - let's look at the Tetra flow jsapi and
11:48 - i'm actually already here because i was
11:51 - looking at this earlier you can see
11:52 - there's actually different optimizers
11:54 - that we could try a well-known one atom
11:57 - the a da here being for adaptive i
11:59 - believe like a de grad a dad a de Delta
12:03 - but if I look at this we can see oh this
12:05 - constructs an atom optimizer that uses
12:07 - the atom algorithm I could click on this
12:10 - link here and I could find this whole
12:11 - paper that explains it beyond the scope
12:13 - of what I'm doing right now but just out
12:15 - of curiosity all I would need to do if I
12:18 - owe maybe stochastic gradient descent
12:19 - isn't the best algorithm for fitting
12:21 - this curve let's just change this to
12:23 - Adam and I kind of have a feeling that
12:26 - this learning rate being so high is
12:27 - gonna not work forever let's leave it so
12:30 - warning it might flicker quite a bit
12:32 - let's just see what happens with like a
12:33 - very high learning rate yeah you can see
12:38 - look at it like whoa that is really fun
12:42 - actually I kind of like it with the high
12:44 - learning rate because look how quickly
12:45 - but you can see it's kind of like
12:46 - bouncing around is because it's high
12:48 - learning rate it's gonna overshoot the
12:50 - optimal spot so let's just make that
12:52 - point one and let's also I think it
12:55 - would be fun to change this to Mouse
12:57 - dragged although I'm worried I'm gonna
12:59 - get so many points that it's gonna
13:01 - really slow down yeah
13:04 - Oh while I'm adding the points that's
13:07 - interesting so while I'm adding the
13:11 - points let's see this is a little bit
13:15 - weird but let
13:17 - adding points equals false I'm just
13:20 - gonna have this is like a terrible idea
13:23 - adding points equals true adding points
13:33 - equals false this doesn't make any sense
13:35 - why would that be really slow while I'm
13:38 - adding the points because what I want to
13:44 - say is like if and if not adding I was
13:48 - thinking I could like don't run the
13:49 - training if I'm not adding the points
13:53 - only run the training if I'm not adding
13:55 - the points yeah that didn't really help
13:58 - why what am I missing here all right I'm
14:01 - kind of off in the weeds here a little
14:04 - bit of something that's not irrelevant
14:05 - to this example but I've been noticing
14:06 - that it's like really slow as I'm like
14:08 - drawing and then as soon as I like let
14:10 - go it's perfectly happy to like animate
14:12 - very quickly I'm trying to figure out
14:14 - why this is so one thing I'm gonna try
14:16 - is I'm gonna just change I'm gonna add
14:18 - mousepressed and I'm gonna say let
14:20 - adding whoops I'm gonna create a new
14:22 - variable let adding points I'm gonna
14:29 - just actually call it let dragging equal
14:32 - the false and I'm going to say I'm gonna
14:38 - say my mouse press I'm gonna say
14:40 - dragging dragging equals true and then
14:44 - I'm gonna add mouse released and say
14:49 - dragging equals false and then in draw
14:54 - I'm gonna I'm gonna do this now in draw
14:58 - so this way I'm gonna kind of like
15:01 - really keep everything in draw and I'm
15:03 - gonna say if dragging then add points
15:07 - and if you're adding points maybe don't
15:09 - try minimizing the just wait till
15:13 - you're done adding points to minimize
15:16 - the function let's see how that works
15:18 - yeah there we go so it's not happening
15:22 - in real time in the same way but at
15:24 - least it's letting me add the points so
15:28 - I I'm intrigued
15:30 - why I wonder if there's probably a
15:31 - different way to think about this I
15:32 - really love how you that animation of
15:34 - watching it fit oh it's really just very
15:37 - satisfying so in any case let's try I'm
15:41 - just you know let me just try a smaller
15:45 - learning rate just because I'm curious
15:47 - so you can see with a smaller learning
15:49 - rate it's moving much more slowly so the
15:53 - higher move learning rate it's gonna get
15:55 - there much more quickly abouts good
15:56 - about you could do something that's
15:57 - called annealing I believe of the
15:59 - learning rate meaning start with a high
16:01 - learning rate but lower it over time
16:03 - we'd have to look at the tensorflow das
16:05 - API to see how that's done
16:06 - but here's thing I really want to do
16:08 - this I want to do this with a equation
16:11 - of degree three just because so I'm
16:13 - gonna do that manually so I'm gonna add
16:15 - a D here and then I'm gonna add a D here
16:20 - and this is gonna lead to my exercise
16:23 - for you and then I'm gonna say ax cubed
16:26 - plus BX squared plus CX Plus D so I want
16:32 - oh is there a cube mathematical
16:35 - operation in tensorflow TAS square
16:39 - operations operations there's a square a
16:44 - POW I guess I could do I'm just
16:46 - wondering if there's a TF dot square is
16:50 - there a cube no so I probably want to do
16:54 - power where is that it's funny how
17:00 - that's under it react so I want to do
17:02 - this
17:05 - POW base exponents so I would do X's tau
17:10 - 3 multiplied by a adding X's squared
17:18 - multiplied by B adding C X's multiplied
17:25 - by C and maybe I should start to do
17:29 - something where I put these on different
17:30 - lines so so let's do this X's pal 3
17:37 - multiply adding X's squared multiplied
17:42 - by B
17:43 - adding X is multiplied by C adding D
17:48 - right did I get this right this is now
17:51 - of degree 3 X is power to the power of 3
17:55 - multiplied by a plus X is squared
17:58 - multiplied by B plus X's plus CX Plus D
18:01 - ok and then I think actually I'm good
18:06 - because this everything else is the same
18:09 - so let's go check this ooh
18:12 - argument exponent passed Oh must be a
18:14 - tensor but got a number of course so if
18:17 - I'm saying power 3 I need to say TF
18:21 - scalar 3 so everything's got to be
18:23 - everything's got to be a tensor I can't
18:25 - just use numbers like I'm used to so
18:27 - that's to be TF to scalar dot 3 and let
18:33 - me put the learning rate back up higher
18:36 - let me put it back up to like point -
18:43 - wow that is immensely satisfied so now
18:47 - you can see and by the way if I happen
18:50 - to draw a line it should still be really
18:53 - happy to sort of like fit a line there
18:55 - because it could just make those
18:57 - coefficients 0 oh this is great oh hi
19:00 - perfect lad I've made this video for
19:01 - myself at least so here's the thing
19:03 - here's my exercise to you how can you
19:06 - make this so that the degree of the
19:08 - polynomial is like something that can be
19:11 - interactive as well so could I have a
19:13 - drop-down that allows me to try it with
19:15 - a degree 2 degree 3 degree 4 or maybe a
19:17 - slider what other kinds of interactive
19:20 - features could I add to this - and how
19:23 - could I maybe like have the data set
19:25 - come into this in a more interesting way
19:26 - besides just kind of drawing with the
19:29 - mouse so could you make what kinds of
19:32 - things could you could you do with this
19:35 - I think there's some interesting visual
19:37 - possibilities so I hope you enjoyed this
19:39 - video I'm going to publish this code now
19:41 - as a coding challenge called it's really
19:44 - part two of the linear regression now
19:45 - polynomial regression and I'll see you
19:48 - the future videos I'm going to work on
19:50 - that I'm gonna look at that tensorflow
19:51 - digest layers API soon enough ok goodbye
19:55 - I forgot something important that I was
19:59 - told by the chat that I should mention
20:01 - and I'm referring of the dangers of
20:04 - overfitting of polynomial equation and
20:07 - that's what this being represented on
20:10 - this is a Wikipedia page for overfitting
20:13 - which is a term that you'll hear a lot
20:15 - in machine learning meaning what is
20:18 - overfitting so if my data set if those
20:20 - points are my data set a really fancy
20:23 - polynomial equation of some high degree
20:25 - is gonna be able to draw something like
20:27 - really accurately connecting all those
20:29 - but it doesn't actually have any real
20:31 - meaning as applied to the data the data
20:33 - can have a lot of noise in it and a line
20:35 - might actually be an appropriate way to
20:38 - make a prediction even though it doesn't
20:40 - fit it has on like a higher loss than
20:42 - the polynomial so so this idea of
20:45 - generalization you don't want your
20:47 - machine learning model to work so
20:51 - accurately with your known data set that
20:54 - it cannot make good predictions with an
20:56 - unknown data set and this is something
20:57 - that will come up more and more as I do
20:58 - more videos and tutorials with things
21:00 - here's the thing though I kind of like
21:02 - this example that I made for the fact
21:05 - that it does something kind of
21:06 - interesting visually in that it just
21:08 - looked figures out how to make this
21:10 - polynomial function fit this arbitrary
21:12 - set of points so I think we're sort of
21:14 - like some kind of artistic output there
21:16 - might be some value here but yes
21:18 - thinking about actual machine learning
21:20 - and how you want to make predictions is
21:22 - important so that's why I'm here at the
21:25 - with this public service announcement
21:26 - the more you know
21:32 - [Music]
21:37 - you