00:00 - [DING]
00:01 - All right, we are ready
to continue our ml5 KNN
00:06 - Classification, what I'm
calling game controller.
00:08 - I want it to be able to
create an interface by which
00:11 - I can train a little
controller for a game
00:14 - to move something left and
right, maybe up and down.
00:16 - That's what I'm doing.
00:17 - Now the last video I left
off with at this step where
00:22 - I load the MobileNet model,
I create a feature extractor,
00:25 - I passed in an image
from the webcam
00:27 - and I get the logit itself.
00:28 - So one thing I do want to
mention, people in the chat
00:30 - were helpful, I was being
a little bit loosey goosey
00:33 - with the terminology of
like scalar, vector, matrix
00:36 - and tensor.
00:37 - And so I should
mention that a scalar
00:39 - being like a single number,
is really zero dimensions.
00:44 - A vector is typically like
a list of numbers, which
00:49 - is really one dimensional.
00:51 - A matrix is thought of
as a grid of numbers.
00:54 - This is 2D.
00:55 - And a tensor is a
term to describe
00:58 - the sort of n
dimensional possibilities
01:01 - of any collection of numbers.
01:04 - So a tensor can be zero
dimensions, one dimension, two
01:09 - dimensions, three, et cetera.
01:11 - So that's what those terms
mean, and in this case,
01:14 - the logits come out as a
one dimensional tensor.
01:17 - And the shape of
it, the dimensions
01:18 - might actually be something
different underneath the hood
01:20 - in tensorflow.js, but that's
one way of thinking about it.
01:22 - So hopefully that helps
clarify a little bit.
01:24 - But let's now move on.
01:26 - And I'm going to the
next thing that we're
01:29 - going to do is, we're just only
going to work from assuming
01:32 - that we have the logits.
01:34 - And the algorithm, we don't
have to write ourselves.
01:37 - I think I have a video
where I do some kind of KNN
01:40 - writing the algorithm
out with some movie
01:43 - recommendations or something.
01:44 - I'll try to reference that
in the video's description.
01:47 - But the next thing
we need to do,
01:49 - and I don't know what
step I'm on, like four,
01:51 - is create an ml5 KNN model.
01:59 - It's going to be empty.
02:00 - It's going to be
creating an empty one.
02:03 - So I'm going to add
a new variable, KNN.
02:09 - And then in Setup also,
I may have to move this
02:12 - somewhere else eventually.
02:14 - But for right now, I'm going
to say KNN is a new ml5 KNN
02:18 - Classifier.
02:20 - Simple as that.
02:21 - So I have the feature
extractor and the classifier.
02:25 - I think it's time to talk about
what is KNN classification.
02:31 - Luckily for us, we can refer
back to Nikhil's article.
02:35 - Thank you again to Nikhil
for publishing this.
02:38 - And I'm going to skip
through some of this more
02:41 - examples of different logits.
02:43 - And I'm going to look down
here and look at this.
02:47 - So this is a nice example
demonstrating the idea
02:52 - of k-nearest neighbors.
02:55 - So let's sort of recreate
that visualization
02:59 - over here on the board so I can
talk through it a little bit.
03:03 - Let's say this is a
two-dimensional space.
03:06 - It is.
03:08 - And I'm going to put in
this two-dimensional space,
03:11 - a bunch of A's and
a bunch of B's.
03:16 - Now I've written them in
a kind of certain way,
03:18 - to make it very obvious.
03:20 - In fact, if I wanted to, I
could draw just visually,
03:23 - I would understand
where's the quote
03:25 - unquote, decision
boundary between these two
03:28 - different categories.
03:30 - And I could kind
of draw like this.
03:31 - That's just me eyeballing it.
03:33 - I'm not sure if that's exactly
those exact mathematical
03:38 - decision boundary.
03:39 - But and then, I
could say like, oh,
03:40 - if I threw a dart at
the wall, the dart's
03:43 - either going to be over here
or it's going to be over here.
03:46 - It's going to be either
of class A or of Class B.
03:51 - It's going to be either,
I want to classify it.
03:55 - Now the way the
k-nearest neighbor works,
03:57 - the n stands for nearest,
the n stands for neighbor.
04:01 - The k stands for how
many nearest neighbors
04:04 - am I looking for?
04:06 - So the way that
it actually works,
04:07 - the way that the decision
boundary is computed,
04:10 - let's take it out
for a second, is
04:12 - that if I were to look at any
given point, like this one.
04:16 - Let's say I have a k of three.
04:18 - And typical you would want
a k that's an odd number.
04:22 - Because what you
want to do, is it's
04:24 - basically like a voting system.
04:25 - I want to look for the
three nearest neighbors.
04:28 - And in this thing, I
mean literally nearest.
04:30 - The Euclidean distance.
04:32 - By Euclidean distance.
04:33 - By proximity in two-dimensional
space, which are the nearest?
04:37 - Well, I can eyeball it.
04:38 - I could say this is the
nearest, this is the nearest,
04:41 - and this is the nearest.
04:42 - So these are my three
nearest neighbors.
04:45 - They all vote, and
three to zero, this spot
04:48 - gets categorized as an A.
04:50 - We could get a
little more complex.
04:52 - Like what if I went over here?
04:55 - What are the three
nearest neighbors?
04:56 - Well, this is the nearest,
this is the nearest,
04:59 - and this is the nearest.
05:01 - Ah, by a vote of 2 to 1.
05:03 - Actually, this k being odd
is actually irrelevant,
05:06 - because you can have
more than two classes.
05:08 - So breaking the tie is a thing.
05:10 - But here, I could say
by a vote of 2 to 1,
05:15 - this point is classified
as a B. And this
05:19 - is exactly what Nikhil's
visualization is doing.
05:22 - And in fact, this would
be nice to actually do.
05:27 - Do a coding challenge of
just like doing exactly this.
05:30 - Remaking this
visual explanation.
05:33 - But what I can do here,
is I can vary this.
05:35 - I can say let's look at
a three class example.
05:39 - Let's make k much
larger, like eight.
05:42 - And we can see here this is
showing what category it is.
05:48 - And I can also show
that decision boundary.
05:49 - And you can see here this
is the decision boundary.
05:52 - So this is the idea
of k-nearest neighbor.
05:55 - Wait, so that may
make sense to you.
05:56 - You might be thinking
to yourself, what?
05:59 - Suddenly you've got there's
two-dimensional graph
06:01 - and a bunch of points, which
ones are near each other?
06:03 - What does that have
to do with images
06:04 - of cats and logits of 1,000?
06:07 - Well, it does.
06:08 - It has something very
specific to do with it.
06:11 - In fact, the reason why the
Nikhil picked that example
06:16 - in his write-up,
and the reason why
06:18 - I'm drawing it this way
to explain it to you,
06:20 - is because our
brains are very well
06:23 - suited to think in small
amounts of dimensions.
06:26 - Two dimensions,
three dimensions.
06:29 - So you could think of
each one of these points
06:31 - in two-dimensional
space as the logits
06:34 - that just has two values.
06:36 - Its x and y-coordinate.
06:38 - But, what the feature extractor
does, so I'm going to say,
06:43 - I'm going to say
infer from an image.
06:47 - Remember this step from
the previous video,
06:50 - this gives us 1,000 logits,
a list of 1,000 numbers,
07:00 - and I said what if
we got two of them,
07:03 - two lists of 1,000 numbers,
how could we compute which?
07:08 - It's the distance
between these two things.
07:12 - It gets weird to
think about, how
07:14 - do I think about the distance
between two things in 1,000
07:16 - dimensional space?
07:18 - Well actually, the
formula that you
07:20 - would use for Euclidean distance
in two-dimensional space
07:23 - is the same exact
formula you would
07:25 - use in 1,000 dimensional space.
07:26 - Beyond the scope of this video,
I could make another video.
07:29 - It basically looks
like the square root
07:32 - of the difference between
all the values squared
07:34 - added together.
07:35 - It's like the
Pythagorean theorem.
07:36 - But actually, this
Euclidean distance formula
07:39 - is not such a good one for
higher dimensional spaces.
07:43 - It kind of breaks down.
07:44 - Because you could have a lot of
the numbers similar, but just
07:46 - one number really far
away from another one.
07:49 - So there is actually other
ways of calculating distances.
07:52 - One of which is called cosine
similarity or cosine distance.
07:56 - And actually, I've referenced
that in my Word2Vec tutorial,
07:59 - as well, this comes up exactly
in that Word2Vec stuff.
08:03 - And this is a way
of looking at, you
08:05 - could think almost as if I
wanted to compare a and b,
08:09 - I might look at the
angle between them.
08:12 - If this is the origin
and I'm looking
08:14 - at these two points
in 2D space, are they,
08:18 - is there a small
angle between them?
08:20 - That's kind of like what
cosine similarity is doing.
08:22 - But what's that sort of
like angle between in 1,000
08:25 - dimensional space?
08:26 - But the point is if I can
infer the logits from an image,
08:33 - I can build up a database.
08:36 - A database of let's say, cat.
08:39 - I can then use
the, what I'm going
08:41 - to do is use the add
example function.
08:44 - So ml5 has an add
example function
08:47 - that I give the
logits and the label.
08:50 - So I can basically
build up a database
08:53 - of a lot of logits
labeled with cat,
08:57 - and store them so that
later when I get a new one,
09:00 - I could see where is this
new one in one-dimensional,
09:04 - in 1,000 dimensional space.
09:05 - Near all those cat ones?
09:06 - This is exactly what
we're going to do.
09:08 - So in other words,
let's come back here.
09:14 - And I'm going to say
under mouse press
09:16 - now, instead of
console logging it
09:19 - out, I'm going to say
KNN add example, cat.
09:24 - And actually, what
I'm going to do is,
09:26 - I'm going to add left and right.
09:27 - I'm going to try to do left
and right, because the idea is
09:30 - I want to control something.
09:31 - So I made an example, these
logits are categorized as left.
09:36 - And I'm going to change
this to key pressed.
09:39 - This is sort of
silly what I'm doing.
09:40 - But I'm going to
say if key equals,
09:44 - uh, I forget how I do the arrow.
09:47 - If key equals L, again, I should
be building a nice interface
09:53 - for this, but.
09:59 - This is going to
just do the trick.
10:02 - Right.
10:03 - So what I'm doing now, is I
can now build up that database.
10:07 - This is the model.
10:08 - The KNN model is actually
just the big database
10:11 - of all of the logits
that are labeled.
10:14 - All right.
10:14 - So the idea here is that
I extract the features
10:17 - from the video.
10:18 - When I press L, I'm saying
this is one to the left.
10:20 - And when I press R, this
is one to the right.
10:22 - Let's take a look at that.
10:24 - Go back here.
10:26 - We'll click in here
to give it focus.
10:28 - So I want, this is confusing,
because it's not mirrored.
10:31 - This is my left.
10:32 - You see it as my right.
10:33 - This is my left hand.
10:34 - So I'm going to
use my actual left.
10:36 - And I'm going to press
L. So I'm training it.
10:40 - And then I'm going to do this
and press R a bunch of times.
10:44 - Now of course, I shouldn't
be console logging to see
10:46 - what's happening.
10:47 - But I at least can look
now at that KNN object.
10:51 - And it can look in it and see.
10:54 - What are some
functions I can call?
10:56 - I can do get number of labels.
11:04 - Two, right?
11:06 - Left and right.
11:07 - I could also probably
like how many,
11:09 - how many examples are there?
11:10 - But this is me training it.
11:12 - Oh, and look.
11:14 - Left and right.
11:14 - I didn't even notice that.
11:15 - Map string to
index, that's there.
11:18 - OK, so now, let's think.
11:21 - Now what do I do?
11:23 - Let's use mouse pressed.
11:24 - Let's go back and
use mouse pressed.
11:27 - What if I get something new
and I want to classify it?
11:30 - Aha.
11:31 - So let's say this
is me training it.
11:33 - Like here's a bunch of lefts,
here's a bunch of rights.
11:36 - And when I click
the mouse, I want
11:38 - to see what it is currently.
11:42 - So what I'm going to
do now, is I'm also
11:44 - going to infer the
logits from the video.
11:48 - Then I'm going to say KNN
classify those logits.
11:52 - Ah, here's the thing.
11:53 - This is where I now
need a callback.
11:56 - So everything so far I've
been able to do synchronously,
11:59 - like I can pull out the
logits, I can train it.
12:01 - But this is where, this
is now the computer,
12:04 - the ml5-library
and tensorflow.js
12:07 - needs to like do
something for a little bit
12:08 - and let me know when it's done.
12:10 - So I am going to say add a
function call, got results.
12:15 - And I'll just write
a separate function.
12:17 - Again, I'm doing this in a
very sort of like es5 callback
12:20 - kind of way.
12:21 - You can use Promises
and all of that
12:22 - as well, if you prefer that.
12:24 - But I'm going to say,
got results, result.
12:29 - And I'm just going to say
console log result to just see
12:32 - what that looks like.
12:34 - So now, I press
keys to train it.
12:36 - And you know what would be
nice for me to console log
12:39 - something, so console log left.
12:42 - Just so I see that it's working.
12:45 - Console log right.
12:48 - So I just console
log something there.
12:50 - Again, if you were
you doing this,
12:51 - you probably want to build
a whole interface for this.
12:54 - I'm doing everything with
just key presses and console
12:56 - logs, which is super awkward in
terms of like user interaction.
12:59 - But it allows me to test and
iterate and explain this idea
13:02 - more quickly.
13:03 - All right, so now, refresh.
13:06 - So what I want to do
is, and by the way,
13:08 - I'm going to get an error.
13:10 - So KNN classify
is not a function.
13:13 - Well, oh, I spelled it wrong.
13:15 - Classify.
13:16 - So here's the thing.
13:16 - I want to say like if,
I only want to do this
13:20 - if it's ready to classify.
13:22 - I want to say, if
KNN, what was it?
13:24 - What did I say
KNN, um, get count
13:30 - or get number of
labels or something?
13:32 - I'm going to say if KNN,
get number of labels
13:36 - is greater than zero, then
actually do the classification.
13:41 - Now let me refresh.
13:43 - And I'm going to click over
here and train it left.
13:49 - Here's like 20 images,
train it right.
13:53 - Here's 20 images.
13:55 - It's not 20, it's 17, it's 21.
13:57 - Now I'm going to
click the mouse.
13:59 - Ah, I know what I did wrong.
14:02 - I always do this wrong.
14:03 - [DING]
14:05 - ml5 uses error first
callbacks, p5 doesn't.
14:08 - And so I'm so used to just the
result being the first thing,
14:12 - but the error and any
error always comes in
14:14 - as the argument to the callback.
14:17 - So I'd want to say
if error console.
14:19 - I probably want to hand, there's
many ways I could actually
14:22 - handle the error.
14:23 - Otherwise, console log
result. Got to do this again.
14:27 - It's fine, here we go.
14:31 - OK.
14:32 - So I'm going to train it.
14:33 - Wait, no, whoops.
14:36 - Da, da, da, da, da, da.
14:38 - 21.
14:39 - I'm going to try to do
about the same amount.
14:43 - 22 now.
14:45 - Please say left.
14:48 - Look at that.
14:49 - Left.
14:51 - Please say right.
14:52 - Oh, no, whoops.
14:54 - Clicked in the wrong place.
14:55 - Ah, no, right, it
said right anyway.
14:58 - Right, left.
15:02 - It's working.
15:02 - Oh, that's exciting.
15:03 - So we've done it now.
15:05 - We, this is basically it, right?
15:06 - And I could, just say
you know by the way,
15:08 - I could say let's add one more.
15:13 - Let's call it up.
15:15 - One of the advantages of
doing KNN, like I was saying,
15:20 - this KNN classification, unlike
the other feature extractor
15:24 - with transfer learning example
that I went through previously,
15:27 - this requires no
separate training step.
15:30 - The model is just the
database of training images.
15:34 - And it's able to do that
distance calculation
15:36 - in real time.
15:38 - So also, I can add labels later,
I can add new images later.
15:42 - So you can kind of train and
classify all at the same time
15:46 - in a very flexible way.
15:47 - That's one of the
advantages of doing
15:49 - it this KNN classification way.
15:51 - All right, let's do one
more thing, which is.
15:56 - Let's actually just have
it constantly try to guess.
16:00 - So I'm going to make
a label paragraph.
16:07 - I'm going to say label
paragraph equals create p.
16:12 - Need training data.
16:16 - Then what I'm going
to do is, let's
16:19 - just make sure that's working.
16:21 - So it says need
training data there.
16:23 - Then what I want to do
is, this is train, is um,
16:30 - I'm going to have a Boolean
variable called ready.
16:35 - Have it be false.
16:37 - And then in.
16:41 - In draw.
16:43 - Basically since draw
is looping anyway,
16:45 - there's a more elegant
way I could do this.
16:47 - As long as it's not ready,
and if it's not ready
16:52 - and there's at least
one label, I can start.
16:55 - So I can start
this classification
16:57 - and I can now say ready is true.
17:01 - And then in the
classification, I can,
17:04 - when I get the result, what
do the results look like?
17:11 - They have a class index
confidence, confidences
17:14 - by label, and the actual label.
17:18 - So I can get the label.
17:19 - I can say labelp.html,
result.label.
17:25 - And then guess what
I can do right then.
17:28 - I can classify again.
17:31 - I probably want to break
this out into a function,
17:34 - but I'm just going to copy
paste the code right here.
17:36 - Whoops, ah.
17:39 - This video was going
very well a moment ago.
17:42 - Well, as well as
I could expect it.
17:43 - OK, I don't need this anymore.
17:45 - No, that's not right.
17:47 - Didn't do what I want to do.
17:48 - These two lines of code,
what do I want to do?
17:50 - Let's put this in
another function.
17:52 - Go classify.
17:53 - I mean, I could think of
a better name for this.
17:58 - And actually, let's take this.
18:02 - I'm going to do this.
18:03 - I'm going to make
this a little nicer.
18:05 - I'm going to go classify there.
18:07 - So if this is true,
then go classify.
18:10 - A little refactoring.
18:12 - Because why not, this
callback is a little silly
18:15 - to have it as a
separate function.
18:16 - Let's just make this
an anonymous function
18:23 - and put it in right here.
18:27 - So this is maybe a little,
so this is what I'm doing.
18:29 - I'm basically saying, classify.
18:30 - What does classify do?
18:32 - It gets the logits.
18:34 - It calls classify
with the logits.
18:36 - Then it has a callback and it
gives an error, it labels it,
18:40 - and then guess what it does.
18:41 - Go classify again.
18:43 - So this is kind
of this recursive
18:45 - just like classify
it when you're done,
18:47 - classify again when you're
done, classify again.
18:49 - OK.
18:51 - Let's see here.
18:53 - Needs training data,
needs training data.
18:56 - So let me do this.
18:57 - I'm going to give it some left.
18:58 - Let's gotta click over here.
19:01 - So it's only going to
ever be able to classify
19:03 - anything that's left right now.
19:06 - And at some point, it
knows that it's right.
19:11 - Left, right, left.
19:14 - Let's train it better.
19:15 - First of all, one thing
that's bothering me,
19:17 - is it's very hard to see that.
19:19 - So style, font size.
19:25 - This is how, this is how CSS
works with p5, Ooh, too big OK,
19:35 - let's try this one more time.
19:36 - Here we go everybody.
19:39 - [DRUM ROLL]
19:39 - Ooh, that's so loud.
19:41 - [DRUM ROLL]
19:45 - Give it lots and lots
of examples of me
19:49 - with my hand to my
left, about 50 of them.
19:53 - Now let's give it lots
and lots of examples of me
19:56 - with my hand to the right.
20:01 - Maybe, OK, now.
20:04 - Left, right, left, right,
left, right, left, look at me,
20:13 - I did left right there.
20:14 - OK, ahh!
20:16 - This is good.
20:17 - So guess what.
20:17 - We're kind of there.
20:18 - I mean, there's so much,
there's a lot of stuff.
20:20 - What's missing here?
20:21 - Well, first of all, I
want this to actually
20:24 - be able to control something,
like moving a paddle left
20:28 - and right.
20:29 - And then also, I might not want
to have to train it every time.
20:32 - I think there's
interesting, probably
20:34 - a lot of really
unique applications
20:36 - for you to make something that
the user is actually expected
20:38 - to train their own controller.
20:40 - There's a tensorflow.js
of what examples.
20:43 - Has an example of training
your own controller
20:44 - to control Pac-Man.
20:45 - I'll link to that in
this video's description.
20:47 - But in this case, what I
might like to show you,
20:50 - is that I could sit here
and train it for a while,
20:52 - save that, and load that model
into like a different sketch,
20:55 - which is just doing
the classification
20:57 - to move something.
20:58 - So that's what I'll do
in the very last video
21:01 - and I'll add up as well.
21:03 - So the last next step
that I'm going to do,
21:05 - is add saving and loading, as
well as a simple game mechanic
21:09 - to control as well.
21:11 - And you probably,
the save the load
21:13 - functions are just called
that, save and load.
21:15 - And so you might even
try this yourself
21:17 - before you watch the next
video where I'm going to do it.
21:19 - OK?
21:19 - Goodbye, and I'll see you soon.
21:21 - [DING]