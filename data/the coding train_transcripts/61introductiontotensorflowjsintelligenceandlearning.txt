00:00 - hello and welcome to another session of
00:03 - my string oddly organized perhaps
00:06 - hopefully somewhat organized set of
00:08 - videos about intelligence and learning
00:10 - so this where are we right now I am in
00:14 - the moment where I have finished a whole
00:16 - set of tutorials about neural networks
00:19 - and some basic machine learning types of
00:22 - things that one might do with the neural
00:23 - network comprehensive but a few
00:26 - demonstrations I built a little neural
00:28 - network library and JavaScript and went
00:29 - through some matrix math stuff so I'm
00:31 - ready finally at the time I've been
00:33 - saying this all along that I'm making
00:35 - this neural network library and kind of
00:37 - looking at how things work and trying to
00:39 - make some creative examples but later
00:41 - eventually someday I will use a more
00:46 - robust thoughtful well-designed
00:48 - framework as the guts as the as the
00:51 - foundation from which I will build all
00:54 - of my examples and projects I'll use
00:56 - somebody else's machine learning code
00:58 - and so today is the day that I'm going
01:01 - to start talking about doing that and
01:02 - the foundation that I will be using for
01:05 - almost all of these videos is a project
01:08 - called tensorflow Chasse so let's
01:11 - discuss for a moment where where where
01:15 - did tensor flow TAS come from so you
01:18 - might have heard of something called
01:20 - tensor weird to write this down ten surf
01:27 - low now first of all you might even be
01:30 - asking yourself oh why is it even called
01:33 - tensor flow what is this thing called a
01:36 - tensor and now this is gonna be really
01:38 - important because when I start to
01:39 - actually look at the code for tension
01:42 - float yes there's going to be stuff in
01:43 - there called tensors and the tensor is
01:46 - actually a mathematical thing it's a
01:50 - structure that holds numbers in it and
01:53 - it's really basically I mean I should
01:56 - look at the Wikipedia page for tensor
01:58 - probably and it'll give you a good I'll
01:59 - link to that in this video's description
02:00 - but we've been I've been referring to
02:03 - things as vectors well we have scalars
02:06 - which is like a single number like three
02:09 - we have this idea of vector
02:11 - which is a list of numbers like 3 1 4
02:19 - etc and we also have this idea of a
02:23 - matrix or matrices if I'm being
02:25 - consistent about singular or plural this
02:26 - idea of a matrix and a 2-dimensional
02:29 - matrix might have a grid of numbers like
02:33 - 3 4 1 5 so a tensor is a structure a
02:41 - data structure essentially that really
02:44 - can store any n dimensional version of
02:46 - these types of things so this is the and
02:50 - because the building blocks of any
02:51 - machine learning algorithm are matrices
02:54 - of numbers this idea of tensorflow
02:58 - know let's flow with the tensors insert
03:02 - animation of me flowing down the river
03:04 - of tensors well that happened in
03:06 - post-production I seriously doubt it
03:09 - this is where the name tensor flow comes
03:11 - from so tensor flow is Google's open
03:14 - source machine learning library it is
03:19 - written you might be surprised to hear
03:21 - this because you might think ah tensor
03:23 - flow it's Python right well yes kind of
03:27 - sure there were people who are watching
03:29 - this who know more about me so if you're
03:32 - watching the recorded version of this
03:33 - check the video's description for home
03:35 - the correction I'll go look for all I'll
03:37 - try to met make any Corrections at the
03:39 - end but the tensor flow is actually a
03:41 - library written in C++
03:43 - it is a low-level C++ library with a lot
03:48 - of functionality for doing machine
03:51 - learning now the reason why you might
03:54 - have thought to yourself oh isn't it
03:58 - Python well there simply is a sort of
04:02 - bindings for Python so to speak a Python
04:05 - a wrapper so to speak for pi so python
04:07 - being a programming language that's
04:08 - primarily used that primarily is but
04:11 - it's very popular in the world of data
04:13 - science it makes sense if you're a data
04:16 - scientist and working with data and you
04:19 - want to do some stuff with machine
04:20 - learning that you would and you're
04:22 - already in Python you
04:23 - want to be able to access something like
04:24 - tensorflow so every most all in every
04:27 - example that you would see working with
04:29 - tensorflow is you're just kind of
04:31 - operating the low-level tensorflow stuff
04:34 - from Python in fact there are also Java
04:39 - bindings for tensorflow and probably
04:42 - other languages as well and in another
04:44 - youniverse if all this JavaScript stuff
04:46 - had never happened
04:47 - oh it's travel back in time and stopped
04:50 - JavaScript from happening maybe what
04:52 - would our life be like should we try
04:54 - that but I don't think better or worse I
04:55 - would probably be investigating here
04:58 - right now talking about the Java
04:59 - bindings for tensorflow
05:01 - in an attempt to maybe go and use them
05:03 - with processing and actually this is
05:05 - something that I really I know that
05:06 - Gough read hater who is the creator of
05:08 - the Raspberry Pi arm version of
05:12 - processing has done some investigation
05:14 - of this and this is actually something I
05:15 - really would like to do but that aside
05:20 - this project tensorflow has been around
05:23 - for quite a while let me go look and
05:25 - find out how long it's been around and
05:26 - then I'll come back ok so I'm back I had
05:31 - to look that up tensorflow was open
05:33 - sourced in 2015 so tasteful actually is
05:36 - a project according to Wikipedia started
05:40 - in 2011 was a proprietary machine
05:43 - learning library used at Google for
05:45 - doing all sorts of stuff with neural
05:46 - networks and deep learning and more and
05:48 - then it was open source in 2015 under
05:51 - the Apache License so here's the thing
05:54 - last year I actually spent some time
05:58 - making some Python examples in
06:00 - tensorflow and I wanted them to talk to
06:04 - JavaScript so what I actually did is I
06:07 - wrote something called a flask server
06:09 - which is a Python flask is kind of like
06:13 - flask is - Python as node is to
06:16 - JavaScript I'm sure that's wrong in many
06:17 - ways and then what I did is I had my p5
06:21 - sketch talked to that flask server the
06:26 - flask server did Python stuff with
06:28 - tensorflow
06:29 - and then I could do machine learning
06:30 - tasks from within p5 and this is what I
06:33 - want to do I want to be able to
06:35 - straight and make examples and show
06:37 - things about how in a beginner friendly
06:40 - programming library like p5 or just in
06:43 - native vanilla JavaScript or using three
06:46 - Jas or whatever JavaScript world you
06:48 - live in I lived in the p5 world most of
06:50 - the time I want to be able to try to do
06:53 - some tensor flow East stuff and so this
06:57 - was the way I was doing it last year in
06:58 - the nature of code intelligence and
07:00 - learning course that I attempted to
07:02 - teach over the summer I think was last
07:06 - summer a project appeared a project
07:10 - appeared and it was called deep learn is
07:14 - now this is a my sense of this project
07:17 - is that this was a speculative project
07:19 - the idea behind deep learn Dutch s is
07:22 - can we do this kind of stuff in
07:29 - JavaScript and if so how so one of the
07:34 - things that's special about doing
07:37 - machine learning in today's modern era
07:39 - with tensorflow
07:41 - is in addition to this whole landscape
07:43 - of all this stuff where these operations
07:47 - that are written in C++ actually get
07:49 - executed they you have this question of
07:52 - do they get executed on the CPU or do
07:55 - they get executed on the GPU and why
07:58 - should we care about this well the CPU
08:01 - the processing unit the computer's
08:04 - processing unit is the C stands for boy
08:07 - hope so
08:08 - is a little thing that chugs along and
08:11 - kind of does most of the work that your
08:13 - computer has to do some time some time
08:15 - in days of your video games and special
08:19 - effects and graphics needed more and
08:22 - more processing and computing power so
08:25 - graphics processing units were created
08:27 - graphics processing units were created
08:30 - and optimized to work with pictures
08:35 - images pixels what are images they are
08:41 - matrices of pixels remember though is
08:44 - talking about how matrices are important
08:46 - to tensors and deep
08:49 - learning all of the mathematical
08:50 - operations that happen in a neural
08:52 - network our matrix based operations
08:54 - multiply these matrices together add
08:57 - these matrices sum these matrices past
08:59 - this activation function over this
09:00 - matrix that sort of stuff so the fact
09:03 - that over years and years and years that
09:05 - graphics processing units got optimized
09:08 - heavily to work with two-dimensional
09:11 - arrays of color information pixels it so
09:14 - happens that all this matrix stuff could
09:16 - be used with GPUs as well so this is
09:18 - really it's is why we deep the term deep
09:22 - learning from my point of view it's kind
09:23 - of in a way of like a rebrand of neural
09:26 - net machine learning with neural
09:28 - networks but now we live in an age of
09:30 - big data sets and really powerful GPUs
09:32 - and a lot of this modern research is
09:35 - coming from the fact that these older
09:37 - algorithms that we didn't think could do
09:38 - as much can do more now in the context
09:40 - of where we live now ok why am I saying
09:43 - this so how is this gonna work if we
09:47 - have a JavaScript implementation of
09:49 - tensorflow
09:50 - is the idea to just have another set of
09:52 - bindings so you're really just
09:54 - controlling C++ from JavaScript well
09:57 - that is certainly a possibility and I
09:59 - believe that exists or at least is in
10:01 - development there is a node dot J S
10:04 - package for working with tensor flow
10:07 - that actually connects directly to the
10:09 - C++ implementation and has a
10:12 - relationship to the tension flow just
10:13 - enough that I'm going to talk about here
10:14 - but that's not what I'm talking about
10:16 - here what the creators of deep learned
10:18 - is Nikhil and Daniel more information
10:21 - about them and the rest of the research
10:24 - teams that they work with in this
10:25 - video's description and want to miss
10:27 - credit anybody important they didn't
10:29 - actually write something to control
10:31 - native C++ GPU they actually just
10:34 - rewrote all the C++ algorithms loosely I
10:37 - don't know about all what's implemented
10:39 - so far not in JavaScript and isn't that
10:45 - gonna be really slow isn't that a
10:46 - terrible idea well first of all if
10:48 - you're me I like things from slow who
10:50 - cares I just want the stuff to run I
10:52 - want to play with it I want to learn
10:53 - about it I can always use something else
10:55 - to get it to run faster later but maybe
10:58 - in JavaScript alone it would run just
10:59 - way too
11:01 - there happens to be something in the
11:03 - world of JavaScript called WebGL WebGL
11:07 - is the browser's interface to OpenGL for
11:12 - doing operations on the graphics card
11:15 - for drawing and making graphic stuff
11:17 - happen in the browser so if the math
11:20 - operations of tensorflow in c++ can run
11:23 - on the GPU why can't the math operations
11:26 - inside of this thing called deep
11:28 - learning is run via the GPU via WebGL so
11:32 - that's really the magic in my mind of
11:34 - what was accomplished with this original
11:36 - project called deep learn Dutch yes so
11:38 - let's go look at that website for deep
11:40 - learn Dutch is first second on March
11:43 - 30th less than a month ago deep learn
11:47 - Jas became adopt this speculative
11:49 - project of doing these machine learning
11:52 - stuff in JavaScript was adopted by the
11:55 - larger tensorflow project itself and
11:58 - there and and has become this vert this
12:01 - project called tensor flow Jas whoo so
12:05 - tensorflow Jas this is now the project
12:09 - oh we can write that over here by the
12:12 - way if you can't see what's written up
12:13 - there it's just some question marks
12:14 - sorry about that
12:15 - tensor flow Jas so we're gonna circle
12:19 - that we're gonna put some hearts on it
12:22 - and a few stars this is now the
12:25 - framework that I am planning to use in
12:29 - my ended set of tutorials that you may
12:33 - or may not choose to watch I may or may
12:36 - not choose to make cuz that's right now
12:37 - I haven't made them yet but that's my
12:39 - plan all right there's something else
12:41 - that's important as part of the picture
12:42 - here that I want to talk about and to do
12:44 - so I'm gonna just erase this area over
12:47 - here so okay so we have this terminology
12:53 - in programming high-level versus low
12:56 - level and I actually saw discussion
12:58 - about this going on in the chat there
12:59 - are low level programming languages
13:01 - there are high level programming
13:02 - languages one way to think about that is
13:04 - low level is actually you're
13:06 - manipulating the RAM and the data in the
13:09 - central processing unit like you just
13:11 - you're all the way in there and the
13:12 - deepest part of the computer moving the
13:14 - numbers around yourself versus high
13:17 - level is something like really high
13:18 - level is like a scratch programming
13:19 - environment for kids where I'm like
13:21 - moving puzzle pieces and blocks around
13:23 - to try to create an algorithm so that's
13:24 - one way of thinking about high level low
13:26 - level so it's kind of there could be
13:28 - this sort of a level of abstraction so
13:32 - tensorflow if I were to make I guess I
13:34 - should put low on the bottom tensorflow
13:38 - in terms of working with machine
13:39 - learning operations tensorflow is a
13:42 - low-level library to do the actual
13:45 - matrix math and gradient descent
13:48 - learning training algorithms all
13:50 - yourself written into the code yourself
13:53 - with tential it's common operations that
13:55 - are implemented for you but this is
13:56 - really low-level control of the
13:58 - algorithm itself you could invent new
14:00 - machine learning models by writing them
14:02 - in tensorflow yourself then in between
14:06 - that there previously was a proper still
14:09 - is sorry this project called Charis
14:11 - Charis Charis Charis I don't have
14:14 - pronounce it can't laughs I always think
14:16 - everything is French for some reason so
14:19 - and that's probably not even a French
14:21 - pronunciation of that word but that
14:23 - aside sorry sorry that you had to watch
14:25 - that
14:26 - caris was meant to be a higher level API
14:31 - built on top of tensorflow and in fact
14:34 - Caruth was actually originally designed
14:37 - to be a higher level API that could sit
14:39 - on top of a variety of other low-level
14:42 - machine learning frameworks so for
14:43 - example there's something called theano
14:47 - is that it what is called I think it's
14:50 - called Theano there's like pi torch
14:52 - which is maybe well pi tors torch and
14:56 - then there's pi tortures python storage
14:58 - there's all these other machine
14:59 - lower-level machine learning frameworks
15:01 - I clearly have not an expert on but
15:03 - Karis the idea of countess's you could
15:05 - kind of write your code make a machine
15:07 - learning thing and it could it could
15:09 - operate on top of any of these so Karis
15:12 - though however more recently became part
15:15 - of the tensorflow project itself and so
15:19 - Karis is actually
15:21 - and tension flow are linked together so
15:23 - this is a higher level API that's
15:25 - written on that's built on top of
15:27 - tension flow and it exists as part as
15:29 - tensor flow Jas and it's so in tension
15:32 - flow digest there's no actual concept of
15:34 - Keres specifically but there is the core
15:37 - API and then what's called the layers
15:40 - API and the layers API is something that
15:43 - I'm gonna use much more in my video
15:45 - tutorials although I'm gonna start with
15:47 - a few that just look at the core stuff
15:49 - because it's kind of important to have a
15:50 - sense of what that is and how that works
15:52 - but layers so layers in tension flow
15:54 - Janus is the equivalent of this thing
15:55 - called Kerris now a project that's being
15:59 - developed here at New York University
16:01 - with some collaborators from ITP and
16:04 - guests and researchers and students is a
16:06 - project called ml 5 the 5 in ml 5 is an
16:11 - omage homage that's French right to the
16:15 - 5 in p5 in the sense that I mean this is
16:18 - flawed for many reasons but in p5 you
16:21 - could think of as like a wrapper on top
16:24 - of canvas and Dom to jeju's like common
16:28 - creative coding functions to make
16:30 - drawing and making pictures and doing
16:32 - creative sketching projects a bit easier
16:34 - and friendlier in JavaScript ml 5 is yet
16:39 - another layer on top of well sorry I
16:42 - shouldn't put this it's only for
16:44 - JavaScript as a layer on top of
16:46 - tensorflow Jas to do some common to
16:50 - allow to sound of like even abstract the
16:53 - concepts even a bit further and you know
16:56 - I think one of the goals of ML 5 is for
16:57 - it to be a library that high school
17:00 - class could use a kind of weekend
17:03 - workshop for artists could use these
17:05 - sort of context of people wanting to get
17:07 - a basic understanding and try some
17:09 - machine learning stuff out so anyway so
17:11 - this is all the stuff whether it was a
17:13 - long introduction to all these pieces
17:15 - this is all the stuff that I'm hoping to
17:17 - cover over the next several months you
17:20 - can kind of pick and choose ultimately
17:22 - you might be able you'll see what's
17:23 - available for you at the time of
17:25 - watching this but ultimately you might
17:28 - want to skip ahead and look at some of
17:30 - these ml5 tutorials because you don't
17:32 - necessarily to do the ml5 example
17:34 - you don't necessarily need to have a
17:36 - knowledge of the core API of tension
17:39 - flow chess or the layers API even but
17:42 - I'm gonna start even though I might buy
17:44 - the goal for the ml5 librarian examples
17:47 - is to give people a starting place that
17:49 - you don't need to have gone through all
17:51 - the lower-level stuff for my own kind of
17:53 - sanity and figuring this stuff out
17:55 - also ml5 doesn't actually have a public
17:57 - release yet whatever that means but the
17:59 - goal is sort of like have a
18:00 - quote-unquote public release in June
18:02 - with more documentation examples and
18:05 - features my I'm gonna start very first
18:09 - thing I'm gonna do in the next video is
18:10 - just look at the core API in attentive
18:12 - flow chess and see like what some of the
18:14 - things you can make do with it what some
18:16 - of the functionality is and that kind of
18:18 - stuff all right
18:18 - how am i doing I think good I probably
18:21 - made a bunch of mistakes and missed a
18:23 - bunch of things so check the video
18:26 - description because I'll write
18:27 - corrections and stuff in there and also
18:29 - I will in the next video if you continue
18:32 - on whatever the Google's YouTube machine
18:36 - learning algorithm tells you to watch
18:37 - next we'll hopefully have some anything
18:39 - that needs to be corrected alright
18:40 - thanks for watching this I hope this was
18:42 - a helpful picture of my of all these
18:44 - pieces and my thinking as it relates to
18:46 - them look ma ok goodbye
18:54 - [Music]