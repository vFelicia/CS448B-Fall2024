00:00 - hello all right this is a good moment
00:02 - for me I think I'm excited to see if I
00:04 - can get through this video because if I
00:07 - can implement this last piece of the
00:09 - Train function in the neural network
00:10 - library then I'll have a working version
00:13 - of some kind of neural network library
00:15 - like thing that I can start to finally
00:17 - apply to some projects and it's my goal
00:19 - actually that in some of the future
00:20 - videos I make that make use of the
00:22 - library and eventually other more
00:24 - sophisticated machine learning deep
00:26 - learning libraries like deep learning is
00:28 - a new library which i think is now gonna
00:31 - be called ml5 which i'll talk about in
00:33 - another video that I'll build really
00:35 - well I'm sad about it's to make stuff
00:36 - and show you how to make stuff but I'm
00:38 - kind of working through this because
00:39 - it's just something I have to do so I've
00:41 - started I have to finish I'm just trying
00:44 - to like vamp till you've stopped move on
00:46 - and watch something else because I'm not
00:48 - sure you should continue so but let's
00:49 - let me try to it's actually been a while
00:51 - since I recorded the previous video
00:52 - unfortunately might be watching them one
00:53 - after another let me try to like reset
00:55 - kind of where I think that we are so we
00:58 - have a something like a two layer
01:01 - network it has you know an input
01:07 - quote/unquote airboat layer it has these
01:10 - so these are the inputs this is the
01:13 - hidden and this is the output it is
01:18 - fully connected meaning every input is
01:22 - connected to every hidden and every
01:26 - output is connected to every every
01:30 - hidden is connected every input every
01:32 - output is connected every hidden and so
01:34 - the outputs come out here the inputs
01:36 - come out here all of these connections
01:38 - have a weight so and we can consider
01:42 - them in a weight matrix and I guess I
01:44 - should put this stuff back in here wait
01:47 - if this is like input one two three this
01:50 - is hidden one two
01:52 - this is weight one two one this is
01:55 - weight two to one this so these all end
01:59 - up in a nice weight matrix these end up
02:01 - in weight matrix the outputs come out oh
02:04 - boy in array like y1 y2 a vector the
02:09 - inputs come in in a vector
02:11 - x2 x3 okay don't get back into this
02:16 - I didn't even draw like off the top
02:19 - wonderful so now the idea is we want to
02:23 - do training and the kind of training
02:25 - that I'm doing right now is called
02:26 - supervised learning where I have some
02:31 - known output I've some known inputs with
02:33 - known outputs inputs with target outputs
02:36 - I send the inputs in I feed them forward
02:39 - I get some actual guests output back and
02:42 - I have some sort of error which we can
02:45 - think of as the error equals the guess
02:50 - I'm the guess - the target so I'm gonna
02:53 - say like y- target okay so this is the
02:58 - error now I should have mentioned before
03:00 - that there are a variety of ways to
03:03 - train a neural network and by training
03:07 - I mean adjusting all of these weights
03:09 - like their knobs to try to actually get
03:11 - the matched target output when you send
03:14 - in the training data so one thing I
03:15 - should mention I really gotta like even
03:17 - just take a minutes and make a video
03:18 - just about this but in most training
03:20 - situations you'll have training data
03:26 - test data and then of course there's the
03:29 - actual unknown data so we want to simply
03:34 - want to say like oh here's some labeled
03:36 - data I'm going to use that to train but
03:38 - I need to save some of that labeled data
03:40 - because what if I train my neural
03:42 - network only to work with the training
03:44 - data but it doesn't actually work with
03:46 - any other data well I can determine that
03:48 - by giving it some saved some data that I
03:50 - didn't train it with but I know the
03:51 - answer to see how it dumps with that and
03:53 - that's how we can evaluate it so I just
03:54 - want to mention that this is going to be
03:56 - the process now this idea of back
03:58 - propagation with gradient descent is one
04:01 - technique and it's a technique I'll put
04:02 - some links in this video's description
04:03 - you read about the history of it it's
04:05 - it's been around for a long time it's a
04:06 - big innovation in training neural
04:08 - networks but there is a lot of questions
04:11 - as to whether that's optimal best for
04:13 - the future etc there's different you
04:15 - know slept weeks of these algorithms and
04:18 - I actually next probably after I get
04:21 - through this plan to use a genetic
04:23 - algorithm to evolve the weights of
04:25 - network which opens up the door for a
04:27 - lot of kinds of projects that I excited
04:29 - to try to make so all that aside here we
04:33 - are what did we get so far we figured
04:35 - out in the previous video we calculated
04:37 - this error and then we calculated the
04:41 - hidden error which I'll just call each
04:43 - error right that's part of back
04:45 - propagation it's while we have this
04:46 - error how do we distribute this error
04:48 - all around so we can adjust all these
04:49 - weights the reason why we're doing this
04:51 - is because we actually did this already
04:52 - twice I've done this twice once with if
04:56 - you look at my videos about a single
04:57 - perceptron right which gets two inputs I
05:00 - forgot about the bias I always I always
05:02 - ever remember the bias I'm biased
05:05 - against the bias I'll come back to that
05:08 - but the single perceptron which just
05:10 - takes in two inputs and one output well
05:13 - it doesn't say it's a single neuron you
05:14 - can take in more than two inputs but a
05:16 - single neuron with multiple inputs and
05:17 - an output I actually did this we did
05:21 - this idea of training this with gradient
05:23 - descent and we also did it I did it we I
05:25 - did it in a video about linear
05:28 - regression where if I have a bunch of
05:33 - points in a two dimensional space I can
05:36 - find the line that fits these points the
05:39 - best and they did this what's
05:41 - interesting here is though this app so
05:44 - for a linear regression if you recall
05:45 - there's actually a mathematical formula
05:48 - to just compute the exact line of best
05:50 - fit called ordinary least squares I
05:52 - think I went through that in a previous
05:53 - video as well but the idea here and so
05:56 - the idea is we're trying to figure out y
05:58 - equals MX plus B the formula for this
06:02 - line well basically what this is doing
06:05 - is this is exactly the same process that
06:07 - we needed here only we have these
06:10 - weights you're going on you can almost
06:14 - think of this as like m and this is B
06:15 - maybe or something if there's just one
06:17 - input and a bias we had to fit we had to
06:21 - fit all the stuff coming through here
06:22 - into a line well this is actually what
06:24 - we need to do with all of these all of
06:27 - these places right we basically need to
06:29 - do the exact same algorithm that we did
06:31 - for this one line to compute this is the
06:34 - weight and this is the bias what we
06:37 - really have right is that why
06:38 - those MX plus B we have y equals M 1 X 1
06:43 - plus M 2 X 2 plus M 3 can you see that X
06:47 - 3 plus M 4 X 4 but added up plus B we
06:51 - have this we basically have exactly the
06:53 - same problem but in a multi-dimensional
06:55 - space so I just need to figure out how
06:58 - do I adjust each one of these M one and
07:00 - two and three and four and B all these
07:03 - weights inside of the matrix so the same
07:05 - training method I did for a linear
07:07 - regression gradient descent the same
07:08 - thing we did with the perceptron we now
07:10 - need to apply it here in this
07:11 - multi-dimensional space so what should I
07:15 - do next now first of all I forgot to
07:17 - thank a bunch of the resources hold on I
07:20 - some viewers rightly pointed out that
07:23 - when I did this previously and I guess
07:25 - the convention is target minus y the
07:27 - nice thing about this is when if you
07:29 - look at a cost function for a machine
07:31 - learning system you'll see that the cost
07:34 - function is the sum of all the errors
07:36 - squared so if you do target minus y or y
07:38 - minus target you square it doesn't
07:39 - really matter but it is an important
07:41 - distinction probably you have to get it
07:43 - right of course otherwise you might
07:45 - start trading in the wrong direction as
07:46 - you'll start to see as we do other stuff
07:48 - okay let me come over here and let me
07:51 - think so I've better come back to this
07:53 - video in a second but let me first say
07:56 - there are three things one is this is a
07:58 - new resource that just came out it's not
08:00 - a new resource but a new page on the ml
08:02 - for a ml for a github IO site this is a
08:06 - site put together by Jean Cogan has a
08:08 - ton of machine learning resources videos
08:11 - examples demos etc it's amazing and
08:14 - there is a nice article here about how
08:17 - neural networks are trained with a lot
08:19 - more detail than I'm gonna get into here
08:20 - but you can see the same sort of idea of
08:22 - talking about linear regression a loss
08:24 - function adding more dimensions this is
08:26 - the idea this is what we're doing
08:27 - of course I highly recommend you watch
08:31 - the three blue one Brown series about
08:34 - rating descent back propagation and back
08:36 - propagation calculus this will give you
08:38 - a massive extreme set background for
08:41 - what I'm gonna attempt to do it's just
08:42 - kind of like let me just tape this in
08:44 - code like kind of squint the press the
08:46 - button hope it works so this is great I
08:48 - highly recommend this and then I also
08:51 - have been using the make
08:52 - your own neural network book which I
08:53 - could hold up and wave around for you by
08:56 - Terry Gross she'd and there's a link to
08:58 - it on the coding train Amazon shop along
09:00 - with some other books that I've been
09:02 - using for videos and then so this is
09:04 - where I what I wanted to do now is try
09:06 - to connect back to here this is from my
09:12 - previous video entitled the mathematics
09:15 - of gradient descent where I go through a
09:17 - long algorithm to arrive at a very
09:20 - simple result which is in this case of y
09:27 - equals MX plus B what I'm looking to do
09:30 - is calculate the change in M and the
09:32 - change in B and so we can now write
09:34 - those formulas over here so in the case
09:38 - of y equals MX plus B we need to
09:42 - calculate Delta M how do we want em to
09:45 - change based on the error and it how we
09:49 - want em to change is the learning rate
09:50 - by the way learning right if you look in
09:52 - textbooks and stuff and sometimes
09:53 - written with this um the reek letter
09:55 - alpha I believe is that how far yeah
09:58 - learning rate times X times error and
10:06 - Delta B equals learning rate times error
10:12 - this is all done through some calculus
10:15 - in looking at the cost function looking
10:17 - at the derivative of the cost function
10:19 - the slope and how to walk around that
10:21 - cost function and find the bottom the
10:24 - minimum error how what values of m and B
10:27 - do we have to have the minimum error and
10:29 - that's what we want to do here what
10:31 - values of w w1 1 W 2 1 Dobie 300 400 I
10:35 - have to have the minimum error and that
10:37 - error each individual error we've got to
10:41 - like back it up and pass around and chop
10:43 - it up okay so how do we take this and
10:47 - move this to a multi-dimensional
10:51 - scenario I'm gonna rewrite these a
10:53 - little smaller further over to the left
10:54 - so I have more room to write out the
10:56 - formulas for the matrix version okay so
11:00 - these are the formulas for the change in
11:03 - slope and
11:05 - in by offset change and Hammack change
11:07 - and B for y equals MX plus B so I have
11:11 - the same situation except I have a
11:14 - slightly different one I have like the
11:16 - output equals Sigma of like that whole
11:20 - you know weight matrix what is it matrix
11:28 - product with the I guess it's uh with
11:32 - the inputs right I have this kind of
11:35 - before but it's basically the same thing
11:36 - plus the bias plus the bias which is a
11:39 - vector so I have like kind of like
11:40 - basically the same thing but instead of
11:42 - like single dimension these are all
11:44 - matrices Multi multi dimensional so what
11:47 - I'm going to attempt to do now is I'm
11:49 - going to just write out a notation for
11:52 - these formulas using matrices and try to
11:56 - like compare and contrast a little bit
11:58 - and I'm not sure I get it right because
12:01 - I kind of I'm using a bunk combination
12:03 - of trying to keep consistent with my
12:04 - notation from before and some
12:05 - conventions but let's see I've got it
12:07 - written down here let's look at this so
12:09 - let's first just say I just want to
12:11 - figure out Delta for these weights so in
12:14 - other words you can think of what I'm
12:16 - doing is instead just Delta n I want all
12:18 - of these weights so I want to say Delta
12:20 - all of the weights and the weights are
12:23 - from hidden to output the change in each
12:27 - weight IJ each row I column J I think
12:32 - that's what I've been using it's hard
12:34 - for me to remember what I used in the
12:35 - last video equals I'm just gonna look
12:37 - very similar first of all learning rate
12:39 - same I always have a learning rate
12:41 - learning rates gonna basically tune like
12:43 - how big of a step are we gonna take and
12:46 - I don't like the way I kind of want to
12:47 - rewrite this just because to have it
12:49 - match the way I'm gonna write the matrix
12:51 - version error times X I'm gonna write
12:57 - this times the vector E right that's
13:03 - that's the that's the vector of output 1
13:08 - minus target one output to our target
13:11 - one minus output one right that error
13:13 - vector that
13:14 - talked about and then X in this case is
13:17 - kind of like the input the input to this
13:22 - neuron to each of this output layer so
13:25 - I'm gonna call that a in this form gonna
13:27 - call that like I could call it x1 we
13:28 - call it H it's what's coming out of the
13:30 - hidden layer I'm gonna put that here
13:33 - when I put that at the end H so these
13:37 - are the components that exactly match up
13:39 - here at times H right we have same way
13:43 - of learning rate we have the error right
13:45 - and by the way if I were doing this for
13:47 - this layer if I would say Delta W Eights
13:50 - from I to J from input to hidden ih
13:53 - would equal do learning rate and then I
13:55 - would say times the hidden error right
13:59 - to remember that from the previous
14:00 - videos where I went through how to get
14:01 - the output error and the hidden error
14:03 - how we pass all that around and then
14:06 - this is times the input so this is the
14:10 - same exact formula but two different
14:11 - layers learning rate hearths or hidden
14:16 - errors the hidden output or the input
14:20 - output
14:20 - sort of way to say they hidden output
14:23 - the hidden output is the input to the
14:24 - output the input is the input to the
14:27 - hidden so you can sort of see how this
14:29 - is this formulas is looking at this part
14:31 - and this formula is looking at this part
14:33 - okay but I didn't put one other thing in
14:35 - here there's something funny that's
14:37 - gonna go in this spot right here so what
14:40 - goes here this is the question now this
14:42 - is where we're so happy to have this
14:44 - book make your up a neural network
14:45 - because we can just look up the formula
14:47 - in here but what's actually is gonna go
14:49 - here is the derivative of the output now
14:52 - let's think about this why why don't I
14:54 - come in with your videos you might have
14:55 - remembered back from the mathematics of
14:57 - gradient descent that we had to take the
15:00 - derivative but in this case of Y in this
15:04 - case it's MX plus B it's very simple
15:07 - derivatives a linear function here we
15:09 - have this sigmoid thing we need to
15:11 - calculate the gradient and the gradient
15:12 - is the errors time the error of the
15:16 - output times the derivative of the
15:18 - output what happened well has the output
15:21 - change has the output change relative to
15:24 - the errors so in this case right we need
15:27 - to add
15:27 - here the derivative of sinh way now
15:29 - here's the wonderful thing sigmoid is
15:31 - the function right the derivative of
15:33 - sigmoid s prime X is simply equal to the
15:38 - sigmoid of x times 1 minus can you see
15:43 - what I'm writing is sigmoid of X so in
15:47 - this case we need to calculate this
15:49 - gradient the derivative of sigmoid and
15:52 - right here now one thing I should really
15:55 - clarify here is so learning rate is a
15:58 - scalar number error is a vector learning
16:01 - rate is a scalar number I'm gonna put an
16:03 - asterisk here err a hidden error is a
16:05 - vector so I need to multiply and the
16:09 - output is a vector so these are actually
16:11 - this is element wise multiplication of
16:13 - the out now I've already what's coming
16:16 - out of here out of output has already
16:18 - had the sigmoid pass through it so I
16:19 - just need to say the output plus 1 minus
16:21 - the output the output plus 1 minus the
16:25 - output now what's weird about this is
16:29 - now the H is really this right we have
16:33 - an exact same formula I have the the
16:35 - learning rate the error gradient here
16:38 - you can sort of consider this gradient
16:39 - here is just the error the gradient here
16:41 - is the error times the derivative of the
16:44 - output and then if I'm multiplying I
16:46 - need to get a weight matrix so the input
16:48 - by the way is also a vector but I need
16:51 - to get a matrix so the interesting that
16:54 - happens if I use the matrix product here
16:56 - and transpose this vector right this is
17:00 - element wise multiplication this is just
17:03 - a scalar now here's the thing if I have
17:10 - the sorry if I have this error and type
17:15 - the gradient essentially at a called
17:17 - gradient as a single column vector for
17:25 - columns if I multiply that by a the
17:31 - hidden output which is also a single
17:33 - column vector but transposed
17:38 - if you take a single column matrix and
17:42 - multiply it by a single row matrix as
17:44 - long as it has this has the same number
17:47 - of columns as this has rows let's let's
17:49 - look at what happens right there's a
17:51 - wonderful website that I use often when
17:52 - I get stuck it's called matrix
17:55 - multiplication XYZ and here what I can
17:58 - do is I can make I'm gonna make this
17:59 - exact right here's an arbitrary single
18:02 - column vector and then I'm gonna make a
18:06 - single row vector so a 1 column matrix a
18:10 - 1 row matrix I'm gonna hit multiply so
18:12 - this is I'm just gonna go to the end
18:14 - here we can this is a nice little
18:16 - animation that goes through everything
18:17 - that I did in previous matrix
18:19 - multiplications and you can see we end
18:20 - up with a 4 by 4 matrix so this is
18:23 - exactly what we need to do to be able to
18:26 - get the deltas for all the weights ok so
18:33 - this also has to be I'm going here this
18:36 - has to be transposed and this should be
18:38 - element wise multiplication with the the
18:43 - I lost track here so I'm taking the
18:48 - output error and multiply by the
18:51 - derivative of the output here I'm taking
18:53 - the hidden error and multiplying it by
18:55 - the derivative of the hidden so that
18:57 - would be H plus h plus 1 so not plus Oh
19:02 - what I have plus here have had that
19:04 - wrong for so long times that's another
19:05 - element wise multiplication times 1 and
19:08 - this is a 1 not an I 1 minus H ok I
19:13 - think we've done it I'm gonna just check
19:15 - here my trusty guide but I highly
19:18 - recommend that you read instead of
19:20 - listing reads that are listening to me
19:21 - and I'm gonna look at this the change in
19:24 - weight matrix equals learning rate times
19:26 - the error times sigmoid of the output
19:30 - well I have sigmoid kind of built in
19:32 - here I'm assuming the output if the
19:34 - sigmoid has already been done
19:35 - times 1 minus Sigma or 2 the output I'm
19:37 - assuming Sigma so I've been done with
19:38 - the dot product of the matrix
19:40 - multiplication of the hidden the output
19:43 - transpose well this is right my notation
19:45 - is slightly different okay so I think
19:49 - we're just about right now of course I
19:51 - forgot
19:51 - the bias so I'm gonna have to basically
19:53 - do exactly the same thing with the bias
19:54 - bias is only simpler because I can just
19:57 - get rid of this so I'm assuming I could
19:58 - probably sort of do the same thing here
20:00 - so but I'll get to that in a bit
20:04 - let me at least try to implement this in
20:06 - the code and then I'll do that in the
20:08 - next video and then we'll come back to
20:10 - the bias
20:14 - [Music]