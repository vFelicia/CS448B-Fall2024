00:00 - [BELL RINGS]
00:00 - Hello.
00:01 - Welcome to another ml5.js video.
00:03 - Now I am really very
excited about this one.
00:06 - I have made three videos
so far at the moment
00:08 - at the time of this
recording, sort of Intro
00:11 - to ml5 and Machine Learning,
a video about doing image
00:14 - classification, and
a video about doing
00:16 - image classification with
real-time images coming in
00:19 - from a webcam.
00:20 - And both of those image
classification projects
00:22 - use a pre-trained
model called MobileNet.
00:24 - So just to pick off where we
last left off, here it is.
00:29 - This is the image
classification MobileNet
00:32 - with MobileNet example.
00:34 - You can see that I am,
today, a-- ah, come on.
00:36 - I want to be a snorkel.
00:37 - I really feel like a snorkel.
00:39 - I am kind of like an oboe.
00:40 - But we could see here, I think,
ah, if I get this ukulele
00:44 - and put it in here,
it's going to see
00:46 - it's an acoustic
guitar pretty quickly.
00:48 - Let me put this down over here.
00:49 - So here's the thing.
00:50 - We have determined
that this model is not
00:54 - good at recognizing
certain things.
00:55 - Like it cannot recognize
a train whistle.
00:58 - It thinks it's a
syringe or an oboe.
01:00 - It cannot recognize my
purple water bottle,
01:06 - sponsor of The
Coding Train water.
01:08 - It thinks it's a power
drill or a microphone.
01:10 - [SINGING] Hello.
01:12 - OK.
01:13 - So what if I forgot what I
was doing or why I'm here?
01:16 - What if I want to have this
example recognize things
01:22 - that I have here in this room
that it doesn't recognize?
01:25 - Could I train it
with my own data?
01:28 - So this brings up
so many questions,
01:31 - and there's so many different
paths we could go down.
01:33 - But the path I want to
go down in this video
01:36 - today is something
called Transfer Learning.
01:44 - So with the concept
of machine learning,
01:48 - I could have a massive database
of images and train a model,
01:52 - and label all those images,
and train a model based
01:55 - on that data, and
show it new images,
01:57 - and it'll tell me what's
inside those new images
01:59 - based on what it learned.
02:00 - But I am a person with no
massive database of images.
02:04 - So something that I could do
is use somebody else's model
02:08 - that already was trained with
a massive database of images
02:11 - and just kind of
say, eh, I'm going
02:12 - to use these images
on top of it.
02:13 - It's not a perfect solution,
but it is a quite powerful one
02:16 - that allows you to do certain
kinds of things very quickly.
02:18 - In fact, I just came
over here apparently
02:20 - to write Transfer Learning.
02:21 - There is a project that
I want to show you--
02:23 - let me just hit Back here--
02:25 - called Teachable Machine.
02:27 - So Teachable Machine is a
project made by various--
02:30 - a collaboration of many
different researchers
02:32 - at Google, led by the
Google Creative Lab.
02:35 - And I'm going to run
through this in a second.
02:38 - And basically, what
I'm going to do,
02:40 - I'm going to introduce
the idea of--
02:42 - show you Teachable
Machine, introduce
02:43 - the idea of transfer
learning, talk
02:45 - about how it works in
ml5 with MobileNet,
02:47 - we're going to take
a break, and then
02:49 - I'm going to come back and
actually make the code example.
02:52 - I'm going to make a
teachable machine.
02:55 - So let's first just
run this and see.
02:57 - So I'm going to
skip the tutorial,
02:59 - and I'm going to
just open this up.
03:02 - I'm going to zoom in
a little bit here.
03:03 - So I should say that
Teachable Machine
03:05 - is using a slightly different
algorithm behind the scenes
03:08 - than what I'm ultimately
going to implement.
03:10 - And I'll kind of talk
about maybe the differences
03:12 - between those after I kind
of get done with all this.
03:15 - But conceptually, it's
exactly the same thing.
03:17 - So right now, I can see there
are three categories-- green
03:21 - purple, orange.
03:22 - So what I'm going to do
is I'm going to say--
03:25 - so now I'm going to attempt
to train the teachable machine
03:28 - with my own images.
03:28 - So I'm going to step out of the
frame just for the time being.
03:32 - I'm going to say, Train Green.
03:33 - I'm holding this
down, so I'm kind of--
03:35 - I'm giving it lots of
examples of a ukulele
03:37 - at different kind of angles.
03:38 - Of course, it has
my arm in it too.
03:40 - That's a part of
what it's learning.
03:41 - Then I'm going to stop.
03:42 - I'm going to put the
ukulele down awkwardly.
03:44 - I'm going to grab
this train whistle,
03:48 - and I'm going to
hit Train Purple.
03:51 - I'm going to give it lots of
examples of the train whistle,
03:54 - and then I'm going to let go.
03:56 - And then let's do one more.
04:00 - Oh, I really should have
made this one purple.
04:02 - I'm going to train it
orange, because it is purple.
04:05 - This is my water bottle.
04:07 - So I'm going to train
it with the water
04:09 - bottle a bunch of times.
04:10 - I'm sort of in the frame.
04:11 - I'm going to get out of the
frame, and I'm going to let go.
04:13 - All right, so I
finished training
04:14 - this teachable machine.
04:16 - If I come into the
picture, it actually
04:18 - thinks I'm the water bottle.
04:19 - I'm orange.
04:20 - But look at this.
04:21 - Let me now show it the ukulele.
04:23 - Confidence-- 99%.
04:26 - Let me now show it
the train whistle.
04:31 - Confidence purple-- 99%.
04:33 - And now let me show
it the water bottle.
04:38 - Confidence orange, so you can
see this works quite well.
04:41 - If I stand in there, it's
kind of confused now.
04:44 - You see, I was standing in
a lot of the training images
04:47 - with the water
bottle, so it really
04:49 - thinks my train whistle
is the water bottle.
04:50 - If I'm not in the picture, it
knows it's a train whistle.
04:53 - So this is very important.
04:54 - You have to remember
the machine learning
04:57 - system is not learning anything
about these particular objects.
05:02 - It's learning about the
exact sets of pixels
05:04 - you're showing it to.
05:06 - So if I'm standing in the
background with the ukulele
05:08 - every time, and then I'm not
in the background anymore,
05:11 - it's going to be confused.
05:12 - We see this in some
machine learning models
05:15 - that if you hold something up,
no matter what you hold up,
05:17 - it always just says Cell
Phone, because there's so
05:19 - many training images of
people holding a cell phone,
05:21 - it thinks, well,
whatever you're holding,
05:23 - it's got to be a cell phone.
05:24 - So this is the idea, so the
wonderful thing about this
05:29 - is it's really fun.
05:30 - There's a really nice interface.
05:32 - It's designed really well.
05:33 - I can have it show different
GIFs, play different sounds,
05:37 - but I'm limited to
these three categories--
05:40 - green, purple, orange.
05:42 - And I want to be
able to do something
05:43 - like this in my own
interactive experiments.
05:46 - OK, so let's talk about
how does it even work.
05:48 - How does this even work?
05:50 - Before we get to the code,
let's talk about how it works.
05:52 - All right, so let's talk about
how image classification works.
05:57 - We said, or I said,
I think, there
06:00 - is something called MobileNet.
06:03 - This is the pre-trained
model that I've talked about.
06:06 - It has 1,000 image
classes, and it
06:09 - was trained on a database
of like 15 million images
06:12 - from a database called ImageNet.
06:16 - When we use it, we send
our own image unit.
06:20 - We send an image.
06:22 - Maybe it's from a webcam.
06:23 - Maybe it's a PNG.
06:24 - We send it into MobileNet,
and then MobileNet gives us
06:27 - back a label and a probability.
06:33 - So maybe it says something like,
cat, 90%, bird, 5%, clock, 5%.
06:43 - This is what it gives us back.
06:46 - So how are we going
to retrain this model?
06:50 - Well, here's the thing.
06:51 - There's a lot of stuff
going on in here,
06:55 - and in order to retrain it,
we need to kind of like peel
06:58 - it open a little bit.
07:00 - And the thing that we're
going to use to peel it open
07:02 - is something that's built
into the ml5 library called
07:06 - a feature extractor.
07:11 - OK, well, internally inside
of the MobileNet model,
07:14 - the MobileNet model is
running a neural network.
07:17 - You might have heard
that term before,
07:19 - and at some point in
this video series,
07:20 - we might get more into that.
07:22 - But a neural network is
something that has layers.
07:26 - It has multiple layers.
07:27 - Maybe it has Layer
1, Layer 2, Layer 3,
07:34 - and that data is actually
being passed into Layer 1.
07:37 - It's being processed, and
then sent into Layer 2.
07:40 - It's being processed again.
07:42 - It's being sent to the Layer 3.
07:43 - It's processed again.
07:45 - Those processes-- there are
different kinds of processes.
07:48 - For example, most likely, since
we're sending it image data,
07:51 - it's using something
called-- you might have heard
07:53 - this term-- a convolution.
07:55 - A convolution is actually
an image process,
07:57 - which is the same thing that
happens like in Photoshop
08:00 - or any kind of image
processing utility, where
08:02 - you open up an image and say,
hey, let's make it brighter.
08:04 - So a neural network is
doing that to the image.
08:07 - It's doing all of
these processes
08:09 - over and over and over again
to try to reduce the image.
08:11 - This thing's got a
lot of pixels in it.
08:12 - Let's process it
down to something
08:14 - smaller and down to
something smaller,
08:15 - and let's do that many,
many, many, many times
08:17 - over multiple layers to
eventually get to something,
08:22 - which we can call features.
08:24 - So if I say that the
last layer, after it
08:27 - does all these processes, is
something called features,
08:30 - and then those features,
which exist here,
08:34 - they are then sent into--
08:37 - they are then converted--
sorry-- through another layer
08:40 - into labels and probabilities.
08:43 - So there's this
whole process that's
08:44 - happening inside of MobileNet.
08:46 - The image comes in.
08:47 - It's processed through
a convolutional layer,
08:49 - maybe through another
convolutional layer,
08:51 - then through some other kind
of layer, blah, blah, blah,
08:53 - ends with all these num
features, which are really just
08:55 - numbers, a whole lot of numbers.
08:58 - And then those numbers are
processed one more time
09:00 - to get probabilities.
09:02 - Well, what if what transfer
learning is is it's,
09:05 - hey, let's just
delete this part.
09:09 - Let's go into MobileNet
and stop right here.
09:11 - Feature extractor-- let's make a
version of the MobileNet model,
09:15 - where we stop right here
at the feature extractor.
09:17 - And then we take those
features and tr-- we
09:21 - put our own training images in.
09:23 - We say, take this training
image of the ukulele.
09:27 - Send it all the way through.
09:28 - Don't bother to get the label.
09:29 - Don't get acoustic guitar.
09:31 - Just stop here, and
say, hey, you know what?
09:34 - These features, these
features are 100% ukulele.
09:39 - So we're going to retrain
the model to map--
09:42 - basically map of the features
to our own labels instead
09:46 - of the labels that
are previously
09:49 - existed in MobileNet.
09:50 - All right, so what
are the features?
09:52 - So here's the thing.
09:54 - In theory, we could-- this--
09:57 - I mean I think in some crazy
sort of theoretical sense,
10:01 - we could eliminate
all of these layers
10:03 - and just teach a machine to
learn that this set of pixels
10:07 - are a cat, and this set
of pixels are a bird.
10:09 - And I'll look at this
whole other set of pixels.
10:11 - Which set of pixels
does it resemble?
10:13 - That's kind of what we're doing,
but what you have to remember
10:17 - is that images have
so many pixels.
10:20 - And so when you have to
compare images pixel by pixel
10:24 - by pixel by pixel,
there's so much data.
10:26 - And so actually, this idea of
features-- the idea of features
10:30 - is boiling the
essence of an image
10:33 - down to a smaller set
of manageable numbers.
10:37 - So in essence, this
image, which might have
10:40 - started as like a 512 by 512--
10:43 - you do the math--
10:44 - 512 squared number
of pixels maybe
10:46 - ends up as 100 features,
just 100 numbers.
10:51 - I don't actually remember
what it is in MobileNet.
10:53 - We should look it up.
10:54 - And then those
numbers are typically
10:56 - just numbers between 0 and 1.
10:59 - This is also often
referred to as a vector,
11:02 - meaning a list of numbers.
11:03 - It's the essence-- it's the
numeric essence of that image
11:07 - that you just passed in.
11:08 - And it's been-- it's learned
these features over lots
11:12 - and lots of time of being
trained with millions
11:14 - and millions of images.
11:16 - So what we need to do in ml5 is
ml5 is a higher level library.
11:21 - We don't have to do all
this manipulation ourselves.
11:23 - We basically just say, hey,
instead of making an image
11:26 - classifier with
MobileNet, we're going
11:28 - to make a feature
extractor with MobileNet,
11:31 - and then turn that feature
extractor into a classifier,
11:35 - and train it with
our own images.
11:38 - So this is what I'm going
to do in the next video.
11:40 - I'm actually going to write
the code to do exactly this,
11:44 - and I'm going to train it with
a few sets of images in here.
11:46 - And actually, what
you'll see is what's
11:47 - interesting is you
can actually get
11:48 - it to work with things like
different facial expressions
11:51 - or different gestures.
11:52 - There's a lot of wonderful
possibilities there.
11:55 - [MUSIC PLAYING]