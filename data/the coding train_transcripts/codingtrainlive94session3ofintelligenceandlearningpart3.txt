00:02 - hello-o do you hear that why I believe
00:07 - that's the sound of the coding train
00:10 - pulling into the station hello good
00:16 - evening afternoon and welcome to another
00:19 - episode of the coding train with me
00:22 - dan Shipman this is a YouTube channel
00:26 - where I make coding tutorials and a
00:29 - variety of other things actually I don't
00:31 - do any other things I'm only do the
00:32 - coding skirt look my thing
00:34 - no tutorials is the right word I try to
00:36 - be here every week on Fridays typically
00:39 - I was here yesterday I do a terrible job
00:42 - at scheduling and keeping it just in
00:44 - time maybe someday I'll get better at
00:46 - that because I hear as a content creator
00:49 - in this world that we live in a 2017 it
00:52 - is very important to have regularly
00:56 - scheduled that's okay so today is
01:00 - actually a follow-up from yesterday if
01:02 - you weren't here yesterday with a thing
01:04 - that I covered is something called
01:06 - linear regression and today I'm gonna do
01:12 - linear regression again I don't know I
01:20 - don't know I don't know about these
01:21 - sound effects okay so yesterday I did
01:23 - something called linear regression we're
01:25 - on this train we're on a train we've got
01:28 - a destination that destination is
01:30 - machine learning station and ultimately
01:34 - I want to get to neural network based
01:37 - machine learning systems and I'm going
01:39 - to build a neural network from scratch
01:40 - and probably JavaScript but maybe in
01:42 - prospect I've decided yet on this
01:45 - channel but I'm trying to do a bunch of
01:47 - tutorials to set up a foundation for
01:49 - making that easier and linear regression
01:52 - looking at linear regression from a
01:54 - statistical approach and now looking at
01:57 - it with something called gradient
01:58 - descent gradient descent is a learning
02:01 - technique that is a fundamental piece of
02:04 - neural network based machine learning
02:06 - systems so today we're really not I it's
02:10 - not my intention that what I'm
02:11 - to show you will yield beautiful
02:14 - generative algorithmic artworks but
02:16 - rather will demonstrate a particular
02:18 - technique that I'm going to apply again
02:21 - later and also hopefully will give you a
02:23 - background for when you're reading blog
02:26 - post or papers or books that involve
02:28 - machine learning based systems and those
02:30 - systems talk about gradient descent and
02:32 - derivative this and calculus that a
02:34 - partial derivative is and chain rule
02:36 - that this hopefully the today's session
02:39 - will give you a kind of a background in
02:41 - that sort of stuff okay so I'm looking
02:43 - here to see what's going on in the chats
02:46 - I can see yep there's a link there we go
02:50 - the rainbow on the t-shirt hasn't the
02:53 - same colors as the wallpaper
02:55 - it should oh oh it's different this is a
02:58 - slightly different design now if you're
03:03 - wondering oh look at that t-shirt where
03:05 - could I get one of those t-shirts like
03:06 - that I want one I don't know if that's
03:08 - what you're wondering you can go to I'm
03:12 - such a shill a sellout coding trained
03:17 - Storenvy comm actually I think the one
03:20 - that I've got here is I ordered a sample
03:23 - of a color that I didn't make available
03:25 - one thing that I'm discovering is the
03:26 - light weight I don't know why I'm
03:28 - talking about t-shirts but the light
03:30 - weight t-shirts with the sort of this is
03:32 - I think Heather blue or something I like
03:33 - those better I don't know how you feel
03:35 - about them but anyway you can you can
03:37 - get your own t-shirt here okay I can add
03:39 - more colors I'll think about that on my
03:41 - own time um okay um I heard a noise it's
03:49 - it's like Friday afternoon here it's a
03:51 - holiday weekend here in the United
03:53 - States I can't imagine that there's
03:55 - anybody roaming in the hallways and I
03:57 - will soon be on my way to enjoy a nice
04:00 - holiday weekend hopefully before I do
04:04 - that I have to torture myself by
04:06 - attempting to talk about gradient
04:09 - descent now um calculus or something
04:13 - like that so before I get started I
04:15 - would like to read my what will now be
04:17 - my weekly quote I did one yesterday
04:19 - every live session I'll read you a quote
04:21 - from this book in her own word
04:23 - Rhodes this I mentioned this yesterday
04:26 - but I'll mention it again red burns was
04:29 - the founder of the interactive
04:30 - telecommunications program it's where I
04:32 - teach at Tisch School of the Arts at NYU
04:34 - she was she led the program for over 30
04:36 - years and in her first class that she
04:39 - used to teach called the applications
04:40 - class though and anybody everybody just
04:43 - called it Reds class she had a slide
04:45 - presentation so this book is a
04:47 - compilation of quotes from her slide
04:49 - presentation I will read the one that I
04:52 - read yesterday so that you get the
04:53 - continuity as you come together depend
04:56 - on each other that was yesterday's and
04:58 - now today's is you will find yourself
05:02 - wearing the ill-fitting clothes of
05:03 - someone else's world dining on the
05:06 - strange food of someone else's thought
05:09 - okay so that's my read Burns quote from
05:16 - today hopefully getting us in the mood
05:18 - and the right frame of mind in the
05:20 - relaxed meditative state that is high to
05:28 - do some coding calculus linear algebra
05:35 - okay so what's going to be for no sound
05:42 - a chart some say no sound I don't
05:43 - believe that that's a one person saying
05:46 - this sound alright so now I'm just
05:50 - playing weird sounds for no reason at
05:52 - all
05:53 - um Oh is somebody here at the coding
05:59 - train I go over York it seems that the
06:04 - door oh there's nobody at the door but
06:09 - someday that would be great right if we
06:11 - had somebody at the door
06:12 - who would come in and say hi and then
06:14 - start talking about something you're
06:15 - over there okay come on and come back
06:17 - over here I think I'm just gonna dive in
06:19 - um dive in you know what I don't
06:24 - actually know how to dive I know how to
06:26 - swim but I don't know how to dive and I
06:29 - think I should take lessons and learn to
06:30 - dive alright so let's see how we're
06:37 - gonna get
06:37 - started with this I am going to first
06:41 - thing I'm going to do here is um clean
06:46 - the whiteboard I don't think that I need
06:49 - I should have done this clearly I should
06:51 - have done this in advance but I did not
06:56 - we're trying to think do I need any of
06:59 - this from yesterday maybe I did maybe
07:05 - I'm not going to clean it just yet and I
07:08 - will do that in the middle of the video
07:09 - at first so I think that what is it this
07:12 - is good this is like a total world
07:14 - record for me
07:16 - seven minutes in I'm going to start with
07:18 - the lesson thank you for somebody
07:27 - screenshotting that in case I need it
07:28 - okay understand the chat rightz finally
07:33 - finally some proper math so let me just
07:35 - make a good point here uh I think if you
07:39 - said finally some math you're the right
07:41 - on the right track there will be nothing
07:43 - proper about this whatsoever I have to
07:46 - say that I am NOT this is not something
07:48 - that I have a deep knowledge of this is
07:50 - not something that I consider myself an
07:51 - expert on this is something that I've
07:53 - been learning and reading about and
07:55 - playing around with over the last few
07:57 - months and so I'm going to attempt to
07:59 - impart some perspective and attempt at
08:01 - this and I have a feeling that there's
08:03 - gonna be a lot of stopping starting and
08:05 - hopefully some helpful suggestions and
08:07 - corrections from the various chats that
08:09 - are going on here oh I didn't change the
08:11 - camera thank you sorry I have a bad I
08:14 - think I probably should do the whole
08:15 - sensor thing that just knows where I am
08:18 - okay so sorry about that um was I going
08:23 - to say so I'm going to get corrections
08:24 - bla bla bla
08:25 - there's something else that I want to
08:26 - say we're on camera math is talking
08:30 - camera okay you guys are all behind I
08:32 - fixed it I fixed it already okay so let
08:41 - me go into session three and linear
08:44 - regression so this one is now
08:50 - okay if I raise this up is that a
08:52 - problem a linear regression I didn't
09:01 - publish a the code from yesterday
09:03 - ordinary least squares and linear
09:13 - regression gradient descent and I get a
09:25 - web browser as always I always forget
09:34 - the dis stop this stop this stop this
09:36 - duck and then GT this stop this stop
09:38 - underneath this dis stop this dot this
09:40 - dot and then using this stock will stop
09:42 - using aqua stop just stop
09:45 - this starts on this table this dot just
09:49 - gotta get to the stop this stop the
09:53 - stock starts on this stop the stock just
09:57 - I never forget to this stop just stop
10:00 - this stock this done and then retain
10:02 - this stock the stock when you lose this
10:04 - this stock the stock just died and
10:06 - you're doing this stop the stop is is
10:08 - not this stop it stop it stop and
10:10 - they're using this stock the stock is
10:12 - use this stock is stock this time and
10:14 - then do this stop
10:19 - [Music]
10:28 - this time this dock is never forget
10:32 - - this stuff I'm gonna do this this dot
10:35 - this dot this dot this dot song never
10:38 - forget the Vince dot but somebody
10:40 - composed that song for me
10:42 - I'm just giving myself a little time to
10:44 - be silent for a second while I attempt
10:47 - to okay great so I now have this is the
10:49 - example from yesterday
10:50 - uh uh fidget spinner trust me
10:54 - I own there's a lot of fidget spinners
10:57 - in my household right now the bit it's a
10:59 - bit of a problem ok so this is what I
11:07 - did yesterday this is the example from
11:09 - yesterday where I am fitting a line
11:13 - let's see if I move this over a little
11:14 - bit actually it's fine it's fine where
11:16 - it is actually whoops nope give
11:20 - ourselves more room here just trying to
11:22 - get everything set up and let's go to a
11:29 - gradient descent we're gonna redo this
11:36 - so shoot hold on a row I'm just trying
11:42 - to get the code kind of ready I wonder
11:46 - if what I should do I wonder if it would
11:49 - be useful for folks I forgot to upload
11:54 - the code from yesterday to github let's
11:56 - see if I can manage to do that really
11:58 - quickly now so if people want to get it
12:00 - so github.com coding train rainbow code
12:04 - am I logged in I'm logged it that's a
12:07 - good sign this goes under nature of code
12:11 - oh boy Wow I don't even I need I don't
12:15 - know where I've got by the way anybody
12:17 - who wants to volunteer to help me
12:21 - organize and rethink this github
12:25 - repository possibly turning it into a
12:27 - a website where people can find listings
12:31 - of all the videos and all the code
12:32 - welcome contributions to thought there's
12:34 - a couple github issue threads people
12:36 - discussing that that said right now I
12:38 - think that this code goes under nature
12:43 - of code because I think technically
12:47 - speaking what what playlist am I in
12:51 - let's look at the numbering here I think
12:54 - I need to make a new I think I need to
12:58 - make a new playlist let's see here I'm
13:03 - sorry I'm like wasting everybody's time
13:04 - here intelligence and learning session 3
13:08 - that's what this is where I am so the
13:13 - videos would be intelligence of learning
13:14 - three point something alright that's
13:16 - fine we are going to this is very
13:19 - confusing how to do this but I am going
13:23 - to go into courses and I'm going to can
13:27 - I make a new folder right on github
13:29 - create new file can I do this
13:33 - intelligence learning readme whoops
13:38 - slash Rio I can read meat readme MD code
13:47 - foreign intelligence and learning
13:50 - tutorials let's commit that file then
13:57 - again I might redo this later but now
14:01 - what I'm going to do is upload and what
14:05 - I'm going to do is grab from yesterday
14:08 - this ordinary least-squares folder and
14:14 - so now I did commit sorry I didn't
14:17 - commit so close so close ordinary
14:22 - least-squares and I'm a hit commit
14:30 - it's amazing what you can do with get
14:33 - and github just using their website
14:35 - right now you know okay so if you want
14:39 - to get the code from yesterday to follow
14:42 - along with what I'm doing you would go
14:45 - to this github repo under courses under
14:51 - oh I put it in the wrong place I'll fix
14:53 - this later right now it's right here in
14:57 - linear regression ordinary least-squares
14:59 - so I got a number everything and put it
15:01 - in the right place but at least it's
15:02 - there right now you can go ahead and
15:04 - grab it it probably should just made it
15:05 - a gist but I did it okay so that's there
15:11 - now I want to do is get this code put it
15:19 - here remember when it was like seven
15:21 - minutes and I said I was about to start
15:22 - actually coding right okay so we are
15:28 - going to do this now great okay so
15:38 - here's the thing I'm gonna mention a
15:42 - couple books but I might mention these
15:46 - when I gets okay so first of all if you
15:49 - haven't watched how I say this every
15:52 - time but if this is your first time
15:54 - watching me live this is a live stream
15:57 - everything that I do today will be
15:58 - edited sometimes out of order into a set
16:00 - of smaller video tutorials that will get
16:02 - uploaded later
16:03 - so sometimes unfortunately there's a bit
16:04 - of redundancy I'll talk something
16:05 - through and then say it again but that's
16:07 - what probably gonna happen these are two
16:10 - books that I wanted to recommend one is
16:13 - make your own neural network by tariq
16:15 - rashid this has been helpful to me it
16:17 - has a really nice appendix also in the
16:19 - back
16:20 - that's kind of like a little
16:21 - introduction to calculus it's a little
16:23 - bit absurd to you know explain and talk
16:26 - about what calculus is in you know just
16:28 - like 10 to 20 pages it's even more
16:31 - ridiculous to do that in you know two
16:32 - three minutes of video that I might do
16:34 - but this I would also recommend for for
16:39 - anybody this is a book called calculus
16:41 - made easy
16:42 - this book is actually from what is the
16:47 - original edition looking for it
16:50 - originally published in 1910 it had
16:55 - subsequent editions in 1914 and 1946 and
16:58 - this is a update revision and
17:01 - republication of it in from I believe
17:04 - 1998 and it's a really it just has like
17:07 - all these like beautiful little diagrams
17:08 - in it maybe I can flip to a random page
17:11 - and do a little reading from it because
17:14 - it's just so I really find actually the
17:17 - first chapter on differentiating is kind
17:19 - of nice let's see what I can find here
17:23 - partial drip okay
17:24 - so partial differentiation is something
17:26 - that we need chapter two I love the fact
17:30 - that okay plus we're going to read this
17:31 - chapter one is titled to deliver you
17:36 - from the preliminary terrors page 39 the
17:46 - bedtime reading I'm going to read this
17:47 - to my children tonight at that time
17:49 - considering how many fools can calculate
17:52 - it is surprising that it should be
17:55 - thought either a difficult course the
17:58 - camera went up it's like a good place
18:05 - when I little stick was totally ruined
18:08 - there tries to get considering how many
18:15 - fools can calculate it is surprising
18:18 - that it should be thought either a
18:19 - difficult or tedious task for any other
18:21 - fool to learn how to master the same
18:23 - tricks so this is the prologue I don't
18:26 - wanna read the prologue I wanted to read
18:28 - to deliver you from prolific wait I need
18:30 - some like ominous bedtime music like
18:35 - I don't have any okay the preliminary
18:39 - terror which chokes off most high school
18:41 - students from even attempting to learn
18:43 - how to calculate can be abolished once
18:45 - and for all by simply stating what is
18:47 - the meaning in the common sense terms of
18:50 - the two principal symbols that are used
18:52 - in calculating the dreadful symbols are
18:54 - d which really means a little bit of the
18:58 - s' DX means a little bit of X or do you
19:01 - mean little bit of you ordinary
19:04 - mathematicians think of it more polite
19:06 - to say an element so instead of a little
19:08 - bit of just as you please but you will
19:11 - find that these little bits or elements
19:13 - may be considered to be infinitely small
19:17 - okay so clear yeah all right so the
19:24 - other thing that's very exciting look at
19:25 - this I developed this like massive
19:28 - technological leap forward in my live
19:30 - streaming capabilities yesterday when I
19:33 - discovered that I could use not one but
19:37 - two whiteboard colors and now I have
19:40 - purple and blue and green so I'm hoping
19:43 - to make use of all these different
19:45 - markers today now and now here we are we
19:53 - are ready to begin let's where's my
19:56 - water
20:04 - this is a very strange water that I'm
20:05 - drinking as I did have some coffee in
20:07 - this mug earlier if coffee was all gone
20:10 - but I didn't like completely rinse it
20:11 - out and I just fills it up with water
20:13 - through there's like this faint air of
20:16 - Oda cafe I guess it's cafe um that sort
20:21 - of still in here okay
20:25 - it's 308 p.m. I have until I have until
20:31 - I would say I'm hoping to finish around
20:33 - 4:30 p.m. but I have a little 15 to 30
20:36 - minute grace period if I need more time
20:37 - but I I'm pretty committed today to
20:40 - getting through this was my list from
20:42 - yesterday and I did these two things I'm
20:44 - gonna jump to 4 first I think it's
20:46 - important for me to attempt to do that I
20:48 - guess rip off the band-aid actually had
20:49 - a band-aid on earlier so it's already
20:52 - off rip off the band-aid and do a
20:54 - gradient descent ok can you put the code
21:01 - on something like Dropbox instead of
21:03 - github I can't do that right now I'm not
21:05 - sort of set up to do that but I'm not
21:06 - opposed to doing something like that
21:07 - another time if that's helpful to people
21:09 - ok I'm looking at the chat I don't see
21:13 - anything - I'm the wrong camera again
21:15 - this monitor that I have to keep track
21:18 - of where I am it's really not I think I
21:19 - need to put up a big one just up there
21:21 - in the wall where I'm looking straight
21:22 - ahead if I look up right now does it
21:25 - appear as if I'm looking at you or can
21:27 - you really tell that I'm looking up
21:28 - because here now I'm really looking at
21:30 - you if I look like slightly above anyway
21:33 - I'll figure this out later on my own
21:35 - time actually I don't never in this room
21:37 - unless I hit the streaming buttons I
21:39 - have to figure everything out
21:40 - while I'm live I don't know what is
21:44 - islets let's get a nice hold on let's
21:49 - get a nice little grip let's get a nice
21:51 - linear regression going here with some
21:53 - points kind of would like it to be more
21:59 - angled ok this is perfect
22:06 - yes I know I oh okay okay I better not
22:13 - look at the chat every once in a while
22:14 - somebody writes a mean comment to the
22:16 - chat I'm really just trying my best it
22:20 - is the Internet after all okay here we
22:23 - go
22:24 - hello okay so this is now another video
22:28 - in my series about linear regression now
22:30 - why are you watching these videos I'm
22:32 - not really sure to be honest but the the
22:36 - topic here and the skills here I hope
22:38 - are laying a foundation for what I'm
22:40 - going to get to in future videos which
22:42 - is building a neural network based
22:44 - machine learning system so at the top of
22:46 - this video why are we making another
22:48 - video about linear regression so what I
22:50 - did in the previous two videos is I
22:51 - created a p5.js sketch which implements
22:55 - linear regression using the ordinary
22:57 - least-squares method so a statistical
22:58 - approach there are a whole bunch of data
23:01 - points into the space and I try to fit a
23:03 - line that that fits to that data as best
23:06 - as possible so that I could predict new
23:08 - data points in this space and you can
23:10 - see as I start to click around how the
23:12 - line is fit sort of changes now I also
23:15 - discussed a little bit of long does
23:17 - linear regression make sense based on
23:19 - your data and these are big important
23:21 - questions in working your data science
23:23 - and machine learning but right now we're
23:24 - just trying to focus on the techniques
23:25 - now one thing you'll notice is here is i
23:28 - refresh this page I make I click twice I
23:31 - get a line instantly because I am
23:32 - actually calculating the perfect exact
23:36 - best fit line according to the least
23:38 - squares method but someday we will have
23:43 - a data set that's not 2-dimensional
23:46 - someday we will have a data set that has
23:49 - hundreds that's a big data set that's a
23:52 - many many dimensional and in this case
23:55 - there isn't going to be an easy
23:58 - statistical approach that could just be
24:00 - done to fit to create a model that fits
24:03 - the data perfectly with a single
24:05 - calculation on the so this is this is
24:10 - the problem that machine learning neural
24:12 - network based deep learning based
24:14 - systems are here to solve to figure out
24:18 - a way to create
24:19 - model to fit a given data set and one
24:22 - technique for doing that which is
24:24 - different than say ordinary
24:25 - least-squares is to use a technique
24:28 - called gradient descent and what
24:30 - gradient ascent descent essentially does
24:32 - it says let me make a guess I'm just
24:34 - going to put the line here and I'm gonna
24:36 - see how is that line good or not so good
24:39 - that's so good let me try shifting it a
24:41 - little closer to the data
24:42 - let me try shifting it a little closer
24:43 - to data let me shift it let me shift it
24:45 - so making lots of little small nudges
24:48 - and tweaks into what that line is doing
24:50 - I think I have a better way of
24:51 - explaining that so I'm going to come
24:52 - over here to the whiteboard I'm going to
24:54 - do a little magic here which is that I'm
24:55 - going to stand right over here and I'm
24:59 - going to snap my fingers and the moment
25:01 - I snap my fingers the whiteboard behind
25:03 - me is going to be erased now for all of
25:07 - your watching that's live that clearly
25:08 - didn't happen but I'm setting up
25:10 - something I'm gonna race it I thought it
25:14 - was going to use this to explain
25:14 - something but I realized that was
25:18 - unnecessary so I don't know talk amongst
25:22 - yourself for a little bit I'm gonna I'm
25:26 - going to use this whiteboard cleaner I
25:33 - guess I should play some music or
25:36 - something I guess I could maybe get the
25:39 - kitten song going unicorns and rainbows
25:41 - and explode else is there
25:46 - yes kids thank you very much kittens and
25:48 - rainbows and cupcakes notice that look
25:51 - what I get I really
25:57 - [Music]
26:43 - I'll get my son Cody Creek
26:45 - [Music]
26:48 - okay uh here we go now what do I need to
26:53 - do I need to do I have a marker in my
26:55 - hand I can't even remember no I didn't
26:57 - because I just walked over here I was
27:03 - standing right about here Wow Oh does
27:08 - that work
27:10 - dorks this is kind of interesting okay
27:12 - so um let's I'm going to make this same
27:16 - kind of diagram that I've made a few
27:18 - times now and I'm going to we're going
27:20 - to like really simplify it so we have
27:23 - this idea of this two dimensional space
27:27 - there is some piece of data we'll call
27:32 - it X for example the temperature that it
27:35 - is outside today and we're trying to
27:37 - predict some outcome maybe based on that
27:40 - temperature yesterday we talked call
27:42 - that Y we heard about that of the sales
27:46 - of ice cream I still actually a dataset
27:47 - that was interesting about like the
27:49 - frequency at which crickets chirp
27:51 - according to the temperature outside
27:52 - that's the data set you can find online
27:54 - somewhere use so we have this idea maybe
27:57 - there's some existing data points based
28:00 - on ice cream store that we have studied
28:03 - and I can graph that data so the idea
28:08 - here is that we have our machine
28:11 - learning recipe we are going to take I
28:16 - know I'm out of the frame here we're
28:17 - going to take one of our inputs called X
28:22 - feed it into the machine learning recipe
28:25 - and the machine learning recipe is going
28:27 - to give us a prediction Y so we have
28:32 - known data and we can if we had new
28:35 - input data we could make a guess take a
28:40 - break for a second is this where I want
28:42 - to be going here uh yeah I think so I'm
28:49 - just thinking about this for a second
28:52 - hydrate someone gave me that I knew the
28:54 - camera was about to shut off
28:56 - what is life going to be like someday if
28:59 - the cameras don't shut off for third
29:01 - some highlights I don't think I'll be
29:03 - able to function okay okay I feel like
29:16 - I'm off here in where the camera view is
29:18 - you see if I can turn it a little bit
29:20 - this way because I like to be able to
29:26 - stand more over here okay great this is
29:28 - better now okay okay so yesterday my
29:36 - machine learning recipe was the ordinary
29:39 - least-squares method meaning I was able
29:41 - to do a statistical analysis of all this
29:44 - data and create the line of best fit and
29:47 - then if I had a new input you know X
29:50 - value of such-and-such I could look up
29:53 - its corresponding spot on the line and
29:55 - that would be the Y output this is a
29:57 - function the machine learning recipe is
29:59 - essentially solving for M and B in the
30:03 - equation of a line so that's what I did
30:06 - yesterday today's technique I want to
30:09 - demonstrate the technique known as
30:11 - gradient descent so the idea of gradient
30:18 - descent is okay so boy so much to say
30:22 - where should I start where should I end
30:25 - I really have no idea so one thing I'll
30:27 - mention is that the math required for
30:30 - gradient descent typically involve
30:33 - calculus and they involve two concepts
30:37 - from calculus one called a partial
30:39 - derivative which if you don't know
30:40 - calculus or what a derivative is well
30:43 - how can you be expected what a partial
30:44 - derivative is as well as a something
30:49 - called the chain rule and I think what
30:51 - I'm going to do is I'm going to walk
30:52 - through this entire system and how it
30:55 - works and explain it without diving
30:58 - deeper into the math um but I will want
31:06 - to make a follow-up video where I
31:07 - discuss some of those pieces in a bit
31:09 - more detail so so okay so but here's a
31:12 - way that you could think of
31:13 - gradient descent thats related to stuff
31:16 - that i have done in previous videos and
31:17 - in my book nature of code where i
31:19 - reference the work of Craig Reynolds
31:22 - steering behaviors so think about this
31:24 - for a second this is this is great let's
31:26 - say you have a two-dimensional space and
31:32 - you have a vehicle that is moving around
31:35 - an agent that is moving around this
31:37 - space and the vehicle has a particular
31:41 - velocity expressed as a vector or an
31:45 - arrow in this case now what if the goal
31:48 - of this vehicle is to reach this target
31:53 - well we could say that this vehicle has
31:57 - a desired velocity its desired velocity
32:01 - is to move at maximum speed from its
32:05 - current location to towards the target
32:08 - so this is a vector which is its desired
32:11 - velocity and if you and you can think
32:14 - about the difference like the its
32:16 - current but this vehicles current
32:18 - velocity is like its guess I don't know
32:20 - where I should go I'm gonna try going
32:21 - this way oh but really I should go this
32:23 - way well if I'm going this way but I
32:25 - really should go this way what if I just
32:27 - turn a little bit towards the target
32:29 - what if I were to just steer a little
32:32 - bit in that direction and this is what
32:34 - gradient descent does you can think of
32:37 - desired as the known output the correct
32:41 - output what if I feed in one of these
32:45 - data points right and I say look at this
32:51 - particular X Y pair let me feed it in
32:55 - let me try to get a guess just sometimes
32:58 - I think written is why a tick I think
33:01 - but I'm going to say what if I get Y I
33:03 - must say Y guess the error is the
33:09 - difference between what I guessed it
33:13 - would be minus what it actually should
33:16 - be right if I start with an XY pair this
33:19 - is the error and you'll notice if you
33:21 - look at Craig Reynolds steering
33:22 - behaviors and all of these animate
33:24 - systems that I that I implemented from
33:26 - that work you'll see there's a formula
33:28 - in it steering equals desired minus
33:34 - velocity so you know I put I put it get
33:38 - in you know I kind of do the reverse
33:42 - here because this is really the
33:43 - equivalent of desire but the point is
33:45 - the difference between the way that I
33:47 - should go and the way that I am going
33:49 - that's the error the difference between
33:51 - what my machine learning recipe what my
33:53 - model currently thinks the output should
33:55 - be compared to the known output that is
33:58 - the error and steering Ike if I adjust
34:01 - my velocity if I steer towards the
34:04 - desired I'm going to get a better model
34:06 - I'm going to move towards the target if
34:08 - I use this error to tweak the parameters
34:10 - of the machine learning recipe I'm going
34:13 - to make my model I'm going to have
34:15 - better M and B values for the next time
34:18 - and I could do this over and over and
34:19 - over and over again and this is we've
34:22 - been talking about this supervised
34:24 - learning I can take the known data send
34:28 - it in get a guess look at the error
34:31 - tweak the knobs send the next data point
34:34 - and get a guess look at the error tweak
34:35 - the knobs I can do this over and over
34:37 - and over again and I can just start with
34:40 - random values for M and B so I don't
34:43 - know what it B I'm going to just put a
34:45 - line here and then I can start moving
34:46 - the line around according to the error
34:48 - as I go through all the data so this is
34:50 - what we're trying to do okay I'm going
34:54 - to pause for a second and see if see if
35:00 - anybody's got any comments or anything
35:03 - in the chat they want to like mention
35:04 - any Corrections that I'm going to move
35:06 - on so I think what I want to do next I
35:14 - think I'm going to start just adjusting
35:15 - the code yeah
35:27 - okay uh all right so change camera Oh
35:36 - Suraj is watching oh no don't don't
35:41 - watch it's gonna be all be wrong I'm
35:43 - going to delete this from the internet
35:45 - we square the difference as well to keep
35:48 - it positive and preserve the magnitude
35:50 - ah
35:50 - some of the squared errors yes this is a
35:52 - good point so I should mention this okay
35:56 - so actually a good point thank you ah
35:59 - thank the Internet thank god that's my
36:04 - thank you sir thank you sorry you guys
36:06 - should watch Suraj's channel especially
36:08 - for all of you are like how come you're
36:10 - not doing Python Java Python Python
36:13 - Python well I'm - someday but go watch
36:18 - Suraj Jenna lots of Python there um so I
36:21 - forget to mention something which is
36:22 - important especially once I start
36:24 - getting the math I don't know if I like
36:25 - to say maths but now I worry that I'm I
36:28 - said this yesterday that I'm sounding
36:29 - pretentious it's a bit of a problem okay
36:31 - so we I'm going to come back into my
36:33 - tutorial okay um okay so I should
36:45 - mention about right cuz I should mention
36:47 - about squaring the error the thing about
36:51 - it is yeah actually I don't I'm gonna
36:55 - come back to that later because when I
36:57 - do the follow up I'm going to so there's
36:59 - something else that I need to mention
37:00 - which is that there's stochastic
37:03 - gradient descent and batch gradient
37:11 - descent so I could adjust the knobs so
37:15 - to speak for each data point one at a
37:17 - time or I could process all the data
37:20 - look at the sort collective error and
37:22 - then adjust the knobs I don't know I
37:29 - don't know what to do here I should
37:30 - probably look at the chat again but I I
37:33 - think I'm going to come back to the
37:34 - squaring of this when I dive deeper into
37:37 - it yeah
37:41 - okay all right let's see here
37:46 - all right so I'm gonna I'm gonna keep
37:48 - going I'm gonna go because what I'm
37:51 - gonna do is I'm boom
37:54 - yeah there's cuz there's the hole right
37:57 - there's the hole error graph thing and
37:59 - square you know you change this
38:01 - parameter how does it look at this slow
38:02 - but I think I'm going to come back to
38:03 - that I think I'm going to leave that out
38:06 - of this sort of initial pass where's my
38:10 - eraser I'm not sure I'm very unsure
38:14 - about this but I think what I'm going to
38:15 - do is I'm just going to dive in and
38:17 - start programming it and then I'll come
38:19 - back and maybe follow up with looking at
38:22 - sort of like a graph of the error and
38:24 - talking about slope because I think I'm
38:26 - going to save the derivative stuff for
38:28 - like a follow up to if people want to go
38:30 - a little further into it everybody okay
38:34 - with this plan oxfordshire about it okay
38:37 - anyway so now I'm back thank you for the
38:41 - thank you to Matt you who helps to edit
38:43 - a lot of this stuff after the fact so
38:45 - I'm going to pretend it pretend I'm
38:47 - going to walk back and start adding to
38:48 - the soup of the cook okay so there's
38:51 - more to how the math behind this stuff
38:53 - works and how we look at the overall
38:55 - error and there's some stuff that
38:57 - involves the derivative and the slope of
39:01 - the graph of the error and I'm gonna I
39:02 - think I'm going to come back to some of
39:04 - that stuff in a second video where I go
39:06 - a bit further into some of the math here
39:08 - but what I'm actually going to do is
39:09 - just start showing you how to set up to
39:11 - do gradient descent in the code itself
39:13 - so let me come over here so this as you
39:16 - saw before this is the example from
39:18 - yesterday that's using the ordinary
39:20 - least-squares method so what I'm going
39:24 - to do now is I am going to this so I had
39:31 - this function linear regression and this
39:32 - linear regression function calculates
39:35 - the slope of the line and the
39:38 - y-intercept and and B according to or
39:41 - nearly squares so what I'm going to do
39:44 - is I'm just going to completely get rid
39:48 - of this so now nothing happens there
39:53 - so I can click and the first guess of
39:57 - the line I just plugged in some values
39:58 - that typically speaking I think what's
40:00 - probably typically done is these values
40:02 - are initialized at zero these are like
40:05 - weights so to speak and ultimately you
40:07 - can see these are analogous to the
40:08 - weights of connections in a neural
40:10 - network but this M and B values I could
40:13 - start with them randomly I could pick
40:15 - something and hard-code it I could get
40:17 - that the both be zero I think I'm gonna
40:18 - stick with actually one comma zero just
40:20 - to sort of see because then at least I
40:21 - can see that the line is there so now
40:23 - what I want to do is I want to look at
40:25 - with the existing data points I want to
40:29 - look at the error and I want to adjust N
40:32 - and B in the direction of the error so
40:35 - let's see how that goes so I'm going to
40:38 - call this now gradient descent and so in
40:45 - the draw function I think I want to call
40:48 - this now gradient descent I hear a noise
40:52 - in the hallway who may it be I won't go
40:56 - over there to check its middle making a
40:58 - video it's okay um it's just very
41:02 - distracting gradient descent I'm a
41:06 - person in a world in a world where I'm
41:10 - programming linear regression with
41:12 - gradient descent I've lost my mind this
41:16 - is gonna get edited out I have to go
41:17 - away oh oh I hurt my knee also so I
41:19 - can't really bend it okay um I'm back a
41:31 - little digression there that I had to
41:32 - edit out thanks for uh thanks for tuning
41:36 - in ok so where I am is that I'm changing
41:39 - the name of the function to gradient
41:40 - descent and what I want to do is I'm
41:43 - going to just look through all of the
41:45 - data so let's just first look through
41:52 - all the data and okay so for each data
41:57 - set I have the Y is data index i dot y
42:01 - so we can get the X and the y
42:06 - and I can actually calculate a guess so
42:11 - my guess is M times X plus B right this
42:16 - is my machine learning recipe I am
42:18 - taking the input data X I am multiplying
42:21 - it by n I am adding B and that is my
42:23 - guess so now my error equals my error
42:30 - equals y minus the guess and I think
42:34 - technically speaking I think I should be
42:36 - saying guess - why now you'll you may
42:40 - recall that in the ordinary
42:42 - least-squares method I would always
42:44 - square the error because I want to get
42:47 - rid of the sort of positive or negative
42:48 - aspect of it in this case and again I'm
42:51 - going to go a little further into this
42:52 - in the next video I actually want the
42:54 - positive or negative direction of the
42:56 - error because I want to know which way
42:58 - in essence to tune the M and B values to
43:02 - get a better result so the issue here is
43:06 - now that and this is what's known as
43:08 - stochastic gradient descent so I want to
43:10 - make an appoint that's available I want
43:14 - to make a change to M and B so I need to
43:21 - calculate how should I change M and how
43:24 - should I change B so really what I'm
43:25 - saying is M equals M plus some amount of
43:28 - change B equals B plus some amount of
43:31 - change and we can in this case kind of
43:33 - say this is a one way to think about and
43:36 - understand it I have this error whose
43:38 - respondent who is to blame here is it um
43:41 - is it UB who's in charge here
43:44 - what's the what's going on I got to
43:45 - figure this out so in essence we could
43:48 - say if I adjust those values according
43:50 - to the error maybe if I tried it again I
43:52 - would get a better result and in this
43:54 - case B can be adjusted directly by the
43:57 - error because it's just the y-intercept
43:59 - should I move it up or down and M which
44:02 - is the slope can be adjusted by the
44:04 - error but according to according to also
44:07 - the input value itself so this is how
44:10 - you can kind of intuitively understand
44:11 - it I want to adjust those values
44:13 - according to the error the slope also
44:15 - relates to what the input actually was
44:17 - the y-intercept
44:19 - just the air itself now so I'm missing a
44:21 - whole bunch of steps and a bit a few
44:22 - pieces of explanation here but let's
44:24 - just run this and see what happens so
44:29 - first I always have to click ok well
44:31 - first of all I got it error uncaught
44:33 - reference error and is not defined in
44:35 - gradient descent where did I have n Oh B
44:38 - equals B plus error yeah I don't know
44:40 - what n is so you can see like okay well
44:43 - I don't know where that line went it was
44:45 - there for a second and it just went far
44:48 - away so here's the thing if I come back
44:50 - to my analogy from the steering one of
44:55 - the things in the steering behavior
44:57 - examples from nature of code and Craig
45:00 - Reynolds examples is that there was a
45:01 - variable called maximum force know if
45:06 - you can see that maximum force because
45:08 - one thing you might think about it here
45:10 - so well how powerful I know what the
45:11 - error is between the way I'm going and
45:14 - where I want to go how powerful is my
45:17 - ability to turn well maybe I've able to
45:19 - turn at like imp with infinite power and
45:22 - that could be good but not so good
45:24 - because if I try to like push myself I
45:25 - might end up going all the way down this
45:27 - way then I'm like oh my god going in the
45:28 - wrong direction and then they end up
45:30 - going all the way up in the other
45:31 - direction maybe I just won't really want
45:33 - to be able to make little adjustments
45:34 - because it's the wrong way I want to
45:36 - just make a slight adjustment I don't
45:38 - want to overshoot the target this target
45:40 - being I want to find the parameters I
45:43 - want to find the weights the M and B
45:45 - values to minimize the error so so I
45:49 - don't want to overshoot what that
45:51 - minimum that that optimal value is and
45:54 - so that is where a variable sometimes
45:56 - called alpha but most commonly called
45:58 - learning rate comes in so I could have a
46:01 - variable called learning rate usually
46:02 - this is a small number something to
46:04 - really reduce the size of that error so
46:07 - in this case I would say well let me
46:10 - take this change in the value of the
46:12 - slope and multiply it by the learning
46:15 - rate and let me change take this for B
46:18 - and multiply it by the learning rate
46:21 - okay so now I'm going to try this again
46:24 - with a learning rate of point zero zero
46:26 - one
46:29 - hey that
46:31 - does it look right come back to me okay
46:35 - so let's think about what might be wrong
46:41 - the problem is I this is my like music
46:44 - for consider your thinking and then I
46:46 - just decide like I'm doing my ticket
46:48 - like and then I don't say I'm like often
46:50 - thinking about what I'm gonna have
46:51 - dinner these are like sickened rain
46:53 - tomorrow and have time to go jogging
46:55 - every what's right one chapter of Harry
46:58 - Potter my on again you know I pulled it
47:00 - behind okay so I was also waiting for
47:06 - somebody in the chat to give me the
47:08 - correction and I most likely I have a
47:10 - positive or negative problem here which
47:12 - is that probably and moving in the wrong
47:15 - direction
47:15 - so whenever that happens I could just
47:17 - switch one of these two - I could also
47:20 - just say it oh yeah actually over here I
47:24 - wrote guess - why and that's really what
47:30 - I that's what I wrote here no I want why
47:32 - - guess I knew it was always the same so
47:37 - hopefully you're not watching this but
47:41 - in this case here right
47:43 - steering if I want to move towards the
47:45 - target the error is the desired the
47:48 - known result - the velocity and so this
47:53 - should really be if I want to move in
47:54 - that direction why - why
47:57 - guess and so let me come back over here
48:01 - and I'm going to do that camera went off
48:10 - this do-over we come back over here and
48:15 - let me change that to I changed it
48:18 - already wait how did I do that
48:19 - okay I must have done it before then I
48:21 - went to explain it so let's try this
48:28 - looks pretty good right now here's the
48:31 - thing let's start let's start with M at
48:34 - zero because that was a little bit weird
48:35 - what just happened there so over time
48:40 - something is definitely wrong here and
48:43 - it might just be that my learning rate
48:46 - isn't good let's try a little bit of uh
48:52 - I'll have to think about this oh yeah
48:59 - why
49:04 - this is better now I'm I'm gonna come
49:07 - back yeah I guess I guess I'm doing this
49:09 - right it's really about the learning
49:12 - right here I guess all right if I click
49:15 - here I'm going to that line is going to
49:16 - stay and if I click here yeah okay
49:21 - hold on so let me go back to this Thank
49:25 - You much yeah this one's definitely need
49:27 - to be aggressively edited okay let's go
49:33 - so okay so let me look I let me put M
49:38 - and B back to zero
49:40 - let me put em in B back to zero hit
49:45 - refresh here and so let's see um so we
49:49 - can see interestingly enough this isn't
49:51 - the correct correct line because the
49:54 - line should really go through those two
49:55 - points you know I think I've got an
49:57 - issue here with the learning rate so you
50:00 - can see how it was kind of like moving
50:02 - to the right spot but then it's still
50:03 - making very very small small changes
50:06 - only have two points not a lot of data
50:08 - it's not a lot of time for it to change
50:09 - I probably just need kind of a larger
50:11 - higher learning rate here just for this
50:13 - demonstration let's make it at 0.05 and
50:17 - we can see now it's kind of moving much
50:18 - more quickly and it's starting to turn
50:21 - albeit very slowly but you can see as
50:24 - it's slowly slowly turning approaching
50:26 - the correct the correct or the optimal
50:29 - spot for this line and as you can see if
50:31 - I were to click again and click again
50:32 - try to you know click a lot up here and
50:36 - a lot down here ultimately eventually I
50:40 - should start getting the line of best
50:42 - fit now so there are you know there's
50:45 - some strategies that in theory you
50:46 - should really need to adjust the
50:48 - learning rate over time but there is a
50:50 - technique and a lot of machine learning
50:52 - systems that you will say see that you
50:53 - can call annealing I think that's the
50:56 - right word where you kind of start with
50:58 - a high learning rate and then slowly
51:00 - over time reduce it so you can kind of
51:02 - get some big Corrections at the
51:03 - beginning and then find some some
51:05 - smaller Corrections so but pause here
51:10 - for a second error equals I'm looking at
51:16 - the chat
51:19 - um to see if I'm missing anything
51:22 - because this I you know I think
51:26 - simulated yeah I have a feeling this is
51:31 - going to perform much better once I do
51:34 - the stochastic I'm sorry the batch
51:38 - gradient descent there's probably an
51:40 - issue with let me see my I have some
51:43 - examples already pre-baked let me see
51:46 - how those let me just make sure to see
51:55 - if there's anything I'm really missing
51:57 - here so this was my example that I made
52:15 - yeah this is kind of the same
52:18 - interestingly enough so this looks
52:21 - similar to what I just demonstrated
52:23 - right now right it's behaving the same
52:25 - way let me look at what values I used in
52:30 - this particular example where is this
52:37 - these are the examples from my course
52:39 - from a while ago why did that not open
52:46 - correctly that's annoying
52:54 - there we go so let me go into this code
52:58 - I just want to compare this code before
52:59 - I come back into the actual video I have
53:02 - like a learning rate slider so this is
53:07 - is this doing the batch right now sorry
53:13 - this is hard to see yeah so this is
53:17 - actually doing the batch delta b yeah
53:24 - why - why guess + equals Delta B so this
53:29 - is a different example which I'm going
53:30 - to do next
53:32 - the bigger the error the bigger the
53:34 - learning rate yeah well okay so wait put
53:43 - two dots right above each other I'm
53:45 - coding along and it's not adjusting
53:47 - let's try that so so you mean if I do
53:52 - this yeah that's not good so let's so
54:07 - hold on let me go let me close this I
54:09 - just want to make sure my example that I
54:11 - made previously didn't do anything so
54:13 - let's give it a very high learning rate
54:24 - I'm just wasting what it's going to do
54:26 - here
54:50 - you know I think we got it close enough
54:53 - okay the the thing about this is I'm
54:56 - less concerned about uh you know there
55:06 - unless here's the thing I'm not looking
55:09 - in this particular example to
55:13 - demonstrate the optimal way to fit line
55:17 - to 2d data I just want to look at the
55:20 - idea of linear regression with a
55:23 - statistical approach and we grade in two
55:25 - cent to set a foundation of knowledge
55:27 - for what I'm going to do in future
55:29 - videos so um
55:32 - oh wait collinear knees dad's regression
55:37 - anyway so there you go should work like
55:43 - wait that's not a that's not a uh yeah
55:48 - so that's yeah exactly
55:50 - thank you okay so where am i where was I
55:53 - in this tutorial I was kind of like
55:55 - checking I think I talked about the
55:57 - learning rate I changed it to 0.5 and I
56:00 - was kind of done ok um okay so some
56:05 - folks in the chat we're asking about
56:07 - like okay well it's sort of performing
56:08 - weirdly if I put a lot of like points
56:10 - above and below but if I put you know
56:13 - points to the right and left it's kind
56:16 - of it fits the line very nicely you can
56:19 - see the Toxic now I'm doing above and
56:21 - below again so here's the thing um
56:23 - collinear T meaning like a lot of
56:25 - vertical points is not really good this
56:27 - this isn't real this data doesn't really
56:28 - make sense for linear regression if I
56:30 - were trying to make predictions so we're
56:32 - not necessarily going to get a good line
56:33 - and part of what I'm doing again is not
56:35 - to demonstrate the optimal way to do
56:37 - linear regression but to demonstrate the
56:39 - technique known as grading a set descent
56:41 - of making small adjustments to weights
56:44 - to parameters to the slope and
56:46 - y-intercept based on an error based on
56:50 - the supervised learning process so this
56:52 - is a start to that you could stop here
56:54 - and I highly recommend that you do
56:56 - because what I'm going to do in the next
56:58 - video I don't really know how it's going
56:59 - to go to be honest but I'm going to try
57:02 - to look a little bit more closely as to
57:05 - why this works out the way that it works
57:09 - how do I know how to change em and be
57:14 - how do I know exactly how to change em
57:16 - and B to minimize the error I said kind
57:20 - of well the error kind of gives us the
57:22 - direction in which to change this has to
57:24 - do with calculus it has to do with
57:26 - comparing how changing one variable
57:28 - affects another variable so if I change
57:32 - m how does that change the error and can
57:35 - I look at the slope of a graph perhaps
57:38 - to see how to move along that graph to
57:41 - minimize that error so this is what I'm
57:43 - going to cover a bit more in the next
57:44 - video I'm not going to really I'm gonna
57:46 - actually also change this is I said I
57:48 - think I said this stochastic gradient
57:50 - descent meaning I'm adjusting the
57:52 - weights I'm adjusting the M and B values
57:54 - with every data point but I could also
57:57 - look at the sort of error in totality
57:59 - and then adjust the weight all at once
58:02 - at the end of one cycle through all of
58:04 - the data and that's known as batch
58:06 - gradient descent so I'm going to do I'm
58:08 - going to do is explain a bit more about
58:10 - the math here and then I'm going to do
58:12 - it and change the code to batch gradient
58:13 - descent in the next video it might be
58:16 - many parts to be honest with you but I
58:19 - don't know what how it's gonna go maybe
58:20 - this videos not going to exist the next
58:21 - one you can look see if it's there
58:22 - because I don't know if I should really
58:24 - make it okay see you soon thanks for
58:25 - watching this okay umm yeah alpha writes
58:33 - in the chat I'm just learning how to
58:37 - program so all of these videos are kind
58:39 - of overwhelming I do apologize I'm kind
58:41 - of on this you know I wanted to make a
58:43 - set of videos that go along with a bunch
58:46 - of topics for this course that I taught
58:48 - this semester some of them are not as
58:51 - useful perhaps what is useful or as
58:53 - exciting or just replace all as some of
58:56 - the other stuff that I've done terms of
58:57 - making games or generative algorithms
58:59 - and some of them also do require a depth
59:02 - of knowledge or experience with certain
59:04 - coding topics that are beyond what some
59:06 - of you who are watching are if you're
59:07 - beginners and new to code so I don't
59:10 - know how to best
59:11 - handle this what I'm hoping to do this
59:13 - summer is kind of like take a week off
59:14 - from machine learning and go back to
59:16 - just some simpler more creative projects
59:19 - and then go back to machine learning so
59:20 - we'll see what I do next week but
59:22 - feedback thoughts suggestions all that
59:23 - is quite welcome
59:27 - okay please do something with this sound
59:30 - to be a bit louder so hopefully I don't
59:35 - know so it's personal to say so how was
59:37 - that
59:37 - so let me do a little inquiry here
59:40 - little poll I could do a strop oh really
59:44 - quick I'm just gonna a formal anecdotal
59:49 - poll if you're watching this and had
59:53 - never learned about gradient descent
59:54 - before did that make sense with this
59:57 - helpful or maybe you learned about it
59:58 - before did this add something I'm just I
60:00 - don't know I guess I'm looking for a
60:01 - little feedback here okay so I'm going
60:09 - to think about how much time I have left
60:15 - today those needleless parentheses make
60:17 - me uncomfortable I totally agree these
60:20 - parentheses are awful oh I should get
60:23 - rid of them I'm going to get rid of them
60:25 - now okay thank you uh Mike in the chat
60:31 - who writes it was helpful learn
60:34 - something new helpful okay good Dawid
60:40 - exact could you get this into 3d and
60:43 - teach a bit about tourniquets okay um
60:53 - all right so now what I'm going to
60:56 - attempt to do is talk about let's let's
61:01 - let's make a plan for this everybody so
61:04 - here are the things I need to know what
61:08 - calculus is I need to know what a
61:12 - derivative is I need to know what the
61:15 - chain rule is and what a partial
61:17 - derivative is try making videos on that
61:22 - stuff and then with all those pieces
61:24 - we can actually like sort of derive this
61:27 - formula why not right okay so I think I
61:33 - need to erase this whiteboard this is a
61:35 - terrible idea I should have left it
61:37 - where it was I should just say I'll put
61:41 - links links in the video's description
61:44 - to more detail about the math behind
61:47 - getting those values let's let's try it
61:49 - all right let's try it
62:03 - what's in what's a succinctly to say
62:11 - actually this is a good metaphor a
62:15 - metaphor is not the writer analogy here
62:17 - to look think about something move like
62:19 - a runner or car moving for speed
62:22 - position to think about that as as kind
62:34 - of a foundation for the what a
62:38 - derivative is and what calculus is
62:45 - calculus like I think right couldn't I
62:48 - say that it's the study of how changing
62:54 - one variable affects another variable or
62:58 - is that actually too narrow because
63:02 - that's really only thinking about
63:06 - derivative as opposed to integration but
63:09 - integration just being the inverse there
63:13 - [Music]
63:15 - throw three blue one Brown that is a
63:17 - great that is a great that's a great
63:24 - YouTube channel actually I watched some
63:26 - of those videos okay okay so here we go
63:40 - where's my calculus book if the chat of
63:48 - Jim Wright's got a calculus exam next
63:51 - week would very much enjoy your teaching
63:53 - it oh boy
63:53 - this is not going to help I am not
63:56 - qualified to help anybody with their
63:58 - calculus exam I think you don't have to
64:01 - do this video about couch boots
64:02 - could we get three blue one brown for a
64:04 - guest tutorial that would be great
64:06 - somebody maybe put that out into the
64:08 - universe on Twitter or something we can
64:10 - make that happen okay
64:14 - all right I'm going to I'm going to
64:19 - against my better judgment I'm going to
64:32 - I'm going to talk a little bit about
64:35 - calculus this is a terrible idea
64:44 - chapter three look just don't pick don't
64:47 - like me I'm just gonna be reading this
64:48 - book chapter three chapter one wait no
64:57 - no one chapter three chapter two I love
65:00 - this book is so nice hmmm constants and
65:06 - variables fixed value called constants
65:08 - burials okay okay okay
65:10 - all right all right all right all right
65:11 - all right all right all right here we go
65:16 - all right here we go okay here we go
65:20 - everybody
65:31 - hello are you watching this video if you
65:34 - are please turn off your computer but go
65:37 - to something else this is gonna be a
65:38 - problem because I'm gonna do something I
65:41 - have no way qualified to do but I just
65:43 - did send tutorials about linear
65:44 - regression 1 using ordinary
65:46 - least-squares method which is a
65:47 - statistical approach another using a
65:50 - technique known as gradient descent
65:51 - which I here I've been told
65:55 - behind the scenes uses something called
65:57 - calculus this is a book that I really do
65:59 - love called calculus made easy
66:01 - it's from 1910 this is a reprint of it
66:04 - as a nice foreword by Martin Gardner and
66:08 - I recommend I recommend this book I've
66:11 - been reading it it's been a delight to
66:13 - delight has wonderful little drawings in
66:15 - it and it's it's a completely absurd
66:17 - that I might even attempt to say that oh
66:20 - I'm gonna make a 5 15 20 30 minute video
66:23 - and I'm going to explain calculus or
66:24 - something like that so I'm going to kind
66:25 - of narrow my focus and talk in
66:27 - generalities and then in some specifics
66:29 - and try to give you a sense of the the
66:34 - aspects of calculus which by the way
66:37 - calculus calculate those words are
66:39 - similar doesn't need to be some scary
66:41 - weird thing uh that I couldn't possibly
66:43 - ever do and how it relates as for any of
66:46 - you who are interested in a bit more
66:47 - behind the scenes for this linear
66:49 - regression with gradient descent stuff
66:50 - but really I don't know am I ever gonna
66:52 - use this again in any of my other videos
66:54 - are you gonna use this I don't know
66:56 - maybe it'll be interesting maybe you
66:58 - should go practice piano which is the
67:00 - lovely thing to do if you play the piano
67:01 - I guess if you don't maybe you should
67:03 - take lessons okay so let's think about
67:05 - this so I'm going to start with I here's
67:09 - so I have a lot of videos and I think
67:12 - this is actually a good place to start
67:13 - from that deal with motion and animation
67:16 - so let's think about motion on a compute
67:22 - in a computer window I might draw an
67:25 - ellipse and that lips would have a given
67:28 - X Y value but let's just simplify our
67:33 - world for a moment and think about only
67:36 - I don't have no plan for what I'm do
67:38 - just wait just get your wondering
67:40 - it's look because it kind of sounds like
67:41 - I have a plan I don't have a plan let's
67:45 - think I'd simplify this and think of
67:47 - just as it only has an X value so this
67:49 - circle is going to move each time
67:53 - through a draw loop each frame of
67:55 - animation X is going to change so I
67:58 - could if I wanted to create some sort of
68:01 - graph where the x-axis and now don't get
68:04 - confused here there's an x point there's
68:07 - an XY plane here now I'm creating a
68:09 - graph of the next y this is confusing so
68:11 - let's adjust this a little bit I'm
68:14 - looking for an eraser to make this let's
68:16 - see you can see now that I don't have a
68:17 - plan let's actually just call this um
68:23 - let's pretend this is forget about the
68:26 - computer window that this is a runner I
68:28 - can't draw I'm wearing a hat for some
68:32 - apparent reason and this Runner starts
68:35 - at 0 meters and is going to move along
68:41 - this bottom of this place where this
68:44 - runner is running so now this is going
68:48 - to be a graph where the y axis is
68:50 - distance and the x axis is time this is
68:56 - going to relate to conscious think okay
68:58 - so the first moment in time the runner
69:02 - is at a distance of zero then one second
69:06 - later the runner is at one meter and two
69:10 - seconds later the runner is at 2 meters
69:13 - and three seconds later the runner is at
69:15 - three meters so I could say this is a
69:19 - graph of the runners distance as it
69:23 - relates to time and calculus or at least
69:26 - a calculus and a key aspect of calculus
69:29 - known as the derivative often written as
69:34 - for example do this is bags this is
69:37 - called distance hmm can I make this X
69:40 - let's make this X so we're going to
69:44 - think of graphing X as it relates to
69:46 - time and so this is the X ax
69:48 - is up here I might write DX over DT
69:54 - calculus is the study part of calculus
69:58 - is the study of how one variable changes
70:02 - when another variable changes as time
70:05 - goes forward
70:06 - how does X Change how does X Change
70:08 - according to how time changes now this
70:11 - is relevant because what I did in the
70:14 - previous video and I want to like tie
70:16 - this together if this isn't just a
70:18 - general video as I was looking at how
70:21 - does the error change when I change the
70:23 - parameter of mission one of my enemy
70:26 - machine learning system right so what
70:28 - how does D error change relative to D
70:32 - wait if I change a weight what does that
70:36 - do to the error so if I can learn about
70:39 - how to study how certain variables
70:42 - affect other variables I can apply that
70:44 - to a machine learning system where I
70:45 - want to change weights to minimize the
70:48 - error okay so this graph being a graph
70:54 - of X as it relates to time we could
70:57 - actually we could write this as a
70:59 - function so time sorry I'm kind of
71:08 - taking a timeout for a second I don't
71:11 - I've got this like I don't love that I
71:13 - called this X and this is the y-axis
71:14 - look at the chat see if anybody has any
71:16 - helpful suggestions to me make distance
71:19 - s for displacement that's a good idea
71:24 - okay hold on I think he started wrong
71:27 - not talking about how derivative it
71:29 - could be explained concretely that was
71:33 - the last hope I'm in the wrong frame I'm
71:36 - very I'm looking at the chat waiting for
71:42 - some really a key piece of information
71:44 - that I want too oh it's 4 o'clock
71:46 - already out of keep moving here all
71:48 - right so I'm not seeing anybody I'm not
71:54 - seeing I'm not seeing anybody shouting
71:56 - at me and that have done anything oh
71:57 - yeah so ok so I'm going to get into ok
72:02 - um I need some more water here you got
72:04 - to stay hydrated especially if you're
72:05 - going to attempt to talk about calculus
72:08 - would it be maybe I should look at the
72:11 - Wikipedia page for calculus that might
72:13 - help me keep going it's you that we're
72:17 - here to see okay all right all right so
72:24 - I'm going to keep going now um okay okay
72:42 - so I kinda I don't love what I've done
72:44 - here because I'm you know mixing all my
72:49 - axes and variables so I'm now going to
72:51 - simplify I'm going to change this up
72:53 - again one more time and I'm going to
72:56 - consider the runners distance as the Y
73:00 - value and time as the x value whoa hold
73:09 - on a second time out did I write this
73:13 - correctly hold on if this was X and this
73:18 - was T and let's um let's say that these
73:26 - are 10 20 is this what I'm talking about
73:31 - I'm not talking about DT over yay dy
73:36 - over deep because if I change it then I
73:38 - would have dy over DX
73:39 - ah
73:44 - losing my mind here because what I what
73:46 - I want to get into the next thing I do
73:48 - is I'm gonna talk about acceleration and
73:50 - then I'm going to look at like an
73:52 - exponential curve I lost my train of
73:55 - thought here it's right okay I'm right
73:58 - okay yeah I kind of f of X would be a
74:01 - good way of doing this
74:08 - I'm going to leave this it's fine it's
74:12 - fine that the axes are kind of screwy
74:14 - it's going to be okay okay so actually I
74:19 - want to what I want to do here so hot I
74:21 - want to change the unit's a little bit
74:23 - because I think this will make it a
74:24 - little bit more interesting so let's say
74:26 - that at one second so let me sorry I
74:29 - lost my eraser let's actually change
74:35 - this a little bit so let's I'm going to
74:38 - try to actually get some units of
74:41 - measurement here so 1 2 3 4 5 6 7 8 9 10
74:44 - 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
74:48 - 17 18 19 20
74:49 - so let's say that the runner is running
74:53 - 2 meters per second right DX DT now I 2
74:59 - meters per second this is a kind of
75:01 - leading to objection leading the witness
75:04 - here but so at 1 second the runner is at
75:07 - 2 meters at 2 seconds the runner is at 4
75:10 - at 3 seconds the runner is at 6 so this
75:14 - is what the graph is going to look like
75:16 - like it I can't it should it should
75:19 - actually look like it's point my scale
75:21 - is off but you'll see the idea here so
75:23 - what is the formula the formula for this
75:28 - line is x equals 2 times T I can always
75:34 - determine what the position of the
75:37 - runner is according to what time so at
75:40 - 10 seconds the runners at 20 meters at
75:42 - 100 seconds the runner is at 200 meters
75:47 - so in this case the change in X relative
75:52 - to the change in time is 2 and it is
75:56 - that for any single point it is always
76:00 - that value now here's the thing what if
76:04 - the runner is not running at a constant
76:08 - speed what if the velocity changes over
76:12 - time as well so one of the things if you
76:14 - watch my other videos about motion and
76:16 - animation you'll see that ok velocity
76:19 - describes the rate of change
76:21 - for position so velocity is essentially
76:24 - the derivative of position acceleration
76:27 - being the rate of change describes how
76:29 - velocity changes over time and so that's
76:32 - why by the way when you look at a
76:33 - physics engine they'll often be on using
76:35 - some integration technique I might be
76:39 - using something called Euler integration
76:40 - or Verl a verlet verlet integration
76:44 - relay integration that's because what
76:49 - happens in a physics engine is I start
76:51 - with an object's acceleration and I want
76:54 - to then look at the velocity based on
76:56 - that and then look at the position based
76:58 - on that so I'm going in the reverse so
77:00 - integration being the reverse of the
77:02 - derivative but in this in our case for
77:05 - what we're looking to do in terms of
77:07 - figuring out it minimize the error
77:08 - according to different weights that are
77:10 - changing derivative is the key word here
77:14 - so if this is the function and really X
77:21 - is a function of T the change in X
77:26 - relative to the change in time the
77:28 - derivative of this is to always to the
77:32 - rate of change as T changes forever east
77:36 - as T okay not it out nothing wrong one's
77:45 - gonna time I was looking at the chat ok
77:47 - essence of calculus YouTube but I should
77:49 - just forget about all of these I
77:51 - shouldn't be making my own video here so
77:53 - many better videos on YouTube about this
77:55 - I think I gave enough disclaimers at the
77:56 - beginning ok the derivative the X the X
78:06 - changes the camera just went off over
78:09 - there let me fix that
78:20 - okay how X changes relative to time that
78:25 - is - okay
78:28 - so now here's the thing this is a very
78:30 - unrealistic scenario nothing is going to
78:34 - move at a perfect constant velocity but
78:36 - so let's just let's come up with a
78:39 - simpler let's come up with another
78:40 - scenario what if the graph of the
78:45 - runners position over time looks
78:48 - something like this
78:49 - so this might look familiar to you this
78:52 - parabola this exponential curve you know
78:58 - I try to approximate a drawing of
78:59 - something like y equals x squared so
79:02 - again I've kind of unfortunately named
79:05 - all my variables in a pretty terrible
79:07 - way here but let's look at this so let's
79:10 - try to understand what does how do I
79:13 - calculate so I'm going to come back over
79:17 - here and say in this case maybe x equals
79:25 - T squared the position of the runner as
79:31 - we're rates to time is the number of
79:34 - seconds squared so in this case the
79:37 - runner is accelerating at at zero so you
79:42 - know it's at zero seconds the runners at
79:44 - zero at one second the runner is at one
79:47 - at two seconds
79:49 - the runner is suddenly at four at three
79:52 - seconds the runner is suddenly at nine
79:54 - so the runner is speeding up as fast as
79:57 - it's running so in this case how do I
80:01 - know how X changes according to time so
80:11 - let's take a look though I don't I don't
80:14 - want to be here I want to run away and I
80:16 - now I gotta I went down this path and
80:19 - I'm thinking about this I gotta just
80:21 - keep going it's fine okay you can edit
80:26 - that out because you're wondering I mean
80:28 - you can't it'll be on the internet
80:29 - forever
80:30 - okay so let's look at let's look at a
80:36 - given moment in time let's say over here
80:39 - so this is I don't remember we're in 0 1
80:42 - 2 3 4 5 6 seconds so it's 6 seconds the
80:47 - runner and I obviously didn't draw this
80:49 - correctly for to this exact graph but
80:51 - that 6 seconds
80:52 - we know the runner is a not trying to
80:55 - scale would be at 36 meters it's a very
80:58 - fast runner I guess 36 meters in 6
81:01 - seconds it's not realistic I don't think
81:02 - so
81:03 - ok so now we could kind of what we can
81:07 - do is I could say like I could sort of
81:09 - look at this and say well what if I
81:11 - changed time by 2 seconds where would
81:15 - the runner be well the runner would be
81:18 - then at 8 seconds the runner would be at
81:21 - 64 the runner would be at 64 meters if I
81:26 - change time by 2 seconds so in this
81:29 - sense I can have I can I can say that
81:32 - what what happened here I changed by two
81:37 - seconds and did this I'm thinking about
81:42 - this I'm thinking about how do I want to
81:44 - describe this
81:55 - you
82:02 - where's my music when I need it but but
82:08 - maybe I need to hydrate is it really
82:11 - necessary to explain high school math
82:24 - I'm reading this chat this is
82:27 - fascinating I think I just got to speed
82:33 - this along here I actually got the thing
82:35 - is I want I'm going further into this
82:37 - than I intended and so go speed this
82:44 - along here so how much did it change by
83:00 - okay so I could say okay sorry so I
83:04 - could kind of so I could ask the
83:06 - question when I changed by two seconds
83:10 - how much did distance change and you 64
83:13 - minus 36 is 28 is that right I think
83:17 - it's 28 right so it changed by 28 I know
83:21 - this is this makes sense well the thing
83:25 - is I went way too far out here the point
83:31 - of what I'm doing here is that if I head
83:37 - against the wall this is also I should
83:41 - recommend yeah I get when people are
83:46 - watching this but I though the must have
83:49 - all turned this up this is also a nice
83:50 - book because it has it actually has a
83:53 - kind of explanation that's very similar
83:56 - what I'm talking about here in terms of
83:57 - incline and extent and slope and so
84:05 - actually I should really be talking
84:07 - about this in terms of thinking about
84:10 - going a little bit that way and a little
84:12 - bit that way I got a I got a back up
84:14 - here and we're going to edit this stuff
84:15 - out because and simplify things here
84:17 - okay so let me back up a little bit this
84:24 - is going to be a miracle if this ever
84:25 - gets edited to a video that actually
84:26 - makes sense for anybody
84:37 - okay
84:38 - so if I have this exponential graph
84:40 - where at the value of x is equal to x
84:43 - squared the way that I can understand
84:47 - how X changes according to a according
84:54 - to how time changes is by looking at
84:56 - well let me what if I went a little bit
84:57 - over this way and I went a little bit
84:59 - over this way nice okay well here's a
85:01 - point here and then here's a point here
85:03 - and now I draw this line you can kind of
85:05 - see that this line is describing in a
85:09 - way what you know what the change is
85:12 - right at this point as I move a little
85:14 - bit ahead in time or a little bit behind
85:15 - in time how does how's that distance
85:18 - change well in essence this is called
85:20 - the tangent to the curve right if I were
85:22 - to take these points and make them
85:24 - successively smaller and smaller and
85:25 - smaller and smaller and this is kind of
85:27 - a part of calculus is you know well what
85:29 - if we think about this in terms of like
85:31 - infinitesimally small things I'm going
85:33 - to get a line that is tangent to this
85:36 - curve and the slope of that line
85:39 - describes how the distance changes as
85:42 - you move a little bit ahead in time or a
85:44 - little bit behind in time at that moment
85:46 - and so in this case the way just I can
85:52 - see here like okay well let me actually
85:55 - write this out so the change in X over
85:58 - time is actually two times T for this
86:02 - particular graph right the slope of this
86:04 - line is two then it's 4 then it's 6 then
86:08 - it's 8 then it's 10 that it's 12 all
86:11 - right
86:12 - that's the slope of the line and in fact
86:14 - this is I forgot what this rule is
86:15 - called but in order to if you have any
86:18 - graph if you have any function sorry you
86:22 - can take the derivative that function by
86:24 - looking at the exponent exponent
86:27 - subtract subtracting 1 from it and then
86:31 - taking that exponent and multiplying it
86:34 - so if I have this function you know y
86:37 - equals 4x to the third power the
86:40 - derivative is 4 times 3x to the second
86:46 - power or
86:46 - 12x squared somebody fact-check debug
86:50 - this okay
86:58 - did I get that right power rule Thank
87:04 - You chat that was goes Inc Y o zy and
87:08 - Kane is seeing the chat tells me that
87:09 - this is known as the power rule so this
87:15 - is the first piece of the puzzle we need
87:17 - we need to understand that calculus
87:20 - allows us to look at how a given
87:23 - variable changes according to another
87:24 - variable and if we have a function that
87:28 - describes the relationship between those
87:30 - two variables we can look at that
87:32 - derivative and calculate that derivative
87:34 - in generalities using the power rule
87:37 - that's step one any questions ok I'm
87:42 - going to pause here in the next video
87:44 - I'm going to look at the chain rule and
87:48 - partial derivatives which we're also
87:50 - going to need in the gradient descent
87:52 - problem did I make it okay so oh I'm
88:07 - over here now alright so uh I thought
88:11 - that was there's something else going on
88:15 - in the chat that I'm not following why
88:17 - talk about calculus why talking about
88:20 - calculus magic recess when the topic
88:22 - title is linear regression well because
88:24 - I attempted to do linear regression with
88:26 - gradient descent and I was going to try
88:28 - to derive the formula for adjusting the
88:33 - weights in gradient descent and you kind
88:35 - of need calculus for that but I'm kind
88:39 - of regretting it okay um okay so that
88:45 - seemed to be I'm not seeing any major
88:47 - complaints in the chat I guess I'm going
88:50 - to keep this video I'm going to move on
88:51 - now and like skip over like a universe
88:55 - of information and talk about
88:59 - the chain roll boy do I even know what
89:02 - the chain rule is the chain rule okay
89:05 - okay I know what the chain rule is oh
89:06 - and I need in partial derivatives okay
89:08 - okay
89:09 - Horry says I didn't understand anything
89:12 - oh I'm sorry Horry I'm sorry that makes
89:16 - me very sad
89:18 - okay I'm looking at the chat still okay
89:27 - this is you know one thing that I one
89:31 - thing with my other videos is that I
89:34 - always like typically I if when I'm
89:41 - making a video tutorial
89:42 - I mean coding challenges aside typically
89:45 - what I do is I've kind of like use of
89:47 - material that I've taught in a classroom
89:49 - setting like a bunch of times and what
89:51 - that involves is number one preparing
89:53 - for class
89:53 - I don't really prepare for these videos
89:55 - number two it involves kind of like
89:58 - watching it and then getting feedback
90:00 - and kind of doing it again and then by
90:02 - the time I get up here to make a video
90:03 - I'm sort of more in the frame of mind of
90:06 - knowing what I want to accomplish but
90:09 - anyway this is it is what it is okay so
90:11 - I am now going to there's the chat okay
90:16 - so I'm gonna I'm gonna come back and
90:18 - okay okay welcome back I can't believe
90:24 - that you're if you're here watching this
90:25 - but um so you know I kind of in the last
90:27 - video it wasn't my best work but I
90:30 - discussed a bit about kind of calculus
90:32 - and what a derivative is some people
90:35 - might have helped that might have helped
90:36 - some people it might not have helped
90:37 - other people I will also I think include
90:39 - in these videos links to additional
90:41 - resources that might be better than mine
90:43 - but what so you know I'm skipping over a
90:49 - universe of information here and stuff
90:51 - that we could do and kind of jumping
90:52 - right into some other arbitrary piece
90:54 - but um what I established in the
90:56 - previous video and let me use this
90:59 - eraser here if that I have we have
91:01 - something called the power rule the
91:04 - power rule says that if I have a
91:08 - function
91:11 - f of X equals something like X to the
91:16 - third power plus four x squared plus
91:20 - three X plus two I can get the
91:24 - derivative of this function with the
91:25 - power rule so the power rule says that I
91:28 - take the exponent multiply it and then
91:30 - subtract one to the exponent so I have
91:32 - 3x squared plus 8x plus in this case one
91:36 - subtract one and get zero and then
91:39 - multiply one by three three and then
91:41 - this I get nothing I get zero so I just
91:46 - get rid of the constant so this would be
91:47 - the derivative of this particular
91:50 - function using the power rule power rule
91:55 - there are two so this is part of what we
91:58 - need for the gradient descent algorithm
92:00 - that I'm going to get to again in a
92:02 - couple of videos but there are two other
92:04 - rules that we need and in this video I
92:06 - don't know which order to talk about
92:07 - them in I think in this video I will
92:11 - talk about the chain rule picking the
92:17 - one that I'm much less comfortable with
92:19 - so the chain rule involves in both chain
92:25 - rule and partial derivatives involve
92:28 - systems with multiple variables so in
92:30 - this case this function has a single
92:33 - variable so what if I said to you f of X
92:39 - equals 4x squared but wait wait hold on
92:49 - time out so if I want to do the chain
92:51 - rule what I want to say is something
92:54 - somebody correct me if I'm wrong I want
93:00 - to have another function that depends on
93:03 - this function right hold on a sec
93:06 - there's a nice explanation of it in let
93:11 - me just see what example
93:18 - I'm going to look in in here for a
93:21 - second this is a Tyreke Rasheed's book
93:25 - that has a nice little appendix about
93:27 - calculus I'm pretty sure it had a chain
93:29 - rule example right why oh okay and so if
93:40 - I have one function equals and then if I
93:48 - were to say this doesn't make sense to
93:54 - me oh yeah I see I kind of doing this
93:58 - what if I then said because I could say
94:00 - like this if I said y equals 4x squared
94:05 - and I said like x equals something like
94:08 - Z to the third power plus you know Z or
94:13 - something like that so that oh I'm
94:17 - looking in this I could see you in the
94:22 - edited version okay hold up wait well
94:26 - then we come back uh D of F so hold on
94:37 - I'm gonna let me gently see if I can get
94:39 - I mean see if I can map this out then
94:40 - I'm going to like erase it and and do it
94:43 - as if I know what I'm talking about so
94:46 - what I would say I want now to say how
94:53 - does how does y change relative to Z
95:00 - right this is what I would use the chain
95:02 - rule for and I would say a Y changes
95:08 - relative to Z is a change in Y relative
95:16 - to the change in X times the change in X
95:22 - relative to the change in Z is this
95:24 - correct
95:27 - I think this is the chain rule okay so
95:36 - I'm pretty so many fact check me I'm
95:38 - gonna race this and do it again I think
95:40 - this is the chain rule the question is
95:43 - is my notation kind of awkward here
95:45 - should I have notated this in a
95:47 - different way like should I but I think
95:52 - this is fine
95:55 - okay yes correct okay thank you
95:58 - everybody looks good
95:59 - okay so where's my eraser okay okay I
96:12 - can't remember what I wrote here maybe I
96:17 - did something did I have this to start I
96:18 - can't remember okay maybe I'll just go
96:23 - back so okay so let's say I have a
96:30 - function y equals 4x squared now I know
96:35 - that the change in Y relative to the
96:39 - change in X the derivative of this
96:40 - function is 8x that is the power rule
96:46 - but what if this function actually
96:49 - depends on another function like what if
96:52 - I had a function that says x equals y to
96:57 - the third power plus 2y oh no no not Y
97:04 - sorry ah another variable I don't know
97:07 - what I'm doing here what if X is
97:09 - dependent on another variable right plus
97:14 - so X is dependent on Z X equals Z to the
97:17 - third power plus 2 times Z well
97:20 - according to the cheat according to the
97:23 - power rule the change in X relative to
97:28 - the change in Z is 3 Z squared 3 Z
97:36 - squared plus 2 and this is also
97:40 - the power rule but what if I wanted to
97:44 - ask how does y change relative to Z
97:49 - right Y is dependent on X but X is
97:53 - dependent on Z I could just like plug Z
97:55 - in here and work out all the math I
97:58 - think but I can also use something
98:00 - called the chain rule the change role
98:03 - states that with a function that depends
98:06 - on another function I can I can separate
98:09 - this out into two different parts I can
98:11 - say dy over DZ equals the change in Y
98:16 - relative to x times the change in X
98:23 - relative to the change in Z pause did I
98:29 - get that right is that right is that
98:33 - what I had before is this for ten euros
98:39 - oh god I do have a viewer in the
98:42 - Netherlands who is a seven-year-old boy
98:44 - who's done amazing things I hope that I
98:47 - don't know if this is gonna be at all
98:49 - helpful and it's done some wonderful
98:52 - stuff I've been amazed it's beyond me
98:57 - encoding already I think I get this
99:00 - right any another fact check yes okay
99:03 - and you can and you can see how this
99:08 - could kind of works out because you can
99:10 - almost like cancel those out and you
99:11 - have dy over D Z so if I were just to
99:13 - follow this along
99:14 - I now have 8x times 3z squared plus 2
99:22 - which would he'll give me 8 X Z squared
99:27 - plus 16 X if I did the math there
99:31 - correctly so this would be the chain
99:32 - rule which would give me the derivative
99:34 - of the dy the derivative of Y relative
99:39 - to Z is that is that the right way to
99:41 - describe it
99:43 - if Y depends on X and X depends on Z so
99:46 - this is the chain rule okay so now we've
99:50 - got what is the derivative with the
99:51 - power rule
99:52 - we've got what if I have one function
99:54 - that depends on another function that
99:57 - this depends on another function I could
99:59 - do a derivative with the chain rule I'm
100:01 - setting up things that we need and
100:02 - there's one more thing that I need to
100:04 - talk about which is what is a partial
100:06 - derivative once I talk about what a
100:08 - partial derivatives then we can go back
100:09 - to the gradient descent math and workout
100:12 - and derive that exact formula ty used in
100:14 - that previous video which seems like a
100:16 - lifetime ago okay next video will be
100:18 - partial derivative yeah I'm getting the
100:26 - comment which is that this is much
100:28 - smoother than your derivative
100:29 - explanation which is interesting because
100:31 - I think I felt this need with the
100:32 - derivative explanation to kind of
100:34 - somehow summarize all that is its
100:36 - calculus in one video and I became
100:39 - overwhelmed by that and I kind of messed
100:41 - it up I'm just seeing if anybody wants
100:50 - to is I'm just reading the chat to make
100:53 - sure there's no like wildly wild what
101:00 - anything that I've done like way off
101:01 - base but so far and it's 4:30 I don't
101:05 - have a lot of time left so let's see if
101:09 - I can do this partial derivative thing
101:10 - this is what I felt okay so um okay so
101:17 - let me let's erase some of this where's
101:22 - that eraser
101:36 - okay so if Y is a function of X and X is
101:51 - a function of Z I just want to state the
101:55 - chain rule then dy DZ equals dy DX times
102:07 - DX DZ this is the correct way to state
102:13 - the chain rule okay the power rule
102:30 - Harrell states that y equals some
102:35 - constant times X to the N dy/dx
102:46 - Oh a wait yeah I don't I thought it C
103:02 - times n times X to the N minus 1 this is
103:09 - a good way to write it power rule
103:22 - I don't like my notation first of all I
103:26 - think I want to use asterisks since
103:28 - that's sort of like the coding way I
103:32 - need to put one here any corrections to
103:37 - this because I'm going to add partial
103:39 - derivative next partially out of frame
103:48 - thank you why I think you know I'm just
103:56 - going to do is I'm just going to say
103:58 - edge what what if I just say X to the N
104:04 - then I can say n times X to the N minus
104:09 - 1 and that should be in the frame yes
104:12 - how am i doing okay
104:21 - I'm look write f of X not Y
104:33 - and then say DF here is that better I've
104:45 - also seen right this can you write this
104:47 - right which also indicates derivative
104:56 - use a use F okay yeah all right
105:08 - maybe at the chain rule not always using
105:12 - F FX play yeah okay it's better to have
105:17 - two names for the functions yeah maybe
105:22 - yeah so in other words
105:35 - my notation is kind of bad here because
105:39 - it's not consistent yeah because there's
105:44 - like up but I'm taking the chat very
105:46 - seriously if I everybody hasn't
105:47 - completely different uh everybody has a
105:50 - completely different approach I think I
105:57 - think I better be consistent here and
105:58 - you know what I'm gonna do
106:00 - even though I feel like this is a little
106:02 - bit less mathematical notation so I'm
106:07 - gonna stick to this this is it's a
106:10 - little bit awkward what I wrote here
106:11 - right is this really is you can't see me
106:14 - I'm in the wrong frame this is the thing
106:18 - that I'm this is what I'm worried about
106:19 - being most problematic the way that I
106:22 - wrote this here is this something that a
106:25 - mathematician would never write it this
106:26 - way that's my question
106:39 - xeo x equals G of Z that makes sense
106:42 - yeah right a different function that's a
106:47 - good point okay thank you okay here we
106:53 - go
106:53 - oh boy okay thank you everybody okay
107:06 - hi so I don't know by some miracle I I
107:10 - made a video where I attempted to
107:11 - describe something about a derivative I
107:13 - think that one wasn't that great then I
107:15 - need another video which actually felt a
107:16 - little bit better but about the chain
107:19 - rule so I've kind of summarized how the
107:21 - power rule is a way to take a function
107:23 - and compute the derivative by taking the
107:27 - exponent multiplying it subtracting one
107:28 - by the exponent the chain rule is a way
107:31 - to say if Y depends on X and X depends
107:33 - on Z I can look at the relationship
107:35 - between y and z by chaining the two
107:38 - derivatives of this function in this
107:39 - function so those are two pieces that
107:42 - I've done so far the last piece that I
107:44 - need for the gradient descent algorithm
107:46 - is something called a partial derivative
107:50 - now I think this is actually gonna be
107:53 - somewhat easier to do to explain and
107:56 - then you know hopefully it all makes
107:58 - sense we come back and look at the
107:59 - derivation of the linear regression with
108:01 - gradient descent formula but let's say
108:04 - what if you know in in many cases you
108:08 - have a function with multiple variables
108:13 - so I could say I have a function f of x
108:18 - and y and it equals you know 3x squared
108:23 - plus 2xy plus y to the third power plus
108:28 - um you know I don't know 9 x squared Y
108:34 - so this is some crazy function that I
108:36 - wrote I don't know what the use point of
108:37 - it is but what if what I want to do is
108:42 - look at and let's say this is Z Z is a
108:47 - function of X and
108:48 - and what what I want to do it and by the
108:51 - way there's I've been struggling with
108:53 - this you know if I have a function like
108:55 - f of X equals x squared I can another
108:59 - notation that you might see is this f
109:03 - tik X I don't know what the technical
109:06 - term for this is but this is another way
109:08 - of writing the derivative of the
109:09 - function f is 2x so but I could also say
109:15 - look what I've done here with y I could
109:17 - say the derivative of the function is dy
109:19 - over DX and in this case the reason why
109:21 - this is important is because what if I
109:23 - want to look at how Z changes only
109:27 - relative to X or only relative to Y this
109:31 - is what's known as a partial derivative
109:33 - and then notation is written instead of
109:37 - I'm going to just do this over here
109:39 - right this would be kind of the regular
109:42 - way of writing it I don't fold or if it
109:44 - if it's not partial dy over DX a partial
109:48 - derivative is written with D but in a
109:51 - slightly different style like this I
109:54 - don't know where this notation comes
109:55 - from maybe somebody to check and tell me
109:56 - and I can say it in a second or put a
109:58 - link to it but if I want to know how Z
110:01 - changes relative to X only that's a
110:05 - partial derivative or how Z changes
110:08 - relative to Y only ok I'm pausing for a
110:18 - second how am i doing
110:26 - are people people are freaking out in
110:29 - the chat yeah out of frame I got to fix
110:34 - that so um I I don't know I don't know
110:42 - what's going on the checks I'm bill to
110:43 - follow but if you're watching this and
110:46 - you're kind of like oh do I need to know
110:48 - this or this is annoying or I'm confused
110:52 - first let me say sorry I'm trying my
110:55 - best dear this is an experiment to see
110:56 - if it makes sense they even cover this
110:58 - stuff and this is not something again
110:59 - that I have a deep knowledge of and I
111:01 - also I need to get this back into the
111:04 - frame here but but I will say is that
111:08 - you don't actually need to do all of
111:12 - this math yourself if ultimately what
111:16 - you're going to do is use a machine
111:18 - learning library like say tensorflow
111:20 - to train and calculate all the weights
111:24 - of our model and in essence I would say
111:28 - that you know I would probably say that
111:29 - when I frame the whole set of videos I'm
111:33 - going to make these are videos that you
111:35 - could skip but this will give you a
111:38 - background into how the math works which
111:41 - might gives you give you a sense of some
111:45 - of the terminology language ideas that
111:48 - might be in various papers and blog
111:50 - posts and books and things that you
111:51 - might read as you're learning and
111:53 - exploring this material so don't worry
111:55 - everybody calm down
111:58 - keep calm and carry on so to speak but
112:01 - I'm going to keep going with this
112:03 - because I'm just about done okay so
112:05 - first thing that I need to do is first
112:12 - thing I need to do is get this more into
112:15 - frame so here I'm going to come back
112:18 - thank you machiya this is going to be so
112:20 - much more work than usual
112:22 - okay so sorry I kind of drew this out of
112:25 - frame so let me let me draw this a bit
112:29 - smaller so partial derivative of Z
112:32 - relative to X partial derivative
112:35 - in the frame I get lazy of Z relative to
112:38 - why did I keep that in the frame I think
112:40 - I did okay so how do we do this
112:41 - the way that we do this this partial
112:44 - derivative is calculated with the power
112:46 - rule the same exact way that you would
112:51 - do normally and with treating Y simply
112:55 - as a constant so whatever you would do
112:59 - to a constant you would do you doing the
113:02 - exact same way so what do I mean by that
113:04 - so this would now equal this partial
113:07 - derivative would be so this like this
113:10 - this little section I can do just with
113:12 - the regular power rule so 2 times 3 is 6
113:14 - so 6 X 2 minus 1 is 1 6 X plus now this
113:20 - is tricky so 2 X Y well I want the
113:23 - derivative relative to X another way of
113:26 - writing this is 2 y X now think about
113:29 - this if I had 5 X the derivative of 5x
113:32 - would just be 5 in this case 2 times y
113:36 - is a constant I want to complete as a
113:39 - constant so the derivative of 2y xr2 XY
113:42 - is just 2 y so 6 X plus 2y plus now
113:47 - here's a tricky one y cube do you think
113:50 - oh well it's a constant so it stays is Y
113:52 - cubed but it's not what if I have just a
113:54 - value like I don't have this anymore
113:57 - like 5 the derivative of that would be 0
114:01 - so this goes away and then this is the
114:05 - same thing this is now the equivalent of
114:07 - 9 times y times x squared so that is 2
114:10 - times 9 18 times 18 times 18 Y X right
114:21 - because it's right if I just had a
114:24 - constant 9 x squared I would have 18 X
114:27 - so 9y x squared is 18 times y x so this
114:31 - is now the partial derivative relative
114:34 - to X I treat Y as if it's a constant and
114:37 - I have this
114:39 - the derivative now as an exercise I
114:41 - might say stop watching this video pause
114:43 - this video and try to do the partial
114:46 - derivative of Z relative to Y now I have
114:49 - no room here the whiteboard to do it
114:51 - because I've done a terrible job of
114:52 - organizing myself so I will just put the
114:54 - answer to that in the video's
114:56 - description and you can check it because
114:59 - this is the end so now we have the
115:00 - pieces I know that I've skipped a
115:02 - million details this isn't really a
115:04 - proper and this is the proper anything
115:06 - frankly but this isn't a proper calculus
115:08 - lesson I don't even know what I'm
115:09 - talking about at the time in these
115:11 - videos but I'm trying to set the stage
115:13 - for at least the terminology and the
115:14 - pieces of the puzzle so what when I get
115:18 - into the next video I'm going to go back
115:19 - to gradient descent this thing where I
115:21 - calculated the error and then nudged the
115:24 - slope or the y-intercept of this formula
115:27 - for a line according to that error well
115:29 - why is it that I did it that way I'm
115:31 - going to show you why using the power
115:34 - rule the chain rule and partial
115:36 - derivatives so hopefully this will give
115:38 - you some background for that I'm gonna
115:40 - do that in the next video
115:41 - you know you don't you could just go do
115:43 - something else like but that's what I'm
115:46 - going to do so let me know your answer
115:48 - to the partial derivative here and then
115:50 - I will see you maybe in the next video
115:53 - bye okay
116:02 - did I by the way so first of all uh is
116:07 - that all this legible it is did I oh its
116:14 - prime not tick so this is not tick this
116:19 - is prime why is it prime so I got to
116:23 - correct that hopefully get flamed we're
116:26 - going to insert this we do a little
116:28 - insertion so the chat is telling me that
116:32 - the proper term is not tick for this but
116:36 - it's F prime sorry about that
116:39 - okay little flip that'll get edited in
116:43 - okay F - okay oops this camera went off
116:51 - I probably should leave I really yeah
117:01 - I'm making these videos but you know for
117:04 - me the thing that I'm really interested
117:05 - in is the application of stuff and some
117:06 - creative projects but F - yeah dub the
117:12 - word over F prime okay um okay so let me
117:19 - see here
117:20 - I don't think I have time for the next
117:22 - piece doing 15 minutes before I really
117:24 - have to go so the question is could I do
117:27 - this in 15 minutes um let me think about
117:33 - this mmm cuz it really would be nice to
117:38 - finish this off today I don't want to
117:40 - come back to it for sure okay let me try
117:43 - so what I need to do now Oh oh my god
117:48 - so this is horrible I really got myself
117:51 - into a situation this is correct by the
117:53 - way so if this is wrong a big trouble
117:56 - so hopefully somebody tell me this is
117:57 - right or it was wrong somebody would
117:58 - told me so I've gotten myself into a
118:00 - situation where what I need to do now is
118:04 - show the formula for calculating the
118:07 - error based on the weights and then look
118:17 - at how the derivative shows us which way
118:20 - to go to change the odd I really get
118:24 - myself into the situation where this is
118:25 - what I'm teaching this is a good
118:30 - exercise for me I should understand and
118:31 - learn this stuff if I'm going to mostly
118:34 - I'm doing this because I want to
118:34 - understand and try to learn this stuff
118:36 - so that I feel like I have a bit more
118:37 - background in it if I'm just kind of
118:38 - like hacking together some tensorflow
118:40 - stuff but do I have time to do this
118:45 - today should I come back on Tuesday
118:48 - leave this as is maybe come back fresh I
118:51 - think I might need to come back fresh
118:59 - seeing ya f - f - DZ DX is correct thank
119:09 - you I got lucky I did study this stuff
119:13 - but why did take multivariable calculus
119:15 - that would be right now let's see when
119:19 - would I've taken that character one
119:24 - twenty three twenty four years ago
119:26 - something like that you know then I had
119:28 - wasted a lot of time in New York City
119:30 - with odd jobs and ends that went to AI
119:32 - TPS or learned about programming and
119:34 - then like a week ago I thought let me
119:36 - read about all that multivariable
119:38 - calculus stuff it's a gradient descent
119:39 - again because maybe that's useful okay
119:43 - um all right okay so I think I think I
119:48 - probably I think I shouldn't rush this
119:50 - and try to like do the next thing in ten
119:52 - minutes because it's just going to be a
119:53 - fail I'm hoping that maybe I can
119:57 - actually just come and complete this on
119:58 - Tuesday you know now that it's summer I
120:01 - strangely enough have the flexibility to
120:03 - record much more often and I feel like I
120:05 - need this to be completed cuz then once
120:09 - I'm done with this I can do the
120:12 - perceptron I can do the neural network
120:14 - and I don't have to go back and explain
120:16 - gradient descent I can just use it and
120:19 - if anybody wants you they could go and
120:21 - watch these other videos is it not
120:26 - called multi variable calculus anymore I
120:31 - am 43 I might as well and people ask
120:35 - that a lot in the chat but I might as
120:36 - well if you can find it online pretty
120:38 - easily I think I have a Wikipedia page
120:39 - by the way let's take a look somebody
120:41 - made one is like a class assignment at
120:43 - some other school maybe you guys
120:45 - watching I can't how many people are
120:47 - watching this right now has got to be
120:49 - like 381 people are still watching this
120:54 - that's got to be a mistake you know the
120:57 - camera this room I have like a soft
120:59 - light makes me look much younger
121:01 - ya see the Wikipedia page actually has
121:04 - looking it has my birthday on it but I
121:07 - don't know Internet there's three today
121:09 - you want people watching you can add
121:10 - something brush it off this page for me
121:12 - that would be nice it could be like a
121:14 - picture on it something like inside
121:18 - coding train humor somehow that's an
121:21 - easter egg see if you can do that
121:23 - I do act acting like I'm 25 is very
121:26 - generous thank you
121:27 - guma in the chat okay all right so um
121:34 - [Music]
121:37 - thank you all for watching this today so
121:40 - let's recap what do I have so far that
121:43 - I've done new this week really just
121:46 - linear regression linear regression
121:47 - linear regression so I did a Tommy
121:50 - how many edited videos are going to come
121:51 - out soon what is linear regression
121:53 - linear regression for ordinary
121:55 - least-squares linear regression with
121:57 - gradient descent I know what I could do
122:00 - in ten minutes I know what I could do in
122:02 - ten minutes okay people are really
122:11 - talking about my age I shouldn't said
122:13 - anything too much maybe I maybe I'm
122:15 - actually wrong about my age it's a state
122:17 - of mind man yeah uh maybe what I should
122:23 - do is do I dare dare no no I'm going to
122:29 - come back I'm going to do this stuff
122:31 - next I'm going to answer some questions
122:32 - I also wanted to use this library which
122:36 - will do the regression for you and I
122:43 - want to go through this library and show
122:44 - you how this works I will do this yeah
122:48 - so the chat K week one is asking your
122:51 - Wikipedia page says you studied
122:52 - philosophy has that about had been a
122:54 - help in your further studies well
122:57 - interestingly enough I did study
122:59 - philosophy and the focus first I didn't
123:02 - really pay attention it didn't really
123:03 - make good use of my undergraduate
123:04 - education I would say I regret I take
123:12 - it's a wonderful luxury to
123:14 - and wonderful thing education so anyway
123:17 - but that aside the thing that I focused
123:20 - on was a logic paradoxes and so I think
123:27 - that a lot of this are boolean logic set
123:30 - theoretical paradox kind of like stuff
123:31 - that I was studying this is a long time
123:33 - ago unbeknownst to me at the time became
123:37 - incredibly useful as a way of thinking
123:40 - as applied to coding so coding I think
123:43 - is a learning to program learning to
123:46 - think through a problem in a logical way
123:49 - I think definitely relates to a lot of
123:51 - the stuff that I learned about and
123:52 - studied in terms of philosophy but I
123:54 - never really made that connection okay I
123:59 - am watching this Sasha I'm watching this
124:01 - even after two whiskies and three beers
124:04 - I should go maybe a beers in the cards
124:06 - for me this evening to celebrate the end
124:09 - of a week the weekend is coming up two
124:13 - whiskies and three beers you would have
124:15 - to hospitalize me I think that happened
124:18 - but yeah okay can I teach c-sharp can I
124:22 - teach Arduino IDE I'm not an expert or
124:28 - which by the way clearly is not stopping
124:31 - for me to teach anything not knowing it
124:32 - is kind of the is the criteria for me
124:37 - attempting to teach it apparently but
124:39 - c-sharp Arduino are not things that I'm
124:41 - looking at doing anytime soon but I do
124:43 - want to get back to having more guests
124:45 - tutorials so I would like to have more
124:47 - physical computing related guest
124:49 - tutorials tutorials maybe using things
124:51 - like unity which I know c-sharp is a
124:53 - part of so those things are and I think
124:56 - I need to it's on my to-do list but to
124:58 - schedule is it June yet I was kind of
125:01 - hoping that June would be the month of
125:02 - guest tutorials so I need to start
125:04 - scheduling people to come in and do
125:05 - guest tutorials i pronounced
125:13 - [Music]
125:15 - alright looking at these questions I'm
125:17 - going to play my goodbye way
125:20 - so I'm gonna be off this has been a two
125:24 - hour and five minute livestream I've
125:26 - never tried aerial ask have you ever
125:27 - tried the Elm foreign language I have
125:29 - not I didn't know what that is
125:32 - cigar says smiley emoji smile you smile
125:36 - you smell oh I know I know this chat I
125:43 - have the chat on this monitor on the
125:45 - side and like a really big font so I can
125:47 - read it from faraway Scrolls by this
125:49 - like most almost all other questions
125:52 - how about a Python I do want to do
125:56 - actually I kind of I kind of want to do
126:00 - some intro to Python tutorials because I
126:03 - don't really know Python i hack away at
126:04 - it sometimes when I need to get a Python
126:06 - script working and I thought I would try
126:08 - to do once I get I have this idea that
126:11 - once I get to in my list here week five
126:17 - CNN tensorflow so I'm gonna do all the
126:19 - neural network stuff without Python and
126:21 - then I'm going to use tensorflow
126:22 - and I'm gonna use tensorflow with Python
126:25 - I was thinking of like a series of like
126:27 - teach myself to program in Python so I
126:29 - might just like a few basic Python
126:30 - tutorials I would love to do shaders I'd
126:37 - love to get a guest do shaders I didn't
126:42 - use multiple colors that's why today
126:44 - didn't go well
126:52 - H I I do have HTML job Jerrica boasts 14
126:57 - asks no juice says delay test responded
127:01 - this ASAP that's specially respond to it
127:03 - do it HTML tutorial I have a whole
127:05 - playlist of kind of like what is HTML
127:08 - and some basic to Charles about HTML in
127:10 - the context of using the p5 chess Dom
127:12 - member yes I can juggle could also play
127:19 - piano a little bit and I was thinking
127:22 - maybe I should write a few like parody
127:25 - computer science programming songs and
127:27 - come in and play a few for you I've been
127:29 - learning some Nick Cave Nick Cave is
127:31 - playing tonight at the King's Theatre in
127:35 - Brooklyn I'm not going but I bought some
127:37 - tickets as a gift for somebody else I'm
127:41 - excited for that Nick Cave is also
127:43 - playing at the Beacon Theatre on your
127:44 - boys side in the K stand unlike concerts
127:48 - too loud too proud it ok um so thanks
127:54 - everybody for tuning in I I don't know
127:56 - if I hope this isn't too much of a
127:58 - digression kind of going off on this
128:00 - linear regression stuff and and can a 13
128:08 - year old learn coding of course a 13
128:11 - year old can learn coding a 10 year old
128:13 - could learn coding
128:14 - can learn coding my girl can learn
128:17 - coding concepts and talking my baby I do
128:19 - I do think that for children
128:23 - experiencing the physical world
128:25 - experiencing learning to how to be part
128:28 - of a community and be with people these
128:30 - are sort of like the primary things to
128:33 - learn an experience as in the
128:35 - development of a child this is a
128:36 - non-scientific this is you know but so I
128:40 - do question in terms of all this I've
128:42 - learned to code and you know code with
128:44 - Elsa and on encode with Star Wars and
128:46 - Lee or four years old and play in your
128:47 - code on your iPad app you know I don't
128:49 - have anything necessarily against that
128:51 - so to speak but I do think that well I
128:54 - one thing I'm excited about sort of
128:56 - offline collaborative activities that
128:58 - teach kids ways of thinking of code is
129:01 - sort of a kind of thing that interested
129:03 - but certainly I do think for a
129:04 - thirteen-year-old at that age you know
129:07 - using something like scratch even
129:10 - something like p5.js and JavaScript
129:12 - these are platforms that absolutely can
129:14 - be productive for programming ah
129:18 - mica Erickson in the chat and I have the
129:21 - same birthday
129:22 - wonderful okay I'm out in about 20
129:26 - seconds
129:28 - I'm sorry that this is I know people are
129:34 - really excited about the tensorflow
129:35 - stuff and the neural network stuff I am
129:37 - gonna get to it I wish I was getting to
129:40 - it faster maybe it was a mistake to have
129:42 - all this lead up to it but it's what I'm
129:44 - attempting and trying to do so that's
129:45 - what it is
129:46 - okay um by the way I didn't really even
129:49 - start coding Hilario for 28 or something
129:52 - so I don't remember the exact age but I
129:55 - have to look it up okay thanks everybody
129:57 - um if you're interested in supporting
129:59 - the work that I do you are welcome to
130:01 - join the slack community patreon.com
130:04 - slash coding train you can also buy some
130:09 - merchandise that Cody train store Envy
130:11 - com somebody asked if they could donate
130:13 - with Bitcoin and I'm looking into thing
130:15 - if I could make that possible for people
130:17 - and I have a coin base account but I
130:21 - don't have a merchant account I couldn't
130:22 - really figure out how to do it wants to
130:23 - help me with that please let me know
130:25 - what else do I want to say like
130:28 - subscribe at Schiffman on twitter
130:32 - feedback constructive criticism all of
130:34 - that is welcome I hope you're enjoying
130:36 - everything and I don't know for sure I'm
130:41 - definitely coming back next week on
130:43 - Thursday or Friday I'm not free on
130:46 - Monday and I'm not free on Wednesday if
130:47 - I have time to just come back for a
130:49 - short bit I really want to get through
130:51 - this linear finish off this regression
130:53 - stuff if I can do that on Tuesday I will
130:55 - so just stay tuned at Schiffman on
130:57 - twitter subscribe here's one thing you
131:00 - can do if you want an alert whenever I'm
131:04 - live if you subscribe on YouTube and
131:08 - then click this alarm bell it will
131:10 - you'll get an alert so I suggest doing
131:13 - that and sometimes yeah so that's that's
131:17 - about it okay thanks everybody have a
131:18 - wonderful weekend
131:19 - be safe be with your family and love and
131:22 - friends and loved ones give somebody a
131:25 - hug
131:28 - see you in the next train I'm going to
131:31 - take the train where's my eye it's time
131:34 - I don't know if you know this but I
131:38 - actually have some toy trains in this
131:41 - video so here we go goodbye I'm leaving
131:47 - do the awkward thing is the button for
131:49 - me to stop the YouTube stream is over
131:52 - here so I have to come back by the way
131:55 - look at this well what is this strange
131:57 - invisible laptop look at this it is a
132:00 - strange magical laptop that is invisible
132:03 - it's actually just a laptop with green
132:10 - paper this is where I cheat this is
132:12 - where I have all my secret code on it
132:13 - now I just have the slack chat on here
132:15 - but a lot of people write I could see
132:16 - you looking to the side you're steep
132:18 - you're just typing your code from the
132:19 - other thing which first of all how do
132:20 - they go be cheating that would be just
132:22 - being prepared and going through a
132:24 - lesson that I previously prepared that
132:26 - would be a good thing okay I don't know
132:29 - people want some green screen stuff that
132:31 - they can do some weird things with no I
132:36 - don't know what I'm doing your work I
132:37 - got to go I got to go see you guys have
132:39 - a wonderful weekend
132:40 - and I will see you in the future