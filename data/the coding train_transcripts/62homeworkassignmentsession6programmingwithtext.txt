00:00 - okay so maybe you've watched my videos
00:03 - so far about engrams and Markov chains
00:05 - and that sort of thing and you're
00:06 - thinking what what's the next step for
00:08 - you what could you possibly make so on
00:10 - the one hand I think the assignment are
00:13 - the exercise for this week it's really
00:15 - legitimate to not program anything new
00:17 - in fact a lot of using a Markov chain to
00:20 - generate text is about two things
00:22 - outside of the actual algorithm itself
00:24 - number one it's what source text are you
00:26 - picking and why are you picking that
00:28 - source text and number two is what are
00:30 - you presenting to a reader a viewer a
00:34 - user an audience and what medium are you
00:37 - presenting it in are you using a Markov
00:39 - chain to generate dialogue for a
00:41 - performance are using a Markov chain to
00:43 - generate tweets are you recreating you
00:46 - know news articles and the project about
00:49 - designing a news like a website that
00:51 - looks like real news but actually it's
00:52 - Markov generated news there are a lot of
00:54 - possibilities there using a Markov chain
00:56 - to generate text is not really a novel
01:00 - idea at all and you'll see I'll try to
01:01 - link in this video's description to lots
01:03 - of existing projects that use this
01:05 - technique that you can also find for
01:06 - experimentation now here's the thing
01:08 - though
01:08 - I have some additional examples that you
01:11 - know the videos that I made that you if
01:13 - you watch the Markov chain video
01:14 - previously i'm just implementing the
01:16 - sort of raw algorithm and doing this
01:17 - kind of simple not simple but doing this
01:19 - sort of basic implementation of it so
01:21 - one thing I want to do and that's this
01:23 - example that's trying to generate code
01:25 - or full code in Bo's that's kind of
01:27 - awesome fifth Matt's ratings shift so
01:30 - you can see there's some new names that
01:31 - I'm trying to think of for this channel
01:32 - now I do have some pre-made examples
01:36 - that if you go to the link to the A to Z
01:40 - github repository you'll see I want to
01:42 - highlight some of these number one and
01:45 - these are by the way this is a Markov
01:47 - chain example that does something much
01:49 - like my youtube channel naming and it
01:50 - just takes naming of names of lots of
01:53 - media art programs and creates name so
01:57 - till we get anything good sent lis
01:58 - program in arts and media arts encode
02:00 - media arts program technologies program
02:03 - creations in entrepreneurship
02:05 - anti discipline
02:05 - 'native technology program set
02:07 - technology people so you can see
02:10 - computer action that's pretty great so
02:11 - you can see this is the kind of thing
02:13 - for naming you can also look at my
02:16 - second example and try to find the
02:17 - differences right between generating
02:19 - short text this reads Mary Shelley's
02:21 - Frankenstein from Project Gutenberg and
02:23 - generates kind of an actual very very
02:26 - long text with paragraphs on everything
02:28 - written in the quote unquote style which
02:30 - isn't exactly accurate but uses a markup
02:32 - change a generated new text so you might
02:34 - look at those look at the difference
02:35 - between short text and long text both of
02:38 - those examples are generating text on
02:41 - the character by character basis so when
02:43 - I say a trigram I mean a collection of
02:45 - through a sequence of contiguous
02:46 - sequence of three characters and three
02:48 - characters but you can also do this by
02:50 - word so you can consider the tokens not
02:53 - as individual characters but as words a
02:55 - rainbow is rainbow is a is a
02:58 - meteorological so this is an example
03:01 - that instead of using sequences of
03:03 - characters using sequences of words and
03:05 - you can see the results here in a
03:07 - primary rainbow with inverse order of
03:09 - its colors reversed so at some point I
03:12 - could go back and do another coding
03:13 - challenge to implement it by word if
03:14 - people are interested in that but right
03:16 - now I might just refer you to this code
03:17 - example another thing you might think
03:19 - about is what are other tokens or things
03:23 - that happen in sequence in text for
03:25 - example this text is not just this
03:27 - sequence of characters or this sequence
03:29 - of words it's actually a sequence of
03:31 - parts of speech verb noun noun verb
03:35 - determinate adjective adverb getting
03:38 - that in the wrong order but what this
03:40 - actually does is evaluate the contiguous
03:42 - sequence of parts of speech use a Markov
03:45 - chain to chain to generate a sequence of
03:48 - parts of speech and then pick words that
03:50 - are parts of that parts of speech to
03:52 - generate the text and the results here
03:54 - you can see is the observer it reaches
03:56 - the meteorological set the raised
03:57 - section in a section and 40 to 42
03:59 - degrees percent or more in the system
04:01 - between an arc and 50% excetera etc so
04:04 - you can take a look at that example as
04:06 - well another thing you might think about
04:07 - is mixing two texts within a Markov
04:11 - chain right what if I generate this uses
04:14 - if I take the slider all the way over
04:15 - here this is generating text using a
04:18 - Markov chain only
04:19 - from Midsummer Night's Dream this is now
04:22 - generating text only from my nature of
04:24 - codebook and now if I kind of put it
04:27 - like over here let's see if we can
04:29 - generate something good that's a little
04:32 - bit too Shakespearean let's let's let's
04:35 - give it a little more nature of code do
04:38 - it again unto his point numbers rather
04:40 - voice versions when program how do I
04:42 - knowledge about that we add one we can
04:44 - be the mouse location strong prevalence
04:46 - of the true for both X's and dispose of
04:48 - zero zero we still have this kind
04:50 - wanting but how to mic inside of length
04:53 - one then have our motion the close to
04:57 - your youth to one okay anyway I got a
05:01 - little too caught up in like pretending
05:02 - I'm like acting Shakespeare with my
05:03 - Markov chain but anyway so you can see
05:05 - that's the thing you might think about
05:06 - what if you're mixing two texts three
05:08 - texts how do you adjust the
05:09 - probabilities I'll give you a little
05:10 - hint behind the scenes if the
05:13 - probability if I want the probability of
05:14 - the Shakespeare text to be higher I just
05:16 - feed the Shakespeare text in multiple
05:18 - times again it's not the most efficient
05:20 - way of doing this but it works you could
05:22 - think about other ways that you might do
05:23 - that you could get you can use a Google
05:27 - sheet I have a mad libs generator that
05:28 - pulls data from a Google sheet could do
05:30 - the same thing with a Markov chain you
05:32 - could pull from an API I have an example
05:34 - that pulls from reddit and then also I
05:36 - should mention of course that even
05:39 - though I'm using my own Markov
05:41 - generating code based on code written in
05:43 - Python by Allison parish you could also
05:46 - use the rita library which has a markov
05:49 - generator built into it and I have a
05:50 - previous video that I should link to in
05:52 - this description all about the Rita
05:53 - library and encourage you to look at
05:54 - that and see its implementation so these
05:58 - are things that you could think about I
06:00 - made a little list here also that I'll
06:02 - reference so on the one hand just
06:05 - generate what's your own existing text
06:08 - what's your what's what's the order
06:10 - you're picking how are you presenting it
06:11 - you could think about just sort of
06:13 - visualizing the frequencies or the the
06:16 - possible pathways this web grant
06:19 - trigrams by Chris Harrison is a project
06:21 - I reference you can see this
06:22 - visualization of those of those engrams
06:26 - is something you might think about and
06:28 - then ice about mashing up to text the
06:31 - Benoa tree project
06:32 - is another useful reference that I'll
06:33 - have a link to and then the other thing
06:36 - that I think is really important to
06:37 - mention is even though this course and
06:40 - this set of videos are I'm focusing on
06:43 - this idea of text and the characters are
06:45 - words being States and this continuous
06:47 - sequence of states this Markov chain you
06:50 - the units of the Markov chain don't have
06:52 - to be text what if there are musical
06:54 - notes or rhythms or colors or vectors
06:58 - what other types of visual designs or
07:01 - sounds or music could you generate
07:04 - through a Markov chain so I hope you
07:06 - will think about making something with
07:08 - this you can share it with me at
07:09 - Schiffman on twitter you could share it
07:11 - in the comments here you can also
07:13 - subscribe to this channel Speight Rhian
07:15 - there's a link in the video's
07:17 - description if you would like to make a
07:18 - financial contribution of course that's
07:19 - optional but with that you can get
07:21 - membership to a slack channel where you
07:23 - can also share your assignments and get
07:24 - feedback from a community of folks in
07:26 - the slack channel so thank you so much
07:28 - for watching I look forward to seeing
07:31 - what you make from this and look forward
07:32 - to seeing you I don't actually see
07:34 - anybody was doing this from just a
07:35 - camera but it feels like I am so I look
07:37 - forward to seeing you in the next video
07:39 - okay bye bye