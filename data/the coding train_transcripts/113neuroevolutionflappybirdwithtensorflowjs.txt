00:00 - (bell dings)
00:01 - - Hello, and welcome to
another Coding Train video
00:04 - about neuroevolution.
00:05 - So this video, what I'm going to do is,
00:08 - I'm going to take my previous, finished,
00:11 - sort of finished, version
of neuroevolution,
00:13 - the process of evolving
the optimal weights
00:16 - of a neural network to solve some kind of,
00:18 - perform some kind of
machine learning task.
00:20 - In this case, I'm making a guess
00:22 - based on the environment
00:24 - of this very crude Flappy Bird game.
00:26 - I'm not making this,
00:26 - the neural network is making a guess
00:28 - whether it should jump or not.
00:30 - So this is a very simple,
actually, scenario to solve.
00:33 - And so it's a nice
demonstration of the idea.
00:36 - I've been meaning to return to
this topic for quite a while
00:38 - so I can try to look at
some more complex scenarios.
00:41 - So neuroevolution can be
applied to a lot of problems,
00:44 - at different,
00:45 - reinforcement learning
algorithms are also applied too,
00:47 - but it's a pretty different technique.
00:49 - There's like five, or six, or seven videos
00:51 - all about neuroevolution
00:53 - that I would recommend you check out,
00:55 - or you can start right here.
00:56 - Because what I'm going
to do in this video is,
00:59 - I am going to take the
existing version that I made,
01:03 - which used a toy neural
network JavaScript library
01:06 - that I implemented in
yet another video series
01:08 - to, kind of, learn more
about neural networks
01:11 - and spin up a very, very basic one
01:13 - in JavaScript without other dependencies.
01:16 - But I'm going to replace
that now with TensorFlow.js.
01:19 - What you're seeing here
01:20 - is the last example from Part Five,
01:23 - where I saved a trained model
01:26 - and I'm loading it into
a version of the game
01:29 - and watching it play out.
01:31 - What I did previous to that,
01:33 - is what you're seeing right here,
01:35 - which is, this is launching
500 random neural networks
01:40 - to try to make guesses.
01:42 - I can use this slider to
sort of speed up the system,
01:44 - and over time those neural networks
01:47 - are going to perform the processes
01:50 - of crossover and mutation.
01:52 - It's not actually doing crossover,
01:53 - it's doing crossover which
would be the act of combining
01:57 - two agents' genetic
material together into one.
02:01 - I'm just taking one and copying it,
02:03 - but then I am mutating it
02:04 - to optimally search for the
configuration of weights
02:09 - that will work the best.
02:10 - And just to remind you, when
I'm talking about the weights,
02:14 - this is what I'm talking about.
02:15 - So we have the Flappy Bird game,
02:19 - this is the agent that
needs to make a decision.
02:22 - In a way it's a classification problem.
02:24 - Should I go up or should
I not go up, right?
02:27 - These are the only two possibilities,
02:29 - a very simple classification problem.
02:30 - Two categories.
02:32 - I have done, I'm the human being,
02:34 - have basically done
the feature extraction.
02:36 - I could use the environment,
the input to the neural network
02:39 - as this image, like all
the pixels of the game,
02:42 - that would be a wonderful thing to try
02:43 - and I would love to do that,
02:44 - especially once I have
convolutional layers
02:46 - with TensorFlow.js,
02:48 - which is not something I have
02:48 - in my toy neural network library.
02:50 - You might not know what
a convolutional layer is.
02:52 - Don't worry, if I use
it, I will explain it.
02:55 - But, I have done that feature extraction.
02:57 - So I have decided that the features
03:00 - that I want to use as inputs
into my neural network
03:02 - are, I think it was like,
the bird y position,
03:07 - the bird, y velocity,
03:10 - the top pipe location,
the bottom pipe location.
03:17 - And then, I'll call this x,
03:20 - the distance to the nearest pipe.
03:22 - So these are what I've decided
03:24 - might be all the important
values to use from this game,
03:28 - to feed into the neural network.
03:29 - So that means the neural network
03:31 - has one, two, three, four, five inputs.
03:34 - These all get normalized
03:36 - into a value with a range
between zero and one,
03:38 - and fed into the neural network.
03:40 - Then the outputs is a
classification problem.
03:43 - So there are just two outputs,
03:45 - and they would each output a number.
03:47 - If I got something like
0.8 here and 0.2 here,
03:51 - that means there is an 80%,
03:52 - basically, an 80%
probability confidence score
03:54 - that I should jump, so I will jump.
03:57 - Now I could actually pick random numbers
03:59 - and that kind of thing,
03:59 - but I'm just going to
take the highest one,
04:01 - the arg max, so to speak, and go.
04:04 - So, neural networks are able to learn
04:08 - a sophisticated amount of
information through hidden layers.
04:13 - So the inputs don't pass
directly to the outputs,
04:16 - but the inputs pass
through a hidden layer.
04:18 - And I don't recall what I picked
04:20 - as the configuration of the hidden layer,
04:22 - but I think it was something like eight.
04:24 - So if there were eight, I
think that's what it is.
04:26 - We'll look.
04:26 - One, two, three, four,
five, six, seven, eight,
04:29 - these are what are known as dense layers.
04:31 - Meaning, every single node
is connected to every node.
04:36 - I will not sit here and draw all of them,
04:37 - but I will start that process.
04:40 - And then all of these, sorry,
are connected to the outputs,
04:45 - like this.
04:46 - So a neural network's core data,
04:49 - the configuration of
that data is in a matrix.
04:52 - 'Cause every one of these
connections has a weight.
04:54 - So if there are five here and eight here,
04:57 - that is 40 weights.
04:59 - If there are eight here and
two here, that's 16 weights.
05:03 - Now the truth of the matter
is, there's also a bias.
05:06 - So there's a bias for each one of these
05:07 - and a bias for each one of these.
05:08 - So the configuration
is all of the weights,
05:11 - all of the biases for each of these nodes.
05:13 - But these are the details
that I cover a lot more
05:17 - in the series about
building a neural network
05:19 - from scratch in JavaScript.
05:20 - So if you want to learn
about that, you can go back.
05:22 - But all you need to know for here,
05:24 - is that I have a way of
creating this neural network.
05:27 - I have a way of feeding
these values into it
05:29 - and looking at the stuff that comes out.
05:31 - So let's go look at that code.
05:34 - So if I go look at the
code, we can see that here,
05:36 - there's this idea of a bird object.
05:39 - And the bird object makes a neural network
05:42 - with five inputs, eight
hidden nodes, and two outputs.
05:46 - I don't know what I've said
here, but this is known
05:48 - as a feedforward neural network.
05:50 - Multilayer Perceptron, it's two layers.
05:52 - The inputs, it looks like a layer,
05:54 - but these are just numbers coming in.
05:55 - There is a hidden layer
in the output layer.
05:57 - That's important terminology.
05:59 - So that's happening here,
and you can see in the bird,
06:03 - the bird sort of like, takes
all of those properties,
06:06 - its y position, the closest pipe,
06:08 - the closest pipe's x
position, its velocity,
06:11 - makes that into an array,
06:13 - feeds that into a predict function,
06:14 - and then I just sort of like,
06:16 - if the first output is greater
then the second output, jump.
06:19 - So this is that process.
06:21 - And all of this happens
06:27 - in this neural network library.
06:29 - So there is neuralnetwork.js
and matrix.js.
06:32 - This is like if TensorFlow.js
was made by one person
06:36 - who really didn't know what
they were doing (laughs).
06:38 - That's what's in here.
06:39 - So what I want to do, is I am
going to delete this folder.
06:45 - Right, I'm deleting it, it's gone.
06:47 - Gone, move to trash, boom!
06:50 - So now when I go over
here and I hit refresh,
06:53 - of course, it doesn't work.
06:54 - Because we have no neuralnetwork.js.
06:57 - Can I get this, I don't have a watch on,
06:59 - in however long I have right now,
07:01 - in the course of this video,
07:02 - working again without my library,
07:04 - but with TensorFlow.js instead.
07:06 - That's the thought experiment here.
07:08 - So here is my html file,
07:10 - and you can see that it
has the P5 libraries,
07:13 - it's loading my neural network library,
07:15 - and then these are all the files
07:16 - for the Flappy Bird game itself,
07:17 - as well as the file that
includes some information
07:20 - about how the genetic algorithm works.
07:22 - So what I want to do, I
need to take these out,
07:24 - and then I want to import TensorFlow.js.
07:27 - So I have the TensorFlow.js website up
07:30 - and this is the script tag
for importing TensorFlow.js.
07:34 - So I'm going to add that here,
and actually I am pretty sure
07:38 - that the most current version
is 1.0.4, so let's add that.
07:43 - And now I'm going to go
back here and hit refresh.
07:47 - Now, of course, ah,
NeuralNetwork is not defined,
07:49 - so we have fewer error messages
here now, which is good.
07:51 - I just want to make sure tf is loaded,
07:53 - so, for example, I can call tf memory,
07:55 - and there is no memory being used,
07:58 - but I can see that tf js is loaded,
08:01 - 'cause I can call tf
functions in the console now,
08:03 - which I'm going to need to
do to figure all this out.
08:06 - All right, so now the thing
I think I want to do is that,
08:09 - actually, I don't need to do this,
08:11 - but I think it would be
nice for me to create,
08:14 - I'm going to create a file called nn.js,
08:19 - and I'm going to make a little
wrapper for a neural network.
08:22 - And actually one of the reasons
why I want to do this is,
08:25 - as you know, I've made a lot of videos
08:27 - using a library called ML5,
08:29 - which is a wrap around TensorFlow.js,
08:32 - which allows you to work
08:33 - with some of the machine
learning algorithms
08:35 - without having to manipulate
08:36 - the lower level details of TensorFlow.js,
08:39 - and so this is, kind of, a little bit,
08:41 - of maybe a preview of that.
08:42 - Because this ultimately,
08:43 - this idea of neuroevolution is something
08:44 - that I would like to work on
putting into the ML5 library.
08:48 - So if I make a neural network class,
08:50 - and I write a constructor,
08:53 - we know here, that in the bird object
08:56 - what it's doing is, it's
making a neural network
08:59 - with five inputs, eight
hidden nodes, and two outputs.
09:02 - So there's no reason why I can't actually
09:05 - just keep that same structure,
09:07 - and I'm going to have three arguments,
09:09 - I'm just going to call them a, b, and c,
09:11 - because at some point I might need
09:12 - to call the constructor in different ways,
09:14 - and I'm going to then say,
09:17 - this.input_nodes equals a.
09:22 - this.hidden_nodes equals b,
09:27 - and this.output_nodes equals c.
09:31 - Then I'm going to put, I already,
09:33 - by the way, I did this in a coding class
09:35 - that I'm teaching, last week,
09:37 - so I kind of have quite a bit
of the sense of the plan here.
09:40 - So I'm also going to write a function
09:42 - called this.createModel
09:45 - 'cause I might need to do
that in different places
09:48 - and then I'm going to
make a separate function
09:49 - called createModel.
09:51 - So this now is the function,
09:53 - where I want to create my neural network,
09:57 - using TensorFlow.js.
09:58 - And I'm going to use the layers API.
10:00 - The truth of the matter is,
10:02 - this particular architecture, right,
10:05 - this is perhaps one of the
simplest, the most basic,
10:08 - vanilla, so to speak, neural
network architectures.
10:11 - It's got two layers, one
hidden layer, very few nodes,
10:15 - there's no convolutions, no fancy stuff,
10:18 - just the very basic.
10:19 - So I could probably do this
10:21 - with just simple TensorFlow
operations itselves,
10:25 - but I'm going to use
something called tf.layers
10:27 - which is actually based on Keras,
10:30 - which is a Python library for TensorFlow,
10:33 - and Python, so many things,
10:34 - I've talked about these in videos.
10:35 - But basically tf.layers is an API
10:38 - that allows me to create a model
10:40 - from a slightly higher level perspective.
10:42 - So one of the key functions in tf.layers,
10:47 - and I'm going to create
a variable called model,
10:49 - is tf.sequential.
10:51 - So if I say tf.sequential,
10:54 - that should create for
me a sequential model.
10:58 - And we can just take a look
at that here on the console,
11:02 - and see, there we go.
11:04 - It made something that works,
11:05 - you can see all these
parameters that it's training.
11:07 - Now interestingly enough,
what I'm doing in this video
11:10 - is not really what
TensorFlow.js is designed for.
11:13 - Almost everything in TensorFlow.js
11:14 - has to do with optimization functions
11:17 - and algorithms for learning
11:19 - and tweaking weights,
and back propagation.
11:22 - I'm not doing any of that.
11:23 - This neural evolution
thing is quirky and weird
11:26 - and all I actually need is
a bunch of neural networks
11:28 - and the ability to feed
the data through them
11:29 - and then I can just delete
that and mix them up.
11:32 - So actually most of this
stuff will not get used.
11:35 - But what I want to create
then, is my first layer.
11:38 - So the first layer, I'm
going to call hidden.
11:41 - So I'm going to say,
11:44 - const hidden equals tf,
11:47 - I think it's dense.
11:48 - So I think I better look up the
documentation at this point,
11:51 - I certainly don't have
the stuff memorized,
11:53 - so I'm going to go to the
API, tf.layer's dense.
11:56 - This is what I'm looking to do.
11:57 - And I can see a nice
little example of it here.
12:01 - This.
12:02 - So I'm going to create a tf.layers dense.
12:06 - Dot dense.
12:07 - Then I need a little object
12:11 - which is going to contain
12:13 - all of the configuration
information for that dense layer.
12:16 - And what are some things I can configure?
12:20 - I can give it the number of
units, that's my hidden nodes,
12:25 - which is the hidden nodes.
12:28 - I also want to tell it how
many things are coming in,
12:31 - like what is the input
shape, so this is the layer,
12:34 - I need to tell it about the input shape,
12:36 - I forget what that's called,
somewhere in here I'll find,
12:41 - inputDim, which would be the
input dimensions, I guess,
12:44 - which would be this.input_nodes,
12:47 - I'm forgetting my this dots,
as always, I always forget.
12:52 - So there we go, what else do I need?
12:55 - Ah, I think an activation
function I definitely need.
12:59 - So activation.
13:02 - Now this merits quite a bit of discussion.
13:05 - An activation function is
how the data gets processed
13:10 - as it's fed forward.
13:11 - So all of these inputs,
13:13 - the numbers out of these
values come in here,
13:15 - they get multiplied by
those weight values,
13:17 - summed together and passed
through an activation function.
13:20 - The activation function's job is basically
13:22 - to squash the values, to
enforce non-linearity,
13:26 - to allow for the stuff
to go to the next layer.
13:28 - And there are all sorts of reasons
13:30 - why you might pick one or the other,
13:31 - there's more recent research
and change over the years,
13:33 - and I've talked about
this in other videos.
13:35 - I'm going to pick sigmoid
13:37 - as a kind of default activation function
13:39 - that squashes all values
between zero and one.
13:41 - Now I'll include some links about sigmoid
13:43 - in the video's description,
if you're curious.
13:46 - So I forget exactly what I do here,
13:48 - but I think I can just
write sigmoid as a string.
13:51 - And somewhere in the documentation
13:53 - there's a list of all
the activation functions.
13:55 - Do I need anything else?
13:57 - I think that's good.
13:58 - So then I just want to say
this.model.addlayer, maybe?
14:02 - So tf.Sequential is what I'm doing here,
14:05 - which would be here,
14:08 - and oh, input shape is
probably what I want,
14:11 - not input dimensions, we'll see.
14:13 - I'm noticing that there's input shape,
14:15 - actually I think that's what I want.
14:17 - That's certainly what I used previously.
14:19 - So input shape,
14:21 - and it would be just that number of nodes,
14:23 - but inside an array.
14:25 - A shape is a way of defining
the dimensionality of data.
14:30 - So if it's just a list of
stuff, like a list of numbers,
14:34 - it's an array with five things in it.
14:37 - So that's how I define a shape.
14:40 - So then I would say, model.add
14:43 - so this.model.add the hidden layer.
14:47 - Now I want to make an output layer
14:51 - which is going to be tf.layers also dense,
14:56 - and I want to,
15:00 - what do I want?
15:01 - The units is
15:04 - this.output_nodes, and then,
15:09 - I actually do not have to define
15:12 - the input shape for this layer,
15:14 - because it can be inferred
from the previous one.
15:18 - So I couldn't infer the number
of inputs from the hidden,
15:21 - because hidden is first.
15:22 - But the last layer here, these outputs,
15:26 - I know that I just added this layer,
15:28 - if it's dense, this is
the shape of the input.
15:29 - So all I need to do is that,
15:31 - and I also need an activation function,
15:36 - I'm going to make it softmax.
15:39 - So softmax is a activation
function which writes sigmoid,
15:44 - squashes all the values to
arrange between zero and one,
15:47 - but it does something even more.
15:49 - It basically enforces
that all of those values
15:52 - also add up to one.
15:54 - It basically turns them into probability
15:56 - or confidence scores and all
of them add up to 100% or 1.0.
16:00 - And there's more to it than that,
16:01 - it's a kind of interval
mathematics involved,
16:04 - but that's the basic gist of it.
16:05 - And that's what I want,
I want the probability
16:07 - of whether I should jump or not jump.
16:09 - So now, I should be able to say,
16:13 - this.model.add output
16:18 - and then if I go back
to the documentation,
16:20 - I don't need to do this fit stuff,
16:22 - but do I need to compile it?
16:25 - So interestingly enough,
usually we need to compile,
16:28 - we need to give a loss function.
16:29 - Traditional machine learning
16:31 - you're going to have a loss function,
16:32 - an optimization function,
16:33 - you're going to train it with data,
16:35 - I'm not doing any of
that in neuroevolution.
16:37 - Remember, if you watched my
other videos on neuroevolution,
16:39 - I'm doing this weird thing,
16:39 - I just need a random neural
network, that's all I need.
16:43 - So I don't know if I
need that compile step,
16:44 - I'm guessing I kind of do,
16:46 - so I'm going to say this.compile,
16:48 - but I'm just going to
give it an empty object.
16:50 - 'Cause I don't care
about those other things.
16:51 - Let's try this.model.compile
16:56 - So now that I have that,
I should be able to,
16:59 - with my code, I'm kind of
getting a little bit further.
17:02 - Like look at that.
17:03 - Cannot read property
'minimize' of undefined.
17:06 - Because I didn't give it any object.
17:07 - Let's get rid of the compile.
17:08 - Let's see if I get it to work
without the step model.compile
17:12 - Maybe compile only has to
do with the learning process
17:14 - that I'm not using.
17:15 - Great, this.brain.predict
is not a function.
17:18 - So now I have the next thing
that I need to implement.
17:20 - So where is that?
17:21 - That's in bird.js and what is it saying?
17:24 - So I'm able to make the neural network,
17:27 - and then I am calling predict
17:31 - and I'm giving it an array.
17:33 - Perfect.
17:34 - So what I need to do now in my class here
17:37 - is I'm going to write a
function called predict,
17:39 - which will receive an array.
17:43 - Now what I would essentially be doing
17:45 - is now calling this.model.predict
with that array.
17:50 - Only here's the complexity
of using TensorFlow.js.
17:54 - TensorFlow doesn't work with
traditional job script arrays.
17:57 - It works with something called tensors,
17:59 - which are multidimensional arrays,
18:01 - they are multidimensional
things, collections of numbers,
18:06 - that are living in the GPU's memory.
18:09 - And that's for optimization purposes.
18:11 - The operations can
happen faster on the GPU
18:14 - which is optimized for
a lot of matrix map.
18:16 - I don't really need that
optimization, to be clear,
18:19 - because I'm working with
the tiniest thing here,
18:21 - but I still need to convert
this stuff to tensor.
18:24 - So I'm going to need to say,
the xs, I'm going to call this,
18:28 - the inputs are often referred
to as xs, equals tf.tensor,
18:34 - and what do I have, what
is the shape of my array?
18:37 - It's one-dimensional,
right, it's just an array.
18:40 - But the thing is that what
TensorFlow.js expects,
18:44 - so this is my data, it comes
in as an array of numbers.
18:47 - One, two, three, four, five, right?
18:49 - All of this stuff, that's
only four, whatever,
18:52 - you get the idea, there's
another one there.
18:54 - The thing is TensorFlow.js thinks like,
18:57 - you would never be so crazy
as to give me just one thing.
18:59 - Usually you're going to
give me a thousand rows
19:02 - or data samples.
19:04 - So it's actually looking for
a shape that is like this,
19:07 - that would have a bunch of these.
19:09 - So I just need to take my inputs
19:12 - and put them in a 2D tensor.
19:15 - Because it's an array of arrays.
19:18 - So if I do that, tensor2d xs.
19:21 - I think that's all I need to do.
19:23 - Then I need to call predict with the xs.
19:25 - Now that's happening synchronously,
19:29 - I need to get the data off of,
19:33 - oh, and this would be ys, sorry.
19:35 - The ys would be the result
of feeding it forward.
19:39 - So that's exactly this process,
19:40 - predict is the function
to feed this stuff forward
19:43 - and now I have ys.
19:45 - I'm going to do something that
is a little bit ill-advised,
19:48 - I want to be able to look at those values.
19:50 - So right now the ys are a tensor
19:53 - and that's living on the GPU.
19:55 - In order for me to pull
it off the GPU to use it,
19:57 - I need to call a function called data,
20:00 - which is getting the actual data
20:01 - and that's not going to
happen asynchronously.
20:03 - So I need to deal with
a promise or a callback,
20:05 - but because I'm working with
such tiny bits of data here,
20:08 - it's going to be much
simpler for me right now
20:09 - to get this working,
to just say, basically,
20:15 - the outputs equals ys.dataSync
20:20 - So the dataSync function
gives me those outputs.
20:22 - Let's console.log them, and
let's say return outputs also,
20:27 - just so we see what's going on here.
20:29 - So I'm now going to run this,
20:32 - NeuralNetwork.predict at
Bird.think is not defined.
20:35 - What did I miss?
20:36 - (bell dings)
20:37 - Sorry, there's a major error in here
20:38 - that you probably all
noticed, but I didn't.
20:40 - I'm taking that array and
converting it to tensor.
20:43 - Arr is the array,
20:44 - and maybe it would make sense
to me to call these inputs.
20:48 - I'm taking the inputs as a raw array,
20:51 - converting them into a tensor,
20:53 - passing them through the model,
20:54 - pulling out the outputs and returning it.
20:56 - Okay, let's see how that goes.
20:59 - Tensor2d requires shape to be provided
21:01 - when values are flat.
21:04 - Oh, guess what, this has to
be inside an array, right?
21:07 - Because the whole point
is it wants the 2d thing,
21:10 - so it wants that array
inside of that array, okay?
21:13 - So that should fix that.
21:16 - Great, oh look, I'm getting stuff.
21:19 - I'm getting things, oh,
look how slow it is.
21:23 - It's so slow.
21:24 - So it's kind of ironic in a sense.
21:26 - Like I'm using a much more
powerful job script engine
21:30 - for deep learning, that
does all the operations
21:33 - on the GPU with WebGL to optimize it
21:35 - and yet it runs super slow.
21:37 - This is really because I'm a crazy person
21:39 - and kind of doing this in a weird way
21:41 - that TensorFlow.js is
not really designed for.
21:43 - I have all these different models,
21:46 - and I'm copying the data with dataSync.
21:48 - So there's a couple of things I could do.
21:49 - Number one is, let's just for now,
21:51 - and I'll have to come back to this
21:53 - or think how to make this run faster
21:54 - or be more thoughtful or maybe
do this stuff asynchronously.
21:57 - But what I'm going to right now,
21:59 - is I am going to just reduce
the number of birds to half,
22:03 - and I'm also going to
something a little bit silly,
22:05 - which I'm going to
inset tf.setBackend cpu.
22:10 - Because actually the amount of data
22:11 - that I'm working with
is so, so, so, so small,
22:15 - I don't really need to use the GPU,
22:16 - I don't need to copy stuff back and forth,
22:18 - let's just use the CPU,
it's perfectly fast enough.
22:20 - Remember how well it worked
22:21 - when I had my own silly
little JavaScript library?
22:24 - So let's use the CPU and I
think you'll find that now
22:27 - it's still running kind of slow.
22:29 - I have a feeling this console.log
22:31 - is definitely making
things kind of unhappy.
22:35 - So let's take out that console.log
22:38 - and let's refresh and there we go.
22:39 - So now it's performing pretty well.
22:41 - Is it as fast as it was before?
22:42 - I don't know.
22:43 - Can I get it back to 500?
22:44 - But it's better.
22:45 - Ah, but now we have a problem.
22:47 - Copy.
22:48 - So copy is not a function.
22:50 - Because I didn't write a function
to copy a neural network.
22:53 - So what do I mean by
copy a neural network?
22:55 - So normally this isn't something
you would necessarily do,
22:57 - but it is fundamental to neuroevolution.
23:00 - And by copying it, we can say
like, this was a good one,
23:03 - let's make a copy of it, to keep it.
23:05 - And I actually want to copy
it, not keep a pointer to it.
23:09 - Because I could also want to mutate it,
23:11 - and I want to mutate it a bunch of times.
23:13 - So I have to figure out,
23:14 - what is the way to copy
a model in TensorFlow.js.
23:18 - And ultimately if you want to try,
23:20 - the proper way to do this,
in terms of neuroevolution,
23:23 - would not be to just make a copy
23:25 - to go to the next generation,
23:26 - but to take two of them,
23:29 - and make a new one
that's kind of a mixture.
23:31 - When I say mixture, I mean
mixture of all these weights.
23:33 - But I'm just going to copy it.
23:36 - So what I need to do,
23:38 - is I need to go back to this class again,
23:40 - and I need to say copy.
23:43 - So if I say copy, I could just say,
23:47 - return new NeuralNetwork,
23:50 - and I could just put like,
23:52 - inputs_nodes, hidden_nodes, output_nodes,
23:53 - but it wouldn't have the right weights.
23:58 - So let's think about this.
24:00 - Let's do,
24:03 - I'm going to say modelCopy
24:07 - equals this.CreateModel.
24:12 - So I'm creating a model,
24:16 - which is the same,
24:17 - ah, and I need to create model
with, oh, it has that stuff.
24:23 - So I can call CreateModel,
it's going to do the right
24:26 - inputs_nodes, hidden_nodes,
output_nodes, okay.
24:27 - So it's not a copy yet, it's
another random neural network.
24:30 - But really, the architecture's the same.
24:33 - So in a sense I've
copied the architecture,
24:35 - I've made a new neural network
with the same architecture,
24:37 - now I need to get those weights.
24:38 - So luckily for me, there is a ntf.js,
24:42 - there are two functions called
getWeights and setWeights.
24:49 - So let's take a look at those functions.
24:51 - So for example, if I
go to the console here,
24:54 - and I would say something like,
24:57 - can I say, let n equal new NeuralNetwork,
25:03 - what's it, 8.5.2, right? No, 5.2.8, 5.8.2.
25:06 - So I'm making one of those.
25:07 - And so n.model is that model itself.
25:11 - And you can see, it's got
all that data in there.
25:14 - So what I want to do is,
say n.model.getweights.
25:19 - No? Oh, I said mode.
25:22 - I want to say n.model,
can we just click here,
25:25 - model.getWeights.
25:26 - These are all the weight
matrices, why are there four?
25:29 - Shouldn't there just be two?
25:30 - Well, there's four because there are
25:34 - the weights and the biases, one, two,
25:36 - the weights and the biases, three, four.
25:38 - So that is what is here.
25:42 - So all I need to do is take
these weights and copy them.
25:46 - So let's see.
25:48 - I could say, constant weights
equals this.model.getWeights
25:55 - and then I could say modelCopy.setWeights
26:04 - Then I could say return new NeuralNetwork
26:11 - with modelCopy.
26:12 - Now there's a bit of a problem here
26:14 - that I think I'm going to have to do,
26:16 - but this is pretty decent start.
26:18 - Right, make a model of
the same architecture,
26:20 - get the weights from the other one,
26:22 - and put those weights in the
new model, and then return.
26:24 - I think there's going
to be a problem here,
26:26 - but this is kind of the idea.
26:27 - So now, if I were to ...
26:32 - Ah, what I need to do here is,
26:34 - now I'm making a neural network
26:35 - without giving it the input nodes,
26:37 - I need to basically say,
26:40 - if a is an instance of
tf.sequential, is that right?
26:47 - Let's see, so then if I
go back to the console,
26:50 - n.model instanceof
26:54 - tf.sequential false,
26:57 - tf.sequential with a capital, true!
27:00 - Okay, I need to check if
it's tf.sequential object,
27:04 - then this.model equals a.
27:08 - So, in other words, I have two ways
27:10 - of creating a neural network.
27:11 - If I pass the constructor
an existing model,
27:15 - then I just assign it.
27:17 - Otherwise, I create a new one.
27:20 - However, I've really always
got to keep track of what,
27:22 - it's a little, this is
some redundancy here,
27:25 - 'cause the input nodes,
hidden nodes, and output nodes
27:27 - are in the model itself.
27:28 - But let's just add another argument here.
27:30 - And then I can say, also, this is awkward,
27:34 - I could refactor this later.
27:37 - So now I'm also going to always require
27:40 - that I also give it the shape basically,
27:43 - and so here when I'm making
this new neural network
27:46 - with model.Copy,
27:47 - I'm also going to say this
dot, what is it, input_nodes,
27:54 - this.hidden_nodes,
27:59 - this.output_nodes.
28:01 - Okay, so this is me making
28:03 - that new neural network with modelCopy
28:06 - and I'm also just giving it the shape also
28:08 - so that it retains that information.
28:10 - So now, I should be able to go back,
28:13 - and we've got a copy function.
28:17 - Let's speed this up.
28:19 - Ah, can't read!
28:21 - Cannot read property
'setWeights' of undefined.
28:23 - That's not good.
28:24 - (bell dings)
28:25 - Okay, I've sort of made
a mistake in the way
28:27 - that I've designed this program.
28:28 - What I want to say is
28:30 - this.model equals this.createModel
28:33 - and I don't actually want to set it here,
28:35 - I'm going to just make in
the createModel function,
28:38 - which is where I'm going
to make this a variable,
28:44 - const model, and then I'm
going to say model.add,
28:50 - model.add,
28:52 - and then I'm going to say return model.
28:54 - So this function, this could
be leading somewhere else,
28:56 - a static function or something,
28:58 - but it's making the model
29:00 - and it is in setup,
sorry, in the constructor.
29:05 - It is creating it and returning it
29:07 - and placing it in the instance variable,
29:09 - or here it's creating it and returning it
29:12 - and putting it in these copy variables.
29:13 - So that should work better.
29:16 - So let's see.
29:18 - Great, so now I don't
have a mutate function.
29:20 - No problem, no problem, this
is the way to deal with that.
29:25 - Bird.js line 33,
29:28 - Bird.js line 33,
29:33 - let's just not bother at mutate.
29:36 - No worries, don't mutate.
29:40 - Let's speed this up.
29:42 - (upbeat electronic music)
29:46 - All right, let it run for a bit,
29:47 - we can sort of see, did it get better?
29:49 - Kind of, but there's only one,
29:51 - they're all making the
exact same decisions.
29:54 - So two things are wrong here.
29:56 - Number one is, without mutation
I can't get any variety.
30:00 - But there's something else
that I'm pretty sure is wrong,
30:02 - not a 100% sure, but I'm
pretty sure is wrong,
30:05 - so I want to protect myself against this,
30:07 - is I haven't really made a copy.
30:09 - So I think in the case of my
copy function, which is here,
30:14 - this is the weights as tensors.
30:17 - And I am assigning them to the copy.
30:20 - But they are all being
assigned the same tensors.
30:22 - Even though they are multiple models,
30:24 - they're all using the same weight table.
30:26 - So if I were to mutate them,
30:28 - they're all going to mutate
in exactly the same way.
30:30 - So I need to really make
sure I'm copying them.
30:32 - And there is a function
in ts.js called clone,
30:35 - which is to make a new
tensor that is a copy
30:37 - of an existing one.
30:39 - So I think what I should do is,
30:41 - I should say const
weightCopies is a new array,
30:47 - and then for let i equals zero,
30:50 - i is less than weights
index i, i plus plus
30:54 - No, sorry, weights.length,
30:56 - so I'm going to iterate
over all the weights,
30:59 - and I'm going to say
weightCopies index i equals
31:03 - weights index i dot clone.
31:05 - So this is kind of the idea,
like deep copying, I guess,
31:09 - I'm not sure if I'm 100%
accurate about that,
31:11 - but the idea of actually
copying them into a new tensor,
31:14 - so it's not the previous one,
31:16 - and then I should be able
31:18 - to set the weights to weightCopies.
31:21 - So this should guarantee
31:24 - that I really got it copied correctly.
31:26 - So let's do that.
31:29 - I'll run it again, speed
it up a little bit.
31:32 - So it's still converging, kind of,
31:35 - into it alternately
being just one of them,
31:37 - that the one that happened to do the best.
31:39 - But it's not doing that
31:40 - because I haven't properly copied,
31:42 - it's doing that because
there's no mutation.
31:43 - And you can see actually,
31:44 - that it seems to be taking a
little longer to get there.
31:47 - So I think maybe that
copying is better now.
31:50 - Okay, so I do need that mutate function.
31:53 - So what do I need?
31:55 - Where was mutate?
31:57 - So I need a function called mutate,
31:59 - and it's getting argument 0.1,
32:01 - which is presumably the mutation rate.
32:03 - So the process of mutation
in a genetic algorithm
32:06 - is to say, look at the
genetic information,
32:08 - which is basically all of
these weights and biases,
32:11 - and one by one examine them,
32:13 - and at some probability
make it sum adjusted,
32:17 - make it random, change it, mutate it.
32:19 - So that's what I need to do.
32:21 - So it's more involved
32:23 - than just making a copy of the weights.
32:25 - I need to look at every
single weight individually
32:28 - and then mutate it 10% of the time.
32:31 - I'm not sure how to do
this, but let's see, okay.
32:36 - So if I go into neural network
32:39 - and I go into,
32:42 - I made a copy function, now let's look at,
32:45 - sorry, let's look at mutate.
32:49 - So I want to do the same thing.
32:50 - The first thing that I want to do,
32:55 - is I want to get the weights.
33:00 - What I call this, this.model.getWeights.
33:04 - Then I want to maybe have mutatedWeights,
33:13 - is a new, just an array.
33:16 - So I get the current weight tables
33:19 - and then, weight tensors, sorry,
33:21 - and then I'm going to go through
and make mutated weights.
33:24 - So let me once again loop over this array,
33:30 - and I'm going to say, tensor
equals weights index i,
33:35 - 'cause it's a tensor, and
then what I want to do,
33:39 - I want to get the values,
33:43 - so I'm going to use dataSync again.
33:46 - I want the shape,
33:48 - 'cause I'm going to need
that when I convert it back,
33:50 - I think I can just do this.
33:52 - So I need the tensor, which is the data,
33:56 - I'm going to need to retain the shape,
33:58 - because I need to put
it back into a tensor,
34:01 - but I want to mutate them.
34:02 - I don't think I can mutate the tensor
34:04 - without looking at the values.
34:05 - Who knows?
34:07 - So now I'm going to go
through and I'm going to say,
34:10 - let j equals zero, j is
less than values.length,
34:18 - j plus plus.
34:21 - So the actual weight which
I call w is values index j.
34:26 - And what I want to do is say
34:28 - values index j equals a new random weight.
34:30 - So I could do something like
this, right, just give it,
34:34 - and I don't even actually
need the current weight,
34:38 - I could just make a random one, right?
34:40 - I'm mutating, oh, but
only 10% of the time.
34:43 - So this gets a rate, so I could say,
34:45 - if random one is less than that rate,
34:49 - then make a new weight.
34:53 - I do think this is a case
34:54 - where there might be some benefit
34:56 - to rather than picking
a totally new weight,
34:58 - and I don't know what
initialization was used
35:01 - to make the initial random weights,
35:02 - that's probably somewhere
in TensorFlow.js,
35:05 - so there might be some
operation I can call here,
35:07 - but I'm just going to tweak it.
35:08 - So this is why I want to grab that weight,
35:10 - and I'm going to say, weight plus,
35:13 - and I'm going to use P5's
random Gaussian function
35:16 - which basically gives me a random number
35:18 - that's kind of near, with a mean of zero
35:20 - and a standard deviation of one,
35:22 - so it's just going to adjust
that weight a tiny bit.
35:24 - So it's like, kind of,
35:25 - what you might see with graded
descent, dial up, dial down,
35:29 - but I'm just making a total guess,
35:30 - just dial it, some direction.
35:33 - So it's going to equal that,
35:35 - this loop is looking at
individual set of weights,
35:38 - it might be between inputs and hidden
35:40 - or hidden and outputs or
might be one of the biases,
35:43 - either of those weights.
35:45 - And I have it as a tensor.
35:48 - And then I have its values,
now I mutated those values.
35:51 - Aha, I need to make a new tensor,
35:54 - newTensor equals tf.tensor,
and give it those values again,
36:00 - right, the mutated values, with the shape.
36:03 - So this is what's nice.
36:05 - I don't have to worry about
what the original shape was,
36:09 - dataSync is just going
to flatten everything
36:11 - and give me all the numbers,
pretty sure, I think.
36:15 - And then, what I can do, is
then mutate those, it's flat,
36:20 - and I can put it back into
a tensor with the same shape
36:23 - and it should be back
just like it was before.
36:25 - I can say mutatedWeights index
i, is now that new tensor.
36:31 - So I grabbed the tensor, I got
its shape, I got the values,
36:34 - I mutated them, I made a new tensor,
36:35 - and it's going to go in my
new array, of mutated weights.
36:39 - And then when that's done, I can say,
36:42 - this.model.setWeights mutatedWeights.
36:49 - I think this is mutation.
36:51 - There is one mistake here,
which is that tensor, sorry,
36:54 - which is that
36:56 - dataSync is actually not
making a copy of the values.
37:01 - So I'm still going to be stuck
37:04 - with a lot of things pointing
to the same exact data,
37:06 - so if I mutate one and
it's copied for a bunch,
37:10 - all of those are going to
get mutated in the same way,
37:11 - and I want to have a bunch
of different mutations
37:13 - happening in parallel.
37:14 - I discovered this issue around
dataSync in this discussion,
37:18 - and thank you so much
to one of the creators
37:20 - of TensorFlow.js, Nikhil Thorat,
37:22 - who answered my question about this,
37:25 - so dataSync is mutating,
37:28 - I'm mutating the underlying
values in the tensor.
37:31 - Apparently there is an arraySync function
37:33 - that I could use instead of dataSync,
37:35 - that doesn't actually copy,
37:36 - but me, according to a Coding Train viewer
37:39 - actually suggested that
I just call .slice,
37:42 - which is a job script function
for copying the array.
37:44 - So this I really really need in there.
37:46 - Because I really have to make sure
37:48 - that I'm not mutating the
same underlying information.
37:52 - So I should add .slice here.
37:55 - And then I think I have this done,
37:57 - we're about to find out?
38:00 - The only way to find out, there
are other ways to find out,
38:03 - I'm sure, is to let this run for a while
38:06 - and sort of see if it gets anywhere.
38:08 - So I'll let that do
that for a little while.
38:10 - (upbeat electronic music)
38:15 - So I would say that this works.
38:17 - Because, there we go.
38:19 - I have an agent that learned basically,
38:21 - again this is a very simple
problem for it to solve,
38:24 - but this technique that I've now done,
38:27 - there's no reason why
it couldn't be applied
38:29 - to a more complex scenario.
38:31 - So I now have neuroevolution
with TensorFlow.js,
38:35 - but I'm not finished.
38:37 - I could do this in the next part,
38:39 - but there's a huge problem here.
38:41 - I haven't thought at all
about memory management.
38:43 - And if I do tf.memory in the console,
38:49 - ah, I have 128,870 tensors
and I'm using all these bytes,
38:54 - oops sorry,
38:56 - if I start from the beginning
I have that many tensors,
38:58 - and look how it's going up.
38:59 - So I'm leaking memory like crazy.
39:02 - I'm not cleaning up any of
the tensors that I'm using.
39:06 - And so the way to do that,
39:08 - I don't know why I'm coming over here,
39:09 - but it's very important.
39:11 - In addition to the getWeights
and setWeights functions,
39:13 - there are two functions
39:15 - that I really need to
be conscientious about
39:18 - when working with TensorFlow.js.
39:19 - One is tf.tidy, and this is a function
39:23 - that you can basically give it a callback,
39:25 - and any code that you put up in there,
39:27 - it will do the cleanup for you.
39:29 - So it's kind of like TensorFlow.js's
39:31 - garbage collection feature.
39:32 - It's like just put the
coding here and don't worry,
39:34 - we'll take care of disposing
memory that's not used.
39:37 - So this is probably one I
want to use most of the time,
39:39 - but any tensor I could also
individually call dispose on,
39:44 - when I'm not using it anymore.
39:45 - So let me go and find everywhere
in my neural network code
39:48 - that I'm using tensors,
39:50 - and make sure I'm cleaning up memory
39:52 - that's not being used anymore.
39:53 - So I'm in my code, scrolling,
scrolling, scrolling,
39:57 - this should be fine,
39:58 - I don't think I need to dispose models,
40:00 - maybe I do, but here, okay,
oh, look at this mutation.
40:04 - So I'm creating new
tensors, I'm doing stuff,
40:07 - this is definitely a place
where I want to say tf.tidy,
40:13 - and the nice thing is it's
literally as simple as this,
40:18 - I just put everything inside of tf.tidy.
40:22 - So maybe there's a slightly more optimal,
40:24 - better way of doing this,
but this should work.
40:27 - In predict, this one's a little easier,
40:29 - I could say like, I don't
need the xs anymore,
40:33 - so dispose of those.
40:36 - And now once I have the outputs,
I can dispose of the ys.
40:40 - So I can put manual dispose in here,
40:42 - like that's me disposing,
40:43 - but I probably will also just use tf.tidy,
40:49 - same exact thing.
40:53 - But there's a little something
else I need to do here.
40:56 - Because this code is returning something,
40:58 - I need to return the result of tf.tidy.
41:01 - So that should be fine.
41:02 - I feel like the model stuff is fine,
41:04 - this should be all the
cleanup that I need.
41:06 - But let's see.
41:07 - Oh, copy, oh, yeah.
41:10 - I guess let's do tf.tidy
in here also, tf.tidy,
41:19 - let's do this.
41:20 - I also need to say return,
since I'm returning a new model,
41:23 - I mean I could put tf.tidy in
this, let's see if I need to.
41:28 - Let's look at the memory now.
41:32 - So that's a 1000 tensors,
that makes sense,
41:34 - because I have 250 birds
41:38 - and there's four tensors,
four weight matrices,
41:41 - that's a thousand.
41:42 - So 3000, oh, I missed something.
41:46 - So every generation, I'm
keeping some of the old tensors
41:50 - that I don't need anymore.
41:52 - So let's try putting, I
suppose, tf.tidy here,
42:02 - and also return, and
let's see what happens.
42:06 - Ah wait, something undefined.
42:09 - What did I mess up?
42:10 - (bell dings)
42:12 - That didn't seem to work.
42:13 - But I have a better idea,
42:15 - I don't know why that didn't work,
42:17 - there's probably a very good reason,
42:18 - and I will hear from somebody about it,
42:20 - but I think I know what I need to do.
42:22 - Where am I copying?
42:23 - Here, but I have tf.tidy here.
42:28 - I was thinking that once I
make this new neural network
42:34 - I would call dispose
on the existing model.
42:37 - (bell dings)
42:38 - Okay, after examining this for a bit,
42:39 - with a little bit of help from the chat,
42:41 - it looks like the issue
that I have is that,
42:44 - it's a little bit weird,
what I'm doing here,
42:45 - I have a bunch of different arrays,
42:47 - I have birds, saved and savedBirds.
42:51 - So savedBirds is kind of
like the previous generation,
42:55 - and I make all these new
birds for the next generation,
42:58 - put them in the birds array.
42:59 - But the models, the
savedBirds models still exist.
43:04 - So I need to dispose those.
43:08 - So the way I could do that
43:09 - is find the place where I'm
done making the next generation,
43:13 - and iterate over the savedBirds.
43:14 - The way I could do that is, I could also,
43:16 - in the bird object right now,
43:18 - I could add a function called dispose,
43:21 - little silly to do this, but why not,
43:23 - which just says this.brain.dispose
43:27 - and then in the neural network class,
43:32 - I now have a function called dispose,
43:35 - which I would say this.model.dispose.
43:38 - So this is sort of a manual
disposal of all of the memory
43:43 - that that particular model is using.
43:45 - And then somewhere in the sketch,
43:48 - when I call next generation,
43:50 - that makes the next
generation and then I go on,
43:56 - that's in my genetic
algorithm, which is here.
43:58 - So this is making the next generation,
44:02 - and then clearing the savedBirds.
44:04 - So before I clear the savedBirds,
44:08 - I think it's, I could
just use this total value,
44:12 - what I can do now is,
44:13 - I can say savedBirds index i dot dispose.
44:20 - So that should call the dispose function
44:21 - which calls the dispose function
44:23 - which calls the dispose function.
44:24 - So now, I'm looking at a 1000 tensors,
44:30 - and then let me speed this up,
44:32 - so it gets to the next generation faster.
44:34 - Wow, it just got like a good one.
44:36 - Next generation and now,
44:39 - still a 1000 tensors.
44:41 - (whistles)
44:42 - So this is really complete.
44:43 - I mean there's so much
more that I could do,
44:45 - but this is complete
44:47 - in terms of the idea of
neuroevolution with TensorFlow.js
44:51 - and this particular Flappy Bird game.
44:53 - All right, so what's coming next,
44:55 - what could you do, what could I do next?
44:57 - So one thing is, in my
previous coding challenge
45:00 - I saved the model to reload later,
45:02 - and TensorFlow.js has
functions for doing that.
45:04 - Save and load layers, models,
45:06 - I forgot what those functions are called,
45:08 - but they're in there.
45:09 - So I could come back and add that to this.
45:12 - The other thing would
be to apply this idea
45:14 - to different scenarios, different games,
45:16 - to different environments.
45:18 - One of the things that I would like to do
45:20 - is work with steering agents,
45:21 - whether they're finding
food or avoiding predators,
45:23 - that would be an interesting thing to try.
45:25 - So hopefully I'll come
back and do this same idea,
45:28 - but with a different environment
45:30 - that has maybe more complexity to it,
45:32 - with a less obvious result.
45:33 - Because I have a feeling
45:35 - that I could have gotten
a bird to solve this game
45:38 - with just some if statements.
45:39 - Like I could probably guess like,
45:40 - you should always jump,
if you're too high,
45:43 - you shouldn't jump if you're higher
45:45 - than where the pipe opening is,
45:48 - and if you're lower, you should jump.
45:49 - So I could write an if statement
45:51 - instead of using neural network.
45:52 - Thank you for watching.
45:53 - If you make your own version of this,
45:55 - if you have ideas for
how to make this better,
45:57 - please let me know in the comments
45:58 - and also over at thecodingtrain.com,
46:00 - you can submit a link
to your version of this
46:03 - running in the browser,
46:04 - and I'll take a look at those
46:05 - and hopefully share them in a
future video or a livestream.
46:07 - Thanks for watching and goodbye.
46:09 - (whistles)
46:10 - (upbeat electronic music)