00:00 - (bell dinging)
00:01 - - Hello, and welcome to
another Beginner's Guide
00:03 - to Machine Learning with
ml5.js in JavaScript.
00:06 - So I'm here.
00:07 - It's been a while since I
added a video to this playlist,
00:09 - and a bunch of things
00:10 - about the ml5 library itself have changed.
00:13 - There's a new release, 0.3.1.
00:17 - There is a brand new website,
00:18 - which you can find
right here at ml5js.org.
00:21 - So to some extent, this
video is really an update
00:24 - about the library, but
I'm also going to look
00:26 - at one particular feature,
00:28 - a new feature of the library,
sound classification.
00:31 - The machine learning
model that I'm gonna use
00:33 - in this video is the
Speech Command Recognizer,
00:36 - and this is a model available from Google
00:39 - as part of TensorFlow.js models.
00:42 - Now, so this is a really
important distinction.
00:44 - I am not here to train a sound classifier.
00:48 - I might do that in a future video
00:49 - and show you about how to
apply transfer learning,
00:52 - which is something I did
with images, also to sounds.
00:55 - I just gonna make use
of a freely available,
00:58 - pre-trained machine learning model.
01:00 - Anytime you use one of those things,
01:03 - even in just a playful
and experimental way,
01:05 - which is what I'm doing,
01:06 - it's good to do a little bit of research
01:07 - and take a look at like
well, how was this trained,
01:09 - what the data, what are the considerations
01:13 - around how the data was collected?
01:15 - And so I encourage you to
read through the read me
01:18 - here on GitHub and in particular,
01:21 - to click over and read the original paper
01:23 - about this speech commands model,
01:26 - and there you'll see, if you look,
01:28 - it talks about some of the datasets
01:30 - like Mozilla's Common Voice dataset,
01:32 - 500 hours from 20,000 different people,
01:35 - this LibriSpeech, 1,000
hours of read English speech.
01:40 - I don't know how to say this, TIDY DIGITS,
01:42 - TIDIGITS, T DIGITS,
25,000 digit sequences,
01:46 - which apparently was probably neat, right?
01:48 - It's just like hours
and hours of me reading
01:49 - this random number book
over and over again.
01:52 - But so I encourage you
to check out this paper,
01:54 - and you can also find code
for how to use this model
01:58 - at TensorFlow.js in the tfjs
models, GitHub repo itself.
02:03 - I also want to interrupt
this video for a second
02:05 - to talk about how the sound
classifier actually works.
02:08 - This is kind of a
surprising little tidbit,
02:10 - and I'll come back to this more
02:12 - if at some point I create a video
02:14 - about training your own sound classifier.
02:16 - Now, there different
ways you could do this.
02:17 - This isn't the way to
make a sound classifier,
02:20 - but this is the way that
this particular model works.
02:22 - It's actually shockingly,
02:24 - amazingly doing image classification.
02:26 - So if you image we have this thing
02:29 - that's called a
convolutional neural network.
02:32 - This is the underlying architecture,
02:34 - the structure of that
machine learning model
02:37 - that does the classification.
02:40 - Typically this kind of model is something
02:42 - that we would put images in.
02:44 - Like we might have images of cats.
02:46 - We might have an image of a turtle.
02:50 - That's not really turtle, but whatever.
02:51 - So the idea is that we're
sending these images in
02:54 - and getting back a label
02:57 - and maybe a confidence score.
02:59 - So it's the same idea.
03:00 - The only thing is now
we wanna send in audio
03:04 - and get back a label like up
03:06 - or one and a confidence score.
03:10 - So how would we convert
sound into an image?
03:14 - Now, again, there are other
neural network architectures
03:17 - which you could receive sound data
03:20 - in maybe a more direct fashion,
03:23 - but if you have ever looked
at a graphic equalizer
03:27 - or some type of sound
visualization system,
03:30 - I've made examples like this in p5,
03:32 - you can draw something
that's often referred
03:34 - to as the spectrogram,
03:36 - which is basically a graph
of all the various amplitudes
03:41 - of frequencies, the wave
patterns of the sound itself.
03:45 - So if we took a one second spectrogram
03:50 - and made that into an image,
03:52 - we could then send that image
03:53 - into a convolutional neural network
03:55 - saying that's the image that is produced
03:58 - from the spectrogram of
somebody saying the word, up.
04:02 - So underneath the hood, this
machine learning system,
04:05 - even though it's designed
to work with audio data,
04:08 - it first takes that audio data,
04:10 - converts it into an image
04:11 - and then sends it through
a very similar types
04:14 - of neural network architecture
04:16 - to standard image classification models.
04:18 - And you can read more about
that in that paper itself.
04:22 - However, I'm gonna show you
how to access this model
04:26 - in a quick way with the ml5 library.
04:29 - And this is the new as of
today, which is I dunno.
04:33 - What's today's date?
04:34 - June 13th, 2019 (laughing).
04:38 - I'm gonna show you how to
use this with the ml5 library
04:40 - as it stands today.
04:41 - So I'm gonna click here under reference.
04:43 - One thing you should see,
there's a lot of new features
04:45 - have been added to the ml5 library.
04:47 - I'm gonna come back and do
videos about more of those,
04:49 - but the one I wanna highlight
is sound classifier.
04:52 - So I'm gonna click on this,
04:52 - and for all of the different
functions available in ml5,
04:57 - you'll find a documentation page
04:59 - with some narrative documentation,
05:01 - a little bit of a code snippet
05:02 - and then some written documentation
05:04 - about what the function names are
05:06 - and the various parameters
and things like that.
05:08 - And by the way, I'm
noticing now (laughing).
05:11 - This will hopefully not read.
05:12 - This is like a mistake (laughing).
05:14 - This is documentation that's actually
05:16 - for either Body-Pix or
maybe the U-Net model,
05:19 - which does something
called image segmentation.
05:21 - So we gotta get that fixed.
05:22 - I'm sure many GitHub issues and fixes
05:25 - will be out and done by
the time you see this.
05:27 - So in case you've forgotten
how to use the ml5 library,
05:30 - I'm just gonna show you as it's documented
05:32 - on the ml5 webpage.
05:34 - So first of all, you can
go here to this Quickstart.
05:36 - You can actually just click on this
05:38 - open p5 web editor
sketch with ml5js added.
05:41 - You know what, I'm gonna so that.
05:42 - That's the way I'm gonna do it.
05:44 - But you also could just put a
script tag in your HTML page
05:49 - referencing the current
version of the library,
05:50 - which, as I said, is 0.3.1 as of today,
05:53 - but probably while you're watching it,
05:54 - it will be a higher number.
05:56 - So lemme go and just
open up this link here,
06:00 - and now I'm in the p5 web editor.
06:04 - You could see the name of the
sketch is ml5js boilerplate.
06:08 - Thank you, Joey Lee who's
a contributor to ml5.
06:10 - He's done a ton of work on the website
06:12 - and all of the different features.
06:14 - And oh, this should actually be 3.1.
06:17 - I'm gonna fix that, uh-huh.
06:19 - I'm gonna hit save, and
then I'm gonna rename it
06:22 - to sound classifier.
06:26 - And I am going to then go over here
06:29 - and go to sketch.js,
06:30 - and I'm then I'm gonna run this,
06:32 - and we should see.
06:33 - There we go.
06:33 - So now we know it's working
06:35 - because there's a little console log
06:36 - to log ml5.version.
06:38 - If I hadn't imported the ml5 library,
06:41 - I wouldn't see that, and we see that here.
06:43 - So, what are we gonna do?
06:44 - Let's load the sound classifier.
06:47 - Now, most of the models,
I haven't been using this
06:50 - in my previous videos,
06:51 - most of the models in ml5 are
now actually available to you
06:54 - in preload, meaning you don't
need a callback function.
06:57 - You can just load the model in preload,
06:59 - and it'll be ready by the
time you get to setup.
07:01 - So I'm gonna make a variable
called soundClassifier.
07:05 - In preload, I'm gonna say soundClassifier
07:08 - equals ml5.soundClassifier.
07:11 - Now, I need to tell it
07:12 - what model I want to load.
07:15 - So I need to, in here, put the name
07:18 - of the model I wanna load,
07:19 - and in theory, in the future,
07:21 - there might be a bunch
of different options,
07:23 - different kinds of sound classifiers
07:24 - or maybe a sound classifier
you've trained yourself
07:27 - that you wanna put in there,
07:28 - and I'll come back eventually,
07:30 - show you videos about how to do that.
07:32 - But for right now, I'm just gonna say
07:33 - SpeechCommands,
07:35 - and then I already forgot
what it was called.
07:37 - So I'm gonna go back to the
ml5 website, which is here.
07:40 - I'm gonna go to reference.
07:41 - I'm gonna go to soundClassifier,
07:43 - and I'm looking for it here.
07:44 - So it's SpeechCommands18w.
07:47 - This is a particular model
07:49 - that's been trained on 18 specific words,
07:51 - and you can see what those are.
07:53 - The 10 digits from zero to nine,
07:56 - up, down, left, right, go,
stop, yes, no, that's 18.
08:00 - 10 digits, eight different words.
08:03 - All right, so now I'm gonna go,
08:05 - so it was 18w,
08:08 - and then, once that model is loaded,
08:11 - I need a callback.
08:13 - So I could just say
soundClassifier.Classify,
08:18 - and I might just call it gotResults.
08:20 - So in other words, I'm.
08:21 - Oh, it's not defined, right?
08:22 - So I'm telling the sound
classifier to classify.
08:25 - Now, by default, it's just going to listen
08:28 - to the microphone's audio.
08:30 - Maybe in the future, part
of ml5 will be able to offer
08:33 - hooks to how you can, to connect it
08:35 - to a different audio source,
08:36 - but it's basically just gonna work
08:38 - with the microphone's audio.
08:40 - Then I can write a
function called gotResults,
08:46 - and I'm gonna get rid of the draw loop
08:48 - 'cause I don't need that right now.
08:49 - Lemme just turn off auto refresh
08:51 - so that it doesn't keep refreshing.
08:55 - And then now, if you remember,
08:57 - ml5 employs error first callbacks,
09:01 - meaning the callback function
requires two arguments,
09:05 - an error argument in case
something went wrong,
09:08 - and a data or results
or some other argument
09:10 - where the actual stuff is.
09:12 - So I'm gonna say error,
09:13 - and then I'm gonna say results.
09:15 - And then I could do a little
like basic error handling.
09:17 - I'm just gonna say console.log
09:22 - something went wrong,
09:24 - and then I can also actually
log the error, all right.
09:31 - And then, so now,
09:34 - and then I'm gonna say
console.log(results).
09:38 - So let's see if we get anything.
09:40 - Oh, I have to run it again.
09:43 - And you could ignore this error.
09:44 - Oh, (gasping) something came in!
09:47 - Ready?
09:49 - Up.
09:51 - I just wanna stop and mention
09:52 - that if you're following this along,
09:54 - hopefully your browser
is asking for permission
09:56 - to use the microphone.
09:58 - The reason why that didn't
happen here in this video
10:00 - is because I've already set my browser
10:03 - to allow use of the microphone
on the p5 Web Editor pages,
10:07 - but for security, you can't
just access anybody's microphone
10:10 - from a webpage without the
user giving permission.
10:12 - So hopefully you saw that happen,
10:14 - and if you you didn't,
10:15 - that might be why you run into an error
10:17 - if you haven't given that permission.
10:19 - This is getting a little hard to debug
10:21 - just because so much
stuff is happening here
10:22 - on the console and this huge arrays,
10:24 - but there's actually
something that I missed
10:26 - that I could add here, which
is an options variable.
10:29 - So one of the things I could tell,
10:31 - there's a lot of things
I can set as properties
10:33 - or parameters for how the
sound classifier should work,
10:36 - but there's a very simple one,
10:38 - which I'm gonna just look
it up in the documentation
10:40 - 'cause I don't remember.
10:42 - It's called the probabilityThreshold.
10:44 - I'm actually just gonna
copy-paste this here.
10:46 - What this means is basically
10:49 - the sound classifier is
going to trigger an event.
10:53 - Right now I'm console logging
all of this information
10:56 - about what it thinks it heard
10:58 - based on a confidence
level for how sure it is
11:01 - it heard one of those keywords.
11:02 - And right now, a lot of
those events are triggering
11:05 - because I don't know
11:06 - what the default probability threshold is.
11:08 - Maybe it was .7.
11:09 - Maybe it's .5, but I'm
gonna make that really high.
11:12 - I'm gonna say .95.
11:13 - So it has to have a 95%,
11:16 - the machine learning model
has to calculate a 95%
11:20 - confidence score before it
11:22 - gives the event back to me in ml5.
11:25 - Once I've created that
options variable with .95,
11:27 - I need to pass it into the constructor
11:30 - as the second argument.
11:31 - So now we pass it in there.
11:33 - I'm gonna run the sketch.
11:34 - I'm gonna say the keyword up,
11:36 - and then I'm gonna try
to look into the console
11:38 - to see if that's what came in.
11:42 - Up.
11:48 - And there we go.
11:49 - Look at that!
11:50 - Now other stuff is coming
in, but you saw it there!
11:53 - So rather than kind of
debug with the console,
11:55 - let me actually put what I said
11:58 - onto the webpage itself.
12:00 - Also, to make this easier to see,
12:02 - let me actually console.log(results
index zero label
12:06 - and results index zero,
12:08 - I believe it's called confidence.
12:09 - So rather than have this big
array logging in the console,
12:13 - let me do this.
12:14 - All right, we need to
have a 95% confidence,
12:17 - and I'm gonna run this.
12:20 - Up.
12:23 - Three, four, five,
12:27 - six, seven, eight.
12:30 - I'm quickly adding background color white
12:33 - to the HTML body
12:36 - because what I wanna
then do, just quickly,
12:38 - before I finish this off,
but to finish this off,
12:40 - let me just add a DOM element
using the p5 DOM library.
12:43 - I'm gonna just say resultP
for results paragraph.
12:47 - I'm gonna say resultP equals createP
12:54 - waiting, and then,
right now, I'm gonna say
12:59 - resultP.html.
13:02 - Then I could turn these
results into a string
13:04 - by using a string literal.
13:05 - So back tick and then put curly brackets.
13:09 - Put a colon here and curly brackets
13:12 - and a closed back tick, okay.
13:14 - And let me also say resultP.style,
13:20 - is it font size, font-size,
13:23 - just 32 point so we'll be able to see it.
13:26 - All right, here we go.
13:28 - Ready for this?
13:32 - One, two, five,
13:36 - up, down, left, right.
13:42 - Okay, so (clapping),
13:43 - you could imagine now what
you could do with this.
13:46 - For example, you could control
a game with your voice.
13:49 - And in fact, I'm gonna do that
13:51 - in one of my coding challenge videos.
13:52 - So take a look in this
video's description.
13:54 - I'm gonna do a coding
challenge where I program
13:56 - the Google Dinosaur game,
13:57 - and then I'm gonna add
this sound classifier
14:00 - to have the dinosaur jump
14:01 - except it won't be a dinosaur.
14:02 - It'll be a unicorn,
14:03 - to have the unicorn jump
when I say the keyword, up.
14:06 - All right, thanks for watching
14:07 - this additional ml5 tutorial video
14:09 - about sound classification in the browser.
14:12 - (bell dinging)
14:13 - (energetic dance music)
14:21 - (bell dinging)