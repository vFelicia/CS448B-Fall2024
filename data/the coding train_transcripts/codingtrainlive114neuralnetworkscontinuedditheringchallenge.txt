00:02 - good morning Internet
00:05 - it's his me digital Schiffman live here
00:10 - from New York City at New York
00:14 - University at Tisch School of the Arts I
00:17 - am here like I often am on Fridays to
00:20 - present to you it went so much better
00:27 - during rehearsal this morning
00:30 - there's no rehearsal to present to you
00:33 - the coding train a weekly thing on
00:37 - YouTube where I do some coding stuff and
00:41 - I have an announcement to make which is
00:44 - that I do intend to keep Friday most
00:47 - likely as the day of my live streams
00:49 - going forward I kind of operate on the
00:52 - academic calendar but it looks like
00:55 - Friday is gonna be a free day for me in
00:57 - this spring once NYU classes get going I
00:59 - will probably alternate sometimes do a
01:02 - morning to my time morning timeslot
01:04 - which is this time right now around
01:05 - 10:30 11:00 Eastern Time and I will
01:08 - sometimes do a later time slot around a
01:12 - 3 p.m. 4 p.m. 5 p.m. Eastern Time so
01:15 - that's that's the kind of that's the
01:21 - kind of Oh update that I have so what's
01:25 - happening today on today's episode of
01:28 - the coding train you may be wondering
01:30 - well I am yet again going to continue my
01:33 - series on building a neural network in
01:36 - JavaScript pointless or fruitless as
01:39 - that may be I'm going to continue it I
01:42 - might smile the way up to the end of the
01:47 - feed-forward algorithm meaning data into
01:51 - the neural network output out of the
01:53 - neural network the training and learning
01:55 - process is something I intend to
01:57 - approach next week
01:59 - I am also planning on doing a coding
02:01 - challenge I'm gonna try to do one coding
02:05 - challenge per week at least separate
02:07 - from the usual whatever sort of tutorial
02:10 - material that I'm doing
02:12 - and Floyd what is this called well let
02:15 - me just look up image stippling and I'll
02:18 - do a google image search for image
02:19 - stippling this is what I intend to do is
02:22 - work on an algorithm that can create an
02:26 - image from just a simple black and white
02:28 - dots and I'm going to use most likely
02:32 - the particular algorithm that's called
02:38 - Floyd
02:39 - it's a Floyd steinberg I want to say
02:42 - yeah
02:43 - Floyd Steinberg dithering and here's a
02:50 - nice Wikipedia page so I will attempt
02:52 - I will implement this particular
02:53 - algorithm in processing to get some sort
02:56 - of results like this ok so that's that's
02:59 - kind of what I have on the agenda for
03:01 - today any questions
03:03 - feeling kind of serious this morning I
03:05 - don't know why all right what else what
03:10 - else is happening oh I have something
03:14 - very exciting to announce a viewer act
03:24 - on github I believe the github name of
03:29 - this particular viewer is Neal's web
03:33 - Neal's web created an entire new website
03:39 - so this why am I not hold on bear with
03:44 - me for a moment
03:45 - let's make this happen let me login I'm
03:48 - gonna log into my github account so that
03:50 - I've logged in as I do this live stream
03:52 - today it's gonna take me a minute
03:54 - because I have very high security
03:55 - measures so the first thing that I need
03:57 - to do is type my username and then I
04:00 - need to go over here to my password
04:01 - manager boy oh oh yes live on the coding
04:05 - train me looking up my password and then
04:10 - I'm going to reveal it and it's like a
04:13 - really crazy weird string of characters
04:19 - and I'm gonna do this now I need to get
04:24 - my authentication code Oh bear with me
04:29 - everybody things I should have done
04:31 - before I started live-streaming titute
04:36 - Authenticator here waiting music I can't
04:47 - hear this four six one seven six eight
04:55 - verify ok back myself there alright so
05:14 - Neil's web created us go to this
05:17 - repository this is the repository that
05:19 - you might be familiar with because it's
05:22 - generally where I keep all the
05:23 - corresponding source code that goes with
05:26 - my videos so for example if I want to
05:28 - look for the source code for Langston's
05:30 - ant lankton zant which was a coding
05:34 - challenge I did last week I would go
05:35 - here under coding challenge and I would
05:37 - go all the way to the bottom and I would
05:39 - find first of all I be so thrilled to
05:41 - see that in addition to coding challenge
05:44 - 89 langston Langton Zant there is coding
05:48 - challenge 89 lenghtens and p5.js which
05:51 - even though I said in I would make a
05:54 - JavaScript version of the coding
05:56 - challenge I didn't but if I go to our
06:05 - the history here actually if I think if
06:08 - I just actually click on that we will
06:11 - see we can give a hearty thank you to
06:13 - Waratah me on github who I believe I
06:16 - mean more time he made the last commit
06:18 - to this directory and I'm pretty sure it
06:20 - was for Tommy who if I just go up here
06:23 - to sketch j s and go to history we can
06:27 - see yes this was for Tommy who
06:29 - contributed the JavaScript version so
06:31 - thank you
06:32 - we're Tommy alright so but I but I
06:36 - digress
06:39 - so this is what you might be familiar
06:41 - with this is where you can find the
06:43 - source code for all the coding challenge
06:44 - but uh oh but oh what is this new
06:48 - directory I see underscore coding
06:52 - challenges underscore coding challenges
06:54 - so you might have in the past
06:56 - I forgot this all right like oh I you
06:58 - want to submit your version of Langton
07:04 - Zant you might go here and see that
07:06 - there's a readme
07:07 - and then you could do a pull request of
07:10 - the dreamy and add your community
07:11 - variation this this is now the old
07:14 - system I'm gonna keep this old system
07:16 - going for a little bit longer because
07:19 - we're still working out the kinks of the
07:20 - new system created by Neil's web the new
07:23 - system is if I go back to this
07:27 - underscore coding challenges directory
07:30 - and I look for Langton ant right here
07:35 - this is now a markdown file this
07:38 - markdown file now it's a little weird to
07:40 - look at github is attempting to format
07:44 - this markdown file in a nice way for us
07:46 - to look at I'm just gonna click on raw
07:47 - so we can see it so this is a particular
07:51 - markdown that is oh and I believe
07:54 - actually oh no this is correct I believe
07:57 - this is a particularly the system called
08:01 - Jekyll and if there's time at the end of
08:04 - live streams I love this live stream I'm
08:06 - gonna run through a little tutorial
08:07 - about how to setup and run Jekyll on
08:09 - your computer Jekyll is a engine for
08:13 - templating and building websites with
08:15 - github pages well you don't have to use
08:17 - just github pages with it but you know
08:18 - pages works very well if it and so what
08:22 - you'll see here is now there is all this
08:24 - information right there is a
08:26 - contribution here that has a title and
08:28 - an author in a URL and another URL the
08:31 - URL of the author and a URL of this the
08:33 - source so maybe this is the URL of it
08:35 - running online the source and you can
08:37 - see there's some other stuff here what
08:39 - where what what is going on here this
08:41 - markdown file feeds a new website we
08:45 - coding train github.com Oh - code
08:51 - rainbow - code anyway so you can see
08:55 - here look at this this is now the new
08:58 - website that will compile and keep track
09:01 - of all of the challenges all the
09:03 - tutorials all of the streams and in fact
09:06 - someday there might be like a big
09:08 - blinking red light on air right now or
09:11 - something when I'm actually live that's
09:13 - a great idea and if I go back to coding
09:16 - challenges and I click here to the
09:19 - challenge you can see this is the page
09:21 - now for the coding challenge which has
09:23 - the video it has links to the example
09:26 - running in the browser I know my second
09:28 - laptop is visible I can get the code I
09:30 - can see the community contributions a
09:32 - link to how to add your own version and
09:34 - even all of the links that were
09:36 - referenced or discussed in the video so
09:39 - I'm tremendously excited about this
09:41 - because I've been wanting to put
09:43 - together something like this for quite a
09:44 - while but I could use your help so I
09:47 - would like this website to be a
09:48 - community project Neil's web sort of the
09:51 - de-facto manager now this website having
09:53 - volunteered to create the first version
09:55 - of it and so I'll come back to this I
09:59 - don't want to I could probably go on and
10:01 - on about this but I would encourage you
10:03 - to as you're floating around this
10:06 - rainbow code repository - number one
10:09 - check the wiki which has some pages that
10:12 - are like go through a little bit how
10:14 - this site works to check the github
10:17 - issues I could particularly use some
10:20 - input in the realm of interface graphic
10:24 - user experience design so if you are a
10:26 - designer and would like to contribute to
10:28 - this that would be wonderful and yes so
10:33 - hopefully there's some issues here that
10:36 - you can read through you can help with
10:38 - these and yeah so I'm looking I'm
10:46 - looking I'm looking
10:48 - see anyone so that's that's an important
10:51 - announcement other things to mention are
10:55 - if you want to join our slack
10:59 - you can support this channel through the
11:00 - patreon link and like and subscribe and
11:06 - share with your friends I'm supposed to
11:07 - say that stuff okay so now let's just
11:10 - get started but boom but bang the bun
11:12 - the bun bang boom okay here we go i what
11:15 - am i doing today I got to get myself set
11:17 - up here I was thinking about switching
11:22 - to using Visual Studio code but it's too
11:27 - used to Adam okay so I've got the neural
11:32 - network thing open let me run a server
11:50 - oh I already have this open does this
12:02 - [Music]
12:17 - are we doing how are we doing everybody
12:19 - all right ah ah okay I guess I need to
12:27 - put my little green paper on this laptop
12:30 - you can now see my very elaborate system
12:36 - let's switch briefly to the whiteboard
12:38 - camera which is all like no it's not
12:42 - this is my my cloaking device my
12:48 - live-streaming cloaking device piece of
12:50 - paper with a very well worn tape on the
12:52 - back which I will now bring over to here
12:55 - technology look at this cloaking device
12:57 - Oh cloaking device all right
13:01 - I am now going to put the cloaking
13:07 - device on the laptop to make the second
13:10 - laptop invisible so you can see the code
13:12 - more easily I'm going to slide this over
13:22 - [Music]
13:28 - girl networks live on the coding tree
13:45 - okay
13:46 - all right so let me what do I need here
13:53 - alright so the first thing that I think
13:54 - that I need to do is I want to make some
13:58 - improvements to the matrix library let
14:02 - me make a list of those things all right
14:08 - I want to I'm gonna make a mental list
14:11 - maybe I'll write down I want to use
14:17 - static methods that's item number one
14:23 - number two is I want to add a randomized
14:31 - function and number three is I want to
14:35 - add a map function so these are three
14:42 - things so I think the first thing I'm
14:43 - gonna do is just do some cleanup on the
14:46 - matrix library and actually before I
14:48 - start these are three things that were
14:50 - in my head let me see if anyone out
14:56 - there oh and you know what I want to add
15:00 - a I want to add a print method that's
15:06 - another one I don't want to die or a
15:08 - tape maybe like a print table I don't
15:10 - know what to call it but print method so
15:13 - let me see I now let this so in other
15:16 - words what I want to do first is make
15:17 - some improvements to this matrix library
15:20 - I want to change the name of this dot
15:22 - matrix to this data and I want to do do
15:27 - this list of things to it
15:29 - any other suggestions from the chat on
15:34 - on other things I should do to the
15:37 - matrix library before I start
15:42 - and by the way thank you
15:45 - everyone's compliments about my beard
15:47 - last week
15:48 - I think I was really planning to go get
15:51 - a haircut this week and do some grouping
15:54 - and I put it off it's like 60 degrees in
15:58 - New York today though Fahrenheit that is
16:00 - and I really feel like it's hot in here
16:03 - I should have gone to the barber shop
16:04 - yesterday but you were also kind to be
16:06 - on my mother if she's watching nicely
16:08 - now she's watching if she watches the
16:10 - first couple minutes I don't think she's
16:11 - made it this far and if she would tell
16:13 - me to get a haircut okay Tim says merge
16:18 - my pull request Tim are you the pull
16:21 - request that does the that is the like
16:25 - make the nice HTML table out of the out
16:29 - of the matrix I do want to merge I was
16:31 - just kind of holding off I don't know
16:32 - there's no reason for me to hold it hold
16:33 - off actually because eventually I'm
16:35 - gonna replace that library with the one
16:37 - that I'm building live it's mostly the
16:39 - same but yes
16:43 - okay Tim thank you for that I keep every
16:45 - time I look you add something and I'm
16:46 - very excited by it so apologies for not
16:48 - merging it there's actually no reason
16:49 - for me not to merge it let me merge it
16:51 - since Tim is watching live so you might
16:55 - be wondering you can look ahead there's
16:57 - a lot of mistakes there is actually this
17:02 - particular github repository shipment
17:04 - slash knurled - network - p5 this is
17:07 - essentially a finished version of what
17:09 - I'm building step by step in these
17:11 - videos and there's a wonderful pull
17:15 - request that I just learned is by viewer
17:18 - Tim and the live chat who created this
17:21 - amazing function that generates an HTML
17:25 - table and like displays it on the page
17:28 - so let's just go ahead and merge that in
17:31 - right now live on air nothing like
17:33 - merging a so let's see if I can make
17:38 - this
17:41 - confirm birds boo thank you Tim for the
17:49 - merge pull request we will speak I'll
17:53 - try this out later but there's there's a
17:56 - bunch of errors in the library and if I
17:58 - go under issues a lot of them were is it
18:05 - all in this thread there's a long
18:07 - discussion here about many things that
18:09 - are kind of problematic with the version
18:10 - of library that will hopefully be
18:11 - correcting as I go but I kind of been
18:13 - holding off because I will come back to
18:16 - this after I keep going okay
18:18 - so Tim created something I'm gonna I'm
18:22 - just gonna make a little print method
18:23 - that just does the console table for me
18:26 - but you can see Tim's method for putting
18:29 - the matrix on screen Oh another thing
18:30 - that I wanted to show is if you go to I
18:33 - can't remember who share this with me
18:34 - but if you go to I think matrix
18:37 - multiplication dot XYZ there's this
18:44 - wonderful little animation that
18:46 - demonstrates why should I go zoom out
18:48 - here that that demonstrates how the
18:54 - matrix multiplication works with the dot
18:56 - product and I actually just skipped to
18:59 - the end so I encourage you to check this
19:00 - out if you want some supplemental
19:01 - visuals for what I did last week okay
19:05 - I'm looking in so I'm getting some
19:13 - suggestions again replacing and names
19:16 - and adding more functions random I hope
19:18 - you people are just saying all right so
19:19 - um as I go if anybody else these other
19:22 - improvements of the matrix library that
19:24 - I can do please let me know again this
19:26 - is not meant to be a comprehensive
19:29 - linear algebra math library for
19:31 - JavaScript it's meant to be a little bit
19:33 - of like a little toy library right now
19:35 - to learn I have a randomize already
19:37 - I made a randomize already okay that's
19:40 - fine so I don't need to do that that's
19:43 - funny
19:45 - this is meant to be a little bit of a
19:46 - toy matrix library to sort of learn all
19:49 - the pieces
19:50 - kind of get our hands in there to
19:52 - manipulate how the matrix math works and
19:55 - learn a little bit about library
19:56 - development in JavaScript programming
19:58 - but ultimately I'm going once I get
20:00 - further along this will be replaced by a
20:02 - much more highly optimized and GPU what
20:07 - does I mean I'll talk about GPU
20:09 - compatible engine called deep learning
20:13 - is all right okay so let us begin all
20:36 - right welcome this is now part for our
20:40 - notes actually it's actually yet I would
20:42 - like part five I've no idea what the
20:43 - numbering is don't listen to me about
20:45 - the numbering just look for the number
20:46 - in the tail of the video but this is
20:47 - another video or this is the last video
20:49 - that I'm gonna do specifically about
20:52 - [Music]
20:54 - sorry I saw some stuff in the chat let's
20:57 - do it what are you coding okay sorry I
21:01 - got distracted all right what am i doing
21:03 - again all right here we go
21:12 - hello welcome to what I hope will be the
21:15 - last video about matrix stuff in this
21:20 - series building a neural network so what
21:22 - I want to do is in this video it's just
21:23 - a little bit of cleanup I'm working on
21:25 - this toy matrix library in JavaScript
21:28 - that does the math operations I'm going
21:31 - to need for the neural network that I'm
21:33 - gonna start building in the next video
21:35 - but before I get to that next video I
21:36 - want to clean up a few things and make
21:38 - the library a little better again my
21:41 - goal with this is really educational and
21:44 - for my own amusement in many ways I'm
21:47 - not trying to create this highly
21:49 - optimized or efficient comprehensive
21:52 - matrix math library just a little simple
21:54 - toy library with some key functions that
21:57 - I'm going to need and as we get later as
21:58 - I get further and further along I will
22:00 - eventually replace this with something
22:02 - else that someone
22:03 - has made which will be much better okay
22:05 - so what is it that I want to do I made a
22:07 - little list I'm checking it just once
22:12 - it's a list of my favorite things I'm
22:14 - just trying to reference random songs
22:15 - okay this is the list of things that I
22:18 - want to do and a lot of these came from
22:20 - comments from viewers so thank you for
22:21 - those one is I'm gonna add something
22:24 - called static methods to the library and
22:27 - why am I gonna do this well some of the
22:29 - functions that I have currently in the
22:31 - library affect the current matrix object
22:35 - that I call the function on matrix dot
22:37 - add some number to it that matrix itself
22:41 - changes when I add a number to all of
22:43 - the elements but some of the functions
22:45 - in the library actually create a new
22:47 - matrix object and return it don't affect
22:49 - the library the object that the
22:52 - functions being called on and this is
22:53 - where I want to introduce something
22:54 - called static methods which will make
22:56 - this more clear so I'll get into that as
22:58 - I introduce it I just notice there
23:00 - already is a randomized function in
23:01 - there but I might want to improve that a
23:04 - little bit although I'll come back to
23:05 - that later I want to add something
23:07 - called a math function and what I want
23:09 - to do with the map function is I want to
23:11 - be able to take any arbitrary function
23:13 - and apply it to every element in the
23:16 - matrix so if I could write a function
23:18 - that doubles the number and then I could
23:19 - just apply to the matrix to double every
23:21 - number in the matrix that's gonna be
23:23 - incredibly useful when I get to that
23:24 - neural network stage I have to say hey
23:26 - compute something like the sigmoid or
23:28 - the tan H the arktech there's gonna be
23:31 - all sorts of things that I want to do to
23:32 - every element of the matrix so a generic
23:34 - function called map like map this
23:36 - function to each number that's gonna be
23:39 - really useful and then I the other thing
23:41 - that I thought of his eyes just I do so
23:43 - much console table the values if I just
23:46 - make a print function that does that for
23:47 - me that'll make my life easier and nicer
23:49 - and programming anyways it should be
23:53 - more than it's more than this but
23:54 - sometimes it's just about making your
23:55 - life a little bit nicer on a daily basis
23:56 - okay so I'm gonna start working on these
23:59 - things there's a live chat going for the
24:01 - people who will happen to be watching
24:02 - this live and not the archived edited
24:04 - version so maybe I'll get some other
24:05 - suggestions along the way okay so let's
24:11 - first ah first thing that I want to do I
24:13 - got this comment several times
24:16 - from viewers this is a particularly I
24:19 - did a kind of poor job choosing the name
24:22 - for this variable so this variable right
24:24 - here this dot matrix which is part of a
24:27 - matrix the matrix class every matrix
24:29 - object has a matrix variable it's kind
24:31 - of problematic because it has the same
24:33 - name as the name of the class and this
24:34 - can really confuse people so one thing I
24:36 - want to do is just change this to data
24:38 - so a matrix object has a certain number
24:40 - of rows and columns in the
24:42 - two-dimensional array that stores all
24:44 - the values that's just the data of the
24:46 - matrix object so I want to change that
24:47 - and then there's some fancy those of you
24:50 - watching probably know there's a like a
24:52 - fancy way to do this an atom let's see
24:54 - how do I do it like I always do this by
24:57 - accident and I can't do it on purpose
24:58 - where you click on something and it like
25:00 - changes everywhere else it is sorry
25:12 - sorry I I'm just looking at the chat
25:16 - control D okay I'm breaking news from
25:22 - the chat I've told the control D no but
25:27 - let's try it it's a lot no it must be
25:31 - command deep for the yeah there we go oh
25:38 - oh this is crazy
25:39 - but I don't want it never mind scratch
25:44 - that I'm gonna do it the old-fashioned
25:45 - way and I'm going to do just a
25:47 - find/replace I'm going to say this dot
25:48 - matrix replace that with dis data
25:51 - because basically everywhere that I'm
25:53 - referring to it I'm referring to it by
25:55 - this time matrix so let's do that
25:56 - replace all feel confident about that
25:58 - and hopefully that's okay all right so
26:00 - I've done that now let's let's move on
26:05 - what is the thing that I want to do I
26:07 - want ock static methods okay so let's
26:11 - look through this function randomize
26:14 - affects the data of this object fine
26:18 - this function add adds a number or adds
26:22 - another matrix to this particular matrix
26:24 - changing the values done transpose
26:28 - I know well I have such clear vision of
26:38 - what to do good to see by the stupid
26:41 - camera that goes on so I'm a little
26:53 - confused what to do here with transpose
26:55 - I'm gonna leave that I'll come back to
26:57 - it this is the one oh boy this is the
27:02 - one where I really really need to fix up
27:06 - cuz this is such this is such a disaster
27:09 - the way that I've written this function
27:11 - if if I get a matrix I create a new
27:15 - object and return that object if I get a
27:18 - single number I just multiply every
27:21 - value to this objects matrix so this is
27:24 - this is kind of a disaster I've kind of
27:25 - done two different wildly different
27:27 - things in the same function so what I
27:29 - want to do is add a static function and
27:31 - a static function the difference between
27:34 - a shoot let me do a little erasing of
27:40 - the whiteboard for a second
27:49 - move on I guess you can come with me as
27:53 - I erase the whiteboard we can lose all
27:59 - this stuff Oh God this whiteboard needs
28:05 - to be re painted I think it's just got
28:08 - so much so many imperfections
28:23 - okay race more since I know I'm going to
28:30 - need you later anyway
28:50 - okay all right
28:59 - so what do I need exactly by a static
29:02 - method so methods are functions that are
29:06 - part of a class that are attached to
29:08 - objects made from that class from that
29:11 - template and so in the case here what
29:13 - I'm really doing is I'm saying things
29:14 - like let M be a new matrix and if I were
29:20 - to say something like m dot multiply
29:26 - multiply - this is not a static method
29:30 - in fact this is what I guess I think you
29:32 - would refer to as an instance method
29:34 - maybe it's a it's a regular function
29:36 - it's a function that's called on this
29:38 - particular instance of a matrix object
29:40 - but what if what I wanted to do is say
29:43 - what if I had two matrices M an N and I
29:47 - wanted to create our m1 and m2 and I
29:49 - wanted to say m3 is and currently the
29:54 - way the library works right now is I
29:56 - would say m1 dot multiply m2 this is
30:03 - currently the way the library works
30:05 - right now I don't won't be fine to keep
30:06 - it this way but I think what would work
30:09 - better is for me to actually say matrix
30:13 - dot multiply m1 m2 and this is a and
30:20 - you've seen this before right any time
30:22 - in jail JavaScript when you've done
30:24 - something like math dot random or math
30:29 - dot sign these functions are static
30:33 - functions part of the math object Oh God
30:39 - JavaScript it really tries to be crazy
30:41 - the naming of all these things but the
30:43 - point is it's a it's kind of a nice way
30:45 - of name spacing a collection of just
30:47 - utility functions and so there are some
30:49 - functions in the matrix library that
30:52 - operate on actual instances of matrix
30:54 - objects and other functions that are
30:56 - just
30:56 - kind of namespaced as part of the matrix
30:58 - world the matrix world and you could
31:01 - just put things in them and get things
31:03 - out of them and that's what I think I
31:05 - want to have this multiply function do
31:07 - so now I need to look up I actually have
31:08 - to look up that syntax for static
31:10 - methods in JavaScript because I don't
31:13 - know I've done this a bunch of times in
31:14 - the Java programming language but it's
31:16 - probably different in JavaScript so let
31:19 - me come back over here and let me look
31:21 - up static method JavaScript and I'm
31:26 - gonna type in Mozilla because I like to
31:28 - generally get the Mozilla Foundation
31:29 - documentation and here we go static
31:33 - keyword ah so it looks like all I need
31:35 - to do it's very very simple
31:36 - it's just put the keyword static in
31:39 - front of the function so if I now and
31:43 - the real question is can I have a stat
31:46 - the same the same function with two
31:48 - names in other words am I allowed to
31:50 - have I basically want to have a static
31:57 - version of multiply that receives two
32:00 - matrices m1 and m2 and a non-static
32:04 - version that receives a single number
32:06 - that is that that actually affects the
32:10 - that matrix itself so what I'm gonna do
32:13 - here is I'm gonna go and find I'm gonna
32:15 - go and take out this all of this math so
32:19 - this is all of the matrix product math
32:24 - that I worked out in the previous video
32:25 - and naming wise incidentally I called
32:29 - the two different matrix objects a and B
32:31 - so in a much simpler way I can just
32:35 - actually make the argument names a and B
32:37 - and remember I'm checking if as columns
32:39 - are not equal to B's rows okay so this
32:43 - should be fine I'm done
32:46 - and then here this multiply function is
32:49 - just the scalar product there we go
32:55 - I think I'm good right so here's the
32:57 - question can I make let's let's see if
33:02 - this works
33:06 - timeout what is going on the thermometer
33:10 - says 78 degrees in this room I came in
33:14 - this room this morning and it's at 76 I
33:15 - tried to open the window outside of the
33:17 - room get some fresh air in here it's so
33:23 - hot in this room I don't know why the
33:27 - climate control is not working you can't
33:29 - see what I'm doing but I'm trying to I'm
33:32 - trying to we'll some fresh your hair
33:37 - into this room all right hit that by
33:48 - accident
33:50 - what's having real temperature unit
33:52 - someone's gonna have to do the
33:53 - conversion for me okay all right let's
34:00 - see if this works but before I see if it
34:03 - works let me actually add this print
34:05 - function this is not going to be I'm
34:08 - gonna put do this on the bottom I'm
34:10 - gonna add a function called print and I
34:13 - couldn't just call it log maybe maybe a
34:15 - log is a better no no let's call it
34:16 - print and in the print function I'm
34:18 - going to say console dot table this dot
34:23 - data right so I want to just console
34:26 - table to this this dot data so let's
34:28 - take a look and let's let's let's try to
34:33 - manipulate some stuff in the console so
34:37 - let's say oops cannot read property
34:40 - matrix Jas line 45 of undefined so I
34:43 - messed something up so I had some other
34:52 - places in the code where I was not
34:54 - saying this dot data but some other
34:56 - matrix dot what neat now needs to be
34:59 - data so I can look for anywhere I'm
35:02 - actually saying dot matrix and you're
35:08 - some other places and some other places
35:15 - that's pretty good okay so I took care
35:18 - of that let's go
35:19 - back here all right oh I forgot that I
35:22 - had a sketch that's running that's
35:23 - trying some things out but let's let's
35:25 - just say m1 equals a new matrix that is
35:30 - 2x2 and I'm gonna say m1 randomized m1
35:40 - dot print okay that's the matrix m1 dot
35:44 - multiplied 2 so I want to double all
35:47 - those values and then m1 dot print again
35:52 - m1 dot print and we can see okay so
35:55 - that's working my old multiply function
35:57 - the multiply function that takes a
35:59 - single number and affects every element
36:02 - of that matrix is working now let's see
36:05 - if the static one works so now I'm going
36:08 - to say let em m to be a new matrix and
36:13 - I'm gonna say m2 dot randomized and m2
36:18 - dot print just to check it out whoops
36:22 - why did that not work
36:24 - oh I didn't give it so I probably so I
36:28 - forgot to give it a number of rows and
36:31 - columns I should probably add some error
36:33 - handling into the library instead it's
36:34 - like hey you forgot to give me the
36:35 - columns and rows or use a subtree fault
36:37 - value let's let's try that again m2
36:41 - equals a new matrix that's also 2x2
36:45 - let's print it out and let's randomize
36:49 - it and let's print it out and there we
36:52 - go
36:53 - okay so now let m3 equal matrix dot
36:58 - multiplied m1 m2 this is now how I want
37:04 - to get a new matrix which is the result
37:07 - of the dot product a matrix product
37:10 - between matrix 1 and matrix 2 and if I
37:14 - say m3 dot print okay so I can only
37:18 - assume that the math works correctly
37:21 - somebody watching this I should feel
37:23 - free don't pause and check that method
37:25 - write the comments if it's wrong but
37:27 - what I was really looking for I checked
37:29 - the math at the
37:30 - to work correctly last week so I'm
37:31 - really looking for now and I get am I
37:35 - allowed to have two methods one that's
37:37 - static with a given name and one that's
37:38 - not static with a given name and it
37:40 - looks like I am and it makes sense that
37:42 - it would be able to do that because
37:43 - they're called in completely different
37:44 - ways there's no reason why the computer
37:47 - should ever get confused between the
37:49 - instance the computer the JavaScript
37:51 - interpreter should ever get confusing
37:53 - the instance version and the static
37:55 - version all right everyone
37:58 - pause for a second all right let's see
38:06 - all right so what do I am I gonna do
38:09 - next here math oh I'm into the map
38:12 - function okay all right so as an
38:20 - exercise to the viewer what I would say
38:24 - for you if you're watching this and
38:25 - you're following along take this
38:27 - transpose function and make a static
38:29 - version that does what this is doing
38:32 - returns a new matrix object and and make
38:37 - and make and keep this version but make
38:40 - it actually internally change the matrix
38:44 - itself and transpose it so it's reusing
38:46 - the same variable
38:47 - I don't know if that's totally necessary
38:49 - but that may be well I'll do that on my
38:52 - own and adjust it I don't know it's the
38:55 - heat all right let me just for poor
39:01 - machi editing let me just say that again
39:04 - could you make a sign on the screen
39:06 - telling people what you are doing
39:08 - mmm that's an interesting idea the issue
39:11 - is I would want that to only go out on
39:13 - the live stream and not on the recorded
39:16 - versions and right now I don't have oh
39:18 - then I don't have it right now I don't
39:20 - have a way of separating that so but
39:22 - this is a good what I used to do is like
39:24 - just right in the video's description
39:25 - what I was doing that day and then I
39:27 - became lazy and stopped doing that but I
39:29 - will take that under consideration as an
39:37 - exercise to the viewer you know we can
39:39 - make some arguments whether what's
39:41 - what's the sort of optimal or most
39:42 - elegant way to write this live
39:45 - but for as an exercise for the viewer
39:46 - what you could do is take this transpose
39:48 - function and make a static version of it
39:51 - which is essentially this right we
39:53 - almost about to to it make a static
39:55 - version of this and perhaps leave an
39:57 - instance version which changes the
40:00 - internal data of the matrix object
40:01 - itself to be the transpose matrix but
40:03 - I'm going to leave that as an exercise
40:04 - for the viewer so the last thing that I
40:07 - need to do in this particular video is
40:09 - create a map function that's going to be
40:12 - a function that takes any generic
40:15 - function as an argument whoa
40:16 - so if you visit I think I think this
40:19 - might be the first time I've really done
40:21 - this in a video in JavaScript let's
40:25 - think about this for a second I can
40:26 - write a function let's let's think about
40:28 - how I'm gonna write this can I use this
40:31 - eraser it looks like I can all right so
40:33 - I am going to create a function that we
40:36 - got to think about this for a second so
40:40 - if I were writing a function and I did
40:42 - this already called something like add
40:44 - and it receives a number right this is a
40:49 - function I have in my matrix object
40:50 - world I'm gonna do is say plus equal you
40:54 - know I'm gonna have a whole bunch of for
40:55 - loops for every element of the matrix
40:58 - add this number to it that's what this
41:02 - function does what I'm saying I want to
41:05 - do is create a function called map and
41:09 - what that function does is it receives a
41:14 - variable maybe I'll call it FN right now
41:15 - for function maybe callback is it's not
41:18 - really a callback so it's not a callback
41:20 - FN for function and what it's going to
41:23 - do is it's going to execute this
41:25 - function somewhere in the code and this
41:29 - is something that you can't really
41:31 - easily do in a programming language like
41:34 - Java you can't make a function a
41:37 - variable that you pass around easily
41:40 - without some like real crazy gymnastics
41:43 - but this is very native to how
41:45 - JavaScript works arguments we think of M
41:48 - parameters to a function we think of
41:49 - Emma's Oh send these numbers in
41:51 - send these yeah send these other objects
41:54 - in but a function is the thing that I
41:56 - can send in just as well so let's look
41:58 - at how this would work in other words
42:00 - instead of adding a number to every
42:02 - element of the matrix I want to apply a
42:04 - function to every element of the matrix
42:06 - and let's look at how we do that okay so
42:13 - let's
42:14 - I think the ad function actually we can
42:16 - look here at this is kind of a useful I
42:19 - mean I'm gonna use this multiply
42:20 - function that instead of add it just
42:22 - multiplies this number to every elevator
42:24 - as a starting point so I'm gonna call
42:27 - this function map as a kind of generic
42:29 - term for mapping a function and what I'm
42:32 - going to do is I'm going to say so I
42:36 - want this same exact apply a function to
42:41 - every element of matrix and I'm gonna
42:46 - say right now I'm just going to create a
42:48 - variable let value equal this data
42:52 - IJ in other words I want to say I want
42:55 - to look through every element of the
42:56 - matrix and I want to take that value now
42:58 - this would be me just assigning that
43:02 - element of the matrix its value what's
43:04 - its value assignment this is this would
43:06 - not change the matrix at all but in two
43:08 - steps I pull the value out and I put it
43:10 - back in but if I have this function what
43:13 - why don't I just apply that function to
43:15 - the value and put it in so I can execute
43:19 - that function with the value and then
43:22 - assign the result of that function to
43:24 - the data makes sense
43:26 - so this'll might make even more sense
43:28 - once we see how it's used and you know
43:31 - this is only gonna work if I send in
43:33 - functions that are written in a certain
43:34 - way but we'll look at that and maybe
43:35 - somebody watching can let me know what's
43:37 - it is there a conventional name for what
43:40 - you might call a generic function that's
43:41 - being passed in you know I don't wanna I
43:44 - don't want to call it function because
43:45 - that's a reserved keyword I could call
43:47 - it func map its funk funk that's good I
43:52 - like that
43:53 - funk let's change it to funk okay all
43:57 - right so now let's let's in order to see
44:00 - if I've done this correctly let me go to
44:02 - sketch KS which is sort of just a
44:05 - bit of like code that I can use to sort
44:07 - of mess around and see if things are
44:08 - working uh lambda okay oh okay
44:14 - time out time out time out hugs I'm
44:16 - getting some good wouldn't data I'm
44:21 - sorry I'm reading some comments that I'm
44:22 - getting in the slack group here wouldn't
44:25 - data I J dot slice
44:27 - array.prototype.slice.call am de
44:36 - function need that I and J so yeah I
44:40 - know this is I see what you're saying
44:42 - here oh yeah you mean like this yes this
44:52 - is a good point okay
44:55 - I'm going to yeah I'll come back and
45:06 - mention that okay
45:08 - so cuz that that can be useful you're
45:10 - right but I'm not gonna worry about that
45:11 - right now okay okay so let me keep and
45:19 - in fact this will actually negate the
45:21 - need for certain functions like
45:23 - randomized potentially or ad because I
45:26 - could always create those functions and
45:27 - then call them through the map but let's
45:29 - look at this so let's say I have a
45:30 - matrix and I'm gonna say a dot
45:33 - randomized and then I'm going to say a
45:35 - dot print so what I want to see is just
45:38 - and I'll keep this just as a two by two
45:39 - matrix it's easier to see so I'm gonna
45:42 - refresh this page and we can say here's
45:43 - my random 2x2 matrix now what I want to
45:46 - do is I'm going to write a function
45:48 - called double it and it takes some
45:52 - arbitrary value let's call it X and it
45:55 - returns x times two so this is a
45:58 - function a generic function that
46:00 - receives a value and returns the doubled
46:02 - version of it so if I say an a dot map
46:07 - double it what this should do is it
46:11 - should send this function double it in
46:14 - through the function map it's going to
46:19 - it here run through and now it's saying
46:21 - this data i j equals double it now so
46:25 - it's calling the double it function for
46:27 - every value in the matrix so let's go
46:30 - back to here and if i say a dot print
46:34 - now and i run this we should see look at
46:38 - this here's a matrix with values two
46:40 - seven seven three or two seven actually
46:42 - whoa
46:43 - and now I have four fourteen fourteen
46:45 - six all the values are doubled great
46:49 - that works
46:51 - myself a little high I need a little
46:53 - reassurance no file in the form of bad
46:56 - lame sound effects okay hmm so ah so I'm
47:01 - being told in the chat that and this is
47:05 - by the way it's often referred to as a
47:06 - lambda function maybe but sometimes it
47:09 - is useful to actually use the index
47:14 - values so there are there are so there
47:19 - might be a reason why you want to
47:21 - optionally also pass in the IJ the row
47:24 - column location of of each when you're
47:30 - calling the function that's being passed
47:32 - in because you might want you might want
47:33 - to operate on it depending on where it
47:35 - is in the matrix maybe if it's an edge
47:37 - value you do something different so in
47:39 - that case this can be useful so this is
47:42 - a little bit of an additional advanced
47:44 - thing I'm gonna leave it out of my very
47:45 - simple library but I thought I should
47:47 - mention that and and it's up to you to
47:51 - then therefore so I could do something
47:54 - like as long as I is greater than zero
47:57 - return x times two otherwise return just
48:02 - X so I could start to do sort of
48:04 - advanced things like I don't want to
48:05 - just do this to everything but not to do
48:07 - something different to the first row
48:09 - kind of thing all right let me put this
48:14 - back all right
48:25 - all right today they do this the term
48:31 - for functions like map and functional
48:32 - programming is higher-order all right
48:34 - thank you for that Thank You Vlade for
48:37 - that all right so this brings me to the
48:48 - end of this particular video I am sure
48:52 - there are things I mean I'm really sure
48:54 - there are things this matrix class is
48:56 - missing that any comprehensive matrix
49:00 - class would need and I wouldn't be
49:02 - surprised even if we discover some
49:04 - things we have to change and alter to
49:05 - this as I start building the neural
49:07 - network but in the next video what I
49:09 - want to do is return back to this topic
49:12 - of a perceptron what happens when I have
49:14 - multiple perceptrons a multi-layered
49:17 - perceptron and why all of a sudden does
49:20 - this concept of a matrix become so
49:23 - meaningful and important for the math of
49:25 - feed-forward a neural network is
49:28 - actually often referred to as a
49:30 - universal function approximator
49:33 - right it receives input it generates an
49:35 - output why do I need matrices inside of
49:38 - that neural network to manipulate how I
49:40 - get the output from the input that's
49:42 - what I want to look at the next video
49:44 - the feed-forward algorithm okay thank
49:46 - you Oh Oh Alka is saying in transpose I
49:56 - have a nested loop but I only need one
50:02 - loop and a slice yes that is correct
50:05 - so the other thing I should mention is
50:07 - that right I could you slice here there
50:10 - are lots of array functions like reduce
50:15 - fill map and you know in fact there's
50:20 - already the so I'm just not using those
50:26 - right now I was I was thinking about
50:29 - revising this library to use all of
50:31 - those array functions but then someone
50:33 - told me they were like very slow so I'd
50:35 - only be using them just to like make
50:37 - things look sure
50:38 - in terms of the code but I'm gonna I'm
50:39 - gonna I'm gonna I'm gonna I'm gonna I'm
50:44 - gonna stick to what I have all right so
50:46 - I'm definitely going to need to and I'm
50:52 - gonna open this door
50:54 - so try to get some fresh air into this
50:56 - room while I erase the whiteboard
51:05 - danger here is that there's people
51:07 - walking by in the hallway I have no idea
51:09 - that I'm live-streaming
51:10 - want to say hello but I'm going to live
51:14 - dangerously in order to get some
51:17 - hopefully cooler fresh air into this
51:19 - room so I think probably what makes
51:27 - sense is for me to divide this
51:32 - particular feed-forward algorithm into
51:36 - two videos one where I just walked
51:40 - through and diagram how it's going to
51:44 - work and another I and J are bad
51:47 - variable names yes they are yeah I
51:53 - should be reading the chat diagram out
51:57 - the feed-forward algorithm and explain
51:59 - how it works and then implement the code
52:04 - for it and then I will do a third video
52:11 - where I add the bias well maybe I need
52:14 - to talk about the bias right from the
52:16 - start so I have to figure I'm not retire
52:18 - Lee sure how do we do this this is like
52:20 - hard let's see over here
52:34 - okay let's see if I can get a little
52:43 - more fresh air that's gonna have to do
52:56 - that's a race over here what's the time
52:58 - everybody 11:30 so I this has been about
53:05 - an hour I was hoping to be done at 1:00
53:07 - let's see how that goes okay
53:13 - just those of you with the slack channel
53:17 - who are discussing the optimal way to
53:18 - rewrite some of the matrix library
53:20 - functions just to be clear you're just
53:22 - discussing that amongst yourself right I
53:24 - haven't done anything horribly wrong I
53:25 - need like we need like a secret emoji
53:27 - that you can include maybe use a trained
53:30 - emoji if it's something really important
53:31 - for me or something like that all right
53:41 - no you have not missed the challenge
53:42 - okay oh right yes yes yes so many people
53:49 - okay so many good comments all right
54:02 - actually before I start this let me go
54:05 - back and what I want to see here is I
54:07 - want to look at me open the door a
54:09 - little bit longer while I'm doing what I
54:14 - want to do is look at my playlist and
54:27 - this one I think okay so really what I'm
54:35 - doing here is returning back to ten
54:38 - point five so I'm going to return to ten
54:44 - point five and talk about the structure
54:48 - and look at how suddenly the matrix math
54:51 - is relevant to that and on to es6 three
54:59 - four hundred one more five so that was
55:01 - part five
55:01 - yeah okay all right okay
55:22 - close this other stuff and okay all
55:39 - right okay all right I'm back to talk
55:47 - about neural networks again now two
55:49 - things I would mention I'll be right
55:51 - back I swear is again I need all the
55:54 - information that I'm showing you has
55:56 - come primarily from two sources
55:59 - I mean I've definitely looked at other
56:00 - sources as well this book make your own
56:03 - neural network by Tyreke rasheed I'll
56:05 - include a link to the coding training
56:06 - Amazon shop if you want to get your own
56:08 - copy of this as well as three blue one
56:12 - Browns videos so three blue one Brown
56:15 - has a series as a YouTube channel that
56:17 - has a series of videos about neural
56:19 - networks and deep learning and I would
56:21 - encourage you to watch those as well
56:24 - along along with mine so I'm going to
56:26 - try to explain the stuff as I go but
56:28 - really my focus is on implementing the
56:30 - stuff in JavaScript whereas if you watch
56:32 - the three blue one Brown videos those
56:34 - really aren't the focus is on how does
56:36 - it work in gaining an intuition for
56:38 - understanding the math behind how the
56:39 - neural network works and the animations
56:42 - diagrams will be much better than
56:43 - anything
56:44 - I totally illustrate for you on this
56:46 - whiteboard but I'm going to go ahead
56:48 - anyway thanks for being here with me but
56:50 - go and watch those videos okay so a
56:52 - neural network is something that you can
56:56 - think of as a function it receives
56:58 - inputs and generates outputs and if you
57:01 - remember we started this playlist I
57:05 - started with this idea of a simple
57:06 - perceptron being a single neuron and
57:09 - that neuron could receive two inputs we
57:14 - might have cuff call those X 0 X 1 and
57:18 - generate one output calling that why
57:23 - would I now and so we use this to like
57:28 - train this perceptron if X 0 and X 1 or
57:31 - is actually a point in a two dimensional
57:32 - plane we can train this
57:34 - trying to return a positive one if it's
57:37 - above a line or a negative one it's a
57:38 - below a line so these are really toy
57:46 - problems just to kind of get a sense to
57:48 - produce a very like obvious result to
57:50 - see that it works so let's go forward
57:52 - with another toy problem that I refer to
57:54 - in my perceptron video what if these
57:57 - were just inputs were binary boolean
58:02 - values so true or false in other words 1
58:05 - or 0 and this one I'm like hold on a sec
58:14 - turn this a little bit and this one is
58:20 - also true or false 1 or 0 this would
58:24 - work if I wanted this to up to solve for
58:28 - and basically I want the output to be a
58:31 - 1 or true only if both of the inputs are
58:34 - true and in all other cases it should be
58:37 - a false a 0 if I if I wanted this
58:39 - perceptron to solve or or I would get it
58:43 - by an output of 1 if just one of these
58:46 - were true and we could probably even
58:48 - like manually assign the weights to
58:50 - figure out to make this work but as I
58:53 - referenced in my previous video there is
58:55 - another kind of boolean operation called
58:57 - exclusive-or which means true and true
59:01 - gives me false true and false gives me
59:04 - true false and true gives me true and
59:07 - false and false gives me false so this
59:10 - is exclusive or meaning it's only true
59:13 - if one is true if they're both true it's
59:15 - false and this single perceptron can
59:18 - actually not solve this this is where we
59:21 - need a multi-layered perceptron meaning
59:24 - we need a second perceptron we need to
59:27 - send both inputs into that as well and
59:31 - and then both of these need to be sent
59:36 - into the output known as Y so I'm
59:41 - actually going to rewrite this I'm now
59:42 - going to draw this like this this is
59:44 - actually now a neural network because we
59:47 - can think of
59:48 - the inputs as nodes these two
59:51 - perceptrons that I've been served in
59:53 - here as nodes and this output is a node
59:55 - and this is I better write it down here
59:58 - a 3 layer Network input the input layer
60:05 - can you see that the this is referred to
60:08 - as the hidden layer I'll talk about that
60:10 - in a second the hidden layer and the
60:13 - output now I am missing from this
60:17 - diagram something called the bias I will
60:19 - return to the bias later we saw that we
60:22 - needed a bias to like sort of like move
60:24 - this line that we're trying to learn
60:25 - where it is up and down so I will come
60:27 - back to the bias we'll leave that out
60:28 - for right now so we have this input
60:30 - layer a hidden layer out but now why is
60:32 - this called the hidden layer well it's
60:35 - kind of like hidden because me the
60:37 - person who's operating this neural
60:39 - network I am in charge of the input I
60:41 - won't give it the input and now me the
60:43 - person in charge of 0 network will read
60:45 - the output and use it presumably in my
60:47 - program so the user of the network is is
60:50 - kind of sending in the input and
60:52 - examining the output so hidden is hidden
60:55 - because it's actually a part of the
60:56 - network that the user the end user
60:58 - doesn't manipulate at all and as you'll
61:01 - see later on there can be multiple
61:03 - hidden layers these kinds of
61:05 - architectures can get quite
61:07 - sophisticated and complex and I'm gonna
61:09 - go through hopefully eventually on all
61:10 - these videos many much more elaborate
61:13 - neural network architectures that
61:14 - involve you know inputs in different
61:17 - ways and multiple hidden layers and
61:18 - attach this other thing and bla bla bla
61:20 - bla bla outputs can come back and feed
61:21 - back into the input there's all sorts of
61:23 - crazy stuff you can do but this is a
61:24 - very basic basic starting point a 3
61:27 - layer Network one input layer one hidden
61:30 - layer one output layer
61:32 - ok I'm gonna pause for a second to see
61:35 - if there's any questions just gonna pop
61:45 - I'm just gonna turn this camera back on
61:52 - ok
61:54 - have a little drink of water here house
61:56 - how's this working out so far it I miss
61:58 - anything okay
62:17 - okay all right all right so a couple
62:24 - things one is you know the goal of this
62:28 - is use more interesting inputs and the
62:30 - hello world of machine learning that
62:32 - you'll see in every example in demo it's
62:34 - in the three blue one brown videos is a
62:38 - handwritten digit recognizer so in other
62:41 - words these inputs could be all the
62:43 - values of pixels of a given image and
62:45 - you could also be used you could use a
62:46 - system like this to try to predict the
62:48 - price of a house so the inputs could be
62:50 - the number of bedrooms or the zip code
62:54 - how do you know kind of getting
62:55 - convoluted here but so data how you
62:58 - collect and prepare and use data is a
63:01 - big important fundamental topic I can't
63:05 - really I'm not right now I'm using the
63:07 - simplest possible scenario where I don't
63:08 - have to worry about it my input data is
63:09 - just zeros ones true or false but don't
63:12 - don't miss out on this question thinking
63:14 - about how you collect data what could be
63:17 - missing from the data this is a really
63:19 - big important question I hope to come
63:20 - back to that in future videos as well
63:21 - okay but in this case I want to just use
63:24 - this very simple idea now how does the
63:28 - feed-forward algorithm work so let's say
63:32 - we receive an input and the input is 1
63:36 - and a 0 or 1 all right let me let me
63:44 - just start over my little train of
63:45 - thought there for a second
63:57 - let's take an input so let's have an
64:01 - input it again I'm gonna have to get to
64:02 - the bias later but let's say the inputs
64:04 - coming in the input coming in ah okay
64:06 - so first of all I know I know what I'm
64:08 - doing here I need to represent the input
64:11 - in some way so the typical way that we
64:13 - would represent the input is in vector
64:16 - notation looking like this X 0 X 1 so
64:21 - let's say I take an input of 1 & 1 what
64:25 - I need to do right remember these
64:27 - connections all have a weight this is if
64:31 - we think of this as hidden 0 this is
64:34 - this is X 0 X 1 maybe this is kind of
64:37 - like hidden 0 this is hitted 1 and again
64:40 - the hidden layer kind of a different
64:42 - number of neurons then the then the
64:44 - input layer but we're doing a simple
64:46 - scenario here this connection is a
64:49 - connection between 0 and 0 this
64:53 - connection is a connection between 0 & 1
64:56 - it's a weight between 0 & 1 this is a
64:59 - connection between weight 1 & 0 sorry
65:02 - and this is a connection between weight
65:05 - between this is a connection of weight
65:07 - between input 1 and hidden neuron 1 now
65:12 - look at all these now what happens in a
65:15 - perceptron in a single neuron a single
65:19 - neuron receives the weighted sum of all
65:22 - the inputs meaning this particular
65:25 - neuron should receive and I'll write it
65:27 - down here X 0 times weight 0 1 plus X 1
65:37 - X 1 times weight 1 0 whoa whoa whoa whoa
65:47 - whoa this is 0 0
65:50 - apologies my handwriting is very poor
65:52 - this is weight input 0 2 hidden 0 so
65:56 - this is weight 0 0 this makes sense
65:58 - right the weighted sum is both inputs
66:02 - multiplied by their weights of
66:04 - corresponding weights added together is
66:05 - this now I also have to
66:07 - something called the bias in here come
66:09 - back to that so this is what I get this
66:14 - one down here is X 1 times the weight of
66:20 - from 1 to 0 plus X 2 times the weight of
66:27 - 1 to 1 right 1 to 0 1 to 1 ah no no no
66:34 - no no I keep getting this stuff wrong 0
66:39 - 1 right this is the sum of X 0 X 0 I
66:45 - should have had more coffee this morning
66:46 - X 0 X 1 X is here Oh times wait
66:50 - 0 between 0 & 1 plus X 1 times the
66:54 - weight from 1 to 1 plus a bias will call
67:00 - this bias 0 and bias 1 I'll come again
67:06 - so they come back to the bias this now
67:09 - look at this interesting this is so
67:11 - fascinating doesn't this all look so I
67:14 - could scope I could I could stop here
67:16 - and say hey guess what I'm gonna do for
67:17 - now I'm gonna write the code because I
67:20 - can use for loops right if I have like
67:22 - these neuron objects and I keep an array
67:24 - of them then I have like the weights
67:25 - then I could do a for loop that could
67:27 - just sort of like loop through
67:29 - everything and sum everything up but but
67:31 - but but guess what it so happens that
67:34 - this exact operation resemble something
67:39 - that I've been spending 4 or 5 videos
67:41 - about this is actually the formula and
67:45 - let's take out the itthat's why wouldn't
67:47 - remove the bias a second we'll come back
67:48 - to it for what if I think about these
67:53 - weights as living in a matrix right
67:58 - there's actually the if I have the if I
68:02 - have an input layer and an hidden layer
68:05 - and I need a weight that connects every
68:07 - input neuron to every head and neuron
68:08 - then I have a matrix whose dimensions
68:12 - are pause
68:19 - let me think about this yeah I'm gonna
68:24 - get I'm gonna hi grace watching
68:28 - six-year-old grace hi grace I appreciate
68:30 - your watching I have a six-year-old as
68:33 - well she does not watch my youtube
68:36 - channel well she's in school right now
68:39 - okay I doubt I seriously doubt her first
68:42 - grade classroom is watching my youtube
68:43 - channel so I know this is going to be
68:47 - two by two I'm just thinking about this
68:49 - for a second so this is going to be zero
68:55 - zero one zero
68:56 - yeah zero zero one zero zero one one one
69:00 - okay but if this worked if if there were
69:04 - three here this would be 3 by 2 so it's
69:12 - the number of inputs by the number of
69:14 - hidden I think that's right is that
69:17 - right could someone confer fact check
69:18 - for this for me if there were 3 inputs
69:21 - here there were this would be 3 by 2
69:24 - right because you have 3 things all
69:27 - connected to there yeah okay
69:35 - assignment is telling me that I made a
69:37 - video explaining all this already it's
69:40 - quite possible doesn't hurt to do it
69:42 - more than once 2 by 3 yes yes yes 2 by 3
69:49 - I meant to 2 by 3 because 2 rows 3
69:53 - columns I just said 3 by 2 but I was
69:56 - like mentally I had it right
69:59 - 2 by 1 2 3 because I need to send 3
70:05 - across to do the dot product with these
70:09 - 2 I have to have the same number of rows
70:14 - no the same number of columns as I have
70:17 - Rose don't uptick no no no no no no 8
70:24 - this would be to buy this would be 3 by
70:27 - 2 1 2 3 by 2 1 1
70:30 - these will be the connections
70:32 - connections from here to there
70:33 - connection from here to there can okay
70:34 - okay I got it
70:35 - think about this a few times okay okay
70:40 - number of columns equals inputs number
70:43 - of rows is hidden news okay if you think
70:52 - about this this relates exactly to
70:55 - something I've been spending many many
70:56 - many videos about if I take these
71:01 - weights and put them into a matrix okay
71:03 - so the number of columns should equal
71:07 - how many input nodes I have right and
71:10 - then the number of rows would have to be
71:13 - the number of hidden right remember so
71:15 - look at this this would be I could store
71:17 - the weights between 0 and 0 the weights
71:20 - between 1 and 0 the weights between 1
71:24 - and 0 and the weights between 1 1 now I
71:27 - should have probably not used a square
71:29 - matrix let's making our life a lot
71:31 - easier but if I happen to have another
71:33 - hidden another input there were three
71:36 - inputs I would just have another row I
71:38 - would have weights from 2 to 0 weights
71:41 - from 2 to 1 but look at this if I take
71:45 - the dot product of this vector with this
71:48 - vector look what I get this X 0 times
71:55 - weight 0 0 plus X 1 times weight 1 0 if
71:59 - I take the dot product of this vector
72:01 - with this vector that is oh sorry this
72:08 - it's the matrix product the matrix
72:12 - product of the inputs with the matrix of
72:16 - weights gives you a resulting vector
72:22 - okay hold on and write this vector is
72:29 - going to be a 2 by 1 vector same as this
72:33 - and it's going to have this in the first
72:36 - spot and this in the second spot and
72:38 - that is those are the values that of the
72:41 - hidden
72:42 - now that's the value of the weighted sum
72:46 - of the hidden neurons if you remember
72:47 - from my perceptron video we have to pass
72:51 - those weighted sums through something
72:53 - called an activation function I will get
72:58 - to that in a second I'm trying to get to
73:02 - that site so what I want to do now is
73:08 - write this up a little bit better boy
73:11 - this is like hard okay let me erase this
73:20 - over here I need some more whiteboard
73:23 - space
73:44 - okay so let me rewrite this up here
73:48 - Philby wait hold on word I have a lot of
73:55 - space you have a lot of space this goes
73:58 - all the way to here I'm gonna rewrite
74:01 - this up here so I'm going to say the for
74:12 - any given let me think about this all
74:23 - right let me rewrite this up here so
74:29 - what I have just to review is I have a
74:32 - matrix of weights weight 0 0 weight 1 0
74:36 - weight 0 1 weight 1 1 then I have the
74:41 - inputs X 0 X 1 and this would be a good
74:47 - time to add in the bias so why do I need
74:51 - a bias so if you recall from the
74:55 - perceptron example we needed the bias as
74:58 - a way of saying like you know you have
75:00 - this problem if you're sending in zeros
75:02 - the weighted sum of a whole bunch of
75:04 - things multiplied by zeroes is always
75:06 - going to be 0 but sometimes that
75:08 - actually should be a neuron that act a
75:10 - neuron that activates and so the bias is
75:12 - something that can help move say it
75:15 - doesn't it's not as hard to act we could
75:17 - have a high bias meaning it's not so
75:19 - hard to activate this neuron we're gonna
75:21 - add some arbitrary number to it to like
75:23 - move the value up a little bit or we
75:25 - could have a negative bias saying like
75:26 - oh we've got to really make this hard so
75:28 - this is a way of giving us more ability
75:30 - to sort of like have the neurons behave
75:32 - accurately and the bias ease is actually
75:36 - just another number that we add to that
75:38 - weighted sum so we can actually look at
75:41 - it like this this is and so another way
75:50 - of looking at this is to say the hidden
75:53 - layer equals the weight
75:56 - matrix I'll say that's capital a the
75:59 - mate with the matrix products of the
76:01 - inputs plus the bias I'm like thinking
76:15 - about my notation very carefully here
76:17 - because I need to pass this through an
76:21 - activation function but the activation
76:23 - function happens for each one yeah so
76:28 - okay so the weights are kind of like
76:31 - weights between I and J the inputs are 4
76:35 - from 0 to J or J and the biases are like
76:39 - J we could think of it that way thinking
76:46 - about this where's the woo part of the
76:58 - white board oh yeah wo still the wrong
77:04 - order on the indices where do I have the
77:06 - wrong order on the indices right the W's
77:15 - before the x's and the indices are more
77:17 - logical okay hold on
77:30 - oh yeah crap it's just this one that's
77:37 - wrong right oh yeah oh my God look at
77:43 - this oh boy alright I guess what you've
77:50 - been watching this video for quite a
77:51 - while and I have a big big major mistake
77:53 - here and let me fix this I rewrote it up
77:58 - here so this by the way is 0 0 1 0 and
78:05 - this is 0 1 1 1 but don't I write the oh
78:12 - you hold on II I kind of supposed to do
78:18 - row column so this should be 0 0 0 1 Row
78:27 - 1 column 0 Row 1 column this is now
78:30 - right is this correct this would be the
78:33 - standard way of writing this is this
78:35 - correct
78:43 - yeah now I have to fix the diagram up
78:47 - here okay all right slightly awkward
78:53 - edit point there but I fortunately had a
78:55 - major mistake
78:57 - I appreciate that some of you were
78:58 - probably watching this video screaming
79:00 - at it then I have this mistake but so
79:04 - the convention for writing fertile I had
79:06 - the numbers wrong anyway but the
79:07 - convention for writing the the index
79:11 - values in a matrix is row column row
79:14 - column a three by two matrix is three
79:17 - rows two columns so these numbers here
79:20 - should be row column row column row 0
79:23 - column 0 rows 0 column 1 this is row 0
79:26 - this is column 0 this is column 1 now
79:28 - I'm in Row 1 column 0 Row 1 column 1 so
79:32 - that affects these right 0 0 times X 0 0
79:38 - 1 times X 0 then I have X 0 times 1 0
79:48 - and X 1 times 1 1 which is correct and I
79:54 - like to also refer to weight 0 0 as well
79:57 - so now let me correct this whoo still in
80:03 - Row 1 column 1 now I'm in Row 1 column 0
80:09 - and Row 1 column 1 and by the way I have
80:13 - seen right because gonna give it row row
80:15 - row row that makes sense because if
80:20 - these were if this is a matrix that just
80:22 - has one column this is really this but I
80:24 - don't I don't need those there I'm gonna
80:25 - sort of assume that you you will see a
80:30 - lot of people conventionally not
80:31 - starting with 0 but say X 1 X 2 but you
80:34 - know it's the same thing I guess I'm
80:36 - just I'm so used to counting from 0
80:37 - that's how I have to do it I'm gonna
80:39 - pause for a second to fact-check that
80:48 - all right now can somebody fact-check me
80:51 - that I now finally do have that correct
81:02 - yeah tend to sing the network gets two
81:05 - layers not three but I you know it's the
81:10 - input and outputs are you know this is
81:12 - the input is a special case the input
81:15 - layer is just sort of like the input
81:16 - numbers so you're right but I'm gonna
81:20 - consider it three layers it's sort of
81:22 - the convention that I kind of understand
81:26 - and the input layer does the activation
81:29 - stuff only happens with these two layers
81:31 - not the first layer okay this video is
81:37 - not editable you you should be surprised
81:39 - what Mattia can do all right
81:42 - diagrams still wrong but matrix seems
81:45 - fine
81:45 - what's wrong in the diagram oh the
81:48 - diagram is wrong all right so now that I
81:52 - bat went backwards and fixed up these
81:54 - now I'm realizing that these are of
81:56 - course wrong right because the
81:59 - connections no wait wait
82:01 - I think the diagram is correct because
82:09 - these are the connections for for the
82:12 - first row and these are the connections
82:14 - for the second row so what's wrong with
82:19 - the diagram
82:26 - me I am somebody tells me the diagram is
82:29 - wrong but I it looks right to me
82:49 - the row is the target perceptron wo I WI
82:55 - o or wrong way around how come I don't
82:59 - see this
83:10 - this is the this is the weight from zero
83:13 - oh oh you're saying yeah yeah yeah yep
83:23 - yes yes yes yes I see okay okay I get it
83:27 - I get it I see it now oh this could be a
83:29 - case you know sometimes much it does say
83:31 - to me just go back and do it again and I
83:33 - wouldn't be opposed to coming back like
83:36 - later today are like someday next week
83:39 - and just redoing this if this doesn't
83:40 - work cuz a lot of times I have to do
83:43 - these explanations to work them and
83:44 - understand them but I see the mistake
83:46 - now because these are the two things
83:52 - that are getting summed x one times zero
83:55 - one okay okay good thing I stopped to
83:59 - fact-check this thank you again to the
84:01 - live chat I have the diagram wrong now
84:03 - because this even though this looks kind
84:07 - of right wait between zero and zero wait
84:10 - between zero and one I have the order of
84:12 - these incorrect because this neuron is
84:18 - being multi this this this weighted sum
84:22 - is X 0 times 0 0 X 1 times 0 1 so I need
84:27 - to put the row first the column second
84:29 - so this is correct these were the only
84:31 - two ones that were incorrect so now the
84:32 - diagram is correct
84:35 - this is correct and this is correct row
84:40 - being I column being J so we're ready
84:47 - now once I've got this once I've got
84:50 - this so the hit end this is what I'm
84:55 - this is basically what I'm saying hidden
84:58 - for every hidden node the weighted sum
85:01 - is the weight the weight matrix product
85:07 - with the input plus all the biases but
85:11 - this is not enough I need to figure out
85:13 - what do I actually send out through out
85:17 - of the hidden layer so what is the
85:19 - output it we the the math is it needs to
85:23 - through some activation function so we
85:26 - saw in my previous perceptron example
85:29 - that I had a very simple activation
85:30 - function it was the sine function and
85:33 - the sine function says if it's greater
85:35 - than zero send out a 1 if it's less than
85:39 - zero send out a negative one and if it's
85:41 - zero I don't know send out a 100
85:42 - negative one pick one of those
85:43 - arbitrarily not random but be consistent
85:45 - so it turns out that what you pick as
85:50 - the activation function for a neural
85:52 - network can really affect how the neural
85:55 - network behaves and how well it solves a
85:58 - particular kind of problem and there
86:00 - this is a field of active research and
86:02 - there I'm gonna use the sort of function
86:05 - called the sigmoid function which I will
86:07 - represent with the Greek letter Sigma
86:10 - this is this is kind of the original
86:13 - activation function that was developed
86:15 - as neural network research began in the
86:17 - 80s 90s however we will see today that
86:21 - there are so sigmoid is one possible
86:24 - activation function tan H or inverse
86:27 - tangent oh and you can't see that sorry
86:41 - sigmoid is one possible activation
86:44 - function I know that's very high up
86:46 - terms white board tan H is another
86:49 - activation function and there is
86:51 - something also called relu or I have to
86:55 - say the French hey dude that's Tears for
86:58 - rectified linear unit the point of the
87:01 - activation function is what we want are
87:03 - values coming out of the neural network
87:05 - that have a very consistent range and we
87:08 - want to squish like then these weighted
87:10 - sums can be any value to be that could
87:12 - be like negative 5 bazillion right but I
87:15 - want to be able to take a function to
87:16 - kind of squash those values into some
87:19 - particular range tan H does a nice job
87:21 - of squashing the values between negative
87:23 - 1 and 1 sigmoid is a function that does
87:26 - a nice job of squashing the values
87:28 - between 0 and 1 and they do doesn't
87:33 - actually squash the values I'll come
87:35 - back to it later when we get to some
87:36 - other
87:36 - but it just takes away the negative
87:38 - value but this is act this is this is
87:39 - activation but I'm gonna use this
87:41 - sigmoid function I'm gonna use the
87:51 - sigmoid function let's take a look at
87:52 - the sigmoid function sigmoid function
88:05 - sigmoid function is a mathematical
88:08 - function having an s-shaped curve and it
88:10 - looks something like this and I guess
88:12 - that was wrong I don't know why I said
88:13 - between zero and one oh yeah yeah yeah
88:15 - it was between serán one so hold on or
88:19 - logistic function let me come back so
88:24 - let's take a look this is the Wikipedia
88:25 - page for the sigmoid function actually
88:27 - it's not the Wikipedia page that's the
88:28 - Wikipedia diagram but I can go to the
88:30 - Wikipedia page and we can see here this
88:33 - is also referred to as the logistic
88:35 - function it uses that magic number e and
88:38 - it takes whatever the value is and does
88:43 - this to it at all no matter what number
88:45 - you pass into the sigmoid function you
88:47 - get a number back between 0 & 1 high
88:50 - values approach closer to 1 negative
88:53 - values approach closer to 0 so we unlike
88:56 - a sort of biological neural network
88:58 - where a neuron is either going to choose
89:00 - to fire or not we're firing these
89:02 - continuous activation functions that
89:04 - fire somewhere in the range but the
89:06 - given number outputs somewhere in the
89:07 - range between 0 and 1 so again this
89:12 - sigmoid function is not used that much
89:14 - anymore in kind of current neural
89:16 - network research and as I get further
89:17 - we'll look at other activation functions
89:19 - as well but it's sort of the classic one
89:21 - is the one used both in they make your
89:23 - own neural network book and the three
89:24 - blue one Brown videos ok
89:38 - so I'm thinking what is he what is he is
89:43 - the why am i blanking on why am i
89:51 - blanking on the is e is e the natural
89:57 - number and is there a constant in
90:00 - JavaScript right you a lers number no
90:15 - Euler's constant it's pronounced Euler
90:19 - right and it is the natural I go through
90:24 - all over the natural math e okay thank
90:26 - you all right that's what I want to
90:29 - check okay it's the base for the natural
90:34 - logarithm right okay it's close enough
90:38 - to point seven ish all right okay all
90:49 - right if if you've made it to the end of
90:51 - this video you deserve a special prize I
90:53 - don't know what that prize should be no
90:56 - it's gonna like give you a pair for my
90:59 - beard that's that's a very weird and
91:01 - creepy prize let's edit that out except
91:04 - not now that I actually do edit these
91:06 - videos that will be edited out I don't
91:07 - so confusing here so somebody will also
91:10 - make this decision the neural network
91:11 - but if you finish for the end of this
91:13 - video and this has helped you please let
91:15 - me know in the comments I'm sure this
91:17 - will spark a lot of confusion and
91:18 - questions and I can always come back and
91:20 - revisit this topic but I think I'm now
91:22 - at the point where I can actually take
91:24 - this feed-forward
91:25 - algorithm and by the way the output and
91:28 - what I'm not mentioning here is this
91:30 - output layer is going to do the exact
91:32 - same thing it's going to take the
91:34 - weighted sum of all of its inputs pass
91:37 - it through the active plus a bias pass
91:40 - it through the activation function and
91:41 - and then give an output so you could
91:44 - quibble with this idea of being a 3
91:46 - layer Network 3 layer Network because
91:48 - this first layer the input layer is kind
91:50 - of a special
91:50 - case it doesn't do anything it's just
91:52 - the input numbers but I'm going to call
91:54 - it a lair the input layer the hidden
91:57 - layer performs this math and the output
92:00 - layer right we could write the same
92:01 - thing what's is Sigma of all of the
92:05 - weights between the hidden and the
92:09 - output times the hiddens output plus
92:14 - some biases so and again this this this
92:18 - weight matrix refers to these weights
92:21 - this weight matrix refers to these
92:24 - weights and same thing for the biases so
92:27 - this is the idea can I now take this
92:31 - mathematical formula that I somewhat
92:33 - understand and implement it in code and
92:36 - use the matrix library that I've
92:39 - developed this is what and then if so
92:43 - can we actually solve train a neural
92:46 - network we have to do the whole training
92:48 - thing to produce the correct output for
92:51 - the XOR problem again this is just to
92:55 - sort of learn and figure out the lingo
92:57 - the and how all this stuff works so that
93:00 - later when I go to use some higher-level
93:02 - libraries that do all this
93:03 - implementation for me because I really
93:05 - don't want to write all the code for
93:06 - this I mean I I sort of do but you don't
93:09 - maybe you do anyway
93:10 - I just want to get a sort of foundation
93:12 - of knowledge so now let's go to that
93:14 - next step come with me if you so choose
93:18 - creepy music underneath me down it up
93:26 - all right
93:27 - oops all right where am i timewise mm-hm
93:34 - and I thought it was gonna be like way
93:36 - ahead of schedule yes thank you for
93:41 - everybody donating through the super
93:43 - chat I really do appreciate that I
93:44 - actually don't have a good system to
93:46 - like see those so I'm sorry this thing
93:49 - oh go look at them afterwards but I
93:54 - would say if you do want to support the
93:55 - channel patreon is an excellent way to
93:57 - do so
93:59 - as well okay all right
94:04 - the heat is having an effect certainly
94:09 - is it still sort of still so 78 all
94:14 - right let me have a drink of water here
94:16 - and anyone have any Corrections they
94:20 - want to offer Oh math IXP X is e to the
94:32 - X thank you that's good to know but
94:33 - using that bees and H's indices must be
94:42 - I naught J let me take a look at that
94:48 - good thing I and J look kind of similar
94:52 - so oh there I yes you're right you know
95:04 - I just left but in the corrections
95:06 - department I really having trouble being
95:08 - consistent and doing a good job of my
95:10 - kind of row columns indices I kind of
95:13 - had to do this to like get used to it I
95:15 - probably just go back and redo this
95:16 - whole video but these should really be I
95:20 - good thing I and J look very similar you
95:23 - could almost say that I got it correct
95:26 - but it's because it's the there is only
95:30 - one column there are many rows so the
95:34 - column is assumed to be like zero but
95:38 - we're looking at all the different rows
95:40 - etc etc so it should be i0 that should
95:45 - be a nice thanks all right let's see if
95:49 - I let's see if there are any other
95:59 - Corrections all right
96:05 - H and O I believe - oh yeah
96:10 - I also missed over here I I thank you
96:19 - all right Matt yeah boy I'm so sorry so
96:35 - this we could also do amateur when you
96:37 - watch this later you know I'm not
96:41 - opposed to using some overlay in the
96:46 - video to like point out where the errors
96:48 - are during the video if they think
96:49 - that's going to be helpful okay okay we
96:52 - c'mon is leaving I love your index
96:57 - nagging index nagging it's like that
96:59 - should be the new song I'm nagging you
97:02 - about your indices get them right it's
97:05 - rows by columns not columns by rows but
97:07 - sometimes X by Y butts is the X is first
97:10 - or it's the I it's the J I don't know
97:12 - this is the number from my musical
97:15 - machine learning the musical it's the
97:17 - opening number indices indices
97:20 - everybody's got lots of indices nice all
97:24 - right
97:26 - yes yeah and so me so I do think also
97:30 - the convention is to start with one I
97:33 - just can't bring myself to do that
97:36 - because I'm so used to array indices as
97:38 - starting from zero but I do think you're
97:41 - right I think in a mathematical textbook
97:44 - you would see these as 1 1 1 2 2 1 2 2
97:52 - when I make this video over because my
97:55 - gia tells me it was a total disaster and
97:58 - I should not be releasing this content
97:59 - whatsoever under any circumstances I
98:02 - will maybe start from what okay alright
98:16 - so look how many people are watching I
98:21 - don't know how many people are watching
98:22 - I can't look I don't have an I do have a
98:25 - place I can look actually who 634 that
98:33 - is nuts okay okay hold on hold on hold
98:49 - on okay all right so I okay I want to be
98:55 - able to do this coding challenge and I
98:57 - want to try to get out here out of here
98:58 - like a little after one so let's do that
99:00 - and I also want to get some fresh air in
99:02 - this room let's try it just for a minute
99:05 - open this up so the next point that I
99:09 - need to do and is to write the code for
99:11 - this stuff so let's do that there could
99:20 - be more neurons than inputs right yes
99:22 - all right
99:26 - okay let's try to get some fresh air a
99:29 - little more fresh air for the coding
99:36 - challenge I might have to just wear my
99:38 - t-shirt yes Simon I did see your comment
99:44 - about activation functions thank you for
99:46 - that so I will as I get further will
99:49 - come back but I'm going to use sigmoid
99:50 - just for this demonstration
100:01 - hmm I realized what I need now but it's
100:07 - okay I'll add that into the matrix
100:09 - library during this video okay
100:12 - now we're at the moment I spent a lot of
100:16 - time looking like a crazy person trying
100:21 - to write out and understand the
100:24 - feed-forward algorithm for a neural
100:26 - network a simple three layer network
100:28 - with an input layer a hidden layer and
100:31 - an output layer I try to work out the
100:34 - math and understand it understand it as
100:37 - the output of the hidden layer is the
100:40 - matrix product of the inputs times the
100:43 - weights plus the biases pass through an
100:46 - activation function that is sent out
100:48 - into the output which does exactly the
100:51 - same thing with the output of the hidden
100:54 - times the weights plus the biases pass
100:57 - through an activation function and then
100:58 - we get our answer is it true or false
101:02 - now I'm gonna try to take this algorithm
101:04 - and apply it in code and luckily for me
101:07 - I have already written all the code to
101:10 - do the matrix product so I just need to
101:12 - create matrices that have weights I need
101:16 - to generate an input I need to run
101:18 - through this math and look at the output
101:20 - I'm not gonna get a useful output in
101:22 - this video I'm going to get essentially
101:24 - a random output I do need to do the next
101:27 - part which is not going to be easy which
101:29 - is training the neural network adjusting
101:31 - law and tuning all the weights and
101:33 - biases so that the neural network learns
101:35 - what are the optimal weights and biases
101:37 - to output the correct that well correct
101:41 - is kind of a weird word to use here but
101:42 - this or desired output based on some
101:45 - training data so gotta get into all that
101:46 - training stuff later but here I just
101:49 - want to get the feed-forward algorithm
101:51 - to work off I go alright so here's a
101:57 - find I'm realizing ie I need all these
102:01 - matrix videos to make my little matrix
102:03 - library and now I'm realizing there's
102:05 - something I really kind of needed which
102:08 - is that I mean I'm kind of gonna be ok
102:10 - with it here but I think it would be
102:12 - really convenient
102:13 - if my when I make a matrix if I could
102:20 - make one from an array and so what just
102:23 - really quickly I I don't like the
102:24 - positioning of this because basically
102:27 - what I want to do is this this input I
102:34 - want to be able to feed input into the
102:36 - neural network and I expect that the
102:39 - input is going to come in from a one
102:42 - dimensional array in JavaScript
102:43 - like I'm gonna generate an input because
102:44 - I pull it from a spreadsheet or
102:46 - something and but I need to quickly take
102:48 - that and make it into a matrix that has
102:52 - one row and one column in many rows so I
102:55 - need to add a function and I'm gonna
102:57 - make it a static function in this little
103:01 - matrix library I guess I'll put it red
103:03 - cut where I'm gonna call it static from
103:06 - array and I'm gonna assume that I
103:08 - receive an array so how do I do that
103:11 - what I need to do is I need to make a
103:14 - matrix that has the number of rows based
103:21 - on that array and one column I need to
103:27 - make a matrix object and then I can just
103:30 - say for let I equals zero eyes less in
103:35 - the arrays length I plus plus and I can
103:38 - say m dot data index I index 0 equals
103:44 - array index I now there's probably some
103:46 - and then I can return this matrix
103:48 - there's probably again I'm sure there's
103:50 - some way of using slice or something
103:53 - just to like sort of put the array in
103:54 - the right spot but this is my
103:56 - long-winded way to say okay if I have a
103:58 - 1 dimensional array let's make a matrix
104:01 - that has the number of rows for each
104:02 - spot in that array one column and then
104:04 - let's quickly put all the values from
104:06 - the array in all the spots in all those
104:07 - rows alright so we've got that so if we
104:12 - have that that assumes now I can call
104:15 - this feed-forward function input and
104:17 - what do I need to do well first of all
104:22 - this neural network needs a weight
104:24 - matrix I need a
104:26 - weight matrix i need a weight matrix
104:28 - step stores the number of weights all
104:33 - the weights that connect all the input
104:36 - neurons with the hidden neurons and so
104:38 - I'm gonna call that this dot input let's
104:46 - call this weights input hitted now a
104:51 - naming is not my strength so I might
104:54 - come back and refactor the name for this
104:55 - later I could look up what it's called
104:58 - in this book that I've been again using
105:00 - to like learn a lot of this stuff and
105:01 - I'll reference again three one three
105:03 - blue one Browns video if you haven't
105:05 - watched the one about what is a neural
105:07 - network you might want to watch that
105:08 - first I'm implementing the code a lot of
105:11 - it based on the explanation in that
105:13 - video but I need all the weights that
105:15 - connect the inputs to the hidden and
105:16 - this is a new matrix that has how many
105:22 - columns and how many rows how many rows
105:25 - and how many columns is the way I should
105:26 - ask it because Rose comes first
105:28 - well for every input I need a boy I
105:37 - don't like I melt it out of my brain
105:40 - this is gonna be a little edit point for
105:43 - you Mattie oh I make sure I get this
105:44 - right so we this is I is the row J is
105:52 - the column so I need the number of rows
106:00 - or the number of the number of rows are
106:11 - the number of inputs and the number of
106:13 - columns
106:21 - why have I have I lost my mind here
106:24 - oh no this is there's hidden this okay
106:25 - is it number the number of rows is the
106:28 - number of hit call of the number of
106:30 - inputs the number of columns is a number
106:32 - of hitting neurons right okay I need a
106:45 - weight matrix that has us number of rows
106:48 - based on how many inputs there are and a
106:51 - number of columns based on how many
106:53 - hidden neurons are on and I kind of did
106:55 - a poor example of making this two by two
106:57 - so I don't ever see the difference but
106:59 - again there were another input I would
107:01 - just have another row right here of this
107:03 - of this particular of this particular
107:07 - matrix okay so I need to say this dot
107:14 - input nodes comma this dot hidden knows
107:19 - that's telling me the dimensions of the
107:23 - matrix stop fact-check tonight I got the
107:26 - wrong way around in it and I didn't even
107:27 - come over to this screen I totally got
107:29 - it the wrong way around
107:29 - ah come back to that again they're so
107:45 - hard you know if I were just doing this
107:48 - by myself in my office without like all
107:50 - these people watching on the livestream
107:51 - totally not make any mistakes
107:55 - I have heard really alright so if if
108:00 - there were another one of these right
108:05 - zero zero there would be another column
108:10 - because let's think about if there's
108:13 - only two here no I'm getting confused
108:17 - this is the end this is the inputs ah
108:19 - this is not the there would be another
108:22 - there was another input there would be a
108:25 - third one here so I would need
108:35 - the number of columns here to match the
108:39 - number of rows here so I would need
108:41 - another column here I would need another
108:43 - column here let's try that one more time
108:48 - go to the barbershop yeah the weight of
108:51 - I think the weight of my hair and beard
108:53 - is like dragging my brain slowly out of
108:56 - them through my ears so I need to make
109:02 - that weight matrix have let's think
109:06 - about this if I added another so this is
109:09 - I kind of done a problem here a little
109:11 - bit of a flaw here because this is 2x2
109:13 - square it's so easy to like get mixed up
109:16 - but think about it if there were another
109:19 - input then I would need another column
109:23 - of weights so that the number of the
109:28 - number of rows matches the number of
109:30 - columns here so I should have the number
109:34 - of columns is the number of inputs I
109:38 - have and the number of rows is the
109:40 - number of hidden nodes so the weight
109:47 - matrix has the number of rows that is
109:50 - the hidden nodes and the number of
109:52 - columns that is the input nodes and it's
109:59 - so this is this is the number that when
110:02 - I create this neural network that's a 3
110:04 - layer Network I'm going to set up the
110:06 - total number of inputs a total number of
110:08 - hidden not hidden layers nodes in one
110:11 - hidden layer and then total number of
110:12 - output nodes we'll see why you might
110:14 - want to have just more than one output
110:15 - node okay so those are the weights now I
110:18 - might as well while I'm here create the
110:21 - weights for between hidden and output
110:24 - again I'm going to think about the
110:26 - naming for this stuff later but the
110:28 - weights between input and hidden now I
110:30 - want the weights between hidden and
110:31 - output and in this case the number of
110:35 - columns is determined by the hidden
110:38 - nodes and the number of rows is
110:40 - determined by the output nodes so I have
110:43 - these two different weight matrices
110:46 - pause some fact-check me I got that
110:49 - right right right I'm waiting for
111:01 - someone to tell me that I got it right I
111:04 - probably should just have a little more
111:06 - confidence in myself today I feel like I
111:10 - have very little confidence in myself
111:18 - okay seems correct is good enough for me
111:25 - okay now I should also add in the bias I
111:29 - think I don't know why but I'm sort of
111:31 - like a kind of I'm kind of like I get to
111:33 - come back to the bias we've added it at
111:35 - the end let's do the feed-forward
111:36 - algorithm for a second just without the
111:39 - bias and let's add that in in a little
111:41 - bit so a couple of things one is how do
111:44 - you cede the weights of a neuron when I
111:46 - create a neural network the whole point
111:48 - is I need to start with some set of
111:50 - weights and I'm gonna just start with
111:52 - random weights so I'm gonna say this dot
111:56 - weights ih dot randomized and this dot
112:03 - weights H o randomized so I would just
112:08 - put random weights in there again the
112:10 - whole point of doing this is to learn
112:11 - the optimal rates I'm gonna get to that
112:13 - in future videos but I need to start
112:16 - somewhere and by the way this is another
112:17 - area of active research like well how do
112:20 - you initialize are there good starting
112:22 - places like if you could start from a
112:23 - good place are you gonna have better
112:24 - results and random you know one thing
112:27 - one improvement that I've seen that I
112:28 - can make is I could just use like a
112:29 - Gaussian distribution of random numbers
112:31 - I should while I'm here though probably
112:33 - under randomize at least in that matrix
112:35 - function I think what I want is not I
112:39 - before I just made it random numbers
112:42 - between Oaks sorry I have the wrong
112:45 - thing clicked here random numbers
112:46 - between zero and ten but this needs to
112:49 - change I really want to have random
112:53 - numbers I'll say times two - let's just
112:59 - do
113:00 - actually say minus 0.5 so that'll give
113:02 - me starting weights between negative 5
113:04 - yeah whatever I'll do times 2 minus 1
113:07 - times 2 minus 1 so that should give me
113:10 - random numbers between negative 1 and 1
113:12 - so just to start with somewhere this is
113:15 - a good starting point for me having
113:17 - random weights whoops okay all right so
113:21 - now what can I do I can say let the let
113:27 - the input I'll call input layer equals
113:30 - matrix dot from array input so I first
113:35 - need to make the imp I need to make that
113:37 - a matrix and then I want to say let the
113:42 - hidden layer equal matrix multiply what
113:54 - do I want to do I want to multiply the
114:00 - weight matrix times the inputs this dot
114:07 - weights between the inputs and the
114:10 - hidden times the input layer now this
114:17 - I'm going to do this this formula right
114:19 - I kind of just did this part of the
114:22 - formula so I need to add the bias which
114:25 - I go back to and pass it through the
114:27 - activation function but I think I'm
114:29 - going to do this in multiple lines of
114:32 - code it's gonna make it a little bit
114:33 - simpler so I'm gonna put a comment here
114:37 - right now I'm gonna say hidden I'm gonna
114:40 - put a comment like add in the bias and
114:44 - then I'm gonna say hidden layer top map
114:49 - sigmoid right I need to take that hidden
114:53 - layer and apply the sigmoid function to
114:56 - it so I can use that map function that I
114:58 - wrote in the matrix library that takes a
115:00 - function and applies it to every single
115:02 - right this is doing the weighted sum
115:03 - this is creating the weighted sum and
115:06 - now I'm now I
115:07 - apply the sigmoid function now this is
115:09 - but if I run this code it's gonna say I
115:11 - don't know what the sigmoid function is
115:12 - so this is an area where what I need to
115:15 - do is write a sigmoid function and again
115:19 - as what I'm going to do probably later
115:21 - with this library is allow you to create
115:23 - a neural network that doesn't know that
115:26 - can operate with different activation
115:28 - functions there might actually be a
115:29 - variable inside the neural network
115:31 - that's just called the activation
115:32 - function but in this case I'm gonna
115:34 - write a function called sigmoid that
115:35 - takes X and then it returns what let's
115:39 - actually go and look at again I have it
115:41 - right here 1 divided by 1 plus e to the
115:45 - negative X so I believe in JavaScript I
115:50 - can say 1 plus wait wait who's that 1
115:53 - divided by 1 plus and then I can from
115:57 - the math library the math library
115:59 - actually has let's look at this
116:02 - JavaScript math exp this particular
116:07 - function returns e to the X so this is
116:12 - actually such a common function that
116:14 - it's they're built into the JavaScript
116:16 - library but it returns e to the X I need
116:19 - to make it negative
116:20 - so X opponent XP negative X so this
116:28 - should be the sigmoid function so now I
116:30 - have the sigmoid function I've mapped it
116:33 - and that's now the hidden layer now the
116:39 - outputs are equal to matrix what we're
116:43 - like almost done with this we didn't so
116:45 - much work up into this point multiply
116:49 - this dot weights between what the hidden
116:53 - and the output times what times what the
116:57 - outputs of the hidden layer which is
116:58 - right here this is what I've now done
117:02 - hidden layer and then I can just return
117:06 - outputs now realistically I might want
117:09 - to return the outputs back as an array
117:12 - so technically speaking right
117:15 - we know that the inputs is an array the
117:19 - weights are in a matrix the outputs of
117:21 - the hit
117:21 - lair in an array times oh and then these
117:24 - weights are in a matrix and then the
117:26 - output is array in this case it might
117:27 - just be one number so what I could do is
117:29 - I just want the first column so can I
117:34 - say outputs oh I'm gonna oh I'm gonna
117:40 - say to array hold on but I didn't come
117:45 - back to the right screen anyway and that
117:48 - went off hold on are we on time here
117:54 - 12:40 oh I forgot about it sigmoid oh
118:02 - and I forgot about sigmoid okay
118:15 - what did I do
118:17 - what's an easy way to turn the outputs
118:23 - because it's the second the column is
118:26 - the second dimension in the array how
118:29 - can I turn that I guess I could
118:31 - transpose that that seems crazy though I
118:35 - should guess I could write a two array
118:38 - function it's like almost eighty degrees
118:47 - can you see that I'm sweating it's like
118:48 - almost 80 increase in this room right
118:59 - once I've multiplied the weights times
119:02 - the output of a hidden layer I also need
119:06 - to apply sigmoid and now I could just
119:09 - say return output like this is it the
119:13 - input comes in turn it into a matrix
119:16 - create the hidden outputs by multiplying
119:19 - the weights map activating the sigmoid
119:21 - function then do the next weights to the
119:24 - output activating the sigmoid function
119:26 - return the output but I kind of want if
119:28 - I've said if I'm the end user of this
119:29 - library and I'm sending in an array for
119:31 - the inputs I kind of want to get an
119:33 - array back as the outputs so let's write
119:35 - a silly function to array which just
119:39 - takes any matrix and puts all the values
119:42 - into a one dimensional array and so I
119:48 - need to add that also to my matrix
119:50 - library which I can do to array and what
119:58 - I want to do there is let array equal an
120:02 - array and then if I just go through
120:04 - everything and say array dot push this
120:10 - date it now I guess I should be careful
120:13 - about how I'm doing this like I know how
120:18 - I want to do this if it's a matrix
120:19 - that's one-dimensional I just want to
120:22 - have these in an array this one first
120:24 - this one second but if it's like this I
120:26 - probably want to do
120:27 - column column then the next row column
120:30 - column which is exactly what I'm doing
120:34 - here the columns in the inner loop so
120:35 - that should be fine
120:36 - and then say return array so this now is
120:41 - a function again there's probably some
120:43 - magical fancy functional programming
120:46 - array functions I could do to make this
120:49 - more elegant but this should work all
120:52 - right
120:53 - so guess what we're done except I
120:56 - remember I said I have to add in the
120:57 - bias and where I have to have to add in
121:00 - the bias here add in the bias so this is
121:08 - this is compute output of hidden layer
121:17 - right here and this is compute output of
121:21 - output layer so the only thing I'm
121:25 - missing now is the bias what is the bias
121:27 - the bias is another matrix just like
121:33 - just like the input so for each layer
121:36 - it's just a one-dimensional array or you
121:39 - know one column one column multiple row
121:42 - matrix of random values and how I pick
121:45 - those random values and how we train the
121:47 - random values that's coming but let's
121:49 - come back to here and I can say I'm
121:52 - going to now also add this dot hidden
121:58 - bias equals a new matrix that has the
122:04 - number of hidden nodes for columns for
122:08 - rows and one column and this dot output
122:15 - bias is the number of output nodes and
122:20 - one column so why do we get we need a
122:23 - hidden bias an output bought bias why do
122:25 - we not need an input bias
122:27 - well again even though this is three
122:28 - layers this isn't really a layer in the
122:31 - in the truest sense it's just the actual
122:34 - raw input values as those go into the
122:37 - end layer all that math is applied with
122:39 - the bias as
122:40 - that output comes into the output layer
122:42 - all the math is applied with the bias
122:44 - for our final output okay so I should be
122:47 - able to also say now this dot hidden
122:52 - bias not randomized and again we might
122:55 - want to randomize the bias values in a
122:57 - completely different way but in this
122:59 - case I'm not gonna worry about that
123:01 - output bias dot and uh Mize and now
123:06 - where do we need to add that in I need
123:09 - to say hidden look we did them add in
123:17 - layer add hidden bias look at that
123:22 - oh so simple because we did all this
123:24 - work already right we have a matrix
123:26 - library that has an ad function that if
123:29 - you get an instance of another matrix
123:30 - you just add all the corresponding
123:32 - values if you get if you get an instance
123:42 - of a matrix you just add all the
123:44 - corresponding values so this is exactly
123:46 - what we want to do and whoops and it's
123:49 - done for us add the hidden bias and then
123:52 - here with the outputs at the output bias
123:58 - so I think strangely enough I'm done
124:01 - with this feed-forward algorithm by no
124:04 - means am i done whatsoever with this
124:07 - series about building a neural network
124:09 - somewhat from scratch in JavaScript but
124:12 - I think I am truly done in terms of this
124:16 - particular algorithm now here's the
124:18 - thing this video is gonna end the next
124:22 - video ostensibly I'm gonna start looking
124:25 - at how to do the training process and
124:27 - I'm gonna use the XOR problem as a very
124:29 - simple problem to start with I wouldn't
124:32 - be surprised if I've made some mistakes
124:34 - here or want to fix some things up here
124:36 - so if that occurs and I get some
124:38 - comments and some feedback I will
124:41 - actually have another video where I do
124:43 - some cleanup or some alterations here
124:45 - before I get to that so the next video
124:46 - is either going to be the learning part
124:47 - or or the or fixing up this part but
124:52 - let's actually at least make sure I not
124:53 - any syntax
124:54 - errors we can't know whether we've
124:55 - gotten a good result or not because we
124:57 - have no test data to work with we
124:59 - haven't done the training part yet but I
125:01 - can at least now if I run this live if I
125:06 - let's let's let's go to my sketch I
125:08 - could write some test code so I could
125:10 - say hi I want to have a neural network
125:12 - call it NN be a new neural network that
125:16 - has two inputs two hidden nodes and one
125:22 - output node right and then what I want
125:25 - to do is create some inputs like 0 comma
125:30 - 1 and then I want to get the output the
125:37 - outputs to be neural network
125:39 - feed-forward inputs right see this is
125:43 - the idea here we go machine learning or
125:46 - as I like to say trust Sheen kerning but
125:50 - anyways I make the neural network object
125:54 - my inputs are some data
125:55 - maybe that's eventually going to be some
125:57 - project and I'm really interested in
125:58 - that as exciting and interesting data
126:00 - that I've been thoughtful about and
126:01 - collected carefully and been transparent
126:03 - about how where that data comes from and
126:05 - how it works but all that aside here's
126:07 - my test data 0 comma 1 I want to feed
126:10 - that data into the neural network and I
126:11 - want to look at the output so now I'm
126:14 - gonna say console dot log outputs so
126:19 - here we go let's just see what happens
126:26 - ah input is not defined sketch line
126:33 - seven oh I need an S there that wasn't
126:36 - so bad
126:39 - alright I think this one is the sketch s
126:55 - line seven Oh input nodes what the no
126:59 - that's not a Vista I just assumed I have
127:01 - this dot somewhere never ever use this
127:05 - drum sound effect it only means I have
127:06 - terrible errors this might be this might
127:13 - be this time this is definitely a
127:17 - [Music]
127:19 - because I need to multiply in the hidden
127:24 - bias and the output buys the sigmoid is
127:26 - not this dot because I just defined it
127:28 - as a global function sigmoid just to be
127:30 - able to use it arbitrarily anywhere but
127:32 - so hopefully this is right now wait
127:36 - someone send me line 33 Oh out this
127:40 - should be outputs thank you save me from
127:41 - another error okay let's see here we go
127:49 - hey just watch to like very long
127:58 - convoluted poorly explained about neural
128:02 - networks and all you got was this number
128:05 - someone could make a t-shirt that says I
128:07 - just watched your YouTube video and all
128:08 - I got was zero point two one zero three
128:10 - seven nine three three seven one eight
128:11 - six eight eight zero three four so we
128:13 - don't really know if I did it correctly
128:14 - because I haven't like done anything to
128:17 - test this and so if I find out I made
128:20 - some mistakes I'm sure we'll hear about
128:21 - them in the comments but I will
128:22 - certainly correct them in the next video
128:24 - thank you for watching okay whew yeah
128:33 - somebody please make that I
128:35 - definitely want to make that t-shirt
128:38 - that I think would be a new patreon
128:40 - level yes a hard password Thank You
128:46 - Simon is telling me that I could
128:48 - transpose I could call the transpose
128:50 - function and then just take zero but
128:56 - okay
128:57 - hello Lebanon as amazing that someone
129:00 - from Lebanon is watching I love the
129:03 - International nasaw this audience okay
129:05 - so it is in some ways the end of today's
129:08 - livestream however it is not because
129:12 - even though it's like 79 degrees
129:16 - fahrenheit in this room apologies for
129:20 - being now the other thing is I'm gonna
129:24 - want to use the whiteboard for my coding
129:29 - challenge I don't think there's anything
129:36 - here that I really need to save so I'm
129:43 - gonna do the image stippling coding
129:45 - challenge before I leave I like to have
129:49 - four I like to make four like each live
129:52 - stream to make four standalone videos my
129:54 - goal is like live stream on Friday new
129:56 - video Monday Tuesday Wednesday Thursday
129:58 - that's kind of my goal for the channel
130:01 - so if you don't mind listening to my
130:06 - erase the whiteboard music
130:11 - totally should I kind of loved the idea
130:13 - of that t-shirt and I so I definitely
130:25 - feel like the video where I wrote the
130:27 - code is fine but it's possible I should
130:31 - redo the whole explanation video I will
130:37 - wait and see I've definitely made worse
130:45 - matches edited together videos with many
130:49 - more mistakes and confusion than that
130:52 - one
131:03 - I won't have time today to do a video
131:09 - about Jekyll and how to contribute to
131:11 - the new coding train website that I
131:13 - showed at the beginning of this
131:15 - livestream but there is a lot of
131:19 - information about it on the github
131:21 - repository and I would love
131:24 - contributions in the form of code or
131:27 - visual design or interaction design so
131:30 - please consider that is something if you
131:32 - have some time and are looking to learn
131:34 - about contributing to an open-source
131:35 - project I should get a fan I need to
131:54 - send an email and figure out because
131:56 - maybe the climate-control it's actually
131:58 - not on in this room all right
132:08 - [Music]
132:13 - thank you for that snapshot of the
132:17 - whiteboard Matthew Braun perfect
132:32 - [Music]
132:43 - [Music]
132:47 - all right ah question what's a good
132:50 - sample image for me to use for doing
132:57 - this Oh a cat I'll use one of my kittens
133:02 - it's not one of my kittens
133:04 - where are all those images sorry I'm
133:10 - Alka yes I am implementing a dither
133:13 - algorithm so this isn't really maybe an
133:15 - image stippling challenge I'm gonna do a
133:19 - dithering challenge and then I'll maybe
133:21 - I'll do a part two of a part two so this
133:34 - is Floyd Steinberg dithering that's what
133:37 - we're gonna do and I need to find the
133:40 - cats I had from where are those cat
133:44 - images they were in the like kittens
133:57 - so this is here we go
134:01 - oh these are like low resolution because
134:07 - I'm Chrome extensions here we go
134:10 - kittens all right we're gonna do a
134:15 - strawpoll I'm picking the kitten one
134:19 - he's actually numbered you know what I'm
134:24 - just gonna pick nonce that one two three
134:29 - four five no there's only four I mean
134:36 - what's gonna work best this one's
134:38 - probably gonna work best I think this
134:41 - one probably works best for the for
134:44 - doing a coding challenge alright so let
134:48 - me quickly make a I want it to be square
134:52 - well I want it give me a pixel values in
134:55 - both oh so annoying right I have this
135:00 - nice little Prius I just can't see it
135:04 - mm six six six three eight four no let
135:16 - me just get something relatively close
135:18 - to getting the cat this is too many
135:22 - pixels let me hold on we make a copy of
135:25 - this kitten tools
135:30 - image size just like it like a thousand
135:38 - or something it's 1,200 just so I have
135:41 - more to work with and zero and now
135:50 - this is like the worst way to make
135:52 - something as square if I just open up
135:54 - Photoshop I know how to do it six three
136:00 - four five six five three there we go
136:07 - drop close enough tools adjust size 512
136:14 - there we go there's my kitten oh I could
136:17 - do shift for square Oh
136:19 - I'm the worst oh it makes it a square
136:25 - Thank You Alka amazing let's go from
136:30 - here and do this it's so hot in here I
136:38 - cannot even express to you opening the
136:42 - door I'm just doing the setup here
136:45 - kitten I need to save save this now I
136:48 - each open up processing can't believe
136:50 - how much time who spent trying to sew is
137:01 - high contrasts yeah I think this will
137:04 - work
137:04 - contrast wise I think this will work so
137:12 - I'm gonna call this dithering dithering
137:15 - dithering dithering oh you know what I'm
137:20 - kind of wearing the perfect shirt for
137:21 - this and since it's so hot
137:31 - hopefully audio wise this won't affect
137:34 - things too much let's see here how's the
137:58 - sound the sound okay is the sound okay
138:03 - my wife gave me this shirt
138:05 - I need that kitten image where did I
138:16 - save that kitten image documents
138:20 - downloads desktop through the Ring a new
138:33 - folder
138:57 - all right ten challenges left before
139:00 - coding challenge number 100 oh boy
139:02 - alright so let me think about this what
139:04 - is let me understand what is the true
139:07 - definition of image stippling creation
139:11 - of a pattern simulating varying degrees
139:13 - of solidity or shading using small dots
139:16 - so this is that this particular
139:18 - dithering algorithm is a form of
139:20 - stippling but I think almost what's more
139:23 - interesting an example of image
139:25 - stippling is like look up at Robert
139:29 - Haugen why do I keep closing that door
139:31 - before I'm ready I'm closing that door
139:37 - before I'm ready it's like a reflex
139:39 - thing oh yeah here we go
139:44 - so these are some interesting examples
139:48 - which I will reference and then I will
139:53 - look at this algorithm and then yes okay
139:57 - okay
139:59 - and did the ring is what's the
140:02 - definition of dithering besides this be
140:10 - indecisive yeah add white noise huh did
140:14 - there yeah okay look at this the same
140:17 - thing one bit to the ring okay noise to
140:20 - random quantization here are blue
140:22 - alright well who cares about these
140:24 - definitions all right I'm ready for this
140:27 - challenge gradation using dots okay
140:30 - thank you all right is alka a real
140:39 - person all a good question okay let's
140:43 - get some air in here
141:02 - okay here we go
141:10 - hello welcome to today's coding
141:12 - challenge today's colleague challenge is
141:14 - to implement Floyd Steinberg dithering
141:17 - dithering dithering dithering and I'm
141:19 - going to do this dithering which is the
141:22 - thing that I actually spend most of my
141:23 - day just doing about I'm gonna do this
141:27 - dithering in processing which is a Java
141:30 - based programming environment works
141:31 - great for graphics I can load images
141:33 - that can replace souls which is what I
141:34 - need to do now the reason why I'm doing
141:36 - this is because I'm interested in this
141:38 - overall topic of image stippling which
141:40 - is a way of making an image basically
141:43 - out of dots and just as one reference
141:45 - for this I'm going to show you the work
141:46 - of Robert Haugen did some interesting
141:49 - attempts to do image stippling using
141:51 - like particle systems moving around and
141:53 - forces between the particles and I'd
141:55 - love to think about that as a follow-up
141:57 - but I'm going to in this video look at a
142:00 - particular dithering algorithm a way of
142:03 - quantizing an image and looking at
142:05 - errors and I'll kind of get to that as I
142:07 - go through it to get this type of look
142:11 - for an image but I do want to think of
142:13 - this for at least for you as a beginning
142:15 - step because if I could make this what
142:18 - if I consider all of these dots to be
142:20 - particles that can move and experience
142:22 - physics and go and find another place
142:25 - for another image there's a lot of
142:26 - possibilities there so let's get started
142:29 - so I have a blank processing sketch and
142:31 - in the folder I have a data folder and I
142:33 - have this image of a kitten which is 512
142:37 - by 512 pixels so the first thing that I
142:39 - want to do is I just want to write a
142:42 - simple sketch where I have ap image
142:48 - object called kitten and then I say
142:51 - kitten equals load image and the name of
142:54 - the file is kitten dot JPEG and I am
142:58 - going to make a window that is 1024 by
143:02 - 512 so that I can draw the image of the
143:08 - kitten at on the left-hand side and if I
143:12 - run this we should see oh boy
143:15 - that image is that kitten is not 512 by
143:18 - 512 I must have missed something up so
143:22 - let me go in here and say tools adjust
143:25 - size 512 there we go little smaller and
143:30 - there we go
143:32 - so what I want is I want to be able to
143:33 - look at see if you see the original
143:35 - kitten here and that I want to see the
143:38 - did kitten on the other side so how does
143:41 - this algorithm work well fortunately for
143:43 - me among this Wikipedia page which
143:45 - explains it and it's got it right here
143:48 - but before I get to that let's talk a
143:52 - little bit about some of the things that
143:54 - have to happen for example in the
143:56 - pseudocode there's like this lance's
143:58 - find closest palette color what's that
144:01 - mean and then there's like this quant
144:03 - error thing so let's discuss sort of on
144:09 - the whiteboard what some of these pieces
144:10 - are so if I have an image an image is
144:14 - just a grid of pixels any given pixel
144:19 - having a column row location and I might
144:25 - think of that as X comma Y right it's in
144:30 - the X column 0 1 2 it's in the Y row 0 1
144:34 - 2 reason I picked the pixel 2 comma 2 I
144:39 - mean it has a color typically that color
144:42 - is going to be an RGB color meaning it's
144:45 - going to have some red value some green
144:47 - value and some blue value the idea of
144:52 - quantizing and image so typically
144:55 - speaking if I'm using the full range of
144:57 - digital color I have a lot of
145:00 - possibilities I have 0 to 255 256
145:03 - possible Reds 256 possible greens and
145:06 - 256 possible blues but what if I wanted
145:09 - to reduce the number of possibilities
145:11 - what if there are only 4 Reds 0 1 2 & 3
145:15 - 4 greens I would have and I take an
145:19 - image of an original color take an image
145:23 - of that has the original full colors and
145:26 - I reduce it to a smaller
145:28 - instead of colors and this kind of
145:29 - process is applied to to make images
145:32 - most smaller file sizes and to do
145:35 - various kinds of effect so action let's
145:37 - just do that first and actually what I'm
145:38 - gonna do is quantize this image so that
145:41 - there's only two possible colors there's
145:44 - basically zero red or 255 red 0 green or
145:48 - 255 green and 0 blue or 255 blue so
145:52 - there's really 2 times 2 times 2
145:54 - possible colors 8 possible colors so
145:57 - instead of 256 to the third power colors
146:00 - I want to see what does this image look
146:01 - like with just 8 possible colors let's
146:04 - make that happen first and we'll see
146:06 - later why that's part of this algorithm
146:09 - so the way I'm going to do that is I'm
146:13 - gonna operate on the same kitten image
146:15 - so I'm gonna do is load the original
146:17 - image and display the original image and
146:18 - setup then operate in on it and display
146:21 - the new image and draw you know there
146:24 - could be some animation stuff that I do
146:26 - or a different order but I'm gonna do
146:27 - that as a simple thing for right now
146:28 - okay so let's the first thing that I
146:32 - need to do is I need to look at all of
146:34 - the pixels and here's the thing even
146:36 - though I could just the pixels are
146:38 - stored in a one dimensional array so but
146:41 - I do want to say look at all the X
146:43 - values kitten and and based on that
146:50 - images with sorry I lost my what I was
146:53 - doing here for a second so I need a
146:54 - nested loop to look at every pixel for
146:58 - every X and for every Y so this is going
147:02 - to allow me to look at a given pixel
147:06 - from the kitten the problem is the
147:14 - pixels are actually stored into one
147:16 - dimensional array and I need and I want
147:21 - to think about the pixels as their XY
147:23 - positions come to need that later
147:25 - luckily for us there's a very simple
147:27 - formula X plus y times kitten dot with
147:32 - and I go through this formula in another
147:34 - video that I will link to from this
147:35 - video's description if you want to
147:36 - understand why this formula takes an X Y
147:39 - position and gives me the one
147:40 - dimensional location in the
147:42 - so here's the thing in order to quantize
147:45 - this image this color this color
147:48 - variable is actually just a big integer
147:51 - and I need to pull out the red value of
147:56 - the pixel the green value of the pixel
147:59 - and processing has these nice helper
148:02 - functions that if I just pass an RGB
148:04 - color to the red function I get the red
148:06 - value and the blue value of the pixel
148:09 - now how can i quantize the image
148:12 - basically what I want to say it's kind
148:14 - of like a threshold effect I want to say
148:16 - if if the range of college between 0 and
148:19 - 255 if I'm above 127 just make it 255 if
148:24 - I'm below 127 just make it zero and so I
148:27 - could use it if statement for that but
148:28 - there's actually kind of a fancy way I
148:30 - could do this let's say I take so I'm
148:33 - gonna say the new R is going to be equal
148:37 - to let's say I take the current R value
148:41 - and divide it by 255 what does that give
148:43 - me that gives me a number between 0 & 1
148:46 - well what if I just round that number so
148:50 - if that number is 0.5 it'll be 1 if that
148:52 - number is 0 the low point 5 it'll be 0
148:56 - so I can actually use the round function
148:57 - and then I could just multiply it by 255
149:00 - so this is basically giving me two only
149:05 - two possible numbers no matter what R is
149:07 - I'm either gonna get 0 or 255 and I can
149:11 - do the same exact thing for G and the
149:15 - same exact thing for B and now what am I
149:22 - going to do I want to say kitten dot
149:25 - pixels index equals a new color with
149:28 - those values so this is me just saying
149:31 - pull the RGB values out quantize them to
149:35 - a smaller number of possibilities and
149:37 - make a new color and set it back to the
149:39 - pixels okay so and you know I've
149:44 - forgotten something kind of important
149:45 - which is that when I operate on an image
149:47 - in processing before app and operate on
149:49 - the pixels I should say kitten dot
149:51 - update pixels and then when I'm done I
149:53 - should say kitten dot
149:55 - no no no update pixels is when I'm done
149:57 - and before I operate on the pixels I
149:59 - need to say load pixels like lo pics are
150:02 - the same like hey hey you I want to work
150:03 - on the pixels right now update pixels
150:05 - saying hey I'm done but I'm done I'm
150:12 - done
150:12 - so now image kitten 512 comma 0 so I
150:20 - should see the quantized version on the
150:22 - right and the original image on the left
150:24 - let's see this looks pretty good right
150:28 - this kind of makes sense like if I only
150:29 - have a certain number of possibilities
150:31 - this is what I'm left left over with we
150:34 - can you know let's well interestingly
150:36 - enough what happens if I already right
150:39 - before I do any of this make it
150:42 - grayscale so I can just quickly filter
150:45 - that image and make a grayscale with the
150:47 - filter function and processing we can
150:49 - run it again so we can see you can see
150:51 - how this is working now there's only two
150:53 - possibilities
150:53 - it's either white or black and so this
150:55 - is identical to a threshold in effect
150:57 - and here well whoops let's let's leave
151:02 - that in but comment it out because we'll
151:05 - come back to it later now interestingly
151:08 - enough while we're here what if I want
151:12 - to have more possibilities oops since I
151:17 - had a little pause there let me see okay
151:27 - I mean just checking the chat to see if
151:28 - I've made something weird okay what if I
151:33 - want to have more possibilities what if
151:36 - I actually want to have instead of just
151:38 - two possibilities for each color four
151:41 - possibilities for each color well I can
151:44 - do something similar for example what I
151:48 - could do I think this would work right
151:50 - what if I multiply this by 4 and then
151:58 - divide it by this and divide 255 by 4
152:02 - let me just put this in here usually
152:04 - don't like to do this and then let me
152:05 - explain it
152:08 - think about this what I'm going to get
152:11 - now is if I'm multiplying this number
152:15 - that's a floating-point number between
152:16 - zero and one by four and I round it I'm
152:19 - gonna get a zero one two or three right
152:22 - those are the only things I'm gonna get
152:23 - zero one two or three now I want to
152:27 - scale that up to basically like what are
152:31 - four possible colors in between zero and
152:34 - 255 so this also and this I was going to
152:37 - say I have to put floor here but luckily
152:39 - I don't and this is gonna really trip us
152:41 - up later if I'm not careful
152:44 - RGB our floats so this is going to turn
152:46 - into a float round will turn the result
152:49 - back into an integer zero one two or
152:50 - three can I get for ya I could get for
152:56 - this is actually giving me five
152:58 - possibilities right because if R is zero
153:01 - I'm gonna get zero so this is actually
153:04 - interesting enough giving me five
153:05 - possibilities 0 1 2 3 4 4 5
153:07 - possibilities just like multiplying it
153:10 - by one gave me two possibilities zero or
153:12 - one so I'd actually have five
153:14 - possibilities here and then I want to
153:15 - scale that up this though this is an
153:17 - integer this is an integer it's going to
153:19 - give me an integer back so and what I
153:22 - might like to do here just to be like
153:25 - protect myself here a little bit is I
153:27 - might want to say nu nu R and I want
153:30 - these to be sure that these are integers
153:31 - because I'm quantizing to just a few
153:34 - sets of possibilities so this is indeed
153:42 - this is indeed an integer and so now so
153:48 - now I'm going to say a new r nu g newby
153:54 - newby so let's now run this and see what
153:58 - happens there you can see how now I have
154:03 - more color possibilities but still I've
154:05 - reduced the image to a smaller number of
154:07 - colors five five this was actually 5
154:11 - times 5 times 5 so 125 possible colors I
154:16 - think not very many colors ok I should
154:19 - probably make this a variable
154:22 - call it like factor and I'll set that
154:26 - equal to four and put this in here and
154:30 - the same number goes here if I've done
154:33 - this correctly and if we run this it
154:36 - looks like this if I set this to be one
154:39 - it looks like this so this makes her if
154:42 - I want to do something interactive here
154:43 - I wouldn't want to operate on the
154:45 - original kitten image I want to have a
154:46 - separate image so I could like change it
154:48 - on the fly but I don't need that for
154:49 - right now okay so we've done the
154:51 - quantizing part now the other thing that
154:54 - I need to understand is the quantization
154:57 - error is that the right way to say that
154:59 - I'm not entirely sure but what I mean by
155:02 - that is let's say the actual let's look
155:04 - at a particular example the actual color
155:06 - is 255 comma 100 comma 10 look this is
155:13 - an actual color or if I were to reduce
155:15 - this color with one with two levels with
155:19 - a factor of one two levels I would get
155:22 - to 5500 right because this would round
155:28 - up to 255 this would round down to zero
155:31 - and this would round down to zero so the
155:34 - error is the difference between these
155:37 - two this minus this is 0 this minus this
155:40 - is 100 and this minus this is 10 that's
155:44 - the actual error let's let's let's do
155:46 - this a little bit differently let's make
155:48 - this 150 just so we can see so this
155:50 - would then be 10 but this would round up
155:53 - to 255 so the error would be negative
155:58 - 105 right so you can see this is
156:01 - calculating the error the reason why I
156:04 - need to work with this error and let's
156:07 - put this into the code real quick so I'm
156:10 - gonna say here err err err r equals nu r
156:19 - minus nu r error g equals g minus nu g
156:26 - and error B equals B minus newby newby
156:31 - okay
156:33 - so so the reason why I need this error
156:37 - is let's go back to that Wikipedia page
156:40 - basically this algorithm achieves
156:42 - dithering tillering using error
156:45 - diffusion meaning it pushes the residual
156:48 - quantization error of a pixel on to its
156:50 - neighboring pixels to be dealt with
156:52 - later so in other words it just keeps ah
156:55 - it's so different put it on it's kind of
156:57 - it keeps pushing the colors further or
156:59 - further apart away from each other kind
157:01 - of based on the error and so the pixel
157:04 - indicated with a star indicates the
157:06 - pixel currently being scanned and this
157:08 - is the amount of error it passes to its
157:10 - neighbors so in this case oh and what's
157:13 - kind of the order of matters here for
157:15 - each y from top to bottom for each X
157:18 - because I'm actually pushing the error
157:21 - based on pixels to the right bit so
157:25 - actually so the pixels that I'm using
157:29 - are pixels this is a pixel to the right
157:31 - this is a pixel to the left and down
157:32 - this is a pixel down it's kind of like
157:34 - the pixels on the bottom right of the
157:36 - image so let's check this out for each y
157:39 - from top to bottom no I have X so I need
157:44 - to do Y first that's going to make a
157:46 - difference so right now let's just make
157:49 - sure this still works it still works
157:51 - but now I need to start doing this error
157:54 - thing so for each so now this is what
157:57 - I've done already
157:58 - right I've gotten the the quantized
158:02 - pixel like I've done this part I
158:04 - calculated the error so all I need to do
158:05 - is start like funneling the error off
158:09 - okay so how do I do that let's actually
158:12 - grab this and put it in our code right
158:21 - here and let's comment it out comment
158:29 - that out and let's put this up here
158:31 - right
158:32 - this in fact is that whole first part of
158:35 - the algorithm this whole first part of
158:37 - the algorithm matches exactly with these
158:41 - three lines of code right here right I
158:44 - look at the old pixel I get
158:46 - new pixel and I said it and then I find
158:48 - the error so I've done that already now
158:50 - I just need to do this part so doing
158:53 - this part is hmm so first I need to say
158:56 - okay so here's the thing let's you know
158:59 - how I have this formula here X plus y
159:01 - times kitten dot with let's make that
159:03 - actually a function I'm going to call
159:05 - the function index and it just takes an
159:07 - int an integer
159:08 - Oh exit oh I should probably take a
159:10 - whiff too but I'm gonna be sort of silly
159:13 - about it and it's just gonna return
159:17 - because I'm gonna need to do this a lot
159:19 - X plus y times kitten dot with y oh and
159:22 - it's not a void function it returns an
159:24 - integer and then I'm gonna do this index
159:29 - X comma Y so what I'm going to do is
159:31 - like whenever I have an X Y I could just
159:33 - quickly get the index and return it and
159:36 - that will be the correct index into the
159:37 - pixels array I could have made this two
159:39 - lines of code but I think I think we can
159:41 - follow us I could follow this hopefully
159:42 - you can follow this because the reason
159:45 - why I just did that is because I need to
159:48 - say kitten dot pixels index X plus one
159:53 - comma Y right I am rewriting exactly
159:57 - this pseudocode right here
159:58 - and why okay so that's so I need to do
160:02 - this two x plus one comma Y I need to do
160:08 - this 2 X minus 1 y plus 1 I need to do
160:15 - this 2 X y plus 1 and I'm sure I could
160:21 - do this in some kind of like loop or
160:23 - something but I'm just going to write it
160:24 - all out right now just to know that it
160:26 - works X plus 1 y plus 1 so and like if I
160:30 - just for a second put zeros here just so
160:32 - I don't have any syntax errors and put a
160:36 - semicolon here semicolon let's just see
160:40 - okay great so this is what I'm doing and
160:42 - I'm gonna be kind of anal retentive
160:44 - about this and add some extra spaces
160:47 - just so it all lines up so I know I need
160:50 - to operate on these four pixels that's
160:53 - what it's telling you down here and what
160:55 - do I need to do this is the important
160:57 - part
160:58 - so this gets the the if you can see that
161:02 - these are all getting parts of the error
161:04 - seven plus three plus five plus six is
161:06 - 16 so this is getting like you know
161:09 - almost half the error like forty high
161:11 - 40s percent this is getting like a
161:13 - little bit of around a little less than
161:14 - a quarter of the error so okay so this
161:16 - amount is important so for each one of
161:18 - these I actually need to make a new
161:20 - color so I need to say so I need to
161:28 - mmm-hmm I'm gonna have to do this with
161:31 - our I have to for everything have to do
161:33 - this with an RG and a B so when this is
161:37 - gonna be index XY here as well so now
161:41 - what I need to do is let's think about
161:43 - this I want to get a another new are I
161:51 - already used the variable name new are
161:53 - and I already called it an integer so uh
161:56 - let's okay let's say I'm gonna okay
162:01 - let's I'm gonna have a color called see
162:02 - I have an idea now I'm gonna rename just
162:06 - so I have different variable names I'd
162:07 - make this old are old G old be sorry for
162:12 - all this variable naming up old are old
162:18 - G bold be and then old old old okay so
162:29 - the reason why I'm doing that is because
162:31 - this is what I want to do I'm gonna
162:35 - actually say int index equals this index
162:39 - then color is kitten dot pixels that
162:43 - index now kitten dot pixels that index
162:49 - should now equal R equals red see red of
162:55 - see G equals green of see B equals blue
162:59 - of see it should equal and that our
163:03 - shouldn't how equal that I got it I got
163:05 - it I got it
163:06 - r should equal itself
163:11 - plus error r times 7/16 times 7/16 do
163:23 - you see why this is so what I need to do
163:25 - is I need to do this for our G and B G B
163:29 - G B G B so what I have a mistake here
163:34 - but for each one of these and now I set
163:40 - it back to the color I'm passing the
163:41 - error so let me look at that color get
163:43 - its RGB past part of the error onto it
163:47 - and then set that new color okay so the
163:51 - thing that's wrong with this is 7
163:53 - divided by 16 is what I said it was like
163:55 - almost 50% almost point 5 but it's
163:58 - actually zero because both of those are
164:00 - integers so I need to be very careful
164:02 - and say 16.0 I need to at least
164:05 - explicitly in job in JavaScript I
164:07 - wouldn't have this problem
164:07 - and you'd explicitly say point zero
164:09 - because I want that to be a float so now
164:11 - I just need to do this with every single
164:14 - one of these so I need to do this over
164:17 - and over again four times and each time
164:23 - I do it I'm going to just use the same
164:25 - variable names but not read eclair them
164:28 - I don't love this style but it will do
164:31 - so now I need the next one is X minus 1
164:35 - y plus 1 and the next amount of error I
164:41 - want is 3 comma 16 3/16 so 3 so I'm
164:45 - going to do that then I'm going to do
164:49 - what the next one is X y plus 1 X Y plus
164:54 - 1 and the amount of error is 5/16 X Y
164:58 - plus 1 is 5/16 then the last one and I
165:07 - can get rid of all my notes here from
165:09 - the pseudocode the last one is X plus 1
165:13 - Y plus 1 X plus 1 Y plus 1 and 1/16 of
165:19 - the error okay
165:23 - I think yes yes I could make a function
165:26 - there's so many ways this could be
165:28 - cleaned up and I appreciate people who
165:30 - are watching this live different you
165:31 - good feedback I believe that is an
165:33 - exercise to the viewer I just want to
165:34 - run this right now okay so I have an
165:37 - array and there's Oh what is my problem
165:41 - here look at this I am looking at every
165:44 - single pixel but for every single pixel
165:48 - I'm dealing with neighboring pixels
165:50 - right I'm dealing with things like for
165:53 - this pixels XY I'm dealing with X plus
165:56 - one comma Y which is this pixel it's one
165:59 - over an X the same Y but when I do this
166:03 - there's no pixel over here so I need to
166:06 - deal with the edges and I could be
166:08 - thoughtful about this but just to get
166:10 - this algorithm to work let's look at
166:12 - these I am going to the right and the
166:15 - left but only to the right of Y only
166:18 - down Y wise wise so I can start X 1 if I
166:24 - start at 1 I'll always have a neighbor
166:25 - to the left and I can go all the way up
166:27 - to minus 1 meaning I always have a
166:29 - neighbor to the right and for y I can I
166:32 - can start at 0 because I never look up
166:34 - and I can go over this way so now let's
166:38 - run this and I really should I've
166:41 - learned my lesson so many times to not
166:43 - use this drum roll effect so I usually
166:45 - have some mistakes but but try it right
166:47 - now hey I think I kind of got this yeah
166:55 - look it's sort of hard to see cuz
166:58 - there's so much crazy detail in the
167:00 - outside here but I think did I leave the
167:03 - factor as one I think this is right you
167:05 - don't ease your way for me I'll see if
167:06 - this is right is let's filter out let's
167:09 - juice the grayscale one and look at it
167:11 - yeah it's definitely right so you can
167:14 - see this image is now kind of dithered
167:16 - so to speak you could ask the question
167:19 - using a lot of white dots is it a lot of
167:22 - white dots on a black background or is
167:24 - it a lot of black dots on a white
167:25 - background the truth of the matter it's
167:27 - neither
167:28 - all I've done is set every pixel to
167:29 - either white or black but I've made it
167:32 - have this appearance of being like a lot
167:33 - of dots next to
167:34 - each other and so the question here is
167:36 - how might I take this like if I used a
167:38 - lower resolution image drew the dots as
167:41 - ellipses and then blew it up what kind
167:44 - of other visual effects could I get here
167:46 - and we could see this we could kind of
167:47 - look you know if I change the factor to
167:50 - four and ran it again you know you can
167:53 - see that it's the same sort of thing is
167:55 - happening it's dithered but I'm only now
167:58 - I have I think five different gray
168:00 - possibilities it's a white white dark
168:04 - dark gray light gray medium gray no dark
168:09 - bright great gray I don't know what you
168:11 - get the idea so I think a bit rate is an
168:15 - issue here so ah I think I've done
168:17 - something which is that this video is
168:21 - not gonna work very well on YouTube
168:23 - because you're not really gonna be able
168:25 - to see this detail but I think if i zoom
168:27 - in right I think you can you can kind of
168:30 - see hopefully you can see see this
168:32 - detail all right so what could you do
168:34 - with this I'm gonna stop here I'm gonna
168:36 - go back to taking out gray let's look at
168:40 - it with so you can see here this is
168:43 - dithered but now many different RGB
168:46 - colors maybe 125 RGB possibilities I
168:49 - think so what could you do with this I
168:53 - and I'm gonna actually leave the code
168:56 - with one and leave the grayscale in what
169:00 - I think could be interesting is number
169:01 - one work with a much lower resolution
169:03 - image but display it at much higher
169:05 - resolution and maybe draw ellipses or
169:08 - particles as each one of these dots
169:10 - whether it's a black dot or a white dot
169:12 - draw it with some texture some image or
169:14 - something what if those dots somehow are
169:16 - the seeds of a particle system and then
169:19 - I think eventually I want to come back
169:20 - maybe I'll do a follow-up challenge like
169:22 - that but those would be some exercises
169:23 - that I would try you can read over a
169:25 - link also to Robert Hodgins description
169:27 - of this stippling you can see the
169:30 - particle checks the pixel array to see
169:32 - what shade of grey needs to represent if
169:34 - it shows blackness it grows smaller and
169:36 - it's magnetic charge diminishes it fits
169:38 - white it grows larger as does its charge
169:40 - so this is kind of like a force directed
169:43 - self-organizing physics based system
169:44 - that creates these white and black dots
169:47 - to represent
169:48 - an image you know and certainly a I will
169:50 - try I will create or somebody will pull
169:52 - requests for me a JavaScript version of
169:54 - this so that you can also see a version
169:55 - that runs in the browser
169:56 - although the pixel array works
169:58 - differently in html5 canvas and in p5.js
170:02 - and pixel operations in the browser
170:04 - unless you're using shaders or some kind
170:06 - of advanced technique often are very
170:07 - very slow
170:08 - whereas in Java and processing that can
170:10 - operate quite quickly all right thank
170:14 - you everybody
170:15 - oh if I stop moving I'm told it's just
170:17 - to show this to you I'm told that if i
170:19 - zoom into it and then I stop moving it
170:25 - will resolve this will be the thumbnail
170:29 - let's get that mouse out of there this
170:32 - will be my thumbnail okay thanks for
170:37 - watching alright everybody I am finished
170:50 - today I'm finished today it is 1:30 I do
170:55 - have to go I haven't had lunch this has
170:59 - been a close to three hour livestream
171:01 - which is usually about as much as I can
171:04 - handle I will read some random numbers I
171:11 - don't know I I had the system where I
171:13 - was like going through it and
171:15 - remembering where I left off I don't
171:18 - know what to do about this random
171:19 - numbers that maybe I need new shtick
171:20 - [Music]
171:23 - I think that cat image should be for
171:31 - behind me here I'll stand right here in
171:39 - the white area fifty nine thousand five
171:42 - hundred and thirty six for those of you
171:44 - who are in Europe actually Europe it's
171:46 - only like dinnertime
171:48 - those are you are somewhere in the world
171:49 - right now where it's bedtime lie down
171:51 - relax I will read to you from the coding
171:54 - train bedtime story a million random
171:56 - digits with a hundred thousand normal
171:58 - deviance twelve thousand five hundred
172:01 - twenty sixty three thousand two hundred
172:03 - and forty sixty nine thousand and maybe
172:06 - eight eleven thousand eight hundred and
172:08 - thirty six sixty three thousand nine
172:11 - hundred and thirty five forty eight
172:13 - thousand nine hundred and fifty eight
172:15 - ninety three thousand and forty six
172:18 - thirteen thousand five hundred
172:20 - eighty-two that was it
172:23 - alright okay I will take questions from
172:32 - the chat for about two two-and-a-half
172:36 - minutes so you can post your questions
172:38 - in the chat now I will remind you pause
172:43 - let me just remind you I don't have time
172:46 - to go through this in more detail the
172:49 - coding train github is rainbow code this
172:55 - is the new system for compiling every
173:00 - single video a page for each video links
173:03 - relevant to the video contributions from
173:05 - the community the live stream schedule
173:08 - everything now should be is on this new
173:11 - website which is a github repository
173:13 - rainbow code and if you would like to
173:17 - contribute to it please take a look at
173:23 - these github issues I should probably
173:24 - label the ones that are website related
173:26 - as a post that are just like code
173:28 - example related so that's something I
173:30 - should
173:31 - and you can also look on the wiki that
173:34 - has a guide for contributing and I would
173:37 - love as much help as possible with this
173:39 - thank you again to Niels web on github
173:43 - who just had the idea to make this
173:45 - website and it's amazing and in
173:47 - particular some visual design or
173:49 - interface design user experience design
173:51 - that I think would be helpful okay so in
173:54 - if you want to support the channel you
173:55 - could go to patreon.com/scishow to you
173:58 - will get an invitation to the slack
174:03 - group where I have my first question
174:05 - what was your first computer good
174:07 - question my first computer was an Apple
174:10 - 2 plus I believe I was in second grade
174:13 - at the time I grew up in Baltimore
174:15 - Maryland and I was an Apple 2 plus and I
174:19 - learned a little bit about programming
174:20 - and basic on it when I had it I was not
174:22 - I never really did programming more than
174:25 - just a tiny bit until I was a 28 or 29
174:29 - but in second grade and did a little bit
174:31 - and in seventh grade and in a little bit
174:32 - and I did basic in second grade yeah
174:39 - Thank You Gil for your donation I
174:45 - appreciate it
174:46 - did you take computer science in school
174:49 - it did not I have never taken a computer
174:51 - science course I did take I did go to
174:53 - the graduate program here at ITP where I
174:56 - took introduction to computational media
174:58 - which is a learner program computer
175:00 - science like course through the lens of
175:01 - media and art will go to the barber shop
175:06 - what is your morning beard vet Eugene I
175:08 - take a shower wash that's it do I play
175:11 - any video games yes
175:14 - I recently I have two children they are
175:16 - six years old and nine years old
175:18 - we recently over the holidays acquired a
175:21 - Nintendo switch so far I have played a
175:23 - lot of just dance snipper Clippers and
175:26 - Mario Kart and today arrived in the mail
175:30 - Mario what's it Super Mario Odyssey so
175:33 - I've not played it yet but I will play
175:34 - it this weekend he's toying around
175:36 - especially for my all my just dance
175:39 - moves I've all I've learned some new
175:40 - dance moves I have to switch to a
175:42 - different song
175:44 - what's a good one this is work this is a
175:46 - new dance move either from jest against
175:54 - some other ones too like all right
175:58 - whatever I can't do it but I've been
176:00 - playing I love Just Dance it's the thing
176:01 - about the game that's ridiculous though
176:03 - is unlike like a Kinect tracking game
176:05 - which is doing some full skeleton it's
176:07 - just kind of I assume using the
176:08 - accelerometer and the control that you
176:09 - hold your hand so you can almost do
176:11 - nothing you just move this one hand
176:12 - around and get just as good vice Kors if
176:14 - you did all the dance routines move all
176:16 - right that was about a minute in I think
176:20 - so I don't know if people would watch a
176:22 - gaming channel for me and maybe with my
176:24 - kids involved I'll consider it
176:29 - [Music]
176:33 - what does your school think of these
176:35 - videos and patreon any legal issues
176:37 - others should know if they want to do
176:39 - something similar I'm definitely
176:42 - operating in somewhat of a gray area
176:44 - here I advance I do have the benefit of
176:48 - being at an art school so I think it's
176:50 - different and so I consider this work
176:55 - that I do you know that there's other
176:57 - departments here Tisch like the dance
176:59 - department and the film school and the
177:01 - TEL and a lot of people who teach make
177:02 - movies or make TV shows and I make
177:06 - youtube videos and I've been ITP so I
177:08 - consider this an extension of my
177:10 - research and professional practice it's
177:12 - complicated and confusing and there's
177:14 - certain things that I probably am and
177:16 - I'm not allowed to do I don't really
177:17 - know what those things are I'm just
177:18 - experimenting and learning and would
177:20 - somebody tell three I can or cannot I
177:22 - will rethink what I'm doing but yeah I
177:24 - do have a lot of support from my
177:25 - department ITP in learning about this
177:27 - and making videos and I really am also
177:30 - you know I'm trying to use this channel
177:33 - as a place for other people to feature
177:35 - other people and tutorials and that type
177:37 - of thing as well are your children
177:42 - interested in programming I guess I
177:44 - should just let this play while I'm
177:45 - talking
177:45 - I pause for every question never be done
177:47 - yes they're there they're interested you
177:51 - know I think most of my time as a parent
177:54 - has been trying to keep them away from
177:55 - technology at least as they were very
177:58 - very little I think that there's hard to
177:59 - replace the wonders of the physical
178:01 - world and the companionship of other
178:04 - human beings but you know now that
178:06 - they're you know getting a little bit
178:09 - older and almost towards middle school
178:11 - age one of them at least I think it's
178:13 - working with computers and learning
178:15 - about programming as a wonderful thing
178:18 - to do not to the exclusion of other
178:20 - things but could you write an OS if you
178:23 - wanted seriously doubt it no can you do
178:26 - a JavaScript tutorial without p5 I
178:28 - probably could and maybe I will but I
178:31 - you know that's not really my wheelhouse
178:33 - so much but I appreciate the thought
178:39 - kid what was your first programming
178:41 - language asked Simon in the slack group
178:44 - my first programming language was basic
178:46 - when I did a little bit of it in second
178:47 - grade but I think I just did it a tiny
178:49 - bit then then I believe in middle school
178:52 - I used assembly language for a class
178:55 - that I took and maybe a little basic
178:57 - again but then I didn't program all the
178:59 - way up until I was 28 or 29 I have to
179:02 - look at him I can't remember and the
179:04 - first programming I learned then was
179:05 - actually with the lingo programming
179:07 - language with Macromedia director and I
179:09 - recently got to meet John Henry Thompson
179:14 - who created the lingo programming
179:16 - language and is I this is not his
179:22 - current website well here it is who's
179:25 - amazing and is working on a lot of
179:26 - really interesting projects if you don't
179:28 - know about John Henry Thompson learn
179:31 - about John Henry Thompson's work so many
179:33 - things that I'm doing that we're doing
179:35 - now come directly from all the work that
179:37 - he did with the lingo programming
179:39 - language Macromedia director and many
179:41 - other things and he's working on a
179:42 - project right now called dice I saw a
179:44 - presentation distributed instrument for
179:46 - computed expression so I encourage you
179:47 - to check this out it's wonderful how old
179:50 - are you 44 how is your elbow it is doing
179:54 - much better you can see my I mean I'm
179:56 - basically recovered although I don't
179:57 - have full extension
179:59 - but I do have full and I still have a
180:02 - little weakness I'm still doing some
180:03 - physical therapy exercises but I would
180:05 - say I'm pretty well okay all right I've
180:08 - got to go now I will just take these
180:10 - last two questions are you currently
180:12 - writing planning new books no but the
180:15 - three book projects that I would like to
180:17 - do if I had time were one nature of code
180:20 - in JavaScript which I may actually get
180:21 - to a version of learning processing but
180:24 - like an intro book but with JavaScript
180:26 - and the third one would be a book all
180:28 - about programming with text as data so
180:31 - algorithms for generating poetry or
180:32 - analyzing texts and that kind of thing
180:34 - so by the time that's what I would do
180:35 - but right now I need like a big block of
180:39 - time to just focus on something like
180:41 - that so I speak a little bit of French
180:43 - okay so anyway thank you for watching
180:48 - everybody I will see you next Friday I
180:52 - expect next Friday will also be a
180:54 - morning time around this time but stay
180:57 - tuned I announced on Twitter when I have
180:59 - a time I post it on this new website or
181:02 - you know I'm making out in this whatever
181:05 - I try to announce everywhere and
181:07 - schedule it on YouTube as best as I can
181:09 - so thank you have a wonderful weekend
181:12 - the edited versions of all these videos
181:14 - will come out next week and I look
181:16 - forward to seeing you all next time on
181:17 - YouTube goodbye