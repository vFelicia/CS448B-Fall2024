00:00 - OK, it is time now.
00:02 - I am going to build a image
classifier using my own images.
00:07 - I am going to teach
this example to not
00:11 - say acoustic guitar or electric
guitar, but to say ukulele.
00:13 - I am going to teach this
example not to say syringe,
00:18 - but just say train whistle.
00:19 - OK, this is what
we're going to do--
00:21 - and the process that I'm
going to use is transfer .
00:23 - Learning
00:24 - I described this process
in the previous video.
00:27 - You can go back and
watch that if you want,
00:28 - or you can just keep
following with me.
00:30 - I'm going to write
the code for this.
00:32 - One thing that I
want to mention is--
00:33 - you might be wondering,
oh, why is it
00:35 - called a feature extractor?
00:39 - How does this stuff happen?
00:40 - So I want to first
mention it, and thank
00:41 - Gene Kogan for making the image
classification and regression
00:45 - transfer learning with
MobileNet examples.
00:47 - You can find these sort of
native TensorFlow.js versions
00:51 - of what I'm doing right
now here at the ml4a--
00:54 - machine learning for artists--
00:55 - website.
00:56 - It's a wonderful website
with tons of resources
00:58 - about machine learning.
00:59 - And also it's an interesting
whole discussion here,
01:02 - in terms of how should
the ml5 API work,
01:04 - or what should things be called.
01:06 - So if you're curious about how
open-source projects choose
01:09 - their names of things,
I might reference
01:12 - this particular thread.
01:14 - But the important
piece for us is
01:17 - to be at the ml5 website on the
featureExtractor documentation
01:22 - page.
01:23 - I going to need to make
heavy use of this page
01:25 - to look up what the names of
all the functions I need to type
01:27 - are in.
01:28 - And then, of course, I
should mentioned, if I just
01:30 - go down here under
Examples, Classifier
01:32 - with Feature Extractor,
this is basically
01:34 - what I'm going to build.
01:36 - The point of this video is
I'm going to build it up,
01:40 - but you could just look at this
example instead, if you want.
01:43 - But anyway, all right.
01:44 - So let's go back to the
featureExtractor documentation
01:48 - page.
01:49 - What I've got here
is the code that I
01:52 - wrote from a previous
video, a couple videos ago,
01:55 - using the MobileNet
model to classify images
01:57 - from the webcam.
01:59 - OK, so if I go to the
code, the main thing
02:02 - that I need to change
is I no longer want
02:04 - to make an ml5 imageClassifier.
02:06 - I want to make an
ml5.featureExtractor.
02:13 - And the difference
here, also, is
02:15 - I'm not going to
reference the video yet.
02:18 - So I'm just going to make
a featureExtractor built
02:21 - on top of MobileNet.
02:22 - And this callback
modelReady means
02:24 - the model has been loaded,
the MobileNet model
02:26 - is downloaded from wherever
it needed to download it from,
02:29 - and it's there and ready to go.
02:32 - So this should say
model is ready.
02:34 - I'm going to get rid of
this predict function.
02:36 - So now, if we just refresh
this-- a lot of stuff's
02:38 - going to break here--
but I see model is ready,
02:41 - and the video appears.
02:42 - Now, there's no labels
anymore, because I got rid
02:43 - of the imageClassifier--
02:44 - I have a featureExtractor now.
02:46 - But I want an
imageClassifier, so what I do
02:49 - is I'm going to add a variable.
02:52 - I'm going to write a
variable called classifier.
02:54 - So the variable mobilenet
is now referring
02:57 - to the featureExtractor,
and the classifier
03:00 - equals mobilenet,
the featureExtractor,
03:04 - dot classification.
03:06 - So I want to make--
03:09 - classification.
03:10 - I want to make a
classification object
03:14 - from the featureExtractor,
and I need to give that--
03:16 - I want to say, and I want to
use images from the video.
03:19 - And again, now, if I were doing
this with a database of JPEGs
03:22 - that I'm loading,
or something else,
03:24 - I would do it in
a different way.
03:25 - But I'm going to use the
video in this example.
03:27 - So I'm going to say
video, and then I'm
03:28 - going to say videoReady, because
I want to have a callback also
03:31 - to know that the video is ready.
03:34 - I don't really
need that callback,
03:35 - but it's sort of useful.
03:37 - And I write that up here--
03:39 - videoReady.
03:40 - And I'm going to say
a video is ready.
03:42 - So let's refresh this again, and
we should see my image pop up,
03:47 - model's ready, video's ready.
03:48 - OK, now we are ready to
train our own labels.
03:55 - We have the featureExtractor,
we have the classifier.
03:57 - We can give it
images of a ukulele--
04:00 - and I don't know if I
wrote this on the board
04:03 - just now, or earlier,
or when, but I used
04:04 - to have ukulele spelled wrong.
04:06 - It's spelled U-K-U-L-E-L-E.
I don't know why,
04:10 - but I feel like it's very
important to say right now.
04:13 - OK, so how do I do that?
04:16 - Well, I could look up in
the featureExtractor page,
04:20 - there's a function
called addImage.
04:23 - And it's right here.
04:26 - I know some of this API
from having practiced
04:28 - this a little bit,
but this is what
04:29 - I'm looking for-- addImage.
04:31 - addImage and a label.
04:32 - So I can say--
04:34 - oh, but when do I
want to add an image?
04:36 - I want to make a button that,
every time I press the button,
04:41 - I'm saying that's
a ukulele image.
04:43 - So let's first create ukeButton,
and then I'm going to say,
04:47 - in setup, ukeButton equals
createButton ukulele--
04:57 - but if it's uke, it's just
U-K-E. Boy, it's confusing.
05:00 - Now, here's the thing--
05:02 - I am creating this
example in a very basic,
05:06 - what I hope is
beginner-friendly way.
05:08 - There are so many ways you can
build an interface, and style
05:12 - your page, and handle events.
05:14 - I'm just going to use
simple p5 functions that
05:17 - place a button on the page,
and a simple callback function
05:21 - for when I press the button.
05:24 - So then I'm going to say
[? ukePressed ?] mousePressed.
05:27 - And I could put the
function name here
05:29 - and write the function
somewhere else, which
05:30 - I do in some videos, to be kind
of-- but I'm just going to put
05:33 - an anonymous function in here.
05:35 - The idea of this
anonymous function
05:37 - is, when I press the
button, this function
05:40 - should be executed.
05:40 - So the code I'm
writing in here will
05:42 - happen when I press the button.
05:44 - And what do I want to do there?
05:45 - I want to say
classifier.addImage,
05:49 - and I want to give it a label--
05:51 - ukulele.
05:53 - I hope that's spelled right.
05:55 - So now, whenever I press
the button, it's a ukulele.
05:58 - Let's add oh train whistle
one while we're at it.
06:03 - Let's say
whistleButton, and let's
06:07 - do exactly the same thing,
but with whistleButton.
06:15 - whistleButton, createButton--
I'm going to have the button
06:20 - say whistle.
06:20 - I cannot spell anything.
06:23 - whistleButton, and then
addImage("whistle").
06:26 - So you can see here
I have two buttons.
06:28 - Ukulele adds an image
assigned the label ukulele,
06:33 - whistle adds an image
assigned the label whistle.
06:36 - Now, there's not really much
point to me running this code.
06:39 - I should just make
sure I have no errors.
06:40 - We can see the
buttons are there now.
06:42 - The image comes up, but
nothing's going to happen.
06:45 - I can keep pressing
this ukulele button,
06:46 - I can keep pressing the whistle
button-- nothing's happening,
06:49 - because I'm not giving
myself any feedback.
06:53 - So right now, I just have
to hope it's working.
06:55 - But the thing that
I need to do next
06:59 - is actually apply
a training step.
07:01 - Now, one thing that
was interesting--
07:03 - I showed in the previous
video the Teachable Machine,
07:08 - this project I'm basically
making exactly the same thing.
07:12 - Train green, train
purple, train orange.
07:14 - My buttons are train
ukulele, train whistle.
07:17 - But it just started
to work immediately.
07:19 - That's because there's a
slightly different algorithm
07:21 - at play here.
07:22 - The algorithm that
I'm using in ml5
07:27 - requires an additional
step, a training step.
07:30 - I'm going to write
this here-- training.
07:33 - So basically, the process is
add a bunch of images-- say,
07:37 - hey, this is a ukulele, this
is ukulele, this is a ukulele,
07:40 - this is a whistle, this is a
whistle, this is a ukulele,
07:42 - this is a whistle,
this is a ukulele.
07:43 - Once I've added all those
images, then I explicitly say,
07:47 - I'm done with all
of these images.
07:49 - I'm going to let the model train
itself using those the features
07:54 - that it extracted
of those images,
07:56 - and map those features
basically to these labels.
07:58 - And by map, I really mean--
there's another machine
08:02 - learning model right here.
08:03 - It's actually like a
neural network model.
08:08 - And so that mapping between
the features and the new labels
08:10 - happens with another
neural network here.
08:12 - But ml5 and TensorFlow.js are
handling all of that for us.
08:17 - OK, now, let me
come back over here
08:20 - and add the training steps.
08:22 - So I certainly need
one more button--
08:25 - train choo-choo button.
08:30 - So I'm going to add that button.
08:33 - createButton trainButton--
I'm going to say train--
08:40 - trainButton-- oh, OK, now here--
now, what am I doing in here?
08:44 - OK, what I'm doing
in here is I'm
08:46 - going to say classifier.train.
08:51 - So I'm going to say, hey,
classifier, train yourself.
08:54 - Now, I could just
leave it at this
08:56 - and kind of say,
hey, great, I'm done.
08:58 - But something that
I should do here
09:01 - is I can actually put a
callback inside of here.
09:04 - Now, this is getting
pretty awkward.
09:06 - I'm going to allow it-- and you
know what, actually, though?
09:08 - I'm going to make a
separate function just for--
09:10 - because I might want to
do a bunch of things.
09:14 - I'm going to call
it whileTraining.
09:17 - So I just want to put this
in a separate function
09:20 - so I can look at it on its own.
09:22 - What did I call
it? whileTraining--
09:25 - so this function
whileTraining is a function
09:28 - that's kind of kind run over and
over again during the training
09:31 - process, and it's going to
report back to me information
09:34 - about the training process.
09:35 - And the information it's
going to report to me
09:37 - is something called loss.
09:40 - Loss.
09:40 - Let's just actually
run this, and then I'm
09:44 - going to explain what loss is.
09:45 - Let's just see if I can
get this to work so far.
09:48 - So I'm going to refresh.
09:50 - My model is ready,
my video is ready.
09:52 - I have a ukulele.
09:54 - I can press the ukulele,
ukulele, ukulele.
09:55 - I'm going to show it
a lot of ukuleles.
09:57 - Here's a lot of
examples of ukuleles.
10:00 - And now, I'm going to
hold up my train whistle.
10:02 - Here's a lot of examples
of train whistles.
10:05 - Now, I'm going to
hit the button train.
10:08 - And yes, we see this
number here in the console,
10:12 - it's training and training
a training while training
10:15 - console.log this value, loss.
10:19 - What is the loss?
10:22 - So loss is a really
important term
10:25 - in machine learning,
data science.
10:28 - It's often also called cost.
10:31 - And by this, I mean loss
function or cost function.
10:34 - And what the loss function our
cost function is calculating
10:37 - is the error.
10:38 - So what do I mean by error?
10:40 - So basically, if I say
to the machine learning
10:44 - model that's training,
hey, here's a ukulele--
10:49 - this is a ukulele,
but just pretend
10:50 - you don't know what
it is for a second.
10:52 - What do you think it is?
10:53 - And if it comes back and
says, I think it's a ukulele,
10:55 - guess what?
10:56 - The error is 0.
10:58 - If it comes back and says, I
think it's a train whistle,
11:01 - then the error is not
0, it's something else.
11:04 - But something that I haven't
really mentioned, because we're
11:06 - kind of living above
the fray here--
11:09 - we're talking in
terms of labels.
11:11 - The machine learning
model underneath the hood
11:13 - is just working with numbers.
11:15 - The labels are only a thing
for us, the human being,
11:18 - to use at the end
of the process.
11:20 - So when it's actually
calculating an error,
11:22 - even if it guesses
it's a ukulele,
11:24 - it's calculating error based
on the numeric probability
11:27 - that it guessed.
11:28 - So the machine learning
model might think, OK,
11:31 - that's 80% likely
to be a ukulele.
11:35 - So that might be
the right answer,
11:37 - but we know the right answer
is it's 100% a ukulele.
11:39 - So the error is
actually the difference
11:41 - between mean 100%
and 80%, or 0.2.
11:43 - So this idea of
the aggregate error
11:46 - of all of the training
images over time--
11:48 - ml5 and TensorFlow.js is
calculating that for you
11:52 - during that training process.
11:53 - And we're seeing that here--
11:55 - [CLOCK TICKING]
11:57 - There's a clock tick tocking,
because I have a clock sound
12:00 - effect, for some unknown reason.
12:01 - We can see that that loss
is getting very, very low.
12:03 - So you want the error to be low.
12:05 - The error started kind of
high-- it was like 6.92,
12:10 - and then it got lower and lower
lower, as it was training.
12:12 - And one thing you'll notice
is, eventually, it's stopped.
12:15 - Now, it's an arbitrary decision
when do you stop training.
12:18 - Ml5 has default settings.
12:20 - It's going to train for
a while until the loss
12:22 - of a certain amount,
or something like that.
12:23 - But you can see
when it's finished.
12:25 - It gives you the low as--
12:26 - the loss as no.
12:28 - The low.
12:28 - The loss as null.
12:30 - So I can actually say here, if
loss equals null, console.log--
12:41 - I'm going to say
training complete,
12:46 - else I can console.log the loss.
12:49 - And then guess what?
12:50 - When the training is complete,
what do I want to do?
12:53 - I want to classify.
12:54 - So I'm going to say
classifier.classify(gotResults).
13:01 - And I already have the
gotResults function.
13:06 - I'm getting results,
just like I got results
13:08 - from the raw MobileNet
model, but now I'm
13:11 - getting the results from my
new transfer learn trained,
13:16 - custom trained model.
13:17 - So I say
classifier.classify(gotResults).
13:21 - I get the results,
if there's an error.
13:23 - Otherwise, I can give the
class name to the label
13:25 - to get drawn on the screen.
13:26 - And then what I
want to do-- again,
13:28 - I want to say
classifier.classify.
13:30 - So before, it was
called predict.
13:33 - Now, it's called classify.
13:35 - I'm not sure why.
13:36 - Maybe, at some point,
check that code
13:38 - that goes along with this
video to see if that's changed.
13:40 - But that's what it is right now.
13:41 - OK, so I think we're actually
done with this example,
13:44 - sort of.
13:46 - Let's try.
13:49 - OK, model is ready,
video is ready.
13:51 - Oh, boy.
13:52 - What's the chance that
this is going to work?
13:54 - Stop and guess.
13:55 - I give it like 10
to 1 this works.
13:59 - OK, I'm going to step
out of the frame,
14:01 - and I'm going to train it with
a bunch of images of a ukulele.
14:06 - I'm going to move
the ukulele around
14:07 - to give it a lot of
different examples.
14:09 - Further back-- and
again, I should probably
14:14 - add something that shows me
how many training examples,
14:16 - because I probably
want to give around
14:18 - the same number of training
examples for the whistle
14:20 - as with the ukulele.
14:21 - I'm going to train this.
14:23 - Whistle, whistle,
whistle, whistle, whistle.
14:25 - Now, I'm going to hit train.
14:28 - When it's done, we should
start seeing labels.
14:30 - Training complete.
14:31 - I don't see labels.
14:35 - No labels, that's so sad.
14:37 - What did I do wrong?
14:38 - OK, so something's wrong.
14:40 - I got to figure this out.
14:41 - I got to debug this.
14:42 - All right, I really should
put console.dot(results)
14:45 - in, because I don't even know if
this function is being called.
14:49 - Let's make sure dot classify is
the right name of the function.
14:53 - Classify, callback-- yeah.
14:58 - Oh, no, no-- yeah, callback.
15:02 - And then the callback is a
function, otherwise blah,
15:05 - blah, blah.
15:06 - So I think this is right.
15:09 - Everything looks right to me.
15:11 - Am I just not drawing
the label properly?
15:13 - No, there it is, label--
15:18 - This is me not paying
close attention.
15:21 - This is interesting, and
this is now a question
15:24 - for the ml5 library itself.
15:27 - But the thing that comes
in the gotResults function
15:31 - is actually just the
label that it guesses.
15:34 - So there's not like the class
name and the probability,
15:36 - all that stuff that
came with MobileNet.
15:38 - It's not giving us
all that information,
15:40 - which probably makes sense.
15:41 - So actually, all I want to do
is say label equals results.
15:48 - So that was a little
bit of a digression.
15:50 - I had to figure out
what I did wrong there.
15:52 - Now, this should work.
15:55 - OK, ready?
15:56 - Let's try it with the ukulele.
16:01 - Giving it a lot of
examples of a ukulele.
16:06 - Giving it that a lot of
examples of a whistle.
16:12 - Now, I'm going to train it.
16:13 - I'm going to do
my training dance.
16:17 - And training is complete.
16:21 - Here's a ukulele.
16:24 - It is a ukulele.
16:29 - It is a whistle.
16:33 - It is a ukulele.
16:36 - It is a whistle.
16:39 - Yay, OK.
16:42 - So this actually works.
16:43 - Here's the thing--
16:43 - I want to show you something
more interesting here.
16:47 - It's not more interesting--
16:48 - I'm going to show you
something different.
16:50 - I'm going to leave all the
variable names the same,
16:53 - but I might change
this to smile,
16:55 - I'm going to change it to happy,
I'm going change this to sad.
17:02 - OK, so I'm going
to run this again,
17:06 - and I'm going to do this.
17:10 - And we get lots of
pictures of me smiling.
17:19 - Then I'm going to hit train,
let it train for a little bit.
17:24 - It's finished.
17:38 - How long should I do that for?
17:39 - This is pretty cool.
17:40 - Here's the thing--
it doesn't just
17:42 - have to be image recognition.
17:43 - I can training with
different facial expressions.
17:47 - What MobileNet is really
good at, if you recall,
17:50 - is taking an image
and boiling it
17:52 - down to a smaller
list of numbers
17:54 - that kind of define the
essence of that image.
17:56 - So it doesn't matter
what the content is,
17:58 - it can figure out the essence
of me smiling, the essence of me
18:01 - frowning.
18:02 - It is important,
though, to remember
18:04 - it's not really learning smiling
or frowning, it's kind of--
18:08 - and if I were to
turn this this way,
18:11 - you can see some of the
room here-- and I go here,
18:19 - it's not working so well,
because it learned all
18:23 - that stuff with the background.
18:25 - So there's a lot
of nuance to this,
18:27 - and it's really
important to remember.
18:29 - It's not some magic.
18:31 - The computer doesn't
have suddenly
18:32 - some understanding of emotions.
18:35 - It's really just looking
at, I've got a new image,
18:37 - and I'm comparing it to a
bunch of images I got before.
18:41 - What's it most similar like?
18:42 - Let's see if it--
18:43 - now that I moved the camera
around-- is it working.
18:50 - So here's the thing--
18:53 - go.
18:54 - Go forth.
18:55 - Add a third category.
18:57 - Make a project.
18:58 - Make a game where
I'm playing Pong--
19:00 - actually, this is a project I
should reference by Alejandro.
19:03 - Let me go to ml5js.org.
19:06 - Alejandro, under
experiments, made
19:09 - a project called Pong
ML, which basically
19:12 - is the same exact technique to
train various hand gestures,
19:17 - and then you could play
Pong with hand gestures.
19:19 - So there's so many possibilities
of what you could do just
19:22 - from being able to train
starting with the MobileNet
19:26 - model, and applying
transfer learning.
19:30 - OK, are you with me?
19:31 - Did this makes sense?
19:32 - Make something.
19:33 - Go and try this.
19:34 - Make your own version of it.
19:35 - Really think about
the interface.
19:37 - And one more thing--
19:39 - something you're really
going to want to do--
19:40 - and you've probably
noticed this--
19:42 - every time I
restarted the sketch,
19:44 - I had to retrain
with images again,
19:46 - which is fun for the sort of
real-time interactive nature
19:49 - of this.
19:50 - But if you're actually going
to do this for a while,
19:53 - you're building an
installation or something,
19:55 - you've trained it
with a bunch of images
19:56 - and you want to, I don't know,
save the model that you've
20:00 - done, save all that work,
you're going to want to save
20:03 - and be able to reload the result
of that transfer learning.
20:06 - At present, at the
time of this recording,
20:08 - there isn't a way to do that
easily with the ml5 library.
20:11 - It's certainly
technically possible.
20:13 - There is a GitHub issue.
20:14 - Currently, it is
ml5-library/issues/174 with
20:18 - a discussion about this feature.
20:19 - Certainly, once
this feature exists,
20:22 - I will come back
and make a video
20:24 - to show you how to do that.
20:25 - OK, thanks for
watching this video.
20:27 - Oh, I'm going to
do one more video.
20:29 - I'm going to something
interesting--
20:31 - hopefully.
20:31 - I'm going to do one more
video with something
20:33 - very similar to what
I did just here,
20:35 - but I'm going to do something
called a regression.
20:37 - So I'm going to define what
a regression is, and show you
20:39 - how I can do transfer learning
with the MobileNet model
20:42 - to perform a regression.
20:43 - Why would I want to do that?
20:44 - I don't know.
20:45 - I guess we'll have to watch.
20:46 - Basically a slider.
20:47 - If I want to be able to control
something with my hand moving
20:49 - back and forth as
a slider, that's
20:50 - something that I can
use a regression for.
20:52 - All right, see you there.
20:54 - [MUSIC PLAYING]