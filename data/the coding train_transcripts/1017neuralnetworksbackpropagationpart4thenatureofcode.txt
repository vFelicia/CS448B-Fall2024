00:00 - okay suppress shift miss here trying to
00:05 - move on and start to implement this code
00:07 - so what I need to do is okay so what
00:09 - I've done successfully somewhat
00:11 - successfully in the code so far is
00:12 - compute the errors the output errors and
00:15 - the hidden errors and I've added I have
00:17 - a transpose function I have a matrix
00:19 - multiply function so what I need to do
00:21 - is compute the deltas the gradients I
00:24 - need to figure out how what I need to do
00:25 - to change all these weights I should say
00:27 - that I did say something I think wrong
00:28 - in the previous video so I might as well
00:30 - at least say it here which is that in
00:32 - with matrix multiplication and this
00:34 - shouldn't be an asterisk I don't know
00:35 - you know this you can think of the dot
00:36 - product or just an X for matrix
00:38 - multiplication I need to have be a this
00:41 - is matrix a and this is matrix B a the
00:43 - number of columns in a has to be the
00:45 - same as the number of rows in B so this
00:48 - could have as long as there's one column
00:50 - this could have three rows and this
00:51 - could have eight columns but this is one
00:54 - dimensional this way one dimensional
00:56 - this way and that's going to that's
00:57 - going to always give us in the end the
00:59 - correct dimensions for the weight matrix
01:01 - so I kind of miss stated that in the
01:03 - previous video I don't know if you were
01:06 - worried about that I don't know if
01:07 - correcting it by now but that's enough
01:09 - so let's try to put this stuff in here
01:11 - now one thing that I think I really need
01:12 - to do unfortunately is that I had this
01:15 - lovely idea
01:16 - previously of like Oh what I need to do
01:19 - first to get the error right is I need
01:22 - to feed the inputs in get the output and
01:26 - compare it to the target right that's
01:29 - going to give me the errors and then I
01:32 - can compute the hidden error by doing
01:34 - the weighted percentage stuff that I did
01:36 - previously the issue is once I start
01:39 - wanting to do all this stuff it would be
01:41 - nice if I could remember all the parts
01:43 - that happen during the feed-forward
01:44 - process so as much as I just wanted to
01:47 - call feed-forward here I think it's
01:49 - actually gonna work better if I run the
01:53 - feed-forward stuff here so I'm gonna
01:54 - actually grab all of this copy it and
01:58 - I'm going to paste it right here so
02:00 - there's a little definitely some
02:01 - redundancy in the code this is going to
02:03 - help me figure it out so I need to feed
02:06 - everything forward inputs hidden which
02:12 - then gets the bias
02:13 - passes through sigmoid sidon this hidden
02:16 - matrix is left in the state of the
02:19 - values coming out of here good then I
02:25 - have the output be adding them the bias
02:28 - and the output is left in the state of
02:31 - it coming out now in the state of it
02:34 - coming out of here now Bri viously I had
02:39 - taken the outputs from the root from the
02:42 - feed forward function I don't want to do
02:43 - that anymore in fact I guess I'm being
02:45 - consistent
02:46 - I should call these outputs so now I
02:49 - have the outputs plus I have like all
02:52 - the other stuff that happened before if
02:53 - case I need to reuse it and I have the
02:56 - targets and I can have the output errors
02:58 - so now I need the gradients so what do I
03:02 - need for the gradient I'm gonna call
03:04 - this let gradient equal outputs okay so
03:12 - how am I going to do this how to do them
03:13 - it's like oh I don't have numb by this
03:15 - is where you really need numpy which is
03:17 - a Python library for doing matrix
03:18 - calculations so let me think about this
03:20 - I need to I need to take those outputs
03:25 - which have already been passed through
03:26 - sigmoid and multiply it times what I
03:28 - need to do but this won't work right I
03:30 - need to calculate this gradient as those
03:32 - outputs the derivative outputs times 1
03:34 - minus outputs so I have to do this with
03:35 - my matrix library so I need to somehow
03:38 - get this gradient which is this piece
03:42 - right here the nice thing is I can
03:44 - actually use functionality that I have
03:46 - built into the matrix library so for
03:49 - example I have that map function so I
03:51 - can take every element of output and set
03:54 - it equal to output at that element times
03:57 - 1 minus itself so what I need is another
03:59 - function right much like I have sigmoid
04:02 - what I need is the derivative of sigmoid
04:05 - now this there's a little bit something
04:06 - strange that's gonna go on here let me
04:07 - just write this if I write a function
04:09 - called d sigmoid what I really mean is
04:11 - return sigmoid of x times 1 minus Sigma
04:20 - of X this technically speaking is the
04:25 - Rivet of sigmoid but that's actually not
04:28 - what I want to do here because if you
04:31 - know if you're following along and where
04:33 - am I here in train I've already mapped
04:36 - the output through sigmoid so actually
04:40 - what I want is and I kind of like oh I
04:42 - would call it like fake dig see sigmoid
04:45 - but I'm just gonna put Y in here
04:47 - and I'm going to comment this out and
04:49 - kind of as if Y is I'm just changing the
04:51 - very ammonium Y has already been sigmoid
04:53 - and I'm gonna say return Y times 1 minus
04:57 - y so what I can do now to calculate this
05:01 - gradients is I can basically say outputs
05:05 - I kind of want to call it gradient but
05:07 - right outputs dot map and maybe I should
05:14 - make a copy of it or something
05:15 - d sigmoid right so now I've taken
05:19 - outputs and I've set each element equal
05:24 - this now I need two element wise
05:25 - multiply that by the error so I need to
05:29 - say outputs now here's the thing in my
05:34 - matrix library this is like I have this
05:40 - right here the multiply function
05:42 - currently if it gets a matrix it does
05:44 - what's what I was for the element wise
05:46 - multiplication which I'm referring to as
05:48 - the Hadamard product so otherwise so I
05:53 - guess what I mean I'm gonna keep this
05:57 - and I'm gonna say outputs dot x output
06:04 - errors there we go so now I've done this
06:13 - piece and this piece now I need to
06:15 - multiply it by the learning rate do I
06:16 - even have I all I've been waiting my
06:19 - whole life just to get to the point
06:20 - where I could put the learning rate in
06:21 - the code because I feel like once you
06:22 - have the learning rate in the code I
06:23 - kind of done so let's make that a
06:25 - variable this dot learning rate I don't
06:30 - know I'm just gonna set equal like point
06:31 - one right now and so now I also need to
06:34 - say outputs dot multiply
06:39 - by learning rate and now what I need to
06:43 - do I've done this whole piece here all I
06:47 - need to do is take what came out of hit
06:50 - and transpose it and do the matrix the
06:53 - matrix product matrix multiplication
06:55 - between that matrix and this matrix and
06:58 - then I have all the Delta weights and I
07:00 - can just adjust them you know I've got
07:02 - to talk about stochastic etc but let's
07:05 - just let me let me just we try to get
07:08 - through what I'm doing here all right I
07:10 - now am going to say what am I looking
07:15 - for I'm looking for this particular
07:18 - array right this particular vector I
07:21 - need to get let hidden T which is hidden
07:25 - transpose is matrix transpose hidden and
07:29 - then let deltas wait wait what am i
07:34 - calling these like the this dot weights
07:38 - I H so I'm going to say weights H Oh
07:42 - deltas equals matrix dot multiplied so
07:48 - this is calculate I want to put a
07:50 - comment in here calculate gradient and
07:55 - then calculate deltas calculate deltas
08:00 - matrix top multiply what do I want to
08:03 - multiply I got to do this in the right
08:05 - order I want the column vector which is
08:10 - the the gradients thing that I've been
08:12 - doing and the row vector which is the
08:15 - output to hit ok so I'm going to say
08:18 - multiply outputs so I hate that I've
08:21 - done this does the map function what I
08:23 - want to have is a you know what I want
08:25 - is I want a static version of the map
08:27 - function that will pull out make it new
08:29 - so let me do that really quickly so
08:31 - where's the map function map I made a
08:41 - static
08:43 - I made a static version already how
08:45 - exciting Oh life is good sometimes well
08:49 - that's so lucky so I want to do this let
08:51 - gradients equal the matrix dot map this
08:57 - solves all my problems outputs D sigmoid
09:03 - no I don't need all right so I want to
09:06 - map all the outputs with Sigma the
09:08 - derivative of sigmoid then multiplied by
09:11 - output errors multiplied by learning
09:13 - rate transpose the hidden output and
09:15 - then matrix multiply the gradients by
09:18 - the hidden output transposed and now
09:24 - wait H of this this dot wait H Oh add
09:34 - wait H Oh deltas so this is me just like
09:39 - taking going into just going out to
09:41 - saying I calculated those deltas change
09:43 - all the weights by those deltas so I'm
09:45 - no if this is this right let's think
09:48 - about this more later
09:49 - I need the bias well that's fine I'm
09:50 - gonna do the vice all right so that's
09:52 - good now I have to deal with the hidden
09:55 - layer this should be much easier now
09:57 - that I've done this once okay I need to
10:01 - calculate the hidden gradient which is
10:07 - matrix dot map the hiddens what came out
10:10 - of hidden passed through d d sigmoid
10:17 - then i need to take the hidden gradient
10:21 - and what did i do up here i multiplied
10:25 - by the output errors but have i
10:26 - calculated the hidden errors i have
10:29 - calculated the hidden errors somewhere i
10:30 - did back propagation hidden errors right
10:33 - there ah how lucky lucky me hidden
10:36 - gradient x hidden errors then hidden
10:41 - gradients x learning rate and that's
10:46 - this dot learning rate and did I forget
10:50 - that up here yeah this this stop
10:53 - learning rate right and
10:57 - then so this is calculate hidden
11:02 - gradient now oh my god
11:07 - calculate hidden Delta's alright you
11:13 - know that it's input to hidden input to
11:16 - hidden I'll just input to hidden deltas
11:21 - okay so that is just like I did up here
11:24 - the first thing I need to do is
11:26 - transpose input inputs T equals inputs
11:34 - dot transpose
11:36 - I know matrix dot transpose inputs
11:38 - matrix dot transpose inputs okay and
11:44 - then what did I call those deltas I
11:48 - called these weight hidden output deltas
11:52 - so let wait input hidden Delta 's equal
11:57 - matrix multiply the inputs
12:01 - no no nothing inputs the gradients times
12:07 - the transpose inputs so you hidden
12:12 - gradient times the what I call that
12:18 - inputs t transposed inputs and then I
12:28 - can just adjust those my goodness I have
12:34 - just less PII I'm speechless I without
12:37 - speech okay I think I might be done with
12:42 - this for the bias I'm gonna save that
12:45 - for I'm gonna add the bias in a separate
12:47 - video because I feel like I just need a
12:48 - break so let's think about this I might
12:51 - have made some mistakes but I think that
12:55 - I've gotten through this I think what
12:57 - I've done is I have figured out a way to
13:00 - use the train function to calculate the
13:03 - errors use back propagation to chop up
13:07 - and divide the error in a sine blame all
13:09 - over the place
13:11 - right I know the output errors I need to
13:13 - figure out the hidden layers errors then
13:15 - that's what I've got here those were the
13:17 - first two videos in this the third year
13:19 - I kind of talked through these formulas
13:20 - and now in this video I have used a math
13:22 - function to calculate the gradient the
13:24 - errors times the derivative of the
13:26 - output I adding the learning rate in I'm
13:29 - multiplying it by what's coming in
13:31 - transposed to get the weight deltas I've
13:33 - done that for both this layer this
13:36 - matrix and this matrix again at some
13:39 - point I really need to extract in you
13:42 - with this library to make it something
13:43 - useful I need to be able have multiple
13:45 - hidden layers and this back propagation
13:47 - would happen in a loop but I'm doing
13:48 - this to separate distinct chunks just to
13:50 - understand them and then I'm gonna
13:52 - adjust all the weights so there's things
13:55 - that I haven't talked about yet number
13:56 - one is I've got to adjust the bias
13:58 - values and number two is when and where
14:02 - should I be doing this should I run
14:04 - through all of my training data and get
14:06 - like the kind of average error of
14:08 - everything and then adjust all the
14:09 - weights or should I each time adjust the
14:11 - weights for each record or should i do
14:14 - batches like send in these ten data
14:17 - points adjust send in these ten adjust
14:19 - and that has to do with stochastic
14:21 - gradient descent versus a batch gradient
14:24 - descent so I'm gonna get there let me
14:28 - take a break take a few deep breaths and
14:30 - make a separate video where I adjust the
14:32 - bias ease I might have made some
14:34 - something absolutely completely wrong so
14:35 - you you have to watch all the way
14:38 - through to find out if I have other
14:39 - things that I have to correct later
14:40 - which I may but I'm gonna change the
14:43 - biases and then I'm finally going to do
14:46 - I'm just gonna try to train it on the X
14:48 - or to see if I've done this correctly
14:50 - XOR would be a really simple problem so
14:52 - it's a good test problem for me to see
14:53 - if my code is mostly working correctly
14:55 - but before I go there is one error that
14:58 - I can point out here this is input to
15:00 - hidden thank you okay see you in the
15:01 - next video
15:02 - [Music]
15:10 - you