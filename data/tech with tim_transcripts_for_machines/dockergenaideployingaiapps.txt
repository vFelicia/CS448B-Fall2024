if you've ever built any kind of llm or gen type application chances are you've got it working locally and eventually you're going to want to share this with other people or even push this into production now once you set this up locally there's all kinds of dependencies and requirements and trying to show someone else how to do that or get it set up in the same way on another computer can be a massive nightmare especially if you want to go into production so in this video I'm going to show you a solution to that which is using Docker to containerize your gen applications so that all of the dependencies are saved and you can really easily run your apps no matter what your Hardware configuration is this is a really cool video it's going to be really useful for any of you that work with Gen apps and have followed even some of the other tutorials I have on this channel where we're building things like gen chap Bots rag applications Etc so with that said let's get into it and learn how we can use Docker to deploy and containerize our gen apps so let's dive in here and I want to start by thanking Docker for partnering with me on this video they been really helpful in creating the video and providing a lot of documentation and resources so that I can really explain this to you in the best possible way now what we're going to start by looking at is something known as the docker gen AI stack now this is a collaboration between Docker neo4j Lang chain and AMA this is a way that you can actually deploy a gen application with just a few clicks and really minimal configuration which we couldn't say before because typically it takes a lot of different dependencies a lot of setup and and it was a huge headache to actually get these gen apps out into production with Docker now we can containerize the various different components of our application I'm going to show you later in the video exactly how we do that starting completely from scratch now the Gen stack as I said here uses olama for management so for the various llms this means you can actually run your llms locally if you're not familiar with AMA something again that allows you to run llms locally on your computer rather than relying on something like Chad GPT or GPT for next it uses neo4j this is a database and also a knowledge graph for the llms again this can run completely locally on your own computer and then we use Lang chain for the orchestration and then this uses Docker of course for doing all of the containerization deployment Etc now what I'm going to do is go over here to this Docker gen stack repo this is something I'll link in the description if you want to check out for yourself but this just demonstrates to you how you can run all of these different services on your own computer computer with really minimal configuration so this is a repository that has olama configured neo4j configured and actually just serves you a few different gen applications so it has a support bot stack Overflow loader PDF reader Etc and the purpose of this repository at least why I'm showing it to you is that you can see with just one or two commands you can actually get all of this running on your machine without having to do a ton of tedious setup so what I've done to demonstrate that to you is on the right hand side or on my V s code window here I've just cloned this repository the Gen stack now all I've done is just create an environment variable file here based off the example that they had and I simply ran the command Docker compose up now this just uses the docker compose file that's provided in this repository which spins up all of the various different services or containers that we need to run this application now I know it looks a little bit complicated but don't worry I'm going to show you again a much simpler application in a second and how we do this completely from scratch this is just an example of something that you could do again how fast it is so what I'm able to do now is view a few different applications on my computer so you can see that I have a support bot a stack Overflow loader PDF reader so what I'll do is just go to one of the applications here this is the stack Overflow loader again notice running local host on my computer and this allows us to load in various questions and answers from stack Overflow that match a specific tag so what I have done previously is I just loaded in Python list I'm not going to do this again because I already loaded it and this now allows me to go to the main bot UI which is just served on a different port and I can now ask questions um about the various stack Overflow questions so I can say hey what is an index error in Python and now it's going to go and look up that information for me and give me a response so you can see here I got a reply and it's showing me the various different errors from stack Overflow that are relevant to this question using something known as rag which is retrieval augmented generation the important thing here is all of this is running on my own machine and again I didn't really need to set anything up for this to work now another application that this provides just as a demo is something that allows you to chat with your PDFs so I'm going to upload a PDF here and then I can ask questions directly about that so I just uploaded a PDF here which is just my receipt for a QuickBook subscription payment this should be for about $60 so I'm just going to ask the llm how much was the payment for and it should be able to actually use this PDF and give me the reply there here you go you can see the amount was $62 obviously I could ask this something that's a lot more complicated but just a demo of how this works so that's a quick demo of the geni stack but I want to quickly walk you through a few other really cool features that Docker has released recently that are really useful and you guys can find some value from now the first feature is profiles now if you've ever worked with some complicated repositories before you may have actually ridden multiple Docker files or Docker compos files for the different types of configurations or different type of hard where you might be running on a good example of this is running on a CPU versus running on a GPU now Dockers come up with a solution for this you don't need to repeat a ton of code and they've introduced profiles now this allows you to actually just specify a flag while you're running the docker compos file to indicate what profile you want to run with and you can do some different configurations or setup based on that profile so in this case you see that we have Linux and then we have Linux GPU so if we want to run using an Nvidia GPU which is specified right here which you would want to be doing especially if you push this into production then you would simply utilize this profile however in development maybe you're working on a machine that doesn't have a GPU like I am right now or at least with a noncompatible GPU and in that case you can just use the default profile Linux so if you want to actually run the profile command I'll link some documentation to you in the description but you can see it's as simple as just specifying what profile you want to run and now all of a sudden you can change the configuration and the way the Dockers is going to work with just a single change to your Docker compose command so that's the profile feature but another cool feature you'll find useful is the watch feature now if I scroll down here in the docker compos file you can see that we have a section labeled X develop and then we have watch now this is another new feature that Docker has added that allows us to actually watch specific files and then rebuild certain containers whenever any of the contents in those files change now this is really useful because before you would have to manage which containers you were running yourself and you'd have to know which ones to restart based on what you were changing while you were developing certain parts of your application in this case you simply write in the watch which is done right here and now if any changes are applied to boty PDF bot Etc we can automatically rebuild this container without having to rebuild all of the other containers now we can do this in all of the containers for example if we go down here you can see there's a different watch and all we have to do in order to actually utilize this feature is just run a slightly different command so I'm just going to stop this here here and I'll show you the command that we can run so there's a few different commands here but I can go with Docker compose watch and when I do that it's now going to use these watch fields that we've declared and if I make any changes to these files we'll automatically rebuild that container or whatever it says here we can sync it we can rebuild it there's a few different options so hopefully you just learned a bit about the Gen stack and some useful Docker features you can take advantage of now I know a lot of you will be asking now okay that's great I understand geni stack but how do I actually do this for my own gen application that's why I'm going to show you now how you can toriz your own gen app so without having the docker files created how do you actually go through this process now there's some great documentation on the docker website which I'm going to link in the description and I'm just going to follow along with it and kind of walk you through step by step so you understand what we need to do in order to get this running so click on this link in the description if you want to follow along with me and what I'm going to do is copy this sample repository which has a very simple llm app built uses streamlit for the UI AMA and then neo4j it also uses Lang chain now this is a python based app which I'm sure a lot of you will probably be building so what I'm going to begin by doing is just cloning this repo so let's just paste this command in and then once that's done we can go inside of here and we can start initializing kind of our Docker files the compose file the docker file Etc and I'll show you a really unique command that can do that almost instantly for us so the repo is cloned now and what I'm going to do is just CD into it and you'll notice here if I click into it on the side that we just have some basic python files an environment variable file Etc we don't have anything we need to actually kind of start this with Docker and deploy it with Docker now fortunately for us there's a really useful command called Docker AIT that we can run that will actually create all of the necessary files for us now in order for that command to work we are going to need Docker desktop installed on our machine so what you can do is simply go to Docker desktop you can find the correct download link whether it's on Mac Linux or Windows and just make sure you have that on your machine before you start actually trying to initialize this command you also want to make sure that you're running Docker desktop so if you're on Mac for example you can just open up Docker and then as soon as you open it it should actually start the docker engine if you're on Windows or Linux same thing you just have to open Docker desktop and then it should run everything for you and you should be able to initialize this command so I'm going to run Docker and net from inside of this repository and what this will do is go through kind of some basic configuration for me and allow me to configure my app so I'm going to say that this is for a python application which has been automatically detected it's going to ask us what version we want to use I'm just going to go with the newest version here the port I'm going to go with the default port and for the running command this is what you'll actually use to enter your application obviously this will differ depending on the type of app that you have now in our case we're just going to go back to the docs here and you'll see that there is a command that we can copy so it's right here streamlit run app and that's because we're using streamlit for our UI so that's the command we're going to use so let's paste that in here and it just specifies the server address and the port okay so now that we've done that it's actually created the necessary files for us so we have the docker ignore file we have the composed. yaml file which is where our various services are going to be defined we have the docker file which is usually a huge pain to write but now it's just completely done for us and we even have a readme doer. MD file so that's the docker and niit command super useful for getting up and running really fast and now if we just make a few really minor changes we can actually run this application on our local computer so actually I lied all we need to do is just type the command Docker compose up and then D build what this is going to do is use the docker compose file which is already created for us it's going to spin up the container that we need which will be running our application and even though this hasn't containerized everything for us yet it's just showing you kind of baby steps in terms of getting the application running so this here will just actually create a container for the user interace or the server so for kind of like the streamlit application it's not going to containerize Ama or the database or some other things that we need if we want everything to be running completely locally but you'll see what I mean in just one second once this gets loaded so you can see after running this command the streamlet application now is running within the docker container so I'll bring it up for you and you can see what it looks like so this application you see here is actually the chat with PDF application that we looked at previously now the reason why it's giving us these fields here and saying we need neo4j urri username password and we need one of our llm models either the AMA base URL or the open AI API key is because we haven't yet containerized those and made those services that are accessible to this app now we're going to do that in just a second so you can see how all of this runs completely locally but at this point if you wanted to you could connect to a remote database you could connect to open Ai and utilize something like gp4 and you can have this app running on your own computer you're just going to be Outsourcing some of the components and not entirely using the docker containerization but what I want to do is I want to go and actually containerize the rest of our components so you can see what that looks like and how we do the deployment completely locally so what I'm going to do now is stop this application by hitting contrl C then I'm going to shut down the containers by doing Docker compos down and then what we're going to do is containerize the other services like the database and olama so that we can run all of this without relying on some third party dependency so I'm going to say Docker compos down just to remove these different containers that we don't need and now what we're going to do is go back to the documentation and we're going to follow along with the next page so if you scroll down to the bottom of the docs you can press develop your application that's going to bring you to the next step which will bring you to this page right here and this is going to show you how you add a local database and how you add a local or remote llm service which is exactly what we want to do now to start with the local database what we need to do is we need to change the env. example file to be EnV now this is going to provide some of the environment variables we need for the database so what we'll do is go over here to our repository and where it says env. example I'm just going to rename this to say EnV and if we go in here you can see that we're going to have a few different configuration options so we have the URI the username the password the olama base URL we'll change some of these in a second but at least now we have the environment variable file declared in the repository now we're going to go back and let's see what it says next it says okay if we want to add this database then we need to copy the following so let's copy all of this stuff right here and let's go over to the docker compose file so composed. yaml and let's simply paste this in okay so that's pasted you can see that we have the environment variable file it's depending on this database being healthy and then we have some information for the database and notice that we're using those various uh variables from our environment variable file you don't have to worry too much about exactly what I'm copying here but this is going to create the database service for us and run it all locally so let's save that file and go back to the docs and see what we need to do next so we can run this app now and if we do that we should see that it's no longer going to ask us for the database information or at least it will already be connected and then the only thing we need next is the llm so I'm just going to rerun this with Docker compose up build so now you'll see that neo4j is running which means the database has been created and if we want we can go back to the application and we can see the state of it now how however what I'm going to do is go to the next step in the documentation which is going to be adding the llm now I want to have a local llm here which means I want to use AMA so let me explain to you how we do that now in order for AMA to work within our Docker container we're going to need it running on our local computer that's because a llama will have some really large models and we wouldn't want those models to be installed every time we spin up the container instead we want to have it on our local computer and then just access the server that's running on our local computer that's kind of serving these llms so download ama if you don't already have it and then what we're going to do is open up our terminal and we're going to run the AMA command to pull the model we're going to use so we're just going to say ol llama and then pull and then llama 2 now I already have this so it ran extremely quickly but you can see it's about 3.8 GB that it needs to download so this will take a second if you don't already have the model again in order for this to work you're going to need to install AMA on your computer go to the terminal type AMA pull and then llama 2 or whatever model it is that you want to use and then we'll be able to utilize this model from our Docker container now one other note here if you are on Windows you might run into some errors so I'm going to leave a link in the description that discusses how you can run this with WSL and you just need to make sure that you have Docker desktop enabled with WSL when you're running these various containers again I'll link this in the description just in case you have any issues but it kind of discusses how you get a llama running on Windows because it sometimes can be a little bit different than working with Mac or Linux okay so now we're going to go back to the documentation here and we're going to go to where it says add a local or remote llm service now you can see that we can run AMA in a container we can run AMA outside of a container or we can use open AI so we're going to go down here and we're actually just going to click on run olama outside of a container and we're going to use the AMA service that's running on our local machines this say install and run olama on your host machine which we are doing and then update the olama base URL value in your EnV file to be the following so so let's copy this value here let's go to our environment variable file which I have open and let's update the olama base URL now while we're here we're also going to change the sentence Transformer to be llama 2 just so we're using the Llama 2 model as well for our embeddings okay let's go back and see if there's anything else that we need to do it says pull the model to a llama using the fullone command we already did that so if you didn't do that go ahead and run it but in my case I did and now we should be able to run our gen application and we should see that all of our services are running locally so let's go back here I'm just going to get out of whatever was running before we'll rerun this and then hopefully our application will be working fine okay so I'm just going to paste in the command here Docker compose up and then D build let's give that a second to run and then let's look at our finished app so the app is running now and what I can do again is upload a PDF so let's go with the same one as before and then chat with it so I'm just going to ask it what is the date and amount for this invoice and let's see if it can get that result all right so there you go go June 24th 2024 the amount is for $62 obviously we can chat with this more if we want but I think that proves the point that this is indeed working so there you go that is how you deploy and containerize a gen application using Docker now the last thing I want to show you here is a really interesting feature that can save you a ton of time when you actually are going into production and that's something known as Docker Scout now in order to use this feature we need to open up Docker desktop which I already have right here and we're going to go into our images here and you can see that what we can actually do is we can click on one of them so let's go to the Gen stack loader for example and right away it's going to open up this vulnerabilities tab now this is a way that we can actually detect all of the known vulnerabilities in the different images that we're using as well as the various dependencies and not only that we can actually see the solution or the version we should go to to fix these vulnerabilities so you can see on the right hand side of my screen here that actually shows me various different packages which are dependencies for this image and it shows me all of the different vulnerabilities that they have ranking by critical high medium or low priority now I can actually sort by fixable packages if I just want to see the ones that do actually have a fix but if you want you can click in you can see exactly where the vulnerabilities are and then it actually gives you a description of what it is and shows you if applicable what the fixed version is now in this case there's no fix for this one but if we go to this package here cryptography we can go to this one and you can see that will tell us that the affected range is this and if we go up to 42.0 point4 then that'll actually fix this vulnerability so it's super fast and very easy to actually see the fix which is the useful component of this because it's one thing to know the vulnerabilities it's another thing to really quickly be able to find what the solution is now this Docker Scout feature works by analyzing all of the different images and dependencies that you have and then matching it against an inventory of ever updating vulnerabilities so it already has all of the fixes all of the vulnerabilities they pull that from so many different places and they take all of your images and just match it against that which allows them to really quickly give you all of these different recommendations and show you where the vulnerabilities are a super useful thing and definitely something you want to check out especially if you're going to go into production and it's just built in here right to Docker desktop so with that said guys I am going to wrap up the video here all of the relevant documentation for this will be linked in the description if you have any questions leave them in the comments down below I hope you found this helpful and I look forward to seeing you in another video