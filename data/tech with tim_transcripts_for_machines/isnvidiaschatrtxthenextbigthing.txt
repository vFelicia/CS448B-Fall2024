Nvidia just released their chat RTX application which is pretty impressive and solves the two biggest issues with llms today issue number one data access now sure I can copy in the contents of a file or an email I can upload a few documents to a model but it starts to get tricky when I want it to have access to hundreds of different documents or my entire operating system or entire directories on my computer and even if I could give a model access to all this information would I want to do that would I trust a company with all my private and sensitive data that's where issue number two comes in privacy we really cannot trust these Central companies we don't know what they're doing with our data if they're training on it if they're storing it and I don't know about you but I don't want random companies having access to my personal information but at the same time it would be super cool if I had a model that could answer questions about my personal data read through my files go through sensitive documents and do that in a secure way where I can still get information really quickly but I don't have to be worried about this being sent off or sold to some company now fortunately for us nvidia's chat RTX application pretty much solves all of these problems by running a free opensource model locally on your computer which has access to all of your different files that means I can ask it questions about my information get responses back extremely fast and know that all of my data is secure because it's not connected to the cloud a big thank you to Nvidia for sponsoring this video and let me share with you a quick demo of how this tool works so this is what the application looks like it's it's pretty straightforward and this is just running locally in my browser and I'll just quickly walk you through how it works so you can see that we can select a model that we want to use so mistol llama 2 we also could install other open source models if we want and then we have a data set section now the data set section is the cool part here where I can actually select a directory on my computer that I want to give the model access to now what this will use is something known as rag which is retrieval augmented generation going to talk more about that later in the video but the idea is is you can give the model some context about different files on your computer and this is all secure running locally and it's super fast and it can look up information find different things inside of for example PDF documents text documents or Microsoft Word documents and then immediately give you the result so let's test something out here I'll say can you give me a summary of my script related to micro services so this directory has a bunch of different files one of them is a microservices script for my YouTube channel and you can see that what it does here is it finds this document gives me a reference to it that I can click on and then it actually gives me a quick uh summary of that script so this is super cool and you can use this to really quickly look up different information and to have access to all of these different files within the model so let's imagine you had a directory with your flight itinerary and a bunch of different plans and maybe someone asks you hey what are you doing on Thursday rather than having to go through there and look for the information just say hey what am I doing on Thursday based on my flight plans or based on my itinerary point it to that directory and you instantly get that information this is super super cool I'll type in a few other examples here so you can get a sense of how it works so I just selected another folder here and this folder actually contains a bunch of accounting information for my business so invoices receipts tax information Etc now this is highly sensitive data that I wouldn't want to share with other people but that I might want to quickly get some information about without having to go search through all of the different files so just show you an example of doing that here and obviously blur anything out that's going to be too sensitive so I can say can you give me some information about my invoices to course careers I've done some development work for course careers it's the company that I have my software development course with and you'll see that it starts going through here and listing all of the different invoices that I've sent them giving me the information about it I kind of need to blur some of this out cuz I can't show you guys everything that I've done for them uh but yeah it kind of walks through how that works right invoice number 128 invoice number 115 and this is information that would usually take me at least a few minutes to find and it's quite cool that it can just really quickly look it up and share it with me in a quick summary view here so I just asked another style of question here can you find all of the receipts related to video editing obviously it was able to do that it did that super fast by the way as well that's the number one thing I've noticed with this it happens almost instantly I guess that's because it's running locally with a lot of other models I've used they're quite slow especially the ones that are hosted online I'm being R limited I'm waiting on I don't know a bunch of other people to get their replies and sometimes I sit there for 2 3 4 minutes waiting for a long reply to be generated whereas this it seems to happen almost instantaneously I love this for looking up information and I can already think of a bunch of different ways I could use this to really speed up my workflow especially when I'm trying to find different documents or files which for me is a bit difficult to do especially if I don't know like the file name or a specific way to search it here I can just use some natural language which is not something I'd be able to do in like the windows search bar right so now that we've looked at a quick demo let's talk about how you can start using this on your own computer and trust me it's quite a bit cooler when this works on your own machine it answers questions about your own data and files rather than seeing it work on mine anyways it is a free download you can get it from the link in the description just keep in mind it is demo and there are some pretty steep system requirements so let's get into those so first of all the platform is Windows it's not going to work on Mac or Linux and you do need to have an RTX 30 or 40 series GPU with a minimum of 8 GB of vram now that's because this is using RTX features to actually work properly so you do need a newer GPU in order for it to work now you're also going to need 16 GB of normal RAM and you're going to have to have 35 GB of dis space this download is 35 GB it took me an hour to download it and then going through the installer took almost 30 minutes where it downloaded and installed a bunch of other stuff as well so realistically you're probably going to need even more than that maybe 60 70 80 GB of space on your computer that said it is pretty cool and obviously it's all running locally so that's kind of the price you have to pay right if you want ultimate privacy and security you're going to have to run on your own computer which means you need a pretty insane computer with pretty highend specs in order for this to actually work in my case I'm using a 4090 with 64 GB of RAM and it's working extremely fast so with all of that said let's quickly talk about how an application like this works now the first thing to understand is that behind the scenes we're really just using an open source llm now the llms that come with this by default are mistol and llama 2 llama 2 is the one provided by meta open source just means it's free we can use it however we want we can modify it we can finetune it different than a closed Source llm where we don't actually have access to the code now that's one of the reasons why the file size is so large here because we're actually downloading these llms and we're running all of their code on our local computer now what we do with these llms is we add additional data to them using something known as rag now before we dive into rag let's understand how a normal llm would work so if I just go to a normal llm and I ask it a question it can generate a pretty good response for me but it can only give me data based on what it was trained on so if I try to ask it you know what's the most recent YouTube video that Tech with Tim posted it's not going to be able to tell me that because it wasn't trained on that information it doesn't have access to that data it does have access to a bunch of historical facts it knows how to generate replies it can write code for me it could do a bunch of awesome stuff but when it comes to data availability and knowing kind of uptodate or recent information that's where it really struggles so that's why we have something called rag rag is retrieval augmented generation and all this does is ADD information into the llm that it can then reason based on it's just like taking the relevant information giving it to the llm and saying Hey Now use all of this new data and generate replies so obviously that's going to give us better more contextually relevant replies but there are some issues when it comes to giving so much data to an llm so we can't actually just give the llm all of the information we want it to work on we have to be a little bit more sophisticated and inject the right pieces of information with the prompt or in whatever way we're going to end up doing this so that it gives us the right reply so this is where the next stage comes in which is actually retrieving the information so what we'll first do kind of the first step when we ask a question to a rag application is we're going to go to something known as a vector store database and we're going to look up information relevant to the prompt that we've provided so we'll type in some prompt we'll ask some question we then will go to a vector store database which I'll talk about in a second and we'll pretty much look up information related to that prompt we'll then get Snippets of information from our data set we'll take that we'll provide that to the llm and we'll pretty much tell the llm hey now use this data and answer this prompt then it will do that and based on the data we've given it it will give us a better more contextually relevant reply as you saw when I was in that demo now how does a vector store database work well I don't want to get into this too much because it's a bit complicated but the first step is that we're going to take our entire data set so in this case all of our files and we're going to create something known as a vector embedding for them now a vector embedding is a representation in multidimensional space of the meaning and the context of a piece of data now what we do is we take all of our data we vectorize it we put it in this database and this gives us a really really fast efficient way to find different pieces of data so rather than searching through the entire database looking for every single piece of information that may be relevant we search using vectors again we don't need to get into the complexities and we can really quickly find the relevant information that we want and give that to the model if you're curious you can read more about this and you can learn about Vector databases but the idea is the first step is take all of our data create these Vector embeddings which can take a little bit of time to do we're going to store those vector embeddings and then we're going to use them when we actually want to ask questions about our data so you'll see if you start running this application on your own computer when you load in a new folder it's first going to vectorize that data so that we can really quickly look up the data and that's why we get such rapid replies so the process prompt find the data you want from the database inject it into the model and then get an accurate reply based on that data so that is my really high level overview of how this application works I hope it was helpful and gave you a little bit of insight into what rag is and how it's use with these llms that's going to wrap up this video I encourage you to download chat RTX from the link in the description and play around with it it's really really cool and I look forward to seeing you in another YouTube video