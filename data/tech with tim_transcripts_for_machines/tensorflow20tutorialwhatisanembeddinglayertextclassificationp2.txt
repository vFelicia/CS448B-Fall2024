hey guys so now it's time to talk about word embeddings and this embedding layer and then what the global average pooling one D layer is doing now we already have an idea of what these dense layers are with these activation functions like real ooh and sigmoid but what we're actually gonna do today or I guess just in this video is talking about the architecture of this network kind of how it works on a high level understanding and then in the next video we'll do is actually get into training and using the network so what I'm gonna do first is just start by talking about these first two layers and specifically what this embedding layer is because it's very important and then we will draw the whole network or the whole I guess network is right we're way to put it the whole architecture and talk about how it fits together and what it's actually doing so let's get started now the easiest way to kind of explain this is to use an example of two very similar sentences so I'm just gonna say the first sentence is have a great day and the next sentence will be have a good day now I know my handwriting is horrible so just give me a break on that it's also hard to kind of write with this tablet so that's my excuse but anyways these two sentences looking at them as human beings we can tell pretty quickly that they're very similar now yes great and good maybe one has more emphasis on having an amazing day whatever it is but they're very similar and they pretty well have the same meaning right maybe we know when we would use the sentence in kind of the context in which like these words great and good are used and day and day and all this right it just we understand what they are now the computer doesn't have that same understanding at least right off the bat when looking at these two sentences now in our case we've actually integer encoded all of our different values so what we end up having are all of our different words sorry is our sentences end up looking something like this so we're gonna have this first word will represent a zero a will be one great will be two and day will be 3 so then down here we'll have 0 1 in this case we're gonna say good is 4 and day is 3 as well so this means if we integer encode these sentences we have some lists that look something like this now this one clearly is the first sentence and this one down here will be second sentence now if we just look at this and we pretend that you know we don't even know what these words actually are all we can really tell is the fact that two is different from four now notice what I just said there two is different from four when in reality if we look at these two words we know that they're pretty similar yes they're different words yes they're different lengths whatever it is but we know that they have a similar meaning and the context in which they're used in this sentence is the same now our computer obviously doesn't know that because all it gets to see is this so what we want to do is try to get it to have an understanding of words that have similar meanings and to kind of group those together in a similar form or in a similar way because obviously in our application here of classifying movie reviews the types of words that are used in the context in which they are used really makes a massive difference to trying to classify that as either a positive or a negative review and if we look at great and good and we say that these are two completely different words well that's gonna be a bit of an issue when we're trying to do some classifications so this is where our embedding layer comes in now again just to say here one more time like we know these are different but we also would know for example say if we replace this four with a three well all our computer again would know is that two is different from three just like four is different from two it doesn't know how different they are and that's what I'm trying to get at here is our bedding layer is going to try to group words in a similar kind of way so that we know which ones are similar to each other so let me now talk about specifically the embedding layer so let me just draw a little grid here now what our embedding layer actually does kind of like I don't want to say the formal definition but the more mathy definition is it finds word vectors for each word that we pass it or it generates word vectors and uses those word vectors to pass to the future layers now a word vector can be in any kind of dimensional space now in this case we've picked 16 dimensions for each word vector which means that we're gonna have vectors may be something like this and a vector again it's just a straight line with a bunch of different coefficients in some kind of space that is in this case sixteen dimensions so let's pretend that this is a sixteen dimensional vector and this is the word vector for the word half now in our computer it wouldn't actually be have it would be zero because again we have integer encoded stuff but you kind of you get the point so we'll say this is the word vector for half now what we're gonna do immediately when we create this embedding layer is let me I should get out of this quickly for one second is we initially create 10,000 word factors for every single word and in this case every single number that represents a word so what we're gonna do is when we start creating this embedding layer we see that we've have an embedding layer is we're gonna draw 10,000 word vectors and just kind of some random way that are just there and each one represents one word and what happens when we call the embedding layer is it's gonna grab all of those word vectors for whatever input we have and use that as the data that we pass on to the next layer now how do we create these word vectors and how do we group words well this is where it gets into a bit complicated math I'm not really gonna go through any equations or anything like that but I'll kind of give you an idea of how we do it now we want to so let me get rid of this word have because this is not the best word vector example and let's say that this word vector is great now upon creating our word vector our embedding layer we have two vectors we have great and we have good and we can see that these vectors are kind of far apart from each other and we determine that by looking at the angle between them and we say that this angle maybe it's like I don't know 70 degrees or something like that and we can kind of determine that great and good are not that close to each other but in reality we want them to be pretty close to each other we want the computer to look at great and good and be like these are similar words let's treat them similarly in our neural network so what we want to do hopefully is have these words and these vectors kind of move closer together whether it's good going all the way to great or great going all the way to good or vice versa right we just want them to get close together and kind of be in some form of a group so what we do is we try to look at the context in which these words are used rather than just the content of the words which would just be what this looks like want to figure out how they how they're used so we'll look at the words around it and determine that you know when we have a and day and a and day maybe that means that these are like related in some way and then we'll try to group these words now it's way more complicated than that don't get me wrong but it's kind of like a very basic way of how they group together is we look at the words that surround it and just different properties of the sentence involving that word and then we can kind of get an idea of where these words go and which ones are close to each other so maybe after we've done some training what happens is our word embeddings or what is known as learns just like we're learning and teaching our neural network and we get we end up getting great and good very close together and these are what their word vector representations are we can tell them to close again by looking at the angle in between here maybe it's like 0.2 degrees and what that means is these two vectors which are just a bunch of numbers essentially are very close together so when we feed them into our neural network they should hopefully give us a similar output at least for that specific neuron that we give it to now I know this might be a little bit confusing but I'm gonna go we're gonna talk about this a bit more with another drawing of the whole network but I hope you're getting the idea the whole point of this embedding layer is to make word vectors that and then group those word vectors or kind of like make them close together based on words that are similar and that are different so again just like we would have grading good here we would hope that a word vector like bad would be down here where it has a big difference from great and good so that we can tell that these words are not related whatsoever all right so that's how the embedding layer works now what ends up happening when we have this embedding layer is we get an output dimension of what's known as 16 dimensions and that's just how many coefficients essentially we have for our vector so just like if you have a 2d line so like if this our grid in 2d and we say that this is X and this is y we can represent any line by just having like some values like ax plus B y equals C now this is the exact same thing that we can do in in n dimensions which means like any amount of dimensions so for a 16 dimensional line I'm not gonna draw them all but we would start with like ax plus B Y plus cz plus DW and so on and we would just have again 16 of these coefficients and then some kind of constant value maybe we call it lambda that is like what it's what it equals to what the equation equals to and that's how we define a line I'm pretty sure I'm doing this correctly in in n dimensions so anyways once we create that line what we actually want to do is we want to scale the dimension down a little bit now that's just because 16 dimensions is a lot of data especially when we have like a ton of different words coming into our network we want to scale it down to make it a little bit easier to actually compute and to train our network so that's where this global average pooling 1d layer comes in now I'm not gonna talk about this in two depth in too much depth but essentially the way to think of the global average pooling 1d is that it just takes whatever dimension our data is in and just puts it in a lower dimension now there's a specific way that it does that but again I'm not gonna talk about that and it's not super important if you care about that a lot just look it up and it's not like crazy hard but I just I don't feel the need to go into it in this video so anyways let's now start drawing what our network actually looks like after understanding how this embedding layer works so we're gonna initially feed in a sequence and we'll just say that this is like our sequence of encoded words okay so say this is our input and maybe it's something like zero seven nine like a thousand two hundred a thousand twenty we have like nine again maybe we have eight just a bunch of different essentially numbers right so we're gonna pass this into our embedding layer and all this is gonna do is it's gonna find the representation of these words in our embedding layer so maybe are embedding layer well it's gonna have the same amount of words in our vocabulary so to look up say 0 it'll say maybe 0 means 0 is vector is like 0.2 0.3 and it goes to 16 dimensions but I'm just gonna do like 2 for this example here maybe 7 its vector is like 7 and 9.0 and it just keeps going like this and it looks up all these vectors so it takes all of our input data and it just turns them into a bunch of vectors and just spits those out into our next layer now our next layer what this does is it just takes these vectors and just averages them out and it just means it kind of shrinks them there down it down so we'll do like a little smaller thing here and we'll just say like average okay so I'll call this one embedding and that one is average now this average layer now is where we go into the actual neural network well obviously this is a neural network but we go into the dense layers which will actually perform our classification so what we're gonna do is we're gonna start with 16 neurons and this is just again an arbitrary number that we picked for our network you can mess around with different values for this and I encourage you to do that but 16 is what tensorflow decided to use and what I'm just following along with so we're gonna have 16 neurons and we're gonna pass all of our now 16 dimensional data or whatever dimensional data it is into these neurons like this now this is where we start I'm doing the dense layer so we have this dense layer and this is connected to one output neuron like this so what we end up having is this embedding layer which is gonna have all these word vectors that represent different words we average them out we pass them into this 16 neuron layer that then goes into an output layer which will spit out a value between 0 and 1 using the sigmoid function which I believe I have to correct myself because in other videos I said it did between negative 1 and 1 it just takes any value we have and puts it in between 0 and 1 like that all right so that is kind of how our network works so let me talk about what this dense layer is doing just a little bit before we move on to the next video so what this dense layer is going to attempt to do essentially is look for patterns of words and try to classify them using the same methods we talked about before into either a positive review or a negative review I'm gonna take all these word vectors which again are gonna be like similarly grouped words like great good are gonna be similar input to this dense layer right because we've averaged them out and embedded them in all this and then what we're gonna do is we're gonna try to determine based on what words we have and what they come in what our text is and we hope that this layer of 16 neurons is able to pick up on patterns of certain words and where they occur in the sentence and give us a accurate classification again it's gonna do that by tweaking and modifying these weights and all of the biases that are on you know all of these different whatdoyoucallit layers or all of these connections or whatever they are and then it's gonna give us some output and some level of accuracy for our network so i hope that i explained that decently in terms of how the word embeddings work again i always encourage you guys to ask me questions in the comments below if you know the answer or maybe I didn't explain something clearly enough please do me a favor go down and help me out explain it to people that maybe don't understand it or make sure that I'm you know not butchering any of these explanations because just like you guys I'm learning as well and this is what I feel confident in and this what I think everything is but I can always make mistakes just like everyone else so anyways that has been it for this video I hope you guys have a little bit of an understanding on when we would use a word embedding layer how the average pooling works and how this network is kind of going to react in the next video when we start training