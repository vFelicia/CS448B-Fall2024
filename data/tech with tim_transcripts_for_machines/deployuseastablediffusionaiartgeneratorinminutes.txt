foreign tool that allows you to test experiment and deploy machine learning models in the matter of minutes that's right within a few minutes and a few terminal commands you can have this up and running on your machine and server and allow people in your organization your team your friends whoever it may be to mess around with your machine learning models this is also quite useful if you want to deploy already trained models and just see how they work on your own local hardware so with that said let me give you a quick demo of how all of this works then I'm going to explain to you more about the tool kind of what it is how it works and we'll go from there alright so in front of me I have two WSL terminals open on my Windows machine that are running Ubuntu 20.0.4 now the one on the left is running the server and the one on the right is going to be running the client now the point of this demonstration is to illustrate how simple it can be to interact with your deployed machine learning models after I give you this demo then I'm going to walk a little bit through the code and of course I'm going to elaborate on the tool now the tool that we're using framework live Library whatever you want to call it is called Pi Triton now this comes from Nvidia this is a wrapper around their Triton inference server which is pretty much the state of the art when it comes to deploying testing and working with machine learning models this is used by all kinds of different companies Pi Trident is relatively new Nvidia has kind of giving me a brief and I'm teaming up with them for this video because I wanted to make you guys aware of this obviously it's completely free all of the information you need will be available in the description but let's have a quick look at this demo so on the left I've got the Trident inference server running and this is actually going to kind of wrap or allow me to use what's known as a stable diffusion model this model essentially takes text input prompts and then generates some kind of image now on the right side of my screen it's a bit small but you can see down here it says what image would you like to be created this is my client script that's going to interact with this server so on this terminal I'm just going to type a dog in a park okay and hit enter it's going to ask me what file name I would like to save this image as I'm just going to go with test dot PNG and then it's going to take a second here you're going to see that the request will be sent over here to my server where it's going to be executed on my GPU and in a few seconds this will be finished it will save the image and I'll show you what the image looks like alright so the image has now been saved I'm just going to open it up so you guys can see what I get here so this is the image that was generated a dog in the park again this is using a stable diffusion machine learning model which is really a pipeline of multiple models that create this image now just to demonstrate the capabilities of a model like this I'll do a more complicated prompt so something like an alien riding a horse in a desert okay so let's save this as alien Dot PNG let's give that a second and then have a look at the result alright so here we are this is the resulting image I don't know if this is quite an alien but it is riding a horse in the desert so pretty good and obviously you can mess with this and ask it to create all kinds of different images the point here was just to demonstrate how simple it was for me to actually run this program so now that I've given you this quick demo I want to briefly discuss what's typically involved in deploying a machine learning model so you can actually see the benefit of using a tool like Pi Trident alright so I've just opened up the pi Trident GitHub page to go through some of the issues you might encounter if you're trying to deploy a machine learning model on your own now first of all you might use something like flask right this is a pretty popular web framework it's lightweight it's easy to use easy to learn the issue with using something like that for a machine learning deployment is that you're not going to have a lot of the features that you want prebuilt into the flask application you're going to have to write a lot of custom code if you want this to be scalable you're going to have to implement features like Dynamic batching you're going to run into a whole host of issues that's going to take you significant amount of time to actually get a scalable deployment just looking at this diagram here you can see that some of the features you might want in your flask application is dynamic batching managing models supporting different Hardware environments managing multiple Frameworks like Pi torch tensorflow Etc having multiple gpus running model versioning runtime optimization there's a million things that you would have to write on your own and it's just really not time effective to do that so you can continue to read through here and I'm sure if any of you have attempted to deploy machine learning model you can relate to some of these struggles but this is why you would use something like the pi Trident inference server so now it's time to discuss what pi Trident is well Pi Trident is really the python wrapper around nvidia's Trident inference server it provides a flask slash fast API like interface that makes it very easy for you to essentially bind a function to an API endpoint that can then serve machine learning results in a machine learning model I'm going to show you a code example in a minute but essentially Pi try implements all of those features that you would want in your own flask server for you so that means you can spend as little time as possible deploying your machine learning model and just use all of these builtin features and functionality this is regularly optimized it works on pretty much any hardware platform it supports all of your major machine learning models and platforms or development kits like Pi torch tensorflow openvino SK learn name on it probably supports that obviously it's very optimized it allows your parallelism between your CPU GPU and just tons of features that honestly I'm not even qualified to talk about and for anyone that does work with machine learning models I'd highly recommend checking it out even for myself who's not a machine learning engineer or a machine learning expert I found it relatively simple once I got the setup done to actually be able to deploy different machine learning models and for me it was cool because I could actually test out all of these interesting models on my own hardware and my own machine with relative ease so lastly I'll just mention here that in addition to all of the features provided by pi Trident this really is just easy to use it's going to be cost effective and scalable it's supported by Nvidia and this is something that works with Docker kubernetes Etc so it really is truly scalable it's going to make your life a lot easier compared to trying to work in flask now I know it kind of sound like a salesman here but the reality is all of this is free you don't need to pay for this I don't even know how you would pay for something like this so that's kind of why I'm hyping it up because I know this is going to help a lot of you guys so I want to encourage you to check it out if you are in the machine learning AI space anyways let's do a quick code demo here so you guys can see how easy it actually is I don't want to just keep talking about it I want to want to show you myself so again remember on the left side I have the server on the right side I have my client uh rather than showing you the code here in my terminal I'll just show it to you in Visual Studio code it's a little bit of a better interface so let's start with the server which is the thing that's probably most important to you guys so keep in mind all of this code here is what allowed me to deploy the stable diffusion pipeline which I used to generate those images at the beginning of the video so I import my libraries or my modules torch diffusers this is what gives me access to the stable diffusion pipeline I need some things from PI Trident like my model configuration tensor Trident batch Etc and then I import numpy as NP now up here I just Define the model that I want to use now there's all kinds of different models you can use here you can use hugging face models you can use your own custom pie torch models you can use tensorflow models there's all kinds of documentation on how to do this I'm not going to get into that in this video and then I Define the type the revision and then I'm just specifying that I want to utilize my GPU so I'm going to Cuda then I have a little helper function here converting something to a numpy array for me and then I have the function that I'm actually essentially serving using pytrip so this is all I need to actually use the stable diffusion pipeline I have my prompt so I'm just grabbing the kind of prompt field from my input I am then going to do a little bit of kind of numpy version here I'm not going to talk about exactly what I'm doing here and then I essentially passed that to my stable diffusion pipeline I specify the height and the width of the image that I want I grab all of my images and then I essentially return those images in an array so this function is what I'm going to bind down here to the API endpoint that I'm calling text to image so what that means is that I've essentially said Okay I want to have an endpoint called text image I want the inference function to be generate image I want my input structure to look like this my output structure to look like this and then this is the basic configuration for my model where I'm having the match maximum sorry batch input size of 8. that means we can batch up to eight inputs together pass them to the model so that way we can have the best scalability and performance obviously we could batch more less Etc but the point of this is that this is literally all I need to deploy this model I didn't need to set up my own flask server I initialized Trident I use this bind method here I pass the name of the endpoint I pass the function that I want I give some basic information like the input and shape of the input and then the output and the shape of the output the configuration and then I run the inference server and all of a sudden I now have a live endpoint that I can call and as long as I pass the correct input I get my machine learning output which in this case is going to be an array that represents an image that's it so there you go now if I go to client.pi I just wrote a kind of a nice client here so that I can interact with it this could be five lines of code if I wanted it to be the idea is I'm importing uh pill I'm importing numpy I'm importing the pi Trident client I then have my generate image function here I create a prompt array the reason I'm doing this is that you can pass multiple prompts to this model so you can have it generate like 10 images at once you don't always have to do one hence why I'm kind of putting this in an array then I'm saying with model I'm doing this on localhost the name of the endpoint is text image I can have multiple endpoints that I deployed if I wanted to and I'm saying that my result dictionary is equal to client dot in first sample and then the sample that I'm using here is my prompts so that's essentially the input I'm passing to it I then create a pill image from the numpy array that is returned to me from this function I save that image and then I'm able to view it the rest of this code here is just kind of giving me a nice interface so I can type in and give the file name game and all of that but that's it like that's all you need to do to deploy a model and if you wanted to use a different model then you would just change the pipeline up here you'd probably have to do a little bit of preprocessing for your inputs depending on what the structure and shape should be and then you're off to the races that's all you need to do to get them going so just to give you another example or just to show you I guess how this works one last time let me spin up the server one more time for you so I've just shut it down here and we can spin it back up again so I'm going to type python3 server.pi this contains the same code you just saw in vs code let's clear this one and then start up our client and we'll just give this a second to boot up and then we can use the client and generate the images again all of the code you just saw is what's running on the left and the right hand side of my screen all right so these Services now started I'm just going to ask this to generate an image so let's say that we want a snail attacking a pig that is riding a cat all right let's see if it can actually give us that I'll just name this pig dot PNG I don't know how good the stable diffusion model is but so far it's been pretty good to me let's wait for this to finish I'm going to open up the image then I'll show you what we get all right so there we go that's kind of a weird image that was generated here it's kind of like a mixture of a snail and a pig together I don't see a cat anywhere I think I put cat here right yeah riding a cat but either way we we got something that was kind of relevant uh to what it is that we input it so there you go that is mostly what I wanted to show you in this video now I'll just give you a quick notes related to kind of getting this running on your own machine uh the installation and setup is pretty straightforward you do need to be running some kind of Linux distribution that's why I'm in WSL here on Windows the WSL version that worked best for me is uh Ubuntu 20.0.4 once you're inside of that there's a few basic things you need to install and set up but what you can do is just run an Nvidia Docker container all of this documentation is available from the links that I'm going to leave in the description and it pretty much installs everything that you need for you automatically once you're inside of the docker container you just need to install the PIP library for the Nvidia uh what do you call it Pi Trident service or server whatever you want to call it so you would just do something like pip install and then pi and then Nvidia if we could type this correctly Pi Triton and then any other Library they're going to be using like Pi torch diffusers Transformers Etc you just install all of your python dependencies like you normally would and then this is going to work now this runs on python version 3.8 so just keep that in mind and that's really all you need to know now getting it set up on WSL can be a little bit challenging depending on how you have your windows configured what I found was the easiest to do was to install Docker desktop and then from Docker desktop there's a little setting you can click that says enable w USL like configuration or back end or something like that if you do that then all of a sudden it's going to give you access to the docker command inside of your WSL and then all you need to do is pull the docker image again all this is available from the Nvidia GitHub once you're inside of that container you simply create whatever python code it is that you want to run install the different Python dependencies and then you're pretty much good to go obviously there's a lot further you can go with this but that's really all you need to know if you want to get things set up again follow all the links and documentation I'm also going to have a GitHub that contains all of the code you just saw in this video so if you're interested in checking that out and kind of messing with that demo yourself feel free to do that with that said guys I think I'm going to wrap up the video here a big thanks to Nvidia for partnering with me on this video I know that this was really just showcasing their product however this is something that's quite cool when they first showed it to me and gave me kind of an exclusive demo I thought it was really interesting and valuable and that you guys would find some value in it hence why I'm making this video If you enjoyed make sure to leave a like subscribe the channel and I will see you in another one