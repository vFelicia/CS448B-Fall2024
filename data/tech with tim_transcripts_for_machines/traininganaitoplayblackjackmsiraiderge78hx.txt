some of my favorite projects involve training in AI how to play a game I've done this for Flappy Bird chess pong Checkers but today I'm taking on Blackjack now this is an interesting game because we already know how to play it optimally so I'm interested to see if the AI can learn the rules on its own I'm not going to hardcode it I'm going to see if it can play enough games to figure out the optimal strategy and to even beat the casino now by no means am I encouraging gambling here this is just a fun game and the math behind all these casino games has always been interesting to me in this video I'm going to work on this project completely from scratch and I'm going to bring you along for the ride with that said let's dive in so first things first I got to figure out what to do I know there's a bunch of different approaches and right now I'm thinking that I could use something like neat which is neuro evolution of augment topologies I could try to build a neural network or I could go for something like reinforcement learning I'm going to do a bit of research consult the good old chat GPT see what it tells me and report back so I've been doing a bit of research here let me share with you what I found so I asked chat gbt I want to train in AI how to play Blackjack what's the best way to do that these are kind of the methods I know it walked through a bunch of them I'm not going to bore you with all the results but pretty much what I was thinking is somewhat correct we can use a neural network but that's a little bit complex and we're going to kind of mix that in with something known as Q learning now if we go down here Q learning is probably the most basic approach that we can use and what this essentially means is we're going to generate a massive State table that has every single possible result or every single state that we could have in blackj just like those really popular cards you see where it shows your card value or your hand total versus the dealer card and what kind of action you should take so I asked chat gbt explain a bit more about Q learning just so I could kind of refresh my memory on how it worked and and then I asked it to give me a bit of a visualization just so that I could show you guys and I can see for myself exactly what it was talking about here you can see that we have a pretty simple table where we have the sum of our hand we have the Dealer's up card we have if we have a usable Ace or not and then we have the action that we'll take which is hit or stand I think I'm going to start with just hit or stand and then as we get a bit more complex I'm going to start adding in doubling splitting surrendering and a lot of the other more advanced rules in blackjack now what's going to happen is we're we're going to initialize this table with all of these different states and we're going to assign them something known as a q value now the Q value is the predicted result or the predicted outcome based on a specific action from the state before so what we'll do is we'll say hit a 12 versus a two when we hit that we'll either win or lose depending on the result we'll update the Q value and we'll continue to do this Millions upon millions of times eventually once we played enough hands of blackjack the computer will start to figure out which combinations or which states should take what specific action it'll do that by looking at the Q value so later we'll just have this massive table that will have all of the different combinations all of the actions we can take and the expected result for those actions and we'll just pick the action the results in the highest estimated Q value now this does involve a lot of iterations and that's actually why I need a pretty powerful computer for this project now fortunately I teamed up with MSI for this video and they sent me over the Raider g78 hx1 13v which is an absolute Beast for my deep learning workflow this laptop has a 13th gen I9 24 core processor an RTX 490 32 GB of RAM and a 240 HZ 17in QHD display having this laptop feels like carrying around a mobile power horse where I can literally do anything I want with it even if I'm sitting in a coffee shop I can load and train massive models using nvidia's Cuda and this RTX 490 now being a power user this is exactly what I need something that gives me the confidence to know that no matter what my workflow is it can handle it in fact pretty soon I'm actually going to be moving overseas and I can't bring my big desktop PC with me now at first I was a bit worried but now that I have this laptop and it can fit right inside my backpack I know that no matter where I am I'll be able to handle any task and complete any project and that means that literally when I'm on a plane I could be trained a machine learning model and getting the same kind of productivity and work done that I would with my big PC now not to mention when I do eventually set this up in some sort of office I can connect multiple monitors and any peripherals I need because of it's HDMI 2.1 Thunderbolt 4 ethernet SD card reader Etc I don't even have to bring dongles with me now as I transition to a bit more of a nomatic life this is exactly the type of machine I need something where I have complete confidence in its ability it has all of the Power I could ever want and it means I can get any task done editing gaming deep learning doesn't matter I have the machine that can do that now if you guys want to check out more I'll leave some links in the description this thing is insane and it's got a ton of other features I didn't even touch on so it's been about 30 minutes and I realized that before I can actually start doing any training I need to have a functioning game of blackjack so I just built out a custom blackjack game here that allows you to play against the computer we're going to use this with our model to determine whether or not we're winning losing Etc and sorry not model I mean kind of our Q learning Q table whatever you want to call it anyways if you have a look here on the computer I'll just quickly run through the code you can see I just created a simple class so it be nice and reusable I have all of the different symbols here these are kind of the Unicode symbols for heart spade diamond Etc just to make it look a bit better we generate the deck randomly deal the cards start the game and then we have some methods for figuring out the hand value it's actually not that simple in blackjack because of the aces we have a method for the player action so hitting staying Etc dealer action getting the status of the game so player bust player Blackjack Etc we have the game result and then we have a method for formatting the cards then just a bit of driver code here to actually run it so I'll show you quickly what I have is that if I run the code here it shows what the dealer has and what you have for right now it's just hidden stay no doubling or splitting so so let's hit I get a 15 they have a three so I'm going to stay and the dealer hits and has three 10 and a queen and I win let's play that one more time uh I have a three against a dealer three I'm going to stay and they get 17 and we lose that's usually how it goes anyways I'll be back once I start implementing the Q learning I am back it's been about an hour and a half I've messed around with a few different things here and I've got a somewhat working version here of of the Q table now you can see that I've just tested this on 10,000 games and I trained the Q table on 50,000 games now of those 10,000 games from that 50,000 game kind of training sample I have a win rate or number of wins which is about 4,000 about 5,000 losses and 870 draws and that gives me about a 43% win rate the reason why it's 43% is I'm just dividing it out the total wins and losses is kind of avoiding the draws not sure if that's really how you should do the calculation but that's what I've got if we have a look here we can see that this is already making pretty reasonable choices based on blackjack basic strategy now if it was perfect this win rate should be closer to 48.5 49% depending on the rule set but it should be higher than it is right now so if we have a look at a few games here we can see that the dealer showing a six we have a 10 we hit to 17 the dealer has 13 the dealer hits and then we actually end up losing now I don't show all of the cards the dealer has when they're hitting I'm just trying to show the player decision here the AI decision in this case dealer shows an ace we have a 13 we hit to 20 we stay unfortunately the dealer has a black check and we lose now let's see dealer has a five we're showing a five we decide to stay and we end up winning I assume the dealer busted here or they had less well I guess they had to bust right cuz we had a 15 continuing dealer shows a two we have a 12 we actually decide to hit which I thought was interesting and we bust and we lose now this one is where we're kind of making some mistakes right dealer shows a two we have a 14 we're hitting to 15 we have a 15 we're hitting again we're on 17 we're hitting again and we're getting 25 so obviously it's not perfect and you can see that it does make some mistakes especially on some of those lower cards like two and three so I'm going to try to tweak this and see if it can get closer to basic strategy so overall pretty decent progress and I almost argue this plays better Blackjack than the most average people all right so it's been about an hour and a half here I've tried a bunch of different things made some modifications to the code and most notably what I've done with the model is really increased the sample size and made it count for soft and hard hands so previously it didn't know the difference between a soft 16 or a hard 16 or whatever soft and hard hand so it was making a lot of weird choices when we had aces now I believe I fixed that problem but I think by doing that I may have introduced some others now really all that to say after doing all of this work I'm getting pretty much the exact same result kind of the story of my life when it comes to machine learning change a bunch of things mess with a bunch of parameters and not being an absolute expert I just get the same or Worse result I'm sure many of you can probably relate to that anyways I'm getting again about a 43% win rate here but this time it's on 5,000 games that was the training uh kind of data set and then 1 million games is the testing data set for my wins losses and draws now keep in mind I'm not accounting for the fact that blackjacks pay 3 to2 I don't have rules like double down surrender and split which would increase the player odds if I had those rules I'd probably be doing a lot better but also it would be many more hours I'd be sitting here because the code is well much more complicated to write anyways let's just have a look at a few kind of instances here of where the AI is making some mistakes and where it's doing good so here you can see dealers showing a two I have a 16 soft 16 hitting soft 16 to hard 16 and then hitting to 26 so in that instance obviously that's making a mistake it's correct to hit the soft 16 not correct to hit the hard 16 so I think again some improvements could be made there now here dealer has a three we have a 10 hit to 13 and then hit again to 19 now it works out in that instance but that's not the correct play so really what I've noticed is this has a bias to hit more than it should now I could easily fix that by hardcoding a few basic strategy rules in here but that's not what I want to do right I want this to learn purely from reinforcement learning and from trial and error so at this point I'm not exactly sure how to proceed what I will do is quickly show you the code and give you an idea of what I've been doing and then maybe you guys can give me some suggestions in the comments down below and we can do a part two unfortunately I don't have 10 more hours to sit here and mess around otherwise I would try to add those extra rules in and see how much of a difference that makes so if we have a quick look at the code here and by the way I'll leave this available from the link in the description you can see I have my main Blackjack module which is all of the code that we looked at before which essentially simulates one hand of blackjack now we don't play with shoes we use the same cards over and over so it's like we're using a continuous shuffler machine and just every round we reshuffle the cards which is different obviously than real blackjack where you can kind of count the cards and see what you're getting now inside of blackjack Q learning here we start by initializing with three parameters now these are related to the Q learning we have Alpha which is the learning rate gamma which is the discount factor and Epsilon which is essentially the chance of choosing a random action it's how much exploration we're going to do now these three factors are really what make up the core equation for updating the values in the Q table I'll see if I can put that on the screen for you guys right now so you can see what the equation is it's a little bit complex but pretty much Alpha is telling us okay how quickly should we update values based on the result we just got now gamma is telling us how much we should consider future rewards and Epsilon is our chance of kind of exploring or choosing a random choice so rather than always choosing the choice that has the highest Q value sometimes we pick a random choice so the model or the agent has some ability to explore and choose something that maybe hasn't been chosen before but could be the optimal solution these three parameters uh can actually have a huge impact on the model or on the agent I have messed with them a bit but obviously if you were able to find the right relationship here and mess with these a lot then you'd be able to probably get a better model now what we do after that is actually initialize the Q table which is going to have four dimensions the First Dimension is going to represent the player sum then we're going to have whatever the Dealer's up card is and then we're going to have uh the usable Ace so if we have an ace that can be used for soft hand and then we're going to have the action which is either hit or stay if we wanted to add the other decisions like double and split again that's a bit more complicated but we would have to change this last column to four next we have a function that chooses the action using that Epsilon variable so you can pretty much see that we have a 10% chance here of randomly selecting hitter stay whereas otherwise we're just going to see which one in the Q table currently has the most favorable possible reward or output we then have update this is what's actually updating the Q table and you can see we have kind of that Q function here where we're taking the old Value Plus the alpha multiplied by the reward plus the gamma times the future Max minus the old value again a bit complex you don't need to understand it then we have a function here if I can scroll up that tells us us if we have a usable Ace we have a training function and this is the main function where what we're doing is actually simulating the game we're getting the result so we're seeing if we won or if we lost and then we're calling those update functions to actually update the Q table lastly we have this play function here and what this will do is actually use the Q table that we just trained up and has all those values inside of it to determine what choice we should make based on the card that were given so you can see that I was using play after I did all of the training and we had that Q table in memory I could store the Q table as well that way I don't have to train every time I actually want to play the game so that's pretty much it I wish I could spend some more time and improve this but unfortunately I don't have more time to film right now let me know what you guys think in the comments down below and how I could potentially improve this model I think for a first shot pretty decent it's working fairly well and it gave me a chance to kind of refresh myself with Q learning and hopefully taught you a little bit about how that worked and gave you a realistic view of what it's like to work on a project after all that's kind of the goal this video here just to show you a bit of my process and the fact that I don't always get it right the first time many people see those finalized tutorials online and they think that's always what happens I can't tell you how many times I fail how many times I try to work on a project that just doesn't work and how many pieces of code essentially go in the garbage because I had no idea what I was doing kind of like this anyways if you guys enjoyed make sure to leave a like subscribe to the channel and I will see you in the next one oh