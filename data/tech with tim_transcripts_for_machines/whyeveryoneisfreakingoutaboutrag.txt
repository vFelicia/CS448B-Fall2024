what is Rag and what you need to know about it as a developer well rag stands for retrieval augmented generation and this is something that promises to drastically enhance the usability of llms we've probably seen that llms have quite a few issues they are very usable and a massive tool but if you wanted to implement them into your own application you can't be confident that the results that they're giving you are accurate and you don't know how they came up with the answer rag is something that helps with that problem and I'm going to break down everything you need to know about it in this video so let's begin with the problems with current llms these llms are giving you answers that often times are out of date the reason for that is they're only pulling data from the data that they were trained on whenever that date occurred so in the case of Chad gbt you've probably seen that famous reply where it says I don't know that answer because I've only been trained up to September 2021 or in a worst case scenario it gives you an answer but that answer is actually incorrect because since then then we've come to some new discovery or found some new information that makes that answer obsolete for example if you're asking things about scientific research it'll actually give you an answer most times but it might not be the most uptodate or current answer yet it's very convincing and if you don't know the correct answer you could be easily misled and end up using inaccurate information so that's the first major problem out ofd data next problem is that a lot of times you don't have any idea how the llm actually came up with the answer it doesn't give you a source you're kind of just blindly trusting that what it gave you is accurate and even if it is accurate a lot of times you'd probably like to know where it C that information from so you could go and double check it yourself so these are the two major issues outof date and no source so how do we fix that using retrieval augmented generation or rack well rag is actually quite simple what this does is connect an llm to a data store this means that that when the llm actually wants to come up with an answer rather than just using its training data it will actually go and ask a retriever to get a specific piece of data or some content from the data store it will then inject that kind of into the prompt or into the llm model and then it will use that data that it retrieved that likely is up to date to generate a reply so to give you a super simple example of this let's imagine I want to use an llm to retrieve uptodate scores for a football game now obviously there's better ways to do this but let's say for some reason I want to use an llm well what I would do is connect that to a realtime database that contains all of the NFL scores that database we're going to trust is always up toate and contains the accurate information now what will happen is when I have a prompt or I give that to chat GPT or the llm whatever it is it will now go to the data store it will retrieve the relevant information based on the data in my prompt it will kind of inject that into the llm and then it will use the data it just retrieved to answer my question so now we've kind of solved both problems I have updated information and I have a source for where that information is coming from it's that data store so if I want to know where I actually got my data or I want to go there and double check it I can simply go to the original source and now what we're doing is we're using the llm for its reasoning ability and ability to understand natural language not its huge memory from a massive data or training set this is really where this type of technique comes in rather than using an llm as a knowledge base you're using it as something that can reason something that can give you a natural reply and can understand what you're asking better than probably any model you could train on your own so it's really the interface for some type of application and then you have the actual data that gets injected and brought into the llm and you can control that data you could have data for I don't know your call center you could have data for an application or a stat tracking website whatever you want any data you want you can augment the llm with that and allow it to reason solely based on that data and that way you know you're getting accurate uptodate information and if you want to check that information you can go directly to the source now just as a quick note here before we dive in too far to really understand the benefit of something like rag you first need to understand how llms actually work and for most of you you're probably going to be interacting with something like chat gbt now fortunately our video sponsor HubSpot actually has a completely free resource called how to use chat gbt at work that breaks down exactly how chat gbd Works gives you expert insights and tells you how you can use it to its full ability now I put the link in the description so you guys can check it out but this resource is packed with knowledge and it even has 100 actionable prps that you can use today to really leverage the full power of chat GPT knowing how to use a tool like this effectively is absolutely a game Cher especially in the programming industry and again you guys can check that out from the link in the description a massive thank you to HubSpot for making this resource and tons of others completely free so to give you the highlevel architecture here we have a prompt this is the first step once you generate the prompt you can actually vectorize that and you can then go to a retriever which will go and find all of the relevant data required for the specific prompt it will then augment the llm with that so it will essentially pass that into the prompt the llm will give you some type of reply and then it can actually provide evidence directly from your data source as to why it came up with that answer now this also gives another advantage that if your data source doesn't have the answer the llm can tell you that rather than giving you a convincing answer that's actually wrong or misleading it can simply say hey based on what you gave me here for data I don't actually have the answer now you can decide whether that's better or worse but in my opinion I'd rather the llm tell me I don't know than tell me an incorrect and false statement or kind of false information that might mislead me or have some pretty bad repercussions so now let's quickly dive into some of the major benefits of using a technique like rack first of all this allows you to avoid retraining language models and instead augmenting them with uptodate information we've already touched on this but typically if you'd want the llm to have uptodate data you'd have to retrain it on that data or if you want it to work specifically for your company information you train it on that data now you no longer need to do that you simply keep your data source up to dat and all of a sudden the model works exactly as you would expect now the next major benefit of course is the source knowing where the data actually came from and being able to validate that is massive and then lastly kind of touching on that being able to actually answer and say I don't know is a huge benefit so if the model doesn't know you can then go to the data source add the correct information or at least you know you're not getting a misleading answer now as well as all of these benefits there are a few concerns that you want to keep in mind first of all all of this only works works if the data that's augmented into the model is good and relevant now that means you need to come up with a quick efficient and accurate way for retrieving relevant information based on a prompt now there's a lot of complex stuff you can do here but the simplest way to do this would be to use something like a vector database that means you would vectorize the prompt you would then go in the vector database and you would find all of the documents that have similarity based on their Vector representation and then return those but you can't simply return every piece of information you have to be selective at what you're returning and this needs to happen very very quickly it can't introduce a ton of latency otherwise that's going to be a poor user experience now at the same time you want to make sure that the model is accurately using this augmented data so there's some special prompts and ways that you can kind of instruct the model to make sure it only gives you information based on the data that was augmented into it now just as a point of clarity here by no means am I an expert in rag I'm sure there might be some slight inaccuracies in this video but I wanted to share with you the the general idea of rag so that you guys can go look it up and see how you can use it in your applications now I actually did make a video that uses this type of technique and framework to generate a Choose Your Own Adventure game really interesting and probably the simplest example to see exactly how this works so if you want that I'll put it in the description and pop the video on the screen here anyways if you guys enjoyed make sure you leave a like subscribe to the channel and I will see you in another one