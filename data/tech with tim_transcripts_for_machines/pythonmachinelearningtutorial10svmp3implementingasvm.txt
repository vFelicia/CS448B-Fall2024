hey guys and welcome back from the machine learning tutorial with Python so in today's video as promised we're gonna be implementing the SVM algorithm so we're actually going to be using something called SVC which is support vector classification but it's kind of a part of support vector machines obviously it's exactly what we talked about in the last video so before I go too far I just quickly want to give a summary of what the rest of the tutorial series is gonna look like for everyone that is still here and by the way if you are still here thank you and command yourself because most people don't watch paths like the first or second video so because they're still watching and still going along with the destroy really serious that means you guys are actually here to learn and you're getting a lot of value out of it so that's awesome for you guys anyways what we're gonna be doing in the next videos is I'm gonna be talking about the kmeans clustering algorithm which is an unsupervised learning algorithm that you guys will see and you'll talk about the difference between supervised and unsupervised learning which so far we've only been doing supervised learning then I'm gonna take a break for probably like five days five to seven days work on some more deep learning stuff because I don't have too many projects done in that right now and then come back to you guys with some neural network tutorials and some project videos and really get right into that but I do want to take a break and kind of start to us as its own new series just to maybe get some more people watching it cuz a lot of people don't watch like you know the fifteenth video in a Python machine learning tutorial so if that's fine with you guys but anyways let's get into implementing this algorithm so essentially before I start I have actually just imported two new things here so I've imported metrics from SK learned and I've just imported the cane your nearest neighbors classifier which we had in previous videos just because I want to compare our results using an SVM to the K nearest neighbors classifier and see if one's better than the other why that would be and kind of talk about that so what I'm gonna do now actually is really easy right to implement our classifier which going to be CL f equals SVM dot SBC now this stands for support vector classification at least that's what I'm pretty sure it stands for that would make sense and this takes a ton of different parameters now we can leave it like this and this we're gonna do right now and just see exactly how it performs without tweaking anything without giving a colonel without giving a soft margin or hard margin and see what we get so let's do that so to fit this obviously we're going to see LF thought fits and we'll just give it our Exeter store train Y underscore train data and then we're actually going to have to predict some data before we can score this so to predict this we're genders let's say Y underscore prediction is standing for CL f dot predict and then we'll just give it our X underscore test data and then now we can actually score it using this really cool metrics thing from SK learn so essentially to get the accuracy lose do metrics dot us score accuracy score oh I gotta type metrics correctly that's why accuracy accuracy score and then we're going to give it our y underscore test and our y underscore prediction now it doesn't even matter what order you put this in because all this is going to do is just compare the two lists here to see like which are correct and give you kind of the amount of error there and if we want to print our accuracy of the screen obviously you can just print ACC now notice I just got rid of some of the print statements here just because I don't want that coming out and just wasting time when we're trying to run this algorithm so let's go ahead and run this and see exactly how well we're doing so fifty four okay that's probably not what you guys were expecting now that is because we haven't added any of the parameters we haven't tweaked anything so you can imagine that someone that maybe doesn't know how SVM works tries to use this classifier and wow they have 54 and they're like SVM is crap I don't want to use this right but that's because we haven't tweaked the parameters so essentially this is no better than just guessing at this point in fact randomly guessing a number might even do better than this in theory right or in practice so let's actually use some parameters now for SVC so one of the parameters this takes is kernel and this is the first one we're gonna start messing around with now I'm just gonna put in here linear as our kernel okay and there's a huge list of kernels actually I'll bring up the parameters here so you can actually have a look I'll leave this in the description if you guys want to read through all the different parameters that you can mess with because I'm only gonna show like two or three in this video but essentially you can see it has kernel it's a string it's optional defaults to our bf can look that up if you want to know what that is and then it goes there's polynomial sigmoid and linear these are the four that it allows you to use at least for SK learn and you can create your own kernels if you want although I don't recommend you get into that yet because you're probably not math professors anyways we're gonna use a linear and let's just see what the differences that we get here ninety six okay Wow so that is a massive difference and that's probably by tweaking one parameter so you can see what I was talking about where how bringing our dimensions up one can give us that hyperplane is gonna give a much better classification so that's exactly what I was trying to show in the last video now we could mess around we could throw a polynomial in here but I won't show you what happens when I throw a polynomial no polynomial is like it's gonna be going to like exponent six exponent seven giving some like pretty crazy values that might be more accurate but watch how long this takes to Train and we might be sitting here for quite a while in fact I've tried to train this and waited a bit like 1015 minutes and then I get too impatient and just closed it but you can see that this is not as instant as our linear kernel because it's applying a lot more math and especially I'm just gonna stop running this because we're not gonna get to the end of it in this video essentially when we're dealing with super small numbers like we have in these cancer data set applying exponents to those is a very large operation so we could try to do something like degree and change the degree of our polynomial kernel to be two and maybe maybe this will make a difference let's try this and see if we get anything I'll give it like 30 seconds and if it doesn't run then we're just gonna call it quits yeah so essentially right like this is gonna take a long time so maybe if you had enough time you could wait for this to run but it's kind of a mystery on how long this is actually gonna take us to happen right so we'll quit that and you can see like those are some of the things that are changing these parameters would do so let's go back to linear because that seemed to be working for us and now let's actually just tweak something called C now C is actually gonna be that soft margin that I was talking about before so it defaults to one allowing somewhat of a soft margin but if we wanted to increase that we could do something like C equals two okay and this is double the amount of points that are gonna be allowed in that margin then before if you wanted a hard merge and you do C equals zero so let's do C equals one and see what we're getting you can see now we're getting 94.7 now obviously this is all gonna vary depending on what our data is split up into oh wait is the reason that C is one I thought I made that too anyway so let's try this again 96.4 right so essentially you'd probably want to go through and train this and keep tweaking these parameters and maybe trained on the same training data constantly rather than splitting it up like this like just do like a hard split by you just might like this like data up to a hundred is your test data and past that is your training data so that you can really see what these parameters are actually doing for you but obviously we know that linear is making a massive difference adding that kernel in so I should I gotta get rid of square brackets so essentially that's really all there is to using this SVM now I want to compare using K nearest neighbors classifier what the main difference is and if we're gonna get a better value from K nearest neighbors now I'd ask you guys to make your own prediction do you think that using K nearest neighbors is going to make a difference so let's to use this and we can set what do you call it n neighbors equal to something like nine we do have quite a few points so that should be okay and let's try this now okay so 94 seven let me just make sure this is actually running the right script because yeah so working file okay so we are ready running the correct script here okay so the only reason I was doing now is just cuz it got like the exact same amount as the SVM which was kind of surprising to me but let's see if we decrease the neighbors what we're getting 91 so I think with the increasing neighbors you kind of get a what he called a better sense here so ninety three point eight now this is surprising because typically K nearest neighbors does not work as well with huge dimensions and we do have like 30 or something features for this data set but here that's a perfect example of why we have to test out different machine learning algorithms I would still say SVM is probably our best bet to use right now just in terms of you know its accuracy by only adding that linear kernel here we kind of have to mess around with the different amount of neighbors I can imagine that we're gonna get quite a bit of variance as based on our different training and testing data so that's really all I can say about that so essentially now we have learned linear regression we've learned K nearest neighbors and we've learned SVM the three kind of fundamental algorithms and now we can go into kmeans our first unsupervised learning algorithm which is kind of exciting because we're no longer gonna need to do this kind of stuff we're gonna give it training data but we're not gonna tell it what the training data is for it because right now essentially we're saying okay so this this data point is equal to a malignant tumor right I realize I didn't even end up using this list but anyways or a benign tumor we're not gonna tell our trick our model that next time we're gonna say you have to figure out what makes a class this and what makes it class this and why and that's kind of cool and that's the cool kind of machine learning stuff and that's we're gonna be doing with neural networks as well so I hope you guys are excited about that with that being said that's if the video if you guys want the textbased tutorial and the previous and the rest of them go to my website tech with Tim net and I'll see you guys in another video