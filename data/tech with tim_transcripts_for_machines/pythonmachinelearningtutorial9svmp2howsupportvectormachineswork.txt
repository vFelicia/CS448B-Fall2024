hey guys and welcome back John the machine learning tutorial with Python so in today's video we're gonna be talking about SVM and how support vector machines work so SVM obviously stands for support vector machines and these are gonna be used at least in our purposes for classification although we can use them for something known as regression which I talked about in the first few videos but I'm not gonna be explaining that here so essentially really basically how SVM's work is they attempt to create something known as a hyperplane and a hyperplane is something that can divide your data using hopefully something that's straight so I cap Lane is straight a what he called a line is straight so you can have like fourdimensional stuff so anything that's straight right so like a linear way to divide your data so essentially a hyperplane for the data set that I have up here could look something like this right and you can see that it's dividing our data points so Green is on this side and red is on this side right now how do we create this hyperplane what are like the parameters for it like what do we need to do to you to do this well the requirements for a hyperplane essentially are that you find or the hyperplane story is the same distance from the two closest points of each opposite class that's confusing but I'll draw it out and essentially any closest point to the line now is this red point and the closest point from the green side is actually have to draw a new one to make sure that this works well let's do one like here is this green point that I just drew okay so these are the two closest points to the line from either class this red point is the closest and this green point is the closest okay and the distance between this red point and the line and this green point in the line are the exact same so imagine if we start tilting the line this way then obviously this red points give you closer in this green points gonna be further away right so just just you can picture that okay so these two points are the closest points to the line and the distance between them and the line is the same that's how we generate a hyperplane so with that information we actually now know that we can generate an infinite amount of hyperplanes for this data set another hyperplane could look something like this we're the two closest points are this green point and this red point and their distance to the line is the exact same okay we can draw all kinds of different hyper planes but which ones are the best right so for example I could probably I'm trying to try to do one like this we could say that like this point and this point are the two closest points aligned and their distance is the same from the line and that again is another hyperplane now how do we pick which hyperplane to generate what is the best one what is going to give us the best result so essentially since I didn't really talk about it what anything on this side of the hyperplane is gonna be green right for when we're predicting and anything on this side is gonna be red that's essentially how we're gonna use this hyperplane but how do we pick the best one well this one would probably be the best hyperplane to for our data set and the reason is because the two closest points to the line are actually the farthest possible points away like the distance between them and the lines so let's say like this one and maybe let's say this one I know this one's a bit closer but just imagine the same distance is the largest distance that we can generate no matter what other points we pick or where we draw this line there's no way that we can find a distance d2 noted by D that is greater than this now why would we want this distance to be so so big like why would we want this distance to be as large as possible well I'm gonna just draw two lines here quickly let's say like this okay and the distance denoted by D and between these two points are like in between all this right so where there's no other points is known as our margin okay and we obviously we want to maximize this margin now why why is that the case why do we want to do that well let's think about it for you for a second alright so if I remove all this okay so let's erase I don't click that let's erase this line and let's draw another line and let's compare that line that I just use like that one to the one I'm about to to do okay so let's say we have a point that we're trying to predict its class and the point is right here well what point do you think is like a human this should be English we green in English be red I would hope most of you would say it's green but if we use a what he called a hyper like this it's going to classify this as red because it's on the left side of the hyperplane so why would we want it to be as large as possible because one's like this if we had that at that same point then somewhere around here it's going to be classified more correctly as green because the larger the distance and the larger the margin the more we can separate the two classes and do more accurate predictions so essentially that is the basis behind support vector machines we're trying to find something like a line or a plane that we can separate our data points by we want to find the largest distance between our what's known as support vectors which is going to be this red one and this green one and then we'll generate that hyperplane we'll use it to predict out it based on what side of the hyperplane song okay that's the most basic understanding now let's talk about the problems that we run into when we do this so right now we have data that is in my opinion quite nice right we look at this and we can tell really quickly that we're gonna have a line like this that's what it's gonna look like right but what if we change this up a bit and we go to make look at data that is not like this it doesn't linearly kind of correlate like this is really nice data that is not common to find especially in the real world when you're dealing with well real data and stuff that is not gonna look exactly like that so let's do some red points I'm not gonna do as many cuz I'm gonna do something with these in a second and maybe they look like this okay that's our red points and maybe we have some green points and our green points look something like this well if I asked you can you draw me a hyperplane for this you would say what a hyperplane where where does a hyperplane go does it go like that does it go like that does it go like that we honestly have no idea where to draw the hyperplane and even if we can come up with one is it gonna be accurate because on here we have no idea if it's actually gonna be red or green or that's gonna be a correct prediction we're pretty much just guessing at that point so what do we do to fix this problem well we use something called kernels now this is in two dimensions right so we have X 2 and X 1 as our features and then red it's green is denoting what the class is going to be ok so what we want to do is we actually want to turn this data into a form where we can draw a for plane or make an accurate hyperplane through it that divides our training data so what we're gonna do is we're actually gonna bring this whole thing up so this whole twodimensional data we're gonna bring this into three dimensions and the way we do that is using something called a kernel now a kernel sounds confusing is essentially just a function so you can have F okay denoting a function that takes inputs in this case it's gonna have x1 and x2 and spits out an output which is gonna be x3 which is a higher dimension okay I'm gonna show this in and drawing in just one second so it's gonna take our two features x1 and x2 for every one of our data points and based on that input it's gonna give us an output of x3 which is going to represent like the third coordinate for our points because we're going to go up a dimension we're gonna go to three dimensions from two dimensions so if right now we have x1 and x2 for every one of our points we're gonna be adding an x3 and then plotting it on a three dimensional graph so let's erase this and let's do just that so three dimensional graphs we're gonna have our three axes so we'll keep this as X 2 this is X 1 and we'll draw one out like here which is gonna be our X 3 okay now I'm just gonna draw some red points and some green points so do some red ones and we'll do some green ones now imagine that these are the same points that we had in two dimensions right so I'll draw a small graph here just so you remember what it looks like in two dimensions right we had like we're at red points and green points kind of all just scattered around in the middle so I know this is not the same but essentially when we go from this to this now we look here and well we can draw a hyperplane look at this by generating that Third Point we've successfully been able to divide our green points and our red points because of this third dimension right so if you imagine now you squish this back to two dimensions then we're gonna come back to looking something like this okay so they're right if we remove that x3 coordinate but now that we have this and we've used this kernel we can actually generate a hyperplane looking something like this that divides data and this works the exact same as we did for hyperplanes in two dimensional space where let's say like the distance here to this red point is the same as the distance here to this green point and they're the two closest points to our plane so I know this might be a bit confusing because I'm trying to explain a pretty complex topic like really simply and sometimes that even makes it more confusing but essentially if we have data that looks like this right in two dimensions and we can't classify it in two dimensions we can't draw a hyperplane we can add a third dimension to it in hopes of getting data that looks like this right and hopes of getting a graph that looks like this now obviously this doesn't always work right so when we use this function we use that kernel function we get X 1 and we get X 2 and that brings us to X 3 maybe our data still is is impossible to classify right maybe we can't do a hyperplane through it there's just there's no way and we still get points over here and here and they're all mixed up in that case we would repeat the process and add a fourth dimension to our data now I want to just talk about kernels a little bit more because I feel like some people might have still have no idea what they are essentially it's just a function that takes our features so in two dimensions we so here we had X 2 and we had X 1 and we fed that into the function it did something with it and it returned to us a third dimension and then with that third dimension right we can now plot our data and by using that third dimension we've spread our data out we went from like only being on this plane right in two dimensions and we've just brought it up and down so that we can divide it by a hyperplane so I hope that makes sense all kernel is is a function there's all different kinds of kernels um you typically don't create your own you just use an existing kernel an example of a kernel could be something like x1 squared plus x2 squared this is a kernel this is perfectly fine and that would result in the value x3 right so say this is our point and in 2 space it's coordinates is like 1 & 2 okay if we want to turn this into a 3 coordinate or a 3d object or point we take x1 squared plus x2 squared so we get 1 plus 4 because 2 squared right and that the value 5 and now we have a czar let's actually let's draw it here I guess we have as our corner point one two and five and that's what we plot here and we hope that by applying this kernel it's gonna make all of our red points kind of maybe either shoot up and all of our green points go down or go to the side or something that we can draw an accurate hyperplane for so that is essentially how a support vector machine works I'm not gonna go any further than this because if I keep going it's just gonna get way more confusing and way more math there's a ton of stuff behind how we generate this and how we do this this is a super highlevel understanding now you kind of know enough to be able to implement this and so we can kind of tune some parameters and whatnot but yeah that is essentially all there is to SVM there's one more thing which is like a soft um what do you call it a soft margin a hard margin which I'll cover really quickly but this reason I left it to the end is not that important essentially like if we have our twodimensional data like this I'll do really quick because I don't want to waste any more time but let's say we have some pink points or purple points all right and we have some red points like this okay well we can again we can draw our hyperplane and right now you might say ok so the hyperplane is gonna have to look something like that but that might not be a great hyperplane to draw what we hopefully might want to draw instead is something that looks like this now you're saying well we can't do this hyperplane and you're correct right now based on what I've told you because well this point is what do you call it it's the closest point to the line and like we're not using that because we're gonna use this and this is our support vectors for this this hyperplane and this red point you're like well that can't be there that that's incorrect well you're correct right now but this is where we introduced something called a soft margin so if we're saying that this is our what we call it support vector and this is our support vector we'll draw these and we'll say that we're gonna use something called a soft margin and we're gonna allow for a few points like this to exist in between the margin we're gonna say you know what by allowing these kind of outlier points to exist we're actually going to get a better classifier with what he called hyperplane here and that's fine so that's another parameter that we can we can tweak we can give something called a soft margin allowing a few points or however many we say to exist inside this margin and not affect the hyperplane so that's another parameter if you're using a hard margin essentially all that means is what I taught before you can't have any points like this like this red this is not a valid hyperplane but with the soft margin this is fine so there it is that's it for support vector machines please feel free to contact me on discord ask me any questions leave some questions in the comments I try to respond to everyone if you guys enjoyed the video please make sure you leave a like and in the next video we're gonna be getting into implementing this algorithm