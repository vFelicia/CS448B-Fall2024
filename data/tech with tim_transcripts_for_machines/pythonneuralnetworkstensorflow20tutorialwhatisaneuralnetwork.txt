hey guys and welcome to a brand new tutorial series on neural networks with Python and tensorflow 2.0 now tensorflow 2.0 is the brandnew version of tensorflow still actually in the alpha stages right now but it should be released within the next few weeks but because it's an alpha tensorflow has been kind enough to release us that alpha version so that's what we're going to be working with in this tutorial series and this will work for all future versions of tensorflow 2.0 so don't be worried about that now before I get too far into this first video I just want to quickly give you an overview of exactly what I'm gonna be doing throughout this series so you guys have an idea of what to expect and what you're going to learn now the beginning videos and especially this one are going to be dedicated to understanding how a neural network works and I think this is absolutely fundamental and you have to have some kind of basis on the math behind a neural network before you're really able to actually properly implement one now tensorflow does a really nice job of making it super easy to implement neural networks and use them but to actually have a successful and complex nail network you have to understand how they work on the lower level so that's we're gonna be doing for the first few videos after that what we'll do is we'll start designing our own neural networks that can solve the very basic M&S data sets that tensorflow provides to us now these are pretty straightforward and pretty simple but they give us a really good building block on understanding how the architecture of a neural network works what are some of the different activation functions how you can connect layers and all of that which will transition us nicely into creating our own neural networks using our own data for something like playing a game now personally I'm really interested with neural networks playing games and I'm sure a lot of you are as well and that's what I'm gonna be aiming to do near the end of the series on kind of our larger project I'll be designing a neural network and tweaking it so they can play a very basic game that I've personally designed in Python with PI game now with that being said that's kind of it for what we're gonna be doing in this series I may continue this on future in later videos and do like very specific neural network series maybe a chatbot or something like that but I need you guys to let me know what you'd like to see in the comments down below with that being said if you're excited about the series make sure you drop a like on this video and subscribe to the channel to be notified when I post the new videos and with that being said let's get into this first video on how a neural network works and what a neural network is so let's start talking about what a neural network is and how they work now when you hear neural network you usually think of neurons now neurons are what compose our brain and I believe don't quote me on this we have billions of them in our body or in our brain now the way that neurons work on a very simple and high level is you have a bunch of them that are connected in some kind of way so let's say these are four neurons and they're connected in some kind of pattern now in this case our pattern is completely like like random we're just arbitrary we're just picking a connection but this is the way that they're connected okay now neurons can either fire or not fire so you need to be on or off just like a 1 or 0 ok so let's say that for some reason this neuron decides to fire maybe you touch something maybe you smelt something something fires in your brain and this neuron decides to fire now it's connected to in this case all of the other neurons so what it will do is it will look at its other neurons and a connection and it will possibly cause it's connected neurons to fire or to not fire so in this case let's say maybe did what this one firing causes this connected neuron to fire this one to fire and maybe this one was already firing and now it's decided it turned it off or something like that ok so that's what happened now when this neuron fires well it's connected to this neuron and it's connected to this nerve well it's already got that connection but let's say that maybe when this one fires it causes this one to on fire because it was just fired something like that right and then this one now that it's off it causes this one to fire back up and then it goes it's just a chain of firing and unfired and that's just kind of how it works right firing and unfired now that's as far as I'm going to go into explaining neurons but this kind of gives us a little bit of a basis for a neural network now a neural network essentially is a connected layer of neurons or connected layers so multiple of neurons so in this case let's say that we have a first layer we're going to call this our input that has four gnomes and we have one more layer that only contains one now these neurons are connected now in our neural network we can have our connections happening in different ways we can have each what decodes neuron connected to each other neurons so from layer to layer or we can have like some connected to others some not connected so I'm connected multiple times it really depends on the type of neural network we're doing now in most cases what we do is we have what's called a fully connected neural network which means that each neuron in one layer is connected to each neuron in the next layer exactly one time so if I were to add another neuron here then what would happen is each of these neurons would also connect to this neuron one time so we would have a total of eight connections because four times two is eight right and that's how that would work now for simplicity sake we're just gonna use one neuron in the next layer just to make things a little bit easier to understand now all of these connections have what is known as a wait now this is in a neural network specifically okay so we're gonna say this is known as weight one this is known as weight to this is weight three and this is weight 4 and again just to rehab size this is known as our input layer because it is the first layer in our connected layers of neurons okay and going with that the last layer in our connected layer of neurons is known as our output layer now these are the only two layers that we really concern ourselves with when we look and use a neural network now obviously when we create them we have to determine what layers we're gonna have in the connection type but when we're actually using the neural network to make predictions or to Train it we are only concerning ourselves with the input layer and the output layer now what does this do and how do these neural networks work well essentially given some kind of input we want to do something with it and get some kind of output right in most instances that's what you want input results in the output in this case we have four inputs and we have output but we could have a case where we have four inputs and we have 25 outputs right it really depends on the kind of problem we're trying to solve so this is a very simple example but what I'm going to do is show you how we would or how a neural network would work to train a very basic snaking so let's look at a very basic snake game so let's say this is our snake okay and this is his head actually yeah let's say this is his head but like this is what the position the snake looks like where this is the tail okay we'll circle the tail now what I want to do is I want to train a neural network that will allow this snake to stay alive so essentially its output will be what direction to go in or like to follow a certain direction or not okay essentially just keep this snake a lot that's what I want it to do now how am I gonna do this well the first step is to decide what our input is gonna be and then to decide what our output is going to be so in this case I think a clever input is gonna be do we have something in front of the snake do we have something to the left of the snake and do we have something to the right to the snake because in this case all that's here is just the snake and he just needs to be able to survive so what we'll do is we'll say okay is there something to the left yes no something in front yes no so 0 1 something to the right yes no and then our last input will be a recommended direction for the snake to go in so the recommended direction could be anything so in this case maybe we'll say the recommended direction is left and what our output will be is whether or not to follow that recommended direction or not or to try to do a different recommendation essentially or go to a different direction so let's do one case on how we would expect this neural network to perform without Trant like once it's trained right based on some given input so let's say there's not something to the left so we're gonna put a 0 here because this one will represent if there's anything to the left the next one will be front so we'll say well there's nothing in front the next one will be to the right so we'll say right and we'll say yes there is something to the right of the snake and our recommended direction what can be anything we'd like so in this case we say the recommended direction is left and we'll way will do the recommend direction is negative 101 where negative one is left a zero is in front and one is to the right okay so we'll say in this case our recommended Direction is negative one and we'll just denote this by direction now our output in this instance should either be a zero or one representing do we follow the recommended direction or do we not so let's see in this case following the recommended direction would keep our snake alive so we'll say 1 yes we will follow the recommended direction that is acceptable that is fine we're gonna stay alive when we do that now let's see what happens when we change the recommended direction to be right so let's say that we say 1 as a recommended direction again this is durn here then what should our output be well if we decide to go right we're gonna crash into our tail which means that we should not follow that direction so our output should be 0 so I hope you're understanding how we would expect this neural network to perform all right so now how do we actually design this neural network how do we get this work how do we train this right well that is a very good question and that is what I'm gonna talk about now so let me actually just erase some of this stuff so we have a little bit more room to work with some math stuff right here but right now what we start by doing is we start by designing what's known as the architecture of our neural network so we've already done this we have the input and we have the output now each of our inputs is connected to our outputs and each of these connections has what's known as a weight now another thing that we have is each of our input neurons has a value right we had in this case we either had 0 or we had 1 now these values can be different right these values can either be decimal values or they can be like between 0 and 100 they don't have to be just between 0 and 1 but the point is that we have some kind of value right so what we're gonna do in this output layer to determine what way we should go is essentially we are going to take the weighted sum of the values multiplied by the weights I'm gonna talk about how this works more in depth in a second but just just follow me now so what this symbol means is take the sum and what we do is I'm gonna say in this case I which is gonna be our variable and talk about how this kind of thing works in a second we'll say I equals one and I'm going to say we'll take the weighted sum of in this case value I multiplied by weight I so what this means essentially is we're going to start at I equals one we're gonna use I as our variable for looping and we're gonna say in this case we're gonna do b1 times VI or sorry VI times WI and then we're gonna add all this so what this will return to us actually will be V 1 W 1 plus V 2 W 2 plus V 3 w 3 plus V 4 w 4 and this will be our output that's that's what our output layer is going to have as a value now this doesn't really make much sense right now right like why why we doing this weights what is this multiplication we'll just follow with me for one second so this is what our output layer is going to do now there's one thing that we have to add to this as well and this is what is known as our biases okay so what we're gonna do is we're going to take this weighted sum but we're also going to have some kind of bias on each of these weights okay and what this bias is known as it's denoted by C typically but essentially it is some value that we just automatically add or subtract it's a constant value for each of these weights so we're gonna say all of these these connections have a weight but they also have a bias so we're gonna have B 1 B 2 B 3 and B 4 twice what we'll call it B instead of C so what I'll do here is what I'm also going to do is I'm also gonna add these biases in when I do these weights so we're gonna say B I as well so now what we'll have is we'll have at the end here plus bi or plus B 1 plus B 2 plus B 3 + B 4 now again I know you guys like what the heck am I doing with this this makes no sense it's about to make sense in one second so now what we need to do is we need to train the network so we've understood now this is essentially what this output layers doing we're taking all of these weights and these values we're multiplying them together and we're adding them and we're taking what's known as the weighted sum okay but how do we like what are these values how do we get these values and how is this gonna give us a valid output well what we're going to do is we're gonna train the network on a ton of different information so let's say we play 1,000 games of snake and we get all of the different inputs and all the different outputs so what we'll do is we'll randomly decide like a recommended direction and we'll just take the state of the snake which will be either there's something left to the right or in front of it and then we'll take the output which will be like did the snake survive or did the snake not survive so well what we'll do is we'll train the network using that information so we'll generate all of this different information and then train the network and what the network will do is it will look at all of this information and it will start adjusting these biases and these weights to properly get a correct output because what we'll do is we'll give it all this input right so let's say we give it the input again of zero one zero and maybe one like this random input and let's say the output for this case is what do you call it so one is go to the right the output is one which is correct well what the network could do is say okay I got that correct so what I'm gonna do is I'm not gonna bother adjusting the network because this fine so I don't have to change any of these biases I don't have to change any of these weights everything is working fine but let's say that we get the answer wrong so maybe the output was zero but the answer should have been one because we know the answer obviously because we've generated all the input and the output so now what the network will do is it will start adjusting these weights and adjusting these biases they'll say all right so I got this one wrong and I've gotten like five or six wrong before and this is what was familiar when I got something wrong so let's add one to this bias or let's multiply this weight by two and what it will do is it'll start adjusting these weights and these biases so that it gets more things correct so obviously that's why neural networks typically take a massive amount of information to Train because what you do is you pass it all of this information and then it keeps going through the network and at the beginning it sucks right because it doesn't this network just starts with random weights and random biases but as it goes through and it learns it says okay well I got this one correct so let's leave the weights and the bias is the same but let's remember that this is what the way in the bias was when this was correct and then maybe he gets something wrong and it says okay so let's adjust bias one a little bit let's adjust weight one let's mess with these and then let's try another example and then it says okay I got this example right maybe we're moving in the right direction maybe you will adjust another way maybe we'll adjust another bias and that's eventually your goal is that you get to a point where your network is very accurate because you've given it a ton of data and it's adjusted the weights and the biases correctly so that this kind of formula here of this weighted average will just always give you the correct answer or has a very high accuracy or high chance of giving you the correct answer so I hope that kind of makes sense I'm definitely over simplifying things in how the adjustment of these weights and these biases work but it's not crazy important and we're not going to be doing any of the adjustment ourselves we're we are just gonna be kind of tweaking a few things with the network so as long as you understand that when you feed information what happens is it checks whether the network got it correct or it got it incorrect and then it adjusts the network accordingly and that is how the learning process works for a neural network alright so now it's time to discuss a little bit about activation functions so right now what I've actually just described to you is a very advanced technique of linear regression so essentially I was saying we are adjusting weights we're adjusting biases and essentially we are creating a function that given the inputs of like X Y Z W or like left front right we are giving some kind of output but all we've been doing to do that essentially is just adjusting a linear function because our degree is only 1 right we have weights of degree one multiplying by values of degree one and we're adding some kind of bias and that kind of reminds you of the form MX plus B we're literally just adding a bunch of MX plus B is together which gives us like a fairly complex linear function but this is really not a great way to do things because it limits the degree of complexity that our network can actually have to be linear and that's not what we want so now we have to talk about activation functions so if you understand everything that I've talked about so far you're doing amazing this is great you understand that essentially the way that the network works is you feed information in and it adjusts these weights and biases there's a specific way it does that which we'll talk about later and then you get some kind of output and based on that output you're trying to adjust the weights and biases and and all that right so now what we need to do is talk about activation functions and what an activation function does is it's essentially a nonlinear function that will allow you to add a degree of complexity to your network so that you can have more of a function that's like this as opposed to a function that is a straight line so an example of an activation function is something like a sigmoid function now a sigmoid function what it does is it'll map any value you give it in between the value of negative 1 and 1 so for example when we create this Network our output might be like the number 7 now this number 7 well it is closer to 1 that is to 0 so we might deem that a correct answer or we might say that this is actually way off because it's way above 1 right but what we want to do essentially is in our output layer we only want our values to be within a certain range we want them to be in this case between 0 and 1 or maybe we want them to be between negative 1 and 1 I'm saying like how close we are to 0 making that decision how close we are to 1 something like that right so what the sigmoid activation function does it's a nonlinear function and it takes any value and essentially the closer that value is to infinity the closer the output is to 1 and the closer that value is to negative infinity the closer that output is to negative 1 so what it does is it adds a degree of complexity to our network now if you don't if you're not a high level like math student or you only know like very basic high school math this might not really make sense to you but essentially the degree of something right is honestly how complex you can get if you have like a degree 9 function then what you could do is you can have some crazy kind of curve and stuff going on especially in multiple dimensions that will just make things like much more complex so for example if you have like a degree nine function you can have curves that are going like like this like all around here that are mapping your different values and if you only have a linear function well you can only have a straight line which limits your degree of complexity by a significant amount now what these activation functions also do is they shrink down your data so that it is not as large so for example right like say we're looking with data that is like hundreds of thousands of like characters long or digits we'd want to shrink that into like normalize that data so that it's easier to actually work with so let me give you a more practical example of how to use the activation function I talked about what sigmoid does what we would do is we would take this weighted sum so we did the sum of wivi plus bi right and we would apply an activation function to this so we'd say maybe our activation function is f of X and we would say F of this and this gives us some value which is now going to be our output neuron and the reason we do that again is so that when we are adjusting our weights and biases and we add that activation function in now we can have a way more complex function as opposed to just having the kind of linear regression straight line which is what we've talked about in my other machine learning courses so if this is kind of going a little bit over your head it may be my lack of explaining it I'd love to hear in the comments below how you think this explanation but essentially that's what the activation function does now another activation function that is very popular and is actually used way more than sigmoid nowadays is known as rectified linear unit and what this does is it having draw it in red actually so it we can see it better is it takes all of the values that are negative and automatically puts them to zero and takes all of the values that are positive and just makes them more positive essentially or like to some level positive right and what this again is gonna do is it's a nonlinear function so it's going to enhance the complexity of our model and just make our data points in between the range zero and positive infinity which is better than having between negative infinity and positive infinity for when were calculating air all right last thing to talk about for neural networks in this video I'm trying to kind of get everything like briefly into one long video is a loss function so this is again gonna help us understand how these weights and these biases are actually adjusted so we know that they're adjusted and we know that what we do is we look at the output and we compare it to what the output should be from our test data and then we say okay well it's adjust the weights and the biases accordingly but how do we adjust that and how do we know how far off we are how much to tune by if an adjustment even needs to be made well we use what's known as a loss function so a loss function essentially is a way of calculating error now there's a ton of different loss loss functions some of them are like mean squared error that's the name of one of them I think one is like I can't even remember the name of this one but there's there's a bunch of very popular ones if you know some leave them in the comments love to hear all the different ones but anyways what the loss function will do is tell you how wrong your answer is because like let's think about this right if you get an answer of let's say maybe our output is like zero point seven nine and the actual answer was one well that's pretty close like that's pretty close to one but right now all we're gonna get is the fact that we were zero point two one off okay so zero point two went off so adjust the weights a certain degree based on zero point two one but the thing is what if we get like zero point eight five well is this like this is significantly better than zero point seven nine but this is only gonna say that we were better by what is this zero point one five so we're still gonna do significant amount of justing to the weights and the biases so what we need to do is need to apply a loss function to this that will give us a better kind of degree of like how wrong or how right we were now these loss functions are again not linear loss functions which means that we're gonna add a higher degree of complexity to our model which will allow us to create way more complex models and neural networks that can solve better problems I don't really want to talk about loss functions too much because I'm definitely no expert on how they work but essentially what you do is you're comparing the output to the what the output should be so like whatever the model generated based what it should be and then you're gonna get some value and based on that value you are going to adjust the biases and the weights accordingly the reason we use the loss function again is because we want a higher degree of complexity they're nonlinear and you know if you get 0 if you're 99 percent like say your point one away from the correct answer we probably want to adjust the weights very very little but if you're like way off the answer you're two whole points maybe our answer is negative one we want it to be one well we want to adjust the model like crazy right because that model was horribly wrong it wasn't even close so we would adjust it way more than just like two points of adjustment right we'd adjust it based on whatever that loss function gave to us so anyways this has kind of been my explanation of a neural network I want a bear I want to stay right here for every one that I am no pro on neural networks this is my understanding there might be some stuff that's a little bit flawed or some areas that I skipped over and quickly actually because I know some people probably gonna say this when you're creating neural networks as well you have another thing that is called hidden layers so right now we've only been using two layers but in most neural networks what you have is a ton of different input neurons that connect to what's known as a hidden layer or multiple hidden layers of neurons so let's say we have like an architecture maybe that looks something like this so all these connections and then these ones connect to this and what this allows you to do is have way more complex models that can solve way more difficult problems because you can generate different combinations of inputs and he didn't what what is known as hidden layered neurons to solve your problem and have more weights and more biases to adjust which means you can on average be more accurate to produce certain models so you can have crazy neural networks that look something like this but with way more neurons and way more layers and all this kind of stuff I just wanted to show a very basic network today because I didn't want to go in and talk about like a ton of stuff especially because I know a lot of people that watch my videos are not pro math guys are just trying to get a basic understanding and be able to implement some of this stuff so anyways again that has been my understanding if you guys have any questions please leave them below or join my discord server and feel free to ask I'm always helping people out but that being said if you guys enjoyed the video please make sure you leave a like and subscribe and I will see you again in the next one