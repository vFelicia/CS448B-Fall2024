hey guys and welcome back to part 2 of AI chat bots in Python now in the last video we were kind of just working on figuring out how the chat BOTS gonna work talking about the data how we're gonna feed it information and now we're just gonna work a little bit more on the preprocessing of the data so we just kind of were loading in some the JSON file I just want to add a little bit to what we've already done because I kind of forgot it in the last video so what I'm gonna do is create a new list call it Docs underscore @y I'm gonna change this to Doc's underscore X now the reason I'm gonna do this is because for each pattern I also want to put another element in Doc's Y that stands for what intent it's a part of so like what tag it is a part of so to do that we're just gonna say Doc's underscore Y dot append and in this case we'll just say intent and tag so this way each entry in Doc's X corresponds to an entry in Doc's Y and the entry in Doc's X is going to be that pattern and then the intent will be in Doc's Y so we know kind of how to classify each of our patterns which will be important for training the model so now that we've done that we're actually gonna stem all of the words that we have in this words list and remove any duplicate elements because we just want to figure out what our kind of vocabulary size of the model is so how many words it has seen already so to do that we're gonna say words equals and in this case we're gonna say stammer dot stem and W dot lower it's important you add this lower because we want to convert all of our words into lowercase so they don't get mixed up where we don't think they're different than uppercase words the same we're gonna say for W in words just like that now with that what we can do now is take our words list we can make it a set and just you guys will see how this to this is gonna remove all the duplicates essentially we're gonna say sorted list set words now what set does is it takes all the words make sure there's no duplicates or just removes any duplicate elements list is just gonna convert this back into a list because the set is its own data type and sorted is obviously just gonna sort these words just so we can use those a little bit nicer now for our labels I don't think we need to do anything with that but we can actually we could sort it if we want a so let's sort our labels as well to say labels sorted labels and now what we're gonna do is start creating our training and testing output now right now all we've done essentially set up these few lists so we have all of our labels in one list all of the different words in our file in one or all of the words sorry in our patterns in one we have docks X which has list of all of the different patterns and then docks Y and the corresponding entries in docks X and docks Y are like the words and then the tag for that for those words which is the pattern right hopefully I didn't confuse you too much with that but this output or this input is not actually going to work for our neural network because right now we have strings and neural networks only understand numbers so what we're gonna do is create what's known as a bag of words that represents all of the words in any given pattern and we're gonna use that to train our model now a bag of words is what's known as one hot encoded which means that essentially we're gonna have a list maybe something like this that's gonna be the length of the amount of words that we have so if we had 100 words then each encoding is going to have 100 entries and it's just going to be zeros and ones now each position in this list will represent either if a word exists or if a word doesn't exist now this could also be it could be three could be four it just tells you how many times each word occurs so if you're confused right now let me try to break this down a little bit more we're gonna say that when we encode this the first word in our list maybe is a the second word in our list is like maybe bytes maybe the third one is like goodbye okay so what we do essentially is Ranse we're gonna look at a sentence and we're gonna encode it in this form and we're gonna say is there an A in our sentence if there's an A we're gonna put a 1 if there's two A's we're gonna put two and so on so the frequency goes as the entry because it's matching up the first entry with the first number right guessing now we're on the second one now despite exist in our sentence if it doesn't we're gonna put a zero if it does we're gonna put how many times it exists and we're just gonna keep going throughout this list and throughout all of our words and figure out the frequency of our words and put them in a list that's like this now the reason it's called one hot encoded is because it just represents like if the word is there or not usually one hot is because you only do if the word exists in this case we're doing it with the frequency so like um two or or three you're gonna have that as well and this is just a really good input to our neural network so we can essentially just determine what words are there and what words aren't there as opposed to giving it some string which it has nothing to do or it doesn't have any idea what to do with so to create this bag of words we're gonna have to do the following so we're gonna say training equals a blank list we're going to say output equals a blank list and we're going to say out underscore empty equals in this case 0 for X or actually for underscore in range and then the blend of classes now I first to talk about our output data so our input data is going to be that big list with however many word entries we have an interest gonna say whether a word exists or whether it doesn't so we're gonna have a bunch of zeros and a few ones for each word that exists so a bag of words that's what that is now our output actually has to be in a different form as well so right now our output is simply something like what's in it what's an intent we have we have greeting so like it's all of our tags so it's a bunch of strings we have greeting we have like goodbye or something we have shoppe we have all of these different tags now we need to turn these into one hot encoded as well which means we're gonna have a list that has a 0 in it for all of the different classes so for however many classes we have and if that class or if that tag exists or is the one that we want we'll put a 1 there so let's say we have a list again that looks like 0 0 0 1 maybe for tags and the first tag is like hi the second one is by third one is like cell and the last one is help or something like that then this would mean that help is the tag associated with whatever output or input we have here because it has the one beside it and that's the way that this is going to work for our output so we're gonna have a bunch of output list and they are all going to be the length of the amount of classes we have all right so now it's time to actually create these bag of words we're gonna say for Doc in Doc's underscore X and we're actually gonna do X comma enumerate because we're gonna need to use this what we're gonna do is we're gonna say bag equals a blank list and this is gonna be our bag of words so that one hot encoded words that you guys will see we're gonna save pattern underscore words equals in this case actually we don't need to do that sorry am i bad we're gonna say W RDS equals and now we're gonna stem all of the words that are in our patterns because when we stem them we only stem each word in our like words list we didn't stem them when we added them into Docs X so we're just gonna stem them here we could have stemmed them when we added them but we're just gonna go ahead and stem them here so we're gonna say stammer don't stem and in this case we'll do W for W in and I guess in this case it's gonna be docs are just talk like that because doc will get all of the different docs in dot X now let's make sure we close that list off and I think that should be good okay so now what we're gonna do is we're gonna go through all of the different words that are in our document or in this word list now that our stemmed and we're gonna put either a 1 or a 0 into our bag of words depending on if it's in the main word list which we have here or not whereas say for and in this case we're gonna say W in words and again we're looping through this which means all of the words that we have I'm gonna say if what'd he call its if W in this case we're gonna say Doc's underscore X or just doc my bad um is it doc centers for X no it's W RDS my apologies guys so if W in W RDS I probably should have called this something else but it's fine meaning that the word exists in the current pattern that we're looping through then what we're gonna do is we're gonna say bag dot append 1 which means that yep this word is here so we put a 1 representing that this word exists otherwise what we're gonna do is say bag dot append 0 which means that you know this word isn't here so we're putting a 0 it's not here there we go so actually I lied when I was telling you we were using frequency we're just doing one hot encoded which means if the word exists we put a 1 we don't care how many times it exists we just put a 1 2 exists 5 times we still only put a 1 so my apologies about that now what we're gonna do is now generate the output and append these into training and output so the output has to be generated like this right where we have a bunch of different zeros or ones representing the tag that is well that thing so what we're gonna do is say output underscore row equals in this case list and then out empty and actually we're not gonna do list what I want to do is just make a copy of this list would have done that as well but we'll just do that it's a little bit nicer and we're gonna say output row in this case classes dot index and then this we're gonna say Docs underscore Y X and we'll talk about all this in a second equals one so what we're gonna do is we're gonna look through this classes list or actually what am i calling it classes for its labels my apologies we're gonna look through this labels list here we're gonna see where the tag is in that list and then we're gonna set that value to one in our output row and that should work if you guys to understand that leave a comment but it's fairly straightforward and now what we're gonna do is real estate training dot append and in this case we are simply going to append list that has deep bag and I'm going to go to output and we're gonna go into a pend the output row so now what we're gonna have is two lists so we're gonna have a training list that is gonna have a bunch of bags of words which are like just a list of zeros and ones and we're gonna have a bunch of outputs which again our list of zeros and ones now they are both one hot encoded and we talked about what that means okay so now that we've done that what we're gonna do is we're going to shuffle up our data actually we won't shuffle it because if we shuffle it then it's not gonna work from output and training but we're gonna do is we're gonna turn these into actually NP arrays just cuz if we need to work with numpy arrays for TF learns we say numpy dot array training and I'm gonna say output equals NP array and in this case codebook alright so that is actually all we're gonna do for this tutorial we wrote this bit of code here which essentially is gonna give us a bat bunch of bags of words and we're just getting our data ready to feed into our model so in the next video what we're gonna do is start writing the model hopefully we can get done with writing the model maybe do a little bit of testing on it and then in the final one or two videos of the series depending on how long this takes we're going to work on actually using the model and making a nice framework for it so that we can type much of stuff and have it look really awesome and nice so anyways that has been it for this video quick recap again we just did the bags words here if you guys to understand this leave a comment join discord I will try to help you out and I will see you in the next video