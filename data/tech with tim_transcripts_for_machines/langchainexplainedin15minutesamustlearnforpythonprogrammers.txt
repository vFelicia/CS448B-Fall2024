if you write code in Python and you're at all interested in AI then you need to check out Lang chain now this is a relatively new framework that allows you to build AI powered applications in both Python and JavaScript now we'll be looking at python here but the main benefit of Lang chain is that you can actually connect to external data sources databases apis and really any data that you'd like and you can very easily build applications that are context aware and that can reason using popular language models like chat GPT now in this video I'll give you a highspeed overview of Lang chain show you some of the simple features and capabilities and give you enough so that you can get started today building AI applications in Python in my opinion this is a mustlearn really cool module and surprisingly simple stick around for the whole video to see exactly how you can use this in your next python app so the first thing we need to do to work with Lang chain is install it now they have fantastic documentation that I'll link down below that will show you all of these steps but of course you can just follow along with me so since we're going to be installing this for python on we can see on this page here we have a few options pip install Lang chain pip install Lang chain llms or pip install Lang chain all now I'm going to recommend that you go with the second option here you do not need to install all the dependencies but we do want to install the dependencies related to the language model so that we can connect with those immediately so let's go ahead and copy that command and paste that in our terminal I am running on Windows if you're on Mac or Linux you're likely going to want to adjust this command to be pip 3 so I've gone ahead and installed that and we're just going to install a few modules that we're going to use for these specific examples so I'm going to do pip install and then open AI we're going to use this to connect with open AI again if you're on Mac or Linux you're going to want to change that to pip 3 now once we install open AI I am also going to install the EnV Library so pip install python d.v now this is going to allow me to load in environment variable file so I can store my API keys in a specific file that's kind of hidden from the code you'll see what I I mean in one second those are all the modules that we need Lang chain with llms we need open aai and then we need python. EnV now in the name of saving us a little bit of time I've already ridden a few examples and I'll just walk through the code with you so you can see exactly how this works now the first thing I want to mention is that before you start using laying chain in any of the llms you want to have a decent idea of how they work specifically you want to understand what they're good at how you can use them properly and when you should be cautious of their replies now I imagine most of you are going to be using chat GPT at least that's what I'll be doing for this tutorial and fortunately for you our video sponsor HubSpot actually has a free guide that shows you how to use chat GPT at work now this free guide gives you a comprehensive overview of exactly how chat GPT works it shares some best practices and it gives you some expert insights so you really know how to use this and get the full power out of it now I've left a link in the description so you can check it out completely for free now this resource provides a ton of knowledge and even has 100 actionable prompts that you can try today to really leverage chat gbt knowing this especially as a programmer is an absolute GameChanger and I want to give a big thank you to HubSpot for providing this resource and a ton of others completely for free make sure to check it out from the link in the description and a massive shout out to HubSpot so now that you're an expert at chat gbt after clicking on that resource we can dive into some of the code so the first thing I want to show you is just a very simple example of interacting with an open AI model in this case chat GPT and using it to make a prediction now you you can use all kinds of llms but obviously this is the one most of you know so it's the one I'll use in this video now first what I've done is I've created a EnV file here and I've placed my open AI API key I'll show you how to retrieve that in 1 second if you don't have it but this way I can actually put the key inside of here and then you guys can't see it while I'm going through the tutorial so I've just put my actual key inside of there and you can see that the way I'm loading in this API key is I'm importing from the EnV module the load. EnV what this does is look for a EnV file in the same directory as our python script and loads in any of the variables then what I do is access the open aai API key variable here so I have it and I pass that to my chat open aai kind of interface component whatever you want to call it now this will actually create a connection to open AI into chat GPT and I can simply use this by just running chat model. predict and then passing inside of here a string so if I go to my terminal and I run python simple example.py you can see that we're able to interact with GPT here it just takes a second and it responds to us says hello how can I assist you today very simple you can see how easy it is to actually use the module from python now let's dive into some more complex examples and before I forget if you need an openai API key then you can go to platform. open.com account API keys if you're not using it very much you're not going to have to pay for it I've used it a ton and I think I've paid 10 cents over the past few months anyways let's move on to the next example all right so moving on to the next example I just want to show you that there's there's actually a way to pass multiple messages and get a single reply or response based on all of those messages combined this can be useful because sometimes you have some pieces of context that you want to provide to the model you don't necessarily want it to give you a prediction or a reply based on that you want to actually feed some other text or some other information and get one kind of aggregated reply so in this case what I've done is just create an array of messages here I've specified that these are human messages now we can also have system messages assistant messages agent messages there's a bunch of different types they're all in the documentation I won't go over them here and you can see that what I'm doing is saying from now on 1 + 1al 3 use this in your replies and then I'm asking it some questions based on the context I just provided so what is 1+ 1 and what is 1 plus 1+ 1 so here I go chat model. predict messages pass in the messages and if I go ahead and run the code here we should see that it's actually going to be using this context so according to the new rule 1 + 1 = 3 therefore 1 + 1 + 1 would be equal to 3 + 1 which equals 4 kind of interesting how you can do that just wanted to show you that quick example so now we get into a slightly more interesting example where we can use another feature from langing chain known as a prompt template now you may have seen this before but quite frequently what we want to do is actually inject some data into a prompt and this prompt we've kind of crafted engineered and we know that it's going to give us a specific type reply so in this case what we've done is we've set up a template we say you are a helpful assistant that translates some input message to some output language and we put those inside of curly braces we then have a human template and this is just some text but we could also have something here as well what we then do is create a prompt template from uh link chain so we say chat prompt is equal to chat prompt template. from messages and then we can specify as many messages as we'd like to be a part of this larger template so what this does now is specify that this is a system message so this template right here kind of telling the model what it's about to do and then we have a human message which is what we're providing to it each time then what we can do is we can actually create the messages that we want to pass the model from our chat prompt so we say chat prompt. format messages and we pass in the variables corresponding with what we've written here inside of the template and they're automatically going to get injected inside now obviously in Python you could do this manually but having this API makes things a little bit easier so we have input language output language and text so that would make the template you are a helpful assistant that translates English to French and then we pass our human template which is I love programming in this simple example I know it seems very easy but when you get into more complex templates and complex tasks this can be quite useful then what we can do is we can predict messages again using the same chat model we had before and we can get the output so let's see what this does now if we go up here and go python prompt template it should give us whatever the translation of I love programming is in French and it does it says adore programmer again imagine you had like seven system messages a bunch of different variables you could create some really interesting applications here using this prompt template style so continuing we get even more complex here and what I'm now showing you is something known as an output parser again all this stuff you can write manually but having these hooks in this API is quite useful and can make your life a lot easier so a lot of times when you're using a model what you actually want to do is parse the output into a specific format for example it might give you a comma separated list it might give you AJ file it might give you some code but that's going to be in a string and typically what you'll want to do is actually take that string and parse it based on some rules so you can actually get the data that you'd like for example parsing a price or parsing a piece of code Json like I said right so what we can do here is actually write a custom output parser and you'll see why this is useful in a sec when we kind of combine it in the chain what this will do is return to us the parsed response in the format that we want so all we've done here is just overridden the base output parser which I've uted recorded up here we've just overridden this parse method and inside of here we take in some text we strip the text and then we split it at answer equals this now the reason that makes sense is because in my template I said you are a helpful assistant that solves math problems and shows your work output each step then return the answer in the following format answer equals and then the answer provided here make sure the output answer is in all lower cases and to have exactly one space and one equal sign following now I'm instructing the model to give me a specific reply in AIC specific format such that I can parse it so I get the data that I'm looking for and I could use that in the rest of my application then I have the human template which is just whatever the math problem is again I use my chat prompt template here and then what I do is format my message by providing some math problem in this case just a basic I guess quadratic formula thing that you need to solve then what we do is we predict messages using chat bottle and then I'm manually invoking the output parser here and parsing the reply you'll see in the next example how we actually combine this into a chain and make it even easier to use then what I'm doing is I'm parsing out the steps and the answer from my pars reply which is now a list that was split at answer equal to so you'll see now if I go ahead and run my code so python output parser dop and you can see that we get X is equal to 3 over2 or X is equal to 1 now if we also wanted to see the steps here then I could print out the steps and you can see how this could be useful because if we had some type of calculator we could allow maybe the student or whoever is using it to either view the steps or hide the steps or just view the answer or hide the answer because we've parsed that reply so let's have a look at this here gives us all of the different steps for solving the problem and then finally the answer down here all right so now we're going to kind of tie everything together and have a look at creating a chain now this is where laying chain really comes in handy and obviously you'd get much more complicated than this but you'll see in this example that we can combine all of those steps that we were doing manually before into a chain using some special syntax from Lang chain so just like before we have our model we have an output parser this time what it does is just parse a comma separated list and then we have a template it says you are helpful assistant who generates comma separated list a user will pass in a category and you should generate five objects in that category only return a comma separated list and nothing more we then have the human template again we create our prompt template this time though rather than manually calling all these different methods we create a chain now we use this pipe operator here which is special syntax in Lang chain which combines these three operators into one kind of main object that we can utilize to invoke so you see we have our chat prompt we have our chat model so the first thing we do right is we get the prompt we pass that to the model and then we pass the reply to the output parser and again this happens automatically when we call chain. invoke so now we just pass in what we need for our prompt so in this case we have an argument text which is equal to colors and we can print the result so let's go go here and give this run python chain. Pi so you can see we get our reply red blue green yellow purple and this is actually a python object that has been parsed for us so just to give you a bit more information on the capabilities of Lang chain let's have a look at one of the use cases they have in their documentation which is SQL now as I was saying Lang chain can also connect to data sources so they have a bunch of Integrations for connecting with popular databases and even Vector databases as well now in this case what we can actually do is use an llm to generate SQL queries so that we can ask in natural language for information from our database so imagine you had a large database you had a bunch of information and you wanted to actually provide an interface to your users so that they could use natural language to query the database and grab maybe information or analytics from their profile or whatever it is in this case you can see that we can build SQL queries query a SQL database and interact with the SQL database by using natural language and this is kind of the diagram we ask a question goes to the llm generates the query goes to SQL SQL returns that's the L and the L parses that and gives us the answer so in this case we can have a quick look at how this might work we say DB chain. run how many employees are there what it does is it generates the SQL query gets the result and then gives us an answer in a human readable format obviously a little bit easier said than done but you can see the simple connection that happens here connect to the SQL database connect to open AI create the chain here and then you can start using this exact example so the next use case they have here which is interesting thing is question answering so let's say we have a large piece of documentation maybe related to code maybe a PDF maybe a website what we can actually do is inject that into the model and then ask questions about that piece of data that the llm can respond to so you can see we have maybe some code some structured data some SQL pass that into llm and then interact with that directly by asking questions of the llm so where this becomes actually really interesting is we can ask the llm in natural language what it is that we want and then it can go and retrieve the correct piece of data if we set this up in the correct way to actually give us the answer so in some instances we don't even need to provide all the context to the llm immediately it can actually ask for the specific data it needs and send the correct request using whatever code it's generating this is a little bit more complicated than the example that they have here but you can kind of see how they're setting things up and how we would be able to input some data and then ask again in natural language questions about that that could be answered almost instantly now as a last example here I actually made a video on my channel a few weeks ago that injected memory into an llm so that we could actually remember and have context about all the previous replies and answers now I did this by making a Choose Your Own Adventure AI based game you guys can check it out from the link in the description it's a full tutorial walkthrough of how this works but pretty much what we can do is generate a Choose Your Own Adventure game that has multiple different variable paths and where you're going is going to be based on previous answers that you've had so if I said I was going left at the first step where I decided to have an axe as my weapon for the game the llm would actually use that context to continue to generate the story for me and make it context relevant really it's going to be cooler if you guys go check out that video and see exactly how that works but really interesting stuff you can do here and I did that all in Lang chain anyways with that said guys I'm going to wrap up the video here I hope you found this helpful and this gave you a quick overview of Lang chain and got you excited about using it in Python if you enjoyed make sure to leave a like subscribe to the channel and I will see you in the next one