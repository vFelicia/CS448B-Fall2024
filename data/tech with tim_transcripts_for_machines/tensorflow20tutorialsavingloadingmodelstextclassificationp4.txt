hey guys and welcome back so in today's video we're gonna be doing is talking about saving and loading our models and then we're gonna be doing a prediction on some data that doesn't come from this actual data set now I know this might seem kind of trivial we already know how to do predictions but trust me when I tell you this is a lot harder than it looks because if we're just taking in string data that means we have to actually do the encoding all of the preprocessing removing certain characters making sure that that data looks the same as the data that our neural network is expecting which in this case is a list of encoded numbers right or have encoded words that is essentially just numbers so what were you do to start is just save our model so let's talk about that now so up until this point every time we've wanted to make a prediction we've had to retrain the model now on small models like this that's fine you have to wait a minute two minutes but it's not very convenient when you have models that maybe take you days weeks months years to Train right so what you want to do is when you're done training the model you want to save it or sometimes you even want to save it like halfway through the training process this is known as checkpointing the model so that you can go back and continue to train it later now in this video we're just gonna talk about saving the model once it's completely finished but in future videos when we have larger networks we will talk about checkpointing and how you know how to load your or train your model in like batches of a different size data and all that so what I'm gonna start by doing is just actually bumping the vocabulary size of this model up to 88000 now the reason I'm doing that is just because for our next exercise which is gonna be making predictions on outside data we want to have as many words in our model as possible so that when it gets kind of some weirder words that aren't that common it knows what to do with them so I've done a few tests and I noticed that with the what he called with the vocabulary size bumped up it performs a little bit better so we're gonna do that so what I mean is we bump the vocabulary size and now after we train the model we need to save it now to save the model all we have to do is literally type the name of our model in this case model dot Save and then we give it a name so in this case let's call it model dot H 5 now H 5 is just like an extension that means I don't know it's like I honestly don't know why they use H 5 but it's the extension for a saved model and Cara's and tensorflow so we're just gonna work with that and that's as easy as this is it was just gonna save our model in binary data which means we'll be able to read it in really quickly and use the model when we want to actually make predictions let's go ahead and run this now and then we're gonna have the model saved and then from now on we won't have to continually train the model when we want to make predictions but I'm gonna say Python tutorial and I'll be right back once this finishes finishes running alright so the model is finished training notice that our accuracy is slightly lower than it was in the previous video really kind of a negligible difference here but anyways just notice that because we did bump the vocabulary size so anyways now that we've saved the model we actually don't have to go through this tedious process every time we run the code of creating and training and fitting the model and in fact we don't actually need to save it as well either here to load our model in now that's save and you can see the file right here with all this this big massive binary blob here all we have to do to load this in is just type one line now the line is whatever the name of your model is it doesn't matter I'm just gonna call it model is equal to in this case Kara's dot models dot load underscore model and then here you just put the name of that file so in this case model dot h5 now what's really nice about this as well as you can actually train a bunch of different models and tweak like hyper parameters of them and only save the best one what I mean by that is like maybe you mess with for example the amount of neurons in the second activation layer or something like that or in the second hidden layer and then you train a bunch of models you figure out which one has the highest accuracy and then you only save that one that's nice as well and that's something we could do like overnight you could run like your script for a few hours train a bunch of models figure out which one is the best only save and then use that one so anyways we're gonna load in this model notice that I've actually just commented out this aspect down here is we are not gonna use this anymore and now what we're gonna start doing is actually training or testing model on some outside data so I've gone ahead and picked a movie review for one of my favorite movies some of you guys can read this if you want but it's the Lion King absolutely loved that movie so I've decided to go with this this review was a 10 out of 10 review so a positive review we're gonna test our model on this one now actually did take this off like the IMDB website or whatever that's called but the data set that they use is different so this is you guys will see it why this works a little bit differently and what we have to do with this so this is in a text file so what I'm gonna do is load in the text file here in code and then get that big blob that's string and convert it into a form that our model can actually use so the first step to do this obviously is to get that string so we're gonna say with open and in this case I've called my file test dot txt and then I'm just gonna set the encoding because I was running into some issues here you guys probably don't have to do this I was gonna say UTF 8 which is just kind of a standard text encoding and we're gonna say as f now again the reason I use width is just because that means I don't have to close the file afterwards better practice if you want to use that and now I'm gonna say poor line in F dots read lines which essentially just means we're gonna each line in this case we only have one line but if we wanted to throw in a few more reviews in here and do some predictions on those that would be very easy to do by just keeping this code structure you just throw another line in there and now I'm just gonna say we're gonna grab this line and we're gonna start preprocessing it so that we can actually feed it to our model now notice that this when we read this in all we're gonna get is a large string but that's no good to us we actually need to convert this into an encoded list of numbers right and essentially we need to say okay so of that's a word what number represents that put that in a list same with all same with the same with animation right and we keep going and keep going pretty well for all the words in here and we also have to make sure that the size of our text is only at max 250 words because that's what we were using when we were training the data so it's expecting a size of that and if you give it something larger that's not gonna work or it might but you're gonna get a few errors with that so anyways the first step here is I'm going to say n line is equal to line dots and I'm gonna remove a bunch of characters that I don't want so I'm just gonna say dot replace I think this is the best way to do it but maybe not um and I'm gonna replace all the commas all of the period all of the brackets and all of the colons and I'll talk about more why we want to do that in just one second so we'll do daughter place I guess this daughter place should probably be outside the bracket and then workplace with a bracket with nothing and I know this is there probably is a better way to do this but for our purposes it's not really that important and finally we will replace all our colons with nothing as well now again the reason I'm doing this is because let's go here if you have a look for example when we split this because we're just gonna split this data by spaces and to get all the words what will end up happening is we're gonna get words like company comma we're gonna get words like I'm trying to find something it has a period like art dot and then a quotation mark right and we don't want those to be words in our list because there's no mapping for art period there's only a mapping for art which means that I need to remove all of these kind of symbols so that when we split our data we get the correct words now there'll be a few times where the split doesn't work correctly but that's okay as long as the majority of them are working well same thing with brackets right I can't have irons and then a closing bracket as one of my words so I need to get rid of that now this reminds me I need to remove quotation marks as well because they use quite a few of those in there I don't know why I closed that document so let's do that as well with one last replace this a daughter place in this case we'll actually just do backslash quotation mark and then again with nothing now I'm adding a dot strip here to get rid of that backslash N and now we're gonna say dot split and in this case we'll split out of space now I know this is a long line but that's all we need to do to remove everything and now we actually need to encode and trim our data down to 250 words so to encode our data I'm gonna say encode equals in this case and we're just literally will make a function called like review underscore encode and we'll pass in our end line now what review and code will do is look up the mappings for all of the words and return to us an encoded list and then finally what we're gonna do and we'll create this function in just a second don't worry it doesn't already exist is we're actually gonna use what we've done up here with this test data train data care as pre processing stuff we're just going to apply this to in this case our encoded data so we add those pad tags or we trim it down to what it needs to be so this case will say encode equals Kara's dot preprocessing instead of train data we'll just pass in this case actually a list and then encode inside it because that's what it's expecting to get a list of list all right so now that we've done that our final step would be to use the model to actually make a prediction so we're gonna say model dot predict and then in this case we'll pass it simply this encode right here which will be in the correct form now we'll save that under predict and then what we'll do is just simply print out the model so we'll say print or not the model sorry we'll print the original text which will be the review so in this case we'll print line and then we will print out the encoded review just so we can have a look at what that is and then finally we will print the prediction so what whether the model thinks it's positive or negative so we'll just say predict and in this case we'll just put 0 because we're only gonna be doing like one at a time right ok sweet so now the last thing that we need to do is just simply write this reviewing code function and it'll be good to go and start actually using our model so I'm just gonna say define review underscore encode this is gonna take a string we'll just call that s lowercase s and what we're gonna do in here is set up a new list that we're going to append some stuff into so I'm just gonna say like return let's just say like it encoded equals and then I'm gonna start this with 1 now the reason I start one in here is because all of our data here where it starts has a 1 so we're gonna start with 1 because we won't have added that in from the other way I hope you guys understand that just we're setting like a starting tag to be consistent with the rest of them and now what we're gonna do is we're gonna loop through every single word that's in our S here which will be passed in as elisa words we'll look up the numbers associated with those words and add them into this encoded list they're gonna say for word and in this case we're gonna say word a and s now you'll say if words in this case word underscore index and again we're going to use word underscore index as opposed to reverse word index because word index stores all of the words corresponding to the letters or not the letters the numbers which means that we can literally just throw our data into word index and it'll give us the number associated with each of those words so we're gonna say if word in word index then we'll say encoded God append and in this case we'll simply append in this case word index word now otherwise what we'll do is we'll say encoded dot append now what will happen is we're gonna check here if word if the word is actually in our vocabulary which is represented by word index which is just a dictionary of all the words corresponding to all the numbers that represent those words now if it's not what we'll do is we'll add in that unknown tag so that the program knows that this is an unknown word otherwise we'll simply add the number associated with that word now one last thing to do is actually just do word dot lower here just to make sure that if we get any words that have some weird capitalization they are still found in our vocabulary so like words at the beginning of a sentence and stuff like that uh and now with that being done I believe we're actually finished and ready to run this code so what's nice about this is now that we've saved the model we don't have to train it again so I can literally just run this and it should happen fairly quickly fingers crossed let's see all right must be a list of integrals found non iterable object so what air is that here um in code encoding coded code alright so print reviewing code ah well it would be helpful if I returned the encoded list and that would have been our issue there so let's run that one more time and see what we're getting there and there we go sweet so this is actually the review I know it's very really hard to read here but if you guys want to go ahead and read it feel free since it's on the Lion King it's obviously a positive review and then you could see this is what we've ended up with so our review has been translated into this which means we'd actually trimmed quite a bit of the review and you can see that wherever it says that is actually a word that we didn't know or that wasn't in our vocabulary for represents the that's why there's a lot of fours and then all the other words have there correspondence right now fortunately for us we picked a 88000 vocabulary which means that we can get indexes like 20,000 whereas before would all been under 10,000 and you can see that our prediction here is now 96% positive which means that obviously like we were going between 0 where 0 is a negative review and once a positive review so this classified correctly as very positive review and we could try this on all other cons reviews and see what we get but that is how you go about kind of transforming your data into the form that the network expects and that's what I'm trying to get you guys at right now is to understand that yes it's really easy when we're doing it with this kind of data that just comes in like IMDB like Cara's load data but as soon as you actually have to start using your own data there's a quite a bit of manipulation that you have to do and things that you might not think about when you're actually feeding it to the network and in most cases you can probably be sure that your network is not actually the thing that's happening incorrectly but it's the data that you're feeding it is not in the correct form and it can be tricky to figure out what's wrong with that data so with that being said that has been it for this video I hope you guys enjoyed that's gonna wrap up the text classification aspect here of neural networks I'm thinking about either going into convolutional neural networks which is image processing which is really interesting or the kind of game playing neural networks in the next few videos please let me know what you guys want in the comments down below and I'll make sure to accommodate that