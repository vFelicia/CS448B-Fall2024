With timestamps:

00:00 - in this video we're going to dive deeper
00:01 - into how neural networks and machine
00:03 - learning actually works behind the
00:04 - scenes and if you're learning machine
00:06 - learning you've probably come across a
00:08 - diagram that looks like this which often
00:10 - times if not most of the time confuses
00:13 - people because it's a bunch of circles
00:15 - and arrows and the explanation is
00:16 - usually just as confusing well it's
00:18 - simple you take your input circles and
00:20 - move them through your circles and you
00:22 - end up with your voila neural networks
00:24 - um and so that's not really a very good
00:28 - explanation at least it wasn't very good
00:29 - for me when i was learning so hopefully
00:31 - we're gonna break this down make it
00:32 - really simple because it's not really
00:34 - rocket science going into this it's not
00:36 - uh simple like abc's but it's not super
00:39 - complex either
00:41 - so if you guaranteed if you watch this
00:43 - video a few times it'll give you a big
00:44 - head start on what neural networks are
00:46 - and maybe even you'll be able to go and
00:48 - code some yourself if you have coding
00:50 - experience so here's the picture of the
00:52 - neural network we're actually going to
00:53 - use this diagram later but you're going
00:55 - to understand it so a good way to start
00:57 - is to block off this center layer we're
00:59 - going to call that the black box and
01:00 - this is what we've been doing if you've
01:02 - been watching my last two videos on
01:04 - machine learning let's just stick to the
01:06 - input and output data the training data
01:09 - right given this information here's the
01:11 - outcome these are our input dimensions
01:14 - and the output the outcome dimensions
01:16 - and then somewhere that black box in
01:18 - between runs over that information over
01:20 - and over and over again and each time it
01:22 - has a thousand knobs on that black box
01:24 - and it twists a little knob just a
01:26 - little bit at a time until while the
01:28 - black box is tuned and we can give it
01:30 - new input information and get the
01:32 - correct predicted outcome
01:34 - and so this video is going to dive into
01:35 - the hidden layers area that black box
01:38 - and how we configure it and what it does
01:41 - so there's some common configuration
01:43 - options no matter what machine learning
01:45 - or neural network library you're using
01:47 - there's some common configuration
01:48 - options that you're going to come across
01:50 - for configuring that neural network one
01:52 - how many hidden layers do we put in that
01:54 - black box we know we have an input layer
01:57 - and an output layer how many layers go
01:58 - in the middle and usually one is where
02:00 - you need to start off with that's pretty
02:02 - simple and straightforward and then how
02:04 - many nodes how many neurons go in that
02:06 - hidden layer
02:07 - and there's a few different ways you can
02:08 - kind of give yourself an answer to that
02:10 - there's not an exact science to it
02:13 - a good path is if your input and output
02:15 - dimensions are drastically different go
02:18 - somewhere in between if you have seven
02:20 - input values and two output values then
02:22 - yeah maybe three or four
02:24 - also you definitely want to be less than
02:26 - two times the input nodes if you get
02:28 - bigger than two times the input nodes
02:30 - you can get a situation called
02:31 - overfitting where you're just not gonna
02:33 - get accurate outcome
02:35 - also two-thirds the input nodes plus the
02:37 - output nodes that's a good kind of way
02:39 - to go and so if you kind of run the math
02:41 - on these three situations you'll kind of
02:43 - get some ideas on what how many nodes
02:45 - that hidden layer should have
02:47 - an activation function this is very
02:49 - important but i'm not going to go into
02:50 - it right now also learning rate and
02:52 - momentum are important but i'm not going
02:54 - to go into those right now
02:56 - lastly the iterations and the desired
02:58 - error level so iterations is how many
03:00 - times it goes over all the data in your
03:03 - data set um the desired error level is
03:05 - how accurate do you want this thing to
03:07 - be
03:08 - and depending on what you're going for
03:09 - basically training stops until you get
03:11 - to one or the other till you've gone
03:13 - through say 20 000 iterations or until
03:15 - your error level is 0.0001 and it's
03:18 - pretty accurate whatever you set the
03:20 - training is going to go till you get to
03:21 - one or the other of those so that's the
03:23 - configuration options that's actually
03:25 - not too bad that's how you tune your
03:26 - black box
03:28 - so if you have a little bit of knowledge
03:29 - as to what the black box is doing you
03:31 - can usually tune a neural network and
03:32 - move on from there and get good results
03:34 - so let's go to a real example using
03:36 - these circles and arrows that we have
03:38 - from before we have a data set this data
03:41 - set gives us fur color and weight of
03:43 - animals and also tells us if those
03:45 - animals were who's its or what's-its so
03:47 - we want to go through all of that data
03:50 - train our neural network to predict
03:51 - whose it's and what's it's based on fur
03:53 - color and weight um and so we've chosen
03:55 - three hidden layers based on the options
03:58 - that we showed you on the last slide
04:00 - so let's go ahead and then get our
04:02 - neural network initialize so to start
04:05 - off we basically want to create a stupid
04:08 - brain a brain that knows nothing so
04:10 - we're gonna just randomly create a bunch
04:12 - of weights you can see i'm going to get
04:13 - my mouse out here you can see we've got
04:15 - we've assigned a point one way to 0.38
04:18 - just completely random numbers and these
04:20 - neurons these hidden layer neurons and
04:22 - nodes are going to also get a bias
04:24 - consider them coming in with an opinion
04:27 - already even though we don't know
04:28 - anything
04:29 - about how fur color and weight
04:32 - apply to who's it's and what's it's
04:34 - we're just going to start making guesses
04:36 - based on our bias consider them the
04:38 - liberals conservatives and libertarians
04:40 - and we're going to get them all in a
04:41 - room show them a bunch of real-life data
04:44 - and hopefully at the end of the thing
04:45 - they'll all agree
04:47 - on what is truth i'm you know i'm not
04:49 - going to say anything i no okay nope go
04:51 - not going there moving on so that's our
04:53 - hidden layers we initialize the network
04:55 - with just a bunch of completely random
04:57 - weights
04:59 - and biases
05:00 - and then moving on from there we're
05:02 - going to start with the first entry in
05:04 - our data set so entry number one we have
05:07 - an animal for color 25 and weight of 15
05:09 - and it's a who's it it's a who's it one
05:11 - it's a what's it zero so now we start
05:14 - moving these input dimensions through
05:16 - the neural network it's called a
05:17 - feed-forward neural network for that
05:19 - reason so basically what we're going to
05:21 - do is we're going to take both of the
05:23 - input dimensions we're going to multiply
05:26 - them times the weight and then add the
05:28 - bias so here we go node 1 gets fur times
05:33 - 0.1 plus weight times 0.8 so we have our
05:36 - 0.1 weight and our 0.8 weight and then
05:39 - we're going to add the 0.71 bias so
05:42 - that's node number one so we're
05:44 - basically saying let's randomly
05:46 - guess
05:48 - how these two nodes uh affect our
05:51 - outcome let's just start with a guess
05:53 - and we're going to do that for each node
05:55 - a completely different set of weights
05:56 - and biases here a completely different
05:59 - set of weights and biases here you can
06:00 - see when we get down here we're taking
06:02 - fur times 0.57
06:04 - plus weight times 1 and we're adding a
06:07 - 0.09 bias so we're basically making a
06:10 - random set of guesses
06:12 - and then we do the magic and then we run
06:15 - all those through an activation function
06:16 - and here's where the activation function
06:19 - is important here's where the magic of
06:20 - neural networks comes in is because
06:22 - we're trying to figure out answers to
06:23 - non-linear data
06:25 - and so
06:26 - i'm going to do a little bit of a
06:27 - tangent over here and kind of show you
06:29 - the difference between linear and
06:30 - non-linear data linear data is a
06:32 - straight line we can say that hey
06:34 - um as the weight and the fur collar goes
06:37 - up we know it's a greater chance of
06:38 - being a who's it as it goes down it's a
06:41 - greater chance of being a who's it or
06:42 - what's it or hey if the fur color's high
06:45 - or if the weight is high it's always
06:46 - going to be a who's it right so you
06:48 - could use this with some species if it's
06:50 - big it's a dog if it's small it's a
06:52 - mouse if it's small
06:54 - smaller than a certain size there's no
06:55 - way it's going to be a dog that's linear
06:57 - information you don't really need a
06:58 - neural network to solve answers on these
07:01 - you just need a little bit of math and
07:02 - the data should be pretty clear as to
07:05 - how the input correlates with the output
07:09 - non-linear is like this it just turns
07:11 - out differently um it may be high here
07:14 - and high here
07:15 - or it may kind of follow some unusual
07:17 - curves that are really difficult to
07:19 - figure out um on your own and so that's
07:22 - basically where your activation
07:24 - functions come in we're going to use a
07:26 - sigmoid or a tan or tan h or a tinch
07:29 - depending on who you are and where you
07:30 - come from uh and so we're going to
07:32 - basically apply that that sum value
07:36 - we're going to run it through one of
07:37 - these functions and introduce
07:39 - non-linearity to our neural network to
07:42 - help us kind of find out the answers to
07:45 - those questions so we've taken the sum
07:47 - right we've taken the sum times the
07:49 - weights and we've added in our bias and
07:51 - we're going to run that through our
07:52 - activation function now we're going to
07:54 - get a non-linear guess here
07:57 - and then we're going to continue running
07:58 - through until we get to our outcome and
08:00 - let's just throw out a number here let's
08:02 - say we ran it through ran it through
08:04 - activation function came through again
08:05 - here and we end up with just a random
08:08 - drawn out of the hat guess
08:10 - that uh it's point three five one two
08:12 - that it's a who's it and point seven
08:14 - eight one that it's a what's it
08:15 - completely wrong right our neural
08:17 - network is stupid it came in with just
08:19 - biases and random numbers and made a
08:21 - guess that was way off so then it's back
08:24 - propagation time we're going to
08:26 - calculate the error in the delta which
08:28 - is the difference and we're going to
08:29 - adjust all the weights and the biases
08:31 - we're going to go backwards through each
08:33 - step and we're going to adjust these
08:35 - biases and these weights
08:37 - some we're not going to necessarily
08:39 - adjust them all the way but we're going
08:40 - to adjust them some how do we know how
08:43 - much we're going to adjust them well we
08:45 - do that through our configurations the
08:46 - first one is learning rate learning rate
08:48 - says how much should this step outcome
08:51 - affect our weights and our biases and
08:53 - learning rate is you almost want to
08:55 - think of it as
08:56 - personality types there's the slow
08:58 - calculating type of person where if you
09:00 - were to show them here's a cat here's a
09:03 - dog they're going to say hmm
09:05 - i i have some ideas show me some more
09:07 - and then after you show them a whole
09:09 - bunch they'll slowly
09:11 - lean into the answer and then come up
09:12 - with a very calculated answer that's a
09:15 - low learning rate a high learning rate a
09:18 - ridiculously high learning rate would be
09:20 - oh you showed me a cat and a dog i
09:21 - calculated the differences between the
09:23 - two i know the difference
09:25 - and then you show them
09:27 - a really fluffy cat and they think it's
09:29 - a dog because they had way too high of a
09:30 - learning rate so that's what learning
09:32 - rate is
09:33 - and the momentum also says how should
09:36 - our past outcomes affect our weights and
09:38 - biases so past outcomes could say hey
09:41 - that that initial weight there keeps
09:43 - giving us way too high of an answer so
09:46 - we're going to keep that weight kind of
09:47 - going downward no matter what at least
09:50 - we're going to give it a little bit of
09:51 - momentum into this next step so each
09:53 - step kind of takes learning rate and
09:55 - momentum into account and we don't just
09:57 - want to make snap judgments and we also
09:59 - don't want to learn too slowly because
10:00 - we learn too slowly it takes forever to
10:03 - train our neural network so right it's
10:05 - kind of that balance between how fast do
10:07 - we want to train our data versus how
10:10 - accurate do we want that data to be
10:11 - that's kind of where your learning rate
10:12 - and momentum come from so here's kind of
10:15 - an example formula we're going to take
10:17 - our learning rates we're going to
10:18 - multiply the multiply it by the
10:20 - difference and then multiply it by what
10:22 - the actual value is now and we're going
10:24 - to add that to our momentum
10:27 - times our past change amount and that is
10:29 - our current change amount so that's kind
10:32 - of an example formula for how we use
10:34 - learning rate and momentum to determine
10:36 - how much to change a given weight and a
10:38 - bias
10:40 - so now we go through the next piece of
10:41 - data yay we did it we did one iteration
10:44 - through the first piece of our data set
10:47 - we're not through our third iteration
10:48 - yet we're gonna go to the next piece of
10:50 - our data set where fur color is 15 and
10:52 - weight is 35 and that's a what's it and
10:55 - so we do that through each piece of data
10:58 - in our data set and boom that is
11:00 - considered one iteration so we have an
11:03 - iteration and we have an average error
11:06 - rate there and then we can determine do
11:08 - we go through and do more iterations
11:10 - have you requested more than that um or
11:12 - have you requested that to be
11:14 - an acceptable error rate and if not then
11:16 - we just keep going so that's how you
11:19 - train a neural network you give it
11:21 - inputs
11:22 - through the random weights which get
11:24 - more accurate over time
11:26 - biases activation function which is huge
11:30 - and then you're going to calculate your
11:31 - error and you're going to back propagate
11:33 - some adjustments here to the weights and
11:35 - biases and when you're done you get a
11:37 - set of weights that are accurate a set
11:40 - of biases that are accurate and you can
11:42 - then run any input into it it goes
11:45 - through the weights and biases and you
11:47 - get a pretty good guesstimation of what
11:49 - that output's going to be and that's
11:51 - neural networks and machine learning in
11:52 - a nutshell i hope this video helped in
11:55 - the next video we're gonna actually go
11:56 - back to brain.js and watch all these
11:59 - configurations and all these options in
12:01 - action

Cleaned transcript:

in this video we're going to dive deeper into how neural networks and machine learning actually works behind the scenes and if you're learning machine learning you've probably come across a diagram that looks like this which often times if not most of the time confuses people because it's a bunch of circles and arrows and the explanation is usually just as confusing well it's simple you take your input circles and move them through your circles and you end up with your voila neural networks um and so that's not really a very good explanation at least it wasn't very good for me when i was learning so hopefully we're gonna break this down make it really simple because it's not really rocket science going into this it's not uh simple like abc's but it's not super complex either so if you guaranteed if you watch this video a few times it'll give you a big head start on what neural networks are and maybe even you'll be able to go and code some yourself if you have coding experience so here's the picture of the neural network we're actually going to use this diagram later but you're going to understand it so a good way to start is to block off this center layer we're going to call that the black box and this is what we've been doing if you've been watching my last two videos on machine learning let's just stick to the input and output data the training data right given this information here's the outcome these are our input dimensions and the output the outcome dimensions and then somewhere that black box in between runs over that information over and over and over again and each time it has a thousand knobs on that black box and it twists a little knob just a little bit at a time until while the black box is tuned and we can give it new input information and get the correct predicted outcome and so this video is going to dive into the hidden layers area that black box and how we configure it and what it does so there's some common configuration options no matter what machine learning or neural network library you're using there's some common configuration options that you're going to come across for configuring that neural network one how many hidden layers do we put in that black box we know we have an input layer and an output layer how many layers go in the middle and usually one is where you need to start off with that's pretty simple and straightforward and then how many nodes how many neurons go in that hidden layer and there's a few different ways you can kind of give yourself an answer to that there's not an exact science to it a good path is if your input and output dimensions are drastically different go somewhere in between if you have seven input values and two output values then yeah maybe three or four also you definitely want to be less than two times the input nodes if you get bigger than two times the input nodes you can get a situation called overfitting where you're just not gonna get accurate outcome also twothirds the input nodes plus the output nodes that's a good kind of way to go and so if you kind of run the math on these three situations you'll kind of get some ideas on what how many nodes that hidden layer should have an activation function this is very important but i'm not going to go into it right now also learning rate and momentum are important but i'm not going to go into those right now lastly the iterations and the desired error level so iterations is how many times it goes over all the data in your data set um the desired error level is how accurate do you want this thing to be and depending on what you're going for basically training stops until you get to one or the other till you've gone through say 20 000 iterations or until your error level is 0.0001 and it's pretty accurate whatever you set the training is going to go till you get to one or the other of those so that's the configuration options that's actually not too bad that's how you tune your black box so if you have a little bit of knowledge as to what the black box is doing you can usually tune a neural network and move on from there and get good results so let's go to a real example using these circles and arrows that we have from before we have a data set this data set gives us fur color and weight of animals and also tells us if those animals were who's its or what'sits so we want to go through all of that data train our neural network to predict whose it's and what's it's based on fur color and weight um and so we've chosen three hidden layers based on the options that we showed you on the last slide so let's go ahead and then get our neural network initialize so to start off we basically want to create a stupid brain a brain that knows nothing so we're gonna just randomly create a bunch of weights you can see i'm going to get my mouse out here you can see we've got we've assigned a point one way to 0.38 just completely random numbers and these neurons these hidden layer neurons and nodes are going to also get a bias consider them coming in with an opinion already even though we don't know anything about how fur color and weight apply to who's it's and what's it's we're just going to start making guesses based on our bias consider them the liberals conservatives and libertarians and we're going to get them all in a room show them a bunch of reallife data and hopefully at the end of the thing they'll all agree on what is truth i'm you know i'm not going to say anything i no okay nope go not going there moving on so that's our hidden layers we initialize the network with just a bunch of completely random weights and biases and then moving on from there we're going to start with the first entry in our data set so entry number one we have an animal for color 25 and weight of 15 and it's a who's it it's a who's it one it's a what's it zero so now we start moving these input dimensions through the neural network it's called a feedforward neural network for that reason so basically what we're going to do is we're going to take both of the input dimensions we're going to multiply them times the weight and then add the bias so here we go node 1 gets fur times 0.1 plus weight times 0.8 so we have our 0.1 weight and our 0.8 weight and then we're going to add the 0.71 bias so that's node number one so we're basically saying let's randomly guess how these two nodes uh affect our outcome let's just start with a guess and we're going to do that for each node a completely different set of weights and biases here a completely different set of weights and biases here you can see when we get down here we're taking fur times 0.57 plus weight times 1 and we're adding a 0.09 bias so we're basically making a random set of guesses and then we do the magic and then we run all those through an activation function and here's where the activation function is important here's where the magic of neural networks comes in is because we're trying to figure out answers to nonlinear data and so i'm going to do a little bit of a tangent over here and kind of show you the difference between linear and nonlinear data linear data is a straight line we can say that hey um as the weight and the fur collar goes up we know it's a greater chance of being a who's it as it goes down it's a greater chance of being a who's it or what's it or hey if the fur color's high or if the weight is high it's always going to be a who's it right so you could use this with some species if it's big it's a dog if it's small it's a mouse if it's small smaller than a certain size there's no way it's going to be a dog that's linear information you don't really need a neural network to solve answers on these you just need a little bit of math and the data should be pretty clear as to how the input correlates with the output nonlinear is like this it just turns out differently um it may be high here and high here or it may kind of follow some unusual curves that are really difficult to figure out um on your own and so that's basically where your activation functions come in we're going to use a sigmoid or a tan or tan h or a tinch depending on who you are and where you come from uh and so we're going to basically apply that that sum value we're going to run it through one of these functions and introduce nonlinearity to our neural network to help us kind of find out the answers to those questions so we've taken the sum right we've taken the sum times the weights and we've added in our bias and we're going to run that through our activation function now we're going to get a nonlinear guess here and then we're going to continue running through until we get to our outcome and let's just throw out a number here let's say we ran it through ran it through activation function came through again here and we end up with just a random drawn out of the hat guess that uh it's point three five one two that it's a who's it and point seven eight one that it's a what's it completely wrong right our neural network is stupid it came in with just biases and random numbers and made a guess that was way off so then it's back propagation time we're going to calculate the error in the delta which is the difference and we're going to adjust all the weights and the biases we're going to go backwards through each step and we're going to adjust these biases and these weights some we're not going to necessarily adjust them all the way but we're going to adjust them some how do we know how much we're going to adjust them well we do that through our configurations the first one is learning rate learning rate says how much should this step outcome affect our weights and our biases and learning rate is you almost want to think of it as personality types there's the slow calculating type of person where if you were to show them here's a cat here's a dog they're going to say hmm i i have some ideas show me some more and then after you show them a whole bunch they'll slowly lean into the answer and then come up with a very calculated answer that's a low learning rate a high learning rate a ridiculously high learning rate would be oh you showed me a cat and a dog i calculated the differences between the two i know the difference and then you show them a really fluffy cat and they think it's a dog because they had way too high of a learning rate so that's what learning rate is and the momentum also says how should our past outcomes affect our weights and biases so past outcomes could say hey that that initial weight there keeps giving us way too high of an answer so we're going to keep that weight kind of going downward no matter what at least we're going to give it a little bit of momentum into this next step so each step kind of takes learning rate and momentum into account and we don't just want to make snap judgments and we also don't want to learn too slowly because we learn too slowly it takes forever to train our neural network so right it's kind of that balance between how fast do we want to train our data versus how accurate do we want that data to be that's kind of where your learning rate and momentum come from so here's kind of an example formula we're going to take our learning rates we're going to multiply the multiply it by the difference and then multiply it by what the actual value is now and we're going to add that to our momentum times our past change amount and that is our current change amount so that's kind of an example formula for how we use learning rate and momentum to determine how much to change a given weight and a bias so now we go through the next piece of data yay we did it we did one iteration through the first piece of our data set we're not through our third iteration yet we're gonna go to the next piece of our data set where fur color is 15 and weight is 35 and that's a what's it and so we do that through each piece of data in our data set and boom that is considered one iteration so we have an iteration and we have an average error rate there and then we can determine do we go through and do more iterations have you requested more than that um or have you requested that to be an acceptable error rate and if not then we just keep going so that's how you train a neural network you give it inputs through the random weights which get more accurate over time biases activation function which is huge and then you're going to calculate your error and you're going to back propagate some adjustments here to the weights and biases and when you're done you get a set of weights that are accurate a set of biases that are accurate and you can then run any input into it it goes through the weights and biases and you get a pretty good guesstimation of what that output's going to be and that's neural networks and machine learning in a nutshell i hope this video helped in the next video we're gonna actually go back to brain.js and watch all these configurations and all these options in action
