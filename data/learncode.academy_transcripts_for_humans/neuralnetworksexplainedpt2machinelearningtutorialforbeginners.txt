With timestamps:

00:00 - in the last video i showed you a
00:01 - presentation on how a neural network
00:03 - works and what's going on underneath the
00:05 - hood so to kind of help illustrate it in
00:07 - a different way we're going to do a
00:08 - real-life training scenario here and
00:11 - we're going to actually do a ton of
00:12 - console logging and output
00:14 - to actually help you see what it's doing
00:16 - to the real data that we're training so
00:19 - i've gone ahead and hacked brain.js and
00:21 - added tons of console logging that
00:23 - should really help us understand what
00:25 - it's doing to our training data and then
00:27 - if you've been following the examples
00:28 - we've done in the past videos all of
00:30 - which will be in the description
00:32 - then you'll really understand this this
00:34 - is very simple what we're doing we're
00:35 - creating a neural network and we're
00:37 - training it with one piece of data and
00:39 - we're only doing one training iteration
00:40 - which of course is silly in real life
00:42 - because normally we'd have tons of
00:44 - training data and 20 000 or so
00:47 - iterations but we don't want to do that
00:49 - because that'd be just log bonanza so
00:51 - we're gonna go ahead train this one
00:53 - piece of data one time and let's show
00:55 - you what happens so there we go i've
00:57 - gone ahead and run it and let me scroll
00:59 - back through my logs yes this is all a
01:01 - mess
01:02 - and before i start going through this
01:03 - i'm going to take the final log output
01:05 - and put it in a gist and add it to the
01:07 - description here
01:08 - because if you step through this it
01:10 - actually is really really helpful this
01:12 - is doing things like this actually
01:13 - helped me understand how neural networks
01:15 - work when i was learning them
01:17 - so what we've done is we've ta we've
01:18 - started training it says hey here's our
01:20 - training data we have basically one
01:22 - piece of data and it's got an input and
01:25 - an output so let's go ahead and run
01:26 - input set 0 which is this whole thing
01:29 - right here we only have one piece one
01:31 - input set
01:32 - and so then we're going to look at our
01:33 - neural network hey layer two which is
01:36 - this layer right here layer number two
01:38 - it has three nodes layer three has two
01:41 - nodes
01:42 - so layer two has three nodes let's start
01:44 - with node zero so we're gonna calculate
01:46 - this node then calculate this node then
01:49 - calculate this node and then we're gonna
01:50 - move on to these calculate this
01:52 - calculate this
01:53 - so let's start with node zero hey look
01:55 - we've initialized it with a bias there's
01:57 - our bias and we've also initialized
01:59 - weights for each of our input values so
02:01 - zero has been given a weight
02:04 - and dimension one has been given weight
02:06 - so dimension zero which happens to be
02:08 - zero has been given this weight and that
02:10 - so we're going to calculate the node
02:12 - value which is zero times this
02:15 - one times this number we're going to add
02:18 - those two calculations together and then
02:20 - add our bias as well
02:22 - and we end up with a value for that node
02:25 - but we're not done yet then we're going
02:26 - to run that through the sigmoid function
02:28 - because our neural network by default
02:30 - with brain will use the sigmoid function
02:32 - and there we go that is the value for
02:35 - our node node one is done boom let's go
02:37 - start with node two which i'm actually
02:40 - calling node one there
02:42 - uh node zero node one and now node one
02:44 - we're gonna do the exact same thing
02:45 - different biases different weights same
02:48 - values same input values
02:50 - zero and one now we're going to go on to
02:52 - node two same thing new bias new weights
02:55 - same input values of zero one you can
02:57 - see we have all of our sigmoid output
03:00 - values done so now let's move on to
03:02 - layer three layer three has two nodes
03:05 - two output layers for our neural network
03:07 - so there we go we have our biases and
03:09 - look we have three inputs now because we
03:11 - have three of those middle hidden layers
03:14 - here's those outputs you can see 0.45
03:16 - 0.47 and 0.459
03:19 - for example
03:20 - node number 3 has 0.459 there's that
03:24 - number so that's our input to the next
03:26 - layer
03:27 - and we're going to multiply those by the
03:29 - weights we're going to sigmoid them and
03:30 - now our two output nodes have a final
03:34 - value so there's our final value let's
03:36 - go ahead and calculate the error the
03:39 - delta uh and then we're going to go
03:41 - ahead and back propagate and we're gonna
03:43 - go make some adjustments to the weights
03:45 - here's our weight adjustment formula
03:48 - it's our learning rate times the delta
03:51 - times the value of the current weight
03:53 - and we're also going to add the momentum
03:54 - plus the prior change that we made which
03:57 - is going to be zero right now because we
03:58 - didn't make a prior change in the last
04:00 - step
04:01 - so this is basically just doing that for
04:04 - step number one this is gonna output to
04:07 - zero
04:08 - so then we're going to go ahead and make
04:10 - an adjusted change point zero zero
04:13 - two we've modified our weight
04:16 - for this layer we've modified the weight
04:18 - and we've modified our bias as well
04:22 - down here we've adjusted the bias
04:24 - by that much so we adjusted our bias we
04:26 - adjusted our weight let's go down to the
04:28 - next layer
04:30 - and let's go down to the next layer
04:31 - we've adjusted all of our biases and our
04:33 - weights we're ready for the next
04:34 - iteration so let's go change this now
04:36 - let's run two iterations
04:40 - and let's look at what happens on
04:41 - iteration number two it's going to look
04:44 - exactly the same except for the weights
04:45 - and the biases are all going to be
04:47 - different
04:48 - the weights have changed the biases have
04:49 - changed ever so slightly because our our
04:52 - learning rate is 0.03 we don't want to
04:54 - make these brash guesses and say oh you
04:57 - showed me one piece of data one time
04:58 - well now i know exactly what the answer
05:00 - is um if you'll notice now our momentum
05:03 - times change has changed a little bit
05:06 - since we had a prior change value we
05:08 - actually have a little bit of momentum
05:10 - right now so last iterations change
05:12 - value is going to slightly affect this
05:15 - iterations change value basically if
05:17 - this number was on its way down we're
05:19 - going to make sure it keeps it wants to
05:20 - go down at least a little bit
05:22 - and so that's basically how that works
05:24 - that is two iterations on one piece of
05:27 - data and if we were to then go ahead and
05:30 - change this
05:31 - let's go ahead and give it two pieces of
05:33 - data let's say that equates to a one and
05:35 - a one
05:37 - well now we're gonna do four complete
05:39 - loops let me scroll up here pardon with
05:42 - me
05:43 - we're gonna do four loops through our
05:45 - neural network we're going to say hey
05:47 - we're training we have
05:48 - two pieces of data
05:50 - here and so we're doing two iterations
05:53 - so it's going to take these two values
05:56 - put them in the inputs run them through
05:57 - the neural network then it's going to
05:59 - take these two input values input sets
06:02 - run them through the neural network
06:04 - that's training iteration one and then
06:06 - it's going to take all of them and run
06:08 - it through training iteration two again
06:10 - and we're going to keep doing those
06:11 - iterations until we've either reached
06:13 - the desired error level or the desired
06:16 - count of iterations when we're done here
06:18 - i can console log once we have a trained
06:21 - network i can console log let's
06:24 - stringify this because it's going to be
06:25 - json
06:28 - we're going to json stringifynet.2json
06:31 - so i'm going to output the json of what
06:33 - i have here and i'm going to stringify
06:35 - that so it's easy to see so when our
06:36 - train network is done this is actually
06:38 - what we've come up with we have some
06:40 - sizes here we can see our sizes are two
06:43 - three two you can change this in the
06:44 - options but that's what it shows
06:47 - and then here's our layers our final
06:49 - train network has biases and weights for
06:52 - all of our different layers biases and
06:54 - weights you can see that we used a
06:56 - sigmoid activation function which is the
06:58 - default and then these were our training
07:00 - options and our default error threshold
07:03 - there we go we've trained a neural
07:05 - network and this right here this json
07:08 - object is our trained neural network we
07:11 - can now take two pieces of input without
07:14 - knowing the output run it through this
07:16 - network and the output
07:18 - should work i say should because clearly
07:20 - this is just made up data and we only
07:22 - did two iterations
07:24 - so that's how a neural network works
07:26 - hopefully you're comfortable now looking
07:28 - at a diagram like this and understanding
07:30 - what's actually going on how it's
07:32 - calculating really complex outcomes from
07:35 - from data that would be hard to predict
07:37 - otherwise neural networks are awesome
07:39 - they're tons of fun
07:41 - getting to them the documentation for
07:42 - brain.js is great it's really easy to
07:44 - pick different activation functions
07:47 - to change the amount of hidden layers in
07:49 - your network play around with it have
07:51 - fun with some data sets that are
07:53 - available online and have a great day
08:01 - you

Cleaned transcript:

in the last video i showed you a presentation on how a neural network works and what's going on underneath the hood so to kind of help illustrate it in a different way we're going to do a reallife training scenario here and we're going to actually do a ton of console logging and output to actually help you see what it's doing to the real data that we're training so i've gone ahead and hacked brain.js and added tons of console logging that should really help us understand what it's doing to our training data and then if you've been following the examples we've done in the past videos all of which will be in the description then you'll really understand this this is very simple what we're doing we're creating a neural network and we're training it with one piece of data and we're only doing one training iteration which of course is silly in real life because normally we'd have tons of training data and 20 000 or so iterations but we don't want to do that because that'd be just log bonanza so we're gonna go ahead train this one piece of data one time and let's show you what happens so there we go i've gone ahead and run it and let me scroll back through my logs yes this is all a mess and before i start going through this i'm going to take the final log output and put it in a gist and add it to the description here because if you step through this it actually is really really helpful this is doing things like this actually helped me understand how neural networks work when i was learning them so what we've done is we've ta we've started training it says hey here's our training data we have basically one piece of data and it's got an input and an output so let's go ahead and run input set 0 which is this whole thing right here we only have one piece one input set and so then we're going to look at our neural network hey layer two which is this layer right here layer number two it has three nodes layer three has two nodes so layer two has three nodes let's start with node zero so we're gonna calculate this node then calculate this node then calculate this node and then we're gonna move on to these calculate this calculate this so let's start with node zero hey look we've initialized it with a bias there's our bias and we've also initialized weights for each of our input values so zero has been given a weight and dimension one has been given weight so dimension zero which happens to be zero has been given this weight and that so we're going to calculate the node value which is zero times this one times this number we're going to add those two calculations together and then add our bias as well and we end up with a value for that node but we're not done yet then we're going to run that through the sigmoid function because our neural network by default with brain will use the sigmoid function and there we go that is the value for our node node one is done boom let's go start with node two which i'm actually calling node one there uh node zero node one and now node one we're gonna do the exact same thing different biases different weights same values same input values zero and one now we're going to go on to node two same thing new bias new weights same input values of zero one you can see we have all of our sigmoid output values done so now let's move on to layer three layer three has two nodes two output layers for our neural network so there we go we have our biases and look we have three inputs now because we have three of those middle hidden layers here's those outputs you can see 0.45 0.47 and 0.459 for example node number 3 has 0.459 there's that number so that's our input to the next layer and we're going to multiply those by the weights we're going to sigmoid them and now our two output nodes have a final value so there's our final value let's go ahead and calculate the error the delta uh and then we're going to go ahead and back propagate and we're gonna go make some adjustments to the weights here's our weight adjustment formula it's our learning rate times the delta times the value of the current weight and we're also going to add the momentum plus the prior change that we made which is going to be zero right now because we didn't make a prior change in the last step so this is basically just doing that for step number one this is gonna output to zero so then we're going to go ahead and make an adjusted change point zero zero two we've modified our weight for this layer we've modified the weight and we've modified our bias as well down here we've adjusted the bias by that much so we adjusted our bias we adjusted our weight let's go down to the next layer and let's go down to the next layer we've adjusted all of our biases and our weights we're ready for the next iteration so let's go change this now let's run two iterations and let's look at what happens on iteration number two it's going to look exactly the same except for the weights and the biases are all going to be different the weights have changed the biases have changed ever so slightly because our our learning rate is 0.03 we don't want to make these brash guesses and say oh you showed me one piece of data one time well now i know exactly what the answer is um if you'll notice now our momentum times change has changed a little bit since we had a prior change value we actually have a little bit of momentum right now so last iterations change value is going to slightly affect this iterations change value basically if this number was on its way down we're going to make sure it keeps it wants to go down at least a little bit and so that's basically how that works that is two iterations on one piece of data and if we were to then go ahead and change this let's go ahead and give it two pieces of data let's say that equates to a one and a one well now we're gonna do four complete loops let me scroll up here pardon with me we're gonna do four loops through our neural network we're going to say hey we're training we have two pieces of data here and so we're doing two iterations so it's going to take these two values put them in the inputs run them through the neural network then it's going to take these two input values input sets run them through the neural network that's training iteration one and then it's going to take all of them and run it through training iteration two again and we're going to keep doing those iterations until we've either reached the desired error level or the desired count of iterations when we're done here i can console log once we have a trained network i can console log let's stringify this because it's going to be json we're going to json stringifynet.2json so i'm going to output the json of what i have here and i'm going to stringify that so it's easy to see so when our train network is done this is actually what we've come up with we have some sizes here we can see our sizes are two three two you can change this in the options but that's what it shows and then here's our layers our final train network has biases and weights for all of our different layers biases and weights you can see that we used a sigmoid activation function which is the default and then these were our training options and our default error threshold there we go we've trained a neural network and this right here this json object is our trained neural network we can now take two pieces of input without knowing the output run it through this network and the output should work i say should because clearly this is just made up data and we only did two iterations so that's how a neural network works hopefully you're comfortable now looking at a diagram like this and understanding what's actually going on how it's calculating really complex outcomes from from data that would be hard to predict otherwise neural networks are awesome they're tons of fun getting to them the documentation for brain.js is great it's really easy to pick different activation functions to change the amount of hidden layers in your network play around with it have fun with some data sets that are available online and have a great day you
