00:00 - hey guys and welcome back to the machine
00:02 - learning tutorial with Python so in
00:04 - today's video we're going to be
00:05 - finishing up with k-means this will be a
00:07 - quick video we're just going to do a
00:08 - really simple implementation of k-means
00:10 - I'm kinda gonna leave you on a bit of a
00:12 - cliffhanger here in terms of I'm not
00:14 - going to explain everything I do in here
00:15 - I'm gonna give you kind of some further
00:17 - reading for those of you that are
00:18 - interested in looking at that and then
00:20 - as we move into more unsupervised
00:22 - learning algorithms so as I get into
00:25 - neural networks which will be another
00:26 - series coming soon excuse me that I'll
00:29 - be talking more about specifically what
00:31 - some of the accuracy measures mean for
00:34 - this data set because unlike other data
00:37 - sets actually more difficult to kind of
00:39 - test this for accuracy and validity so
00:42 - you'll see that in just a second so
00:44 - essentially these are my imports here
00:46 - again all this stuff will be up on Tech
00:47 - with Tim net if you guys want to copy
00:49 - that code the code that I showed in the
00:50 - last video that ran all of the woody
00:54 - code like that image stuff it'll be up
00:55 - on tech with Tim done that as well that
00:56 - you guys can just go ahead and copy with
00:58 - that so go ahead
00:59 - link with me the description and you can
01:01 - copy the code from there if you don't
01:02 - want to type it out with me but
01:03 - essentially we're gonna load in our data
01:05 - so we've got low digits here from SK
01:07 - learn this is from datasets same thing
01:10 - as like loaned what did we load before I
01:12 - forget which one we load it before we
01:14 - use some kind of a scalar and data set I
01:16 - forget what it was but essentially we
01:17 - use that so we'll do is let's say digits
01:19 - equals load underscore data and now load
01:25 - digits is what it's called isn't it load
01:28 - digits and now what we're gonna do is
01:30 - we're gonna say data equals scale talk
01:34 - about this in a second and we'll say
01:36 - digits dot data so essentially this dot
01:40 - data part right is all of our features
01:43 - so we're gonna scale all of our features
01:45 - down so that they're within the value
01:48 - negative one and one and the reason we
01:50 - do that is because our digits by default
01:52 - are gonna have large values I believe
01:53 - it's an RGB value or might be like a
01:56 - grayscale value I honestly don't know
01:57 - but they're gonna be large so by scaling
02:00 - them down we're gonna save time on the
02:02 - computations especially cuz we're doing
02:04 - including distance between points so
02:06 - having smaller values will just be
02:07 - better and it'll lead kind of to less
02:09 - outliers and all that all right so it'll
02:11 - make things faster essentially as we're
02:12 - doing the scan
02:14 - now we're gonna get our our labels so
02:16 - what we'll do is say y equals data dot
02:19 - targets like that target or targets I
02:23 - want to say it's target's but we'll see
02:25 - anyways so we've got our targets now and
02:27 - what I'm gonna do now is just set the
02:29 - amount of clusters that we're gonna look
02:30 - for the amount of centroids to make so
02:33 - we could do this fancy thing or we do
02:35 - like NP dot unique and we get of data
02:39 - targets of Y and we do the length of
02:43 - that and it would be like the dynamic
02:44 - way to do it if we were going to change
02:47 - this data set or we could just type 10
02:49 - because we're gonna have 10 digits so I
02:51 - mean feel free I just want to show you
02:52 - that way in case you want to see how you
02:54 - get it the amount of different classes
02:56 - or like classifications for data set
02:59 - dynamically you can use what I just did
03:00 - so now what we're gonna do is we're
03:03 - gonna get the amount of instances like
03:04 - the amount of numbers that we have that
03:06 - we're gonna classify and what do you
03:09 - call the amount of features that go
03:10 - along with that data so to do this I'm
03:12 - just gonna say samples features think
03:17 - that's how you spell features is equal
03:19 - to we'll say data dot shape the reason
03:22 - this works is because we have shape it
03:24 - kind of looks something like this like
03:25 - it'll look like let's say we have like a
03:26 - thousand instances and like it's by 728
03:29 - then it'll just decompose this for us
03:31 - and the side we can do that in Python
03:32 - pretty straightforward you guys probably
03:34 - already know what and now what we're
03:36 - gonna do is we're actually I'm gonna
03:37 - bring in a function that we used in the
03:41 - last video you saw when I did all that
03:43 - kind of the images were popping up and
03:46 - all that and we saw like the centroids
03:47 - and it was like a nice graph on that
03:48 - plot lib well they have a really nice
03:50 - way of scoring this and you know I could
03:53 - come up with my own way to score this
03:54 - and do the accuracy but why don't we
03:56 - just take it straight from SK learn as
03:58 - that's the module reusing so I'm just
04:00 - gonna copy this in and we'll talk about
04:02 - kind of what this is doing it's it's a
04:04 - big function just be aware okay so
04:07 - essentially this actually reminds me now
04:09 - I have to import metrics from SK learn
04:11 - so say from SK learn import metrics and
04:16 - SK learn has a bunch of functions in
04:19 - there that will automatically score are
04:22 - like supervised learning or unsupervised
04:24 - learning algorithms now we can see that
04:26 - we
04:27 - ton of different ones here it's a
04:28 - completeness score V measure score
04:30 - adjusted ran score mutual info
04:32 - silhouette score and honesty I don't
04:35 - know what all of them do there's a lot
04:37 - of them here all I know is kind of the
04:39 - range to what you're looking for for
04:41 - some of these different scores because
04:43 - they have like a crazy math background
04:45 - behind how they score the model and get
04:48 - like the best accuracy and all of them
04:49 - represent kind of a different thing now
04:52 - essentially what we do um here right is
04:56 - we have this bench k-means and we're
04:58 - gonna create a classifier down here I'm
04:59 - just gonna call this function on our
05:01 - classifier it's gonna print out all this
05:04 - information well this allows us to do
05:05 - essentially is train like a ton of
05:07 - different classifiers and just score
05:08 - them by calling this function so
05:10 - essentially what we do is we give it the
05:12 - classifier it's gonna fit our data which
05:14 - is another argument to that classifier
05:17 - and then it's just gonna use a bunch of
05:19 - different things to score it
05:21 - so essentially forward like homogeneity
05:23 - score I think that's how you say it
05:25 - we're gonna take our Y values which are
05:27 - up here right so all our targets we're
05:29 - gonna compare them to the labels that
05:31 - are estimated gave for each of our data
05:34 - now remember because this is an
05:36 - unsupervised learning algorithm and we
05:38 - don't give it the Y values when we train
05:40 - it automatically generates a Y value for
05:43 - every single test data point that we
05:44 - give it so we don't actually have to
05:46 - split into test and training data
05:48 - because well it never it doesn't know to
05:50 - start what our test data is so we can
05:53 - actually just compare the test data
05:55 - labels to what our estimator or our
05:58 - classifier estimated right like what it
06:00 - predicted each label was and that allows
06:03 - us to kind of train on maybe less data
06:05 - per se because we don't need to have
06:08 - like that training data testing data I
06:09 - do like that split whatever split test
06:12 - train thing that we used in all the
06:14 - other videos so that's good to know for
06:16 - this metric here all this is doing
06:18 - essentially is we're just saying that
06:20 - when we do like the silhouette score
06:22 - we're gonna use Euclidean distance and
06:24 - that's just like the absolute distance
06:25 - between two points or two vectors in a
06:28 - space there's some other distances that
06:30 - we could mess around with but we're just
06:31 - gonna use Euclidian for now so to make
06:33 - our classifier we're gonna say
06:34 - classifier equals k-means now this takes
06:38 - a few different per
06:40 - is this incorrect is it a capital M yes
06:43 - okay so for this classifier we need to
06:46 - define first of all them as centroids we
06:49 - need to give it the amount of times
06:50 - we're gonna actually haven't run the
06:51 - classifier we can give it a max one
06:53 - iterations there's there's a ton of
06:54 - different parameters and I'll actually
06:55 - I'll show you here so if I go to this
06:58 - one you can read through like all of the
07:00 - different parameters and they kind of go
07:02 - like this okay so the first one we're
07:04 - gonna do is n underscore clusters and
07:06 - this is just gonna be set equal to
07:08 - however many things we're trying to
07:09 - classify right so for the clusters we'll
07:11 - say under square clusters and this is
07:13 - gonna be the same as like the amount of
07:14 - centroids essentially we'll say is equal
07:16 - to K and that's what we've defined up
07:18 - here as ten okay what else do we need
07:20 - let's go back here and I'm just gonna
07:22 - read these off because obviously don't
07:23 - remember all of them an it okay so what
07:25 - this will do is remember how I was
07:27 - saying we can have our centroids like
07:29 - those little X's in random positions
07:32 - when we generate them well that is
07:34 - correct but we can also have them in
07:36 - somewhat of them more somewhat of in a
07:40 - way that makes a bit more sense and I
07:42 - don't know exactly how it does it
07:43 - mathematically but I'm pretty sure it
07:45 - just lays them out so they're like equal
07:47 - distance from each other on the grid or
07:49 - on in the space and we can do that if we
07:51 - just set k-means plus plus so you can
07:54 - play around with either choosing random
07:56 - or k-means plus plus and see if you're
07:58 - getting a better classifier it shouldn't
08:00 - make a massive difference but k-means
08:02 - plus plus essentially is just gonna
08:03 - change the location of your initial
08:06 - centroids so that maybe it runs a bit
08:08 - faster you don't have to iterate as many
08:09 - times so for a net I'm actually just
08:11 - gonna use random for right now it
08:13 - doesn't really matter that much and an
08:15 - internet I think it's a net equals a
08:18 - random okay so let's go back here so
08:21 - what else do we need n in it okay so
08:23 - this is the amount of times we're gonna
08:25 - run the algorithm so what actually is
08:26 - gonna happen is cuz we're randomly or
08:29 - because sorry we are randomly kind of
08:31 - placing these centroids we might
08:34 - sometimes get a better result by running
08:36 - the algorithm with a different random
08:39 - location for the centroid so it's
08:40 - essentially what this is saying
08:41 - all right saying the number of times
08:42 - k-means algorithm will run with
08:43 - different centroid seeds that's like how
08:46 - many times are gonna randomly generate
08:47 - the centroids for the first iteration so
08:50 - we can run this ten times and then
08:53 - essentially it's going to
08:53 - take the best one and that's gonna be
08:55 - our classifier I hope that makes sense
08:57 - to you so for n in it we can set it
09:00 - equal to ten just so we kind of have
09:02 - this here that's the default value you
09:04 - can increase it obviously if you
09:05 - increase it it's gonna take longer if
09:06 - you decrease it it's gonna be shorter
09:08 - max iterations so I recommend you just
09:11 - leave this as 300 but essentially
09:13 - remember I was saying we're gonna
09:14 - continually keep repeating the process
09:15 - until eventually nothing changes well to
09:19 - do that could take a very long time
09:21 - especially if we have a ton of data so
09:23 - this is automatically gonna cap us at
09:25 - 300 iterations now if you have if you
09:28 - want the best possible classifier and
09:31 - you want to make sure that it doesn't
09:33 - doesn't matter our time how much time it
09:34 - takes you can set this to like actually
09:36 - I don't know if there's like an infinite
09:38 - number but I think you just said it's
09:40 - like a very high value and hopefully it
09:42 - never even gets to that because it'll
09:43 - just have like a perfect classifier by
09:45 - that point does that make sense okay so
09:48 - these ones now we're kind of going into
09:49 - some more super like hyper parameters
09:52 - that I'm not really gonna talk about
09:53 - verbose like if you guys want to read
09:55 - through this I'll I have all the links
09:56 - on my website and in the description so
09:58 - you can see but essentially that's all
09:59 - we kind of need for our classifier so
10:02 - now we will pass this actually to bench
10:04 - k-means so we'll say bench k-means we'll
10:07 - give it our classifier which can be CLF
10:09 - we'll give it the name which I'm just
10:11 - gonna say is one in this case and we'll
10:12 - give it our data which is just called
10:14 - data and now if I don't know if I have
10:17 - this as the right configuration right
10:18 - now let's edit this k-means tutorial I
10:21 - believe it's working file yes it is so
10:23 - it'll apply that and let's just run this
10:25 - and see what we're getting MP dot an
10:28 - array object has no attribute targets
10:31 - pretty sure its target let's try this y
10:35 - equals target data this I think it might
10:39 - be digits data did just saw targets
10:43 - let's try this one no targets let's try
10:46 - target sorry guys no actually target
10:50 - okay give me one second yeah so I
10:53 - probably help if I spell target
10:55 - correctly Wow all right so target and in
10:59 - it go an unexpected keyword and
11:00 - underscore cluster so I believe that
11:02 - should be clusters and there we go okay
11:07 - so
11:07 - awesome so now it's printing out all of
11:09 - our accuracy scores for us okay so we
11:12 - have six nine four one seven which
11:14 - actually I believe is just giving us ya
11:15 - the inertia okay and then we have all of
11:19 - these different scores which are will
11:20 - represent like homogeneity completeness
11:22 - V measure adjusted R and all that now
11:25 - I'm not gonna talk about what these mean
11:26 - essentially the higher the better for
11:28 - most of them not all of them but if you
11:30 - want to read and I recommend you do I'll
11:32 - leave this link in the description and
11:34 - it essentially goes through what all of
11:36 - the different scores mean it'll give you
11:38 - like a mathematical like derive the
11:41 - mathematic mathematical equations for
11:43 - you and you can kind of look at that and
11:45 - it's pretty interesting so I'm not gonna
11:46 - talk about that we will do that in the
11:48 - neural networks we'll talk about what
11:49 - all these mean but for right now I'll
11:51 - leave the link if you guys want to read
11:52 - that and that's gonna be it for this
11:54 - machine running tutorial if you guys
11:55 - enjoyed these tutorials please make sure
11:57 - you let me know in the comments join my
11:59 - Twitter or join my discord and neural
12:02 - networks will be coming soon in the
12:03 - meantime I'm probably thinking about
12:04 - doing some kind of discord bought
12:06 - tutorial maybe we'll do some Kibby app
12:08 - development let me know what you guys
12:10 - want to see down below and with that
12:11 - being said I'll see you again in another
12:12 - video
12:13 - [Music]