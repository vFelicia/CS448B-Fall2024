00:00 - hey guys and welcome back from the
00:02 - machine learning tutorial with Python so
00:04 - in today's video as promised we're gonna
00:06 - be implementing the SVM algorithm so
00:08 - we're actually going to be using
00:09 - something called SVC which is support
00:11 - vector classification but it's kind of a
00:13 - part of support vector machines
00:15 - obviously it's exactly what we talked
00:17 - about in the last video so before I go
00:19 - too far I just quickly want to give a
00:21 - summary of what the rest of the tutorial
00:23 - series is gonna look like for everyone
00:24 - that is still here and by the way if you
00:26 - are still here thank you and command
00:29 - yourself because most people don't watch
00:30 - paths like the first or second video so
00:32 - because they're still watching and still
00:34 - going along with the destroy really
00:35 - serious that means you guys are actually
00:36 - here to learn and you're getting a lot
00:38 - of value out of it so that's awesome for
00:39 - you guys anyways what we're gonna be
00:41 - doing in the next videos is I'm gonna be
00:43 - talking about the k-means clustering
00:45 - algorithm which is an unsupervised
00:48 - learning algorithm that you guys will
00:50 - see and you'll talk about the difference
00:51 - between supervised and unsupervised
00:53 - learning which so far we've only been
00:54 - doing supervised learning then I'm gonna
00:56 - take a break for probably like five days
00:58 - five to seven days work on some more
01:01 - deep learning stuff because I don't have
01:03 - too many projects done in that right now
01:05 - and then come back to you guys with some
01:08 - neural network tutorials and some
01:10 - project videos and really get right into
01:12 - that but I do want to take a break and
01:13 - kind of start to us as its own new
01:15 - series just to maybe get some more
01:17 - people watching it cuz a lot of people
01:19 - don't watch like you know the fifteenth
01:20 - video in a Python machine learning
01:23 - tutorial so if that's fine with you guys
01:24 - but anyways let's get into implementing
01:27 - this algorithm so essentially before I
01:30 - start I have actually just imported two
01:32 - new things here so I've imported metrics
01:34 - from SK learned and I've just imported
01:35 - the cane your nearest neighbors
01:37 - classifier which we had in previous
01:39 - videos just because I want to compare
01:40 - our results using an SVM to the K
01:44 - nearest neighbors classifier and see if
01:45 - one's better than the other why that
01:47 - would be and kind of talk about that so
01:49 - what I'm gonna do now actually is really
01:51 - easy right to implement our classifier
01:55 - which going to be CL f equals SVM dot
01:57 - SBC now this stands for support vector
02:00 - classification at least that's what I'm
02:02 - pretty sure it stands for that would
02:04 - make sense and this takes a ton of
02:06 - different parameters now we can leave it
02:08 - like this and this we're gonna do right
02:09 - now and just see exactly how it performs
02:12 - without tweaking anything without giving
02:13 - a
02:13 - colonel without giving a soft margin or
02:15 - hard margin and see what we get so let's
02:17 - do that so to fit this obviously we're
02:21 - going to see LF thought fits and we'll
02:23 - just give it our Exeter store train Y
02:26 - underscore train data and then we're
02:28 - actually going to have to predict some
02:29 - data before we can score this so to
02:31 - predict this we're genders let's say Y
02:34 - underscore prediction is standing for CL
02:37 - f dot predict and then we'll just give
02:40 - it our X underscore test data and then
02:44 - now we can actually score it using this
02:45 - really cool metrics thing from SK learn
02:48 - so essentially to get the accuracy lose
02:50 - do metrics dot us score accuracy score
02:55 - oh I gotta type metrics correctly that's
02:56 - why accuracy accuracy score and then
02:59 - we're going to give it our y underscore
03:01 - test and our y underscore prediction now
03:04 - it doesn't even matter what order you
03:05 - put this in because all this is going to
03:07 - do is just compare the two lists here to
03:09 - see like which are correct and give you
03:11 - kind of the amount of error there and if
03:13 - we want to print our accuracy of the
03:14 - screen obviously you can just print ACC
03:16 - now notice I just got rid of some of the
03:18 - print statements here just because I
03:19 - don't want that coming out and just
03:20 - wasting time when we're trying to run
03:22 - this algorithm so let's go ahead and run
03:24 - this and see exactly how well we're
03:25 - doing so fifty four okay that's probably
03:29 - not what you guys were expecting now
03:31 - that is because we haven't added any of
03:34 - the parameters we haven't tweaked
03:35 - anything so you can imagine that someone
03:37 - that maybe doesn't know how SVM works
03:38 - tries to use this classifier and wow
03:41 - they have 54 and they're like SVM is
03:43 - crap I don't want to use this right but
03:45 - that's because we haven't tweaked the
03:46 - parameters so essentially this is no
03:48 - better than just guessing at this point
03:49 - in fact randomly guessing a number might
03:51 - even do better than this in theory right
03:53 - or in practice
03:54 - so let's actually use some parameters
03:57 - now for SVC so one of the parameters
03:59 - this takes is kernel and this is the
04:00 - first one we're gonna start messing
04:01 - around with now I'm just gonna put in
04:03 - here linear as our kernel okay and
04:05 - there's a huge list of kernels actually
04:07 - I'll bring up the parameters here so you
04:10 - can actually have a look I'll leave this
04:11 - in the description if you guys want to
04:12 - read through all the different
04:13 - parameters that you can mess with
04:14 - because I'm only gonna show like two or
04:16 - three in this video but essentially you
04:18 - can see it has kernel it's a string it's
04:20 - optional defaults to our bf
04:22 - can look that up if you want to know
04:23 - what that is and then it goes there's
04:25 - polynomial sigmoid and linear
04:27 - these are the four that it allows you to
04:28 - use at least for SK learn and you can
04:30 - create your own kernels if you want
04:32 - although I don't recommend you get into
04:33 - that yet because you're probably not
04:35 - math professors anyways we're gonna use
04:37 - a linear and let's just see what the
04:39 - differences that we get here
04:41 - ninety six okay Wow
04:44 - so that is a massive difference and
04:45 - that's probably by tweaking one
04:47 - parameter so you can see what I was
04:48 - talking about where how bringing our
04:50 - dimensions up one can give us that
04:53 - hyperplane is gonna give a much better
04:55 - classification so that's exactly what I
04:57 - was trying to show in the last video
04:59 - now we could mess around we could throw
05:01 - a polynomial in here but I won't show
05:04 - you what happens when I throw a
05:05 - polynomial no polynomial is like it's
05:06 - gonna be going to like exponent six
05:08 - exponent seven giving some like pretty
05:10 - crazy values that might be more accurate
05:12 - but watch how long this takes to Train
05:14 - and we might be sitting here for quite a
05:18 - while in fact I've tried to train this
05:21 - and waited a bit like 10-15 minutes and
05:23 - then I get too impatient and just closed
05:25 - it but you can see that this is not as
05:27 - instant as our linear kernel because
05:29 - it's applying a lot more math and
05:30 - especially I'm just gonna stop running
05:32 - this because we're not gonna get to the
05:33 - end of it in this video essentially when
05:36 - we're dealing with super small numbers
05:37 - like we have in these cancer data set
05:39 - applying exponents to those is a very
05:42 - large operation so we could try to do
05:45 - something like degree and change the
05:47 - degree of our polynomial kernel to be
05:49 - two and maybe maybe this will make a
05:50 - difference let's try this and see if we
05:52 - get anything
05:53 - I'll give it like 30 seconds and if it
05:55 - doesn't run then we're just gonna call
05:57 - it quits yeah so essentially right like
06:00 - this is gonna take a long time so maybe
06:02 - if you had enough time you could wait
06:04 - for this to run but it's kind of a
06:05 - mystery on how long this is actually
06:07 - gonna take us to happen right so we'll
06:10 - quit that and you can see like those are
06:12 - some of the things that are changing
06:14 - these parameters would do so let's go
06:15 - back to linear because that seemed to be
06:17 - working for us and now let's actually
06:19 - just tweak something called C now C is
06:21 - actually gonna be that soft margin that
06:23 - I was talking about before so it
06:25 - defaults to one allowing somewhat of a
06:26 - soft margin but if we wanted to increase
06:28 - that we could do something like C equals
06:30 - two okay
06:31 - and this is double the amount of points
06:33 - that are gonna be allowed in that margin
06:34 - then before if you wanted a hard merge
06:36 - and you do C equals zero so let's do C
06:38 - equals one and see what we're getting
06:40 - you can see now we're getting 94.7 now
06:43 - obviously this is all gonna vary
06:44 - depending on what our data is split up
06:46 - into oh wait is the reason that C is one
06:50 - I thought I made that too anyway so
06:51 - let's try this again
06:54 - 96.4 right so essentially you'd probably
06:56 - want to go through and train this and
06:57 - keep tweaking these parameters and maybe
07:00 - trained on the same training data
07:02 - constantly rather than splitting it up
07:04 - like this like just do like a hard split
07:06 - by you just might like this like data up
07:10 - to a hundred is your test data and past
07:12 - that is your training data so that you
07:14 - can really see what these parameters are
07:15 - actually doing for you but obviously we
07:17 - know that linear is making a massive
07:18 - difference adding that kernel in so I
07:22 - should I gotta get rid of square
07:23 - brackets so essentially that's really
07:25 - all there is to using this SVM now I
07:27 - want to compare using K nearest
07:29 - neighbors classifier what the main
07:30 - difference is and if we're gonna get a
07:31 - better value from K nearest neighbors
07:33 - now I'd ask you guys to make your own
07:36 - prediction do you think that using K
07:37 - nearest neighbors is going to make a
07:39 - difference so let's to use this and we
07:41 - can set what do you call it n neighbors
07:44 - equal to something like nine we do have
07:47 - quite a few points so that should be
07:48 - okay and let's try this now okay so 94
07:53 - seven let me just make sure this is
07:55 - actually running the right script
07:56 - because yeah so working file okay so we
07:58 - are ready running the correct script
08:01 - here okay
08:02 - so the only reason I was doing now is
08:03 - just cuz it got like the exact same
08:04 - amount as the SVM which was kind of
08:06 - surprising to me but let's see if we
08:08 - decrease the neighbors what we're
08:09 - getting 91 so I think with the
08:11 - increasing neighbors you kind of get a
08:13 - what he called a better sense here so
08:15 - ninety three point eight now this is
08:18 - surprising because typically K nearest
08:19 - neighbors does not work as well with
08:21 - huge dimensions and we do have like 30
08:24 - or something features for this data set
08:25 - but here that's a perfect example of why
08:28 - we have to test out different machine
08:29 - learning algorithms I would still say
08:31 - SVM is probably our best bet to use
08:34 - right now just in terms of you know its
08:36 - accuracy by only adding that linear
08:38 - kernel here we kind of have to mess
08:40 - around with the different amount of
08:41 - neighbors I can imagine that we're gonna
08:42 - get quite a bit of variance as based on
08:44 - our different training and testing data
08:47 - so that's really all I can say about
08:50 - that so essentially now we have learned
08:53 - linear regression we've learned K
08:54 - nearest neighbors and we've learned SVM
08:56 - the three kind of fundamental algorithms
08:58 - and now we can go into k-means our first
09:00 - unsupervised learning algorithm which is
09:01 - kind of exciting because we're no longer
09:03 - gonna need to do this kind of stuff
09:05 - we're gonna give it training data but
09:07 - we're not gonna tell it what the
09:09 - training data is for it because right
09:11 - now essentially we're saying okay so
09:12 - this this data point is equal to a
09:14 - malignant tumor right I realize I didn't
09:17 - even end up using this list but anyways
09:18 - or a benign tumor we're not gonna tell
09:21 - our trick our model that next time we're
09:23 - gonna say you have to figure out what
09:25 - makes a class this and what makes it
09:27 - class this and why and that's kind of
09:30 - cool and that's the cool kind of machine
09:31 - learning stuff and that's we're gonna be
09:33 - doing with neural networks as well so I
09:34 - hope you guys are excited about that
09:35 - with that being said that's if the video
09:38 - if you guys want the text-based tutorial
09:39 - and the previous and the rest of them go
09:41 - to my website tech with Tim net and I'll
09:43 - see you guys in another video
09:45 - [Music]