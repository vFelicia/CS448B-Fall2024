00:00 - some of my favorite projects involve
00:01 - training in AI how to play a game I've
00:04 - done this for Flappy Bird chess pong
00:07 - Checkers but today I'm taking on
00:09 - Blackjack now this is an interesting
00:11 - game because we already know how to play
00:13 - it optimally so I'm interested to see if
00:15 - the AI can learn the rules on its own
00:18 - I'm not going to hardcode it I'm going
00:19 - to see if it can play enough games to
00:21 - figure out the optimal strategy and to
00:23 - even beat the casino now by no means am
00:26 - I encouraging gambling here this is just
00:28 - a fun game and the math behind all these
00:30 - casino games has always been interesting
00:32 - to me in this video I'm going to work on
00:34 - this project completely from scratch and
00:36 - I'm going to bring you along for the
00:37 - ride with that said let's dive
00:42 - in so first things first I got to figure
00:44 - out what to do I know there's a bunch of
00:46 - different approaches and right now I'm
00:48 - thinking that I could use something like
00:49 - neat which is neuro evolution of augment
00:52 - topologies I could try to build a neural
00:54 - network or I could go for something like
00:56 - reinforcement learning I'm going to do a
00:58 - bit of research consult the good old
01:00 - chat GPT see what it tells me and report
01:03 - [Applause]
01:05 - back so I've been doing a bit of
01:07 - research here let me share with you what
01:09 - I found so I asked chat gbt I want to
01:11 - train in AI how to play Blackjack what's
01:13 - the best way to do that these are kind
01:15 - of the methods I know it walked through
01:17 - a bunch of them I'm not going to bore
01:18 - you with all the results but pretty much
01:21 - what I was thinking is somewhat correct
01:23 - we can use a neural network but that's a
01:25 - little bit complex and we're going to
01:27 - kind of mix that in with something known
01:29 - as Q learning now if we go down here Q
01:31 - learning is probably the most basic
01:33 - approach that we can use and what this
01:35 - essentially means is we're going to
01:37 - generate a massive State table that has
01:39 - every single possible result or every
01:42 - single state that we could have in
01:43 - blackj just like those really popular
01:45 - cards you see where it shows your card
01:48 - value or your hand total versus the
01:49 - dealer card and what kind of action you
01:52 - should take so I asked chat gbt explain
01:55 - a bit more about Q learning just so I
01:57 - could kind of refresh my memory on how
01:58 - it worked and and then I asked it to
02:00 - give me a bit of a visualization just so
02:02 - that I could show you guys and I can see
02:03 - for myself exactly what it was talking
02:06 - about here you can see that we have a
02:08 - pretty simple table where we have the
02:10 - sum of our hand we have the Dealer's up
02:12 - card we have if we have a usable Ace or
02:14 - not and then we have the action that
02:16 - we'll take which is hit or stand I think
02:18 - I'm going to start with just hit or
02:20 - stand and then as we get a bit more
02:22 - complex I'm going to start adding in
02:23 - doubling splitting surrendering and a
02:26 - lot of the other more advanced rules in
02:27 - blackjack now what's going to happen is
02:29 - we're we're going to initialize this
02:30 - table with all of these different states
02:33 - and we're going to assign them something
02:34 - known as a q value now the Q value is
02:37 - the predicted result or the predicted
02:39 - outcome based on a specific action from
02:42 - the state before so what we'll do is
02:45 - we'll say hit a 12 versus a two when we
02:48 - hit that we'll either win or lose
02:50 - depending on the result we'll update the
02:52 - Q value and we'll continue to do this
02:54 - Millions upon millions of times
02:57 - eventually once we played enough hands
02:59 - of blackjack the computer will start to
03:01 - figure out which combinations or which
03:03 - states should take what specific action
03:06 - it'll do that by looking at the Q value
03:09 - so later we'll just have this massive
03:11 - table that will have all of the
03:12 - different combinations all of the
03:14 - actions we can take and the expected
03:16 - result for those actions and we'll just
03:18 - pick the action the results in the
03:20 - highest estimated Q value now this does
03:22 - involve a lot of iterations and that's
03:25 - actually why I need a pretty powerful
03:27 - computer for this project now
03:29 - fortunately I teamed up with MSI for
03:31 - this video and they sent me over the
03:32 - Raider g78 hx1 13v which is an absolute
03:36 - Beast for my deep learning workflow this
03:39 - laptop has a 13th gen I9 24 core
03:43 - processor an RTX 490 32 GB of RAM and a
03:47 - 240 HZ 17-in QHD display having this
03:51 - laptop feels like carrying around a
03:53 - mobile power horse where I can literally
03:55 - do anything I want with it even if I'm
03:57 - sitting in a coffee shop I can load and
03:59 - train massive models using nvidia's Cuda
04:01 - and this RTX 490 now being a power user
04:04 - this is exactly what I need something
04:06 - that gives me the confidence to know
04:08 - that no matter what my workflow is it
04:10 - can handle it in fact pretty soon I'm
04:12 - actually going to be moving overseas and
04:13 - I can't bring my big desktop PC with me
04:16 - now at first I was a bit worried but now
04:18 - that I have this laptop and it can fit
04:20 - right inside my backpack I know that no
04:22 - matter where I am I'll be able to handle
04:24 - any task and complete any project and
04:27 - that means that literally when I'm on a
04:28 - plane I could be trained a machine
04:30 - learning model and getting the same kind
04:31 - of productivity and work done that I
04:33 - would with my big PC now not to mention
04:36 - when I do eventually set this up in some
04:37 - sort of office I can connect multiple
04:39 - monitors and any peripherals I need
04:42 - because of it's HDMI 2.1 Thunderbolt 4
04:45 - ethernet SD card reader Etc I don't even
04:48 - have to bring dongles with me now as I
04:50 - transition to a bit more of a nomatic
04:52 - life this is exactly the type of machine
04:53 - I need something where I have complete
04:55 - confidence in its ability it has all of
04:58 - the Power I could ever want and it means
05:00 - I can get any task done editing gaming
05:03 - deep learning doesn't matter I have the
05:05 - machine that can do that now if you guys
05:06 - want to check out more I'll leave some
05:08 - links in the description this thing is
05:10 - insane and it's got a ton of other
05:11 - features I didn't even touch
05:15 - on so it's been about 30 minutes and I
05:18 - realized that before I can actually
05:19 - start doing any training I need to have
05:21 - a functioning game of blackjack so I
05:23 - just built out a custom blackjack game
05:26 - here that allows you to play against the
05:27 - computer we're going to use this with
05:29 - our model to determine whether or not
05:31 - we're winning losing Etc and sorry not
05:34 - model I mean kind of our Q learning Q
05:36 - table whatever you want to call it
05:38 - anyways if you have a look here on the
05:40 - computer I'll just quickly run through
05:41 - the code you can see I just created a
05:43 - simple class so it be nice and reusable
05:45 - I have all of the different symbols here
05:47 - these are kind of the Unicode symbols
05:49 - for heart spade diamond Etc just to make
05:51 - it look a bit better we generate the
05:53 - deck randomly deal the cards start the
05:57 - game and then we have some methods for
05:58 - figuring out the hand value it's
06:00 - actually not that simple in blackjack
06:02 - because of the aces we have a method for
06:04 - the player action so hitting staying Etc
06:07 - dealer action getting the status of the
06:09 - game so player bust player Blackjack Etc
06:12 - we have the game result and then we have
06:15 - a method for formatting the cards then
06:16 - just a bit of driver code here to
06:18 - actually run it so I'll show you quickly
06:20 - what I have is that if I run the code
06:22 - here it shows what the dealer has and
06:24 - what you have for right now it's just
06:26 - hidden stay no doubling or splitting so
06:29 - so let's hit I get a 15 they have a
06:31 - three so I'm going to stay and the
06:34 - dealer hits and has three 10 and a queen
06:37 - and I win let's play that one more time
06:40 - uh I have a three against a dealer three
06:42 - I'm going to stay and they get 17 and we
06:44 - lose that's usually how it goes anyways
06:47 - I'll be back once I start implementing
06:48 - the Q
06:51 - learning I am back it's been about an
06:53 - hour and a half I've messed around with
06:55 - a few different things here and I've got
06:57 - a somewhat working version here of of
06:59 - the Q table now you can see that I've
07:01 - just tested this on 10,000 games and I
07:05 - trained the Q table on 50,000 games now
07:08 - of those 10,000 games from that 50,000
07:11 - game kind of training sample I have a
07:14 - win rate or number of wins which is
07:16 - about 4,000 about 5,000 losses and
07:20 - 870 draws and that gives me about a 43%
07:24 - win rate the reason why it's 43% is I'm
07:26 - just dividing it out the total wins and
07:28 - losses is kind of avoiding the draws not
07:31 - sure if that's really how you should do
07:32 - the calculation but that's what I've got
07:34 - if we have a look here we can see that
07:36 - this is already making pretty reasonable
07:38 - choices based on blackjack basic
07:40 - strategy now if it was perfect this win
07:43 - rate should be closer to 48.5 49%
07:47 - depending on the rule set but it should
07:49 - be higher than it is right now so if we
07:51 - have a look at a few games here we can
07:52 - see that the dealer showing a six we
07:55 - have a 10 we hit to 17 the dealer has 13
07:59 - the dealer hits and then we actually end
08:01 - up losing now I don't show all of the
08:03 - cards the dealer has when they're
08:04 - hitting I'm just trying to show the
08:06 - player decision here the AI decision in
08:09 - this case dealer shows an ace we have a
08:11 - 13 we hit to 20 we stay unfortunately
08:14 - the dealer has a black check and we lose
08:16 - now let's see dealer has a five we're
08:18 - showing a five we decide to stay and we
08:21 - end up winning I assume the dealer
08:22 - busted here or they had less well I
08:24 - guess they had to bust right cuz we had
08:26 - a 15 continuing dealer shows a two we
08:28 - have a 12 we actually decide to hit
08:30 - which I thought was interesting and we
08:32 - bust and we lose now this one is where
08:34 - we're kind of making some mistakes right
08:36 - dealer shows a two we have a 14 we're
08:38 - hitting to 15 we have a 15 we're hitting
08:40 - again we're on 17 we're hitting again
08:42 - and we're getting 25 so obviously it's
08:44 - not perfect and you can see that it does
08:46 - make some mistakes especially on some of
08:48 - those lower cards like two and three so
08:50 - I'm going to try to tweak this and see
08:52 - if it can get closer to basic strategy
08:54 - so overall pretty decent progress and I
08:56 - almost argue this plays better Blackjack
08:59 - than the most average
09:02 - people all right so it's been about an
09:05 - hour and a half here I've tried a bunch
09:06 - of different things made some
09:08 - modifications to the code and most
09:10 - notably what I've done with the model is
09:12 - really increased the sample size and
09:14 - made it count for soft and hard hands so
09:17 - previously it didn't know the difference
09:19 - between a soft 16 or a hard 16 or
09:21 - whatever soft and hard hand so it was
09:23 - making a lot of weird choices when we
09:25 - had aces now I believe I fixed that
09:28 - problem but I think by doing that I may
09:30 - have introduced some others now really
09:32 - all that to say after doing all of this
09:34 - work I'm getting pretty much the exact
09:36 - same result kind of the story of my life
09:38 - when it comes to machine learning change
09:39 - a bunch of things mess with a bunch of
09:41 - parameters and not being an absolute
09:42 - expert I just get the same or Worse
09:44 - result I'm sure many of you can probably
09:46 - relate to that anyways I'm getting again
09:48 - about a 43% win rate here but this time
09:51 - it's on 5,000 games that was the
09:53 - training uh kind of data set and then 1
09:56 - million games is the testing data set
09:58 - for my wins losses and draws now keep in
10:01 - mind I'm not accounting for the fact
10:03 - that blackjacks pay 3 to2 I don't have
10:05 - rules like double down surrender and
10:07 - split which would increase the player
10:09 - odds if I had those rules I'd probably
10:12 - be doing a lot better but also it would
10:14 - be many more hours I'd be sitting here
10:16 - because the code is well much more
10:17 - complicated to write anyways let's just
10:19 - have a look at a few kind of instances
10:21 - here of where the AI is making some
10:23 - mistakes and where it's doing good so
10:25 - here you can see dealers showing a two I
10:27 - have a 16 soft 16 hitting soft 16 to
10:30 - hard 16 and then hitting to 26 so in
10:34 - that instance obviously that's making a
10:35 - mistake it's correct to hit the soft 16
10:38 - not correct to hit the hard 16 so I
10:40 - think again some improvements could be
10:42 - made there now here dealer has a three
10:44 - we have a 10 hit to 13 and then hit
10:46 - again to 19 now it works out in that
10:48 - instance but that's not the correct play
10:50 - so really what I've noticed is this has
10:52 - a bias to hit more than it should now I
10:54 - could easily fix that by hardcoding a
10:56 - few basic strategy rules in here but
10:59 - that's not what I want to do right I
11:00 - want this to learn purely from
11:01 - reinforcement learning and from trial
11:03 - and error so at this point I'm not
11:05 - exactly sure how to proceed what I will
11:08 - do is quickly show you the code and give
11:10 - you an idea of what I've been doing and
11:12 - then maybe you guys can give me some
11:13 - suggestions in the comments down below
11:15 - and we can do a part two unfortunately I
11:17 - don't have 10 more hours to sit here and
11:19 - mess around otherwise I would try to add
11:21 - those extra rules in and see how much of
11:23 - a difference that makes so if we have a
11:24 - quick look at the code here and by the
11:26 - way I'll leave this available from the
11:27 - link in the description you can see I
11:29 - have my main Blackjack module which is
11:31 - all of the code that we looked at before
11:33 - which essentially simulates one hand of
11:35 - blackjack now we don't play with shoes
11:38 - we use the same cards over and over so
11:40 - it's like we're using a continuous
11:42 - shuffler machine and just every round we
11:44 - reshuffle the cards which is different
11:46 - obviously than real blackjack where you
11:47 - can kind of count the cards and see what
11:49 - you're getting now inside of blackjack Q
11:51 - learning here we start by initializing
11:53 - with three parameters now these are
11:55 - related to the Q learning we have Alpha
11:57 - which is the learning rate gamma which
11:59 - is the discount factor and Epsilon which
12:01 - is essentially the chance of choosing a
12:03 - random action it's how much exploration
12:06 - we're going to do now these three
12:07 - factors are really what make up the core
12:10 - equation for updating the values in the
12:12 - Q table I'll see if I can put that on
12:14 - the screen for you guys right now so you
12:15 - can see what the equation is it's a
12:17 - little bit complex but pretty much Alpha
12:20 - is telling us okay how quickly should we
12:22 - update values based on the result we
12:24 - just got now gamma is telling us how
12:26 - much we should consider future rewards
12:28 - and Epsilon is our chance of kind of
12:30 - exploring or choosing a random choice so
12:33 - rather than always choosing the choice
12:35 - that has the highest Q value sometimes
12:37 - we pick a random choice so the model or
12:39 - the agent has some ability to explore
12:42 - and choose something that maybe hasn't
12:43 - been chosen before but could be the
12:45 - optimal solution these three parameters
12:47 - uh can actually have a huge impact on
12:49 - the model or on the agent I have messed
12:51 - with them a bit but obviously if you
12:53 - were able to find the right relationship
12:54 - here and mess with these a lot then
12:56 - you'd be able to probably get a better
12:57 - model now what we do after that is
12:59 - actually initialize the Q table which is
13:01 - going to have four dimensions the First
13:03 - Dimension is going to represent the
13:04 - player sum then we're going to have
13:06 - whatever the Dealer's up card is and
13:08 - then we're going to have uh the usable
13:11 - Ace so if we have an ace that can be
13:13 - used for soft hand and then we're going
13:15 - to have the action which is either hit
13:16 - or stay if we wanted to add the other
13:18 - decisions like double and split again
13:20 - that's a bit more complicated but we
13:21 - would have to change this last column to
13:23 - four next we have a function that
13:25 - chooses the action using that Epsilon
13:27 - variable so you can pretty much see that
13:28 - we have a 10% chance here of randomly
13:31 - selecting hitter stay whereas otherwise
13:33 - we're just going to see which one in the
13:35 - Q table currently has the most favorable
13:37 - possible reward or output we then have
13:40 - update this is what's actually updating
13:42 - the Q table and you can see we have kind
13:43 - of that Q function here where we're
13:45 - taking the old Value Plus the alpha
13:48 - multiplied by the reward plus the gamma
13:51 - times the future Max minus the old value
13:53 - again a bit complex you don't need to
13:54 - understand it then we have a function
13:56 - here if I can scroll up that tells us us
13:58 - if we have a usable Ace we have a
14:01 - training function and this is the main
14:03 - function where what we're doing is
14:04 - actually simulating the game we're
14:06 - getting the result so we're seeing if we
14:08 - won or if we lost and then we're calling
14:10 - those update functions to actually
14:11 - update the Q table lastly we have this
14:13 - play function here and what this will do
14:15 - is actually use the Q table that we just
14:17 - trained up and has all those values
14:18 - inside of it to determine what choice we
14:21 - should make based on the card that were
14:22 - given so you can see that I was using
14:24 - play after I did all of the training and
14:26 - we had that Q table in memory I could
14:28 - store the Q table as well that way I
14:31 - don't have to train every time I
14:32 - actually want to play the game so that's
14:34 - pretty much it I wish I could spend some
14:35 - more time and improve this but
14:37 - unfortunately I don't have more time to
14:38 - film right now let me know what you guys
14:40 - think in the comments down below and how
14:42 - I could potentially improve this model I
14:44 - think for a first shot pretty decent
14:46 - it's working fairly well and it gave me
14:48 - a chance to kind of refresh myself with
14:50 - Q learning and hopefully taught you a
14:51 - little bit about how that worked and
14:53 - gave you a realistic view of what it's
14:55 - like to work on a project after all
14:57 - that's kind of the goal this video here
14:59 - just to show you a bit of my process and
15:01 - the fact that I don't always get it
15:02 - right the first time many people see
15:04 - those finalized tutorials online and
15:06 - they think that's always what happens I
15:08 - can't tell you how many times I fail how
15:10 - many times I try to work on a project
15:12 - that just doesn't work and how many
15:14 - pieces of code essentially go in the
15:15 - garbage because I had no idea what I was
15:17 - doing kind of like this anyways if you
15:20 - guys enjoyed make sure to leave a like
15:21 - subscribe to the channel and I will see
15:23 - you in the next
15:24 - [Music]
15:27 - one
15:31 - oh