00:00 - hey guys so now it's time to talk about
00:02 - word embeddings and this embedding layer
00:04 - and then what the global average pooling
00:06 - one D layer is doing now we already have
00:09 - an idea of what these dense layers are
00:10 - with these activation functions like
00:12 - real ooh and sigmoid but what we're
00:14 - actually gonna do today or I guess just
00:16 - in this video is talking about the
00:18 - architecture of this network kind of how
00:20 - it works on a high level understanding
00:21 - and then in the next video we'll do is
00:23 - actually get into training and using the
00:25 - network so what I'm gonna do first is
00:28 - just start by talking about these first
00:29 - two layers and specifically what this
00:31 - embedding layer is because it's very
00:33 - important and then we will draw the
00:35 - whole network or the whole I guess
00:38 - network is right we're way to put it the
00:39 - whole architecture and talk about how it
00:41 - fits together and what it's actually
00:42 - doing so let's get started now the
00:47 - easiest way to kind of explain this is
00:48 - to use an example of two very similar
00:51 - sentences so I'm just gonna say the
00:53 - first sentence is have a great day and
00:59 - the next sentence will be have a good
01:03 - day now I know my handwriting is
01:06 - horrible so just give me a break on that
01:08 - it's also hard to kind of write with
01:10 - this tablet so that's my excuse but
01:11 - anyways these two sentences looking at
01:14 - them as human beings we can tell pretty
01:16 - quickly that they're very similar now
01:18 - yes great and good maybe one has more
01:20 - emphasis on having an amazing day
01:22 - whatever it is but they're very similar
01:24 - and they pretty well have the same
01:26 - meaning right
01:27 - maybe we know when we would use the
01:28 - sentence in kind of the context in which
01:30 - like these words great and good are used
01:32 - and day and day and all this right it
01:34 - just we understand what they are now the
01:36 - computer doesn't have that same
01:38 - understanding at least right off the bat
01:41 - when looking at these two sentences now
01:43 - in our case we've actually integer
01:45 - encoded all of our different values so
01:47 - what we end up having are all of our
01:49 - different words sorry is our sentences
01:50 - end up looking something like this so
01:52 - we're gonna have this first word will
01:53 - represent a zero a will be one great
01:56 - will be two and day will be 3 so then
01:58 - down here we'll have 0 1 in this case
02:00 - we're gonna say good is 4 and day is 3
02:03 - as well so this means if we integer
02:05 - encode these sentences we have some
02:07 - lists that look something like this now
02:09 - this one clearly is the first sentence
02:11 - and this one down here will be
02:13 - second sentence now if we just look at
02:16 - this and we pretend that you know we
02:18 - don't even know what these words
02:20 - actually are all we can really tell is
02:23 - the fact that two is different from four
02:25 - now notice what I just said there two is
02:28 - different from four when in reality if
02:31 - we look at these two words we know that
02:33 - they're pretty similar yes they're
02:35 - different words yes they're different
02:36 - lengths whatever it is but we know that
02:38 - they have a similar meaning and the
02:40 - context in which they're used in this
02:42 - sentence is the same now our computer
02:45 - obviously doesn't know that because all
02:47 - it gets to see is this so what we want
02:49 - to do is try to get it to have an
02:51 - understanding of words that have similar
02:53 - meanings and to kind of group those
02:55 - together in a similar form or in a
02:57 - similar way because obviously in our
02:59 - application here of classifying movie
03:01 - reviews the types of words that are used
03:03 - in the context in which they are used
03:05 - really makes a massive difference to
03:08 - trying to classify that as either a
03:09 - positive or a negative review and if we
03:12 - look at great and good and we say that
03:14 - these are two completely different words
03:15 - well that's gonna be a bit of an issue
03:17 - when we're trying to do some
03:18 - classifications so this is where our
03:19 - embedding layer comes in now again just
03:23 - to say here one more time like we know
03:25 - these are different but we also would
03:26 - know for example say if we replace this
03:28 - four with a three well all our computer
03:30 - again would know is that two is
03:32 - different from three just like four is
03:33 - different from two it doesn't know how
03:35 - different they are and that's what I'm
03:36 - trying to get at here is our bedding
03:38 - layer is going to try to group words in
03:41 - a similar kind of way so that we know
03:43 - which ones are similar to each other so
03:46 - let me now talk about specifically the
03:48 - embedding layer so let me just draw a
03:50 - little grid here now what our embedding
03:52 - layer actually does kind of like I don't
03:54 - want to say the formal definition but
03:56 - the more mathy definition is it finds
03:59 - word vectors for each word that we pass
04:01 - it or it generates word vectors and uses
04:04 - those word vectors to pass to the future
04:07 - layers now a word vector can be in any
04:10 - kind of dimensional space now in this
04:13 - case we've picked 16 dimensions for each
04:15 - word vector which means that we're gonna
04:17 - have vectors may be something like this
04:19 - and a vector again it's just a straight
04:21 - line with a bunch of different
04:22 - coefficients in some kind of space that
04:25 - is in this case
04:27 - sixteen dimensions so let's pretend that
04:28 - this is a sixteen dimensional vector and
04:30 - this is the word vector for the word
04:33 - half now in our computer it wouldn't
04:36 - actually be have it would be zero
04:38 - because again we have integer encoded
04:40 - stuff but you kind of you get the point
04:42 - so we'll say this is the word vector for
04:44 - half now what we're gonna do immediately
04:47 - when we create this embedding layer is
04:49 - let me I should get out of this quickly
04:51 - for one second is we initially create
04:53 - 10,000 word factors for every single
04:56 - word and in this case every single
04:58 - number that represents a word so what
05:00 - we're gonna do is when we start creating
05:02 - this embedding layer we see that we've
05:04 - have an embedding layer is we're gonna
05:06 - draw 10,000 word vectors and just kind
05:09 - of some random way that are just there
05:12 - and each one represents one word and
05:14 - what happens when we call the embedding
05:16 - layer is it's gonna grab all of those
05:18 - word vectors for whatever input we have
05:21 - and use that as the data that we pass on
05:24 - to the next layer now how do we create
05:27 - these word vectors and how do we group
05:29 - words well this is where it gets into a
05:31 - bit complicated math I'm not really
05:33 - gonna go through any equations or
05:34 - anything like that but I'll kind of give
05:35 - you an idea of how we do it now we want
05:38 - to so let me get rid of this word have
05:40 - because this is not the best word vector
05:42 - example and let's say that this word
05:44 - vector is great now upon creating our
05:47 - word vector our embedding layer we have
05:49 - two vectors we have great and we have
05:51 - good and we can see that these vectors
05:53 - are kind of far apart from each other
05:55 - and we determine that by looking at the
05:57 - angle between them and we say that this
05:59 - angle maybe it's like I don't know 70
06:01 - degrees or something like that and we
06:03 - can kind of determine that great and
06:04 - good are not that close to each other
06:06 - but in reality we want them to be pretty
06:09 - close to each other we want the computer
06:10 - to look at great and good and be like
06:12 - these are similar words let's treat them
06:14 - similarly in our neural network so what
06:17 - we want to do hopefully is have these
06:18 - words and these vectors kind of move
06:21 - closer together whether it's good going
06:23 - all the way to great or great going all
06:25 - the way to good or vice versa right we
06:27 - just want them to get close together and
06:29 - kind of be in some form of a group so
06:32 - what we do is we try to look at the
06:34 - context in which these words are used
06:35 - rather than just the content of the
06:38 - words which would just be what this
06:39 - looks like
06:40 - want to figure out how they how they're
06:42 - used so we'll look at the words around
06:44 - it and determine that you know when we
06:45 - have a and day and a and day maybe that
06:49 - means that these are like related in
06:51 - some way and then we'll try to group
06:53 - these words now it's way more
06:54 - complicated than that don't get me wrong
06:56 - but it's kind of like a very basic way
07:00 - of how they group together is we look at
07:02 - the words that surround it and just
07:04 - different properties of the sentence
07:06 - involving that word and then we can kind
07:08 - of get an idea of where these words go
07:10 - and which ones are close to each other
07:11 - so maybe after we've done some training
07:14 - what happens is our word embeddings or
07:17 - what is known as learns just like we're
07:19 - learning and teaching our neural network
07:20 - and we get we end up getting great and
07:23 - good very close together and these are
07:25 - what their word vector representations
07:27 - are we can tell them to close again by
07:29 - looking at the angle in between here
07:30 - maybe it's like 0.2 degrees and what
07:33 - that means is these two vectors which
07:34 - are just a bunch of numbers essentially
07:36 - are very close together so when we feed
07:38 - them into our neural network they should
07:40 - hopefully give us a similar output at
07:43 - least for that specific neuron that we
07:45 - give it to now I know this might be a
07:47 - little bit confusing but I'm gonna go
07:48 - we're gonna talk about this a bit more
07:50 - with another drawing of the whole
07:51 - network but I hope you're getting the
07:53 - idea the whole point of this embedding
07:54 - layer is to make word vectors that and
07:57 - then group those word vectors or kind of
07:59 - like make them close together based on
08:01 - words that are similar and that are
08:02 - different so again just like we would
08:04 - have grading good here we would hope
08:06 - that a word vector like bad would be
08:09 - down here where it has a big difference
08:12 - from great and good so that we can tell
08:13 - that these words are not related
08:15 - whatsoever all right so that's how the
08:17 - embedding layer works now what ends up
08:19 - happening when we have this embedding
08:21 - layer is we get an output dimension of
08:22 - what's known as 16 dimensions and that's
08:25 - just how many coefficients essentially
08:27 - we have for our vector so just like if
08:29 - you have a 2d line so like if this our
08:31 - grid in 2d and we say that this is X and
08:34 - this is y we can represent any line by
08:37 - just having like some values like ax
08:40 - plus B y equals C now this is the exact
08:45 - same thing that we can do in in n
08:47 - dimensions which means like any amount
08:49 - of dimensions so for a 16 dimensional
08:51 - line I'm not gonna draw them all but we
08:52 - would start with like ax
08:54 - plus B Y plus cz plus DW and so on and
09:00 - we would just have again 16 of these
09:02 - coefficients and then some kind of
09:05 - constant value
09:06 - maybe we call it lambda that is like
09:09 - what it's what it equals to what the
09:11 - equation equals to and that's how we
09:12 - define a line I'm pretty sure I'm doing
09:15 - this correctly in in n dimensions so
09:18 - anyways once we create that line what we
09:20 - actually want to do is we want to scale
09:22 - the dimension down a little bit now
09:24 - that's just because 16 dimensions is a
09:26 - lot of data especially when we have like
09:28 - a ton of different words coming into our
09:30 - network we want to scale it down to make
09:32 - it a little bit easier to actually
09:34 - compute and to train our network so
09:36 - that's where this global average pooling
09:39 - 1d layer comes in now I'm not gonna talk
09:41 - about this in two depth in too much
09:42 - depth but essentially the way to think
09:44 - of the global average pooling 1d is that
09:47 - it just takes whatever dimension our
09:49 - data is in and just puts it in a lower
09:50 - dimension now there's a specific way
09:52 - that it does that but again I'm not
09:54 - gonna talk about that and it's not super
09:56 - important if you care about that a lot
09:57 - just look it up and it's not like crazy
09:59 - hard but I just I don't feel the need to
10:01 - go into it in this video so anyways
10:03 - let's now start drawing what our network
10:05 - actually looks like after understanding
10:07 - how this embedding layer works so we're
10:10 - gonna initially feed in a sequence and
10:12 - we'll just say that this is like our
10:13 - sequence of encoded words okay so say
10:16 - this is our input and maybe it's
10:18 - something like zero seven nine like a
10:21 - thousand two hundred a thousand twenty
10:24 - we have like nine again maybe we have
10:27 - eight just a bunch of different
10:28 - essentially numbers right so we're gonna
10:31 - pass this into our embedding layer and
10:33 - all this is gonna do is it's gonna find
10:35 - the representation of these words in our
10:38 - embedding layer so maybe are embedding
10:41 - layer well it's gonna have the same
10:42 - amount of words in our vocabulary so to
10:44 - look up say 0 it'll say maybe 0 means 0
10:47 - is vector is like 0.2 0.3 and it goes to
10:53 - 16 dimensions but I'm just gonna do like
10:54 - 2 for this example here maybe 7 its
10:57 - vector is like 7 and 9.0 and it just
11:02 - keeps going like this and it looks up
11:03 - all these vectors so it takes all of our
11:06 - input data and it just
11:07 - turns them into a bunch of vectors and
11:09 - just spits those out into our next layer
11:11 - now our next layer what this does is it
11:15 - just takes these vectors and just
11:17 - averages them out and it just means it
11:18 - kind of shrinks them there down it down
11:20 - so we'll do like a little smaller thing
11:21 - here and we'll just say like average
11:24 - okay so I'll call this one embedding and
11:28 - that one is average
11:30 - now this average layer now is where we
11:33 - go into the actual neural network well
11:35 - obviously this is a neural network but
11:36 - we go into the dense layers which will
11:38 - actually perform our classification so
11:40 - what we're gonna do is we're gonna start
11:42 - with 16 neurons and this is just again
11:43 - an arbitrary number that we picked for
11:46 - our network you can mess around with
11:48 - different values for this and I
11:49 - encourage you to do that but 16 is what
11:51 - tensorflow decided to use and what I'm
11:52 - just following along with so we're gonna
11:54 - have 16 neurons and we're gonna pass all
11:57 - of our now 16 dimensional data or
11:59 - whatever dimensional data it is into
12:01 - these neurons like this now this is
12:04 - where we start I'm doing the dense layer
12:05 - so we have this dense layer and this is
12:07 - connected to one output neuron like this
12:11 - so what we end up having is this
12:14 - embedding layer which is gonna have all
12:16 - these word vectors that represent
12:18 - different words we average them out we
12:20 - pass them into this 16 neuron layer that
12:24 - then goes into an output layer which
12:26 - will spit out a value between 0 and 1
12:29 - using the sigmoid function which I
12:31 - believe I have to correct myself because
12:33 - in other videos I said it did between
12:34 - negative 1 and 1
12:35 - it just takes any value we have and puts
12:37 - it in between 0 and 1 like that all
12:41 - right so that is kind of how our network
12:43 - works so let me talk about what this
12:46 - dense layer is doing just a little bit
12:48 - before we move on to the next video so
12:50 - what this dense layer is going to
12:51 - attempt to do essentially is look for
12:54 - patterns of words and try to classify
12:57 - them using the same methods we talked
12:59 - about before into either a positive
13:02 - review or a negative review I'm gonna
13:04 - take all these word vectors which again
13:06 - are gonna be like similarly grouped
13:08 - words like great good are gonna be
13:09 - similar input to this dense layer right
13:11 - because we've averaged them out and
13:14 - embedded them in all this and then what
13:16 - we're gonna do is we're gonna try to
13:17 - determine based on what words we have
13:20 - and what
13:21 - they come in what our text is and we
13:24 - hope that this layer of 16 neurons is
13:26 - able to pick up on patterns of certain
13:28 - words and where they occur in the
13:30 - sentence and give us a accurate
13:32 - classification again it's gonna do that
13:34 - by tweaking and modifying these weights
13:36 - and all of the biases that are on you
13:39 - know all of these different
13:40 - what-do-you-call-it
13:42 - layers or all of these connections or
13:43 - whatever they are and then it's gonna
13:44 - give us some output and some level of
13:46 - accuracy for our network so i hope that
13:49 - i explained that decently in terms of
13:51 - how the word embeddings work again i
13:53 - always encourage you guys to ask me
13:55 - questions in the comments below if you
13:57 - know the answer or maybe I didn't
13:58 - explain something clearly enough please
14:00 - do me a favor go down and help me out
14:02 - explain it to people that maybe don't
14:03 - understand it or make sure that I'm you
14:06 - know not butchering any of these
14:07 - explanations because just like you guys
14:08 - I'm learning as well
14:10 - and this is what I feel confident in and
14:12 - this what I think everything is but I
14:14 - can always make mistakes just like
14:15 - everyone else so anyways that has been
14:17 - it for this video I hope you guys have a
14:18 - little bit of an understanding on when
14:20 - we would use a word embedding layer how
14:22 - the average pooling works and how this
14:24 - network is kind of going to react in the
14:25 - next video when we start training