00:00 - one of the best projects that you can
00:01 - work on that has real legitimate
00:03 - potential to make you a ton of money has
00:05 - to do with web scraping the ability to
00:07 - collect real-time data about travel
00:09 - e-commerce Healthcare real estate you
00:12 - name it is already a multi-billion
00:14 - dollar industry and you can tap into
00:16 - that with a project like the one that
00:18 - I'm just about to show you so first
00:19 - let's discuss the potential
00:21 - imagine you're a drop shipper or an
00:24 - e-commerce seller and you're constantly
00:25 - competing against hundreds maybe even
00:27 - thousands of competitors where your main
00:29 - differentiation is price and maybe stock
00:31 - or availability now if you had access to
00:33 - real-time information and were able to
00:35 - undercut your competitors prices or
00:37 - raise your prices when your competitors
00:39 - went low or out of stock think about how
00:41 - much money that could potentially make
00:43 - you
00:43 - seriously think about this for a second
00:45 - if you have access to real-time
00:47 - information this can inform Million
00:49 - Dollar business decisions and if you can
00:51 - provide that data to different clients
00:53 - or people who can actually use it you
00:55 - can take a cut of those profits
00:57 - now I won't lie to you this is not as
00:59 - trivial as I might make it sound most
01:01 - companies are going to actively block
01:02 - you from grabbing this type of
01:04 - information and even if they do provide
01:06 - an API you're most likely going to deal
01:07 - with rate limiting out of date
01:09 - information and a whole bunch of other
01:11 - issues that can completely ruin the
01:13 - whole point of even having this system
01:14 - that's why what you most likely need to
01:16 - do is build a web scraper now that's
01:18 - exactly what I attempted to do for this
01:20 - video I set out to build an automated
01:22 - web scraper that would scan e-commerce
01:24 - marketplaces for products of interest
01:26 - for either me or my clients
01:27 - automatically tracking their prices and
01:29 - alerting me or my clients of any changes
01:32 - in those prices such that we could react
01:34 - and modify our prices so that we could
01:36 - take advantage of an opportunity
01:39 - now before I did all of this I did ask
01:41 - my community if they had any advice on
01:43 - how to go about building a project like
01:44 - this what they suggested is to use a
01:46 - popular framework like playwright or
01:48 - selenium set up a web scraper go to the
01:51 - website scan for a specific product go
01:54 - parse the different HTML grab all of the
01:56 - prices of that product store that in a
01:58 - database and then scan the database and
02:00 - see if there's been any changes since
02:01 - the last time we updated that product
02:03 - they then said I could run the scanner
02:05 - automatically at a set increment so
02:07 - every day every hour every minute
02:09 - whatever increment I choose and that
02:11 - would be how we can work on this project
02:14 - now unfortunately every time I attempted
02:16 - to do this my IP address got blocked I
02:18 - got stuck behind captchas and I just got
02:20 - completely denied and shut out of the
02:22 - website after just a few attempts at
02:24 - scraping it now it turns out the
02:25 - websites are pretty smart they can
02:27 - detect Bots and they really don't want
02:29 - you scraping their website and doing
02:30 - exactly what I was attempting to do now
02:32 - that's what I remembered that about a
02:34 - year ago I had worked with a company
02:35 - called bright data now bright data
02:37 - essentially unblocks your browser for
02:39 - you it'll automatically solve captures
02:41 - and all that kind of stuff so I reached
02:43 - out to them we connected and they agreed
02:45 - to sponsor this video so that I could
02:46 - continue building this project
02:49 - now what bright data gave me access to
02:51 - is something called their scraping
02:53 - browser which is a head full browser
02:55 - that works with Puppeteer selenium and
02:57 - playwright and automatically unblocks
02:59 - websites for you it will connect to a
03:01 - proxy Network rotate your IP address and
03:04 - automatically solve captchas essentially
03:06 - allowing you ungated access into a
03:08 - website and to perform web scraping now
03:10 - most importantly it also allows you to
03:12 - do scaling meaning I could run hundreds
03:14 - of instances of my scraper at the exact
03:16 - same time and I wasn't limited to a
03:18 - single instance or whatever my local
03:20 - machine could handle so that's what I
03:21 - ended up using for this project I'm
03:23 - going to show that to you in a second
03:24 - but if you do want to check out bright
03:25 - data they are the sponsor of this video
03:27 - you can check them out from the link in
03:29 - the description and you'll get access to
03:30 - some free credits that you can try this
03:32 - out for yourself and see the power of a
03:34 - browser like this
03:36 - all right so now let me show you the
03:38 - project that I actually ended up
03:39 - building that's fully functioning and
03:41 - working right now if you want you can
03:43 - extend this project I'm leaving a
03:45 - completely open source so click the
03:46 - GitHub Link in the description and feel
03:48 - free to do whatever you want with this
03:50 - code you can make something really cool
03:51 - this is kind of a solid base and you
03:53 - could extend this and make a legitimate
03:55 - business from anyways you can see that
03:57 - what I have here is a product search
03:58 - tool really it's kind of analyzing
04:00 - e-commerce prices and right now it goes
04:02 - to Amazon and it will automatically
04:04 - scrape Amazon every single day for all
04:07 - of the different products that I have
04:08 - that are set up here so I can either
04:10 - enable or disable tracking of this
04:12 - product I can add a new product that I
04:14 - want to track and then I can view the
04:15 - individual prices for these products so
04:17 - let's click into ryzendine 5950x that's
04:20 - going to generate a table for us down
04:22 - here of all of the information we have
04:24 - on this product currently from amazon.ca
04:27 - now you could scrape multiple sources at
04:29 - once for right now I've just set it up
04:31 - to do Amazon but you'll see the way I
04:33 - wrote my code you can automatically add
04:34 - other web scrapers and set up different
04:36 - sites now it shows me all of the
04:38 - relevant listings that contain the ryzen
04:40 - 9 5950x processor from Amazon gives me
04:43 - the current price and any price change
04:45 - since the last time this scraper ran so
04:48 - you can see down here we have kind of a
04:49 - combo of a CPU and a motherboard and
04:52 - this has gone up 0.76 percent since the
04:55 - last time I ran this so you can see here
04:57 - we have this Micro Center AMD whatever
04:59 - gives you all the information you can
05:01 - view the product URL view the source
05:03 - newest price time current price and then
05:05 - it gives you a graph actually telling
05:07 - you the whole history of this product
05:08 - from when this web scraper has been
05:11 - running again the way this works is
05:12 - every single day it automatically
05:14 - scrapes all of your tracked products and
05:16 - then gives you any updates on them now
05:18 - you're not always going to have a price
05:19 - change and it depends on how frequently
05:21 - you run the scraper but in my case I've
05:23 - seen a few different changes I was
05:25 - actually pretty surprised to find out
05:26 - that even large sites like Amazon are
05:28 - constantly updating their prices and you
05:30 - can see quite a few different price
05:31 - changes here so we'll go into one more
05:33 - to see if there's any changes in
05:35 - something like Dove men shampoo don't
05:37 - think we're going to see any here just
05:38 - because this is a pretty basic kind of
05:40 - product and I only actually ran this
05:41 - scraper one time so that makes sense we
05:43 - wouldn't see a price change there now if
05:45 - I want I can even track another product
05:47 - so I can go up here and just search for
05:49 - one one time or I can add it to my
05:51 - tracked product list to be automatically
05:53 - updated and ran every single day so
05:56 - let's just track a random product here
05:57 - uh let's do something like Intel I9 uh
06:02 - 10 900k I don't know if that's recent or
06:04 - not but let's scrape this okay we'll
06:07 - give that a second to update and then
06:09 - I'll show you the table that it
06:10 - generates alright so this is finished so
06:12 - I'll click on this now gives me the
06:13 - table and shows me all of the different
06:15 - listings from Amazon that were relevant
06:17 - that contained Intel I9 10900k so I can
06:21 - then run this every single day if I
06:22 - wanted to and I could add this into my
06:24 - tracked product list if I wanted to
06:26 - automatically run the Scraper on it
06:28 - every few days then again I can kind of
06:30 - go here and toggle them on or off if I
06:32 - no longer want to track a specific
06:33 - product obviously a lot more stuff you
06:36 - can do here but you see we have a very
06:37 - solid base setup so now let me hop over
06:39 - the code I'll give you a bit of
06:41 - information on how this code functions
06:43 - so you have some insight on how you
06:44 - could maybe change this if you wanted to
06:46 - work on a project Sim similar to this
06:49 - all right so I've just hopped over to
06:51 - the code here and I'm just going to give
06:52 - you a very quick overview of the
06:54 - architecture of this project and then
06:56 - again feel free to look at the code from
06:57 - the link in the description and extend
06:59 - this as you see fit what I have for this
07:02 - is kind of three or four main components
07:03 - I have a front end that's written in
07:06 - react I have a back end that's written
07:08 - in flask with python and then I have the
07:10 - actual scraper which is a separate
07:12 - process I run from my back end that's
07:14 - written in Python and uses playwright I
07:17 - also have an automation script which is
07:18 - very simple and just automatically sends
07:21 - a API request to my back end every day
07:24 - now you can set this every hour every
07:26 - minute you can use a Cron job you can
07:28 - use all kinds of stuff but that's kind
07:29 - of how I've set this up so I'm just
07:30 - going to walk you through very briefly
07:32 - the back end and then the web scraper
07:33 - and we'll kind of go from there okay so
07:36 - in my back end I have my app.pi haven't
07:38 - organized this a ton and what I do is I
07:40 - set up here or I connect to a local SQL
07:43 - Lite 3 database this is what I'm using
07:45 - to store all of my data obviously you
07:47 - could use something like postgres or
07:49 - mongodb but this was the simplest for
07:51 - this project
07:52 - I set up my different database tables
07:54 - here using kind of the SQL connector or
07:57 - SQL Alchemy whatever you want to call
07:58 - this in flask and then I have a bunch of
08:00 - different endpoints slash results is
08:03 - actually where the web scraper submits
08:05 - its results so the API really allows the
08:07 - web scraper to interact kind of with my
08:10 - database and the back end so I send a
08:12 - post request to results that submits the
08:14 - results and then gets added to the
08:16 - database and then the front end can view
08:17 - that data so that's what slash results
08:19 - is unique search text this is going to
08:21 - give all of the unique kind of searches
08:23 - that we've done so the product names
08:25 - essentially we then have a get request
08:27 - here for slash results which gives all
08:28 - of the results for a specific product
08:30 - name which is the search text we then
08:33 - have all results which will give all of
08:35 - the results for all of the different
08:36 - products in our database we have start
08:38 - scraper which is a post request method
08:40 - which is going to start the web scraper
08:42 - based on a URL in a specific search text
08:44 - we have ADD tracked products
08:46 - straightforward we then have a put for a
08:48 - tracked product which allows us to
08:49 - update it so we can either toggle if
08:51 - we're tracking it or not then we can
08:53 - have the ability to get all our tracked
08:54 - products and then to update the tracked
08:56 - products where what this will do is
08:58 - automatically run the web Scraper on
09:00 - every single product instance so you can
09:02 - see what this does is create a new sub
09:04 - process where it then runs our python
09:06 - scraper script which I'll go into now
09:09 - so I'm just going to very quickly kind
09:10 - of run through this but I have this
09:12 - main.pi file and what this does is kind
09:14 - of dynamically allow you to add
09:16 - different websites that you could be
09:17 - scraping for now I've just set it up
09:19 - with amazon.ca but you can connect this
09:21 - with really any website you want so long
09:23 - as you write a very kind of specific
09:25 - piece of logic that will get the uh kind
09:28 - of elements that you're looking for
09:29 - which I'll show you in one second now
09:31 - what I do here is I connect right away
09:32 - to the bright data scraping browser it's
09:34 - actually extremely simple I was
09:36 - surprised how easy it was all I do is
09:38 - just have to have my username password
09:40 - and then my credential which is what I
09:42 - get access to from my auth.json file
09:45 - which is inside of here and then I just
09:47 - use that as the URL in playwright for
09:49 - the browser I want to connect to also
09:51 - works with Puppeteer and selenium but
09:53 - it's literally like three or four lines
09:55 - of code and you're automatically
09:56 - connected to this scalable Network where
09:58 - you can run multiple instances of the
10:00 - scraper so I won't really run through
10:01 - all of the text here but you can see we
10:03 - fill the input field we press the search
10:04 - button we retrieve the products we grab
10:06 - all of that and then eventually what we
10:08 - do is we post the results here to our
10:10 - back end where they get submitted to the
10:12 - database now the core logic here is kind
10:14 - of connecting to the browser so you can
10:16 - see we're connecting over CDP to this
10:18 - browser URL generate a new page go to
10:20 - the URL we want to go to load the
10:22 - initial page Etc we then have amazon.pi
10:25 - which is actually what's responsible for
10:27 - scraping the search page on amazon.ca so
10:30 - it goes in gives us the image name price
10:32 - URL get access to all of that and then
10:34 - just return that for each individual
10:36 - product obviously there's a lot more
10:38 - logic that's going on here but that's
10:39 - the basics and really everything that
10:41 - you need to know so that's kind of how
10:43 - this works then I have my react front
10:44 - end don't need to walk through that and
10:46 - then lastly I have this scheduler here
10:48 - where all it does is just hit this URL
10:50 - here so update tracked products once a
10:52 - day the way this works is I've set up a
10:54 - Windows batch file that I put in my
10:56 - windows process scheduler where every
10:58 - day it's automatically going to run this
11:00 - where it spins up the API and then it
11:02 - will wait 10 seconds then call python
11:04 - main.pi which is right here which will
11:06 - send that post request which will then
11:08 - go retrieve all of the different
11:09 - products which will be spun up in
11:11 - parallel in separate processes where we
11:13 - are scalably grabbing all of this data
11:15 - and adding it to the API
11:18 - now just to quickly show you in case you
11:20 - want to do this for yourself what you
11:21 - would need to do is go to Bright data
11:23 - create a new account you can do that
11:24 - from the link in the description and
11:26 - access the scraping browser I'll leave
11:28 - some links in the description but you
11:29 - can see it kind of gives you some
11:30 - information here so from right data go
11:33 - in I'm gonna go to
11:35 - my proxies and scraping infrastructure I
11:38 - have scraping browser right here I'll
11:40 - click on that you can see if I went to
11:42 - uh kind of the parameters here it would
11:44 - give me like the hostname password Etc
11:46 - which I don't want to share with you and
11:47 - that's really all you need once you
11:49 - activate this and you have access to
11:50 - that information you put that inside of
11:52 - the file so you get your username
11:54 - password password host sorry and then
11:56 - you can just start using this there's
11:57 - really no more configuration required if
11:59 - we look at my stats here you can see I
12:01 - used it a ton when I was actually
12:02 - developing this project and then
12:04 - recently I've been running it a few more
12:05 - times I believe you pay per gigabyte
12:08 - um anyways super super useful tool and
12:11 - again you get some free credits if you
12:12 - want to check it out from the
12:13 - description so with that said I think
12:15 - I'm going to wrap it up here the last
12:16 - thing I will mention is that if you want
12:18 - to extend this project the next logical
12:20 - thing to do would to be to build an
12:22 - alert system where what this will do is
12:24 - automatically tell you in any of the
12:25 - prices change I haven't done that just
12:28 - because I figured that'd be a nice thing
12:29 - that you guys could work on if you want
12:30 - but what I would do to accomplish this
12:32 - is just simply check every time we
12:34 - submit new results if any of the prices
12:36 - have changed on the same product since
12:38 - the last call if they have then we just
12:40 - grab all of those products throw them in
12:42 - some type of email template send an
12:44 - email to someone or you could send a
12:45 - text message do whatever you want with
12:48 - that said I'm going to wrap up the video
12:49 - here I hope you guys found this helpful
12:51 - and this gave you some inspiration for a
12:52 - great project idea if it did leave a
12:55 - like subscribe to the channel and I will
12:56 - see you in the next one
12:59 - [Music]