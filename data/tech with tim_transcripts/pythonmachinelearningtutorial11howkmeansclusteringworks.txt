00:06 - hey guys and welcome back from their
00:09 - Python machine learning tutorial so in
00:11 - today's video we're gonna be talking
00:12 - about our first unsupervised learning
00:14 - algorithm which is called k-means
00:16 - clustering now this algorithm is
00:18 - different than the other ones that we've
00:20 - been using because we don't actually
00:22 - have to feed the labels for our data
00:24 - points when we are training the model
00:26 - all we do is we just give it a bunch of
00:29 - features that make up one point we don't
00:31 - say which points are which so in the
00:33 - case of what we're gonna be doing in the
00:34 - next video classifying handwritten
00:36 - digits we don't say this digital one
00:38 - this digits of five this digits of six
00:39 - we just say this is a digit this is a
00:42 - digit and this is a digit and then our
00:44 - computer has to figure out what makes
00:46 - what digit okay whereas before we were
00:49 - giving both like let's say the same
00:51 - example what made up the digit and we
00:54 - were giving the digit and all it had to
00:56 - do then was be able to predict given
00:58 - some data
00:58 - what digit it was doing so it was had a
01:00 - lot easier job than actually having to
01:02 - determine what features make up for
01:04 - example let's say a 7 or an 8 right in
01:07 - the computer and you'll see this as we
01:08 - go through the example now what I
01:10 - actually want to start doing is just I'm
01:12 - just gonna run this script by the way
01:14 - this is not exactly what we're gonna be
01:15 - making in the next video I just stole
01:17 - this off the SK learn website but it
01:18 - gives a really good visualization as to
01:21 - how k-means clustering works so wait for
01:25 - this to run and we get a lovely photo
01:27 - here that might look kind of confusing
01:29 - but essentially decay in k-means
01:32 - clustering stands for how many clusters
01:35 - now in the case of handwritten digits
01:37 - which is what we're gonna be classifying
01:39 - we have the digits 0 through 9 which is
01:41 - 10 digits so in this case you can see
01:43 - that we have 10 X is okay 10 white X's
01:46 - now these white X's are known as
01:49 - centroids and they are what's going to
01:51 - determine what a given data point
01:54 - what cluster that belongs to so you can
01:56 - see we have 10 clusters and they're all
01:57 - represented by all these different
01:58 - colors here right so anything that this
02:01 - falls inside of this red box here is
02:03 - gonna be a part of the red class which
02:06 - could be like a 6 it could be a 7 it
02:07 - could be an 8 we have to determine that
02:09 - anything that falls inside of this Brown
02:11 - point here is going to
02:13 - be another class anything inside the
02:15 - green and and so on okay and that's
02:18 - exactly how K means works it attempts to
02:20 - divide our graph of data points into a
02:23 - bunch of different let's say sections
02:24 - like this and then based on what section
02:27 - given point is that were trying to
02:28 - predict we can just say that it is a
02:31 - part of the closest what's known as this
02:34 - one of these X's okay so if we had a
02:36 - data point let's say you got to see my
02:37 - mouse right up here it's gonna say it's
02:39 - part of this pink place because it's
02:41 - closest to this white X so I'm gonna go
02:44 - through an example here but I just want
02:45 - to show this so you can kind of see how
02:47 - it works when we were dealing with a ton
02:48 - of different clusters because I'm just
02:50 - gonna show you an example to to make it
02:52 - easier so you might have heard of
02:54 - something called centroids before so
02:57 - essentially the way this algorithm works
02:58 - is we're gonna start off by creating two
03:00 - centroids in a random position in our
03:03 - graph so typically we're going to deal
03:05 - with multi-dimensional graphs right
03:06 - where we have like ten twenty thirty
03:08 - different features for each of our data
03:11 - points right so we'll just randomly
03:13 - choose a place to put two centroids on
03:15 - our graph if we're dealing with K equals
03:17 - two now if we were gonna deal with K
03:19 - equals like five K equals ten K equals
03:21 - twenty that's how many centroids we
03:22 - would put onto our graph in a random
03:25 - position and you'll see how this works
03:26 - in a second okay so I'm just gonna put a
03:28 - centroid and I'm gonna mark these by
03:30 - triangles here and I'm gonna put a
03:31 - centroid this blue one up here okay
03:34 - just in random positions now note that I
03:36 - could put them like right beside each
03:38 - other it doesn't matter where I put them
03:40 - it just completely random that we're
03:41 - gonna generate this okay now what we're
03:44 - gonna do once we have these two random
03:45 - centroids and we're gonna do this a
03:47 - bunch of time so don't worry if if it's
03:49 - like how was this gonna work it's gonna
03:50 - happen a bunch of times what we're gonna
03:52 - do is we're gonna repeat the process
03:53 - that I'm just about to go through so
03:54 - we're gonna draw a straight line between
03:56 - our two centuries okay and then we're
04:00 - gonna try to divide all of the points
04:02 - that are here to be either a part of
04:04 - this red Sun centroid or this blue
04:07 - centroid okay so what we do is that we
04:09 - now will take a line like this and we'll
04:12 - draw it through the midpoint of the line
04:14 - connecting our centroids and anything on
04:17 - this side is gonna be a part of the red
04:19 - centroid and anything on this side is
04:21 - gonna be part of the blue Center okay so
04:23 - that's what we do first of all so I'll
04:25 - get rid of all this drawing
04:26 - and we'll just say that she will go back
04:30 - here then now we're gonna assign this
04:32 - points gonna be blue like this like all
04:34 - these points that were on that side of
04:36 - the line will be blue and then all the
04:38 - other points are going to be red like
04:40 - this so red red red red red now how does
04:45 - this work and why are we doing this well
04:47 - essentially the reason that we'd say
04:49 - this point here is a part of the red
04:51 - centroid is because it's closer to the
04:53 - red centroid than it is to the blue
04:55 - centroid now that goes for every other
04:57 - point we're just gonna find the points
04:59 - that are closest to each centroid we're
05:00 - gonna say okay so is this point closer
05:02 - to the red centroid or the blue centroid
05:04 - whatever one is closer to we're just
05:06 - going to assign it that value so in this
05:07 - case we give it red same thing with
05:09 - these points over here and you know this
05:10 - one actually we might want to say is
05:11 - probably a part of the blue one so I
05:13 - guess we can change that just for the
05:15 - purpose of the example here okay so
05:16 - we'll say that's blue this one might be
05:18 - kind of in between anyways so that's
05:20 - what we're gonna do now that we've done
05:22 - that we need to do something else we
05:24 - need to find the center of these points
05:27 - so essentially all these blue points
05:30 - here we want to put this centroid in the
05:32 - middle of these blue points so what
05:34 - we're gonna do is well we're gonna erase
05:36 - this centroid and we're gonna find the
05:39 - middle of these points now the way we
05:40 - can do this so I'm gonna say the middle
05:41 - is probably somewhere about like here
05:44 - the way that we can do this is by just
05:46 - simply taking the average of these
05:47 - points so what we do is we take x1 plus
05:50 - x2 plus x3 for all of our points all of
05:54 - our blue points and we divide it by the
05:56 - amount of points so we'll say the amount
05:59 - of points is n now that will give us the
06:01 - X 1 coordinate or certain like so X 1 1
06:05 - X 2 1 like if you're talking about this
06:07 - is 2nd point this is third point I think
06:09 - you guys get that for like the x
06:10 - coordinates anyways so we'll do that
06:13 - that will give us the first coordinate
06:14 - for the centroid then what we'll do is
06:16 - we'll do the same thing except now we'll
06:18 - talk about x2 so we'll say like point 2
06:21 - x2 plus point 1 x2 plus point 3 x2 and
06:29 - again we'll divide that by the number of
06:31 - points and that'll give us the next
06:33 - coordinate for our century and that's
06:34 - how we can determine the the middle of
06:36 - these kind of data points okay we render
06:38 - the exact same
06:39 - thing for our red centroid so what we'll
06:42 - do now is we'll take this and we'll draw
06:45 - our red centroid let's say somewhere
06:47 - about here okay and again the way we did
06:50 - that we just took the average of the
06:51 - coordinates for all of the points that
06:53 - are red and all the points that are blue
06:55 - so now what we're gonna do is we're
06:57 - gonna repeat the steps that we just did
06:59 - so we're gonna again we're gonna draw a
07:02 - line between these two centroids we're
07:04 - gonna find the midpoint of this line and
07:05 - we're just gonna draw ninety degrees
07:07 - down like this okay
07:09 - and now what we're gonna do is we're
07:11 - gonna reassign points so now we're gonna
07:13 - we're gonna forget that these points
07:15 - were blue and these points were red and
07:16 - we're gonna reassign them so in this
07:18 - case we need to change well this red
07:20 - point to blue and we need to change this
07:23 - blue point to red right because again
07:25 - now this point is closer to the red
07:27 - centroid that is to the blue centroid so
07:29 - it's gonna be red and then maybe with
07:31 - this one that was red before it's now
07:33 - close to the blue centroid so it has to
07:35 - be blue and we're gonna repeat this
07:37 - process until eventually we get no
07:40 - changes between our data points right so
07:44 - now again what we're gonna do is we're
07:46 - going to redraw our centroids so we need
07:49 - to find the average once again except
07:51 - this time when we find the average we're
07:52 - gonna use the what do you call it the
07:55 - new points that we just created so the
07:57 - red centroid maybe it goes like
07:59 - something like this and the blue
08:01 - centroid maybe goes somewhere around
08:03 - here like that okay and now that we have
08:07 - the average we render the exact same
08:09 - thing and we're just gonna keep going
08:10 - until eventually we have no change so if
08:13 - I do this and I find the midpoint and
08:16 - then I draw a straight line well let's
08:18 - just pretend that like this what do you
08:21 - call it this point is over here on this
08:22 - side well now there's no change between
08:24 - our data points none of the points here
08:26 - have changed to be red and now the
08:28 - points here have changed to be blue so
08:31 - we can officially say that we've
08:33 - clustered our points into two separate
08:36 - clusters right so all the points that
08:38 - are red here and in this side of the
08:40 - graph are gonna be closest to the red
08:41 - centroid so they're red and all the ones
08:43 - on this side are closest to the blue
08:45 - centroid so they're blue so now if we
08:46 - have a black point let's say like up
08:48 - here we
08:50 - say that since it's closest to the blue
08:52 - centroid we should predict it as blue
08:54 - and that's essentially how the k-means
08:57 - algorithm works so we keep going through
09:00 - a bunch of iterations of essentially
09:02 - creating these centroids so I'll bring
09:04 - this one back and then we average the
09:06 - centroid out and we put it in the middle
09:07 - of the points that we just decided were
09:09 - there and we just keep going and keep
09:11 - going until eventually we get to a point
09:13 - where our centroids are so good that
09:15 - every time we redo the calculations
09:18 - nothing changes and our centroid stays
09:20 - in the exact same place I hope that
09:22 - makes sense in terms of how that works
09:25 - in clustering now obviously if we're
09:27 - doing with like three clusters well what
09:29 - we're gonna have to do now is we'll say
09:31 - well this point is gonna be part of the
09:33 - Bloch cluster this point will be a part
09:34 - of the Bloch cluster as well because
09:36 - it's closest to the Bloch centroid and
09:38 - then we'll do the exact same thing
09:40 - except we're just gonna repeat the
09:41 - process with three centroids right
09:43 - instead of two and that is how k-means
09:46 - clustering works and now it's time to
09:48 - talk quickly about like some advantages
09:50 - disadvantages of it so essentially speed
09:53 - okay so let's think about this
09:54 - realistically we have two for every
09:57 - single point in our data set determine
10:00 - the distance between not only one of the
10:02 - centroids but the other century which
10:04 - means that if we have let's say we bump
10:06 - this up to four right now we have to do
10:09 - two times the amount of calculations for
10:11 - every single point because we got to
10:13 - determine the distance from this point
10:14 - to here to here until two other
10:17 - centroids and then we have to now figure
10:19 - out which one is closest and assign this
10:21 - point to that centroid right we have to
10:23 - do that for every single one of our
10:25 - points which means that we're gonna have
10:26 - like the number of points so let's say P
10:29 - multiply it I'll just do that by a dot
10:30 - by the number of centroids which will be
10:34 - C but now we don't only have to do this
10:37 - once we have to do this until every
10:39 - single data point is what do you call it
10:42 - not moving right until we get to a point
10:44 - where our centroids are so well defined
10:46 - that we're not moving so that's now by
10:48 - the number of iterations right but guess
10:51 - what we don't normally have to do with
10:53 - this many times we have to do it based
10:55 - on how many features we have because if
10:57 - we have x1 we have x2 and let's say we
11:00 - have up to
11:01 - 700 which is actually very possible that
11:04 - we can have that many features we now
11:05 - have to do this
11:07 - 700 times so times the number of
11:10 - features I think I butchered that word
11:13 - but anyways you guys know what I mean
11:14 - okay so we have to do the number of data
11:16 - points times the number of centroids
11:18 - times the number of iterations so how
11:20 - many times we resent are those centroids
11:22 - and change the data points times the
11:24 - amount of features so that is well it's
11:27 - gonna take a long time compared to some
11:28 - of our other machine learning algorithms
11:30 - but it's actually a lot slower or a lot
11:33 - faster sorry then some other clustering
11:36 - algorithms so you can imagine though
11:37 - like if you have a ton of features and a
11:40 - ton of data points this is gonna take a
11:43 - decent amount of time to perform for us
11:45 - it's not gonna take more than like 30
11:46 - seconds but if you have a ton of
11:49 - features and a ton of data points then
11:52 - it's gonna take a long time because
11:53 - right si is probably not gonna change
11:55 - that many that much because it's
11:56 - probably only gonna be up to like 10 or
11:58 - 20 for the amount of centroids because
12:00 - you're not gonna have that many
12:00 - different classes and the number of
12:02 - iterations well this could be enough
12:03 - like a few hundred but our data points
12:05 - is probably gonna be the most
12:06 - influential parameter here right so yeah
12:09 - so anyways let's just go back to this
12:11 - example really quickly I'll talk about
12:13 - it one last time with the image here
12:16 - okay so it's having some weird issue
12:18 - with my mouse but anyways we'll run this
12:20 - now and let's look at the centroids and
12:22 - now we hopefully have a better idea of
12:23 - exactly what we've done here right so
12:25 - these centroids started off in random
12:27 - positions maybe we had one up here we
12:29 - had one down here we had them all over
12:30 - and essentially we found the points of
12:32 - her closest to them we said that it's
12:34 - gonna be that centroid like it's gonna
12:36 - be a part of that we average them out so
12:38 - we move the centroid to be in the middle
12:40 - of all those points that are belonging
12:42 - to that centroid then what we did was we
12:45 - perform that operation again and again
12:48 - and again and again until eventually we
12:50 - get something that looks like this where
12:52 - every time that we find points that are
12:55 - closest to each centroid and we average
12:57 - them out we're not we're not moving the
13:00 - centroid at all right so here like final
13:02 - points all these points that are here if
13:04 - we take the average of all of these
13:05 - points and we move the centroid it
13:08 - doesn't move and that's how we know that
13:10 - we essentially found the best possible
13:12 - cluster for our data set
13:14 - and this is just done in two dimensions
13:16 - just so you can see but obviously this
13:18 - is happening in like a crazy high
13:19 - dimension that's just impossible to
13:21 - visualize so anyways that is how K means
13:23 - clustering works as always if you guys
13:25 - have questions please don't hesitate to
13:26 - ask me in the comments you can join my
13:29 - discord server ask for help on there as
13:30 - well and make sure to follow my Twitter
13:32 - in the next video we're gonna get into
13:33 - actually using the k-means clustering
13:35 - algorithm and then we'll be done with
13:37 - machine learning at least for like a
13:38 - week or so until we move into some more
13:39 - neural networks
13:43 - [Music]