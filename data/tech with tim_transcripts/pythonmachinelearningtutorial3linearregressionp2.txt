00:00 - hello and welcome back to part two of
00:03 - linear regression machine learning
00:05 - tutorial so in today's video we're gonna
00:07 - be actually implementing the algorithm
00:09 - and using it to predict and test or
00:12 - train our data sets and models and
00:14 - whatnot
00:15 - but at first before I move into that I'm
00:17 - actually gonna be explaining what linear
00:18 - regression is using a few drawings and
00:21 - kind of some mathematical proofs and
00:23 - whatnot so if you guys would like to
00:26 - understand how this is actually working
00:27 - please watch through the entire video if
00:30 - you're more interested in just the
00:31 - coding aspect of it that's fine just
00:33 - skip through to about like four or five
00:34 - minutes or essentially when I'm done
00:36 - doing all the drawings you kind of tell
00:38 - that by just looking at the preview in
00:39 - the little slider all right but I do
00:42 - recommend you watch this because it'll
00:44 - give you a fundamental understanding
00:45 - that's pretty interesting and I found it
00:47 - interesting as well so anyways let's
00:49 - talk about what linear regression is so
00:52 - linear regression is a very basic
00:53 - algorithm and what that algorithm
00:55 - attempts to do is it looks at a scatter
00:57 - of data points and attempts to find a
01:01 - best fit line to those data points so if
01:05 - I add or I'll create like a little plot
01:08 - here of some data and I will show you
01:11 - what a best fit line to this data might
01:14 - look like okay so this our data I'm just
01:18 - gonna say say that this is our y axes
01:21 - like this and this is our x axis or axis
01:23 - I don't know whatever you say it and our
01:25 - best fit line to this data point would
01:27 - look something like this okay it's not
01:30 - perfect but something along this line
01:32 - and we can see there's some kind of
01:34 - correlation between x and y where as x
01:36 - increases Y increases as well and we can
01:39 - see that it starts kind of at a certain
01:41 - value alright so we see there's some
01:43 - sort of correlation so we can actually
01:45 - draw this best fit line to this data
01:48 - okay and this is what linear regression
01:50 - is going to attempt to do now I just
01:53 - want to show actually another data set
01:55 - and show when linear regression may not
01:58 - be the best instance or best algorithm
02:01 - to use okay so if we have data that kind
02:04 - of is more randomized so I don't know
02:07 - I'm just like a bunch of data points all
02:09 - over the place we
02:12 - actually still draw a best-fit line to
02:16 - this but it's not going to be very good
02:19 - right it's not gonna be a good fit line
02:21 - it's going to be a best fit line so
02:24 - essentially we can draw a line and you
02:26 - might want to do something like
02:27 - downwards like this but you might want
02:28 - to go across like you don't really know
02:30 - exactly what to do just by looking at it
02:32 - in a case like that we're not gonna want
02:34 - to use the linear regression algorithm
02:35 - linear regression is when we have data
02:38 - that directly correlates to each other
02:41 - so based on for example the students
02:43 - first grade we can kind of somewhat
02:46 - predict what their final grade is gonna
02:48 - be if there's any kind of correlation
02:50 - that is somewhat strong like some kind
02:53 - of strong correlation then that is when
02:55 - you would use linear regression okay so
02:58 - let me just go back to that first
02:59 - example I had so let's just draw or I
03:03 - don't want to use red as my axes we'll
03:05 - use block here and we'll draw another
03:08 - best fit line on some data points and
03:10 - talk about how the math goes on actually
03:13 - using this best fit line
03:14 - - well predict data so again this is our
03:17 - Y this is our X that's gonna be our data
03:20 - of some outliers over here and then
03:22 - we'll have a best fit line that maybe
03:24 - looks something like that okay so if any
03:28 - of you have taken high school math so
03:30 - probably grade 9 or 10 math is linked
03:32 - when I learned it this line can actually
03:34 - be defined by an equation and that
03:36 - equation is y equals MX plus B now some
03:41 - of you may know this as like y equals ax
03:43 - plus B M is the same as a in this
03:46 - equation okay so essentially what this
03:49 - means is Y is dependent on like this
03:52 - equation right here okay and we can find
03:55 - Y by just manipulating these numbers and
03:58 - evaluating this equation now what do
04:00 - these constants mean though so
04:03 - essentially M is gonna be the slope of
04:05 - our line and if you don't know what
04:06 - slope is that is just how fast our line
04:09 - increases we can have a negative slope
04:11 - or a positive slope in this case it is
04:13 - positive because as x increases Y
04:15 - increases you can see our slope can
04:17 - actually be found by taking two points
04:20 - on the line so if I have a point here
04:23 - we're gonna have another point here
04:24 - excuse me what we actually do is we can
04:26 - say at this point we're gonna call this
04:27 - p2 okay and this point here is called p1
04:32 - well these points have an x and a y
04:34 - value and to find the slope all we have
04:36 - to do essentially is say the y2 value of
04:38 - this point minus the Y 1 value over top
04:41 - of the x2 value minus the x1 value is
04:45 - actually equal to our slope and I can
04:49 - prove it to you but I'm not gonna do
04:50 - that because this again is not a math
04:52 - class but this is how you can actually
04:53 - determine what the slope of this line is
04:55 - how much it increases by now obviously
04:57 - if you pick a point like you can do this
05:01 - the other way around as well like you
05:02 - knew y1 minus y2 over x1 minus x2
05:05 - that'll work fine because if you have
05:07 - two negative values they're gonna cancel
05:08 - out and give you a positive but if you
05:10 - had a line that looks something like
05:11 - this you're gonna get a negative slope
05:13 - value because your point one will be up
05:15 - here your point two will be down here
05:17 - you're gonna be subtracting a larger
05:18 - value from the tops you're gonna get a
05:19 - negative number right but anyways that's
05:22 - enough on kind of deep math for that so
05:23 - let's just erase this we don't need this
05:25 - anymore so that is our slope and that is
05:28 - what M is ok how much our line increases
05:30 - by but what is B well B is actually our
05:34 - y-intercept all right and that is where
05:36 - I'm kind of doing this big dot right now
05:38 - where we start our line at so
05:41 - essentially right this is our
05:43 - y-intercept because this is where our
05:44 - line intercepts the y-axis and our line
05:46 - will actually keep going down in this
05:48 - direction as well if we add like a
05:51 - negative x-values right so that's what B
05:53 - stands for and essentially what the
05:54 - computer does when we give it all of
05:56 - this data when using linear regression
05:57 - is it creates this line okay and then
06:00 - when we want to predict a value so it
06:03 - uses this equation to predict the value
06:05 - so say I have a student and say this is
06:07 - like representing their grade 1 and this
06:09 - is representing their final grade if I
06:11 - have a grade 1 of say like 17 because
06:14 - our grade 0 to 20 it's gonna plug 17
06:16 - into this equation I'm gonna say y is
06:18 - equal to whatever our M value is times
06:20 - 17 why is it doing that plus our B value
06:23 - and sorry I just buttered all of that
06:26 - writing but you get what I mean
06:28 - so M times 17 plus B it's going to
06:31 - generate a Y and now Y is actually going
06:33 - to be our predictive value in what we
06:35 - predict now this is really simple if you
06:38 - look at it in 2d space but we actually
06:40 - have multiple variables who are
06:42 - attributes when we create this best fit
06:44 - line and so it's gonna create a best fit
06:46 - line in multi-dimensional space for us
06:48 - which I can't really show you because
06:50 - it's actually impossible to visualize
06:51 - but I can show you an example of what it
06:53 - looks like in 3d space really quickly
06:56 - okay how much time we get six minutes so
06:58 - I'll just keep going and get this
07:00 - explanation out essentially if this is
07:02 - our origin so zero zero zero we could
07:04 - have if we have two variables like say
07:07 - we have G 1 and G 2 and this is gonna be
07:11 - our G 3 well we can create a best fit
07:14 - line that kind of goes through like
07:16 - coordinate space like something like
07:19 - this now this seems weird but
07:20 - essentially if this is on like G 1 G 2
07:24 - and then we have like this we can create
07:26 - it in multi dimensional space using all
07:29 - these different variables now you don't
07:30 - really have to understand like how this
07:32 - is working but that's what the computer
07:35 - is doing for us it's using all these
07:36 - attributes to create a best fit line in
07:38 - like whatever space how many attributes
07:41 - we have okay so I think that's all I'm
07:43 - going to talk about for linear
07:43 - regression essentially it's a best fit
07:45 - line y equals MX plus B and I'm actually
07:47 - gonna get the constants of that line for
07:49 - us so I can show you how it works okay
07:51 - so now let's move into actually coding
07:54 - all right so to actually code this best
07:56 - fit line let me just put my tablet away
07:58 - what we need to do is we need to create
08:01 - a like a training model so I'm just
08:03 - gonna say linear and this is again in PI
08:05 - trim is equal to linear model dot linear
08:10 - regression okay and this is just gonna
08:12 - be uh no I put a semicolon used to Java
08:14 - now is gonna be what we're working with
08:16 - so linear regression and what I'm gonna
08:18 - do now is I'm literally just gonna type
08:20 - linear which is our model don't fit and
08:25 - then I'm gonna give it X underscore
08:27 - train Y underscore train like this okay
08:31 - and what this is gonna do is it is just
08:33 - gonna fit this data to find like a best
08:36 - fit line all right
08:37 - using the X train data in the why train
08:39 - data it's going to store that line in
08:41 - linear and then we can use that to
08:43 - actually test our test add-on so now
08:46 - that we have this line because we fitted
08:47 - the model to it
08:48 - whatever we can actually do linear dot
08:51 - score and then in here we can actually
08:55 - score X test and Y test now this is
08:59 - gonna return to us a value that's gonna
09:01 - represent the accuracy of our model so
09:03 - we're gonna say AC C equals linear dot
09:05 - score and what this is gonna stand for
09:07 - is just accuracy and we're just gonna
09:09 - print this out to the screen to see how
09:12 - accurate we're actually getting and this
09:14 - is literally all you have to do to
09:16 - actually create a model and see how well
09:20 - that model is working or how well the
09:22 - algorithm is working okay this is all
09:25 - you have to do so I'm just gonna run
09:26 - this and instead of samples what's the
09:31 - issue here give me one second guys
09:32 - extra my dream okay so I look through
09:35 - here and I realized I've made a very
09:37 - slight mistake so what actually need to
09:39 - do here just swap this test and train
09:41 - variable so Ali's gonna say and say X
09:44 - underscore test and Y underscore train
09:48 - and this should be hopefully fixing
09:51 - things for us my apologies about that
09:52 - guys so run this now and now we can see
09:54 - we're actually getting an accuracy of
09:56 - eighty six point eight percent
09:58 - we're like zero point eight six like out
10:00 - of one right okay and that's not bad
10:02 - obviously we can do better but for
10:05 - actually determining students grades
10:07 - because it's not a like it's extremely
10:09 - concrete's mathematical thing that we
10:12 - can do with that that's pretty decent
10:14 - that based on just these six attributes
10:17 - of these five attributes g1g to study
10:19 - time failures and absences we can
10:21 - determine with eighty six point eight
10:22 - percent accuracy what a student's grade
10:25 - is going to be at the end of the year
10:27 - and I think that's pretty cool but
10:29 - that's what we're a lot of people will
10:31 - stop right this okay you got eighty six
10:32 - percent accuracy that's great but how do
10:35 - we actually use this model now so now
10:37 - that we've created this model we want to
10:40 - use and actually test it on data and see
10:43 - like what we're actually getting that's
10:45 - what I'm gonna do is first I'm gonna
10:47 - show you the constants because remember
10:49 - I was telling you how this works with
10:51 - like y equals MX plus B I'm gonna show
10:53 - you what those coefficients are so what
10:55 - M is and what B are for this actual line
10:58 - so to do this I'm just gonna print out
11:01 - we'll just say Co has in like
11:03 - coefficients plus linear and dot
11:08 - coefficients like this and it's gonna
11:10 - actually give us a list of all the
11:12 - different coefficients and I'm going to
11:14 - print out intercept like this okay and
11:18 - again plus and now linear dots intercept
11:22 - and this is gonna show us the
11:24 - y-intercept so if I run this did not get
11:29 - a loop with signature matching type one
11:30 - second let's see what the issue is here
11:33 - okay so he turns out the issue was uh
11:35 - what do you call it that I was trying to
11:37 - add these things together because now
11:38 - I'm used to job programming anyways
11:40 - we're just gonna throw a little comma
11:41 - here I just added this coefficient and
11:43 - backs list and so it goes in the new
11:45 - line essentially about sorry that was
11:47 - the issue I was trying to add these
11:48 - things together we just have to do comma
11:49 - okay and you see actually I ran this
11:51 - just to make sure this is working and
11:52 - down here now we're actually getting all
11:55 - the coefficients of our five different
11:58 - variables and we're getting our
11:59 - intercept which is negative one point
12:01 - four seven right you can see the
12:03 - accuracy has dropped a little bit but
12:04 - that's gonna happen once in a while our
12:07 - accuracy is gonna fluctuate within a few
12:08 - percentages just because when we train
12:11 - it it's gonna be different each time so
12:13 - yeah anyways so this is actually these
12:16 - coefficients here that I'm showing sorry
12:18 - down here are actually those that the M
12:21 - coefficient so I was showing you y
12:22 - equals MX plus B now that's the equation
12:25 - for a line in two-dimensional space a
12:28 - line in five dimensional space needs
12:32 - five coefficients so like five kind of
12:35 - M's you can think of it as like M X plus
12:38 - MX plus like I don't know like Zed y
12:41 - plus C W like a bunch of different
12:44 - variables they all have their own
12:45 - coefficients and these are actually the
12:47 - coefficients that we've generated for
12:49 - those variables and you can kind of see
12:51 - the bigger the coefficient the more
12:52 - weight it actually each what he call it
12:56 - attribute actually has in are like
13:00 - defining you know what grade we're gonna
13:02 - get and this is our intercept so yeah so
13:05 - that's great we got our coefficient and
13:06 - we have our intercept but now I actually
13:08 - want to show you how this works on a
13:10 - real student so how we can use this to
13:12 - predict base
13:13 - a student's information what great
13:15 - they're gonna get cuz right now we're
13:17 - just getting a bunch of numbers I'm
13:17 - actually seeing any like no good output
13:21 - on how this work so you'll see what I
13:22 - mean in a second but essentially what
13:24 - I'm gonna do you can use this model to
13:25 - predict I say linear dot predict and I'm
13:29 - going to say X underscore test okay so
13:32 - what I'm gonna do here I'm gonna store
13:33 - this in predict Qin's
13:36 - even how you spell it I don't know but
13:38 - predictions equals linear to predict X
13:40 - underscore test what I'm gonna do is I'm
13:43 - gonna actually print out all the
13:45 - predictions and then I'm gonna show you
13:47 - what the input data was for that
13:49 - prediction so essentially this is gonna
13:51 - take an array or an array of arrays in
13:55 - our case and it's going to do a ton of
13:57 - predictions and guess what all these are
13:59 - on the test data that we did not train
14:01 - our model on because we only trained it
14:02 - on the train data right and then we'll
14:04 - see what we're actually like what that
14:06 - input was so I'm just saying say for X
14:07 - in range and then if I spelled range
14:11 - correctly will do the length of our
14:14 - predictions okay
14:16 - and then what we're going to do so used
14:19 - to Java now sorry guys well we're going
14:21 - to just print out the prediction so in
14:24 - this case we'll do predictions X and
14:27 - then we'll also print out what that
14:29 - input data was so that input data is
14:31 - gonna directly correlate to X underscore
14:32 - test X value and then we're gonna print
14:36 - out what the actual value of the final
14:37 - grade was which is going to be Y
14:39 - underscore test X and now if I run this
14:43 - you'll see we actually get some pretty
14:45 - interesting output so we got a bunch of
14:47 - stuff printing out here cuz just
14:48 - printing all of them but essentially
14:50 - right you can see we have fifteen point
14:53 - two two one nine right as our answer
14:55 - okay and the mid or the beginning grade
14:59 - was fifteen their end of term grade or
15:02 - like second semester grade was that they
15:04 - had two hours of study time they had
15:08 - zero failures and they had four absences
15:10 - and their actual grade was fifty so we
15:12 - got pretty close by getting fifteen
15:14 - points you and I believes we're getting
15:15 - a ton of decimal numbers but these are
15:17 - always gonna be rounded off so we could
15:18 - actually round these and get better
15:19 - accuracy but that's fine for now so this
15:22 - is one where we made mistake right you
15:23 - can see that their first grade was sick
15:26 - so there
15:27 - grade was 5 they had one our study time
15:29 - one failure and they got a grade of zero
15:33 - and we were saying they were gonna get a
15:35 - grade of four okay so we got low but we
15:37 - still didn't get the correct answer here
15:39 - again right 14 the correct answer was
15:42 - fourteen twelve twelve thirteen twelve
15:45 - point seven nine ten nine point four
15:47 - nine eight right so we're getting very
15:48 - very close to the actual answers and you
15:52 - guys can look through these and kind of
15:53 - play around with them but essentially
15:54 - that's how you'd use it to predict based
15:57 - on input what are what the grade would
16:00 - be and you can see we're getting like
16:01 - pretty close with most of these answers
16:04 - at least within one or two points for
16:05 - almost all of them which i think is kind
16:07 - of amazing because only in like 20 or 30
16:10 - minutes we've been able to set this up
16:11 - understand how it works and now we can
16:13 - actually use this on like real in
16:16 - real-life situations to predict a
16:18 - student's certain grade based on these
16:21 - pieces of information anyways that has
16:24 - been it for linear regression in the
16:27 - next video I'm actually not sure what
16:28 - we're gonna talk about but it's gonna be
16:29 - another basic machine learning algorithm
16:31 - a bit more advanced in this we're gonna
16:33 - go through and play with all of them
16:35 - understand how they work and then again
16:37 - move into neural networks so if you guys
16:38 - enjoyed the video please make sure you
16:39 - leave it like you guys should definitely
16:41 - follow my Twitter if you want to know
16:42 - when the next video is coming out and
16:43 - with that being said I guess I'll see
16:45 - you in the next video
16:46 - [Music]