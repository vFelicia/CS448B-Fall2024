00:03 - foreign
00:08 - [Music]
00:14 - tool that allows you to test experiment
00:17 - and deploy machine learning models in
00:19 - the matter of minutes that's right
00:21 - within a few minutes and a few terminal
00:23 - commands you can have this up and
00:24 - running on your machine and server and
00:26 - allow people in your organization your
00:29 - team your friends whoever it may be to
00:31 - mess around with your machine learning
00:33 - models this is also quite useful if you
00:35 - want to deploy already trained models
00:37 - and just see how they work on your own
00:39 - local hardware so with that said let me
00:40 - give you a quick demo of how all of this
00:42 - works then I'm going to explain to you
00:44 - more about the tool kind of what it is
00:45 - how it works and we'll go from there
00:47 - alright so in front of me I have two WSL
00:49 - terminals open on my Windows machine
00:51 - that are running Ubuntu 20.0.4 now the
00:55 - one on the left is running the server
00:57 - and the one on the right is going to be
00:58 - running the client now the point of this
01:00 - demonstration is to illustrate how
01:02 - simple it can be to interact with your
01:04 - deployed machine learning models after I
01:06 - give you this demo then I'm going to
01:07 - walk a little bit through the code and
01:09 - of course I'm going to elaborate on the
01:10 - tool now the tool that we're using
01:12 - framework live Library whatever you want
01:13 - to call it is called Pi Triton now this
01:16 - comes from Nvidia this is a wrapper
01:18 - around their Triton inference server
01:20 - which is pretty much the state of the
01:22 - art when it comes to deploying testing
01:23 - and working with machine learning models
01:25 - this is used by all kinds of different
01:27 - companies Pi Trident is relatively new
01:30 - Nvidia has kind of giving me a brief and
01:31 - I'm teaming up with them for this video
01:33 - because I wanted to make you guys aware
01:35 - of this obviously it's completely free
01:36 - all of the information you need will be
01:38 - available in the description but let's
01:40 - have a quick look at this demo so on the
01:42 - left I've got the Trident inference
01:43 - server running and this is actually
01:45 - going to kind of wrap or allow me to use
01:48 - what's known as a stable diffusion model
01:50 - this model essentially takes text input
01:53 - prompts and then generates some kind of
01:55 - image now on the right side of my screen
01:56 - it's a bit small but you can see down
01:58 - here it says what image would you like
01:59 - to be created this is my client script
02:02 - that's going to interact with this
02:03 - server so on this terminal I'm just
02:05 - going to type a dog in a park okay and
02:09 - hit enter it's going to ask me what file
02:10 - name I would like to save this image as
02:12 - I'm just going to go with test dot PNG
02:14 - and then it's going to take a second
02:16 - here you're going to see that the
02:17 - request will be sent over here to my
02:19 - server where it's going to be executed
02:20 - on my GPU and in a few seconds this will
02:23 - be finished it will save the image and
02:24 - I'll show you what the image looks like
02:26 - alright so the image has now been saved
02:28 - I'm just going to open it up so you guys
02:29 - can see what I get here so this is the
02:32 - image that was generated a dog in the
02:34 - park again this is using a stable
02:35 - diffusion machine learning model which
02:37 - is really a pipeline of multiple models
02:39 - that create this image now just to
02:41 - demonstrate the capabilities of a model
02:43 - like this I'll do a more complicated
02:44 - prompt so something like an alien
02:48 - riding a horse
02:50 - in a desert okay so let's save this as
02:54 - alien Dot
02:56 - PNG let's give that a second and then
02:59 - have a look at the result alright so
03:00 - here we are this is the resulting image
03:03 - I don't know if this is quite an alien
03:04 - but it is riding a horse in the desert
03:06 - so pretty good and obviously you can
03:08 - mess with this and ask it to create all
03:10 - kinds of different images the point here
03:11 - was just to demonstrate how simple it
03:13 - was for me to actually run this program
03:15 - so now that I've given you this quick
03:16 - demo I want to briefly discuss what's
03:18 - typically involved in deploying a
03:19 - machine learning model so you can
03:21 - actually see the benefit of using a tool
03:22 - like Pi Trident alright so I've just
03:24 - opened up the pi Trident GitHub page to
03:26 - go through some of the issues you might
03:28 - encounter if you're trying to deploy a
03:29 - machine learning model on your own now
03:32 - first of all you might use something
03:33 - like flask right this is a pretty
03:35 - popular web framework it's lightweight
03:37 - it's easy to use easy to learn the issue
03:39 - with using something like that for a
03:41 - machine learning deployment is that
03:43 - you're not going to have a lot of the
03:44 - features that you want pre-built into
03:46 - the flask application you're going to
03:48 - have to write a lot of custom code if
03:49 - you want this to be scalable you're
03:51 - going to have to implement features like
03:52 - Dynamic batching you're going to run
03:54 - into a whole host of issues that's going
03:56 - to take you significant amount of time
03:57 - to actually get a scalable deployment
03:59 - just looking at this diagram here you
04:01 - can see that some of the features you
04:02 - might want in your flask application is
04:04 - dynamic batching managing models
04:06 - supporting different Hardware
04:07 - environments managing multiple
04:09 - Frameworks like Pi torch tensorflow Etc
04:12 - having multiple gpus running model
04:15 - versioning runtime optimization there's
04:17 - a million things that you would have to
04:19 - write on your own and it's just really
04:20 - not time effective to do that so you can
04:23 - continue to read through here and I'm
04:24 - sure if any of you have attempted to
04:25 - deploy machine learning model you can
04:27 - relate to some of these struggles but
04:29 - this is why you would use something like
04:31 - the pi Trident inference server so now
04:33 - it's time to discuss what pi Trident is
04:35 - well Pi Trident is really the python
04:38 - wrapper around nvidia's Trident
04:40 - inference server it provides a flask
04:42 - slash fast API like interface that makes
04:45 - it very easy for you to essentially bind
04:47 - a function to an API endpoint that can
04:50 - then serve machine learning results in a
04:52 - machine learning model I'm going to show
04:54 - you a code example in a minute but
04:55 - essentially Pi try implements all of
04:58 - those features that you would want in
04:59 - your own flask server for you so that
05:02 - means you can spend as little time as
05:03 - possible deploying your machine learning
05:05 - model and just use all of these built-in
05:07 - features and functionality this is
05:09 - regularly optimized it works on pretty
05:11 - much any hardware platform it supports
05:13 - all of your major machine learning
05:15 - models and platforms or development kits
05:17 - like Pi torch tensorflow openvino SK
05:21 - learn name on it probably supports that
05:24 - obviously it's very optimized it allows
05:26 - your parallelism between your CPU GPU
05:29 - and just tons of features that honestly
05:30 - I'm not even qualified to talk about and
05:33 - for anyone that does work with machine
05:34 - learning models I'd highly recommend
05:35 - checking it out even for myself who's
05:37 - not a machine learning engineer or a
05:39 - machine learning expert I found it
05:41 - relatively simple once I got the setup
05:43 - done to actually be able to deploy
05:45 - different machine learning models and
05:46 - for me it was cool because I could
05:47 - actually test out all of these
05:49 - interesting models on my own hardware
05:51 - and my own machine with relative ease so
05:53 - lastly I'll just mention here that in
05:55 - addition to all of the features provided
05:57 - by pi Trident this really is just easy
05:59 - to use it's going to be cost effective
06:01 - and scalable it's supported by Nvidia
06:03 - and this is something that works with
06:05 - Docker kubernetes Etc so it really is
06:07 - truly scalable it's going to make your
06:09 - life a lot easier compared to trying to
06:11 - work in flask now I know it kind of
06:13 - sound like a salesman here but the
06:15 - reality is all of this is free you don't
06:16 - need to pay for this I don't even know
06:17 - how you would pay for something like
06:19 - this so that's kind of why I'm hyping it
06:20 - up because I know this is going to help
06:22 - a lot of you guys so I want to encourage
06:23 - you to check it out if you are in the
06:25 - machine learning AI space anyways let's
06:27 - do a quick code demo here so you guys
06:29 - can see how easy it actually is I don't
06:31 - want to just keep talking about it I
06:33 - want to want to show you myself so again
06:35 - remember on the left side I have the
06:36 - server on the right side I have my
06:38 - client uh rather than showing you the
06:40 - code here in my terminal I'll just show
06:42 - it to you in Visual Studio code it's a
06:44 - little bit of a better interface so
06:45 - let's start with the server which is the
06:47 - thing that's probably most important to
06:48 - you guys so keep in mind all of this
06:51 - code here is what allowed me to deploy
06:53 - the stable diffusion pipeline which I
06:55 - used to generate those images at the
06:57 - beginning of the video so I import my
06:59 - libraries or my modules torch diffusers
07:02 - this is what gives me access to the
07:04 - stable diffusion pipeline I need some
07:06 - things from PI Trident like my model
07:07 - configuration tensor Trident batch Etc
07:11 - and then I import numpy as NP now up
07:14 - here I just Define the model that I want
07:16 - to use now there's all kinds of
07:17 - different models you can use here you
07:19 - can use hugging face models you can use
07:21 - your own custom pie torch models you can
07:23 - use tensorflow models there's all kinds
07:25 - of documentation on how to do this I'm
07:26 - not going to get into that in this video
07:28 - and then I Define the type the revision
07:31 - and then I'm just specifying that I want
07:32 - to utilize my GPU so I'm going to Cuda
07:35 - then I have a little helper function
07:37 - here converting something to a numpy
07:38 - array for me and then I have the
07:40 - function that I'm actually essentially
07:42 - serving using pytrip so this is all I
07:46 - need to actually use the stable
07:47 - diffusion pipeline I have my prompt so
07:49 - I'm just grabbing the kind of prompt
07:52 - field from my input I am then going to
07:54 - do a little bit of kind of numpy version
07:56 - here I'm not going to talk about exactly
07:58 - what I'm doing here
07:59 - and then I essentially passed that to my
08:01 - stable diffusion pipeline I specify the
08:03 - height and the width of the image that I
08:05 - want I grab all of my images and then I
08:08 - essentially return those images in an
08:09 - array so this function is what I'm going
08:12 - to bind down here to the API endpoint
08:15 - that I'm calling text to image so what
08:18 - that means is that I've essentially said
08:20 - Okay I want to have an endpoint called
08:21 - text image I want the inference function
08:24 - to be generate image I want my input
08:26 - structure to look like this my output
08:28 - structure to look like this and then
08:29 - this is the basic configuration for my
08:31 - model where I'm having the match maximum
08:34 - sorry batch input size of 8. that means
08:36 - we can batch up to eight inputs together
08:38 - pass them to the model so that way we
08:40 - can have the best scalability and
08:42 - performance obviously we could batch
08:43 - more less Etc but the point of this is
08:46 - that this is literally all I need to
08:48 - deploy this model I didn't need to set
08:50 - up my own flask server I initialized
08:52 - Trident I use this bind method here I
08:54 - pass the name of the endpoint I pass the
08:56 - function that I want I give some basic
08:58 - information like the input and shape of
09:00 - the input and then the output and the
09:02 - shape of the output the configuration
09:03 - and then I run the inference server and
09:06 - all of a sudden I now have a live
09:07 - endpoint that I can call and as long as
09:09 - I pass the correct input I get my
09:11 - machine learning output which in this
09:12 - case is going to be an array that
09:14 - represents an image
09:16 - that's it so there you go now if I go to
09:19 - client.pi I just wrote a kind of a nice
09:21 - client here so that I can interact with
09:23 - it this could be five lines of code if I
09:25 - wanted it to be the idea is I'm
09:27 - importing uh pill I'm importing numpy
09:30 - I'm importing the pi Trident client I
09:32 - then have my generate image function
09:33 - here I create a prompt array the reason
09:36 - I'm doing this is that you can pass
09:38 - multiple prompts to this model so you
09:40 - can have it generate like 10 images at
09:42 - once you don't always have to do one
09:43 - hence why I'm kind of putting this in an
09:45 - array then I'm saying with model I'm
09:48 - doing this on localhost the name of the
09:49 - endpoint is text image I can have
09:51 - multiple endpoints that I deployed if I
09:53 - wanted to and I'm saying that my result
09:55 - dictionary is equal to client dot in
09:57 - first sample and then the sample that
10:00 - I'm using here is my prompts so that's
10:02 - essentially the input I'm passing to it
10:03 - I then create a pill image from the
10:06 - numpy array that is returned to me from
10:08 - this function I save that image and then
10:11 - I'm able to view it the rest of this
10:12 - code here is just kind of giving me a
10:13 - nice interface so I can type in and give
10:15 - the file name game and all of that but
10:17 - that's it like that's all you need to do
10:19 - to deploy a model and if you wanted to
10:21 - use a different model then you would
10:22 - just change the pipeline up here you'd
10:24 - probably have to do a little bit of
10:25 - pre-processing for your inputs depending
10:27 - on what the structure and shape should
10:29 - be and then you're off to the races
10:30 - that's all you need to do to get them
10:33 - going so just to give you another
10:34 - example or just to show you I guess how
10:36 - this works one last time let me spin up
10:37 - the server one more time for you so I've
10:40 - just shut it down here and we can spin
10:41 - it back up again so I'm going to type
10:42 - python3 server.pi this contains the same
10:46 - code you just saw in vs code let's clear
10:48 - this one and then start up our client
10:51 - and we'll just give this a second to
10:52 - boot up and then we can use the client
10:54 - and generate the images again all of the
10:56 - code you just saw is what's running on
10:58 - the left and the right hand side of my
10:59 - screen all right so these Services now
11:01 - started I'm just going to ask this to
11:02 - generate an image so let's say that we
11:05 - want
11:06 - a snail
11:08 - attacking a pig that is riding a cat all
11:14 - right let's see if it can actually give
11:16 - us that I'll just name this pig dot PNG
11:18 - I don't know how good the stable
11:20 - diffusion model is but so far it's been
11:22 - pretty good to me let's wait for this to
11:23 - finish I'm going to open up the image
11:25 - then I'll show you what we get all right
11:26 - so there we go that's kind of a weird
11:28 - image that was generated here it's kind
11:30 - of like a mixture of a snail and a pig
11:32 - together I don't see a cat anywhere I
11:35 - think I put cat here right yeah riding a
11:37 - cat but either way we we got something
11:39 - that was kind of relevant uh to what it
11:41 - is that we input it so there you go that
11:43 - is mostly what I wanted to show you in
11:46 - this video now I'll just give you a
11:48 - quick notes related to kind of getting
11:49 - this running on your own machine uh the
11:52 - installation and setup is pretty
11:53 - straightforward you do need to be
11:54 - running some kind of Linux distribution
11:56 - that's why I'm in WSL here on Windows
11:59 - the WSL version that worked best for me
12:01 - is uh Ubuntu 20.0.4 once you're inside
12:05 - of that there's a few basic things you
12:06 - need to install and set up but what you
12:08 - can do is just run an Nvidia Docker
12:10 - container all of this documentation is
12:12 - available from the links that I'm going
12:14 - to leave in the description and it
12:15 - pretty much installs everything that you
12:17 - need for you automatically once you're
12:19 - inside of the docker container you just
12:21 - need to install the PIP library for the
12:23 - Nvidia uh what do you call it Pi Trident
12:26 - service or server whatever you want to
12:28 - call it so you would just do something
12:30 - like pip install and then pi and then
12:33 - Nvidia if we could type this correctly
12:35 - Pi Triton and then any other Library
12:38 - they're going to be using like Pi torch
12:40 - diffusers Transformers Etc you just
12:43 - install all of your python dependencies
12:44 - like you normally would and then this is
12:46 - going to work now this runs on python
12:48 - version 3.8 so just keep that in mind
12:51 - and that's really all you need to know
12:54 - now getting it set up on WSL can be a
12:57 - little bit challenging depending on how
12:58 - you have your windows configured what I
13:00 - found was the easiest to do was to
13:02 - install Docker desktop and then from
13:05 - Docker desktop there's a little setting
13:06 - you can click that says enable w USL
13:09 - like configuration or back end or
13:11 - something like that if you do that then
13:13 - all of a sudden it's going to give you
13:14 - access to the docker command inside of
13:17 - your WSL and then all you need to do is
13:19 - pull the docker image again all this is
13:22 - available from the Nvidia GitHub once
13:25 - you're inside of that container you
13:27 - simply create whatever python code it is
13:29 - that you want to run install the
13:30 - different Python dependencies and then
13:32 - you're pretty much good to go obviously
13:34 - there's a lot further you can go with
13:35 - this but that's really all you need to
13:37 - know if you want to get things set up
13:38 - again follow all the links and
13:40 - documentation I'm also going to have a
13:42 - GitHub that contains all of the code you
13:44 - just saw in this video so if you're
13:46 - interested in checking that out and kind
13:47 - of messing with that demo yourself feel
13:49 - free to do that with that said guys I
13:51 - think I'm going to wrap up the video
13:52 - here a big thanks to Nvidia for
13:54 - partnering with me on this video I know
13:56 - that this was really just showcasing
13:58 - their product however this is something
13:59 - that's quite cool when they first showed
14:01 - it to me and gave me kind of an
14:02 - exclusive demo I thought it was really
14:04 - interesting and valuable and that you
14:06 - guys would find some value in it hence
14:07 - why I'm making this video If you enjoyed
14:09 - make sure to leave a like subscribe the
14:11 - channel and I will see you in another
14:13 - one
14:18 - [Music]