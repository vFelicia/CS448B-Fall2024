00:00 - if you're interested in AI development
00:02 - then you're in the right place today I'm
00:04 - going to be showing you how to build an
00:05 - advanced AI agent that uses multiple
00:08 - llms we're going to run our agent
00:10 - through a series of steps we're going to
00:12 - connect the two agents together we're
00:14 - going to parse the output and you're
00:16 - really going to see how to get a bit
00:17 - more advanced here and how to do some
00:19 - really cool stuff with llms running on
00:22 - your own computer we're going to be
00:23 - doing everything locally we're going to
00:25 - use something known as ol llama and
00:27 - llama index and you're really going to
00:29 - get a taste of what AI development is
00:31 - capable of in the short video that even
00:33 - beginner or intermediate programmers can
00:35 - follow along with so with that said
00:37 - let's get into a quick demo then I'll
00:39 - walk you through a step-by-step tutorial
00:41 - on how to build this project so I'm here
00:43 - inside of vs code and I'm going to give
00:44 - you a demo of how this works now the
00:47 - concept is that we're going to provide
00:48 - some data to the model this data will be
00:51 - a coding project that we've worked on
00:53 - we've kept it simple for this video but
00:55 - if you scaled this up you could provide
00:57 - it a lot more code files and it could
00:58 - handle all of those potentially at the
01:00 - same time so you can see that we have a
01:02 - readme.pdf file in our data directory
01:05 - and this is a simple kind of
01:07 - documentation for an API that we've
01:09 - written we then have a test. pi file and
01:12 - this is the implementation of that API
01:14 - in Python so the idea here is we want
01:17 - our agent to be able to read in this
01:18 - information and then generate some code
01:21 - based on the existing code that we
01:23 - already have so I've just run the agent
01:25 - here and I've given this a sample prompt
01:27 - that says read the contents of test.py
01:29 - and write me a simple unit test in
01:31 - Python for the API now the idea here is
01:34 - that we've provided this agent some
01:36 - different tools that can utilize and it
01:38 - will decide when it needs to utilize the
01:40 - tools and use them to get the correct
01:42 - information so in this case it will use
01:44 - a tool to read in the test.py file it
01:47 - will then use a tool to actually parse
01:49 - and get access to the information in the
01:50 - readme.pdf it's then going to generate
01:53 - some code for us and then what we'll do
01:55 - is use another model to parse that
01:57 - output and save it into a file so you
02:00 - can see that when I ran this it did
02:01 - exactly that now it doesn't always give
02:03 - us the best result because we are
02:04 - running these models locally and I don't
02:06 - have a supercomputer here so I can't run
02:08 - the best possible models but it's
02:10 - showing you what's possible and
02:12 - obviously all of this is free you don't
02:13 - need to pay for anything because we're
02:15 - running it locally using all open-source
02:17 - models so what this did here is generate
02:19 - this test API file you can see there's a
02:21 - few minor mistakes but if I fix these up
02:24 - by just adding the correct parentheses
02:25 - and removing the Escape characters here
02:28 - you see that we actually have a
02:29 - functioning unit test written for our
02:31 - flask API so this is pretty cool we
02:34 - might need to change this a little bit
02:36 - to make it work but it just outputed all
02:38 - of that for us based on the context of
02:40 - the files that we provided to it now if
02:43 - we go here and read through kind of the
02:44 - output we're getting from the model you
02:46 - can see that it's actually sharing with
02:48 - us its thought process so it says the
02:49 - current language of the user is English
02:51 - I need to use this tool to help me
02:52 - answer the question it's using the code
02:54 - reader tool it passes the file name
02:56 - equal to test.py it reads in the
02:59 - contents of test.py and then it says
03:01 - okay I can answer without using any more
03:03 - tools I'll use the uh users language to
03:05 - answer to write a simple unit test blah
03:07 - blah blah you do this it then writes all
03:09 - of this code now behind the scenes what
03:11 - actually happens is we parse the output
03:14 - of this using a secondary model and take
03:17 - just the code and then generate it into
03:20 - a file and that file name will be
03:21 - generated by a different llm to make
03:24 - sure it's appropriate for the type of
03:25 - code that we have so it is a multi-step
03:28 - process here it worked quite well well
03:30 - and you can see it also gave us a
03:31 - description of what the finished code
03:33 - was so that's what I'm going to be
03:34 - showing you how to build I know that it
03:36 - might seem simple but it's actually
03:38 - fairly complex and it uses a lot of
03:39 - different tools which are really
03:41 - interesting to learn about with that
03:43 - said let's get into the full tutorial
03:44 - here and I'll explain to you how we can
03:46 - build this out completely from scratch
03:49 - so let's get started by understanding
03:50 - the tools and technologies that we need
03:52 - to use to actually build out this
03:54 - project now what we need to do for this
03:56 - project is we need to load some data we
03:58 - need to pass that to our llm we then
04:00 - need to take the result of one llm pass
04:02 - it to another llm and then we need to
04:05 - actually use a tool and save this to a
04:07 - file there's a few different steps here
04:09 - and if we wanted to build this out
04:11 - completely from scratch that would take
04:12 - us a very long time so instead we're
04:15 - going to use a framework now of course
04:16 - we're using python but we're also going
04:18 - to use llama index now I've teamed up
04:20 - with them for this video but don't worry
04:22 - they are completely free and they
04:24 - provide an open source framework that
04:25 - can handle a lot of this heavy lifting
04:28 - and specifically is really good at
04:29 - loading in data and passing it to our
04:32 - different llms I'm on their website
04:34 - right now just because it explains it
04:36 - nicely but you can see llama index is
04:38 - the leading data framework for building
04:40 - llm applications it allows us to load in
04:43 - the data which you'll see right here
04:45 - index the data query it and then
04:47 - evaluate it and also gives us a bunch of
04:49 - tools to connect different llms together
04:52 - to parse outputs you're going to see a
04:54 - bunch of advanced features in this video
04:56 - now as well as using llama index we're
04:58 - going to use something called ol llama
05:00 - now ol llama allows us to run
05:02 - open-source llms locally on our computer
05:05 - that means we don't need to pay for chat
05:06 - GPT we don't have to have an open AI API
05:09 - key we can do all of this locally so the
05:12 - summary here is that we're using python
05:14 - llama Index o Lama we're also going to
05:16 - throw in another tool which is new from
05:18 - llama index called llama parse don't
05:20 - worry it's free as well and all of that
05:22 - is going to allow us to build out this
05:23 - Advanced AI agent that has rag
05:26 - capabilities rag meaning retrieval
05:28 - augmented generation whenever we're
05:30 - taking this extra information and
05:32 - passing it into the model that's really
05:34 - known as rag I have an entire video that
05:36 - discusses how rag works you can check
05:38 - that out here but for now let's get into
05:40 - the tutorial and let's see exactly how
05:42 - we can build this application so I'm
05:44 - back on my computer and we're going to
05:46 - start by installing all the different
05:47 - dependencies that we need we're then
05:49 - going to set up AMA and then we'll start
05:51 - writing all of this code again AMA is
05:53 - how we run the models locally we need to
05:55 - install that first before we can start
05:57 - utilizing it now there are some
05:59 - prerequisites here so what we're going
06:00 - to do is I have a GitHub repository that
06:02 - I'll link in the description and in that
06:05 - GitHub repository you'll find a data
06:07 - directory that contains a readme file
06:09 - and a test.py file now you don't need to
06:12 - use these specific pieces of code but
06:14 - what you should do is create a data
06:15 - directory in a directory in VSS code so
06:18 - I've just opened up a new one here in vs
06:20 - code made a new folder called data and
06:22 - then put these two files inside of here
06:24 - specifically we want some kind of PDF
06:26 - file and some kind of python file we
06:28 - could have multiple of them if you want
06:30 - but the concept is this is the data that
06:32 - we're going to use for rag the retrieval
06:34 - augmented generation so you need
06:35 - something inside of here so either take
06:37 - it from the GitHub repository or
06:39 - populate it with your own data now from
06:41 - the GitHub repository as well there is a
06:43 - requirements.txt file please copy the
06:46 - contents of that file and paste it into
06:49 - a requirements.txt file in your
06:51 - directory or simply download that file
06:55 - this is just going to save you a lot of
06:56 - headache because it has all of the
06:57 - specific versions of python line
06:59 - libraries that we need in order for this
07:01 - project to function properly so really
07:03 - again we want to get this data directory
07:05 - populated with some kind of PDF and some
07:07 - kind of python file we then want this
07:09 - requirements.txt file again you can find
07:12 - it from the link in the description I'll
07:14 - put a direct link to the
07:16 - requirements.txt so you can just copy
07:18 - the contents of the file or you can
07:19 - download the file directory uh directly
07:22 - sorry and put it inside of your vs code
07:24 - folder now that we have that what we're
07:26 - going to do is make a new python virtual
07:28 - environment that's where we're going to
07:30 - install all these different dependencies
07:32 - so that we have this isolated on our
07:34 - system so to do that we're going to type
07:36 - the command python 3- MV EnV and then
07:39 - I'm going to go with the name of AI you
07:41 - can name this anything that you want if
07:44 - you're on Mac or Linux this is the
07:45 - correct command if you're on Windows you
07:47 - can change this to Simply Be Python and
07:49 - this should make a new virtual
07:51 - environment for you in the current
07:52 - directory this is where we'll install
07:54 - all of the different python dependencies
07:57 - now once we've done that we need to
07:58 - activate the virtual envir the command
08:00 - will be different depending on your
08:01 - operating system if you are on Windows
08:04 - or sorry if you're on Mac or Linux so
08:06 - Mac or Linux here you can type Source
08:08 - the name of your virtual environment
08:10 - which in this case is AI slash bin and
08:12 - then slash activate and you'll know this
08:14 - is activated if you see the AI prefix
08:17 - you can ignore this base prefix for me
08:19 - it's just because I have kind of a
08:20 - separate installation of python but you
08:22 - should see this prefix in your terminal
08:24 - indicating that this is activated now if
08:26 - you on Windows what you're going to do
08:28 - is open up poersh and you should be able
08:30 - to type SL AI slash and then this I
08:33 - believe is scripts and then slash
08:36 - activate and that should activate the
08:38 - virtual environment for you otherwise
08:40 - you can just look up how to activate a
08:42 - virtual environment on Windows and again
08:44 - make sure you have this prefix once the
08:46 - virtual environment is activated if we
08:48 - want to deactivate it we can type
08:50 - deactivate we're not going to do that
08:51 - though and we can install the different
08:54 - packages that we need so what we can do
08:56 - is type pip 3 install Das R and then
09:00 - requirements.txt notice this is in the
09:02 - current directory where we are when I do
09:05 - that it's going to read through all of
09:06 - the different requirements and then
09:07 - install them in this python installation
09:10 - or in this virtual environment so we'll
09:12 - do that it's going to read through
09:14 - requirements.txt and install everything
09:16 - for you this is going to take a second
09:17 - for me it's already cached so it's going
09:19 - pretty quickly and that should be it so
09:22 - once this is finished I'll be right back
09:23 - and then we can move on to the next
09:25 - steps all right so all of that has been
09:26 - installed and the next thing we need to
09:28 - do is install olama olama again lets us
09:32 - run all of this locally so in order to
09:34 - install AMA I'm just going to clear in
09:36 - my terminal here I'm going to go to a
09:38 - new browser window and paste in this URL
09:40 - here I'm going to leave it in the
09:42 - description now this is the GitHub page
09:44 - for olama and it shows you the
09:46 - installation setup steps here so again
09:48 - this will be linked in the description
09:49 - so if you're on Mac or Windows you can
09:51 - see the download buttons right here
09:53 - Linux this will be the command I'm on
09:55 - Mac so I'll just click on download when
09:58 - I do that it's going to download the
10:00 - olama installation for me once that's
10:02 - done I'm going to unzip this and then
10:03 - I'm going to run the installer now I've
10:05 - already installed it but I will still
10:06 - run you through the steps and then on
10:08 - Windows same thing you're going to
10:10 - download this and then run through the
10:11 - installer and what this will do is
10:13 - download a kind of terminal tool for you
10:16 - uh that you'll be able to utilize to
10:17 - interact with AMA you can see it says
10:19 - AMA run and that's something like llama
10:22 - 2 and this will actually download the
10:24 - Llama 2 model for you and then allow you
10:26 - to utilize it and interact with it in
10:28 - our case we're actually actually going
10:29 - to use the mistal model but there's a
10:31 - bunch of different models here that you
10:32 - could install and there's a bunch of
10:34 - other ones as well these are just some
10:35 - examples of ones that you can use okay
10:38 - so what we'll do here is unzip this
10:40 - folder and once it's unzipped we're
10:42 - going to run the installer so you can
10:44 - see here that I can click on AMA 2
10:47 - that's going to load the installation
10:48 - tool for me so I'm going to go ahead and
10:50 - click on open we're going to move it
10:52 - into applications and then we should be
10:55 - good to go with AMA so we'll go next and
10:58 - then it says in install the command line
11:00 - okay so we're going to go ahead and
11:01 - install it again I already have this
11:02 - installed so I'm not going to run
11:04 - through this tool but once you do that
11:05 - you should be able to run the oama
11:07 - command which we'll do in just one
11:09 - second all right so once you've gone
11:10 - through that installer what you can do
11:13 - is simply open up a terminal which we're
11:15 - going to do here and we can type let me
11:17 - just zoom in here so we can read this o
11:19 - Lama and just make sure that that
11:21 - command works so if we get some kind of
11:22 - output here we're good and then we can
11:24 - type AMA run and then we're going to run
11:27 - the mistal model okay so you can see
11:30 - here it shows you all the different
11:31 - models you can potentially run in this
11:33 - case this one is 7 uh billion parameters
11:35 - it's 4.1 GB there's a lot larger models
11:38 - here which obviously would perform
11:40 - better but they need some more intense
11:42 - Hardware to actually run properly so
11:44 - we're going to install mistal which is
11:45 - only 4 GB by doing AMA run mistol it's
11:49 - going to then download that model for
11:51 - you and then we can utilize it now in my
11:53 - case it's already downloaded so what I
11:55 - can do is start interacting with it by
11:57 - typing something like hello world and
11:58 - then it's going to give me some kind of
12:00 - output perfect so what I'm going to do
12:02 - now is I'm going to quit this so I think
12:04 - I can just type quit or something uh or
12:06 - contrl C contrl D okay let's get out of
12:10 - that contrl D I'm going to close this
12:13 - terminal and I'm going to show you now
12:14 - how we can run this from code so let it
12:16 - go through let it install it is going to
12:18 - take a second cuz that's to download all
12:20 - of this stuff and then we'll go back to
12:21 - VSS code and see how we interact with
12:23 - AMA from our code all right so I'm back
12:25 - inside of VSS code here and I'm going to
12:27 - continue here by creating a file now
12:29 - this file will be main.py and the idea
12:32 - here is just to initially test o Lama
12:34 - and make sure that it's working as an
12:36 - llm so I'm going to say from and this is
12:38 - going to be
12:40 - llama index. llms if we type this
12:44 - correctly dot o Lama like that we're
12:48 - going to import AMA and then we're going
12:51 - to say llm is equal to olama and inside
12:55 - of here we're going to say the model is
12:57 - equal to mistl because this is the one
12:59 - that we want to use and we can provide a
13:02 - request timeout equal to something like
13:05 - 30 seconds just so that it doesn't take
13:07 - too long now that we have the llm we
13:10 - should be able to do lm. run and then we
13:13 - can say something like hello world if we
13:16 - say the result is equal to this we
13:19 - should be able to print out the result
13:22 - so let's see if that's going to work I
13:23 - can type Python 3 and then
13:26 - main.py we'll give this a second and
13:29 - we'll see if that was a valid command or
13:30 - not or if we need to use a different one
13:33 - so actually my bad here guys rather than
13:35 - lm. run this is going to be lm. complete
13:38 - and then we can type in something like
13:40 - hello world and if we run this we should
13:42 - see that we get some kind of output give
13:44 - this a second it says hello here's a
13:46 - simple hello world program gives us the
13:48 - output and there we go so this was just
13:50 - a simple test to make sure that ama was
13:51 - running locally on our computer we also
13:53 - can run different types of local models
13:56 - for example in a second we're going to
13:57 - run a code generation one uh but now you
13:59 - can see that this is indeed working and
14:01 - we didn't need to have any API key use
14:03 - chat gbt Etc this is a local model
14:06 - running on our own computer so now what
14:09 - we want to do is set up a little bit of
14:11 - the rag application so we can load in
14:14 - some files pass that to the lolm and see
14:16 - how it can query based on that all right
14:19 - so let's go to the top of our program
14:21 - here and we're going to import a few
14:22 - things that we need in order to load our
14:24 - python file as well as to load our
14:26 - documentation we're going to start by
14:28 - looking at how we load in our PDF which
14:30 - is unstructured or semi-structured data
14:33 - depending on the way that it's set up
14:35 - and we're going to use something known
14:36 - as llama pars which can give us a much
14:38 - better parsing of this file so what I'm
14:40 - going to do is say from
14:43 - llore parse we are going to import and
14:46 - then with capital llama parse like that
14:50 - we'll talk about this in one second
14:51 - don't worry I'm then going to say from
14:53 - llore index. core and we're going to
14:57 - import the vector store IND index and
15:00 - the simple directory reader as well as
15:02 - the prompt template while we are here
15:05 - we're then going to say from llama
15:09 - index. core Dot and this is going to be
15:12 - embeddings and we're going to import the
15:15 - resolve embed model and I believe for
15:18 - now that is actually all that we need so
15:21 - let me break down what we're about to do
15:23 - here we need to load in our data now in
15:25 - this case we're loading in PDF documents
15:27 - but with llama index we could load in
15:28 - really any type of data we want and in
15:30 - the previous video I showed you how to
15:32 - load in for example CSV data but in this
15:34 - case we have a PDF now what we need to
15:36 - do is we need to parse the PDF into
15:39 - logical portions and chunks for example
15:41 - if the PDF had something like a chart
15:43 - we'd want to extract that because that's
15:45 - some structured data that we could be
15:47 - able to look at then once we have that
15:49 - we need to create a vector store index
15:52 - now a vector store index is like a
15:54 - database that allows us to really
15:56 - quickly find the information that we're
15:58 - looking for rather than having to load
16:00 - the entire PDF at once what's going to
16:03 - happen is our llm is going to utilize
16:05 - this database and extract just the
16:08 - information that it needs to answer a
16:10 - specific query or a prompt now the way
16:12 - that we'll build this Vector store index
16:14 - is by creating something known as vector
16:16 - embeddings vector embeddings take our
16:19 - textual data or whatever type of data it
16:21 - is and they embed it into
16:22 - multi-dimensional space which allows us
16:25 - to query for it based on all different
16:27 - types of factors based on the context
16:29 - based on the sentiment we don't really
16:31 - know exactly how it works it's handled
16:33 - by llms and some machine learning models
16:35 - in the background and I'm not quite
16:37 - qualified to talk about it in this short
16:39 - section of the video but the point is
16:41 - that rather than loading all of the data
16:43 - at once we're going to query this Vector
16:45 - store index which is like a really
16:47 - really fast database it's going to give
16:49 - us the information we need inject it
16:51 - into the llm and then the llm will use
16:53 - that information to answer the prompt so
16:56 - really all that means for us is we've
16:57 - got to create this index and I'm going
16:59 - to show you how to do that all right so
17:01 - to do this we're going to delete these
17:02 - two lines because these were really just
17:04 - for testing but we will leave this llm
17:07 - what we're going to do is start by
17:08 - setting up a parser now the parser is
17:10 - going to be a llama parse and then we
17:12 - can specify what we want the result type
17:14 - to be which in this case is markdown now
17:17 - llama pars is a relatively new product
17:18 - that's provided by llama index what this
17:21 - will do is actually take our documents
17:23 - and push them out to the cloud they'll
17:25 - then be parsed and then that parsing
17:27 - will be returned to us now the reason we
17:29 - use something like this is because it
17:31 - gives us significantly better results
17:32 - when we are trying to query pieces of
17:34 - data from something like a PDF which is
17:37 - typically unstructured I'll talk more
17:39 - about it in a second because we do need
17:40 - to make an account with llama parse but
17:42 - again it's totally free you don't need
17:44 - to pay for so we're going to make this
17:46 - parser and then what we're going to do
17:48 - is we're going to create a file
17:50 - extractor now the extractor is going to
17:52 - be a dictionary and we're going to
17:54 - specify a file extension which in this
17:56 - case ispdf and we're going to say
17:58 - whenever we find a PDF we want to use
18:01 - this parser which is llama parse to
18:03 - parse through the PDF and then give us
18:05 - back some results that we can then load
18:08 - next we're going to say documents is
18:11 - equal to and this is going to be the
18:12 - simple directory reader and inside of
18:15 - here we're going to specify the
18:16 - directory that we want to read from
18:17 - which is the data directory and then
18:20 - we're going to specify our file
18:21 - extractor here so file extractor is
18:24 - equal to the file extractor that we
18:27 - specified and then we're going to say
18:29 - load data okay so let's write that now
18:32 - if we hover over this you can see that
18:33 - what this is doing is loading data from
18:35 - the input directory so we're using llama
18:37 - index we have something called a simple
18:39 - directory reader what what this will do
18:41 - is go look in this directory grab all of
18:44 - the files that we need and then load
18:46 - them in and use the appropriate file
18:48 - extractor now that we've done that what
18:51 - we can do is we can pass these different
18:53 - documents which have been loaded to the
18:54 - vector store index and create some
18:57 - Vector embeddings for them so we're
18:59 - going to say the embed uncore model is
19:02 - equal to the resolve embed model and
19:05 - then this is going to look a little
19:06 - funky but we're going to type local
19:08 - colon and then b a a a
19:12 - i/b g- M3 now this is a local model that
19:17 - we can use because by default when we
19:19 - create a vector store index it's going
19:21 - to use the open AI model like something
19:23 - like chat GPT we don't want to do that
19:25 - we want to do this locally instead so
19:27 - what we're doing is we're getting access
19:29 - to a local model and this model will be
19:31 - able to create the different Vector
19:32 - embeddings for us before we inject this
19:35 - data into the vector store index so I
19:37 - know it seems a bit weird but we're just
19:38 - grabbing that model this is the name of
19:40 - it here and we're specifying we want it
19:42 - locally which means the first time we
19:44 - run this it's going to download that
19:45 - model for us and then use it okay we're
19:48 - then going to say the vector index is
19:50 - equal to the vector store index and then
19:53 - Dot from documents and then we're going
19:55 - to pass the documents that we've loaded
19:57 - here with the simple directory reader
19:59 - and we're going to specify manually the
20:01 - embed model is equal to the embed model
20:04 - that we got above which is our local
20:06 - embedding model now that we've done that
20:08 - we're going to wrap this in something
20:09 - known as a query engine so we can
20:11 - actually utilize it to get some results
20:13 - so we're going to say query engine is
20:15 - equal to the vector index. as query
20:18 - engine and the llm that we're going to
20:20 - use is going to be the olama llm now
20:24 - what this means is that I can now
20:26 - utilize this Vector index
20:29 - um as kind of like a question and answer
20:31 - bot so what I can do is I can ask it a
20:33 - question like what are the different
20:35 - roots that exist in the API and it will
20:37 - then go utilize the documents that we've
20:39 - loaded in which in this case are the PDF
20:41 - documents in this readme.pdf file and it
20:44 - will give me results back based on that
20:46 - context now in order to test that we can
20:49 - say query engine Dot and then we can
20:52 - actually send this a query and we can
20:54 - say what are some of the roots in in the
20:59 - API question mark and then it should
21:01 - give us back some kind of reasonable
21:02 - response now we do need to print that so
21:04 - we'll say
21:06 - result is equal to this and we will be
21:08 - able to run this code in 1 second once
21:11 - we get access to the API key for llama
21:14 - parse so let me show you how we do that
21:16 - so I am going to show you how to use
21:17 - llama pars here but I quickly want to
21:19 - break down what it actually is so on
21:21 - February 20th 2024 llama index released
21:25 - llama cloud and llama parse now this
21:27 - brings production context augmentation
21:30 - to your llm and rag applications now
21:33 - llama parse specifically is a
21:34 - proprietary parsing for complex
21:36 - documents that contain embedded objects
21:39 - such as tables and figures in the past
21:41 - when you were to do some kind of
21:43 - querying over this data you get really
21:45 - really bad results when you have those
21:47 - embedded objects so llama Parts is kind
21:50 - of the solution to that where it will do
21:51 - some parsing and actually break out
21:54 - these embedded objects into something
21:56 - that can be easily ingested and
21:57 - understood by your model this means
22:00 - you'll be able to answer complex
22:01 - questions that simply weren't possible
22:03 - previously as you can see rag is only as
22:05 - good as your data if the da data is not
22:07 - good if the vector index isn't good then
22:09 - we're not going to get good results so
22:10 - the first step here is that we parse out
22:12 - our documents into something that's more
22:14 - effective to pass into the vector index
22:17 - so that when we eventually start using
22:18 - it we get better results which
22:20 - drastically affect the accuracy uh in a
22:23 - good way so you can kind of read through
22:24 - here and you can see exactly what it
22:25 - does I'll leave this link in the
22:27 - description but just understand that
22:29 - what this does is give us much better
22:30 - results when we are parsing more complex
22:32 - documents specifically things like PDFs
22:35 - so what we're going to do here is create
22:37 - a new llama cloud account you can do
22:39 - that just by signing in with your GitHub
22:40 - I'll leave the link below and this will
22:42 - give us access to a free API key so we
22:44 - can use the Llama parse tool all right
22:47 - so once you've created that account or
22:48 - signed in you can simply just click on
22:50 - use llama parse it's pretty straight
22:52 - forward here and then what you can do is
22:54 - you can use this with a normal API or
22:56 - you can use it directly with llama index
22:58 - which is exactly what we're doing here
23:00 - so what we want to do is just get access
23:02 - to an API key here so we can click on
23:04 - API key and we can generate a new key
23:07 - I'm just going to call this tutorial I'm
23:10 - going to press on create new key you can
23:12 - read through the docs if you want but
23:13 - I'm just going to copy this key and
23:15 - we're going to go back into our python
23:17 - file and I'm going to make a
23:19 - newv file here and then I'm going to
23:22 - create an environment variable that
23:23 - stores this key uh that we'll have
23:25 - access to in our code so we're going to
23:27 - say that this is llore Cloud _ aior key
23:32 - this is going to be equal to and then
23:33 - I'm going to paste in that API key
23:35 - obviously make sure you don't leak this
23:37 - I'm just showing it to you for this
23:38 - video and I'll delete it afterwards then
23:41 - we're going to go into Main dopy and
23:43 - we're just going to load in that
23:44 - environment variable and it will
23:45 - automatically be detected by our parser
23:48 - so I know I went through that quickly
23:49 - but the basic idea is we're going to
23:50 - make a new account here on llac cloud
23:53 - we're then just going to use the free
23:54 - parser so we're going to go and generate
23:56 - an API key once we generate the API key
23:59 - we're going to paste that inside of an
24:00 - environment variable file with the
24:02 - variable llama Cloud API key we're going
24:05 - to close this then we're going to go to
24:07 - our python file and we're going to write
24:09 - the following code which will allow us
24:11 - to automatically load in this
24:13 - environment variable so we're going to
24:15 - say in lower cases from. EnV import
24:21 - load. EnV we need to spell these
24:25 - correctly though and then we're going to
24:27 - call this function and what the load.
24:29 - EnV function will do is look for the
24:31 - presence of a EnV file and then simply
24:34 - load in all of those variables which
24:35 - will give this line right here the
24:37 - parser access to that variable so we can
24:40 - use llama parse great so now that we've
24:43 - written this code we can test this out
24:45 - so what I'm going to do is type Python
24:47 - 3 and then main.py we're going to wait a
24:51 - second for it to install everything that
24:53 - we need and then we're going to see if
24:54 - we get some kind of output all right so
24:56 - this is finished running and you'll see
24:58 - that what happened here is it started
24:59 - parsing this file using llama parse it
25:02 - actually pushed that out to the cloud
25:04 - which means if we had hundreds of files
25:06 - thousands of files Etc we could actually
25:08 - handle all of those push them out to L
25:10 - par and then get the results back then
25:12 - what it did is gave us the result here
25:14 - after querying that PDF so it says the
25:15 - API supports several roots for
25:17 - performing various operations these
25:19 - include SL items SL items items ID items
25:22 - ID Etc so it used that context to
25:25 - actually answer the question for us so
25:28 - now that that we've created this this is
25:29 - going to be one of the tools that we
25:31 - provide to our AI agent the idea here is
25:34 - that we're going to have this and we're
25:36 - going to have a few other tools we're
25:37 - going to give it to the agent and the
25:39 - agent can use this Vector index and this
25:41 - query engine to get information about
25:44 - our PDF or about our uh API
25:47 - documentation and then using that
25:48 - information it can generate a new
25:50 - response and answer questions for us so
25:53 - the agent is going out there utilizing
25:55 - multiple different tools maybe it
25:57 - combines them together maybe it uses one
25:59 - maybe it uses two 3 Etc and then it
26:01 - Aggregates all of the results there and
26:03 - gives us some kind of output so now
26:05 - let's look at how we start building out
26:07 - the agent and we'll build out another
26:08 - tool that allows us to read in the
26:10 - python file because right now we're just
26:11 - loading in the PDF all right so we're
26:14 - going to go back to the top of our
26:15 - program here and we're going to say from
26:18 - llama
26:20 - index.
26:22 - core. tools and we are going to import
26:26 - the query engine tool and then the tool
26:30 - metadata then what we're going to do is
26:32 - we're going to take this query engine so
26:34 - I'm going to delete this right here and
26:35 - we're going to wrap it in a tool that we
26:37 - can provide to an AI agent so I'm going
26:39 - to say tools are equal to and then this
26:42 - is going to be query engine tool for the
26:45 - query engine this is going to be the
26:47 - query engine for our PDF or for our API
26:50 - documentation we're then going to say
26:52 - metadata is equal to and then this is
26:55 - going to be the tool metadata and here
26:57 - we're going to give this a name and a
26:59 - description now the name and the
27:00 - description will tell our agent when to
27:03 - use this tool so we want to be specific
27:05 - so I'm going to call this API
27:08 - documentation and then we want to give
27:09 - this a description so we're going to say
27:11 - the description is equal to and I'm just
27:13 - going to paste in the description to
27:15 - save us a little bit of typing here okay
27:17 - so let's paste it in and this says this
27:19 - gives documentation about code for an
27:21 - API use this for reading docs for the
27:26 - API okay so we're just giving some
27:28 - information about the query engine tool
27:31 - now we are going to write another tool
27:32 - in here in a second but for now I want
27:33 - to make the agent and then show you how
27:35 - we utilize the agent so we need to
27:38 - import another thing in order to use the
27:40 - agent here so we're going to go up to
27:43 - the top and we're going to say from
27:45 - llama
27:47 - index Dot and this is going to be
27:50 - core. agent and we're going to import
27:52 - the react agent okay we're now going to
27:56 - make an agent so we're going to say
27:57 - agent is equal to react agent. from
28:02 - tools and we're going to give it a list
28:04 - of tools that it can use okay so we're
28:06 - going to say tools and then we need to
28:08 - give it an llm which we're going to
28:10 - Define in one second we're going to say
28:12 - verbose equals true if you do this it
28:15 - will give us all of the output and kind
28:16 - of show us the thoughts of the agent if
28:18 - you don't want to see that you can make
28:19 - this false and then we're going to
28:21 - provide some context to this which for
28:24 - now will be empty but we'll fill in in 1
28:26 - second okay so this is is great but what
28:29 - I want to do now is I want to make
28:30 - another llm that we can use for this
28:33 - agent because we want this to generate
28:35 - some code for us rather than just be a
28:37 - general kind of question answer bot so
28:40 - what I'm going to do here is I'm going
28:41 - to say my code
28:42 - llm is equal to O llama and we're going
28:46 - to use a different llm and this is going
28:48 - to be model equal to code llama now code
28:52 - llama is something that does uh code
28:54 - generation specifically so rather than
28:56 - using the normal m model which we had
28:58 - here we're just going to use the code
29:00 - llm because we want to do code
29:01 - generation so now I'm going to pass code
29:03 - llm here and you can see how easy it is
29:05 - to utilize multiple llms locally on your
29:08 - own computer again all of these are open
29:10 - source and when you do this it should
29:12 - automatically download it for you okay
29:14 - last thing we want to do is provide a
29:16 - bit of context to this model so just to
29:18 - clean up our code a bit we're going to
29:19 - make a new file we'll call this prompts
29:22 - dopy and inside of prompts I'm just
29:25 - going to paste in a prompt for the
29:27 - context for this model okay you can find
29:29 - this from the link in the description
29:31 - from the GitHub but it says purpose the
29:33 - primary role of this agent is to assist
29:35 - users by analyzing code it should be
29:37 - able to generate code and answer
29:38 - questions about code provided now you
29:40 - can change that if you want but that's
29:42 - really what it's doing right it's going
29:43 - to read code analyze it and then
29:45 - generate some code for us so that's what
29:47 - we're doing so now what I want to do is
29:49 - import that context so I'm going to go
29:51 - to the top I'm going to say from
29:54 - prompts import and then context and then
29:58 - down here I'm going to pass that context
30:01 - variable okay so now we have made an
30:03 - agent and we can actually test out the
30:05 - agent and see if it utilizes these tools
30:09 - so let's do a simple W Loop here and let
30:12 - me just make this a bit bigger so we can
30:14 - see it we're going to say while prompt
30:17 - colon equals to input and we're going to
30:20 - say enter a prompt and then we're going
30:23 - to say Q to quit so if you type in Q
30:26 - then we're going to quit and we're going
30:27 - to say well all of that does not equal Q
30:31 - okay so let's type this correctly then
30:34 - we are going to do the following which
30:36 - is result equal to agent. query and then
30:40 - we're just going to pass in the prompt
30:42 - and then print the result okay so you
30:46 - might be wondering what we just did well
30:47 - we simply wrote an inline variable here
30:50 - using something known as The Walrus
30:51 - operator in Python this just means it's
30:54 - only defined in the wall Loop and it
30:55 - will get redefined each time the wall
30:57 - Loop runs just to make things a little
30:59 - bit cleaner and we say okay let's get
31:01 - some prompt when it's not equal to Q
31:04 - then we'll simply take the prompt pass
31:06 - it to our agent the agent will then
31:08 - utilize any tools it needs to and then
31:10 - it will print out the result so let's
31:12 - test this out and see if it's working so
31:14 - let's clear and let's run and this time
31:17 - we're not just going to be using the API
31:20 - documentation Vector index we'll use an
31:22 - agent and it will decide when to utilize
31:25 - that tool I know it seems a bit weird
31:27 - because because we only have one tool
31:29 - right now but imagine we had 20 tools 30
31:31 - tools 100 tools then the agent would
31:33 - pick between all of them and have the
31:35 - ability to do some really complex stuff
31:37 - all right so this is running now I'm
31:38 - going to give it a prompt I'm going to
31:39 - say something like send a post request
31:42 - to
31:44 - make a
31:46 - new item using the API in Python okay
31:52 - let's see what this is going to give us
31:53 - here and if this is going to work or not
31:56 - okay sweet so it looks like that worked
31:57 - if if we go here we can see that we get
32:00 - I need to use a tool to help me answer
32:01 - this question API documentation it's
32:03 - looking for post items okay the IPI
32:06 - documentation provides information on
32:07 - how to create an item using the post
32:09 - method I can answer the question to
32:10 - create a new item blah blah blah and
32:12 - then it generates the response and then
32:14 - it gives it to us here right to create a
32:15 - new item we do this import requests URL
32:18 - payload response okay that actually
32:20 - looks good and then come down here and
32:23 - it says this will create a new item and
32:25 - then we can ask it another question or
32:27 - hit Q to quit perfect so that is working
32:31 - however I want to add another tool to
32:33 - this agent that allows it to load in our
32:36 - python files so llap pars itself can't
32:39 - handle python files that's actually not
32:40 - what it's designed for but what we'll do
32:43 - is we'll write a different tool that can
32:45 - just read in the contents of any code
32:47 - file that we want and then give that
32:49 - into the llm so this way it can have
32:51 - access to the API documentation and it
32:54 - can also have access to the code itself
32:56 - so it can read both both of them so
32:58 - let's start doing that and the way we'll
33:00 - do that is by writing a new file here
33:02 - called Cod reader. piy so let's go
33:06 - inside of code reader and we're going to
33:07 - make a new tool that will then pass to
33:09 - our llm so we're going to say from llore
33:13 - index.
33:15 - core. tools Imports the function tool
33:20 - now this is really cool because what we
33:22 - can do is wrap any python function as a
33:25 - tool that we can pass to the llm so any
33:28 - python code that you'd want the model to
33:29 - be able to execute it can do that you
33:32 - just have to give it a description of
33:33 - the tool and it can actually call that
33:35 - python function with the correct
33:37 - parameters this to me is super cool and
33:39 - it really has a lot of potential and
33:41 - possibilities now I'm also going to
33:42 - import OS and then I'm going to define a
33:45 - function which will act as my tool so
33:47 - I'm going to say the code reader
33:48 - function and we're going to take in a
33:50 - file name okay now what we're going to
33:54 - do is say path is equal to os. path.
33:57 - joint in and we're going to join the
33:59 - data directory and the file name because
34:01 - we want to look just inside of data here
34:04 - perfect and then we're going to try to
34:05 - open this so we're going to say try
34:07 - we're going to say with
34:09 - open okay and this is going to be the
34:12 - path and then we're going to try to open
34:14 - this in read mode as F then we're going
34:18 - to say the content equals f. read and
34:22 - then we can simply return the file
34:26 - uncore content and this will be the
34:30 - content okay then we're going to have
34:33 - our accept exception as e we got to
34:37 - spell accept correctly and then what
34:39 - we're going to do here instead is we're
34:40 - going to
34:41 - return some kind of error and that error
34:44 - is going to be the string of e and
34:47 - that's it that's actually all that we
34:49 - need for this function now this is
34:51 - something that we can wrap in a tool and
34:52 - we can provide to the agent it can then
34:54 - call this function and get back either
34:56 - the file content or the error that
34:58 - occurred okay so we're going to say the
35:00 - code reader is equal to the function
35:04 - tool and this is going to be doore
35:07 - defaults and then we're going to say FN
35:09 - standing for function is equal to the
35:11 - code reader Funk and then what we need
35:13 - to do similar to before is we need to
35:15 - give this a name and we need to give
35:16 - this a description so the agent knows
35:19 - what to use or when to use this so we're
35:21 - going to call this the code reader and
35:23 - for the description I'm just going to
35:25 - paste in a description like I had before
35:27 - four let me see if I can move this onto
35:31 - separate lines okay let's just do it
35:34 - like this so that you guys can read it
35:36 - okay so it says this tool can read the
35:38 - contents of code files and return the
35:40 - results use this when you need to read
35:42 - the contents of a file perfect that
35:44 - looks good to me hopefully that's going
35:46 - to work for us and now we can go to
35:48 - main.py and we can import the code
35:51 - reader tool so we're going to say from
35:53 - code reader import code reader which is
35:57 - our tool our function tool and now we
35:59 - can just simply pass that in our list of
36:02 - tools so imagine right you could write
36:04 - any python function you want just wrap
36:06 - it like I said or I did here with the
36:08 - function tool and then just pass it in
36:10 - this list and now all of a sudden your
36:13 - agent has access to this and it can
36:15 - start manipulating things on your
36:16 - computer interacting with python
36:18 - functions this really makes the
36:20 - possibilities of Agents quite unlimited
36:22 - and that's what I really like about this
36:24 - okay so we have the code reader tool now
36:26 - and we also have the API documentation
36:28 - so now our agents should work exactly as
36:30 - before and we can simply read the
36:32 - contents of that file so let's try
36:35 - running this and see what result we get
36:39 - when we give it a prompt that asks it to
36:40 - say read that file okay so start parsing
36:43 - the file and then we'll write a prompt
36:46 - and we'll say something like read the
36:47 - contents of test.py and generate some
36:50 - code okay so
36:52 - read the contents of test.py and give me
36:56 - the exact same code back now remember
36:59 - we're running some local models that
37:01 - don't have a ton of parameters and
37:02 - aren't the best ones so we're not always
37:04 - going to get the best result but I hope
37:06 - this is going to work or it should give
37:08 - us at least some kind of result so you
37:10 - can see it says okay I need to use a
37:12 - tool so it says we're going to use the
37:13 - tool code reader file name test.py and
37:17 - then it gets the contents of the file
37:19 - and then what it says here is you
37:20 - provide a python script that contains an
37:21 - inmemory database for Simplicity which
37:23 - implements a list called items the
37:24 - script defines four endpoints one for
37:26 - creating new items blah BL blah blah and
37:27 - then gives us this whole result so it
37:29 - didn't give us the code that we wanted
37:31 - but it did actually give us a
37:32 - description of what was inside of that
37:34 - file which to me says this is indeed
37:36 - working and notice it only used this
37:38 - tool it didn't use the other tool
37:40 - because it didn't need that for that
37:42 - specific prompt okay so I think that's
37:45 - good that means that it's working and
37:46 - we're able to utilize both the tools now
37:48 - the next thing that we need to do is we
37:50 - need to take any code that this model is
37:53 - generating for us and we need to
37:55 - actually write that into a file now this
37:57 - is where we're going to use another llm
38:00 - so what we want to do is we want to get
38:01 - the result that we just saw there we
38:04 - want to determine if it's valid code and
38:06 - then we want to take that code and write
38:08 - it into a file now in order to do that
38:11 - we need an llm to analiz the result of
38:14 - this output so what we're really going
38:16 - to do right is we're going to take this
38:18 - result we're going to pass it to a
38:19 - different llm and that llm is going to
38:21 - have the responsibility of taking that
38:24 - result and formatting it into something
38:26 - that we can use to write the code into a
38:28 - file now I'm not sure if I'll be able to
38:30 - show this here because I think I cleared
38:32 - the console yeah I did but you would
38:33 - have seen before that it gives us some
38:35 - code output but the code is mixed with
38:38 - like descriptions and other information
38:40 - that we don't want to write into the
38:42 - file so the other lm's job is going to
38:44 - be to parse that output into a format
38:47 - where we can take it and we can write it
38:49 - into the file so let's start writing
38:51 - that this is where it gets a little bit
38:53 - more complicated but I also think it's
38:54 - where it gets quite cool so we're going
38:56 - to go to the top of our program here and
38:58 - we're going to start importing some
39:00 - things that can do some output parsing
39:02 - for us so we're going to say from
39:04 - pantic import the base model we're then
39:08 - going to say from llama index. core.
39:13 - output and I believe this is underscore
39:16 - parsers we are going to import the
39:18 - pantic output parser we're then going to
39:21 - say from llama index. core. query
39:26 - pipeline inut import the query pipeline
39:29 - which allows us to kind of combine
39:30 - multiple steps in one so now we're going
39:33 - to scroll all the way down and after we
39:35 - create our agent and our code llm we're
39:37 - going to start handling the output
39:38 - parsing so we're going to make a class
39:40 - here and we're going to say class code
39:43 - output and this is going to be a base
39:45 - model from
39:47 - pantic then what we're going to do is
39:49 - Define the type of information that we
39:52 - want our output to be parsed into now
39:54 - this is super cool because we can use l
39:57 - index and these output parsers to
39:59 - actually convert a result from an llm
40:01 - into a pantic object so we can specify
40:05 - the type that we want in the pantic
40:07 - object and then llama index and another
40:09 - llm can actually format the result to
40:11 - match this pantic object it's super cool
40:14 - so I'm going to say code and this is
40:16 - going to be type string I'm going to say
40:18 - description and I want this to be a
40:20 - string as well but we could make it
40:22 - other types but in this case we're just
40:23 - going to need strings and then I'm going
40:25 - to say file name is a string okay so
40:27 - I've just made a pantic object this is
40:29 - just a class that we're going to use to
40:31 - do our formatting and then we're going
40:33 - to write some things for our querying so
40:35 - we're going to say parser is equal to
40:38 - the
40:39 - pantic output parser if I can write this
40:42 - here and we're going to pass the code
40:44 - output which is specifying we want to uh
40:47 - use this pantic output parser and get
40:49 - our result and pass it into this code
40:51 - output object we're then going to have a
40:54 - Json prompt uncore string and this is
40:58 - going to be equal to a parser do format
41:01 - and we're going to format the code
41:03 - parser template which is a variable that
41:06 - I'm going to write in 1 second so now
41:09 - what we're going to do is go to prompts
41:11 - and I'm going to write in a prompt here
41:12 - I'll explain how this works but you just
41:13 - got to bear with me because there is a
41:14 - bit of code that we need to write okay
41:17 - so let me copy this in again you can
41:18 - find this from the GitHub or you can
41:20 - just write it out yourself and what this
41:22 - says is parse the response from a
41:24 - previous llm into a description and a
41:26 - string of Val valid code else will come
41:28 - up with a valid file name this could be
41:30 - saved ELO come up with a valid file name
41:32 - this could be saved as that doesn't
41:33 - contain special characters here is the
41:35 - response and then this is the response
41:37 - from the previous llm you should parse
41:39 - this into the following Json format okay
41:42 - so this seems weird but this is what I'm
41:44 - providing to my output parser to tell it
41:47 - how to take the result from this llm and
41:51 - parse it into the format that I want so
41:53 - let's import that and then we'll look at
41:55 - how this works so from here here we're
41:57 - going to do the code parser template and
42:00 - then I'm going to pass the code parser
42:02 - template here now with the parser do
42:04 - format will do is it will take this
42:07 - string and it will then inject at the
42:09 - end of that string the format from this
42:12 - pantic model so I've written my pantic
42:15 - output parser past the code output it's
42:17 - saying hey this is the format we want
42:19 - code string description string file name
42:22 - String what the output parser will do
42:24 - when I do parser do format is it will
42:26 - take this format
42:27 - find the Json representation of it and
42:30 - then pass it or inject it into the code
42:32 - parser template so then when I start
42:35 - using this template on the next step it
42:37 - knows the type of format we want the
42:39 - output to be in so now I say my
42:43 - Json prompt uncore template is equal to
42:48 - and this is a prompt template and I
42:50 - simply pass my Json prompt string now at
42:53 - this stage what we do is we write uh
42:56 - kind of a wrapper on the prompt template
42:57 - so we can actually inject inside of here
43:00 - the response so the response if we look
43:02 - here is this okay just bear with me this
43:04 - will make sense as we get there and then
43:06 - lastly we're going to make an
43:08 - output Pipeline and the pipeline is
43:11 - going to look like this it's a query
43:13 - Pipeline and the query pipeline is going
43:15 - to have a chain and the chain is going
43:17 - to go that we first need to get the Json
43:19 - prompt template where then going to get
43:21 - whatever we need in the template and
43:23 - then we're going to pass that to our llm
43:26 - and notice this time time I'm using my
43:28 - normal mistal llm which is this one
43:30 - right here I'm not using the code llm
43:32 - because I want a different llm for this
43:34 - task more general purpose one not one
43:36 - specifically for code okay so now we
43:39 - have our output pipeline so the idea
43:41 - here is that what we want to do is we
43:43 - want to take the output Pipeline and we
43:45 - want to pass this result to it and then
43:47 - we're going to get the result back which
43:48 - is going to be that formatted object
43:50 - that we want to look at so I know this
43:52 - is a bit complicated but that was kind
43:53 - of the point of this video is to make it
43:55 - a bit more advanced and you're going to
43:56 - see now how we do this so we have the
43:58 - result from agent. query prompt now
44:00 - let's take that result and pass it to
44:04 - our next agent so we're going to say
44:06 - next result is equal to the output
44:08 - pipeline. run and we're going to pass
44:11 - the response equal to the result so
44:15 - whatever the result was from the first
44:17 - agent that's now what we're passing is
44:19 - this variable right here in this code
44:21 - parser template prompt then we can print
44:25 - out the next result and we can see what
44:27 - we get now keep in mind this doesn't
44:30 - always work there is sometimes some
44:32 - errors based on how the parsing occurs
44:34 - but overall it's pretty good so let's go
44:36 - up here and let's run our agent again
44:40 - and let's get it to do a similar thing
44:42 - that it did before where it calls like a
44:44 - post endpoint or something okay so a
44:46 - similar prompt as before read the
44:47 - contents of test.py and write a python
44:49 - script that calls the post endpoint to
44:51 - make a new item let's type enter in here
44:54 - and let's see what we get all right so
44:56 - we can see that we get the kind of
44:58 - thought process here of the first llm
45:00 - which is that it needs to use the code
45:02 - reader tool which it does and then it
45:03 - generates this code and then what
45:05 - happens is we actually get output here
45:08 - from our second llm that says assistant
45:10 - and then it gives us this python object
45:12 - or this Json object really where we have
45:14 - the code which is all of the code that
45:16 - it generated which was this right and
45:19 - then it has the what else description
45:22 - use the request library in Python to do
45:24 - this and then it has a file name which
45:27 - is this so now that we have this kind of
45:29 - output what we want to do is take this
45:32 - output and load it as kind of valid Json
45:34 - um what do you call it uh data in Python
45:37 - I'm going to show you a fancy way to do
45:39 - that once we have that we can then
45:41 - access all the different fields like
45:43 - code description and file name and we
45:45 - can utilize those to save a new file on
45:48 - our computer so again we've gone through
45:51 - we've generated the code we've used our
45:53 - different tools from the rag Pipeline
45:55 - and then we've now parsed our open put
45:57 - into this format where we're able to
45:59 - utilize these different fields we just
46:00 - now need to load this in uh so that it's
46:03 - kind of valid for us to be able to view
46:06 - okay so now that we have that what we're
46:07 - going to do is the following we're going
46:09 - to say our cleaned Json is equal to and
46:13 - we're going to go up to the top of our
46:14 - program and we're going to import as now
46:17 - as is something that's going to allow us
46:19 - to actually load in Python code so what
46:22 - we'll do is we'll take the output from
46:24 - here and we'll load it in as a python
46:26 - diction AR so we're going to
46:29 - say. literal evaluation and then we're
46:32 - going to convert the next result into a
46:35 - string because it's actually a response
46:37 - object and we're going to replace the
46:40 - assistant which was kind of what was uh
46:42 - leading here with an empty string so all
46:45 - we're doing is removing that assistant
46:47 - that came before that valid python
46:49 - dictionary and then we're loading in the
46:51 - rest of this as a python dictionary
46:54 - object so now this is actually going to
46:56 - give us a python dictionary and what I
46:57 - can do is I can print code generated and
47:02 - then I can print what it is so I can say
47:04 - my clean Json and then access the code
47:08 - and then I can print my description so
47:10 - I'm going to go here and go back sln
47:12 - back
47:14 - sln description and this needs to be a
47:17 - back slash not a forward slash okay and
47:20 - then the description will
47:22 - be the cleaned Json of the description I
47:28 - can then say my file name is equal to
47:30 - the cleaned Json and this will be my
47:34 - file name okay so let's run this now and
47:37 - see if we get the correct output and
47:38 - then we're just going to add some error
47:39 - handling here and we're actually going
47:41 - to save the file because it is possible
47:43 - that some errors could occur so let's
47:45 - save and let's bring this up and let's
47:47 - quit and let's copy this prompt because
47:50 - this one worked and we will paste it
47:52 - again and we'll see if we're able to
47:54 - actually get all of those different
47:55 - fields and if we load in the the python
47:57 - object properly all right so you can see
47:59 - here that we're getting our result code
48:00 - generated which is this create item and
48:03 - then it has some Lambda function here
48:05 - and then we have the description and
48:07 - then we didn't print out the file name
48:08 - but we would have had the file name as
48:10 - well so it gave us a different result
48:11 - than we had last time um you can see
48:13 - this is indeed working and now we can
48:15 - quit and we can move to the next step
48:17 - which is a little bit of error handling
48:18 - and then actually saving the file so
48:21 - what we want to do now is just make sure
48:22 - that this works before we move forward
48:25 - because it's possible that we could get
48:26 - error so what we're going to do is retry
48:29 - this prompt or this sequence of steps a
48:31 - few times to just make sure it's hand or
48:33 - it's working properly sorry before we
48:35 - move on to the next step so we're going
48:37 - to say retries are equal to zero and
48:39 - we're going to say while retries are
48:43 - less than three so we'll just retry this
48:45 - three times then inside of here we're
48:48 - going to do this and we're going to say
48:50 - try and we're going to try the following
48:53 - we're then going to say accept and this
48:55 - is going to be exception
48:57 - as e we're going to say
49:00 - retries plus equal 1 here and then we're
49:03 - going to break if this happened
49:05 - successfully so what's going to happen
49:07 - now is we're going to retry this up to
49:09 - three times so every time we fail
49:11 - something happens here that's wrong we
49:13 - simply retry it and it will go and do
49:15 - this again with the same prompt that we
49:17 - typed now we can also do an error
49:20 - message here we can say print error
49:23 - occurred retry number and then we can
49:26 - make make this an F string and we can
49:28 - put in the number of retries and then we
49:31 - can print out what the error actually
49:33 - was okay so that should be our retry
49:35 - block I'm now going to come down here
49:37 - and I'm going to say okay if retries is
49:41 - greater than or equal to three which
49:43 - means this block actually failed we
49:45 - never successfully generated this
49:47 - cleaned Json then I'm simply going to
49:49 - say continue which means we're going to
49:52 - go back up here and ask for another
49:53 - prompt and I'm going to say print on
49:57 - able to
49:59 - process request try again okay now
50:03 - you've probably seen this sequence
50:04 - before but pretty much we'll try to do
50:05 - this if it doesn't work we'll just say
50:07 - hey you know that prompt didn't work for
50:08 - some reason okay give us another one
50:10 - because it's possible they ask us to do
50:12 - something we're not able to do or the
50:13 - output's not possible uh there's all
50:15 - kinds of errors that could occur here so
50:16 - we're just kind of handling that and
50:18 - cleaning it up a bit with this logic now
50:20 - if we do get down to this point here
50:22 - that means that we were able to actually
50:24 - generate this code so what we can do is
50:26 - we can save it to a file so we can write
50:29 - a little try here and we can say try
50:31 - with open and we can say open the file
50:35 - name which we have here and then we can
50:38 - say that we want to open this in W as F
50:42 - and we can say f. write and we can write
50:46 - the cleaned Json code into that file
50:50 - okay then we can say
50:51 - print saved file and we can just print
50:54 - out the file name and then we can have
50:57 - an accept here and we can say
51:00 - print error saving file okay now just to
51:06 - make sure that we're not going to
51:07 - override a file name that we already
51:09 - have what we can do is we can do an os.
51:12 - path. joyin and we can make an output uh
51:17 - folder here so we can say output and
51:19 - then file name now we need to import OS
51:22 - so we'll go to the top of our program
51:24 - and import OS sorry I know I'm jumping
51:27 - around all over the place here then we
51:29 - can go here and we can make a new folder
51:32 - called output so now all of the output
51:34 - will just go inside of this folder so we
51:36 - don't accidentally override any files
51:38 - that we already have okay so kind of
51:41 - final uh run here let's give this a shot
51:44 - and see if this works so let's bring up
51:47 - the code let's clear and let's
51:50 - run and then we'll enter our prompt and
51:53 - we'll see if it gives us that generated
51:55 - code okay so we're going to use the same
51:56 - prompt as before and now what we're
51:59 - looking for is that we actually get a
52:00 - generated file inside of this output
52:03 - directory okay so it seemed this did
52:05 - actually work you can see it has the
52:07 - code generated here and then if we go
52:09 - into our output we have this create item
52:12 - file now we do need to remove this
52:14 - because there was uh a few characters
52:16 - that I guess it left in here what it's
52:18 - doing is looking for an access token
52:20 - open up test.py okay f. read response
52:23 - request. poost access token uh response.
52:26 - status so it's not perfect there's a few
52:27 - things that it probably shouldn't be
52:30 - doing here uh but overall it gave us
52:33 - kind of some good starting code or at
52:35 - least kind of prove the point that hey
52:36 - we can generate some code it is
52:38 - attempting to call the API it is calling
52:40 - it in the correct way so yeah I mean I
52:42 - would call that a success obviously we
52:44 - can mess around with a bunch of
52:45 - different prompts we can see what ones
52:46 - it works for ones it doesn't work for
52:48 - remember we're using these local models
52:50 - which are quite small which don't have
52:51 - the same capabilities of something like
52:53 - chaty BT if we did this at an Enterprise
52:56 - level with the best hardware with the
52:58 - best models obviously we'd get some
53:00 - better results but for now I'm going to
53:01 - quit out of that model and I am going to
53:03 - wrap up the video here all of this code
53:06 - will be available to download from the
53:08 - link in the description a massive thank
53:10 - you to llama index for sponsoring this
53:12 - video I love working with them their
53:14 - framework is incredible and it really
53:16 - just opens my imagination and eyes to
53:18 - what's possible with these llms I mean
53:21 - look what we were able to create in
53:22 - about 30 or 45 minutes obviously I was
53:25 - walking through it step by step was
53:26 - going slower than I would normally code
53:28 - this out and we have an advanced AI
53:30 - agent that can do some code outputting
53:32 - and parsing we're using multiple
53:34 - different agents locally and we can
53:36 - continue to chain these and do some
53:38 - really cool things anyways if you guys
53:40 - want to see more videos like this
53:41 - definitely leave a comment down below
53:43 - like the video subscribe to the channel
53:45 - and I will see you in the next one
53:49 - [Music]