00:00 - hello and welcome back to another
00:01 - machine learning tutorial so in today's
00:03 - video we're gonna be going through
00:05 - implementing the k-nearest now neighbors
00:07 - algorithm but before we do that I'm
00:09 - gonna talk about exactly how it works
00:11 - like in mathematics and yeah so if you
00:15 - guys are interested in learning about
00:16 - how this works please watch the entire
00:18 - video and then near the end is where I'm
00:19 - gonna implement it because it won't take
00:20 - that long to implement it but if you
00:22 - guys are more interested in the code you
00:23 - can either go look at the text-based
00:24 - version at Tech with Tim net where I'm
00:26 - also gonna have like descriptions of
00:28 - what's going on and all the code that I
00:29 - write or you can just skip forward into
00:31 - the video until when you see that I'm
00:32 - done drawing I don't know how long it's
00:34 - gonna take me to explain because it is a
00:36 - bit more complicated than linear
00:37 - regression but again it's not that crazy
00:39 - difficult okay so let's talk about the K
00:43 - nearest neighbors algorithm and how does
00:45 - this work
00:45 - well as we already know K nearest
00:47 - neighbors is a classification algorithm
00:50 - and the way that it works is given a
00:52 - data point so likes may be something
00:53 - like this it attempts to classify this
00:55 - data point with one of the classes that
00:58 - it knows so in this case our three
01:00 - classes would be well red we have green
01:04 - and we have blue okay now obviously
01:07 - these would most likely represent
01:09 - something but for our cases we're just
01:11 - gonna use colors because it's kind of
01:12 - easy to see and this black is gonna be
01:14 - well our question mark because we don't
01:16 - know what classicism we're trying to
01:18 - predict it so the way that we predict
01:21 - this point essentially is we look for
01:23 - groupings of data points right so you
01:26 - can see and I'm just this is not what
01:28 - the algorithm does this is what we're
01:29 - kind of doing as humans okay we see kind
01:33 - of groups right so we see this group of
01:35 - red we see this group of blue we see
01:37 - this group of green and if I asked you
01:38 - well where does this black point belong
01:40 - most likely you would probably say red
01:43 - or blue right because it's closest to
01:46 - blue or red I would say it's closer to
01:48 - red and that is where you might say that
01:51 - this belongs to you might say belongs to
01:53 - red because well it's closest to that so
01:55 - it would make sense if it's a red color
01:56 - right and that is exactly what our
01:59 - algorithm does now he works a bit more
02:02 - advanced than this but essentially it
02:04 - looks for kind of groupings of data
02:06 - points right and I'll show I'll tell you
02:07 - how it works but and then it will pick
02:10 - the closest point to this black
02:13 - right here and say okay so it is gonna
02:15 - be that class now what does this this K
02:19 - me so I keep saying K nearest neighbors
02:21 - well K is actually known as a hyper
02:24 - parameter and it stands for the amount
02:26 - of neighbors that we are going to look
02:28 - for so I'm just gonna do an example with
02:30 - K equals three okay and go through
02:32 - exactly what happens so given our black
02:36 - data point here alright and you know
02:38 - what actually let's just move it for the
02:39 - first example to be kind of easier to
02:42 - understand okay I'm just gonna move it
02:43 - right here if we have this parameter as
02:46 - K equals three
02:47 - it means we are actually gonna look for
02:49 - three neighbors to this black point so
02:52 - we're gonna find the three closest
02:54 - points out of all of this data to this
02:57 - black point so I would argue that that
03:00 - those points would be probably these
03:02 - three points right like the ones I just
03:03 - put the black dots on okay these three
03:05 - red points now once it finds these three
03:09 - points these points are gonna have a
03:11 - vote and essentially our program is
03:14 - gonna look at what class these points
03:15 - are so you can see like this one's red
03:17 - they're all red and they're gonna vote
03:19 - now since these points are red they're
03:22 - gonna vote for red so we're gonna get a
03:24 - vote of red is three green is zero and
03:28 - blue is zero now because red is the
03:32 - Heintz highest a current occurrence of
03:34 - that vote we are going to classify this
03:36 - point as being red and that is
03:39 - essentially the way that it works if I
03:42 - picked K equals five what we would look
03:44 - for the five closest data points now
03:46 - let's just I know this might not be the
03:49 - closest point but I just wanna do it for
03:50 - example let's say we have that other red
03:51 - point and let's say this blue point is
03:53 - close to this black dog maybe it's moved
03:55 - like over here okay what's gonna happen
03:57 - now is the exact same thing we're gonna
04:00 - look at these five data points that are
04:01 - the closest to this black dot and we're
04:04 - going to say okay we have four red so
04:06 - this is four I should probably have just
04:07 - erased that and so I'm trying to write
04:09 - over it and we have blue which is one
04:13 - okay so red four and blue one and
04:15 - because well red is greater than one and
04:18 - is the highest occurrence we are going
04:19 - to classify this point as red and
04:21 - obviously I think we would all say that
04:23 - is probably an appropriate
04:24 - classification
04:26 - so that is essentially how that works
04:29 - now let's go more into some more
04:32 - detailed math okay so let's say I plot
04:35 - my data point here okay kind of in
04:37 - between two clusters of data so again
04:41 - how does this work right so actually
04:45 - let's talk about why we're gonna pick an
04:47 - even value for K or sorry an odd value
04:50 - for K so let's say k equals five let's
04:52 - let's do this example with K equals five
04:54 - then we'll talk more about the math so I
04:55 - would say the closest data points are
04:57 - probably this one this one this one this
04:59 - one and you know what I want to say it's
05:03 - this one as well okay so we have two
05:04 - blue data points and three green data
05:07 - points now we see that we since we have
05:09 - three green and two blue we're gonna
05:11 - classify this as green but what if I
05:13 - said K equals four and instead of five
05:16 - so cross that out k equals four well now
05:18 - this data point I'm gonna scratch that
05:20 - out just assume that's not there now we
05:22 - have two green points and two blue
05:24 - points so how do we pick which data
05:28 - point that we're going to classify this
05:29 - as well that is it obviously why we need
05:32 - to pick K at to be a odd number so one
05:35 - three five seven nine right so that no
05:37 - matter what we always have a winner and
05:40 - we can decide on a class because right
05:42 - now we don't know which class this is
05:45 - because we'll we had a tie in terms of
05:47 - the voting okay so that's why we pick K
05:49 - to be a odd number all right so now
05:53 - let's go into the math so how does it
05:55 - actually do this what what are the
05:57 - mathematical methods well um you know
06:00 - what let's just actually scrap or I
06:02 - don't know what I just did there what
06:04 - the heck okay can I undo that okay one
06:07 - second how do you make this fullscreen
06:09 - exit out of that there we go okay sorry
06:11 - I don't know what I did there so let's
06:13 - uh scrap all this and let's actually
06:15 - just talk about how we get distance so
06:18 - remember we were saying here like if
06:20 - this is I blocked at a point and we had
06:21 - a maybe a green data point here and we
06:24 - had like another green data point here
06:26 - and another green data point here well
06:27 - we could probably tell this one's the
06:29 - closest but the computer needs to do
06:30 - some math to determine this right and so
06:33 - how does it actually know how which
06:35 - point is closer to which point well it's
06:37 - gonna draw a line from one point to the
06:39 - other point
06:40 - and it's going to find what's known as
06:42 - the magnitude of this line okay um you
06:45 - can just say that's em whatever any
06:47 - value for the magnitude so how can we
06:50 - actually determine the magnitude of this
06:52 - line and then based on that what are we
06:54 - gonna do with that
06:55 - so essentially let's just draw our data
06:57 - point again and let's call this p1 okay
07:00 - and in 2-space
07:01 - it's gonna have coordinates of x1 and y1
07:04 - now let's draw another point and let's
07:08 - just make it a little orange point here
07:09 - and let's call this data point p2 okay
07:12 - it's gonna have coordinates x-two and
07:14 - y-two
07:16 - so based on these values just say it's a
07:18 - line here how do we find the magnitude
07:21 - of this line well there's a bunch of
07:23 - different ways that we can actually do
07:24 - this but the way that I'm gonna do it is
07:27 - called Euclidean distance now this is I
07:30 - believe the default one that our
07:32 - k-nearest a breeze uses it's probably
07:35 - one of the simplest ones and all the
07:38 - other ones kind of work similarly to
07:39 - this
07:40 - so essentially Euclidean distance is the
07:42 - absolute distance from here to here and
07:44 - how do we get that so we're inside D
07:46 - which stands for distance so you can
07:48 - change this to do K is equal to the
07:50 - square root all right it's a big square
07:52 - root of x2 minus x1 squared plus y2
08:00 - minus y1 squared and this will apply
08:04 - again to any space coordinates so if I
08:07 - have like an X Y Z coordinate so maybe
08:11 - in three dimensions and we have another
08:13 - coordinate you're gonna do the exact
08:14 - same thing except what you'd add here is
08:16 - you'd add z2 minus z1 squared okay and
08:20 - you just continue the square root like
08:22 - that but we don't need that portion
08:23 - because we're not in three space so
08:25 - essentially what this is gonna do is
08:27 - will give us that absolute position now
08:29 - if we want to prove this on like some
08:30 - standard numbers just to give you an
08:32 - idea if we say like this is zero and
08:33 - this is zero and we say this is zero and
08:36 - this is four well what is the distance
08:38 - here right what's the distance between
08:41 - x1 y1 to x2 y2 well we're just far away
08:45 - on the y axis so it should be four so
08:47 - let's plug this into our formula and see
08:49 - if we get this so we
08:51 - x2 - x1 well that's zero squared okay
08:54 - plus y2 minus y1 what that's 4 so we
08:58 - have 4 squared so we can actually just
09:00 - cross out this 0 and this plus cos well
09:02 - that's nothing so we get the square root
09:04 - of 4 squared which is equal to 4 or you
09:09 - could say it's equal to the root 16
09:11 - which is equal to 4 right and that gives
09:15 - us the distance for our line so Wow I
09:18 - just did a lot of math on here ok so now
09:20 - that we know that we can find the
09:22 - distance between all these different
09:23 - points here okay I'm just trying to do
09:25 - that to show you where I'm actually
09:26 - writing and based on those distances we
09:29 - can determine where which the closest
09:30 - neighbors are okay so let's do an
09:33 - example in 3-space to wrap it up and
09:34 - then let's implement our algorithm so I
09:37 - always draw my grids kind of weird for
09:39 - three sites okay so this is our three
09:41 - dimensional grid now the reason I'm
09:43 - doing this is because obviously well we
09:44 - have like six attributes or six features
09:46 - that we're going to be using to
09:48 - determine our data points and that means
09:50 - we have to plot them in three dimensions
09:52 - so this is the exact same thing as two
09:54 - dimensions except our data points are
09:55 - just gonna have three coordinates so
09:57 - instead of XY we're gonna have X Y Z
09:58 - right and this is I just want to show
10:01 - you how this works kind of in theory -
10:03 - okay so again we have points let's do
10:05 - one more points here and let's do our
10:10 - data point right maybe here okay now
10:14 - what we have to do is we have to
10:15 - determine like what the XY value of
10:17 - these this is XY value of this and XY
10:19 - value of this and then we have to
10:21 - compare based on how many neighbors
10:23 - we're looking for which point is which
10:25 - now this is where I want to get into
10:27 - that hyper parameter of K so what if I
10:30 - say K equals 9 so right now you would
10:35 - probably say that we want to classify
10:36 - this point as red because it's really
10:38 - close to the red data points but if I
10:40 - put a value of K equals 9 and we start
10:43 - looking for the closest points - Takai
10:45 - well we get these four okay but we also
10:48 - get like one two three four five let's
10:53 - imagine these move over a bit okay so
10:55 - what's happening now well since we
10:58 - picked our K to be way too high what's
11:01 - happening is we're now looking at points
11:03 - way
11:04 - side of the range of our data point
11:06 - right and that means we've found these
11:08 - points over here and we've said that
11:10 - this is going to be blowing into the
11:12 - purple group because well we pick too
11:15 - many values for K so this is just
11:16 - showing you an error that you can run
11:18 - into if you pick too many values okay so
11:22 - I think that is almost about it for what
11:26 - do you call it all this stuff I think
11:27 - the last thing I'm going to talk about
11:29 - is just some limitations to this
11:30 - algorithm and then maybe in the next
11:32 - video we're gonna implement it because
11:33 - it won't take very long and I realize
11:34 - I'm already at 11 minutes but I needed
11:36 - to explain this to you guys ok so some
11:38 - limitations well you may have already
11:40 - noticed that saving this model is not
11:43 - going to do anything for us meaning
11:45 - that's a like it is very computationally
11:48 - heavy so right now I'm only doing I
11:51 - don't know what is 5 10 14 data points
11:54 - and 15 if you include this one okay and
11:56 - every time that I want to classify this
11:58 - point I actually have to find the
12:00 - distance between this point and every
12:02 - other point right I have to I have to
12:04 - figure out what the distance is between
12:05 - every other data point that exists on
12:07 - this grid now this doesn't seem like a
12:10 - lot when you're only doing it 14 times
12:11 - but you can imagine that computation on
12:13 - tens of thousands of data points takes a
12:16 - long time and the reason we have to do
12:18 - that is because we have to know the
12:20 - distance to every data point so we can
12:21 - figure out which ones are the closest to
12:23 - our point right so when we save this
12:26 - model like you can save it if you want
12:28 - to but essentially it has to save every
12:30 - single piece of training data that we've
12:33 - given it every single piece because it
12:35 - has to look at every single data point
12:37 - every time we make a prediction and that
12:40 - means our predictions take a long time
12:42 - and this algorithm is essentially it's
12:45 - useless to train beforehand because it
12:47 - has to constantly keep looking at every
12:49 - data point before it can make a
12:51 - prediction on whatever data point we
12:53 - give it so that means that our time is
12:55 - going up linearly rather than being
12:57 - constant which would be what e-codes
13:02 - constant is an example of like linear
13:04 - regression where it doesn't matter how
13:06 - many times we try to predict using our
13:07 - models since we just created a function
13:09 - we can just use that function right and
13:11 - it takes constant time okay so I think
13:14 - I'm going to wrap the video up here as
13:16 - always if you guys have any questions
13:17 - please feel free to leave
13:18 - one in the comment down below and I'll
13:20 - try to help you out as best as I can and
13:21 - if you guys enjoyed the video please
13:23 - make sure you leave a like I will see
13:24 - you again in another one
13:29 - [Music]
13:36 - you