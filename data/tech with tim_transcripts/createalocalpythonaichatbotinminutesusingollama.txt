00:00 - today I'll be showing you how to make a
00:01 - python AI chatbot in just a few minutes
00:04 - and the best part is this chatbot will
00:06 - run locally meaning we don't need to pay
00:08 - for a subscription or connect to
00:10 - something like open AI now the first
00:12 - step here is to download oama so go to
00:15 - ama.com Simply press on download install
00:18 - this application and this is what we'll
00:20 - use to run llms locally on our machine
00:23 - once you've installed o llama the next
00:25 - step is to make sure the installation is
00:26 - working properly to do that you're going
00:28 - to open up a terminal or a command
00:30 - prompt and you're going to type in the
00:32 - following command which is simply olama
00:34 - now this should be working if AMA is
00:36 - running on your computer and assuming
00:38 - this command Works you're ready to move
00:40 - on to the next step now llama is
00:42 - software that allows us to download and
00:44 - run open-source llms I have a list of
00:47 - them on the right hand side of my screen
00:49 - and I'll link this in the description
00:50 - you can see we can access llama 3 with 8
00:53 - billion parameters 7 billion parameters
00:55 - we have a lot of other models available
00:57 - to us as well now notice that these
00:59 - models vary in size and the larger the
01:01 - model is the more difficult it's going
01:03 - to be to run in terms of the type of
01:05 - Hardware you need on your machine now if
01:07 - you scroll down here you'll see there's
01:08 - some different specifications you should
01:10 - have at least 8 GB of RAM to run the 7
01:13 - billion parameter models and then 16 to
01:15 - run 13 billion and 32 to run the 33
01:18 - billion models so make sure that you
01:20 - meet that criteria before pulling these
01:23 - specific models now for our purposes
01:25 - we're just going to use the Llama 3
01:26 - model with 8 billion parameters and in
01:28 - order to get that on our computer we're
01:30 - going to type the following command this
01:32 - is a llama pull and then the name of the
01:35 - model that we want and this is simply
01:37 - llama 3 when we do that it's going to
01:40 - download this model for us this will
01:42 - take a bit of time depending on how fast
01:44 - your internet connection is and then
01:45 - you'll be ready to use it in my case
01:47 - it's already downloaded so it happened
01:49 - instantly but for you it will take a few
01:51 - minutes to download and then I'll show
01:52 - you how we can test it now that the
01:54 - model is downloaded we can test it out
01:56 - and in order to do that we can simply
01:58 - type amaama run and the name of the
02:00 - model that we want it's worth noting
02:02 - that you can have multiple models here
02:04 - and you can use them by simply
02:05 - specifying their name so I'm going to
02:07 - run llama 3 I'll just go with something
02:09 - like hello world and then you can see
02:11 - that it actually gives us a response if
02:13 - you want to quit this you can type slash
02:15 - bu and then you will exit that prompt so
02:18 - now that we have access to this llm it's
02:20 - time to start using it from python Now
02:22 - using llms like this in Python is
02:24 - actually surprisingly easy and you don't
02:26 - need to be an AI expert in order to work
02:28 - with them however I always find it
02:30 - interesting to learn about how they work
02:32 - at a deeper level and that's where our
02:34 - sponsor brilliant can help brilliant is
02:36 - where you learn by doing with thousands
02:38 - of interactive lessons in math data
02:41 - analysis programming and AI they use a
02:43 - first principles approach meaning you'll
02:45 - get the why behind everything each
02:48 - lesson is interactive and filled with
02:50 - Hands-On problem solving which is six
02:52 - times more effective than just watching
02:54 - lectures the content is created by
02:56 - award-winning teachers researchers and
02:58 - pros from place like MIT Caltech and
03:01 - Google brilliant focuses on improving
03:03 - your critical thinking skills through
03:05 - problem solving not memorizing while
03:07 - you're learning specific topics you're
03:09 - also training your brain to think better
03:11 - learning a bit every day is super
03:13 - important and Brilliant makes that easy
03:15 - their fun bite-sized lessons fit into
03:18 - any schedule helping you gain real
03:20 - knowledge in just minutes a day it's a
03:22 - great alternative to Mindless scrolling
03:24 - Brilliance even has an entire AI
03:26 - Workshop that deep dives into how llms
03:28 - work and teaches you about the
03:30 - importance of training data how to tune
03:32 - in llm and more to try everything
03:34 - brilliant has to offer for free for a
03:37 - full 30 days visit brilliant.org
03:40 - techwithtim or click the link in the
03:42 - description you'll also get 20% off an
03:44 - annual premium subscription now the next
03:47 - step here is to create a virtual
03:48 - environment that will install a few
03:50 - different dependencies in that we need
03:51 - for this project what I've done is open
03:54 - to folder in Visual Studio code and the
03:56 - command I'm going to use now is python
03:58 - 3-m
03:59 - ven V and then I'm going to give this
04:01 - virtual environment a name which is
04:03 - chatbot if you're on Mac or Linux this
04:05 - will be the command if you're on Windows
04:07 - you can change this to Python and this
04:09 - will create an environment for us that
04:10 - we can have some isolated dependencies
04:12 - inside them you can see the chap off
04:14 - folder has now been created and the next
04:16 - step is to activate the virtual
04:18 - environment and then install our
04:19 - packages now if you're on Mac or Linux
04:21 - the command to activate this environment
04:23 - is the following it is simply Source the
04:26 - name of your environment which in this
04:27 - case is chatbot and then bin SL activate
04:31 - if you did this successfully you'll see
04:33 - that the name of your virtual
04:34 - environment will prefix your terminal
04:36 - now if you are on Windows the command
04:38 - can vary depending on the shell that
04:39 - you're using one of the commands that
04:41 - you can attempt is the following back SL
04:44 - venv or in this case it's going to be
04:46 - chatbot the name of your virtual
04:47 - environment SL scripts with a capital
04:50 - slash activate Dot and then this is bat
04:53 - executing this should initialize the
04:55 - virtual environment if you are running
04:57 - in command prompt if you are running in
04:59 - Powershell then you can change this to
05:01 - say PS1 and put a capital on activate so
05:05 - try one of these commands to activate
05:07 - the virtual environment on Windows and
05:09 - again make sure you have that prefix
05:11 - before we move forward now the next step
05:13 - is to install the packages that we need
05:15 - I'm just going to make this a bit
05:16 - smaller so we can read all of them we're
05:18 - going to install the Lang chain module
05:21 - the Lang chain dasama module and the
05:25 - olama module so go ahead and hit enter
05:28 - this will install it inside of our
05:29 - virtual environment and then we are good
05:31 - to go and start creating this
05:33 - application that will use our local llm
05:35 - The Next Step here is to Simply create a
05:37 - python file we can call this something
05:39 - like main.py and now to start writing
05:41 - our code now in order to interact with a
05:44 - llama what we're going to do is say from
05:46 - Lang chain uncore oama we're going to
05:49 - import the ol llama llm now here we can
05:54 - connect to AMA so we can say that our
05:56 - model is equal to AMA llm and and then
05:59 - all we need to do is specify the model
06:01 - that we want to work with now in this
06:03 - case it is llama 3 but if you have a
06:05 - different model you can put that here
06:07 - now we can actually use this model in
06:09 - order to use it we can say model do
06:11 - invoke and then we can pass to this
06:14 - function here a prompt that it will act
06:15 - upon so you can see that I can pass some
06:18 - input say input is equal to hello world
06:21 - we can store this in some kind of result
06:24 - and then simply print this out to make
06:26 - sure that it's working so let's print
06:28 - out the result and execute our code so
06:31 - from my virtual environment I'm going to
06:32 - type Python 3 and then
06:34 - main.py and notice here that we're going
06:36 - to get some warning you can ignore that
06:38 - for now and you can see that we get the
06:39 - response which is hello there it's nice
06:41 - to meet you and we've invoked the model
06:43 - with this input so that's the basics of
06:46 - interacting with the model but I'm going
06:48 - to show you a slightly more complicated
06:50 - script here that explains how we can
06:51 - pass some context to the model and how
06:54 - we can make this a little bit more user
06:55 - friendly and have a full chat window
06:57 - interface to interact with it so in
06:59 - order to do that we're going to start by
07:01 - bringing in something known as a prompt
07:02 - template so I'm going to say from Lang
07:04 - chain core. prompts and we are going to
07:07 - import the chat prompt template now Lang
07:11 - chain is something that allows us to
07:13 - more easily interact with llms and one
07:15 - of the things we can do is create a
07:17 - template that we will pass to the llm
07:19 - that contains our specific query or
07:21 - prompt and this way we can give it some
07:23 - more description and instruction on what
07:25 - it should do so I'm going to say
07:28 - template is equal to and then I'm going
07:30 - to do a multi-line string which is
07:32 - simply three quotation marks and I'm
07:34 - going to tell the model to answer the
07:37 - question below but you can give this any
07:39 - kind of detail that you want now I'm
07:41 - also going to provide to this some
07:43 - context I'm going to say here is the
07:46 - conversation history and I'm going to
07:48 - pass to this the context now whenever I
07:51 - want to embed a variable inside of a
07:53 - prompt that I'm passing to the model I
07:55 - can surround that in curly braces which
07:57 - I'm doing here and then I'm going to say
07:59 - question
08:00 - and I'm going to pass the question
08:01 - variable as well and then I'm going to
08:03 - have the answer which I'm expecting the
08:05 - model to generate so you can see I'm
08:07 - giving this a template for how it should
08:09 - behave or respond to my queries now we
08:12 - have our model the next thing we're
08:13 - going to do is create our prompt so
08:15 - we're going to say the prompt is equal
08:17 - to the chat prompt template. from and
08:20 - this is going to be template and then
08:22 - we're going to pass that template that
08:24 - we just wrote which is called template
08:26 - now we have a prompt and we have a model
08:29 - and what we need to do is chain these
08:31 - together using L chain so what I can do
08:33 - is say chain is equal to prompt and then
08:36 - I can use this pipe operator and I can
08:39 - pass my model what this will do is
08:41 - create a chain of these two operations
08:43 - so the first thing we'll have is our
08:45 - prompt which we'll have the question and
08:47 - context embedded inside of then we will
08:49 - pass it to the model where it will be
08:51 - invoked automatically so in order to use
08:53 - the chain now we can change this
08:55 - slightly so rather than model. invoke
08:57 - we're going to say chain. invoke o but
09:00 - this time what we need to do is pass to
09:01 - it the various variables that are inside
09:04 - of our prompt so we're going to say the
09:06 - context is equal to in this case we
09:08 - don't have any context so we'll leave it
09:09 - blank and then we'll say the question is
09:12 - and then whatever our question is and we
09:14 - can just say hey how are you so
09:17 - hopefully you get the idea here let me
09:18 - make this a bit bigger so we can read it
09:20 - we're simply embedding these two
09:22 - variables inside of the prompt and then
09:24 - passing that prompt to the model where
09:25 - it will be invoked using Lang chain so
09:28 - now we can test this Python 3 main.py
09:31 - and it says I'm doing well thanks for
09:33 - asking now that's great but we want to
09:35 - be able to continually talk with the
09:37 - model and store a conversation history
09:39 - so let's write the code to handle that
09:41 - so what I'm going to do now is create a
09:43 - function called handle and then
09:45 - conversation and this is where we'll
09:47 - kind of put all of our main code inside
09:50 - of here I'm going to start by storing
09:51 - some context which will just be an empty
09:53 - string and I'm going to print some kind
09:54 - of Welcome message so I'll say
09:57 - welcome to the AI chatbot and then we'll
10:01 - just tell them that they can type
10:02 - something like exit to quit so they have
10:05 - a way to get out of this on the next
10:07 - line we're going to make a while loop
10:08 - and we're going to say while true and
10:10 - inside of here we're going to collect
10:11 - some input from the user so we're going
10:13 - to say user input is equal to input and
10:15 - we'll just put U col in and then a space
10:17 - that the user has the ability to type
10:19 - we're going to say if the user input.
10:21 - lower so converting this to lowercase is
10:24 - equal to that exit keyword then we are
10:26 - going to break out of the loop so that
10:27 - we don't have an infinite Loop now next
10:30 - we're just going to generate a response
10:32 - so we can take this right here and bring
10:35 - this up so let's paste that inside of
10:37 - here and indent this properly okay let
10:41 - me just get rid of this print now we're
10:43 - going to say the result is equal to
10:44 - chain. invoke but this time we're going
10:46 - to pass the conversation context and
10:48 - we're going to pass the question the
10:49 - user asked which is simply the user
10:52 - input now what we can do is we can print
10:54 - the response so we can say the bot said
10:57 - and then whatever the the result was
11:00 - here and then what I'm going to do is
11:02 - just store all of this in the context so
11:05 - that this bot has kind of the
11:06 - conversation history and it can respond
11:09 - to previous things that have been said
11:11 - so I'm going to say context plus equals
11:13 - to and then we're going to use an F
11:14 - string available in Python 3.6 and above
11:17 - I'm going to put a new line character
11:19 - with the back sln I'm going to say user
11:22 - is equal to and I'm going to embed the
11:24 - user input and then I'm going to do back
11:26 - sln and I'm going to say the AI and then
11:29 - this is equal to the result so I'm just
11:31 - storing what the user asked and what the
11:33 - result was so now every single time I
11:36 - use this prompt I'm passing all of this
11:38 - context back into the bot so it knows
11:41 - what was said previously and can use
11:42 - that to respond so that is our function
11:45 - now all we need to do is call that
11:46 - function so I'm going to say if uncore
11:48 - name equals equals uncore maincore
11:51 - uncore this just means that we're
11:52 - directly executing this python file and
11:55 - then I'm going to say handle
11:56 - conversation and now we are going to run
11:58 - this function function which will simply
12:00 - ask the user to keep typing in things
12:02 - until they hit or enter exit and it will
12:04 - store the conversation history and pass
12:06 - that back to our model so that it can
12:09 - respond based on what we said previously
12:11 - so let's test this out and make sure it
12:13 - works Python 3 main.py welcome to the AI
12:16 - chat bot we're going to say hey how are
12:18 - you doing it's going to give us some
12:21 - response I'm going to say what is your
12:23 - name great no personal name that's fine
12:27 - can you help me understand and history
12:30 - let's see what it says and it gives us
12:33 - this long response here which I imagine
12:34 - is saying yes it can help us if we give
12:36 - it a good question let's try to exit
12:39 - with exit and now we are gone and there
12:41 - you go we have now created a program
12:43 - that we can actually interact with a
12:45 - local llm we don't need an open AI key
12:47 - we don't need to pay any third party
12:49 - service and we can run this open source
12:51 - model on our own computer and utilize it
12:53 - from our python script obviously this is
12:56 - a very simple example you can do some
12:58 - really cool things that are a lot more
12:59 - complicated but that's what I wanted to
13:01 - show you in this video hopefully you
13:03 - found this helpful if you did make sure
13:04 - you leave a like subscribe to the
13:06 - channel and I will see you in the next
13:08 - one
13:10 - [Music]