00:00 - hey guys and welcome to a brand new
00:02 - tutorial series on neural networks with
00:05 - Python and tensorflow 2.0 now tensorflow
00:08 - 2.0 is the brand-new version of
00:10 - tensorflow still actually in the alpha
00:12 - stages right now but it should be
00:14 - released within the next few weeks but
00:16 - because it's an alpha tensorflow has
00:18 - been kind enough to release us that
00:20 - alpha version so that's what we're going
00:21 - to be working with in this tutorial
00:23 - series and this will work for all future
00:26 - versions of tensorflow
00:27 - 2.0 so don't be worried about that now
00:30 - before I get too far into this first
00:32 - video I just want to quickly give you an
00:33 - overview of exactly what I'm gonna be
00:35 - doing throughout this series so you guys
00:36 - have an idea of what to expect and what
00:39 - you're going to learn now the beginning
00:41 - videos and especially this one are going
00:42 - to be dedicated to understanding how a
00:44 - neural network works and I think this is
00:47 - absolutely fundamental and you have to
00:49 - have some kind of basis on the math
00:51 - behind a neural network before you're
00:53 - really able to actually properly
00:55 - implement one now tensorflow does a
00:58 - really nice job of making it super easy
01:00 - to implement neural networks and use
01:02 - them but to actually have a successful
01:04 - and complex nail network you have to
01:06 - understand how they work on the lower
01:08 - level so that's we're gonna be doing for
01:09 - the first few videos after that what
01:12 - we'll do is we'll start designing our
01:13 - own neural networks that can solve the
01:15 - very basic M&S data sets that tensorflow
01:18 - provides to us now these are pretty
01:20 - straightforward and pretty simple but
01:22 - they give us a really good building
01:23 - block on understanding how the
01:25 - architecture of a neural network works
01:26 - what are some of the different
01:28 - activation functions how you can connect
01:30 - layers and all of that which will
01:32 - transition us nicely into creating our
01:33 - own neural networks using our own data
01:36 - for something like playing a game now
01:38 - personally I'm really interested with
01:40 - neural networks playing games and I'm
01:42 - sure a lot of you are as well and that's
01:44 - what I'm gonna be aiming to do near the
01:45 - end of the series on kind of our larger
01:47 - project I'll be designing a neural
01:49 - network and tweaking it so they can play
01:51 - a very basic game that I've personally
01:53 - designed in Python with PI game now with
01:57 - that being said that's kind of it for
01:59 - what we're gonna be doing in this series
02:00 - I may continue this on future in later
02:03 - videos and do like very specific neural
02:05 - network series maybe a chatbot or
02:06 - something like that but I need you guys
02:08 - to let me know what you'd like to see in
02:10 - the comments down below with that being
02:12 - said if you're excited about the series
02:13 - make sure you drop a like on this video
02:15 - and subscribe to the channel to be
02:16 - notified when I post the new videos and
02:19 - with that being said let's get into this
02:20 - first video on how a neural network
02:22 - works and what a neural network is so
02:25 - let's start talking about what a neural
02:27 - network is and how they work now when
02:29 - you hear neural network you usually
02:31 - think of neurons
02:32 - now neurons are what compose our brain
02:35 - and I believe don't quote me on this we
02:37 - have billions of them in our body or in
02:39 - our brain now the way that neurons work
02:41 - on a very simple and high level is you
02:44 - have a bunch of them that are connected
02:46 - in some kind of way so let's say these
02:48 - are four neurons and they're connected
02:50 - in some kind of pattern now in this case
02:53 - our pattern is completely like like
02:55 - random we're just arbitrary we're just
02:57 - picking a connection but this is the way
02:58 - that they're connected okay now neurons
03:01 - can either fire or not fire so you need
03:04 - to be on or off just like a 1 or 0 ok so
03:07 - let's say that for some reason this
03:09 - neuron decides to fire maybe you touch
03:11 - something maybe you smelt something
03:14 - something fires in your brain and this
03:16 - neuron decides to fire now it's
03:19 - connected to in this case all of the
03:20 - other neurons so what it will do is it
03:23 - will look at its other neurons and a
03:24 - connection and it will possibly cause
03:27 - it's connected neurons to fire or to not
03:29 - fire so in this case let's say maybe did
03:32 - what this one firing causes this
03:33 - connected neuron to fire this one to
03:36 - fire and maybe this one was already
03:37 - firing and now it's decided it turned it
03:40 - off or something like that ok so that's
03:42 - what happened now when this neuron fires
03:45 - well it's connected to this neuron and
03:46 - it's connected to this nerve well it's
03:48 - already got that connection but let's
03:50 - say that maybe when this one fires it
03:52 - causes this one to on fire because it
03:54 - was just fired something like that right
03:56 - and then this one now that it's off it
03:59 - causes this one to fire back up and then
04:00 - it goes it's just a chain of firing and
04:02 - unfired
04:03 - and that's just kind of how it works
04:06 - right firing and unfired now that's as
04:09 - far as I'm going to go into explaining
04:10 - neurons but this kind of gives us a
04:12 - little bit of a basis for a neural
04:13 - network now a neural network essentially
04:16 - is a connected layer of neurons or
04:19 - connected layers so multiple of neurons
04:22 - so in this case let's say that we have a
04:24 - first layer we're going to call this our
04:26 - input
04:26 - that has four gnomes and we have one
04:30 - more layer that only contains one now
04:34 - these neurons are connected now in our
04:37 - neural network we can have our
04:38 - connections happening in different ways
04:40 - we can have each what decodes neuron
04:43 - connected to each other neurons so from
04:45 - layer to layer or we can have like some
04:48 - connected to others some not connected
04:50 - so I'm connected multiple times it
04:51 - really depends on the type of neural
04:53 - network we're doing now in most cases
04:55 - what we do is we have what's called a
04:57 - fully connected neural network which
04:59 - means that each neuron in one layer is
05:03 - connected to each neuron in the next
05:05 - layer exactly one time so if I were to
05:08 - add another neuron here then what would
05:11 - happen is each of these neurons would
05:13 - also connect to this neuron one time so
05:16 - we would have a total of eight
05:18 - connections because four times two is
05:19 - eight right and that's how that would
05:21 - work now for simplicity sake we're just
05:24 - gonna use one neuron in the next layer
05:27 - just to make things a little bit easier
05:28 - to understand now all of these
05:31 - connections have what is known as a wait
05:34 - now this is in a neural network
05:36 - specifically okay so we're gonna say
05:37 - this is known as weight one this is
05:39 - known as weight to this is weight three
05:42 - and this is weight 4 and again just to
05:45 - rehab size this is known as our input
05:47 - layer because it is the first layer in
05:50 - our connected layers of neurons okay and
05:53 - going with that the last layer in our
05:56 - connected layer of neurons is known as
05:59 - our output layer now these are the only
06:02 - two layers that we really concern
06:04 - ourselves with when we look and use a
06:07 - neural network now obviously when we
06:09 - create them we have to determine what
06:10 - layers we're gonna have in the
06:12 - connection type but when we're actually
06:13 - using the neural network to make
06:15 - predictions or to Train it we are only
06:18 - concerning ourselves with the input
06:19 - layer and the output layer now what does
06:22 - this do and how do these neural networks
06:24 - work well essentially given some kind of
06:26 - input we want to do something with it
06:30 - and get some kind of output right in
06:31 - most instances that's what you want
06:33 - input results in the output in this case
06:36 - we have four inputs and we have
06:38 - output but we could have a case where we
06:41 - have four inputs and we have 25 outputs
06:43 - right it really depends on the kind of
06:45 - problem we're trying to solve so this is
06:48 - a very simple example but what I'm going
06:49 - to do is show you how we would or how a
06:52 - neural network would work to train a
06:54 - very basic snaking so let's look at a
06:59 - very basic snake game so let's say this
07:00 - is our snake okay and this is his head
07:05 - actually yeah let's say this is his head
07:08 - but like this is what the position the
07:09 - snake looks like where this is the tail
07:11 - okay we'll circle the tail now what I
07:13 - want to do is I want to train a neural
07:15 - network that will allow this snake to
07:18 - stay alive so essentially its output
07:20 - will be what direction to go in or like
07:23 - to follow a certain direction or not
07:24 - okay essentially just keep this snake a
07:26 - lot that's what I want it to do now how
07:29 - am I gonna do this well the first step
07:30 - is to decide what our input is gonna be
07:32 - and then to decide what our output is
07:34 - going to be so in this case I think a
07:36 - clever input is gonna be do we have
07:38 - something in front of the snake do we
07:40 - have something to the left of the snake
07:41 - and do we have something to the right to
07:43 - the snake because in this case all
07:45 - that's here is just the snake and he
07:46 - just needs to be able to survive so what
07:49 - we'll do is we'll say okay is there
07:50 - something to the left yes no something
07:52 - in front yes no so 0 1 something to the
07:55 - right yes no and then our last input
07:57 - will be a recommended direction for the
07:59 - snake to go in so the recommended
08:02 - direction could be anything so in this
08:04 - case maybe we'll say the recommended
08:05 - direction is left and what our output
08:07 - will be is whether or not to follow that
08:09 - recommended direction or not or to try
08:12 - to do a different recommendation
08:14 - essentially or go to a different
08:16 - direction so let's do one case on how we
08:19 - would expect this neural network to
08:21 - perform without Trant like once it's
08:23 - trained right based on some given input
08:25 - so let's say there's not something to
08:28 - the left so we're gonna put a 0 here
08:29 - because this one will represent if
08:30 - there's anything to the left the next
08:33 - one will be front so we'll say well
08:36 - there's nothing in front the next one
08:38 - will be to the right so we'll say right
08:40 - and we'll say yes there is something to
08:42 - the right of the snake and our
08:43 - recommended direction what can be
08:45 - anything we'd like so in this case we
08:47 - say the recommended direction is left
08:48 - and we'll way will do the recommend
08:50 - direction is negative
08:52 - 1:01 where negative one is left a zero
08:56 - is in front and one is to the right okay
09:00 - so we'll say in this case our
09:02 - recommended Direction is negative one
09:03 - and we'll just denote this by direction
09:07 - now our output in this instance should
09:10 - either be a zero or one representing do
09:14 - we follow the recommended direction or
09:16 - do we not so let's see in this case
09:19 - following the recommended direction
09:21 - would keep our snake alive
09:22 - so we'll say 1 yes we will follow the
09:24 - recommended direction that is acceptable
09:26 - that is fine we're gonna stay alive when
09:28 - we do that now let's see what happens
09:30 - when we change the recommended direction
09:32 - to be right so let's say that we say 1
09:35 - as a recommended direction again this is
09:37 - durn here then what should our output be
09:40 - well if we decide to go right we're
09:42 - gonna crash into our tail which means
09:45 - that we should not follow that direction
09:47 - so our output should be 0 so I hope
09:49 - you're understanding how we would expect
09:51 - this neural network to perform all right
09:54 - so now how do we actually design this
09:57 - neural network how do we get this work
09:59 - how do we train this right well that is
10:01 - a very good question and that is what
10:02 - I'm gonna talk about now so let me
10:04 - actually just erase some of this stuff
10:06 - so we have a little bit more room to
10:07 - work with some math stuff right here but
10:10 - right now what we start by doing is we
10:12 - start by designing what's known as the
10:13 - architecture of our neural network so
10:15 - we've already done this we have the
10:16 - input and we have the output now each of
10:19 - our inputs is connected to our outputs
10:21 - and each of these connections has what's
10:23 - known as a weight now another thing that
10:26 - we have is each of our input neurons has
10:28 - a value right we had in this case we
10:30 - either had 0 or we had 1 now these
10:34 - values can be different right these
10:36 - values can either be decimal values or
10:37 - they can be like between 0 and 100 they
10:40 - don't have to be just between 0 and 1
10:41 - but the point is that we have some kind
10:43 - of value right so what we're gonna do in
10:46 - this output layer to determine what way
10:48 - we should go is essentially we are going
10:50 - to take the weighted sum of the values
10:53 - multiplied by the weights I'm gonna talk
10:55 - about how this works more in depth in a
10:57 - second but just just follow me now so
10:59 - what this symbol means is take the sum
11:01 - and what we do is I'm gonna say in this
11:03 - case I which is gonna be our variable
11:05 - and
11:06 - talk about how this kind of thing works
11:07 - in a second we'll say I equals one and
11:09 - I'm going to say we'll take the weighted
11:11 - sum of in this case value I multiplied
11:15 - by weight I so what this means
11:18 - essentially is we're going to start at I
11:20 - equals one we're gonna use I as our
11:22 - variable for looping and we're gonna say
11:24 - in this case we're gonna do b1 times VI
11:27 - or sorry VI times WI and then we're
11:30 - gonna add all this so what this will
11:32 - return to us actually will be V 1 W 1
11:36 - plus V 2 W 2 plus V 3 w 3 plus V 4 w 4
11:44 - and this will be our output that's
11:49 - that's what our output layer is going to
11:50 - have as a value now this doesn't really
11:53 - make much sense right now right like why
11:56 - why we doing this weights what is this
11:58 - multiplication we'll just follow with me
12:00 - for one second so this is what our
12:02 - output layer is going to do now there's
12:04 - one thing that we have to add to this as
12:06 - well and this is what is known as our
12:09 - biases okay so what we're gonna do is
12:12 - we're going to take this weighted sum
12:13 - but we're also going to have some kind
12:16 - of bias on each of these weights okay
12:18 - and what this bias is known as it's
12:20 - denoted by C typically but essentially
12:23 - it is some value that we just
12:25 - automatically add or subtract it's a
12:27 - constant value for each of these weights
12:29 - so we're gonna say all of these these
12:31 - connections have a weight but they also
12:32 - have a bias so we're gonna have B 1 B 2
12:35 - B 3 and B 4 twice what we'll call it B
12:40 - instead of C so what I'll do here is
12:42 - what I'm also going to do is I'm also
12:44 - gonna add these biases in when I do
12:46 - these weights so we're gonna say B I as
12:48 - well so now what we'll have is we'll
12:50 - have at the end here plus bi or plus B 1
12:54 - plus B 2 plus B 3 + B 4 now again I know
12:59 - you guys like what the heck am I doing
13:00 - with this this makes no sense it's about
13:02 - to make sense in one second so now what
13:05 - we need to do is we need to train the
13:07 - network so we've understood now this is
13:09 - essentially what this output layers
13:10 - doing we're taking all of these weights
13:13 - and these values we're multiplying them
13:15 - together and we're adding them and we're
13:16 - taking what's known as the weighted sum
13:18 - okay
13:19 - but how do we like what are these values
13:21 - how do we get these values and how is
13:22 - this gonna give us a valid output well
13:25 - what we're going to do is we're gonna
13:26 - train the network on a ton of different
13:28 - information so let's say we play 1,000
13:32 - games of snake and we get all of the
13:34 - different inputs and all the different
13:36 - outputs so what we'll do is we'll
13:37 - randomly decide like a recommended
13:39 - direction and we'll just take the state
13:41 - of the snake which will be either
13:43 - there's something left to the right or
13:45 - in front of it and then we'll take the
13:46 - output which will be like did the snake
13:49 - survive or did the snake not survive so
13:52 - well what we'll do is we'll train the
13:55 - network using that information so we'll
13:57 - generate all of this different
13:58 - information and then train the network
14:00 - and what the network will do is it will
14:03 - look at all of this information and it
14:05 - will start adjusting these biases and
14:07 - these weights to properly get a correct
14:10 - output because what we'll do is we'll
14:12 - give it all this input right so let's
14:13 - say we give it the input again of zero
14:16 - one zero and maybe one like this random
14:19 - input and let's say the output for this
14:21 - case is what do you call it so one is go
14:25 - to the right the output is one which is
14:26 - correct well what the network could do
14:29 - is say okay I got that correct so what
14:30 - I'm gonna do is I'm not gonna bother
14:32 - adjusting the network because this fine
14:34 - so I don't have to change any of these
14:36 - biases I don't have to change any of
14:37 - these weights everything is working fine
14:40 - but let's say that we get the answer
14:42 - wrong so maybe the output was zero but
14:43 - the answer should have been one because
14:45 - we know the answer obviously because
14:47 - we've generated all the input and the
14:48 - output so now what the network will do
14:50 - is it will start adjusting these weights
14:52 - and adjusting these biases they'll say
14:54 - all right so I got this one wrong and
14:57 - I've gotten like five or six wrong
14:59 - before and this is what was familiar
15:01 - when I got something wrong so let's add
15:03 - one to this bias or let's multiply this
15:05 - weight by two and what it will do is
15:07 - it'll start adjusting these weights and
15:09 - these biases so that it gets more things
15:12 - correct so obviously that's why neural
15:15 - networks typically take a massive amount
15:17 - of information to Train because what you
15:19 - do is you pass it all of this
15:20 - information and then it keeps going
15:22 - through the network and at the beginning
15:24 - it sucks right because it doesn't this
15:26 - network just starts with random weights
15:28 - and random biases but as it goes through
15:31 - and it learns it says okay
15:33 - well I got this one correct so let's
15:36 - leave the weights and the bias is the
15:37 - same but let's remember that this is
15:39 - what the way in the bias was when this
15:40 - was correct and then maybe he gets
15:42 - something wrong and it says okay so
15:44 - let's adjust bias one a little bit
15:46 - let's adjust weight one let's mess with
15:48 - these and then let's try another example
15:50 - and then it says okay I got this example
15:52 - right maybe we're moving in the right
15:53 - direction maybe you will adjust another
15:55 - way maybe we'll adjust another bias and
15:56 - that's eventually your goal is that you
15:59 - get to a point where your network is
16:01 - very accurate because you've given it a
16:03 - ton of data and it's adjusted the
16:04 - weights and the biases correctly so that
16:06 - this kind of formula here of this
16:08 - weighted average will just always give
16:10 - you the correct answer or has a very
16:13 - high accuracy or high chance of giving
16:15 - you the correct answer so I hope that
16:17 - kind of makes sense I'm definitely over
16:19 - simplifying things in how the adjustment
16:22 - of these weights and these biases work
16:23 - but it's not crazy important and we're
16:25 - not going to be doing any of the
16:27 - adjustment ourselves we're we are just
16:29 - gonna be kind of tweaking a few things
16:31 - with the network so as long as you
16:32 - understand that when you feed
16:34 - information what happens is it checks
16:36 - whether the network got it correct or it
16:37 - got it incorrect and then it adjusts the
16:40 - network accordingly and that is how the
16:41 - learning process works for a neural
16:43 - network alright so now it's time to
16:45 - discuss a little bit about activation
16:48 - functions so right now what I've
16:50 - actually just described to you is a very
16:52 - advanced technique of linear regression
16:54 - so essentially I was saying we are
16:57 - adjusting weights we're adjusting biases
16:59 - and essentially we are creating a
17:00 - function that given the inputs of like X
17:03 - Y Z W or like left front right we are
17:06 - giving some kind of output but all we've
17:08 - been doing to do that essentially is
17:09 - just adjusting a linear function because
17:12 - our degree is only 1 right we have
17:15 - weights of degree one multiplying by
17:17 - values of degree one and we're adding
17:19 - some kind of bias and that kind of
17:21 - reminds you of the form MX plus B we're
17:24 - literally just adding a bunch of MX plus
17:26 - B is together which gives us like a
17:28 - fairly complex linear function but this
17:32 - is really not a great way to do things
17:35 - because it limits the degree of
17:37 - complexity that our network can actually
17:40 - have to be linear and that's not what we
17:43 - want so now we have to talk about
17:44 - activation functions
17:46 - so if you understand everything that
17:48 - I've talked about so far you're doing
17:49 - amazing this is great you understand
17:51 - that essentially the way that the
17:53 - network works is you feed information in
17:54 - and it adjusts these weights and biases
17:57 - there's a specific way it does that
17:58 - which we'll talk about later and then
18:00 - you get some kind of output and based on
18:03 - that output you're trying to adjust the
18:05 - weights and biases and and all that
18:06 - right so now what we need to do is talk
18:09 - about activation functions and what an
18:10 - activation function does is it's
18:12 - essentially a nonlinear function that
18:15 - will allow you to add a degree of
18:17 - complexity to your network so that you
18:19 - can have more of a function that's like
18:21 - this as opposed to a function that is a
18:24 - straight line so an example of an
18:26 - activation function is something like a
18:28 - sigmoid function now a sigmoid function
18:31 - what it does is it'll map any value you
18:35 - give it in between the value of negative
18:37 - 1 and 1
18:38 - so for example when we create this
18:41 - Network our output might be like the
18:44 - number 7 now this number 7 well it is
18:48 - closer to 1 that is to 0 so we might
18:50 - deem that a correct answer or we might
18:53 - say that this is actually way off
18:55 - because it's way above 1 right but what
18:57 - we want to do essentially is in our
18:59 - output layer we only want our values to
19:01 - be within a certain range we want them
19:04 - to be in this case between 0 and 1 or
19:06 - maybe we want them to be between
19:07 - negative 1 and 1 I'm saying like how
19:11 - close we are to 0 making that decision
19:13 - how close we are to 1 something like
19:14 - that right so what the sigmoid
19:16 - activation function does it's a
19:17 - nonlinear function and it takes any
19:20 - value and essentially the closer that
19:22 - value is to infinity the closer the
19:25 - output is to 1 and the closer that value
19:28 - is to negative infinity the closer that
19:29 - output is to negative 1 so what it does
19:33 - is it adds a degree of complexity to our
19:35 - network now if you don't if you're not a
19:37 - high level like math student or you only
19:39 - know like very basic high school math
19:41 - this might not really make sense to you
19:42 - but essentially the degree of something
19:44 - right is honestly how complex you can
19:46 - get if you have like a degree 9 function
19:49 - then what you could do is you can have
19:51 - some crazy kind of curve and stuff going
19:55 - on especially in multiple dimensions
19:56 - that will just make things like much
19:59 - more complex
20:00 - so for example if you have like a degree
20:02 - nine function you can have curves that
20:04 - are going like like this like all around
20:06 - here that are mapping your different
20:08 - values and if you only have a linear
20:11 - function well you can only have a
20:12 - straight line which limits your degree
20:14 - of complexity by a significant amount
20:17 - now what these activation functions also
20:20 - do is they shrink down your data so that
20:22 - it is not as large so for example right
20:24 - like say we're looking with data that is
20:26 - like hundreds of thousands of like
20:27 - characters long or digits we'd want to
20:30 - shrink that into like normalize that
20:32 - data so that it's easier to actually
20:34 - work with so let me give you a more
20:36 - practical example of how to use the
20:37 - activation function I talked about what
20:39 - sigmoid does what we would do is we
20:41 - would take this weighted sum so we did
20:43 - the sum of wi-vi plus bi right and we
20:51 - would apply an activation function to
20:53 - this so we'd say maybe our activation
20:55 - function is f of X and we would say F of
20:58 - this and this gives us some value which
21:00 - is now going to be our output neuron and
21:02 - the reason we do that again is so that
21:05 - when we are adjusting our weights and
21:07 - biases and we add that activation
21:09 - function in now we can have a way more
21:11 - complex function as opposed to just
21:14 - having the kind of linear regression
21:15 - straight line which is what we've talked
21:18 - about in my other machine learning
21:19 - courses so if this is kind of going a
21:21 - little bit over your head it may be my
21:23 - lack of explaining it I'd love to hear
21:25 - in the comments below how you think this
21:27 - explanation but essentially that's what
21:28 - the activation function does
21:30 - now another activation function that is
21:32 - very popular and is actually used way
21:34 - more than sigmoid nowadays is known as
21:36 - rectified linear unit and what this does
21:38 - is it having draw it in red actually so
21:41 - it we can see it better is it takes all
21:43 - of the values that are negative and
21:45 - automatically puts them to zero and
21:47 - takes all of the values that are
21:49 - positive and just makes them more
21:51 - positive essentially or like to some
21:54 - level positive right and what this again
21:57 - is gonna do is it's a nonlinear function
21:58 - so it's going to enhance the complexity
22:01 - of our model and just make our data
22:04 - points in between the range zero and
22:06 - positive infinity which is better than
22:07 - having between negative infinity and
22:09 - positive infinity for when were
22:10 - calculating air all right
22:14 - last thing to talk about for neural
22:15 - networks in this video I'm trying to
22:16 - kind of get everything like briefly into
22:18 - one long video is a loss function so
22:23 - this is again gonna help us understand
22:25 - how these weights and these biases are
22:27 - actually adjusted so we know that
22:29 - they're adjusted and we know that what
22:30 - we do is we look at the output and we
22:32 - compare it to what the output should be
22:35 - from our test data and then we say okay
22:38 - well it's adjust the weights and the
22:39 - biases accordingly but how do we adjust
22:41 - that and how do we know how far off we
22:44 - are how much to tune by if an adjustment
22:46 - even needs to be made well we use what's
22:49 - known as a loss function so a loss
22:52 - function essentially is a way of
22:54 - calculating error now there's a ton of
22:56 - different loss loss functions some of
22:58 - them are like mean squared error that's
23:00 - the name of one of them I think one is
23:01 - like I can't even remember the name of
23:05 - this one but there's there's a bunch of
23:06 - very popular ones if you know some leave
23:08 - them in the comments love to hear all
23:09 - the different ones but anyways what the
23:12 - loss function will do is tell you how
23:14 - wrong your answer is because like let's
23:18 - think about this right if you get an
23:19 - answer of let's say maybe our output is
23:22 - like zero point seven nine and the
23:24 - actual answer was one well that's pretty
23:27 - close
23:27 - like that's pretty close to one but
23:29 - right now all we're gonna get is the
23:30 - fact that we were zero point two one off
23:33 - okay so zero point two went off so
23:35 - adjust the weights a certain degree
23:36 - based on zero point two one but the
23:38 - thing is what if we get like zero point
23:42 - eight five
23:44 - well is this like this is significantly
23:46 - better than zero point seven nine but
23:47 - this is only gonna say that we were
23:49 - better by what is this zero point one
23:52 - five so we're still gonna do significant
23:54 - amount of justing to the weights and the
23:55 - biases so what we need to do is need to
23:57 - apply a loss function to this that will
23:59 - give us a better kind of degree of like
24:03 - how wrong or how right we were now these
24:05 - loss functions are again not linear loss
24:08 - functions which means that we're gonna
24:10 - add a higher degree of complexity to our
24:12 - model which will allow us to create way
24:15 - more complex models and neural networks
24:17 - that can solve better problems I don't
24:19 - really want to talk about loss functions
24:20 - too much because I'm definitely no
24:22 - expert on how they work but essentially
24:25 - what you do is you're comparing the
24:26 - output to the
24:28 - what the output should be so like
24:29 - whatever the model generated based what
24:31 - it should be and then you're gonna get
24:32 - some value and based on that value you
24:34 - are going to adjust the biases and the
24:36 - weights accordingly the reason we use
24:38 - the loss function again is because we
24:39 - want a higher degree of complexity
24:41 - they're nonlinear and you know if you
24:43 - get 0 if you're 99 percent like say your
24:45 - point one away from the correct answer
24:47 - we probably want to adjust the weights
24:49 - very very little but if you're like way
24:52 - off the answer you're two whole points
24:54 - maybe our answer is negative one we want
24:56 - it to be one well we want to adjust the
24:58 - model like crazy right because that
25:00 - model was horribly wrong it wasn't even
25:02 - close so we would adjust it way more
25:04 - than just like two points of adjustment
25:06 - right
25:07 - we'd adjust it based on whatever that
25:09 - loss function gave to us so anyways this
25:12 - has kind of been my explanation of a
25:14 - neural network I want a bear I want to
25:16 - stay right here for every one that I am
25:18 - no pro on neural networks this is my
25:20 - understanding there might be some stuff
25:21 - that's a little bit flawed or some areas
25:23 - that I skipped over and quickly actually
25:26 - because I know some people probably
25:28 - gonna say this when you're creating
25:29 - neural networks as well you have another
25:32 - thing that is called hidden layers so
25:34 - right now we've only been using two
25:36 - layers but in most neural networks what
25:37 - you have is a ton of different input
25:39 - neurons that connect to what's known as
25:41 - a hidden layer or multiple hidden layers
25:44 - of neurons so let's say we have like an
25:46 - architecture maybe that looks something
25:47 - like this so all these connections and
25:50 - then these ones connect to this and what
25:51 - this allows you to do is have way more
25:54 - complex models that can solve way more
25:56 - difficult problems because you can
25:58 - generate different combinations of
25:59 - inputs and he didn't what what is known
26:02 - as hidden layered neurons to solve your
26:05 - problem and have more weights and more
26:06 - biases to adjust which means you can on
26:09 - average be more accurate to produce
26:12 - certain models so you can have crazy
26:14 - neural networks that look something like
26:16 - this but with way more neurons and way
26:18 - more layers and all this kind of stuff I
26:20 - just wanted to show a very basic network
26:23 - today because I didn't want to go in and
26:25 - talk about like a ton of stuff
26:27 - especially because I know a lot of
26:28 - people that watch my videos are not pro
26:30 - math guys are just trying to get a basic
26:32 - understanding and be able to implement
26:34 - some of this stuff so anyways again that
26:36 - has been my understanding if you guys
26:37 - have any questions please leave them
26:39 - below or join my discord server and
26:41 - feel free to ask I'm always helping
26:43 - people out but that being said if you
26:44 - guys enjoyed the video please make sure
26:45 - you leave a like and subscribe and I
26:48 - will see you again in the next one
26:54 - [Music]