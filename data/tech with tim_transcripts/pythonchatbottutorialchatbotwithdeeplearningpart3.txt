00:00 - hey guys and welcome back to part three
00:02 - of AI chat bots in Python now up until
00:06 - this point all we've been doing is
00:07 - pre-processing our data and we have a
00:09 - few small things we need to do before we
00:11 - can move on from that but after that
00:13 - what we're gonna do is actually talk
00:15 - about the model that we're gonna use to
00:16 - make predictions based on a string of
00:19 - text and how that kind of works and just
00:22 - talk about that and draw some things I
00:23 - don't understand that as well as coded
00:25 - here now the first thing we need to do
00:27 - just a few minor errors here always run
00:29 - into typos and stuff when I'm doing this
00:31 - and I realize later but up here where it
00:33 - says Doc's X dot append pattern we need
00:36 - to change that to words and that is
00:39 - because we want to append the tokenized
00:41 - words I thought when we do this
00:44 - creating bag of words it actually works
00:46 - we also need to change this here to be
00:50 - labels because before that was classes
00:54 - and I don't have I don't have a list
00:56 - called classes so this needs to be
00:57 - labels I don't know why I called it
00:58 - classes and then the last thing is for
01:00 - these unique words here for W in words
01:04 - what we're actually gonna do is we're
01:05 - gonna say just add an if statement at
01:07 - the end of this just to remove any
01:08 - question marks because question marks
01:10 - are is a pretty common thing that people
01:12 - could type and we don't actually want
01:13 - that to have any meaning to the model so
01:15 - we're just gonna remove it so to do that
01:17 - we're gonna say if W not in and in this
01:21 - case question mark now we can do not in
01:23 - question mark where we just say W does
01:25 - not equal question mark doesn't really
01:27 - matter it's the same thing so we'll do
01:28 - does not equal question mark so those
01:30 - are the few fixes we need to do and now
01:33 - what we're gonna do actually is just
01:34 - change our output and our training into
01:37 - numpy arrays and that's because that is
01:39 - the form that needs to be taken by our
01:42 - model so we're just gonna say training
01:43 - equals an umpire dot array and in this
01:46 - case we're gonna say training and then
01:48 - we can just copy the same thing here and
01:51 - we'll do output equals numpy dot array
01:56 - and in this case we will say output now
02:01 - what this is gonna do is just take these
02:02 - arrays and change them or take these
02:04 - lists and change them into arrays so
02:06 - that we can feed them to our model so
02:08 - now that we've done that what we're
02:10 - gonna do is actually start building our
02:11 - model using T
02:12 - learn now this is very similar to
02:14 - tensorflow so if you've done tensorflow
02:16 - you understand how this works but I'm
02:18 - gonna code the model out and then we're
02:19 - going to kind of draw it and visualize
02:20 - what it actually looks like and
02:22 - understand how it's gonna work on
02:24 - classifying our data so we're gonna say
02:26 - TF which actually we have to type
02:30 - tensorflow because I didn't do import as
02:32 - - yep we'll do it tensor flowed reset
02:35 - underscore default underscore graph now
02:39 - we're just doing this to make sure that
02:40 - we get rid of all like previous settings
02:41 - and stuff it's resetting the underlying
02:44 - data graph or graph data you don't
02:46 - really have to understand what that
02:47 - means we're gonna say net equals in this
02:49 - case TF learned dot input underscore
02:53 - data and this K Swedish it's a shape
02:55 - equals and then none and the length of
03:00 - our training zero now what this is gonna
03:04 - do is define the input shape that we're
03:06 - expecting for our model so in this case
03:09 - we're getting the length of training
03:10 - zero because each training input is
03:13 - gonna be the same length so by doing
03:15 - this we're saying okay well the model
03:16 - should expect us to have an array of
03:18 - length forty five or however many words
03:21 - that we have right now next what we're
03:24 - gonna do is say next equals TF learn dot
03:28 - fully underscore connect it it's a lot
03:32 - of typing but it's we won't have to do
03:33 - that much I'm gonna say net eight and
03:36 - what this means is we're gonna say where
03:38 - you add this fully connected layer to
03:40 - our neural network which starts at this
03:42 - input data and we're gonna have eight
03:43 - neurons for that hidden layer or this
03:46 - first yeah I guess hidden layer now
03:48 - after this we're just gonna copy this
03:50 - again because we're gonna have another
03:52 - hidden layer that has eight neurons as
03:54 - well and then finally we need two more
03:57 - layers so we're gonna do TF learned up
03:58 - fully connected in this case we're gonna
04:00 - do net but it's gonna be our output
04:02 - layer so it's gonna be the length of
04:04 - output 0 and then what we're gonna say
04:07 - here is activation equals softmax now
04:12 - what this is gonna do essentially is
04:14 - allow us to get probabilities for each
04:17 - output and we'll talk about this morning
04:19 - when I draw the model but essentially
04:20 - softmax is gonna go through and give us
04:23 - a probability for each neuron in this
04:26 - lair and that will be our output for the
04:29 - network now after that we're just gonna
04:32 - add next equals TF learned regression
04:39 - regression and we're going to apply that
04:41 - to network now to train our model what
04:46 - we're gonna do is you say model equals
04:48 - in this case TF learned DN n prints a
04:52 - net and that's all we need to do now
04:55 - this is actually our complete model
04:58 - that's the whole a I kind of aspect of
05:00 - this I know it seems really short but
05:02 - let's go through exactly what I just
05:03 - typed and what this is now essentially
05:07 - we should start with an input data which
05:10 - is the length of our training data that
05:11 - we have two hidden layers with eight
05:13 - neurons fully connected I also connected
05:15 - to an output layer that has neurons
05:17 - representing each of our classes so let
05:19 - me actually bring out my drawing time
05:21 - but quickly and show you a little
05:22 - picture of this maybe make it a bit more
05:24 - clear on what's going on here so give me
05:28 - one second just to get this set up here
05:30 - and let's load up our little drawing
05:33 - thing if I can get this going here okay
05:37 - so what we have right now is we have a
05:40 - neural network and I'm just gonna draw
05:41 - it out for us we can have a look at
05:43 - exactly what it is like now we start
05:46 - with a a bunch of input neurons which
05:48 - are the length of our input data which
05:50 - means however many words we have because
05:52 - our bag of words is gonna be how many
05:54 - words we had in the thing so in this
05:55 - case I think we have something like 45
05:57 - or something so we're gonna say we have
05:59 - a bunch of neurons and in this case
06:01 - let's just say we have like 45 of them
06:03 - okay so say this is like 45 now this is
06:08 - our input okay this is our first layer
06:10 - this is our input layer now the next
06:13 - layer that we have is 8 neuron so one
06:15 - two three four five six seven eight and
06:19 - they are connected to each our inputs so
06:21 - each input connects to each neuron just
06:24 - like this now I don't want to draw all
06:25 - of them out but you guys get the point
06:28 - it's fully connected just like that okay
06:31 - that's all I'm gonna draw for that now
06:32 - we have another layer that has eight
06:34 - neurons so we draw another eight neurons
06:39 - and all of these neurons are connected
06:40 - together fully connected once again so
06:42 - each one of these neurons connects to
06:44 - each other one in the lair again I'm not
06:46 - going to draw all the lines and then
06:48 - finally we have our output layer which
06:51 - has a soft max activation and this I'll
06:54 - just make it green so it's a bit
06:55 - different has six nerves so one two
06:57 - three four five six now again these are
07:02 - fully connected so it goes like this
07:04 - and all of them connect not gonna draw
07:06 - them all out so our output layer is
07:08 - special because it has this soft max
07:11 - activation so essentially what this
07:13 - means is all of these neurons are gonna
07:16 - be run through this softmax activation
07:18 - function and what that's gonna do is
07:21 - give a probability to each of these
07:23 - neurons so let's say that this first
07:25 - neuron represents hello okay and maybe
07:28 - that's the tag we have maybe this one
07:30 - represents goodbye and so on you know
07:33 - these all represent specific classes
07:35 - well if the model thinks that our
07:38 - response should be the hello tag so one
07:41 - of the responses under the hello tag the
07:43 - miss neuron will have a higher
07:44 - probability than all of these ones and
07:47 - that's essentially the way that this
07:49 - works it's gonna say well I think that
07:52 - you know it's 70% the hello tag so since
07:55 - that's the highest probability we will
07:57 - take that and well then we will grab
07:59 - some kind of response from Hello and
08:01 - spit that out to the user so all our
08:04 - model is really doing is predicting
08:06 - which tag that we should take a response
08:09 - from to give to the user so again we
08:11 - have six tags are because that's how
08:13 - many labels we have and we just our
08:16 - model picks one of these it actually
08:18 - gives us prediction values for all of
08:20 - them we say whatever one is the most
08:22 - highly predicted so maybe this one is
08:24 - like 90% this one's like 0.1% this one's
08:28 - like 5% we take the greatest highest
08:31 - predicted one we grab some responses
08:33 - from that we randomly pick one and then
08:35 - we give that to our user and that's kind
08:37 - of the way that this model works so we
08:39 - take in as input a bag of words and as
08:41 - output we get some kind of class or
08:45 - sorry label telling us what we think we
08:48 - should respond with what tag it comes
08:50 - from and that is how this very basic
08:52 - model
08:53 - works so what we are hoping is gonna
08:54 - happen when we start training and
08:56 - feeding information into our model so
08:58 - these hidden layers are gonna kind of
09:00 - figure out you know what words represent
09:03 - what one of these outputs may be if it
09:05 - sees the word hi it's gonna start
09:07 - changing some weights and changing some
09:09 - biases in here so that we get hello more
09:13 - commonly from that and these hidden
09:15 - layers are really what is doing kind of
09:16 - all of the work and they're what's gonna
09:18 - figure out what's going on and how this
09:20 - works and all of that kind of stuff with
09:22 - more and more intense or tags you would
09:25 - probably want to add more neurons to
09:27 - your hidden layers but two hidden layers
09:29 - is typically enough for a problem like
09:30 - this and you guys will see when we train
09:32 - the model this is actually a very
09:34 - accurate model for this kind of
09:36 - classification task that we're doing
09:38 - because essentially all we're doing is
09:40 - classifying sentences of words to some
09:42 - kind of output in some kind of tag so
09:46 - that is essentially how that works let's
09:48 - close that up now I hope you guys have a
09:49 - little bit of an understanding again I'm
09:51 - not gonna really teach neural networks
09:52 - but I want you guys to understand a
09:55 - little bit on why I chose this model and
09:57 - how it kind of works
09:59 - now the DNN is just a type of neural
10:01 - network and it's just gonna take these
10:03 - networks in or this network that we've
10:06 - defined here and just use that so now
10:09 - it's time to actually fit or our model
10:11 - so to do this we're gonna say model dot
10:13 - fit and what this means is we're
10:14 - actually gonna start passing it all of
10:16 - our training data so let's do this so
10:19 - we're gonna pass it training we're gonna
10:21 - pass it output we're gonna say number of
10:24 - underscore epochs and number of epochs
10:26 - is the amount of times that it's gonna
10:29 - see the same data so in this case we're
10:31 - gonna show it the same data a thousand
10:33 - times and hopefully the more it sees the
10:35 - data the better it gets at classifying
10:37 - now mess with this number make it 2,000
10:40 - make it 5,000 make it a hundred and see
10:42 - what you get by doing that a lot of
10:44 - machine learning is trial and error so
10:46 - you got to understand that okay so batch
10:48 - underscore size we're gonna set as eight
10:51 - and we're gonna say show underscore
10:53 - metric equals true and this is just so
10:55 - that we get a nice kind of output when
10:57 - we're fitting this model now the last
10:59 - thing to do after we do this is simply
11:01 - save the model so we're just gonna say
11:03 - model dot Save
11:04 - and in this case we're just gonna save
11:06 - it as I guess we'll do like model dot
11:09 - tf2 learn and that's fine we'll save the
11:13 - model and that should just work for us
11:16 - when we start running this if we want to
11:18 - use the model to make some predictions
11:20 - we're gonna do that in the next video
11:22 - but let's just see how this works so let
11:24 - me go ahead I've already activated my
11:25 - environment I'm just gonna run this
11:26 - script and make sure that nothing went
11:29 - wrong here okay so there we go we're
11:32 - running our model you can see it's going
11:34 - through all these different epochs and
11:36 - you'll notice that when it stomps we had
11:37 - an accuracy of 99.97% which means it
11:41 - worked very well for our intents now
11:44 - again this is very simple because we
11:45 - only have six kind of tags here in our
11:47 - intents if you were to add more you
11:49 - would expect your accuracy would drop
11:51 - slightly but this model seems to be
11:54 - working very well at least on the data
11:56 - that it seems so far so the next step
11:58 - and in the next video we're going to
11:59 - start predicting data using this model
12:02 - and then in the final video in this
12:03 - series what we're going to do is
12:04 - actually set up a framework that will
12:06 - allow users to type to the model and get
12:08 - responses there's another kind of bonus
12:10 - part that I might do in this a sixth
12:13 - episode of this series but that's gonna
12:15 - have to wait till later so anyways that
12:16 - has been it for this chatbot AI video if
12:19 - you guys enjoyed please make sure leave
12:20 - a like and subscribe to the channel and
12:22 - I will see you in the next tutorial