00:00 - what is Rag and what you need to know
00:01 - about it as a developer well rag stands
00:04 - for retrieval augmented generation and
00:06 - this is something that promises to
00:08 - drastically enhance the usability of
00:10 - llms we've probably seen that llms have
00:13 - quite a few issues they are very usable
00:15 - and a massive tool but if you wanted to
00:16 - implement them into your own application
00:18 - you can't be confident that the results
00:20 - that they're giving you are accurate and
00:22 - you don't know how they came up with the
00:24 - answer rag is something that helps with
00:26 - that problem and I'm going to break down
00:27 - everything you need to know about it in
00:29 - this video
00:30 - [Music]
00:33 - so let's begin with the problems with
00:34 - current llms these llms are giving you
00:37 - answers that often times are out of date
00:40 - the reason for that is they're only
00:41 - pulling data from the data that they
00:43 - were trained on whenever that date
00:45 - occurred so in the case of Chad gbt
00:47 - you've probably seen that famous reply
00:49 - where it says I don't know that answer
00:51 - because I've only been trained up to
00:52 - September 2021 or in a worst case
00:55 - scenario it gives you an answer but that
00:57 - answer is actually incorrect because
00:59 - since then then we've come to some new
01:01 - discovery or found some new information
01:03 - that makes that answer obsolete for
01:05 - example if you're asking things about
01:06 - scientific research it'll actually give
01:08 - you an answer most times but it might
01:10 - not be the most upto-date or current
01:12 - answer yet it's very convincing and if
01:14 - you don't know the correct answer you
01:16 - could be easily misled and end up using
01:19 - inaccurate information so that's the
01:21 - first major problem out ofd data next
01:24 - problem is that a lot of times you don't
01:26 - have any idea how the llm actually came
01:29 - up with the answer it doesn't give you a
01:31 - source you're kind of just blindly
01:32 - trusting that what it gave you is
01:34 - accurate and even if it is accurate a
01:36 - lot of times you'd probably like to know
01:38 - where it C that information from so you
01:40 - could go and double check it yourself so
01:41 - these are the two major issues outof
01:44 - date and no source so how do we fix that
01:47 - using retrieval augmented generation or
01:52 - rack well rag is actually quite simple
01:55 - what this does is connect an llm to a
01:58 - data store this means that that when the
02:00 - llm actually wants to come up with an
02:02 - answer rather than just using its
02:04 - training data it will actually go and
02:06 - ask a retriever to get a specific piece
02:08 - of data or some content from the data
02:10 - store it will then inject that kind of
02:12 - into the prompt or into the llm model
02:15 - and then it will use that data that it
02:17 - retrieved that likely is up to date to
02:19 - generate a reply so to give you a super
02:21 - simple example of this let's imagine I
02:23 - want to use an llm to retrieve upto-date
02:26 - scores for a football game now obviously
02:28 - there's better ways to do this but let's
02:30 - say for some reason I want to use an llm
02:32 - well what I would do is connect that to
02:34 - a realtime database that contains all of
02:36 - the NFL scores that database we're going
02:39 - to trust is always up toate and contains
02:41 - the accurate information now what will
02:43 - happen is when I have a prompt or I give
02:45 - that to chat GPT or the llm whatever it
02:47 - is it will now go to the data store it
02:50 - will retrieve the relevant information
02:52 - based on the data in my prompt it will
02:54 - kind of inject that into the llm and
02:56 - then it will use the data it just
02:58 - retrieved to answer my question so now
03:00 - we've kind of solved both problems I
03:02 - have updated information and I have a
03:05 - source for where that information is
03:06 - coming from it's that data store so if I
03:08 - want to know where I actually got my
03:10 - data or I want to go there and double
03:11 - check it I can simply go to the original
03:14 - source and now what we're doing is we're
03:16 - using the llm for its reasoning ability
03:18 - and ability to understand natural
03:20 - language not its huge memory from a
03:23 - massive data or training set this is
03:25 - really where this type of technique
03:27 - comes in rather than using an llm as a
03:29 - knowledge base you're using it as
03:31 - something that can reason something that
03:33 - can give you a natural reply and can
03:35 - understand what you're asking better
03:37 - than probably any model you could train
03:38 - on your own so it's really the interface
03:41 - for some type of application and then
03:43 - you have the actual data that gets
03:44 - injected and brought into the llm and
03:47 - you can control that data you could have
03:49 - data for I don't know your call center
03:51 - you could have data for an application
03:53 - or a stat tracking website whatever you
03:55 - want any data you want you can augment
03:57 - the llm with that and allow it to reason
04:00 - solely based on that data and that way
04:02 - you know you're getting accurate
04:04 - upto-date information and if you want to
04:05 - check that information you can go
04:07 - directly to the source now just as a
04:09 - quick note here before we dive in too
04:10 - far to really understand the benefit of
04:12 - something like rag you first need to
04:15 - understand how llms actually work and
04:17 - for most of you you're probably going to
04:18 - be interacting with something like chat
04:20 - gbt now fortunately our video sponsor
04:22 - HubSpot actually has a completely free
04:25 - resource called how to use chat gbt at
04:27 - work that breaks down exactly how chat
04:29 - gbd Works gives you expert insights and
04:32 - tells you how you can use it to its full
04:33 - ability now I put the link in the
04:35 - description so you guys can check it out
04:37 - but this resource is packed with
04:38 - knowledge and it even has 100 actionable
04:41 - prps that you can use today to really
04:43 - leverage the full power of chat GPT
04:45 - knowing how to use a tool like this
04:47 - effectively is absolutely a game Cher
04:49 - especially in the programming industry
04:51 - and again you guys can check that out
04:52 - from the link in the description a
04:54 - massive thank you to HubSpot for making
04:55 - this resource and tons of others
04:57 - completely free
05:00 - [Music]
05:01 - so to give you the highlevel
05:03 - architecture here we have a prompt this
05:05 - is the first step once you generate the
05:08 - prompt you can actually vectorize that
05:10 - and you can then go to a retriever which
05:12 - will go and find all of the relevant
05:14 - data required for the specific prompt it
05:17 - will then augment the llm with that so
05:19 - it will essentially pass that into the
05:21 - prompt the llm will give you some type
05:23 - of reply and then it can actually
05:25 - provide evidence directly from your data
05:27 - source as to why it came up with that
05:29 - answer now this also gives another
05:31 - advantage that if your data source
05:33 - doesn't have the answer the llm can tell
05:35 - you that rather than giving you a
05:37 - convincing answer that's actually wrong
05:39 - or misleading it can simply say hey
05:41 - based on what you gave me here for data
05:43 - I don't actually have the answer now you
05:45 - can decide whether that's better or
05:46 - worse but in my opinion I'd rather the
05:48 - llm tell me I don't know than tell me an
05:50 - incorrect and false statement or kind of
05:53 - false information that might mislead me
05:54 - or have some pretty bad
05:58 - repercussions
06:00 - so now let's quickly dive into some of
06:01 - the major benefits of using a technique
06:03 - like rack first of all this allows you
06:06 - to avoid retraining language models and
06:09 - instead augmenting them with up-to-date
06:11 - information we've already touched on
06:12 - this but typically if you'd want the llm
06:14 - to have upto-date data you'd have to
06:16 - retrain it on that data or if you want
06:18 - it to work specifically for your company
06:20 - information you train it on that data
06:23 - now you no longer need to do that you
06:24 - simply keep your data source up to dat
06:27 - and all of a sudden the model works
06:28 - exactly as you would expect now the next
06:30 - major benefit of course is the source
06:32 - knowing where the data actually came
06:34 - from and being able to validate that is
06:36 - massive and then lastly kind of touching
06:38 - on that being able to actually answer
06:40 - and say I don't know is a huge benefit
06:43 - so if the model doesn't know you can
06:44 - then go to the data source add the
06:46 - correct information or at least you know
06:48 - you're not getting a misleading
06:52 - answer now as well as all of these
06:54 - benefits there are a few concerns that
06:56 - you want to keep in mind first of all
06:58 - all of this only works works if the data
07:00 - that's augmented into the model is good
07:02 - and relevant now that means you need to
07:04 - come up with a quick efficient and
07:06 - accurate way for retrieving relevant
07:08 - information based on a prompt now
07:10 - there's a lot of complex stuff you can
07:12 - do here but the simplest way to do this
07:14 - would be to use something like a vector
07:15 - database that means you would vectorize
07:18 - the prompt you would then go in the
07:19 - vector database and you would find all
07:21 - of the documents that have similarity
07:23 - based on their Vector representation and
07:25 - then return those but you can't simply
07:27 - return every piece of information you
07:29 - have to be selective at what you're
07:30 - returning and this needs to happen very
07:33 - very quickly it can't introduce a ton of
07:35 - latency otherwise that's going to be a
07:36 - poor user experience now at the same
07:38 - time you want to make sure that the
07:40 - model is accurately using this augmented
07:42 - data so there's some special prompts and
07:44 - ways that you can kind of instruct the
07:45 - model to make sure it only gives you
07:48 - information based on the data that was
07:50 - augmented into it now just as a point of
07:51 - clarity here by no means am I an expert
07:54 - in rag I'm sure there might be some
07:55 - slight inaccuracies in this video but I
07:58 - wanted to share with you the the general
07:59 - idea of rag so that you guys can go look
08:02 - it up and see how you can use it in your
08:04 - applications now I actually did make a
08:06 - video that uses this type of technique
08:08 - and framework to generate a Choose Your
08:10 - Own Adventure game really interesting
08:12 - and probably the simplest example to see
08:14 - exactly how this works so if you want
08:15 - that I'll put it in the description and
08:17 - pop the video on the screen here anyways
08:19 - if you guys enjoyed make sure you leave
08:21 - a like subscribe to the channel and I
08:22 - will see you in another
08:24 - [Music]
08:28 - one