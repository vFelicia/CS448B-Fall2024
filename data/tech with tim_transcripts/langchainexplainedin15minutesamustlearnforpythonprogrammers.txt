00:00 - if you write code in Python and you're
00:01 - at all interested in AI then you need to
00:03 - check out Lang chain now this is a
00:05 - relatively new framework that allows you
00:07 - to build AI powered applications in both
00:09 - Python and JavaScript now we'll be
00:11 - looking at python here but the main
00:13 - benefit of Lang chain is that you can
00:15 - actually connect to external data
00:16 - sources databases apis and really any
00:19 - data that you'd like and you can very
00:21 - easily build applications that are
00:23 - context aware and that can reason using
00:25 - popular language models like chat GPT
00:28 - now in this video I'll give you a
00:30 - high-speed overview of Lang chain show
00:32 - you some of the simple features and
00:33 - capabilities and give you enough so that
00:35 - you can get started today building AI
00:37 - applications in Python in my opinion
00:40 - this is a must-learn really cool module
00:42 - and surprisingly simple stick around for
00:44 - the whole video to see exactly how you
00:46 - can use this in your next python app so
00:48 - the first thing we need to do to work
00:49 - with Lang chain is install it now they
00:51 - have fantastic documentation that I'll
00:53 - link down below that will show you all
00:55 - of these steps but of course you can
00:56 - just follow along with me so since we're
00:58 - going to be installing this for python
01:00 - on we can see on this page here we have
01:01 - a few options pip install Lang chain pip
01:04 - install Lang chain llms or pip install
01:06 - Lang chain all now I'm going to
01:08 - recommend that you go with the second
01:09 - option here you do not need to install
01:11 - all the dependencies but we do want to
01:13 - install the dependencies related to the
01:15 - language model so that we can connect
01:17 - with those immediately so let's go ahead
01:18 - and copy that command and paste that in
01:20 - our terminal I am running on Windows if
01:22 - you're on Mac or Linux you're likely
01:24 - going to want to adjust this command to
01:25 - be pip 3 so I've gone ahead and
01:27 - installed that and we're just going to
01:29 - install a few modules that we're going
01:30 - to use for these specific examples so
01:32 - I'm going to do pip install and then
01:35 - open AI we're going to use this to
01:37 - connect with open AI again if you're on
01:39 - Mac or Linux you're going to want to
01:40 - change that to pip 3 now once we install
01:42 - open AI I am also going to install the
01:46 - EnV Library so pip install python d.v
01:51 - now this is going to allow me to load in
01:53 - environment variable file so I can store
01:55 - my API keys in a specific file that's
01:57 - kind of hidden from the code you'll see
01:59 - what I I mean in one second those are
02:01 - all the modules that we need Lang chain
02:03 - with llms we need open aai and then we
02:06 - need python. EnV now in the name of
02:09 - saving us a little bit of time I've
02:10 - already ridden a few examples and I'll
02:12 - just walk through the code with you so
02:13 - you can see exactly how this works now
02:15 - the first thing I want to mention is
02:17 - that before you start using laying chain
02:18 - in any of the llms you want to have a
02:21 - decent idea of how they work
02:22 - specifically you want to understand what
02:24 - they're good at how you can use them
02:26 - properly and when you should be cautious
02:28 - of their replies now I imagine most of
02:30 - you are going to be using chat GPT at
02:32 - least that's what I'll be doing for this
02:33 - tutorial and fortunately for you our
02:35 - video sponsor HubSpot actually has a
02:37 - free guide that shows you how to use
02:39 - chat GPT at work now this free guide
02:42 - gives you a comprehensive overview of
02:44 - exactly how chat GPT works it shares
02:46 - some best practices and it gives you
02:48 - some expert insights so you really know
02:50 - how to use this and get the full power
02:52 - out of it now I've left a link in the
02:53 - description so you can check it out
02:55 - completely for free now this resource
02:57 - provides a ton of knowledge and even has
02:58 - 100 actionable prompts that you can try
03:01 - today to really leverage chat gbt
03:03 - knowing this especially as a programmer
03:05 - is an absolute GameChanger and I want to
03:07 - give a big thank you to HubSpot for
03:09 - providing this resource and a ton of
03:11 - others completely for free make sure to
03:13 - check it out from the link in the
03:14 - description and a massive shout out to
03:15 - HubSpot so now that you're an expert at
03:17 - chat gbt after clicking on that resource
03:19 - we can dive into some of the code so the
03:21 - first thing I want to show you is just a
03:23 - very simple example of interacting with
03:25 - an open AI model in this case chat GPT
03:27 - and using it to make a prediction now
03:29 - you you can use all kinds of llms but
03:31 - obviously this is the one most of you
03:32 - know so it's the one I'll use in this
03:34 - video now first what I've done is I've
03:36 - created a EnV file here and I've placed
03:38 - my open AI API key I'll show you how to
03:41 - retrieve that in 1 second if you don't
03:42 - have it but this way I can actually put
03:44 - the key inside of here and then you guys
03:46 - can't see it while I'm going through the
03:47 - tutorial so I've just put my actual key
03:49 - inside of there and you can see that the
03:51 - way I'm loading in this API key is I'm
03:53 - importing from the EnV module the load.
03:56 - EnV what this does is look for a EnV
03:59 - file in the same directory as our python
04:01 - script and loads in any of the variables
04:04 - then what I do is access the open aai
04:06 - API key variable here so I have it and I
04:08 - pass that to my chat open aai kind of
04:11 - interface component whatever you want to
04:13 - call it now this will actually create a
04:14 - connection to open AI into chat GPT and
04:17 - I can simply use this by just running
04:19 - chat model. predict and then passing
04:21 - inside of here a string so if I go to my
04:23 - terminal and I run python simple
04:26 - example.py you can see that we're able
04:28 - to interact with GPT here it just takes
04:30 - a second and it responds to us says
04:32 - hello how can I assist you today very
04:35 - simple you can see how easy it is to
04:36 - actually use the module from python now
04:38 - let's dive into some more complex
04:40 - examples and before I forget if you need
04:42 - an openai API key then you can go to
04:44 - platform.
04:45 - open.com account API keys if you're not
04:48 - using it very much you're not going to
04:50 - have to pay for it I've used it a ton
04:51 - and I think I've paid 10 cents over the
04:53 - past few months anyways let's move on to
04:55 - the next example all right so moving on
04:57 - to the next example I just want to show
04:59 - you that there's there's actually a way
05:00 - to pass multiple messages and get a
05:02 - single reply or response based on all of
05:05 - those messages combined this can be
05:07 - useful because sometimes you have some
05:08 - pieces of context that you want to
05:10 - provide to the model you don't
05:11 - necessarily want it to give you a
05:13 - prediction or a reply based on that you
05:15 - want to actually feed some other text or
05:17 - some other information and get one kind
05:19 - of aggregated reply so in this case what
05:21 - I've done is just create an array of
05:23 - messages here I've specified that these
05:25 - are human messages now we can also have
05:27 - system messages assistant messages agent
05:29 - messages there's a bunch of different
05:30 - types they're all in the documentation I
05:32 - won't go over them here and you can see
05:33 - that what I'm doing is saying from now
05:35 - on 1 + 1al 3 use this in your replies
05:38 - and then I'm asking it some questions
05:40 - based on the context I just provided so
05:42 - what is 1+ 1 and what is 1 plus 1+ 1 so
05:45 - here I go chat model. predict messages
05:48 - pass in the messages and if I go ahead
05:50 - and run the code here we should see that
05:53 - it's actually going to be using this
05:54 - context so according to the new rule 1 +
05:56 - 1 = 3 therefore 1 + 1 + 1 would be equal
06:00 - to 3 + 1 which equals 4 kind of
06:03 - interesting how you can do that just
06:04 - wanted to show you that quick example so
06:06 - now we get into a slightly more
06:07 - interesting example where we can use
06:09 - another feature from langing chain known
06:11 - as a prompt template now you may have
06:13 - seen this before but quite frequently
06:15 - what we want to do is actually inject
06:17 - some data into a prompt and this prompt
06:19 - we've kind of crafted engineered and we
06:22 - know that it's going to give us a
06:23 - specific type reply so in this case what
06:25 - we've done is we've set up a template we
06:27 - say you are a helpful assistant that
06:28 - translates some input message to some
06:31 - output language and we put those inside
06:33 - of curly braces we then have a human
06:35 - template and this is just some text but
06:37 - we could also have something here as
06:38 - well what we then do is create a prompt
06:41 - template from uh link chain so we say
06:43 - chat prompt is equal to chat prompt
06:45 - template. from messages and then we can
06:47 - specify as many messages as we'd like to
06:49 - be a part of this larger template so
06:52 - what this does now is specify that this
06:54 - is a system message so this template
06:56 - right here kind of telling the model
06:57 - what it's about to do and then we have a
06:59 - human message which is what we're
07:00 - providing to it each time then what we
07:03 - can do is we can actually create the
07:05 - messages that we want to pass the model
07:07 - from our chat prompt so we say chat
07:09 - prompt. format messages and we pass in
07:12 - the variables corresponding with what
07:13 - we've written here inside of the
07:15 - template and they're automatically going
07:16 - to get injected inside now obviously in
07:19 - Python you could do this manually but
07:21 - having this API makes things a little
07:22 - bit easier so we have input language
07:24 - output language and text so that would
07:26 - make the template you are a helpful
07:28 - assistant that translates English to
07:30 - French and then we pass our human
07:32 - template which is I love programming in
07:34 - this simple example I know it seems very
07:36 - easy but when you get into more complex
07:38 - templates and complex tasks this can be
07:40 - quite useful then what we can do is we
07:42 - can predict messages again using the
07:44 - same chat model we had before and we can
07:47 - get the output so let's see what this
07:48 - does now if we go up here and go python
07:52 - prompt template it should give us
07:54 - whatever the translation of I love
07:55 - programming is in French and it does it
07:57 - says adore programmer
07:59 - again imagine you had like seven system
08:01 - messages a bunch of different variables
08:03 - you could create some really interesting
08:05 - applications here using this prompt
08:07 - template style so continuing we get even
08:09 - more complex here and what I'm now
08:11 - showing you is something known as an
08:12 - output parser again all this stuff you
08:14 - can write manually but having these
08:16 - hooks in this API is quite useful and
08:18 - can make your life a lot easier so a lot
08:20 - of times when you're using a model what
08:22 - you actually want to do is parse the
08:23 - output into a specific format for
08:25 - example it might give you a comma
08:27 - separated list it might give you AJ file
08:29 - it might give you some code but that's
08:31 - going to be in a string and typically
08:33 - what you'll want to do is actually take
08:34 - that string and parse it based on some
08:36 - rules so you can actually get the data
08:38 - that you'd like for example parsing a
08:40 - price or parsing a piece of code Json
08:42 - like I said right so what we can do here
08:44 - is actually write a custom output parser
08:46 - and you'll see why this is useful in a
08:48 - sec when we kind of combine it in the
08:49 - chain what this will do is return to us
08:52 - the parsed response in the format that
08:55 - we want so all we've done here is just
08:56 - overridden the base output parser which
08:58 - I've uted recorded up here we've just
09:00 - overridden this parse method and inside
09:02 - of here we take in some text we strip
09:05 - the text and then we split it at answer
09:07 - equals this now the reason that makes
09:08 - sense is because in my template I said
09:11 - you are a helpful assistant that solves
09:12 - math problems and shows your work output
09:14 - each step then return the answer in the
09:16 - following format answer equals and then
09:18 - the answer provided here make sure the
09:20 - output answer is in all lower cases and
09:22 - to have exactly one space and one equal
09:24 - sign following now I'm instructing the
09:26 - model to give me a specific reply in AIC
09:29 - specific format such that I can parse it
09:31 - so I get the data that I'm looking for
09:33 - and I could use that in the rest of my
09:35 - application then I have the human
09:37 - template which is just whatever the math
09:38 - problem is again I use my chat prompt
09:41 - template here and then what I do is
09:43 - format my message by providing some math
09:45 - problem in this case just a basic I
09:48 - guess quadratic formula thing that you
09:49 - need to solve then what we do is we
09:51 - predict messages using chat bottle and
09:54 - then I'm manually invoking the output
09:55 - parser here and parsing the reply you'll
09:58 - see in the next example how we actually
09:59 - combine this into a chain and make it
10:01 - even easier to use then what I'm doing
10:03 - is I'm parsing out the steps and the
10:05 - answer from my pars reply which is now a
10:07 - list that was split at answer equal to
10:10 - so you'll see now if I go ahead and run
10:12 - my code so python output parser dop and
10:17 - you can see that we get X is equal to 3
10:18 - over2 or X is equal to 1 now if we also
10:21 - wanted to see the steps here then I
10:23 - could print out the steps and you can
10:25 - see how this could be useful because if
10:27 - we had some type of calculator we could
10:29 - allow maybe the student or whoever is
10:30 - using it to either view the steps or
10:32 - hide the steps or just view the answer
10:33 - or hide the answer because we've parsed
10:35 - that reply so let's have a look at this
10:37 - here gives us all of the different steps
10:39 - for solving the problem and then finally
10:41 - the answer down here all right so now
10:43 - we're going to kind of tie everything
10:44 - together and have a look at creating a
10:46 - chain now this is where laying chain
10:47 - really comes in handy and obviously
10:49 - you'd get much more complicated than
10:51 - this but you'll see in this example that
10:52 - we can combine all of those steps that
10:54 - we were doing manually before into a
10:56 - chain using some special syntax from
10:59 - Lang chain so just like before we have
11:01 - our model we have an output parser this
11:03 - time what it does is just parse a comma
11:05 - separated list and then we have a
11:07 - template it says you are helpful
11:08 - assistant who generates comma separated
11:10 - list a user will pass in a category and
11:12 - you should generate five objects in that
11:14 - category only return a comma separated
11:16 - list and nothing more we then have the
11:18 - human template again we create our
11:20 - prompt template this time though rather
11:22 - than manually calling all these
11:24 - different methods we create a chain now
11:26 - we use this pipe operator here which is
11:28 - special syntax in Lang chain which
11:30 - combines these three operators into one
11:33 - kind of main object that we can utilize
11:35 - to invoke so you see we have our chat
11:38 - prompt we have our chat model so the
11:40 - first thing we do right is we get the
11:42 - prompt we pass that to the model and
11:44 - then we pass the reply to the output
11:45 - parser and again this happens
11:47 - automatically when we call chain. invoke
11:50 - so now we just pass in what we need for
11:53 - our prompt so in this case we have an
11:54 - argument text which is equal to colors
11:56 - and we can print the result so let's go
11:59 - go here and give this run python chain.
12:02 - Pi so you can see we get our reply red
12:04 - blue green yellow purple and this is
12:06 - actually a python object that has been
12:08 - parsed for us so just to give you a bit
12:10 - more information on the capabilities of
12:11 - Lang chain let's have a look at one of
12:13 - the use cases they have in their
12:14 - documentation which is SQL now as I was
12:17 - saying Lang chain can also connect to
12:19 - data sources so they have a bunch of
12:20 - Integrations for connecting with popular
12:22 - databases and even Vector databases as
12:25 - well now in this case what we can
12:26 - actually do is use an llm to generate
12:29 - SQL queries so that we can ask in
12:31 - natural language for information from
12:33 - our database so imagine you had a large
12:35 - database you had a bunch of information
12:37 - and you wanted to actually provide an
12:38 - interface to your users so that they
12:40 - could use natural language to query the
12:43 - database and grab maybe information or
12:45 - analytics from their profile or whatever
12:47 - it is in this case you can see that we
12:49 - can build SQL queries query a SQL
12:50 - database and interact with the SQL
12:52 - database by using natural language and
12:54 - this is kind of the diagram we ask a
12:56 - question goes to the llm generates the
12:58 - query goes to SQL SQL returns that's the
13:01 - L and the L parses that and gives us the
13:03 - answer so in this case we can have a
13:05 - quick look at how this might work we say
13:07 - DB chain. run how many employees are
13:09 - there what it does is it generates the
13:11 - SQL query gets the result and then gives
13:13 - us an answer in a human readable format
13:16 - obviously a little bit easier said than
13:17 - done but you can see the simple
13:19 - connection that happens here connect to
13:20 - the SQL database connect to open AI
13:23 - create the chain here and then you can
13:24 - start using this exact example so the
13:27 - next use case they have here which is
13:28 - interesting thing is question answering
13:30 - so let's say we have a large piece of
13:32 - documentation maybe related to code
13:33 - maybe a PDF maybe a website what we can
13:36 - actually do is inject that into the
13:38 - model and then ask questions about that
13:40 - piece of data that the llm can respond
13:43 - to so you can see we have maybe some
13:44 - code some structured data some SQL pass
13:46 - that into llm and then interact with
13:49 - that directly by asking questions of the
13:51 - llm so where this becomes actually
13:53 - really interesting is we can ask the llm
13:55 - in natural language what it is that we
13:57 - want and then it can go and retrieve the
13:59 - correct piece of data if we set this up
14:01 - in the correct way to actually give us
14:03 - the answer so in some instances we don't
14:05 - even need to provide all the context to
14:07 - the llm immediately it can actually ask
14:10 - for the specific data it needs and send
14:13 - the correct request using whatever code
14:15 - it's generating this is a little bit
14:17 - more complicated than the example that
14:18 - they have here but you can kind of see
14:20 - how they're setting things up and how we
14:22 - would be able to input some data and
14:24 - then ask again in natural language
14:26 - questions about that that could be
14:27 - answered almost instantly now as a last
14:29 - example here I actually made a video on
14:31 - my channel a few weeks ago that injected
14:33 - memory into an llm so that we could
14:36 - actually remember and have context about
14:38 - all the previous replies and answers now
14:40 - I did this by making a Choose Your Own
14:42 - Adventure AI based game you guys can
14:44 - check it out from the link in the
14:45 - description it's a full tutorial
14:47 - walkthrough of how this works but pretty
14:49 - much what we can do is generate a Choose
14:51 - Your Own Adventure game that has
14:53 - multiple different variable paths and
14:55 - where you're going is going to be based
14:56 - on previous answers that you've had so
14:59 - if I said I was going left at the first
15:00 - step where I decided to have an axe as
15:02 - my weapon for the game the llm would
15:04 - actually use that context to continue to
15:06 - generate the story for me and make it
15:08 - context relevant really it's going to be
15:11 - cooler if you guys go check out that
15:12 - video and see exactly how that works but
15:14 - really interesting stuff you can do here
15:16 - and I did that all in Lang chain anyways
15:18 - with that said guys I'm going to wrap up
15:19 - the video here I hope you found this
15:21 - helpful and this gave you a quick
15:22 - overview of Lang chain and got you
15:24 - excited about using it in Python if you
15:26 - enjoyed make sure to leave a like
15:27 - subscribe to the channel and I will see
15:29 - you in the next
15:30 - [Music]
15:37 - one