00:00 - if you've ever built any kind of llm or
00:02 - gen type application chances are you've
00:04 - got it working locally and eventually
00:06 - you're going to want to share this with
00:08 - other people or even push this into
00:09 - production now once you set this up
00:11 - locally there's all kinds of
00:13 - dependencies and requirements and trying
00:15 - to show someone else how to do that or
00:17 - get it set up in the same way on another
00:18 - computer can be a massive nightmare
00:21 - especially if you want to go into
00:22 - production so in this video I'm going to
00:24 - show you a solution to that which is
00:26 - using Docker to containerize your gen
00:28 - applications so that all of the
00:30 - dependencies are saved and you can
00:31 - really easily run your apps no matter
00:33 - what your Hardware configuration is this
00:36 - is a really cool video it's going to be
00:37 - really useful for any of you that work
00:39 - with Gen apps and have followed even
00:41 - some of the other tutorials I have on
00:43 - this channel where we're building things
00:44 - like gen chap Bots rag applications Etc
00:48 - so with that said let's get into it and
00:50 - learn how we can use Docker to deploy
00:52 - and containerize our gen apps so let's
00:55 - dive in here and I want to start by
00:56 - thanking Docker for partnering with me
00:58 - on this video they been really helpful
01:00 - in creating the video and providing a
01:02 - lot of documentation and resources so
01:04 - that I can really explain this to you in
01:05 - the best possible way now what we're
01:08 - going to start by looking at is
01:09 - something known as the docker gen AI
01:11 - stack now this is a collaboration
01:13 - between Docker neo4j Lang chain and AMA
01:17 - this is a way that you can actually
01:19 - deploy a gen application with just a few
01:22 - clicks and really minimal configuration
01:24 - which we couldn't say before because
01:26 - typically it takes a lot of different
01:28 - dependencies a lot of setup and and it
01:30 - was a huge headache to actually get
01:31 - these gen apps out into production with
01:34 - Docker now we can containerize the
01:36 - various different components of our
01:37 - application I'm going to show you later
01:39 - in the video exactly how we do that
01:41 - starting completely from scratch now the
01:43 - Gen stack as I said here uses olama for
01:47 - management so for the various llms this
01:49 - means you can actually run your llms
01:51 - locally if you're not familiar with AMA
01:53 - something again that allows you to run
01:54 - llms locally on your computer rather
01:56 - than relying on something like Chad GPT
01:59 - or GPT for next it uses neo4j this is a
02:02 - database and also a knowledge graph for
02:04 - the llms again this can run completely
02:07 - locally on your own computer and then we
02:09 - use Lang chain for the orchestration and
02:12 - then this uses Docker of course for
02:13 - doing all of the containerization
02:15 - deployment Etc now what I'm going to do
02:17 - is go over here to this Docker gen stack
02:20 - repo this is something I'll link in the
02:22 - description if you want to check out for
02:24 - yourself but this just demonstrates to
02:26 - you how you can run all of these
02:28 - different services on your own computer
02:29 - computer with really minimal
02:31 - configuration so this is a repository
02:34 - that has olama configured neo4j
02:36 - configured and actually just serves you
02:38 - a few different gen applications so it
02:40 - has a support bot stack Overflow loader
02:43 - PDF reader Etc and the purpose of this
02:46 - repository at least why I'm showing it
02:47 - to you is that you can see with just one
02:49 - or two commands you can actually get all
02:51 - of this running on your machine without
02:53 - having to do a ton of tedious setup so
02:56 - what I've done to demonstrate that to
02:57 - you is on the right hand side or on my V
02:59 - s code window here I've just cloned this
03:02 - repository the Gen stack now all I've
03:05 - done is just create an environment
03:06 - variable file here based off the example
03:09 - that they had and I simply ran the
03:11 - command Docker compose up now this just
03:13 - uses the docker compose file that's
03:15 - provided in this repository which spins
03:18 - up all of the various different services
03:20 - or containers that we need to run this
03:22 - application now I know it looks a little
03:23 - bit complicated but don't worry I'm
03:25 - going to show you again a much simpler
03:27 - application in a second and how we do
03:29 - this completely from scratch this is
03:30 - just an example of something that you
03:32 - could do again how fast it is so what
03:34 - I'm able to do now is view a few
03:36 - different applications on my computer so
03:38 - you can see that I have a support bot a
03:40 - stack Overflow loader PDF reader so what
03:42 - I'll do is just go to one of the
03:44 - applications here this is the stack
03:46 - Overflow loader again notice running
03:48 - local host on my computer and this
03:50 - allows us to load in various questions
03:51 - and answers from stack Overflow that
03:54 - match a specific tag so what I have done
03:56 - previously is I just loaded in Python
03:58 - list I'm not going to do this again
04:00 - because I already loaded it and this now
04:02 - allows me to go to the main bot UI which
04:04 - is just served on a different port and I
04:06 - can now ask questions um about the
04:09 - various stack Overflow questions so I
04:11 - can say hey what is an index error in
04:17 - Python and now it's going to go and look
04:19 - up that information for me and give me a
04:21 - response so you can see here I got a
04:22 - reply and it's showing me the various
04:24 - different errors from stack Overflow
04:26 - that are relevant to this question using
04:28 - something known as rag which is
04:29 - retrieval augmented generation the
04:31 - important thing here is all of this is
04:33 - running on my own machine and again I
04:35 - didn't really need to set anything up
04:36 - for this to work now another application
04:39 - that this provides just as a demo is
04:40 - something that allows you to chat with
04:42 - your PDFs so I'm going to upload a PDF
04:44 - here and then I can ask questions
04:45 - directly about that so I just uploaded a
04:47 - PDF here which is just my receipt for a
04:49 - QuickBook subscription payment this
04:51 - should be for about $60 so I'm just
04:53 - going to ask the llm how much was the
04:55 - payment for and it should be able to
04:57 - actually use this PDF and give me the
04:59 - reply there here you go you can see the
05:00 - amount was $62 obviously I could ask
05:02 - this something that's a lot more
05:03 - complicated but just a demo of how this
05:05 - works so that's a quick demo of the geni
05:08 - stack but I want to quickly walk you
05:09 - through a few other really cool features
05:11 - that Docker has released recently that
05:13 - are really useful and you guys can find
05:15 - some value from now the first feature is
05:17 - profiles now if you've ever worked with
05:19 - some complicated repositories before you
05:22 - may have actually ridden multiple Docker
05:24 - files or Docker compos files for the
05:27 - different types of configurations or
05:28 - different type of hard where you might
05:30 - be running on a good example of this is
05:32 - running on a CPU versus running on a GPU
05:35 - now Dockers come up with a solution for
05:36 - this you don't need to repeat a ton of
05:38 - code and they've introduced profiles now
05:41 - this allows you to actually just specify
05:42 - a flag while you're running the docker
05:45 - compos file to indicate what profile you
05:47 - want to run with and you can do some
05:49 - different configurations or setup based
05:51 - on that profile so in this case you see
05:53 - that we have Linux and then we have
05:54 - Linux GPU so if we want to run using an
05:57 - Nvidia GPU which is specified right here
05:59 - which you would want to be doing
06:01 - especially if you push this into
06:02 - production then you would simply utilize
06:04 - this profile however in development
06:06 - maybe you're working on a machine that
06:08 - doesn't have a GPU like I am right now
06:10 - or at least with a non-compatible GPU
06:13 - and in that case you can just use the
06:14 - default profile Linux so if you want to
06:16 - actually run the profile command I'll
06:18 - link some documentation to you in the
06:20 - description but you can see it's as
06:22 - simple as just specifying what profile
06:24 - you want to run and now all of a sudden
06:26 - you can change the configuration and the
06:27 - way the Dockers is going to work with
06:29 - just a single change to your Docker
06:31 - compose command so that's the profile
06:34 - feature but another cool feature you'll
06:35 - find useful is the watch feature now if
06:37 - I scroll down here in the docker compos
06:39 - file you can see that we have a section
06:41 - labeled X develop and then we have watch
06:44 - now this is another new feature that
06:45 - Docker has added that allows us to
06:47 - actually watch specific files and then
06:50 - rebuild certain containers whenever any
06:52 - of the contents in those files change
06:54 - now this is really useful because before
06:56 - you would have to manage which
06:57 - containers you were running yourself and
06:59 - you'd have to know which ones to restart
07:01 - based on what you were changing while
07:03 - you were developing certain parts of
07:04 - your application in this case you simply
07:06 - write in the watch which is done right
07:08 - here and now if any changes are applied
07:10 - to boty PDF bot Etc we can automatically
07:14 - rebuild this container without having to
07:16 - rebuild all of the other containers now
07:18 - we can do this in all of the containers
07:20 - for example if we go down here you can
07:21 - see there's a different watch and all we
07:23 - have to do in order to actually utilize
07:25 - this feature is just run a slightly
07:27 - different command so I'm just going to
07:28 - stop this here here and I'll show you
07:30 - the command that we can run so there's a
07:32 - few different commands here but I can go
07:33 - with Docker compose watch and when I do
07:36 - that it's now going to use these watch
07:37 - fields that we've declared and if I make
07:40 - any changes to these files we'll
07:42 - automatically rebuild that container or
07:44 - whatever it says here we can sync it we
07:46 - can rebuild it there's a few different
07:47 - options so hopefully you just learned a
07:49 - bit about the Gen stack and some useful
07:51 - Docker features you can take advantage
07:53 - of now I know a lot of you will be
07:54 - asking now okay that's great I
07:56 - understand geni stack but how do I
07:58 - actually do this for my own gen
08:00 - application that's why I'm going to show
08:02 - you now how you can toriz your own gen
08:05 - app so without having the docker files
08:07 - created how do you actually go through
08:08 - this process now there's some great
08:10 - documentation on the docker website
08:12 - which I'm going to link in the
08:13 - description and I'm just going to follow
08:15 - along with it and kind of walk you
08:16 - through step by step so you understand
08:18 - what we need to do in order to get this
08:20 - running so click on this link in the
08:22 - description if you want to follow along
08:23 - with me and what I'm going to do is copy
08:25 - this sample repository which has a very
08:27 - simple llm app built uses streamlit for
08:30 - the UI AMA and then neo4j it also uses
08:34 - Lang chain now this is a python based
08:36 - app which I'm sure a lot of you will
08:37 - probably be building so what I'm going
08:39 - to begin by doing is just cloning this
08:40 - repo so let's just paste this command in
08:43 - and then once that's done we can go
08:44 - inside of here and we can start
08:46 - initializing kind of our Docker files
08:48 - the compose file the docker file Etc and
08:51 - I'll show you a really unique command
08:52 - that can do that almost instantly for us
08:54 - so the repo is cloned now and what I'm
08:56 - going to do is just CD into it and
08:58 - you'll notice here if I click into it on
08:59 - the side that we just have some basic
09:01 - python files an environment variable
09:03 - file Etc we don't have anything we need
09:05 - to actually kind of start this with
09:07 - Docker and deploy it with Docker now
09:09 - fortunately for us there's a really
09:10 - useful command called Docker AIT that we
09:14 - can run that will actually create all of
09:16 - the necessary files for us now in order
09:18 - for that command to work we are going to
09:20 - need Docker desktop installed on our
09:22 - machine so what you can do is simply go
09:24 - to Docker desktop you can find the
09:27 - correct download link whether it's on
09:28 - Mac Linux or Windows and just make sure
09:31 - you have that on your machine before you
09:32 - start actually trying to initialize this
09:35 - command you also want to make sure that
09:36 - you're running Docker desktop so if
09:38 - you're on Mac for example you can just
09:40 - open up Docker and then as soon as you
09:41 - open it it should actually start the
09:43 - docker engine if you're on Windows or
09:44 - Linux same thing you just have to open
09:46 - Docker desktop and then it should run
09:48 - everything for you and you should be
09:49 - able to initialize this command so I'm
09:51 - going to run Docker and net from inside
09:53 - of this repository and what this will do
09:55 - is go through kind of some basic
09:57 - configuration for me and allow me to
09:59 - configure my app so I'm going to say
10:01 - that this is for a python application
10:03 - which has been automatically detected
10:04 - it's going to ask us what version we
10:06 - want to use I'm just going to go with
10:07 - the newest version here the port I'm
10:09 - going to go with the default port and
10:11 - for the running command this is what
10:12 - you'll actually use to enter your
10:14 - application obviously this will differ
10:16 - depending on the type of app that you
10:18 - have now in our case we're just going to
10:19 - go back to the docs here and you'll see
10:22 - that there is a command that we can copy
10:24 - so it's right here streamlit run app and
10:27 - that's because we're using streamlit for
10:28 - our UI
10:29 - so that's the command we're going to use
10:31 - so let's paste that in here and it just
10:33 - specifies the server address and the
10:35 - port okay so now that we've done that
10:37 - it's actually created the necessary
10:39 - files for us so we have the docker
10:40 - ignore file we have the composed. yaml
10:43 - file which is where our various services
10:44 - are going to be defined we have the
10:46 - docker file which is usually a huge pain
10:48 - to write but now it's just completely
10:50 - done for us and we even have a readme
10:52 - doer. MD file so that's the docker and
10:55 - niit command super useful for getting up
10:57 - and running really fast and now if we
10:59 - just make a few really minor changes we
11:01 - can actually run this application on our
11:03 - local computer so actually I lied all we
11:06 - need to do is just type the command
11:07 - Docker compose up and then D- build what
11:11 - this is going to do is use the docker
11:12 - compose file which is already created
11:14 - for us it's going to spin up the
11:16 - container that we need which will be
11:17 - running our application and even though
11:19 - this hasn't containerized everything for
11:21 - us yet it's just showing you kind of
11:23 - baby steps in terms of getting the
11:24 - application running so this here will
11:27 - just actually create a container for the
11:28 - user interace or the server so for kind
11:30 - of like the streamlit application it's
11:32 - not going to containerize Ama or the
11:35 - database or some other things that we
11:37 - need if we want everything to be running
11:39 - completely locally but you'll see what I
11:40 - mean in just one second once this gets
11:42 - loaded so you can see after running this
11:44 - command the streamlet application now is
11:46 - running within the docker container so
11:48 - I'll bring it up for you and you can see
11:49 - what it looks like so this application
11:51 - you see here is actually the chat with
11:53 - PDF application that we looked at
11:55 - previously now the reason why it's
11:57 - giving us these fields here and saying
11:58 - we need neo4j urri username password and
12:02 - we need one of our llm models either the
12:04 - AMA base URL or the open AI API key is
12:07 - because we haven't yet containerized
12:09 - those and made those services that are
12:10 - accessible to this app now we're going
12:12 - to do that in just a second so you can
12:14 - see how all of this runs completely
12:16 - locally but at this point if you wanted
12:18 - to you could connect to a remote
12:19 - database you could connect to open Ai
12:21 - and utilize something like gp4 and you
12:23 - can have this app running on your own
12:25 - computer you're just going to be
12:26 - Outsourcing some of the components and
12:28 - not entirely using the docker
12:30 - containerization but what I want to do
12:32 - is I want to go and actually
12:33 - containerize the rest of our components
12:35 - so you can see what that looks like and
12:37 - how we do the deployment completely
12:39 - locally so what I'm going to do now is
12:41 - stop this application by hitting contrl
12:43 - C then I'm going to shut down the
12:45 - containers by doing Docker compos down
12:47 - and then what we're going to do is
12:49 - containerize the other services like the
12:51 - database and olama so that we can run
12:53 - all of this without relying on some
12:55 - third party dependency so I'm going to
12:56 - say Docker compos down just to remove
12:59 - these different containers that we don't
13:00 - need and now what we're going to do is
13:02 - go back to the documentation and we're
13:04 - going to follow along with the next page
13:06 - so if you scroll down to the bottom of
13:08 - the docs you can press develop your
13:09 - application that's going to bring you to
13:11 - the next step which will bring you to
13:12 - this page right here and this is going
13:14 - to show you how you add a local database
13:16 - and how you add a local or remote llm
13:18 - service which is exactly what we want to
13:19 - do now to start with the local database
13:22 - what we need to do is we need to change
13:24 - the env. example file to be EnV now this
13:28 - is going to provide some of the
13:29 - environment variables we need for the
13:31 - database so what we'll do is go over
13:34 - here to our repository and where it says
13:36 - env. example I'm just going to rename
13:38 - this to say
13:40 - EnV and if we go in here you can see
13:43 - that we're going to have a few different
13:44 - configuration options so we have the URI
13:46 - the username the password the olama base
13:48 - URL we'll change some of these in a
13:50 - second but at least now we have the
13:51 - environment variable file declared in
13:53 - the repository now we're going to go
13:55 - back and let's see what it says next it
13:57 - says okay if we want to add this
13:59 - database then we need to copy the
14:01 - following so let's copy all of this
14:03 - stuff right here and let's go over to
14:05 - the docker compose file so composed.
14:07 - yaml and let's simply paste this in okay
14:10 - so that's pasted you can see that we
14:12 - have the environment variable file it's
14:14 - depending on this database being healthy
14:16 - and then we have some information for
14:18 - the database and notice that we're using
14:20 - those various uh variables from our
14:22 - environment variable file you don't have
14:24 - to worry too much about exactly what I'm
14:25 - copying here but this is going to create
14:27 - the database service for us and run it
14:29 - all locally so let's save that file and
14:32 - go back to the docs and see what we need
14:34 - to do next so we can run this app now
14:37 - and if we do that we should see that
14:38 - it's no longer going to ask us for the
14:40 - database information or at least it will
14:42 - already be connected and then the only
14:44 - thing we need next is the llm so I'm
14:46 - just going to rerun this with Docker
14:48 - compose up-- build so now you'll see
14:51 - that neo4j is running which means the
14:53 - database has been created and if we want
14:55 - we can go back to the application and we
14:57 - can see the state of it now how however
14:59 - what I'm going to do is go to the next
15:00 - step in the documentation which is going
15:02 - to be adding the llm now I want to have
15:05 - a local llm here which means I want to
15:07 - use AMA so let me explain to you how we
15:09 - do that now in order for AMA to work
15:12 - within our Docker container we're going
15:13 - to need it running on our local computer
15:15 - that's because a llama will have some
15:17 - really large models and we wouldn't want
15:19 - those models to be installed every time
15:21 - we spin up the container instead we want
15:23 - to have it on our local computer and
15:25 - then just access the server that's
15:27 - running on our local computer that's
15:28 - kind of serving these llms so download
15:31 - ama if you don't already have it and
15:33 - then what we're going to do is open up
15:34 - our terminal and we're going to run the
15:36 - AMA command to pull the model we're
15:38 - going to use so we're just going to say
15:39 - ol llama and then pull and then llama 2
15:43 - now I already have this so it ran
15:44 - extremely quickly but you can see it's
15:46 - about 3.8 GB that it needs to download
15:49 - so this will take a second if you don't
15:50 - already have the model again in order
15:52 - for this to work you're going to need to
15:54 - install AMA on your computer go to the
15:56 - terminal type AMA pull and then llama 2
15:59 - or whatever model it is that you want to
16:01 - use and then we'll be able to utilize
16:03 - this model from our Docker container now
16:05 - one other note here if you are on
16:07 - Windows you might run into some errors
16:09 - so I'm going to leave a link in the
16:10 - description that discusses how you can
16:12 - run this with WSL and you just need to
16:14 - make sure that you have Docker desktop
16:16 - enabled with WSL when you're running
16:18 - these various containers again I'll link
16:20 - this in the description just in case you
16:22 - have any issues but it kind of discusses
16:23 - how you get a llama running on Windows
16:26 - because it sometimes can be a little bit
16:27 - different than working with Mac or Linux
16:30 - okay so now we're going to go back to
16:32 - the documentation here and we're going
16:33 - to go to where it says add a local or
16:35 - remote llm service now you can see that
16:37 - we can run AMA in a container we can run
16:39 - AMA outside of a container or we can use
16:41 - open AI so we're going to go down here
16:43 - and we're actually just going to click
16:44 - on run olama outside of a container and
16:47 - we're going to use the AMA service
16:49 - that's running on our local machines
16:51 - this say install and run olama on your
16:52 - host machine which we are doing and then
16:54 - update the olama base URL value in your
16:57 - EnV file to be the following so so let's
16:59 - copy this value here let's go to our
17:02 - environment variable file which I have
17:03 - open and let's update the olama base URL
17:07 - now while we're here we're also going to
17:09 - change the sentence Transformer to be
17:10 - llama 2 just so we're using the Llama 2
17:13 - model as well for our embeddings okay
17:16 - let's go back and see if there's
17:17 - anything else that we need to do it says
17:18 - pull the model to a llama using the
17:20 - fullone command we already did that so
17:22 - if you didn't do that go ahead and run
17:23 - it but in my case I did and now we
17:25 - should be able to run our gen
17:27 - application and we should see that all
17:29 - of our services are running locally so
17:31 - let's go back here I'm just going to get
17:33 - out of whatever was running before we'll
17:36 - rerun this and then hopefully our
17:37 - application will be working fine okay so
17:39 - I'm just going to paste in the command
17:40 - here Docker compose up and then D- build
17:43 - let's give that a second to run and then
17:45 - let's look at our finished app so the
17:47 - app is running now and what I can do
17:48 - again is upload a PDF so let's go with
17:50 - the same one as before and then chat
17:52 - with it so I'm just going to ask it what
17:54 - is the date and amount for this invoice
17:56 - and let's see if it can get that result
17:58 - all right so there you go go June 24th
17:59 - 2024 the amount is for $62 obviously we
18:03 - can chat with this more if we want but I
18:05 - think that proves the point that this is
18:06 - indeed working so there you go that is
18:09 - how you deploy and containerize a gen
18:12 - application using Docker now the last
18:14 - thing I want to show you here is a
18:16 - really interesting feature that can save
18:18 - you a ton of time when you actually are
18:19 - going into production and that's
18:21 - something known as Docker Scout now in
18:24 - order to use this feature we need to
18:25 - open up Docker desktop which I already
18:27 - have right here and we're going to go
18:29 - into our images here and you can see
18:30 - that what we can actually do is we can
18:32 - click on one of them so let's go to the
18:34 - Gen stack loader for example and right
18:37 - away it's going to open up this
18:38 - vulnerabilities tab now this is a way
18:41 - that we can actually detect all of the
18:43 - known vulnerabilities in the different
18:44 - images that we're using as well as the
18:46 - various dependencies and not only that
18:49 - we can actually see the solution or the
18:51 - version we should go to to fix these
18:53 - vulnerabilities so you can see on the
18:55 - right hand side of my screen here that
18:56 - actually shows me various different
18:58 - packages which are dependencies for this
19:00 - image and it shows me all of the
19:02 - different vulnerabilities that they have
19:04 - ranking by critical high medium or low
19:06 - priority now I can actually sort by
19:08 - fixable packages if I just want to see
19:10 - the ones that do actually have a fix but
19:12 - if you want you can click in you can see
19:14 - exactly where the vulnerabilities are
19:17 - and then it actually gives you a
19:18 - description of what it is and shows you
19:20 - if applicable what the fixed version is
19:22 - now in this case there's no fix for this
19:24 - one but if we go to this package here
19:26 - cryptography we can go to this one and
19:29 - you can see that will tell us that the
19:30 - affected range is this and if we go up
19:32 - to 42.0 point4 then that'll actually fix
19:35 - this vulnerability so it's super fast
19:37 - and very easy to actually see the fix
19:39 - which is the useful component of this
19:41 - because it's one thing to know the
19:42 - vulnerabilities it's another thing to
19:44 - really quickly be able to find what the
19:45 - solution is now this Docker Scout
19:47 - feature works by analyzing all of the
19:49 - different images and dependencies that
19:51 - you have and then matching it against an
19:53 - inventory of ever updating
19:55 - vulnerabilities so it already has all of
19:57 - the fixes all of the vulnerabilities
19:59 - they pull that from so many different
20:01 - places and they take all of your images
20:03 - and just match it against that which
20:04 - allows them to really quickly give you
20:06 - all of these different recommendations
20:08 - and show you where the vulnerabilities
20:09 - are a super useful thing and definitely
20:12 - something you want to check out
20:13 - especially if you're going to go into
20:14 - production and it's just built in here
20:16 - right to Docker desktop so with that
20:18 - said guys I am going to wrap up the
20:19 - video here all of the relevant
20:21 - documentation for this will be linked in
20:23 - the description if you have any
20:24 - questions leave them in the comments
20:26 - down below I hope you found this helpful
20:28 - and I look forward to seeing you in
20:29 - another video
20:30 - [Music]