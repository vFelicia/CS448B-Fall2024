hey this is anthony tavelos your cloud instructor at exam pro bringing you a complete study course for the google cloud associate cloud engineer made available to you here on free code camp and so this course is designed to help you pass and achieve google issued certification the way we're going to do that is to go through lots of lecture content follow alongs and using my cheat sheets on the day of the exam so you pass and you can take that certification and put it on your resume or linkedin so you can get that cloud job or promotion that you've been looking for and so a bit about me is that i have 18 years industry experience seven of it specializing in cloud and four years of that as a cloud trainer i previously been a cloud and devops engineer and i've also published multiple cloud courses and i'm a huge fan of the cartoon looney tunes as well as a coffee connoisseur and so i wanted to take a moment to thank viewers like you because you make these free courses possible and so if you're looking for more ways of supporting more free courses just like this one the best way is to buy the extra study material at co example.com in particular for this certification you can find it at gcp hyphen ace there you can get study notes flash cards quizlets downloadable lectures which are the slides to all the lecture videos downloadable cheat sheets which by the way are free if you just go sign up practice exams and you can also ask questions and get learning support and if you want to keep up to date with new courses i'm working on the best way is to follow me on twitter at antony's cloud and i'd love to hear from you if you passed your exam and also i'd love to hear on what you'd like to see next welcome back in this lesson i wanted to quickly go over how to access the course resources now the resources in this course are designed to accompany the lessons and help you understand not just the theory but to help with the demo lessons that really drive home the component of handson learning these will include study notes lesson files scripts as well as resources that are used in the demo lessons these files can be found in a github repository that i will be including below that are always kept uptodate and it is through these files that you will be able to follow along and complete the demos on your own to really cement the knowledge learned it's a fairly simple process but varies through the different operating systems i'll be going through this demo to show you how to obtain access through the three major operating systems being windows mac os and ubuntu linux so i'm first going to begin with windows and the first step would be to open up the web browser and browse to this url which i will include in the notes below and this is the course github repository which will house all the course files that i have mentioned before keeping the course up to date will mean that files may need to be changed and so as i update them they will always be reflected and uploaded here in the repo so getting back to it there are two ways to access this repository so the easiest way to obtain a copy of these files will be to click on the clone or download button and click on download zip once the file has been downloaded you can then open it up by clicking on it here and here are the files here in downloads and this will give you a snapshot of all the files and folders as you see them from this repository now although this may seem like the simple way to go this is not the recommended method to download as if any files have changed you will not be up to date with the latest files and will only be current from the date at which you've downloaded them now the way that is recommended is using a source control system called git and so the easiest way to install it would be to go to this url https colon forward slash forward slash git dash scm.com and this will bring you to the git website where you can download the necessary software for windows or any other supported operating system and so i'm going to download it here and this should download the latest version of git for windows and it took a few seconds there but it is done and no need to worry about whether or not you've got the proper version usually when you click that download button it will download the latest version for your operating system so i'm going to go over here and open this up you'll get a prompt where you would just say yes and we're going to go ahead and accept all the defaults here this is where it's going to install it let's hit next these are all the components that they're going to be installed let's click on next and again we're going to go through everything with all the defaults and once we've reached installing all the defaults it's gonna take a couple minutes to install and again it took a minute or so we're going to just click on next and it's going to ask if you want to view the release notes and we don't really need those so we can click on ok and simply close that and we're just going to go over and see if git is installed we're going to run the command prompt and i'm going to just zoom in here so we can see a little better and there we go and we are just going to type in git and as you can see it's been installed and so now that we've installed git we want to be able to pull down all the folders and the files within them from the repository to our local system and so i'm just going to clear the screen here and we're going to do a cd to make sure that i'm in my home directory and then we're going to make a directory called repos and in order to do that we're going to do mkdir space repos and then we're going to move into that directory so cd space repos and so again here we want to clone those files that are in the repository to our local system so in order to do that we're going to use the command git clone so get space clone and then we're going to need our location of the git repository so let's go back to the browser and we're going to go over here to clone or download and here you will see clone with https so make sure that this says https and you can simply click on this button which will copy this to the clipboard and then we'll move back to our command prompt and paste that in and once that's pasted just hit enter and it will clone your repository into the repos directory and so just to verify that we've cloned all the necessary files we're going to cd into the master directory that we had just cloned and we're going to do a dir and there you have it all of the files are cloned exactly as it is here in the repository now just as a note in order to keep these files up to date we need to run a different command which would be a git pull and this can be run at any time in order to pull down any files or folders that have been updated since you did the first pull which in this case would be cloning of the repository again this will provide you with the latest and most uptodate files at any given moment in time and in this case since nothing has changed i have been prompted with a message stating that i'm up to date if nothing is changed you will always be prompted with this message if there was it will pull your changes down to your synced local copy and the process for windows is completed and is similar in mac os and i'll move over to my mac os virtual machine and log in and once you've logged in just going to go over here to the terminal and i'm just going to cd to make sure i'm in my home directory then i'm going to do exactly what we did in windows so i'm going to run the command mk dir space repos and create the repos directory and i'm going to move in to the repos directory and then i'm going to run git now for those of you who do not have get installed you will be prompted with this message to install it and you can go ahead and just install you'll be prompted with this license agreement you can just hit agree and depending on your internet connection this will take a few minutes to download and install so as this is going to take a few minutes i'm going to pause the video here and come back when it's finished installing okay and the software was successfully installed so just to do a double check i'm going to run git and as you can see it's been installed so now that we have git installed we want to clone all the directories and the files from the github repository to our local repos folder so i'm going to open up my browser and i'm going to paste my github repository url right here and you'll see the clone button over here so we're going to click on this button and here we can download zip but like i said we're not going to be doing that we're going to go over here and copy this url for the github repository again make sure it says https and we're going to copy this to our clipboard and we're going to go back to our terminal and we are going to run the command git space clone and we're going to paste in our url and as you can see here i've cloned the repository and all the files and folders within it and so as is my best practice i always like to verify that the files have been properly cloned and so i'm going to run the command ls just to make sure and go into the master directory and do a double check and as you can see the clone was successful as all the files and folders are here and again to download any updates to any files or directories we can simply run the command git space poll and because we've already cloned it it's already up to date and so the process is going to be extremely similar on linux so i'm going to simply move over to my linux machine and log in i'm going to open up a terminal and i'm going to make my terminal a little bit bigger for better viewing and so like the other operating systems i want to clone all the files and directories from the github repository to my machine and so i'm going to cd here to make sure i'm in my home directory and like we did before we want to create a directory called repos so i'm going to run the command mkdir space repos and we're going to create the repos directory we're now going to move into the repos directory and here we're going to run the git command and because git is not installed on my machine i've been prompted with the command in order to install it so i'm going to run that now so the command is sudo space apt space install space get and i'm going to enter in my password and install it and just to verify i'm going to run the command git and i can see here it's been installed so now i'm going to go over here to my browser and i'm going to paste in the url to my repository and over here we'll have the same clone button and when i click on it i can get the url for the github repository in order to clone it again make sure before you clone that this says https if it doesn't say https you'll have the option of clicking on a button that will allow you to do so once it says https then you can simply copy this url to your clipboard by clicking on the button and then move over back to the terminal and we are going to clone this repository by typing in the get space clone command along with the url of the repository and when we hit enter it'll clone it right down to our directory so i'm just going to move into the master directory just to verify that the files are there and again they're all here so again if you're looking to update your repository with any new updated changes you can simply run the get space pull command to update those files and so that's the linux setup so you have a local copy of the lesson files now there's just one more thing that i highly recommend you do and to demonstrate it i'm going to move back over to my windows virtual machine now i'm going to open up the web browser again open up a new tab and i'm going to browse to this url https colon forward slash forward slash code.visualstudio.com and i'll make sure that the url is in the text below there is a version of this code editor available for windows mac os and linux you can simply click on this drop down and you'll find the link to download it for your operating system but in most cases it should automatically show the correct version so just go ahead and click on download and it should start downloading automatically and you should be able to run it right away now the reason behind me asking you to install this utility is for editing code of different sorts whether you're adjusting yaml or python documents for deployment manager or even managing scripts a code editor will give you the ease of use when it comes to managing editing and even syntactical highlighting of code as shown here below it will highlight the code to make it easier to understand now if you have your own editor that you would prefer to use go ahead and use that but for those that don't my recommendation will be to use visual studio code so to install visual studio code we're just going to accept this license agreement and then we're going to click on next and we're just going to follow all the defaults to install it it's going to take a minute or two and for those running windows you want to make sure that this box is checked off so that you can launch it right away let's hit finish another recommendation would be to go over here to the task bar so you can pin it in place so that it's easier to find and so now you have access to all the resources that's needed for this course but with that that's everything that i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i wanted to discuss the various certifications available for google cloud as this number keeps on growing and i am looking to keep this lesson as up to date as possible so with that being said let's dive in now google cloud has released a slew of certifications in many different areas of expertise as well as different experience levels now there are two levels of difficulty when it comes to the google cloud certifications starting off with the associate level we see that there is only the one certification which is the cloud engineer the associate level certification is focused on the fundamental skills of deploying monitoring and maintaining projects on google cloud this is a great starting point for those completely new to cloud and google recommends the associate cloud engineer as the starting point to undergoing your certification journey this was google cloud's very first certification and to me was the entry point of wanting to learn more as an engineer in cloud in my personal opinion no matter your role this certification will cover the general knowledge that is needed to know about starting on google cloud and the services within it which is why i labeled it here as the foundational level course i also consider this the stepping stone into any other professional level certifications which also happens to be a recommended path by google with a great course and some dedication i truly believe that anyone with even a basic skill level in it should be able to achieve this associate level certification now it is recommended from google themselves that prior to taking this exam that you should have over six months experience building on google cloud for those of you with more of an advanced background in google cloud or even other public clouds this certification should be an easy pass as it covers the basics that you should be familiar with adding a google twist to it at the time of this lesson this exam is two hours long and the cost is 125 us dollars the exam is a total of 50 questions which consists of both multiple choice and multiple answer questions each of the questions contain three to four line questions with single line answers that by the time you finish this course you should have the confidence to identify the incorrect answers and be able to select the right answers without a hitch moving into the professional level certifications there are seven certifications that cover a variety of areas of specialty depending on your role you might want to take one or maybe several of these certifications to help you gain more knowledge in google cloud or if you love educating yourself and you're really loving your journey in gcp you will probably want to consider pursuing them all in my personal opinion the best entry point into the professional level would be the cloud architect it is a natural step up from the associate cloud engineer and it builds on top of what is learned through that certification with a more detailed and more thorough understanding of cloud architecture that is needed for any other certification there is some overlap from the cloud engineer which is why in my opinion doing this certification right after makes sense it also brings with it the ability to design develop and manage secure scalable and highly available dynamic solutions it is a much harder exam and goes into great depth on services available the professional cloud architect is a great primer for any other professional level certification and can be really helpful to solidify the learning that is needed in any other technical role i find it the most common path that many take who look to learn google cloud which is why i personally recommend it to them and at the time of this lesson it also holds the highest return on investment due to the highest average wage over any other current cloud certification in the market google recommends over three years of industry experience including one year on google cloud before attempting these exams with regards to the exams in the professional tier they are much harder than the associate level and at the time of this course is two hours long and the cost is 200 us dollars these exams are a total of 50 questions which consists of both multiple choice and multiple answer questions it's the same amount of questions with the same amount of time but it does feel much harder each of the questions contain four to five line questions with one to three line answers it's definitely not a walk in the park and will take some good concentration and detailed knowledge on google cloud to solidify a pass after completing the cloud architect certification depending on your role my suggestion would be to pursue the areas that interest you the most to make your journey more enjoyable for me at the time i took the security engineer out as i am a big fan of security and i knew that i would really enjoy the learning and make it more fun for me this is also a great certification for those who are looking to excel their cloud security knowledge on top of any other security certifications such as the security plus or cissp now others may be huge fans of networking or hold other networking certifications such as the ccna and so obtaining the network engineer certification might be more up your alley and give you a better understanding in cloud networking now if you're in the data space you might want to move into the data engineer exam as well as taking on the machine learning engineer exam to really get some deeper knowledge in the areas of big data machine learning and artificial intelligence on google cloud now i know that there are many that love devops me being one of them and really want to dig deeper and understand sre and so they end up tackling the cloud developer and cloud devops engineer certifications so the bottom line is whatever brings you joy in the area of your choosing start with that and move on to do the rest all the professional certifications are valuable but do remember that they are hard and need preparation for study last but not least is the collaboration engineer certification and this certification focuses on google's core cloudbased collaboration tools that are available in g suite or what is now known as google workspaces such as gmail drive hangouts docs and sheets now the professional level collaboration engineers certification dives into more advanced areas of g suite such as mail routing identity management and automation of it all using tools scripting and apis this certification is great for those looking to build their skill set as an administrator of these tools but gives very little knowledge of google cloud itself so before i move on there is one more certification that i wanted to cover that doesn't fall under the associate or professional certification levels and this is the google cloud certified fellow program now this is by far one of the hardest certifications to obtain as there are very few certified fellows at the time of recording this lesson it is even harder than the professional level certifications and this is due to the sheer level of competency with hybrid multicloud architectures using google cloud anthos google's recommended experience is over 10 years with a year of designing enterprise solutions with anthos then a fourstep process begins first step is to receive a certified fellow invitation from google and once you've received that invitation then you need to submit an application with some work samples that you've done showing google your competency in hybrid multicloud once that is done the third step is a series of technical handson labs that must be completed and is a qualifying assessment that must be passed in order to continue and after all that the last step is a panel interview done with google experts in order to assess your competency of designing hybrid and multicloud solutions with anthos so as you can see here this is a very difficult and highly involved certification process to achieve the title of certified fellow this is definitely not for the faint of heart but can distinguish yourself as a technical leader in anthos and a hybrid multicloud expert in your industry now i get asked many times whether or not certifications hold any value are they easy to get are they worth more than the paperwork that they're printed on and does it show that people really know how to use google cloud and my answer is always yes as the certifications hold benefits beyond just the certification itself and here's why targeting yourself for a certification gives you a milestone for learning something new with this new milestone it allows you to put together a study plan in order to achieve the necessary knowledge needed to not only pass the exam but the skills needed to progress in your everyday technical role this new knowledge helps keep your skills up to date therefore making you current instead of becoming a relic now having these uptodate skills will also help advance your career throughout my career in cloud i have always managed to get my foot in the door with various interviews due to my certifications it gave me the opportunity to shine in front of the interviewer while being able to confidently display my skills in cloud it also allowed me to land the jobs that i sought after as well as carve out the career path that i truly wanted on top of landing the jobs that i wanted i was able to achieve a higher salary due to the certifications i had i have doubled and tripled my salary since i first started in cloud all due to my certifications and i've known others that have obtained up to five times their salary because of their certifications now this was not just from achieving the certification to put on my resume and up on social media but from the knowledge gained through the process and of course i personally feel that having your skills constantly up to date advancing your career and getting the salary that you want keeps you motivated to not only get more certifications but continue the learning process i am and always have been a huge proponent of lifelong learning and as i always say when you continue learning you continue to grow so in short google cloud certifications are a great way to grow and so that about covers everything that i wanted to discuss in this lesson so you can now mark this lesson as complete and i'll see you in the next one welcome back and in this lesson i'm going to be talking about the fictitious organization called bow tie inc that i will be using throughout the course now while going through the architectures and demos in this course together i wanted to tie them to a real world situation so that the theory and practical examples are easy to understand tying it to a scenario is an easy way to do this as well it makes things a lot more fun so the scenario again that i will be using is based on bow tie ink so before we get started with the course i'd like to quickly run through the scenario and don't worry it's going to be very high level and i will keep it brief so bow tie ink is a bow tie manufacturing company that designs and manufactures bow ties within their own factories they also hold a few retail locations where they sell their bow ties as well as wholesale to other thai and men's fashion boutiques and department stores across the globe being in the fashion business they mainly deal with commerce security and big data sets bow tie inc is a global company and they are headquartered in montreal canada they employ about 300 people globally with a hundred of them being in sales alone to support both the brick and mortar stores and wholesale branches there are many different departments to the company that make it work such as instore staff i.t marketing for both instore and online sales manufacturing finance and more the types of employees that work in bow tie inc vary greatly due to the various departments and consists of many people such as sales for both instore and wholesale managers that run the stores and sewers that work in the manufacturing plant and many more that work in these various departments the business has both offices and brick and mortar stores in montreal london and los angeles now due to the thrifty mindset of management concentrating all their efforts on commerce and almost none in technical infrastructure has caused years of technical debt and is now a complete disaster within the brick and mortar location there contains two racks with a few servers and some networking equipment the global inventory of bow ties are updated upon sales in both stores and wholesale as well as new stock that has been manufactured from the factory there are pointofsale systems in each store or office location these systems are all connected to each other over a vpn connection in order to keep updates of the inventory fresh all office and store infrastructure are connected to each other and the montreal headquarters and the point of sale systems and kiosk systems are backed up to tape in the montreal headquarters as well and like i said before management is extremely thrifty but they have finally come to the realization that they need to start spending money on the technical infrastructure in order to scale so diving into a quick overview of exactly what the architecture looks like the head office is located in montreal canada it has its main database for the crm and pointofsale systems as well as holding the responsibility of housing the equipment for the tape backups the tapes are then taken off site within montreal by a thirdparty company for storage the company has two major offices one in london covering the eu and the other in the west coast us in los angeles these major offices are also retail locations that consume i.t services from the headquarters in montreal again being in the fashion business bowtie inc employs a large amount of sales people and the managers that support them these employees operate the pointofsale systems so we're constantly looking to have the website sales and the inventory updated at all times each salesperson has access to email and files for updated forecasts on various new bowtie designs most sales people communicate over a voice over ip phone and chat programs through their mobile phone the managers also manually look at inventory on what's been sold versus what's in stock to predict the sales for stores in upcoming weeks this will give manufacturing a head start to making more bow ties for future sales now whatever implementations that we discuss throughout this course we'll need to support the daytoday operations of the sales people and the managers and because of the different time zones in play the backend infrastructure needs to be available 24 hours a day seven days a week any downtime will impact updated inventory for both online sales as well as store sales at any given time now let's talk about the current problems that the business is facing most locations hold on premise hardware that is out of date and also out of warranty the business looked at extending this warranty but became very costly as well management is on the fence about whether to buy new onpremise hardware or just move to the cloud they were told that google cloud is the way to go when it comes to the retail space and so are open to suggestions yet still very weary now when it comes to performance there seems to be a major lag from the vpn connecting from store to store as well as the head office that's responsible for proper inventory thus slowing down the point of sale systems and to top it all off backups taking an exorbitant amount of time is consuming a lot of bandwidth with the current vpn connection now bowtie inc has always struggled with the lack of highly available systems and scalability due to cost of new hardware this is causing extreme stress for online ecommerce whenever a new marketing campaign is launched as the systems are unable to keep up with the demand looking at the forecast for the next two quarters the business is looking to open up more stores in the eu as well as in the us and with the current database in place providing very inefficient high availability or scalability there is a major threat of the main database going down now when it comes to assessing the backups the tape backups have become very slow especially backing up from london and the offsite storage costs continuously go up every year the backups are consuming a lot of bandwidth and are starting to become the major pain point for connection issues between locations on top of all these issues the small it staff that is employed have outdated i.t skills and so there is a lot of manual intervention that needs to be done to top it all off all the running around that is necessary to keep the outdated infrastructure alive management is also now pushing to open new stores to supply bow ties globally given the evergrowing demand as well as being able to supply the demand of bow ties online through their ecommerce store now these are some realistic yet common scenarios that come up in reality for a lot of businesses that are not using cloud computing and throughout the course we will dive into how google cloud can help ease the pain of these current ongoing issues now at a high level with what the business wants to achieve and what the favorable results are they are all interrelated issues so bowtie inc requires a reliable and stable connection between all the locations of the stores and offices so that sales inventory and pointofsale systems are quick and uptodate at all times this will also allow all staff in these locations to work a lot more efficiently with a stable and reliable connection in place backups should be able to run smoothly and also eliminate the cost of offsite backup not to mention the manpower and infrastructure involved to get the job done while scaling up offices and stores due to increase in demand the business should be able to deploy stores in new regions using pay as you go billing while also meeting the requirements and regulations when it comes to gpdr and pci this would also give the business flexibility of having a disaster recovery strategy in place in case there was a failure of the main database in montreal now as mentioned before the business is extremely thrifty especially when it comes to spend on it infrastructure and so the goal is to have the costs as low as possible yet having the flexibility of scaling up when needed especially when new marketing campaigns are launched during high demand sales periods this would also give bowtie inc the flexibility of analyzing sales ahead of time using realtime analytics and catering to exactly what the customer is demanding thus making inventory a lot more accurate and reducing costs in manufacturing items that end up going on sale and costing the company money in the end finally when it comes to people supporting infrastructure automation is key removing manual steps and a lot of the processes can reduce the amount of manpower needed to keep the infrastructure alive and especially will reduce downtime when disaster arises putting automation in place will also reduce the amount of tedious tasks that all departments have on their plate so that they can focus on more important business needs now that's the scenario at a high level i wanted to really emphasize that this is a typical type of scenario that you will face as a cloud engineer and a cloud architect the key to this scenario is the fact that there are areas that are lacking in detail and areas that are fully comprehensible and this will trigger knowing when and where to ask relevant questions especially in your daytoday role as an engineer it will allow you to fill the gaps so that you're able to figure out what services you will need and what type of architecture to use this is also extremely helpful when it comes to the exam as in the exam you will be faced with questions that pertain to real life scenarios that will test you in a similar manner knowing what services and architecture to use based on the information given will always give you the keys to the door with the right answer and lastly when it comes to the demos this scenario used throughout the course will help put things in perspective as we will come to resolve a lot of these common issues real world scenarios can give you a better perspective on learning as it is tied to something that makes it easy to comprehend and again bow tie inc is the scenario that i will be using throughout the course to help you grasp these concepts so that's all i have to cover this scenario so you can now mark this lesson as complete and let's move on to the next one hey this is anthony cevallos and what i wanted to show you here is where you can access the practice exam on the exam pro platform so once you've signed up for your account you can head on over to the course and you can scroll down to the bottom of the curriculum list and you will see the practice exams here at the bottom now just as a quick note you should generally not attempt the practice exam unless you have completed all the lecture content including the follow alongs as once you start to see those questions you will get an urge to start remembering these questions and so i always recommend to use the practice exam as a serious attempt and not just a way to get to the final exam at a faster pace taking your time with the course will allow you to really prevail through these practice exams and allow you for a way better pass rate on the final exam looking here we can see two practice exams with 50 questions each and so i wanted to take a moment here and dive into the practice exam and show you what some of these questions will look like and so clicking into one of these exams we can get right into it and so as you can see i've already started on practice exam one and so i'm going to click into that right now and as you can see the exam is always timed and in this case will be 120 minutes for this specific exam there are 50 questions for this practice exam and you will see the breakdown in the very beginning of the types of questions you will be asked now for the google cloud exams at the associate level they are usually structured in a common format they generally start with one or two lines of sentences which will typically represent a scenario followed by the question itself this question tends to be brief and to the point immediately following that you will be presented with a number of answers usually four or five in nature and can sometimes be very very technical as they are designed for engineers like asking about which gcloud commands to use to execute in a given scenario as well as theoretical questions that can deal with let's say best practices or questions about the specific services themselves now these answers will come in two different styles either multichoice or multiselect the multichoice is usually about identifying the correct answer from a group of incorrect or less correct answers whereas the multiselect will be about choosing multiple correct solutions to identify the answer as well for this associate exam the overall structure is pretty simple in nature and typically will be either right or wrong now sometimes these questions can get tricky where there are multiple possible answers and you will have to select the most suitable ones now although most of these types of questions usually show up in the professional exam they can sometimes peek their heads into the associate and so a great tactic that i always like to use is to immediately identify what matters in the question itself and then to start ruling out any of the answers that are wrong and this will allow you to answer the question a lot more quickly and efficiently as it will bring the more correct answer to the surface as well as making the answer a lot more obvious and making the entire question less complex so for instance with this question here you are immediately asked about google's recommended practices when it comes to using cloud storage as backup for disaster recovery and this would be for a specific storage type and so quickly looking at the answers you can see that standard storage and near line storage will not be part of the answer and so that will leave cold line storage or archive storage as the two possible choices for the answer of this question and so these are the typical techniques that i always like to use for these exams and so provided that you've gone through all the course content you will be able to answer these technical questions with ease and following the techniques i've just given and applying them to each question can really help you in not only this practice exam but for the final exam landing you a passing grade getting you certified welcome back and in this section i wanted to really hone in on the basics of cloud computing the characteristics that make it what it is the different types of computing and how they differ from each other as well as the types of service models now in this lesson i wanted to dive into the definition of cloud computing and the essential characteristics that define it now for some advanced folk watching this this may be a review and for others this may fulfill a better understanding on what is cloud now cloud is a term that is thrown around a lot these days yet holds a different definition or understanding to each and every individual you could probably ask 10 people on their definition of cloud and chances are everyone would have their own take on it many see cloud as this abstract thing in the sky where files and emails are stored but it's so much more than that now the true definition of it can be put in very simple terms and can be applied to any public cloud being google cloud aws and azure moving on to the definition cloud computing is the delivery of a shared pool of ondemand computing services over the public internet that can be rapidly provisioned and released with minimal management effort or service provider interaction these computing services consist of things like servers storage networking and databases they can be quickly provisioned and accessed from your local computer over an internet connection now coupled with this definition are five essential characteristics that define the cloud model that i would like to go over with you and i believe that it would hold massive benefits to understanding when speaking to cloud this information can be found in the white paper published by the national institute of standards and technology i will include a link to this publication in the lesson notes for your review now these essential characteristics are as follows the first one is ondemand selfservice and this can be defined as being able to provision resources automatically without requiring human interaction on the provider's end so in the end you will never need to call up or interact with the service provider in order to get resources provisioned for you as well you have the flexibility of being able to provision and deprovision these resources whenever you need them and at any given time of the day the second characteristic is broad network access now this simply means that cloud computing resources are available over the network and can be accessed by many different customer platforms such as mobile phones tablets or computers in other words cloud services are available over a network moving into the third is resource pooling so the provider's computing resources are pooled together to support a multitenant model that allows multiple customers to share the same applications or the same physical infrastructure while retaining privacy and security over their information this includes things like processing power memory storage and networking it's similar to people living in an apartment building sharing the same building infrastructure like power and water yet they still have their own apartments and privacy within that infrastructure this also creates a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but they may be able to specify location at a higher level of abstraction so in the end the customer does not really have the option of choosing exactly which server server rack or data center for that matter of where the provided resources are coming from they will only be able to have the option to choose things like regions or sections within that region the fourth essential characteristic is rapid elasticity this to me is the key factor of what makes cloud computing so great and so agile capabilities can be elastically provisioned and released in some cases automatically to scale rapidly outwards and inwards in response with demand to the consumer the capabilities available for provisioning often appear to be unlimited and can be provisioned in any quantity at any time and touching on the fifth and last characteristic cloud systems automatically control and optimize resource usage by leveraging a metering capability resource usage can be monitored controlled and reported providing transparency for both the provider and consumer of the service now what this means is that cloud computing resource usage is metered and you can pay accordingly for what you've used resource utilization can be optimized by leveraging payperuse capabilities and this means that cloud resource usage whether they are instances that are running cloud storage or bandwidth it all gets monitored measured and reported by the cloud service provider the cost model is based on pay for what you use and so the payment is based on the actual consumption by the customer so knowing these key characteristics of cloud computing along with their benefits i personally find can really give you a leg up on the exam as well as speaking to others in your daytoday role as more and more companies start moving to cloud i hope this lesson has explained to you on what is cloud computing and the benefits it provides so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i wanted to go over the four common cloud deployment models and distinguish the differences between public cloud multicloud private cloud and hybrid cloud deployment models this is a common subject that comes up a fair amount in the exam as well as a common theme in any organization moving to cloud knowing the distinctions between them can be critical to the types of architecture and services that you would use for the specific scenario you are given as well as being able to speak to the different types of deployment models as an engineer in the field getting back to the deployment models let's start with the public cloud model which we touched on a bit in our last lesson now the public cloud is defined as computing services offered by thirdparty providers over the public internet making them available to anyone who wants to use or purchase them so this means that google cloud will fall under this category as a public cloud there are also other vendors that fall under this category such as aws and azure so again public cloud is a cloud that is offered over the public internet now public clouds can also be connected and used together within a single environment for various use cases this cloud deployment model is called multicloud now a multicloud implementation can be extremely effective if architected in the right way one implementation that is an effective use of multicloud is when it is used for disaster recovery this is where your architecture would be replicated across the different public clouds in case one were to go down another could pick up the slack what drives many cases of a multicloud deployment is to prevent vendor lockin where you are locked into a particular cloud provider's infrastructure and unable to move due to the vendorspecific feature set the main downfall to this type of architecture is that the infrastructure of the public cloud that you're using cannot be fully utilized as each cloud vendor has their own proprietary resources that will only work in their specific infrastructure in other words in order to replicate the environment it needs to be the same within each cloud this removes each cloud's unique features which is what makes them so special and the resources so compelling so sometimes finding the right strategy can be tricky depending on the scenario now the next deployment model i wanted to touch on is private cloud private cloud refers to your architecture that exists on premise and restricted to the business itself with no public access yet it still carries the same five characteristics that we discussed with regards to what defines cloud each of the major cloud providers shown here all have their own flavor of private cloud that can be implemented on site google cloud has anthos aws has aws outposts and azures is azure stack they show the same characteristic and leverage similar technologies that can be found in the vendor's public cloud yet can be installed on your own onpremise infrastructure please be aware any organizations may have a vmware implementation which holds cloudlike features yet this is not considered a private cloud true private cloud will always meet the characteristics that make up cloud now it is possible to use private cloud with public cloud and this implementation is called hybrid cloud so hybrid cloud is when you are using public cloud in conjunction with private cloud as a single system a common architecture used is due to compliance where one cloud could help organizations achieve specific governance risk management and compliance regulations while the other cloud could take over the rest now i'd really like to make an important distinction here if your onpremise infrastructure is connected to public cloud this is not considered hybrid cloud this is what's known as hybrid environment or a hybrid network as the onpremises infrastructure holds no private cloud characteristics true hybrid cloud allows you to use the exact same interface and tooling as what's available in the public cloud so being aware of this can avoid a lot of confusion down the road so to sum up everything that we discussed when it comes to public cloud this is when one cloud provided by one vendor that is available over the public internet multicloud is two or more public clouds that are connected together to be used as a single system a private cloud is considered an onpremises cloud that follows the five characteristics of cloud and is restricted to the one organization with no accessibility to the public and finally hybrid cloud is private cloud connected to a public cloud and being used as a single environment again as a note onpremises architecture connected to public cloud is considered a hybrid environment and not hybrid cloud the distinction between the two are very different and should be observed carefully as gotchas may come up in both the exam and in your role as an engineer so these are all the different cloud deployment models which will help you distinguish on what type of architecture you will be using in any scenario that you are given and so this is all i wanted to cover when it comes to cloud deployment models so you can now mark this lesson as complete and let's move on to the next one welcome back so to finish up the nist definition of cloud computing i wanted to touch on cloud service models which is commonly referred to as zas now this model is usually called zas or xaas standing for anything as a service it includes all the services in a cloud that customers can consume and x can be changed to associate with the specific service so in order to describe the cloud service models i needed to touch on some concepts that you may or may not be familiar with this will make understanding the service models a little bit easier as i go through the course and describe the services available and how they relate to the model this lesson will make so much sense by the end it'll make the services in cloud easier to both describe and define now when it comes to deploying an application they are deployed in an infrastructure stack like the one you see here now a stack is a collection of needed infrastructure that the application needs to run on it is layered and each layer builds on top of the one previous to it to create what it is that you see here now as you can see at the top this is a traditional onpremises infrastructure stack that was typically used precloud now in this traditional model all the components are managed by the customer the purchasing of the data center and all the network and storage involved the physical servers the virtualization the licensing for the operating systems the staff that's needed to put it all together including racking stacking cabling physical security was also something that needed to be taken into consideration in other words for the organization to put this together by themselves they were looking at huge costs now the advantages to this is that it allowed for major flexibility as the organization is able to tune this any way they want to satisfy the application compliance standards basically anything that they wanted now when talking about the cloud service model concepts parts are always managed by you and parts are managed by the vendor now another concept i wanted to touch on is that unit of consumption is how the vendor prices what they are serving to their customer now just before cloud became big in the market there was a model where the data center was hosted for you so a vendor would come along and they would take care of everything with regards to the data center the racks the power to the racks the air conditioning the networking cables out of the building and even the physical security and so the unit of consumption here was the rack space within the data center so the vendor would charge you for the rack space and in turn they would take care of all the necessities within the data center now this is less flexible than the traditional onpremises model but the data center is abstracted for you so throughout this lesson i wanted to introduce a concept that might make things easier to grasp which is the pizza as a service so now the traditional onpremises model is where you would buy everything and make the pizza at home now as we go on in the lesson less flexibility will be available because more layers will be abstracted so the next service model that i wanted to introduce is infrastructure as a service or i as for short this is where all the layers from the data center up to virtualization is taken care of by the vendor this is the most basic model which is essentially your virtual machines in a cloud data center you set up configure and manage instances that run in the data center infrastructure and you put whatever you want on them on google cloud google compute engine would satisfy this model and so the unit of consumption here would be the operating system as you would manage all the operating system updates and everything that you decide to put on that instance but as you can see here you are still responsible for the container the run time the data and the application layers now bringing up the pizza as a service model is would be you picking up the pizza and you cooking it at home moving on to platform as a service or paz for short this is a model that is geared more towards developers and with pass the cloud provider provides a computing platform typically including the operating system the programming language execution environment the database and the web server now typically with pass you never have to worry about the operating system updates or managing the runtime and middleware and so the unit of consumption here would be the runtime now the runtime layer would be the layer you would consume as you would be running your code in the supplied runtime environment that the cloud vendor provides for you the provider manages the hardware and software infrastructure and you just use the service this is usually the layer on top of is and so all the layers between the data center and runtime is taken care of by the vendor a great example of this for google cloud is google app engine which we will be diving into a little bit later getting back to the pizza as a service model pass would fall under the pizza being delivered right to your door now with the past model explained i want to move into the last model which is sas which stands for software as a service now with sas all the layers are taken care of by the vendor so users are provided access to application software and cloud providers manage the infrastructure and platforms that run the applications g suite and microsoft's office 365 are great examples of this model now sas doesn't offer much flexibility but the tradeoff is that the vendor actually takes care of all these layers so again the unit of consumption here is the application itself and of course getting to the pizza as a service model sas is pretty much dining in the restaurant enjoying your pizza now to summarize when you have a data center on site you manage everything when it's infrastructure as a service part of that stack is abstracted by the cloud vendor with platform as a service you're responsible for the application and data everything else is abstracted by the vendor with software as a service again using the pizza as a service analogy on premise you buy everything and you make the pizza at home infrastructure as a service you pick up the pizza and you cook it at home when it comes to platform as a service the pizza is delivered and of course software as a service is dining in the restaurant now there will be some other service models coming up in this course such as function as a service and containers as a service and don't worry i'll be getting into those later but i just wanted to give you a heads up so now for some of you this may have been a lot of information to take in but trust me knowing these models will give you a better understanding of the services provided in google cloud as well as any other cloud vendor so that's all i wanted to cover in this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i wanted to discuss google cloud global infrastructure how data centers are connected how traffic flows when a request is done along with the overall structure of how google cloud geographic locations are divided for better availability durability and latency now google holds a highly provisioned low latency network where your traffic stays on google's private backbone for most of its journey ensuring high performance and a user experience that is always above the norm google cloud has been designed to serve users all around the world by designing their infrastructure with redundant cloud regions connected with high bandwidth fiber cables as well as subsea cables connecting different continents currently google has invested in 13 subsea cables connecting these continents at points of presence as you see here in this diagram hundreds of thousands of miles of fiber cables have also been laid to connect points of presence for direct connectivity privacy and reduced latency just to give you an idea of what a subsea cable run might look like i have included a diagram of how dedicated google is to their customers as there is so much that goes into running these cables that connect continents as you can see here this is the north virginia region being connected to the belgium region from the u.s over to europe a cable is run from the north virginia data center as well as having a point of presence in place going through a landing station before going deep into the sea on the other side the landing station on the french west coast picks up the other side of the cable and brings it over to the data center in the belgium region and this is a typical subsea cable run for google so continents are connected for maximum global connectivity now at the time of recording this video google cloud footprint spans 24 regions 73 zones and over 144 points of presence across more than 200 countries and territories worldwide and as you can see here the white dots on the map are regions that are currently being built to expand their network for wider connectivity now to show you how a request is routed through google's network i thought i would demonstrate this by using tony bowtie now tony makes a request to his database in google cloud and google responds to tony's request from a pop or edge network location that will provide the lowest latency this point of presence is where isps can connect to google's network google's edge network receives tony's request and passes it to the nearest google data center over its private fiber network the data center generates a response that's optimized to provide the best experience for tony at that given moment in time the app or browser that tony is using retrieves the requested content with a response back from various google locations including the google data centers edge pops and edge nodes whichever is providing the lowest latency this data path happens in a matter of seconds and due to google's global infrastructure it travels securely and with the least amount of latency possible no matter the geographic location that the request is coming from now i wanted to take a moment to break down how the geographic areas are broken out and organized in google cloud we start off with the geographic location such as the united states of america and it's broken down into multiregion into regions and finally zones and so to start off with i wanted to talk about zones now a zone is a deployment area for google cloud resources within a region a zone is the smallest entity in google's global network you can think of it as a single failure domain within a region now as a best practice resources should always be deployed in zones that are closest to your users for optimal latency now next up we have a region and regions are independent geographic areas that are subdivided into zones so you can think of a region as a collection of zones and having a region with multiple zones is designed for fault tolerance and high availability the intercommunication between zones within a region is under five milliseconds so rest assured that your data is always traveling at optimal speeds now moving on into a multiregion now multiregions are large geographic areas that contain two or more regions and this allows google services to maximize redundancy and distribution within and across regions and this is for google redundancy or high availability having your data spread across multiple regions always reassures that your data is constantly available and so that covers all the concepts that i wanted to go over when it comes to geography and regions within google cloud note that the geography and regions concepts are fundamental not only for the exam but for your daytoday role in google cloud so just as a recap a zone is a deployment area for google cloud resources within a region a zone is the smallest entity of google's global infrastructure now a region is an independent geographic area that are subdivided into zones and finally when it comes to multiregion multiregions are large geographic areas that contains two or more regions again these are all fundamental concepts that you should know for the exam and for your daytoday role in google cloud and so that's all i had for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back this lesson is going to be an overview of all the compute service options that are available in google cloud how they differ from each other and where they fall under the cloud service model again this lesson is just an overview of the compute options as we will be diving deeper into each compute option later on in this course so google cloud gives you so many options when it comes to compute services ones that offer complete control and flexibility others that offer flexible container technology managed application platform and serverless environments and so when we take all of these compute options and we look at it from a service model perspective you can see that there's so much flexibility starting here on the left with infrastructure as a service giving you the most optimal flexibility moving all the way over to the right where we have function as a service offering less flexibility but the upside being less that you have to manage and we'll be going through these compute options starting on the left here with infrastructure as a service we have compute engine now compute engine is google's staple infrastructure the service product that offers virtual machines or vms called instances these instances can be deployed in any region or zone that you choose you also have the option of deciding what operating system you want on it as well as the software so you have the option of installing different types of flavors of linux or windows and the software to go with it google also gives you the options of creating these instances using public or private images so if you or your company have a private image that you'd like to use you can use this to create your instances google also gives you the option to use public images to create instances and are available when you launch compute engine as well there are also preconfigured images and software packages available in the google cloud marketplace and we will be diving a little bit deeper into the google cloud marketplace in another lesson just know that there are slew of images out there that's available to create instances giving you the ease to deploy now when it comes to compute engine and you're managing multiple instances these are done using instance groups and when you're looking at adding or removing capacity for those compute engine instances automatically you would use auto scaling in conjunction with those instance groups compute engine also gives you the option of attaching and detaching disks as you need them as well google cloud storage can be used in conjunction with compute engine as another storage option and when connecting directly to compute engine google gives you the option of using ssh to securely connect to it so moving on to the next compute service option we have google kubernetes engine also known as gke now gke is google's flagship container orchestration system for automating deploying scaling and managing containers gke is also built on the same open source kubernetes project that was introduced by google to the public back in 2014 now before google made kubernetes a managed service there was many that decided to build kubernetes on premise in their data centers and because it is built on the same platform gke offers the flexibility of integrating with these onpremise kubernetes deployments now under the hood gke uses compute engine instances as nodes in a cluster and as a quick note a cluster is a group of nodes or compute engine instances and again we'll be going over all this in much greater detail in a different lesson so if you haven't already figured it out google kubernetes engine is considered container as a service now the next compute service option that i wanted to go over that falls under platform as a service is app engine now app engine is a fully managed serverless platform for developing and hosting web applications at scale now with app engine google handles most of the management of the resources for you for example if your application requires more computing resources because traffic to your website increases google automatically scales the system to provide these resources if the system software needs a security update as well that's handled for you too and so all you need to really take care of is your application and you can build your application in your favorite language go java.net and many others and you can use both preconfigured runtimes or use custom runtimes to allow you to write the code in any language app engine also allows you to connect with google cloud storage products and databases seamlessly app engine also offers the flexibility of connecting with thirdparty databases as well as other cloud providers and thirdparty vendors app engine also integrates with a wellknown security product in google cloud called web security scanner as to identify security vulnerabilities and so that covers app engine in a nutshell moving on to the next compute service option we have cloud functions and cloud functions fall under function as a service this is a serverless execution environment for building and connecting cloud services with cloud functions you write simple single purpose functions that are attached to events that are produced from your infrastructure and services in google cloud your function is triggered when an event being watched is fired your code then executes in a fully managed environment there is no need to provision any infrastructure or worry about managing any servers and cloud functions can be written using javascript python 3 go or java runtimes so you can take your function and run it in any of these standard environments which makes it extremely portable now cloud functions are a good choice for use cases that include the following data processing or etl operations such as video transcoding and iot streaming data web hooks that respond to http triggers lightweight apis that compose loosely coupled logic into applications as well as mobile backend functions again cloud functions are considered function as a service and so that covers cloud functions now moving to the far right of the screen on the other side of the arrow we have our last compute service option which is cloud run now cloud run is a fully managed compute platform for deploying and scaling containerized applications quickly and securely cloudrun was built on an open standard called k native and this enabled the portability of any applications that were built on it cloudrun also abstracts away all the infrastructure management by automatically scaling up and down almost instantaneously depending on the traffic now cloud run was google's response to abstracting all the infrastructure that was designed to run containers and so this is known as serverless for containers cloudrun has massive flexibility as you can write it in any language any library using any binary this compute service is considered a function as a service now at the time of recording this video i have not heard of cloud cloudrun being in the exam but since it is a compute service option i felt the need for cloudrun to have an honorable mention and so these are all the compute service options that are available on google cloud and we will be diving deeper into each one of these later on in this course again this is just an overview of all the compute service options that are available on the google cloud platform and so that's all i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back now in the last lesson i covered all the different options for compute services in this lesson we're going to cover the options that are available that couple well with these compute services by diving deeper into the different storage types and the different databases available on google cloud again this is strictly an overview as i will be diving deeper into these services later on in the course now when it comes to storage options there are three services that are readily available to you in google cloud each of them have their own specific use case that i will be diving into in just a second the first one i wanted to go over is cloud storage now with cloud storage this is google's consistent scalable large capacity and highly durable object storage so when i refer to object storage this is not the type of storage that you would attach to your instance and store your operating system on i'm talking about managing data as objects such as documents or pictures and shouldn't be confused with block storage which manages data at a more granular level such as an operating system not to worry if you fully don't grasp the concept of object storage i will be going into further detail with that later on in the cloud storage lesson cloud storage has 11 9's durability and what i mean by durability is basically loss of files so just to give you a better picture on cloud storage durability if you store 1 million files statistically google would lose one file every 659 000 years and you are about over 400 times more likely to get hit by a meteor than to actually lose a file so as you can see cloud storage is a very good place to be storing your files another great feature on cloud storage is the unlimited storage that it has with no minimum object size so feel free to continuously put files in cloud storage now when it comes to use cases cloud storage is fantastic for content delivery data lakes and backups and to make cloud storage even more flexible it is available in different storage classes and availability which i will be going over in just a second now when it comes to these different storage classes there are four different classes that you can choose from the first one is the standard storage class and this storage class offers the maximum availability with your data with absolutely no limitations this is great for storage that you access all the time the next storage class is near line and this is lowcost archival storage so this storage class is cheaper than standard and is designed for storage that only needs to be accessed less than once a month and if you're looking for an even more cost effective solution cloud storage has cold line storage class which is an even lower cost archival storage solution this storage class is designed for storage that only needs to be accessed less than once every quarter and just when you thought that the prices couldn't get lower than cold line cloud storage has offered another storage class called archive and this is the lowest cost archival storage which offers storage at a fraction of a penny per gigabyte but is designed for archival or backup use that is accessed less than once a year now when it comes to cloud storage availability there are three options that are available there is region dual region and multiregion region is designed to store your data in one single region dual region is exactly how it sounds which is a pair of regions now in multiregion cloud storage stores your data over a large geographic area consisting of many different regions across that same selected geographic area and so that about covers cloud storage as a storage option the next storage option that i wanted to talk about is file store now file store is a fully managed nfs file server from google cloud that is nfs version 3 compliant you can store data from running applications from multiple vm instances and kubernetes clusters accessing the data at the same time file store is a great option for when you're thinking about accessing data from let's say an instance group and you need multiple instances to access the same data and moving on to the last storage option we have persistent disks now with persistent disks this is durable block storage for instances now as i explained before block storage is different than object storage if you remember previously i explained that object storage is designed to store objects such as data or photos or videos whereas block storage is raw storage capacity that is used in drives that are connected to an operating system in this case persistent disks are doing just that persistent disks come in two options the first one is the standard option which gives you regular standard storage at a reasonable price and the other option is solid state or ssd which gives you lower latency higher iops and is just all around faster than your standard persistent disk both of these options are available in zonal and regional options depending on what you need for your specific workload so now that i've covered all three storage options i wanted to touch into the database options that are available on google cloud these database options come in both the sql and nosql flavors depending on your use case now getting into the options themselves i wanted to start off going into a little bit of detail with the sql relational options so the first option is cloud sql and cloud sql is a fully managed database service that is offered in postgres mysql and sql server flavors cloud sql also has the option of being highly available across zones now moving into cloud spanner this is a scalable relational database service that's highly available not only across zones but across regions and if need be available globally cloud spanner is designed to support transactions strong consistency and synchronous replication moving into the nosql options there are four available services that google cloud offers moving into the first one is bigtable and bigtable is a fully managed scalable nosql database that has high throughput and low latency bigtable also comes with the flexibility of doing cluster resizing without any downtime the next nosql option available is datastore and this is google cloud's fast fully managed serverless nosql document database datastore is designed for mobile web and internet of things applications datastore has the capabilities of doing multiregion replication as well as acid transactions for those of you who don't know i will be covering acid transactions in a later lesson next up for nosql options is firestore and this is a nosql realtime database and is optimized for offline use if you're looking to store data in a database in real time firestore is your option and like bigtable you can resize the cluster in firestore without any downtime and the last nosql option is memorystore and this is google cloud's highly available in memory service for redis and memcached this is a fully managed service and so google cloud takes care of everything for you now i know this has been a short lesson on storage and database options but a necessary overview nonetheless of what's to come and so that's about all i wanted to cover in this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back now while there are some services in gcp that take care of networking for you there are still others like compute engine that give you a bit more flexibility in the type of networking you'd like to establish this lesson will go over these networking services at a high level and provide you with strictly an overview to give you an idea on what's available for any particular type of scenario when it comes to connecting and scaling your network traffic i will be going into further details on these networking services in later lessons now i wanted to start off with some core networking features for your resources and how to govern specific traffic traveling to and from your network this is where networks firewalls and routes come into play so first i wanted to start off with virtual private cloud also known as vpc now vpc manages networking functionality for your google cloud resources this is a virtualized network within google cloud so you can picture it as your virtualized data center vpc is a core networking service and is also a global resource that spans throughout all the different regions available in google cloud each vpc contains a default network as well additional networks can be created in your project but networks cannot be shared between projects and i'll be going into further depth on vpc in a later lesson so now that we've covered vpc i wanted to get into firewall rules and routes now firewall rules segment your networks with a global distributive firewall to restrict access to resources so this governs traffic coming into instances on a network each default network has a default set of firewall rules that have already been established but don't fret you can create your own rules and set them accordingly depending on your workload now when it comes to routes this specifies how traffic should be routed within your vpc to get a little bit more granular routes specify how packets leaving an instance should be directed so it's a basic way of defining which way your traffic is going to travel moving on to the next concept i wanted to cover a little bit about low balancing and how it distributes workloads across multiple instances now we have two different types of load balancing and both these types of load balancing can be broken down to even a more granular level now when it comes to http or https low balancing this is the type of load balancing that covers worldwide auto scaling and load balancing over multiple regions or even a single region on a single global ip https load balancing distributes traffic across various regions and make sure that the traffic is routed to the closest region or in case there's failures amongst instances or in instances being bombarded with traffic http and https load balancing can route the traffic to a healthy instance in the next closest region another great feature of this load balancing is that it can distribute traffic based on content type now when it comes to network load balancing this is a regional load balancer and supports any and all ports it distributes traffic among server instances in the same region based on incoming ip protocol data such as address port and protocol now when it comes to networking dns plays a big part and because dns plays a big part in networking google has made this service 100 available on top of giving any dns queries the absolute lowest latency with google cloud dns you can publish and maintain dns records by using the same infrastructure that google uses and you can work with your managed zones and dns records such as mx records tax records cname records and a records and you can do this all through the cli the api or the sdk now some of the advanced connectivity options that are available in google cloud are cloudvpn and direct interconnect now cloudvpn connects your existing network whether it be onpremise or in another location to your vbc network through an ipsec connection the traffic is encrypted and travels between the two networks over the public internet now when it comes to direct interconnect this connectivity option allows you to connect your existing network to your vpc network using a highly available low latency connection this connectivity option does not traverse the public internet and merely connects to google's backbone and this is what gives it the highly available low latency connection a couple of other advanced connectivity options is direct and carrier peering these connections allow your traffic to flow through google's edge network locations and pairing can be done directly or it can be done through a thirdparty carrier and so although this is a very short lesson i will be going into greater depth on all these concepts in later lessons in the course so that's all i had to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson we're going to learn about how resources and entities are organized within google cloud and how permissions are inherited through this approach knowing this structure is a fundamental concept that you should know while working in gcp at any capacity so before defining what the resource hierarchy is i'd like to take a little bit of time to define what is a resource now in the context of google cloud a resource can refer to the service level resources that are used to process your workloads such as compute instance vms cloud storage buckets and even cloud sql databases as well as the account level resources that sit above the services such as the organization itself the folders and the projects of course which we will be getting into a little bit deeper in just a minute the resource hierarchy is google's way to configure and grant access to the various cloud resources for your company within google cloud both at the service level and at the account level the resource hierarchy in google cloud can truly define the granular permissions needed for when you need to configure permissions to everyone in the organization that actually makes sense so now that we covered what is a resource i wanted to start digging into the resource hierarchy and the structure itself now google cloud resources are organized hierarchically using a parentchild relationship this hierarchy is designed to map an organization's operational structure to google cloud and to manage access control and permissions for groups of related resources so overall resource hierarchy will give organizations better management of permissions and access control the accessibility of these resources or policies are controlled by identity and access management also known as iam a big component of gcp which we will be digging into a little bit later on in this course and so when an iam policy is set on a parent the child will inherit this policy respectively access control policies and configuration settings on a parent resource are always inherited by the child also please note that each child object can only have exactly one parent and that these policies are again controlled by iam so now to understand a little bit more about how the gcp resource hierarchy works i wanted to dig into the layers that support this hierarchy so this is a diagram of exactly what the resource hierarchy looks like in all of its awesomeness including the billing account along with the payments profile but we're not going to get into that right now i'll actually be covering that in a later lesson so more on that later so building the structure from the top down we start off with the domain or cloud level and as you can see here the domain of bowtieinc.co is at the top this is the primary identity of your organization at the domain level this is where you manage your users in your organizations so users policies and these are linked to g suite or cloud identity accounts now underneath the domain level we have the organization level and this is integrated very closely with the domain so with the organization level this represents an organization and is the root node of the gcp resource hierarchy it is associated with exactly one domain here we have the domain set as bowtie inc all entities or resources belong to and are grouped under the organization all controlled policies applied to the organization are inherited by all other entities and resources underneath it so any folders projects or resources will get those policies that are applied from the organization layer now i know that we haven't dug into roles as of yet but the one thing that i did want to point out is that when an organization is created an organization admin role is created and this is to allow full access to edit any or all resources now moving on to the folders layer this is an additional grouping mechanism and isolation boundary between each project in essence it's a grouping of other folders projects and resources so if you have different departments and teams within a company this is a great way to organize it now a couple of caveats when it comes to folders the first one is you must have an organization node and the second one is while a folder can contain multiple folders or resources a folder or resource can have exactly one parent now moving into the projects layer this is a core organizational component of google cloud as projects are required to use service level resources these projects are the base level organizing entity in gcp and parent all service level resources just as a note any given resource can only exist in one project and not multiple projects at the same time and moving on to the last layer we have the resources layer and this is any service level resource created in google cloud everything from compute engine instances to cloud storage buckets to cloud sql databases apis users all these service level resources that we create in google cloud fall under this layer now giving the hierarchy a little bit more context i want to touch on labels for just a second labels help categorize resources by using a key value pair and you can attach them to any resource and so what labels help you do is to break down and organize costs when it comes to billing now to give you some more structure with regards to the hierarchy under the domain level everything underneath this is considered a resource and to break it down even further everything you see from the organization layer to the projects layer is considered an account level resource everything in the resource layer is considered a service level resource and so this is how the google cloud resource hierarchy is split up and organized and so before i finish off this lesson i wanted to give you a quick runthrough on how policies can be applied at a hierarchical level so i thought i'd bring in tony bowtie for a quick demo so just to give you an example tony bowtie is part of department b and tony's manager lark decides to set a policy on department b's folder and this policy grants project owner role to tony at bowtieinc.co so tony will have the project owner role for project x and for project y at the same time lark assigns laura at bowtieinc.co cloud storage admin role on project x and thus she will only be able to manage cloud storage buckets in that project this hierarchy and permission inheritance comes up quite a bit not only in the exam but is something that should be carefully examined when applying permissions anywhere within the hierarchy in your daytoday role as an engineer applying permissions or policies to resources with existing policies may not end up getting you the desired results you're looking for and may have a chance to be overlooked now i hope these diagrams have given you some good contacts with regards to resource hierarchy its structure and the permissions applied down the chain now that's all i have for this lesson on resource hierarchy so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i will be covering a few different topics that i will touch on when creating a new google cloud account i will be covering going over the free tier and the always free options the differences between them and a demo showing how you can create your own free tier account as well i'll also be going into what you will need in order to fulfill this demo so for the remainder of this course all the demos will run under the free tier now when i built this course i built it with budget in mind and having viewed on ways where i can keep the price to a minimum while still keeping the demos extremely useful and so the free tier falls within all these guidelines and will help you learn without the high ticket price and so getting into a quick overview of the differences between the free tier and the always free option i have broken them down here with their most significant differences in the free tier google cloud offers you a 12 month free trial with a 300 u.s credit this type of account ends when the credit is used or after the 12 months whichever happens first and so for those of you who are looking at taking advantage of this on a business level unfortunately the free tier only applies to a personal account and cannot be attached to a business account now moving over to the always free option the always free option isn't a special program but it's a regular part of your google cloud account it provides you limited access to many of the google cloud resources free of charge and once these limits have been hit then you are charged at the regular per second billing rate and i will show you a little bit later how to monitor these credits so that you don't go over using this in conjunction with the free tier account is not possible you have to have an upgraded billing account which can also include a business account now there are a bunch more stipulations in this program and i will include a link to both of them in the lesson text below for later viewing at your convenience now lastly before we get into the demo i wanted to go through a quick runthrough of exactly what's needed to open up your free tier account so we're going to start off with a fresh new gmail address so that it doesn't conflict with any current gmail address that you may have you're gonna need a credit card for verification and this is for google to make sure that you're an actual human being and not a robot and you won't be charged unless you go above the 300 credit limit as well i highly recommend going into a private browsing session so whether you're using chrome you would use an incognito session if you're using firefox you would use private browsing and in microsoft edge you would be using the in private mode and so in order to start with this free trial you can head on over to the url listed here and i'll also include this in the lesson text so head on over to this url and i'll see you there in just a second okay so here we are at the free trial url i'm here in google chrome in an incognito session and so we're not going to sign up we're going to go over here to create account you can just click on create account for myself because as i mentioned earlier you're not able to create a free trial account with your business so i'm going to click on for myself and it's going to bring you to this page where it says create your google account and you're going to go to create a new gmail address instead and now you're going to fill in all the necessary information that's needed in order to open up this new gmail account once you're finished typing your password you can hit next and now i got prompted for six digit verification code that i have to plug in but in order to do that google needs my telephone number so i'm gonna type that in now and just to let you know this verification is done to let google know that you're not a bot and you're a real human and google just sent me a verification code and this is a onetime verification code that i'm going to plug in and i'm going to hit verify and you can plug in the necessary information here for recovery email address your birthday and gender and this is so that google can authenticate you in case you accidentally misplace your password and then just hit next and here google gives you a little bit more information on what your number can be used for and so i'm going to go ahead and skip it and of course we're going to read through the terms of service and the privacy policy click on agree and as you can see we're almost there it shows here that we're signing up for the free trial i'm in canada so depending on your country this may change of course i read the terms of service and i'm going to agree to it and i don't really want any updates so you can probably skip that and just hit continue and so this is all the necessary information that needs to be filled out for billing and so here under account type be sure to click on individual as opposed to business and again fill in all the necessary information with regards to your address and your credit card details and once you fill that in you can click on start my free trial and once you've entered in all that information you should be brought to this page with a prompt asking you exactly what you need with regards to google cloud and you can just hit skip here and i'm going to zoom in here just see a little better and so here you're left with a checklist where you can go through all the different resources and it even gives you a checklist to go through but other than that we're in and so just to verify that we're signed up for a free tier account i'm going to go over to billing and i'm going to see here that i have my free trial credit and it says 411 dollars and due to the fact that my currency is in canadian dollars it's been converted from us dollars and so we'll be going through billing in a later lesson but right now we are actually logged in and so that's all i wanted to cover for this lesson on how to sign up for your free trial account so you can now mark this lesson as complete and you can join me in the next one where we will secure the account using a method called twostep verification welcome back so in the last lesson we went ahead and created a brand new gcp account in this lesson we'll be discussing how to secure that gcp account by following some best practices whenever any account is created in google cloud and this can be applied with regards to personal accounts as well as the super admin account as it's always good to keep safety as a priority this lesson may be a refresher for those who are a bit more advanced as for everyone else these steps could help you from an attack on your account i'd first like to run you through a scenario of the outcome on both secure and nonsecure accounts as well as the different options that reside in google cloud when it comes to locking down your account i'll then run through a handson demo in the console to show you how you can apply it yourself so in this specific scenario a username and password is used to secure the account here lark a trouble causing manager looks over the shoulder of tony bowtie while he plugs in his username and password so that he can later access his account to wreak havoc on tony's reputation as tony leaves for coffee lark decides to log in and send a companywide email from tony's account to change an already made decision about next season's store opening in rome italy that would not look good for tony it was that easy for lark to steal tony's password and in a real life scenario it would be that easy for someone to steal your password now when someone steals your password they could do even more devious things than what lark did not just sending out harmful emails they could lock you out of your account or even delete emails or documents this is where twostep verification comes in this can help keep bad people out even if they have your password twostep verification is an extra layer of security most people only have one layer to protect their account which is their password with twostep verification if a bad person hacks through your password they'll still need your phone or security key to get into your account so how twostep verification works is that signin will require something you know and something that you have the first one is to protect your account with something you know which will be your password and the second is something that you have which is your phone or security key so whenever you sign into google you'll enter your password as usual then a code will be sent to your phone via text voice call or google's mobile app or if you have a security key you can insert it into your computer's usb port codes can be sent in a text message or through a voice call depending on the setting you choose you can set up google authenticator or another app that creates a onetime verification code which is great for when you're offline you would then enter the verification code on the sign in screen to help verify that it is you another way for verification is using google prompts and this can help protect against sim swap or other phone number based hacks google prompts are push notifications you'll receive on android phones that are signed into your google account or iphones with the gmail app or google app that's signed into your google account now you can actually skip a second step on trusted devices if you don't want to provide a second verification step each time you sign in on your computer or your phone you can check the box next to don't ask again on this computer and this is a great added feature if you are the only user on this device this feature is not recommended if this device is being used by multiple users security keys are another way to help protect your google account from phishing attacks when a hacker tries to trick you into giving them your password or other personal information now a physical security key is a small device that you can buy to help prove it's you signing in when google needs to make sure that it's you you can simply connect your key to your computer and verify that it's you and when you have no other way to verify your account you have the option of using backup codes and these are onetime use codes that you can print or download and these are multiple sets of eightdigit codes that you can keep in a safe place in case you have no other options for verification i personally have found use in using these backup codes as i have used them in past when my phone died so ever since lark's last email tony not only changed his password but added a twostep verification to his account so that only he would have access and would never have to worry again about others looking over his shoulder to gain access to his account as tony leaves for coffee lark tries to log in again but is unsuccessful due to the twostep verification in place tony has clearly outsmarted the bad man in this scenario and lark will have to look for another way to foil tony's plan to bring greatness to bow ties across the globe and this is a sure difference between having a secure account and a not so secure account and so now that i've gone through the theory of the twostep verification process i'm going to dive into the console and implement it with the handson demo just be aware that you can also do this through the gmail console but we're going to go ahead and do it through the google cloud console using the url you see here so whenever you're ready feel free to join me in the console and so here we are back in the console and over here on the top right hand corner you will find a user icon and you can simply click on it and click over to your google account now i'm just going to zoom in for better viewing and so in order to enable twostep verification we're gonna go over here to the menu on the left and click on security and under signing into google you will find twostep verification currently it's off as well as using my phone to sign in is off so i'm going to click on this bar here for twostep verification and i definitely want to add an extra layer of security and i definitely want to keep the bad guys out so i'm going to go ahead and click on the get started button it'll ask me for my password and because i've entered my phone number when i first signed up for the account it actually shows up here this is i antony which is my iphone and so now i can get a twostep verification here on my iphone and again this is going to be a google prompt as it shows here but if i wanted to change it to something else i can simply click on show more options and here we have a security key as well as text message or voice call i highly recommend the google prompt as it's super easy to use with absolutely no fuss and so as i always like to verify what i've done i'm going to click on this try it now button and so because i wanted to show you exactly what a live google prompt looks like i'm going to bring up my phone here on the screen so that you can take a look and it actually sent me a google prompt to my phone and i'm just going to go ahead and open up my gmail app so i can verify that it is indeed me that wants to log in which i will accept and so once i've accepted the google prompt another window will pop up asking me about a backup option and so i'll simply need my phone number and i can either get a text message or a phone call and again you have other options as well so you can use the onetime backup codes which we discussed earlier and you can print or download them but i usually like to use a text message and so i'm going to use that i'm going to send it to my phone and so just to verify it i'm gonna now plug in the onetime code that was sent to me and then just hit next so the second step is the google prompt it's my default and my backup options if i can't get google prompt is a voice or text message and again this is for my account antony gcloud ace at gmail.com sending it to my i antony device so turn on twostep verification absolutely and so there you have it there is twostep verification enabled and if i wanted to change the available steps i can do so here i can also edit it i can edit my phone number and i can also set up any backup codes in case i need it in my personal opinion twostep verification is a musthave on any account best practice is to always do it for your super admin account which would be my gmail account that i am currently signed up with but i find is a necessity for any other users and always make it a policy for people to add twostep verification to their accounts i highly recommend that you make it your best practice to do this in your role as an engineer in any environment at any organization again twostep verification will allow to keep you safe your users safe and your environment safe from any malicious activities that could happen at any time and that's all i have for this lesson on twostep verification and securing your account so you can now mark this lesson as complete and let's move on to the next one welcome back now there are many different ways in which you can interact with google cloud services and resources this lesson is an overview of the gcp console and how you can interact with it using the graphical user interface and so for this handson demo i will be diving into how to navigate through the gcp console and point out some functions and features that you may find helpful so with that being said let's dive in and so here we are back in the console up here you can see the free trial status and then i still have 410 credit again this is canadian dollars so i guess consider me lucky so i'm going to go ahead over here and dismiss this don't activate it because otherwise this will kill your free trial status and you don't want to do that so i'm just going to hit dismiss so over here on the main page you have a bunch of cards here that will give you the status of your environment as well as the status of what's happening within google cloud with these cards you can customize them by hitting this button over here customize and you can turn them on or off and you can go ahead and move these around if you'd like and i'm going to put this up here as well i'm going to turn on my billing so i can keep track of exactly what my spend is i don't really need my get starting card so i'm going to turn that off as well as the documentation i'm going to turn that off as well and the apis is always nice to have as well up here on the project info this reflects the current project which is my first project and the project name here is the same the project id is showing and the project number and i'm going to dive deeper into that in another lesson also note that your cards will reflect exactly what it is that you're interacting with and so the more resources that you dive into the cards will end up showing up here and you can add them and turn them off at will so i'm going to go up here and click on done because i'm satisfied with the way that things look here on my home page and over here to your left i wanted to focus on all the services that are available in their own specific topics so for instance all of compute you will find app engine compute engine kubernetes and so on so note that anything compute related you'll find them all grouped together also another great feature is that you can pin exactly what it is that you use often so if i am a big user of app engine i can pin this and it will move its way up to the top this way it saves me the time from having to go and look for it every time i need it and if i'm using it constantly it's great to have a shortcut to unpin it i simply go back to the pin and click on it again as well if i'd like to move the menu out of the way to get more screen real estate i can simply click on this hamburger button here and make it disappear and to bring it back i can just click on that again and i'll bring it back again now i know that there's a lot of resources here to go through so if you're looking for something specific you can always go up to the search bar right here and simply type it in so if i'm looking for let's say cloud sql i can simply type in sql and i can find it right here i can find the api and if anything associated with the word sql if i'm looking for cloud sql specifically i can simply type in cloud sql and here it is another thing to note is that if you want to go back to your homepage you can simply go up to the left hand corner here and click on the google cloud platform logo and it'll bring you right back and right here under the google cloud platform logo you'll see another set of tabs we have dashboard we also have activity and this will show all the latest activity that's been done and because this is a brand new account i don't have much here now because this is my first time in activity this is going to take some time to index and in the meantime i wanted to show you filters if this were a long list to go through where activity has been happening for months i can filter through these activities either by user or by categories or by resource type as well as the date i can also combine these to search for something really granular and beside the activity tab we have recommendations which is based on the recommender service and this service provides recommendations and insights for using resources on google cloud these recommendations and insights are on a per product or per service basis and they are based on machine learning and current resource usage a great example of a recommendation is vm instance right sizing so if the recommender service detects that a vm instance is underutilized it will recommend changing the machine size so that i can save some money and because this is a fresh new account and i haven't used any resources this is why there is no recommendations for me so going back to the home page i want to touch on this projects menu for a second and as you can see here i can select a project now if i had many different projects i can simply search from each different one and so to cover the last part of the console i wanted to touch on this menu on the top right hand corner here so clicking on this present icon will reveal my free trial status which i dismissed earlier next to the present we have a cloud shell icon and this is where you can activate and bring up the cloud shell which i will be diving into deeper in a later lesson and right next to it is the help button in case you need a shortcut to any documentations or tutorials as well some keyboard shortcuts may help you be a little bit more efficient and you can always click on this and it'll show you exactly what you need to know and so i'm going to close this and to move over to the next part in the menu this is the notifications so any activities that happen you will be notified here and you can simply click on the bell and it'll show you a bunch of different notifications for either resources that are created or any other activities that may have happened now moving on over three buttons over here is the settings and utilities button and over here you will find the preferences and under communication you will find product notifications and updates and offers and you can turn them off or on depending on whether or not you want to receive these notifications as well you have your language and region and you can personalize the cloud console as to whether or not you want to allow google to track your activity and this is great for when you want recommendations so i'm going to keep that checked off getting back to some other options you will find a link to downloads as well as cloud partners and the terms of service privacy and project settings and so to cover the last topic i wanted to touch on is the actual google account button and here you can add other user accounts for when you log into the console with a different user as well as go straight to your google account and of course if you're using a computer that's used by multiple users you can sign out here as well and so that's just a quick runthrough of the console and so feel free to poke around and get familiar with exactly what's available in the console so that it's a lot easier for you to use and allow you to become more efficient and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be going through a breakdown of cloud billing and an overview of the various resources that's involved with billing billing is important to know and i'll be diving into the concepts around billing and billing interaction over the next few lessons as well i'll be getting into another demo going through the details on how to create edit and delete a cloud billing account now earlier on in the course i went over the resource hierarchy and how google cloud resources are broken down starting from the domain level down to their resource level this lesson will focus strictly on the billing account and payments profile and the breakdown are concepts that are comprised within them so getting right into it let's start with the cloud billing account a cloud billing account is a cloud level resource managed in the cloud console this defines who pays for a given set of google cloud resources billing tracks all of the costs incurred by your google cloud usage as well it is connected to a google payments profile which includes a payment method defining on how you pay for your charges a cloud billing account can be linked to one or more projects and not to any one project specifically cloud billing also has billing specific roles and permissions to control accessing and modifying billing related functions that are established by identity and access management cloud billing is offered in two different account types there is the selfservice or online account or you can also choose from the invoiced or offline payments when it comes to the selfservice option the payment method is usually a credit or debit card and costs are charged automatically to the specific payment method connected to the cloud billing account and when you need access to your invoices you can simply go to the cloud console and view them online now when it comes to the invoice account first you must be eligible for invoice billing once you are made eligible the payment method used can be check or wire transfer your invoices are sent by mail or electronically as well they're also available in the cloud console as well as the payment receipts now another cool feature of billing account is subaccounts and these are intended for resellers so if you are a reseller you can use subaccounts to represent your customers and make it easy for chargebacks cloud billing subaccounts allow you to group charges from projects together on a separate section of your invoice and is linked back to the master cloud billing account on which your charges appear subaccounts are designed to allow for customer separation and management so when it comes to ownership of a cloud billing account it is limited to a single organization it is possible though for a cloud billing account to pay for projects that belong to an organization that is different than the organization that owns the cloud billing account now one thing to note is that if you have a project that is not linked to a billing account you will have limited use of products and services available for your project that is projects that are not linked to a billing account cannot use google cloud services that aren't free and so now that we've gone through an overview of the billing account let's take a quick step into the payments profile now the payments profile is a google level resource managed at payments.google.com the payments profile processes payments for all google services and not just for google cloud it connects to all of your google services such as google ads as well as google cloud it stores information like your name address and who is responsible for the profile it stores your various payment methods like credit cards debit cards and bank accounts the payments profile functions as a single pane of glass where you can view invoices payment history and so on it also controls who can view and receive invoices for your various cloud billing accounts and products now one thing to note about payments profile is that there are two different types of payment profiles the first one is individual and that's when you're using your account for your own personal payments if you register your payments profile as an individual then only you can manage the profile you won't be able to add or remove users or change permissions on the profile now if you choose a business profile type you're paying on behalf of a business or organization a business profile gives you the flexibility to add other users to the google payments profile you manage so that more than one person can access or manage a payments profile all users added to a business profile can then see the payment information on that profile another thing to note is that once the profile type has been selected it cannot be changed afterwards and so now that we've quickly gone through an overview of all the concepts when it comes to billing i am now going to run through a short demo where i will create a new billing account edit that billing account and show you how to close a billing account so whenever you're ready join me in the console and so here i am back in the console and so the first thing i want to do is i want to make sure that i have the proper permissions in order to create and edit a new billing account so what i'm going to do is go over here to the hamburger menu up here in the top left hand corner and click on it and go over to i am an admin and over to iam now don't worry i'm not going to get really deep into this i will be going over this in a later section where i'll go through iam and roles but i wanted to give you a sense of exactly what you need with regards to permissions so now that i'm here i'm going to be looking for a role that has to do with billing so i'm simply going to go over here on the left hand menu and click on roles and you'll have a slew of roles coming up and what you can do is filter through them just by simply typing in billing into the filter table here at the top and as you can see here there is billing account administrator billing account creator and so on and so forth and just to give you a quick overview on these roles and so for the billing account administrator this is a role that lets you manage billing accounts but not create them so if you need to set budget alerts or manage payment methods you can use this role the billing account creator allows you to create new selfserve online billing accounts the billing account user allows you to link projects to billing accounts the billing account viewer allows you to view billing account cost information and transactions and lastly the project billing manager allows you to link or unlink the project to and from a billing account so as you can see these roles allow you to get pretty granular when it comes to billing so i'm going to go back over to the left hand menu over on iam and click on there and i want to be able to check my specific role and what permissions that i have or i will need in order to create a new billing account and so if i click on this pencil it'll show me exactly what my role is and what it does and as it says here i have full access to all resources which means that i am pretty much good to go so i'm going to cancel out here and i'm going to exit i am an admin so i'm going to click on the navigation menu and go over to billing and so this billing account is tied to the current project and because it's the only billing account it's the one that shows up and so what i want to do is i want to find out a little bit more information with regards to this billing account so i'm going to move down the menu and click on account management here i can see the billing account which is my billing account i can rename it if i'd like and i can also see the projects that are linked to this billing account so now that we've viewed all the information with regards to the my billing account i'm going to simply click on this menu over here and click on the arrow and go to manage billing accounts and here it will bring me to all my billing accounts and because i only have one is shown here my billing account but if i had more than one they would show up here and so now in order for me to create this new billing account i'm going to simply click on create account and i will be prompted with a name a country and a currency for my new billing account and i'm actually going to rename this billing account and i'm going to rename it to gcloud ace dash billing i'm going to leave my country as canada and my currency in canadian dollars and i'm going to simply hit continue and it's giving me the choice in my payments profile and because i want to use the same payments profile i'm just going to simply leave everything as is but for demonstration purposes over here you can click on the payments profile and the little arrow right beside the current profile will give me the option to create a new payments profile and we're going to leave that as is under customer info i have the option of changing my address and i can click on this pencil icon and change it as well i can go to payment methods and click on the current payment method with that little arrow and add a new credit or debit card and as i said before we're going to keep things the way they are and just hit submit and enable billing now as you can see here i got a prompt saying that a confirmation email will be sent within 48 hours now usually when you're setting up a brand new billing profile with an already created payments profile you'll definitely get a confirmation email in less than 48 hours now in order for me to finish up this demo i'm gonna wait until the new billing account shows up and continue with the demo from then and so here i am back in the billing console and it only took about 20 minutes and the gcloud ace billing account has shown up and so with part of this demo what i wanted to show is how you can take a project and attach it to a different billing account and so currently my only project is attached to the my billing account so now if i wanted to change my first project to my gcloud ace dash billing account i can simply go over here to actions click on the hamburger menu and go to change billing here i'll be prompted to choose a billing account and i can choose g cloud a stash billing and then click on set account and there it is my first project is now linked to g cloud a stash billing so if i go back over to my billing accounts you can see here that my billing account currently has zero projects and g cloud a stash billing has one project now just as a quick note and i really want to emphasize this is that if you're changing a billing account for a project and you are a regular user you will need the role of the billing account administrator as well as the project owner role so these two together will allow a regular user to change a billing account for a project and so now what i want to do is i want to take the gcloud a stash billing and i want to close that account but before i do that i need to unlink this project and bring it back to another billing account which in this case would be my billing account so i'm going to go back up here to the menu click on my projects and we're going to do the exact same thing that we did before under actions i'm going to click on the hamburger menu and change billing i'm going to get the prompt again and under billing account i'm going to choose my billing account and then click on set account so as you can see the project has been moved to a different billing account i'm going to go back to my billing accounts and as you can see here the project is back to my billing account and so now that the project is unlinked from the gcloud a stash billing account i can now go ahead and close out that account now in order to do that i'm going to click on gcloud a stash billing i'm going to go down here on the hand menu all the way to the bottom to account management click on there and at the top here you will see close billing account i'm going to simply click on that and i'll get a prompt that i've spent zero dollars and is linked to zero projects now if i did have a project that was linked to this billing account i would have to unlink the project before i was able to close this billing account so as a failsafe i'm being asked to type close in order to close this billing account so i'm going to go ahead and do that now and click on close billing account just as a note google gives me the option to reopen this billing account in case i did this by mistake and i really needed it i can reopen this billing account so now moving back over to billing you'll see here that i'm left with my single billing account called my billing account with the one project that's linked to it and so that covers my demo on creating editing and closing a new billing account as well as linking and unlinking a project to and from a different billing account so i hope you found this useful and you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be going over controlling costs in google cloud along with budget alerts i will be touching on all the available discounts the number of ways to control costs and go over budget alerts to get a more granular and programmatic approach so starting off i wanted to touch on committed use discounts now committed use discounts provide discounted prices in exchange for your commitment to use a minimum level of resources for a specified term the discounts are flexible cover a wide range of resources and are ideal for workloads with predictable resource needs when you purchase google cloud committed use discounts you commit to a consistent amount of usage for a one or three year period there are two commitment types available and as you can see here they are spend based and resource based commitment types and unlike most other providers the commitment fee is billed monthly so going over the specific commitment types i wanted to start off with spend based commitment now for spend based commitment you commit to a consistent amount of usage measured in dollars per hour of equivalent ondemand spend for a one or three year term in exchange you receive a discounted rate on the applicable usage your commitment covers so you can purchase committed use discounts from any cloud billing account and the discount applies to any eligible usage in projects paid for by that cloud billing account any overage is charged at the ondemand rate spend based commitments can give you a 25 discount off ondemand pricing for a oneyear commitment and up to a 52 discount off of ondemand pricing for a threeyear commitment now spendbased commitments are restricted to specific resources which is cloud sql database instances and google cloud vmware engine and this commitment applies to the cpu and memory usage for these available resources now the other committed use discount is the resourcebased commitment so this discount is for a commitment to spend a minimum amount for compute engine resources in a particular region resourcebased commitments are ideal for predictable workloads when it comes to your vms when you purchase a committed use contract you purchase compute resources such as vcpus memory gpus and local ssds and you purchase these at a discounted price in return for committing to paying for those resources for one or three years the discount is up to 57 percent for most resources like machine types or gpus the discount is up to 70 percent for memory optimized machine types and you can purchase a committed use contract for a single project or purchase multiple contracts which you can share across many project by enabling shared discounts and sharing your committed use discounts across all your projects reduces the overhead of managing discounts on a per project basis and maximizes your savings by pooling all of your discounts across your project's resource usage if you have multiple projects that share the same cloud billing account you can enable committed use discount sharing so all of your projects within that cloud billing account share all of your committed use discount contracts and so your sustained use discounts are also pooled at the same time so touching on sustained use discounts these are automatic discounts for running specific compute engine resources a significant portion of the billing month sustained use discounts apply to the general purpose compute and memory optimize machine types as well as sole tenant nodes and gpus again sustained use discounts are applied automatically to usage within a project separately for each region so there's no action required on your part to enable these discounts so for example when you're running one of these resources for more than let's say 25 percent of the month compute engine automatically gives you a discount for every incremental minute that you use for that instance now sustained use discounts automatically apply to vms created by both google kubernetes engine and compute engine but unfortunately do not apply to vms created using the app engine flexible environment as well as data flow and e2 machine types now to take advantage of the full discount you would create your vm instances on the first day of the month as discounts reset at the beginning of each month and so the following table shows the discount you get at each usage level of a vm instance these discounts apply for all machine types but don't apply to preemptable instances and so sustained use discounts can save you up to a maximum of a 30 percent discount so another great way to calculate savings in google cloud is by using the gcp pricing calculator this is a quick way to get an estimate of what your usage will cost on google cloud so the gcp pricing calculator can help you identify the pricing for the resources that you plan to use in your future architecture so that you are able to calculate how much your architecture will cost you this calculator holds the pricing for almost all resources encapsulated within gcp and so you can get a pretty good idea of what your architecture will cost you without having to find out the hard way this calculator can be found at the url shown here and i will include this in the lesson text below now moving right along to cloud billing budgets so budgets enable you to track your actual spend against your plan spend after you've set a budget amount you set budget alert threshold rules that are used to trigger email notifications and budget alert emails help you stay informed about how your spend is tracking against your budget this example here is a diagram of a budget alert notification and is the default functionality for any budget alert notifications now to get a little bit more granular you can define the scope of the budget so for example you can scope the budget to apply to the spend of an entire cloud billing account or get more granular to one or more projects and even down to a specific product you can set the budget amount to a total that you specify or base the budget amount on the previous month's spend when costs exceed a percentage of your budget based on the rules that you set by default alert emails are sent to billing account administrators and billing account users on the target cloud billing account and again this is the default behavior of a budget email notification now as said before the default behavior of a budget is to send alert emails to billing account administrators and billing account users on the target cloud billing account when the budget alert threshold rules trigger an email notification now these email recipients can be customized by using cloud monitoring to specify other people in your organization to receive these budget alert emails a great example of this would be a project manager or a director knowing how much spend has been used up in your budget and the last concept i wanted to touch on when it comes to cloud billing budgets is that you can also use pub sub for programmatic notifications to automate your cost control response based on the budget notification you can also use pub sub in conjunction with billing budgets to automate cost management tasks and this will provide a realtime status of the cloud billing budget and allow you to do things like send notifications to slack or disable billing to stop usage as well as selectively control usage when budget has been met and so these are all the concepts that i wanted to cover when it came to cloud billing budgets now i know this lesson may have been a bit dry and not the most exciting service to dive into but it is very important to know both for the exam and for your role as an engineer when it comes to cutting costs in environments where your business owners deem necessary and so that's all i had for this lesson so you can now mark this lesson as complete and please join me in the next one where i dive into the console and do some handson demos when it comes to committed use discounts budget alerts and editing budget alerts as well as adding a little bit of automation into the budgeting alerts welcome back in the last lesson i went over a few ways to do cost management and the behaviors of budget alerts in this lesson i will be doing a demo to show you committed use discounts and reservations along with how to create budget alerts and as well how to edit them so with that being said let's dive in so now i'm going to start off with committed use discounts in order to get there i'm going to find it in compute engine so i'm going to simply go up here on the top left hand corner back to the navigation menu i'm going to go down to compute engine and i'm going to go over here to committed use discounts and as we discussed earlier these commitments for compute engine are resource based and as you can see here we have hardware commitments and reservations now reservations i will get into just a little bit later but with regards to hardware commitments we're going to get into that right now and as expected i have no current commitments so i'm going to go up to purchase commitment and so i need to start off with finding a name for this commitment and so i'm going to name this commitment demo dash commitment it's going to ask me for a region i'm going to keep it in us central one with the commitment type here is where i can select the type of machine that i'm looking for so i can go into general purpose and 1 and 2 and 2d e2 as well as memory optimize and compute optimized and so i'm going to keep it at general purpose and one again the duration one or three years and we get down to cores i can have as many vcpus as i'd like so if i needed 10 i can do that and i'll get a popup here on the right showing me the estimated monthly total as well as an hourly rate for this specific vm with 10 cores i can also select the duration for three years and as expected i'll get a higher savings because i'm giving a bigger commitment so bring it back down to one year and let's put the memory up to 64 gigabytes here i can add gpus and i have quite a few to choose from as well as local ssds and here with the local ssds i can choose as many disks as i'd like as long as it's within my quota and each disk size is going to be 375 gigabytes so if you're looking into committed use discounts and using local ssds please keep that in mind again the reservation can be added here and i'll be getting into that in just a second and now i don't want to actually purchase it but i did want to show you exactly what a committed use discount would look like and how you would apply it again here on the right hand side it shows me the details of the estimated monthly total and the hourly rate so i'm going to go over here and hit cancel and if i were to have applied it the commitment would show up here in this table and give me all the specified configurations of that instance right here now touching on reservations reservations is when you reserve the vm instances you need so when the reservation has been placed the reservation ensures that those resources are always available for you as some of you might know when you go to spin up a new compute engine vm especially when it comes to auto scaling instance groups the instances can sometimes be delayed or unavailable now the thing with reservations is that a vm instance can only use a reservation if its properties exactly match the properties of the reservation which is why it's such a great pairing with committed use discounts so if you're looking to make a resourcebased commitment and you always want your instance available you can simply create a reservation attach it to the commitment and you will never have to worry about having the resources to satisfy your workload as they will always be there so again going into create reservation it'll show me here the name the description i can choose to use the reservation automatically or select a specific reservation the region and zone number of instances and here i can specify the machine type or specify an instance template and again this is another use case where if you need compute engine instances spun up due to auto scaling this is where reservations would apply so getting back to machine type i can choose from vcpus as well as the memory i can customize it i can add as many local ssds as my quotas will allow me and i can select my interface type and i'm going to cancel out of here now when it comes to committed use discounts and reservations as it pertains to the exam i have not seen it but since this is an option to save money i wanted to make sure that i included it in this lesson as this could be a great option for use in your environment so now that we covered resourcebased committed use discounts i wanted to move into spend based commitments and so where you would find that would be over in billing so again i'm going to go up to the navigation menu in the top left hand corner and go into billing now you'd think that you would find it here under commitments but only when you have purchased a commitment will it actually show up here but as you can see here it's prompting us to go to the billing overview page so going back to the overview page you'll find it down here on the right and so i can now purchase a commitment and as we discussed before a spend based commitment can be used for either cloud sql or for vmware engine i select my billing account the commitment name the period either one year or three years and it also shows me the discount which could help sway my decision as well as the region as well as the hourly ondemand commitment now you're probably wondering what this is and as explained here this commitment is based on the ondemand price and once this is all filled out the commitment summary will be populated and after you agree to all the terms and services you can simply hit purchase but i'm going to cancel out of here and so that is an overview for the spend based commitment and again these committed use discounts i have not seen on the exam but i do think that it's good to know for your daytoday environment if you're looking to save money and really break down costs so now that i've covered committed use discounts and reservations i wanted to move over to budgets and budget alerts and because i'm already on the billing page all i need to do is go over here to the left hand menu and click on budgets and alerts now setting up a budget for yourself for this course would be a great idea especially for those who are cost conscious on how much you're spending with regards to your cloud usage and so we're to go ahead and create a new budget right now so let's go up here to the top to create budget and i'm going to be brought to a new window where i can put in the name of the budget and i'm going to call this ace dash budget and because i want to monitor all projects and all products i'm going to leave this as is but if you did have multiple projects you could get a little bit more granular and the same thing with products so i'm going to go ahead and leave it as is and just click on next now under budget type i can select from either a specified amount or the last month's spend and so for this demo i'm going to keep it at specified amount and because i want to be really conscious about how much i spend in this course i'm going to put in 10 for my target amount i'm going to include the credits and cost and then i'm going to click on next now these threshold rules are where billing administrators will be emailed when a certain percent of the budget is hit so if my spend happens to hit five dollars because i am a billing administrator i will be sent an email telling me that my spend has hit five dollars i also have the option of changing these percentages so if i decided to change it to forty percent now my amount goes to four dollars and this is done automatically so no need to do any calculations but i'm going to keep this here at 50 percent and vice versa if i wanted to change the amount the percentage of budget will actually change now with the trigger i actually have the option of selecting forecasted or actual and so i'm going to keep it on actual and if i want i can add more threshold rules now i'm going to leave everything as is and just click on finish and now as you can see here i have a budget name of ace budget now because the budget name doesn't have to be globally unique in your environment you can name your budget exactly the same and again it'll give me all the specific configurations that i filled out shows me how much credits i've used and that's it and that's how you would create a budget alert now if i needed to edit it i can always go back to ace budget and here i can edit it but i'm not going to touch it and i'm just going to hit cancel and so the last thing i wanted to show you before we end this lesson is how to create another budget but being able to send out the trigger alert emails to different users and so in order to do that i'm going to go back up here to create budget i'm going to name this to ace dash budget dash users i'm going to leave the rest as is i'm going to click on next again i'm going to leave the budget type the way it is the target amount i'm going to put ten dollars leave the include credits and cost and just click on next and so here i'm going to leave the threshold rules the way they are and right here under manage notifications i'm going to click off link monitoring email notification channels to this budget now because the email notification channel needs cloud monitoring in order to work i am prompted here to select a workspace which is needed by cloud monitoring so because i have none i'm going to go ahead and create one and so clicking on managing monitoring workspaces will bring you to the documentation but in order for me to get a workspace created i need to go to cloud monitoring now workspace is the top level container that is used to organize and control access to your monitoring notification channels in order for your notification channels to work they must belong to a monitoring workspace so you need to create at least one workspace before adding monitoring notification channels and don't worry we'll be getting into greater depth with regards to monitoring in a later section in this course so i'm going to go ahead and cancel this and i'm going to go up to the navigation menu click on there and scroll down to monitoring and then overview and this may take a minute to start up as the apis are being enabled and the default workspace for cloud monitoring is being built okay and now that the monitoring api has been enabled we are now in monitoring the workspace that was created is my first project so now that we have our monitoring workspace created i need to add the emails to the users that i want the alerts to be sent out to and added to the notification channel so in order to do that i'm going to go over here to alerting and up here at the top i'm going to click on edit notification channels and here as you can see are many notification channels that you can enable by simply clicking on add new over here on the right so now what i'm looking for is under email i'm going to click on add new now here i can add the new email address and so for me i'm going to add antony at antonyt.com and you can add whatever email address you'd like and under display name i'm going to add billing admin notification and just click on save and as you can see my email has been added to the notification channel and so this is all i needed to do in order to move on to the next step and so now that i've covered creating my monitoring workspace as well as adding another email to my email notification channels i can now go back to billing and finish off my budget alert let's go over here to budgets and alerts create budget and we're gonna go through the same steps call this billing alert users leave everything else as is and click on next i'm just going to change the target amount to 10 click on next i'm going to leave everything here as is and i'm going to go back to click on link monitoring email notification channels to this budget now if you notice when i click on select workspace my first project shows up and here it will ask me for my notification channels and because i've already set it up i can simply click on it and you'll see the billing admin notification channel and so if i didn't have this set up i can always go to manage notification channels and it'll bring me back to the screen which you saw earlier and so now that that's set up i can simply click on finish and so now that i have a regular budget alert i also have another budget alert that can go to a different email so if you have a project manager or a director that you want to send budget alerts to this is how you would do it and so that about covers this demo on committed use discounts reservations budgets and budget alerts and so that's all i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this short lesson i will be covering the exporting of your billing data so that you're able to analyze that data and understand your spend at a more granular level i will also be going through a short demo where i will show you how to enable the export billing feature and bring it into bigquery to be analyzed now cloud billing export to bigquery enables you to export granular google cloud billing data such as usage cost details and pricing data automatically to a bigquery data set that you specify then you can access your cloud billing data from bigquery for detailed analysis or use a tool like data studio to visualize your data just a quick note here that billing export is not retroactive and this should be taken into consideration when planning for analysis on this data and so there are two types of cloud billing data that you can export there's the daily cost detail data and the pricing data and these can be selected right within the console depending on your use case and so now that we've gone through exactly what billing export is i wanted to get into a demo and show you how to export your cloud billing data to bigquery and go through all the necessary steps to get it enabled so when you're ready join me in the console and so here we are back in the console and so in order to enable billing export i'm going to be going to the billing page so i'm going to move up to the top left hand corner to the navigation menu and click on billing here in the left hand menu you'll see billing export and you can just click on there and so for those just coming to billing export for the first time there's a quick summary of exactly what the bigquery export is used for and as we discussed earlier there is an option for the daily cost detail and for pricing and i'm going to use the daily cost detail in this demo and export that data to bigquery so the first step i'm going to do is to click on edit settings and it's going to bring me to a new page where it will ask me for my project and this is where my billing data is going to be stored but as you can see here i'm getting a prompt that says you need to create a bigquery data set first now the bigquery data set that is asking for is where the billing data is going to be stored so in order to move forward with my billing export i need to go to bigquery and set up a data set so i'm going to simply click on this button here that says go to bigquery and it's going to bring me to the bigquery page where i'll be prompted with a big welcome note you can just click on done and over here in the right hand side where it says create data set i'm just going to click on there and i'm going to create my new data set and so for my data set id i'm going to call this billing export and just as a note with the data set id you can't use any characters like hyphens commas or periods and therefore i capitalize the b and the e now with the data location the default location is the us multi region but i can simply click on the drop down and have an option to store my data in a different location but i'm going to keep it at default i have the option of expiring this table in either a certain amount of days or to never expire as well when it comes to encryption i'm going to leave it as google manage key as opposed to a customer manage key and i'll get into encryption and key management a little later on in this course i'm going to go ahead and move right down to the bottom and click on create data set and now my data set has been created i can now see it over here on the left hand side menu where subtle poet 28400 is the id for my project if i simply click on the arrow beside it it'll show my billing export data set because there's nothing in it nothing is showing and so now that the data set is set up i can now go back to the billing export page and finish setting up my billing export so with that being said i'm going to go back up to the navigation menu head over to billing and go to billing export under daily cost detail i'm going to click on edit settings and because i have a data set already set up and since it's the only one it has been propagated in my billing export data set field if i had more data sets then i would be able to select them here as well so i'm going to leave the data set at billing export and simply click on save and so now that billing export has been enabled i'll be able to check on my billing as it is updated each day as it says here and to go right to the data set i can simply click on this hot link and it'll bring me right to bigquery and so there is one last step that still needs to be done to enable the billing export to work and that is to enable the bigquery data transfer service api so in order to do that we need to go back to the navigation menu go into apis and services into the dashboard and now i'm going to do a search for the bigquery data transfer service and i'm going to simply go up here to the top search bar and simply type in bigquery and here it is bigquery data transfer api i'm going to simply click on that and hit enable and this might take a minute and you may be asked to create credentials over here on the top right and you can simply ignore that as they are not currently needed and so now that the bigquery data transfer service api has been enabled i'm now able to go over to bigquery and take a look at my billing export data without any issues now it's going to take time to propagate but by the time i come here tomorrow the data will be fully propagated and i'll be able to query the data as i see fit and so although this is a short demo this is necessary to know for the exam as well being an engineer and looking to query your billing data you will now have the knowledge in order to take the steps necessary that will allow you to do so and so that's all i have for this lesson and demo on export billing data so you can now mark this lesson as complete and let's move on to the next one welcome back in this handson demo i'm going to go over apis in google cloud now the google cloud platform is pretty much run on apis whether it's in the console or the sdk under the hood it's hitting the apis now some of you may be wondering what is an api well this is an acronym standing for application programming interface and it's a standard used amongst the programming community in this specific context it is the programming interface for google cloud services and as i said before both the cloud sdk and the console are using apis under the hood and it provides similar functionality now when using the apis directly it allows you to enable automation in your workflow by using the software libraries that you use for your favorite programming language now as seen in previous lessons to use a cloud api you must enable it first so if i went to compute engine or when i was enabling monitoring i had to enable the api so no matter the service you're requesting here in google cloud and some of them may be even linked together it always has to be enabled in order to use it now getting a little bit more granular when using an api you need to have a project so when you enable the api you enable it for your project using the permissions on the project and permissions on the api to enable it now since this is a demo i want to go over to the navigation menu and go straight into apis and services and so here is the dashboard of the apis and services you can see the traffic here the errors and the latency with regards to these apis as well up here it has a time frame for the median latency that you can select for a more granular search now when it comes to what is enabled already you can see list here of the apis that are enabled and since we haven't done much there's only a few apis that are enabled now this handson demo is not meant to go into depth with apis but is merely an overview so that you understand what the apis are used for in context with google cloud if you'd like to go more in depth with regards to apis and possibly get certified in it the apogee certification with its corresponding lessons would be a great way to get a little bit more understanding but for this demo we're going to stick with this overview and so in order to search for more apis that need to be enabled or if you're looking for something specific you can come up here to enable apis and services or you can do a quick search on the search bar at the top of the page but just as a quick glance i'm going to go into enable apis and services and so you will be brought to a new page where you will see the api library on the left you will see a menu where the apis are categorized and all the apis that are available when it comes to google cloud and other google services so as you saw before when i needed to enable the api for bigquery i would simply type in bigquery and i can go to the api and since the api is enabled there's nothing for me to do but if i needed to enable it i could do that right there and just as a quick note when going to a service that's available in the console the api automatically gets enabled when you go and use it for the first time and so again this is just a quick overview of apis and the api library with regards to google cloud a short yet important demo to understand the under workings of the cloud sdk and the console so just remember that when using any service in google cloud again you must enable the api in order to start using it and so that about wraps up this demo for cloud apis so you can now mark this lesson as complete and let's move on to the next one welcome back in this demo i'll be creating and setting up a new gmail user as an admin user for use moving ahead in this course as well as following google's best practices we need a user that has lesser privileges than the user account that we set up previously and i'll be going through a full demo to show you how to configure it now in a google cloud setup that uses a g suite or cloud identity account a super administrator account is created to administer the domain this super admin account has irrevocable administrative permissions that should not be used for daytoday administration this means that no permissions can be taken away from this account and has the power to grant organization admin role or any other role for that matter and recover accounts at the domain level which makes this account extremely powerful now since i do not have a domain setup or using a g suite or cloud identity account i don't need to worry about a super admin account in this specific environment as gmail accounts are standalone accounts that are meant to be personal and hold no organization and usually start at the project level and so to explain it in a bit more detail i have a diagram here showing the two different accounts i will be using and the structure behind it now as we discussed before billing accounts have the option of paying for projects in a different organization so when creating new projects using the two different gmail accounts they were created without any organization and so each account is standalone and can create their own projects now what makes them different is that the antony gcloud ace account owns the billing account and is set as a billing account administrator and the tony bowtie ace account is a billing account user that is able to link projects to that billing account but does not hold full access to billing so in the spirit of sticking to the principle of lease privilege i will be using the tony bowtie ace account that i had created earlier with lesser privileges on billing it will still give me all the permissions i need to create edit and delete resources without all the powerful permissions needed for billing i will be assigning this new gmail user the billing account user role and it will allow you to achieve everything you need to build for the remainder of the course so just as a review i will be using a new google account that i have created or if you'd like you can use a preexisting google account and as always i recommend enabling twostep verification on your account as this user will hold some powerful permissions to access a ton of different resources in google cloud so now that we've gone over the details of the what and why for setting up this second account let's head into the demo and get things started so whenever you're ready join me over in the console and so here i am back in the console and so before switching over to my new user i need to assign the specific roles that i will need for that user which is the billing account user role so to assign this role to my new user i need to head over to billing so i'm going to go back up here to the lefthand corner and click on the navigation menu and go to billing again in the lefthand menu i'm going to move down to account management and click on there and over here under my billing account you will see that i have permissions assigned to one member of the billing account administrator and as expected i am seeing anthony g cloud ace gmail.com and so i want to add another member to my billing account so i'm going to simply click on add members and here i will enter in my new second user which is tony bowtie ace gmail.com and under select a role i'm going to move down to billing and over to billing account user and as you can see here this role billing account user will allow permissions to associate projects with billing accounts which is exactly what i want to do and so i'm going to simply click on that and simply click on save and so now that i've assigned my second user the proper permissions that i needed i am now going to log out and log in as my new user by simply going up to the right hand corner in the icon clicking on the icon and going to add account by adding the account i'll be able to switch back and forth between the different users and i would only recommend this if you are the sole user of your computer if you are on a computer that has multiple users simply sign out and sign back in again with your different user and here i'm asked for the email which would be tony bowtie ace gmail.com i'm gonna plug in my password and it's going to ask me for my twostep verification i'm going to click on yes and i should be in and because it's my first time logging into google cloud with this user i get a prompt asking me to agree to the terms of service i'm going to agree to them and simply click on agree and continue and so now i'm going to move back up to overview and as you can see here i don't have the permissions to view costs for this billing account and so all the permissions assigned for the billing account administrator which is antony g cloud ace is not applied to tony bowtie ace and therefore things like budgets and alerts even billing exports i do not have access to so moving forward in the course if you need to access anything in billing that you currently don't have access to like budgets and alerts you can simply switch over to your other account and take care of any necessary changes but what i do have access to is if i go up here to my billing account click on the drop down menu and click on manage billing accounts but as you can see here i do have access to view all the billing accounts along with the projects that are linked to them now because these gmail accounts are standalone accounts this project here that is owned by antony gcloud ace i do not have access to in order to access the project i would have to have permissions assigned to me directly in order for me to actually view the project or possibly creating any resources within that project now if i go back to my home page i can see here that i have no projects available and therefore no resources within my environment and so to kick it off i'm going to create a new project and so under project name i am going to call this project tony and you can name your project whatever you'd like under location i don't have any organization and so therefore i'm just going to click on create and this may take a minute to create and here we are with my first project named project tony as well as my notification came up saying that my project has been created and so now that this project has been created it should be linked to my billing account so in order to verify this i'm going to go over into billing and under the drop down i'm going to click on manage billing accounts and as you can see here the number of projects has gone from one to two and if i click on the menu up here under my projects you can see that project tony is a project that is linked to my billing account i also have the permissions to either disable billing or change billing for this specific project yet in order to change billing i will have to have another billing account but there are no other billing accounts available and so moving forward i will only have this one billing account and so any projects i decide to create will be linked to this billing account and so this is a great example of trimming down the permissions needed for different users and even though this is not a domain owned account but a personal account it's always recommended to practice the principle of lease privilege whenever you come across assigning permissions to any user now as i said before any billing related tasks that you decide to do moving forward you can simply switch over to your other user and do the necessary changes and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this short lesson i'm going to be covering an overview of the cloud sdk and the command line interface as it is an essential component of interacting with google cloud for the exam you will need to get familiar with the command line and the commands needed in order to create modify and delete resources this is also an extremely valuable tool for your tool belt in the world of being a cloud engineer as i have found that is a very common and easy way to implement small operations within google cloud as well as automating the complex ones so what exactly is the cloud sdk well the cloud sdk is a set of command line tools that allows you to manage resources through the terminal in google cloud and includes commands such as gcloud gsutil bq and cubectl using these commands allow you to manage resources such as compute engine cloud storage bigquery kubernetes and so many other resources these tools can be run interactively or through automated scripts giving you the power and flexibility that you need to get the job done the cloud sdk is so powerful that you can do everything that the console can do yet has more options than the console you can use it for infrastructure as code autocompletion helps you finish all of your command line statements and for those of you who run windows the cloud sdk has got you covered with availability for powershell now in order to access google cloud platform you will usually have to authorize google cloud sdk tools so to grant authorization to cloud sdk tools you can either use a user account or a service account now a user account is a google account that allows end users to authenticate directly to your application for most common use cases on a single machine using a user account is best practice now going the route of a service account this is a google account that is associated with your gcp project and not a specific user a service account can be used by providing a service account key to your application and is recommended to script cloud sdk tools for use on multiple machines now having installed the cloud sdk it comes with some builtin commands that allow you to configure different options using gcloud init this initializes and authorizes access and performs other common cloud sdk setup steps using some optional commands gcloud auth login authorizes your access for gcloud with google user credentials and sets the current account as active gcloud config is another optional configuration that allows you to configure accounts and projects as well gcloud components allow you to install update and delete optional components of the sdk that give you more flexibility with different resources now after having installed the cloud sdk almost all gcloud commands will follow a specific format shown here is an example of this format and is broken down through component entity operation positional arguments and flags and i'll be going through some specific examples in the demonstration a little bit later on and so that's all i wanted to cover in this overview of the cloud sdk and the cli so you can now mark this lesson as complete and you can join me in the next one where i go ahead and demonstrate installing the cloud sdk back in this demonstration i will show you how to download install and configure the cloud sdk and i will be using the quick start guide that lies in the cloud sdk documentation which holds all the steps for installing the cloud sdk on different operating systems and i will make sure to include it in the lesson text below this demo will show you how to install the cloud sdk on each of the most common operating systems windows mac os and ubuntu linux all you need to do is follow the process on each of the pages and you should be well on your way so with that being said let's get this demo started and bring the cloud sdk to life by getting it all installed and configured for your specific operating system so as i explained before i'm gonna go ahead and install the cloud sdk on each of the three different operating systems windows mac os and ubuntu linux and i will be installing it with the help of the quick start guide that you see here and as i said before i'll be including this link in the lesson text and so to kick off this demo i wanted to start by installing the cloud sdk on windows so i'm going to move over to my windows virtual machine and i'm going to open up a browser and i'm going to paste in the link for the quick start guide and you can click on either link for the quick start for windows and each quick start page will give me the instructions of exactly what i need to do for each operating system so now it says that we need to have a project created which i did in the last lesson which is project tony so next i'm going to download the cloud sdk installer so i'm going to click on there and i'll see a prompt in the bottom left hand corner that the installer has been downloaded i'm going to click on it to open the file and i'm going to be prompted to go through this wizard and so i'm just going to click on next i'm going to agree to the terms of the agreement it's going to be for just me anthony and my destination folder i'll keep it as is and here's all the components that it's going to install i'm going to keep the beta commands unchecked as i don't really need them and if i need them later then i can install that component for those who are more experienced or even a bit curious you could click on the beta commands and take it for a test drive but i'm going to keep it off and i'm going to click install and depending on the power of your machine it should take anywhere from two to five minutes to install and the google cloud sdk has been installed and so i'm just going to click on next and as shown here in the documentation you want to make sure that you have all your options checked off is to create a start menu shortcut a desktop shortcut you want to start the google cloud sdk shell and lastly you want to run gcloud init in order to initialize and configure the cloud sdk now i'm going to click on finish to exit the setup and i'm going to get a command shell that pops up and i'm just going to zoom in for better viewing and so it says here my current configuration has been set to default so when it comes to configuration this is all about selecting the active account and so my current active account is going to be set as the default account it also needed to do a diagnostic check just to make sure that it can connect to the internet so that it's able to verify the account and so now the prompt is saying you must log in to continue would you like to log in yes you can just click on y and then enter and it's going to prompt me with a new browser window where i need to log in using my current account so that i can authorize the cloud sdk so i'm going to log in with my tony bowtie ace account click on next type in my password again it's going to ask me for my twostep verification and i'm going to get a prompt saying that the google sdk wants to access my google account i'm going to click on allow and success you are now authenticated with the google cloud sdk and if i go back to my terminal i am prompted to enter some values so that i can properly configure the google cloud sdk so i'm going to pick a cloud project to use and i'm going to use project tony that i created earlier so i'm going to enter 1 and hit enter and again whatever project that you've created use that one for your default configuration and it states here that my current project has been set to project tony and again this configuration is called default so if i have a second configuration that i wanted to use i can call it a different configuration but other than that my google cloud sdk is configured and ready to use so just to make sure that it's working i'm going to run a couple commands i'm going to run the gcloud help command and as you can see it's given me a list of a bunch of different commands that i can run and to exit you can just hit ctrl c i'm going to run gcloud config list and this will give me my properties in my active configuration so my account is tony bowtie ace gmail.com i've disabled usage reporting and my project is project tony and my active configuration is set as default now don't worry i'm going to be covering all these commands in the next lesson and i'm going to be going into detail on how you can configure and add other users within your cloud sdk configuration so as we go deeper into the course i'm going to be using a lot more command line just so you can get familiar with the syntax and become a bit more comfortable with it so now that i've installed the cloud sdk on windows the process will be a little bit different when it comes to installation on the other operating systems but will be very similar when it comes to the configuration so now let's head over to mac os and install the cloud sdk there and so here we are in mac os and so the first thing i want to do is i want to open up a web browser and i want to go to the cloud sdk quick start page so i'm just going to paste in the url here and we're looking for the quick start for mac os and so you can either click on the menu from the left hand side or the menu here on the main page and so like i said before this installation is going to be a little bit different than what it was in windows and so there's a few steps here to follow and so the first step asks us if we have a project already created which we've already done and is project tony and so the next step tells us that the cloud sdk requires python and so we want to check our system to see if we have a supported version so in order to check our version we're going to use this command here python minus v and i'm going to copy that to my clipboard and then open up a terminal and i'm going to zoom in for better viewing and so i'm going to paste the command in here and simply click on enter and as you can see here i'm running python 2.7 but the starred note here says that the cloud sdk will soon move to python 3 and so in order to avoid having to upgrade later you'd want to check your version for python 3 and so you can use a similar command by typing in python 3 space minus capital v and as you can see i'm running version 3.7.3 and so moving back to the guide i can see here that it is a supportive version if you do not have a supportive version i will include a link on how to upgrade your version in the lesson text below and so now that i've finished off this step let's move on to the next one where i can download the archive file for the google cloud sdk again most machines will run the 64bit package so if you do have the latest operating system for mac os you should be good to go so i'm going to click on this package and it'll start downloading for me and once it's finished you can click on downloads and click on the file itself and it should extract itself in the same folder with all the files and folders within it and so just as another quick note google prefers that you keep the google cloud sdk in your home directory and so following the guide i'm going to do exactly that and so the easiest way to move the folder into your home directory is to simply drag and drop it into the home folder on the left hand menu it should be marked with a little house icon and nested under favorites i can now move into my home folder and confirm that it is indeed in here and so now moving to the last step which shows as optional the guide asks us to install a script to add cloud sdk tools to our path now i highly recommend that you install this script so that you can add the tools for command completion and i will get into command completion a little bit later on in the next couple of lessons and so here is the command that i need to run so i'm going to copy that to my clipboard again and i'm going to move back over to my terminal i'm going to clear my screen and so to make sure i'm in my home directory where the cloud sdk folder is i'm going to simply type ls and so for those who don't know ls is a linux command that will list all your files and folders in your current path and as you can see here the google cloud sdk is in my path and therefore i can run that script so i'm going to paste it in here and i'm going to hit enter and so a prompt comes up asking me whether or not i want to disable usage reporting and because i want to help improve the google cloud sdk i'm going to type in y for yes and hit enter and so as i was explaining before the cloud sdk tools will be installed in my path and so this is the step that takes care of it and so i'm going to type y and enter for yes to continue and usually the path that comes up is the right one unless you've changed it otherwise so i'm going to leave this blank and just hit enter and that's it i've installed the tools so now in order for me to run gcloud init i have to start a new shell as it says here for the changes to take effect so i'm going to go up here to the top left hand menu click on terminal and quit terminal and so now i can restart the terminal again i'm going to zoom in for better viewing and now i'm able to run gcloud init in order to initialize the installation again the prompt to do the diagnostic tests and i can see i have no network issues but it shows me that i have to login to continue i would like to log in so i'm going to type y for yes and hit enter and so a new browser has popped open prompting me to enter my email and password and so i'm going to do that now i'm going to authorize my account with twostep verification i'm not going to save this password and yes i want to allow the google cloud sdk to access my google account so i'm going to click on allow and it shows that i've been authenticated so now i'm going to move back to my terminal and so just as a note before we move forward in case you don't get a browser popup for you to log into your google account you can simply highlight this url copy it into your browser and it should prompt you just the same so moving right ahead it shows that i'm logged in as tonybowtieace gmail.com which is exactly what i wanted and it's asking me to pick a cloud project to use now i want to use project tony so i'm going to type in 1 and enter and that's it the cloud sdk has been configured and just to double check i'm going to run the gcloud config list command to show me my configuration and as you can see here my account is tonybowties gmail.com my disable usage reporting is equal to false and my project is project tony and again my active configuration is set as default and so that about covers the cloud sdk install for mac os and so finally i'm going to move over to ubuntu linux and configure the cloud sdk there and so here we are in ubuntu and like i did in the other operating systems i'm going to open up the browser and i'm going to paste in the url for the quick start guide and so we want to click on the quick start for debian and ubuntu and so again you have your choice from either clicking on the link on the left hand menu or the one here in the main menu and so following the guide it is telling us that when it comes to an ubuntu release it is recommended that the sdk should be installed on an ubuntu release that has not reached end of life the guide also asks to create a project if we don't have one already which we have already done and so now we can continue on with the steps and so since we are not installing it inside a docker image we're gonna go ahead and use the commands right here now you can copy all the commands at once by copying this to the clipboard but my recommendation is to install each one one by one so i'm going to copy this and i'm going to open up my terminal i'm going to zoom in for better viewing and i'm going to paste that command in and click on enter it's going to prompt me for my password and it didn't come up with any errors so that means it was successfully executed and so i'm going to move on to the next command i'm going to copy this go back over to my terminal and paste it in now for those of you who do not have curl installed you will be prompted to install it and given the command to run it so i'm going to copy and paste this command and click on enter i'm going to type in y for yes to continue and it's going to install it after a couple of minutes okay now that curl has been installed i'm able to run that command again i'm going to clear the screen first and that executed with no errors as well and so now moving on to the last command this command will download and install the google cloud sdk i am prompted to install some packages and so i'm going to type y for yes to continue so now it's going to download and install the necessary packages needed for the google cloud sdk and depending on the speed of your internet and the speed of your machine this could take anywhere from two to five minutes okay and the google cloud sdk has been installed and so now that the cloud sdk has been installed we can now initialize the configuration so i'm going to type in gcloud init again the prompt with the network diagnostics i'm going to type y for yes to log in and i'm going to get the prompt for my email and password i'm going to take care of my twostep verification and i'm going to allow the google cloud sdk to access my google account and success i am now authenticated and moving back to the terminal just to verify it and again i'm going to pick project tony as the cloud project to use and the cloud sdk has been configured as always i'm going to do a double check by running a gcloud config list and as expected the same details has come up and so this is a quick run through on all three operating systems windows mac os and ubuntu linux on how to install the google cloud sdk and this will help you get started with becoming more familiar and more comfortable using the command line interface and so that about wraps up for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in the last demo we went through a complete install of the cloud sdk and configured our admin account to be used within it in this demonstration i will be walking through how to manage the cloud sdk and this will involve how to utilize it and how to customize it to your environment as well as configuring our other user account so that we are able to apply switching configurations from one user to another and so i will be going through initializing and authorization configurations and properties installing and removing components as well as a full run through of the gcloud interactive shell so let's kick off this demo by diving into a preconfigured terminal with the sdk installed and configured with my second user tony bowtie ace gmail.com and so here i am in the mac os terminal and just be aware that it doesn't matter which operating system you're running as long as the sdk is installed and you have your user configured and so as you saw in the last lesson after you install the cloud sdk the next step is typically to initialize the cloud sdk by running the gcloud init command and this is to perform the initial setup tasks as well as authorizing the cloud sdk to use your user account credentials so that it can access google cloud and so in short it sets up a cloud sdk configuration and sets a base set of properties and this usually covers the active account the current project and if the api is enabled the default google compute engine region and zone now as a note if you're in a remote terminal session with no access to a browser you can still run the gcloud init command but adding a flag of dash dash console dash only and this will prevent the command from launching a browserbased authorization like you saw when setting up your last user so now even though i have a user already set up i can still run gcloud init and it will give me a couple different options to choose from so i can reinitialize this configuration with some new settings or i can create a new configuration now for this demo since we already have two users and to demonstrate how to switch between different users i want to create a new configuration with my very first user so i'm going to type in 2 and hit enter and it's going to ask me for a configuration name now it asks me for a configuration name because when setting up your first configuration it's set as default and because i know that this user account has full access to billing as well as administration privileges i'm going to call this configuration master and i'm going to hit enter it did the necessary network checks and now it's asking me for which account i want to use this configuration for now if tony bowtie ace had access to two different google cloud accounts i would be able to add a different configuration here and so because i'm going to log in with a new account i'm going to put in two and hit enter and so again it brought me to my browser window and i'm going to log in using another account and so here you can type in the first account that you created and for me it was antony gcloud ace gmail.com i hit next and i'm going to enter my password it's going to ask me for my twostep verification and i don't want to save this password and i'm going to allow the google cloud sdk to access my google account and i am now authenticated so moving back to the console you can see here that i am currently logged in and it's asking me to pick a cloud project to use now since i only have one project in that google cloud account which is subtle poet i'm going to choose one and since i have the compute engine api enabled i am now able to configure a default compute region and zone and so i'm going to hit y for yes to configure it and as you can see there are 74 different options to choose from and if you scroll up a little bit you should be able to find the zone that you're looking for and so for this course we are going to be using us central one dash a and so this is number eight so i'm going to scroll back down and type in eight and so now my master configuration has been configured with my antony g cloud ace account using us central 1a as the compute engine zone now touching back on authorization if i didn't want to set up a whole configuration i can simply type in gcloud auth login and this will allow me to authorize just the user account only so gcloud init would authorize access and perform the cloud sdk setup steps and gcloud auth login will authorize the access only now as i mentioned in a previous lesson you can use a service account for authorization to the cloud sdk tools and this would be great for a compute instance or an application but would need a service account key file in order to authorize it and so moving back to our user accounts when running the cloud sdk you can only have one active account at any given time and so to check my active account i can type in the command gcloud auth list and this will give me a list of all the accounts that have been authorized and so whenever you run a gcloud init it will use that account as the active account and as you can see here the antony gcloud ace gmail.com has a star beside it and this is marked as the active account and so in essence the account with the star beside it is the active account and so i'm looking to change my active account back to tony bowtie ace and in order for me to do that the command is conveniently shown here and so i'm going to go ahead and run that and the account would be the user shown above and so when i do a gcloud auth list i can see that my active account is now back to tony bowtie bowtieace gmail.com now if you wanted to switch the account on a per command basis you can always do that using the flag dash dash account after the command and put in the user account that you want to use and so let's say i wanted to revoke credentials from an account that i don't need anymore i can simply use the command gcloud auth revoke followed by the username and it will revoke the credentials for that account and so doing this would remove your credentials and any access tokens for any specific account that you choose that's currently on your computer and so if we're looking for that specific account we can always use the gcloud info command and it will give us the path for the user config directory and it is this directory that holds your encrypted credentials and access tokens alongside with your active configurations and any other configurations as well now as you can see here running the gcloud info command will also give you some other information everything from the account the project the current properties and where the logs can be found so now moving on to configurations a configuration is a named set of gcloud cli properties and it works kind of like a profile and so earlier on i demonstrated how to set up another configuration through gcloud init so now if i run a gcloud config list command it would give me all the information of the active configuration so as you can see here my user has changed but my configuration has stayed the same now as seen previously in a different lesson tony bow tie ace does not have access to the project subtle poet this project belongs to antony g cloud ace and the configuration was set for that account now if tony bowtie ace did have access to the subtle poet project then i could use this configuration but it doesn't and so i want to switch back to my other configuration and how i would do this is type in the command gcloud config configurations activate and the configuration that i set up for tony bowtie ace is the default configuration and so now that it has been activated i can now run a gcloud config list and as you can see here the configuration is back to default setup during the initialization process for tony bowtie ace now if i wanted to create multiple configurations for the same user account i can simply type in the command gcloud config configurations create but if i wanted to just view the configuration properties i can always type in the command gcloud config configurations describe and as you can see after the describe i needed the configuration name to complete the command and so i'm going to do that now and i've been given all the properties for this configuration now another thing that i wanted to share when it comes to properties is that you can change the project or the compute region and zone by simply typing in the command gcloud config set now if i wanted to change the project i can simply type in project and the project name if it was for the compute instance i can simply type in compute forward slash zone for the specific zone and just as a note only the properties that are not in the core property section are the ones that can be set as well when you are setting the properties this only applies to the active configuration if you want to change the configuration of one that is not active then you'd have to switch to it and run the gcloud config set command and so moving on i wanted to touch on components which are the installable parts of the sdk and when you install the sdk the components gcloud bq gsutil and the core libraries are installed by default now you probably saw a list of components when you ran the gcloud init command and so to see all the components again you can simply type in the gcloud components list command and if you scroll up you're able to see all the components that are available that you can install at your convenience and so if i wanted to install the cubectl component i can type in the command gcloud components install cubectl and a prompt will come up asking me if i want to continue with this i want to say yes and now it will go through the process of installing these components and so just to verify if i run the command gcloud components list you can see here that i have the cube ctl component installed now if i wanted to remove that component i can simply type in gcloud components remove and then the component that i want to remove which is cubectl i'm going to be prompted if i want to do this i'm going to say yes and it's going to go through the stages of removing this component and it's been successfully uninstalled and so if you're working with a resource that you need a component for you can simply install or uninstall it using the gcloud components command and so one last thing about components before we move on is that you can update your components to make sure you have the latest version and so in order to update all of your installed components you would simply run the command gcloud components update and so before i go ahead and finish off this demonstration i wanted to touch on the gcloud interactive shell the gcloud interactive shell provides a richer shell experience simplifying commands and documentation discovery with as you type autocompletion and help text snippets below it produces suggestions and autocompletion for gcloud bq gsutil and cubectl command line tools as well as any command that has a man page sub commands and flags can be completed along with online help as you type the command and because this is part of the beta component i need to install it and so i'm going to run the command gcloud components install beta and i want to hit yes to continue and this will go ahead and kick off the installation of the gcloud beta commands and so now that it's installed i'm going to simply clear the screen and so now in order to run the gcloud interactive shell i need to run the command gcloud beta interactive and so now for every command that i type i will get auto suggestions that will help me with my commands and so to see it in all of its glory i'm going to start typing and as you can see it's giving me the option between g cloud or gsutil and i can use the arrow to choose either one and below it it'll also show me the different flags that i can use for these specific commands and how to structure them and so for now i'm going to run gsutil version minus l and as you can see here it's giving me all the information about this command and what it can do and so i'm going to hit enter and as you can see my gsutil version is 4.52 and along with the version number i'm also given all the specific information with regards to this gsutil version and this can be used with absolutely any command used on the google cloud platform and so i'm going to go ahead and do that again but running a different command so i'm just going to first clear the screen and i'm going to type gcloud compute instances and as you can see the snippet on the bottom of the screen is showing me not only the command and how it's structured but also the url for the documentation so continuing on gcloud compute instances i'm going to do a list and i'm going to filter it by using the flag dash dash filter and i'm going to filter the us east one a zone and i'm going to hit enter and as expected there are no instances in us east 1a and as you've just experienced this is a great tool and i highly recommend that you use it whenever you can now i know this is a lot to take in and a lot of these commands will not show up on the exam but again getting comfortable with the command line and the sdk will help you on your path to becoming a cloud engineer as well it will help you get really comfortable with the command line and before you know it you'll be running commands in the command line and prefer it over using the console and so that's all i have for this demo on managing the cloud sdk so you can now mark this lesson as complete and let's move on to the next one welcome back in this demonstration i'm going to be talking about the always available browserbased shell called cloud shell cloud shell is a virtual machine that is loaded with development tools and offers a persistent five gigabyte home directory that runs on google cloud cloud shell is what provides you command line access to your google cloud resources within the console cloud shell also comes with a builtin code editor that i will be diving into and allows you to browse file directories as well as view and edit files while still accessing the cloud shell the code editor is available by default with every cloud shell instance and is based on the open source editor thea now cloud shell is available from anywhere in the console by merely clicking on the icon showed here in the picture and is positioned in the top right hand corner of the console in the blue toolbar so let's get started with the cloud shell by getting our hands dirty and jumping right into it and so here we are back in the console and i am logged in as tony bowtie ace gmail.com and as you can see up here in the right hand corner as mentioned earlier you will find the cloud shell logo and so to open it up you simply click on it and it'll activate the cloud shell here at the bottom and because it's my first time using cloud shell i'll get this prompt quickly explaining an overview of what cloud shell is and i'm going to simply hit continue and i'm going to make the terminal a little bit bigger by dragging this line up to the middle of the screen and so when you start cloud shell it provisions an e2 small google compute engine instance running a debianbased linux operating system now this is an ephemeral preconfigured vm and the environment you work with is a docker container running on that vm cloud shell instances are provisioned on a per user per session basis the instance persists while your cloud shell session is active and after an hour of inactivity your session terminates and the vm is discarded you can also customize your environment automatically on boot time and it will allow you to have your preferred tools when cloud shell boots up so when your cloud shell instance is provision it's provisioned with 5 gigabytes of free persistent disk storage and it's mounted at your home directory on the virtual machine instance and you can check your disk storage by simply typing in the command df minus h and here where it shows dev disk by id google home part one it shows here the size as 4.8 gigabytes and this would be the persistent disk storage that's mounted on your home directory now if you've noticed it shows here that i'm logged in as tony bowtie ace at cloud shell and that my project id is set at project tony so the great thing about cloud shell is that you're automatically authenticated as the google account you're logged in with so here you can see i'm logged in as tony bowtie ace and so picture it like running gcloud auth login and specifying your google account but without having to actually do it now when the cloud shell is started the active project in the console is propagated to your gcloud configuration inside cloud shell so as you can see here my project is set at project tony now if i wanted to change it to a different project i could simply use the command stated up here gcloud config set project along with the project id and this will change me to a different project now behind the scenes cloud shell is globally distributed across multiple regions so when you first connect to cloud shell you'll be automatically assigned to the closest available region and thus avoiding any unnecessary latency you do not have the option to choose your own region and so cloud shell does that for you by optimizing it to migrate to a closer region whenever it can so if you're ever curious where your cloud shell session is currently active you can simply type in this command curl metadata slash compute metadata slash version one slash instance slash zone and this will give me the zone where my instance is located and as shown here it is in us east 1b now as you've probably been seeing every time i highlight something that there is a picture of scissors coming up the cloud shell has some automated and available tools that are built in and so one of those available tools is that whenever i highlight something it will automatically copy it to the clipboard for me cloud shell also has a bunch of very powerful preinstalled tools that come with it such as the cloud sdk bash vim helm git docker and more as well cloud shell has support for a lot of major different programming languages like java go python node.js ruby and net core for those who run windows now if you're looking for an available tool that is not preinstalled you can actually customize your environment when your instance boots up and automatically run a script that will install the tool of your choice and the script runs as root and you can install any package that you please and so in order for this environment customization to work there needs to be a file labeled as dot customize underscore environment now if we do an ls here you can see that all we have is the readme dash cloud shell text file if we do ls space minus al to show all the hidden files as well you can see that the dot customize underscore environment file does not exist and this is because we need to create it ourselves and so for this example i want terraform installed as an available tool when my instance boots up and so i have to create this file so i'm going to do so by using the touch command and then the name of the file dot customize underscore environment hit enter and if i clear the screen and do another ls space minus al i can see that my dot customize underscore environment file has been created and so now i'm going to need the script to install terraform which means i would have to edit it and so another great feature of cloud shell is that it comes with a code editor and i can do it one of two ways i can either come up here and click on the open editor button which will open up a new tab or i can simply use the edit command with the file name and i'm going to do just that so edit dot customize underscore environment and i'm just going to hit enter and as you can see i got a prompt saying that it's unable to load the code editor and this is because when using code editor you need cookies enabled on your browser and because i am using a private browser session cookies are disabled and because my cloud shell environment persists i'm going to open up a regular browser window and i'm going to continue where i left off and so here i am back with a new browser window again logged in as tony bowtie ace and so just to show you the persistence that happens in cloud shell i'm going to run the command ls space minus al and as you can see here the customize environment is still here and so again i wanted to install terraform as an extra tool to have in my environment and so i'm going to open up the editor by typing in edit dot customize underscore environment and i'm going to hit enter and here is the editor that popped up as you can see here it's built with eclipse thea and this is an open source code editor that you can download from eclipse and this is what the editor is built on now this menu here on the left i can make it a little bit bigger and because the only viewable file on my persistent disk is the readme cloud shell dot text file i'm not able to see my dot customize underscore environment so in order to open it and edit it i'm going to go to the menu at the top of the editor and click on file open and here i'll be able to select the file that i need so i'm going to select customize environment and click on open and so i'm going to paste in my script to install terraform and i'm just going to paste in my script from my clipboard and i'll be including the script in the github repo for those of you who use terraform and i'm going to move over to the menu on the left click on file and then hit save and so now in order for me to allow this to work the customize environment needs to be loaded into my cloud shell so i'm going to have to restart it and so in order to accomplish this i'm going to move over to the menu on the right i'm going to click on the icon with the three dots and click on restart and you'll be presented with a prompt it's saying that it will immediately terminate my session and then a new vm will be provisioned for me and you'll also be presented with an optional response from google telling them why you're restarting the vm and this is merely for statistical purposes so i'm going to click on restart and i'm going to wait till a new cloud shell is provisioned and my new cloud shell is provisioned and up and running and so i want to double check to see if terraform has been installed so i'm going to go over here to the open terminal button on the right hand side toolbar and i'm going to move back to my terminal and i'm going to simply run the command terraform dash dash version and so it looks like terraform has been installed and as you can see i'm running version.12 but it says my terraform version is out of date and that the latest version is dot 13. and so because i really want to be up to date with terraform i want to be able to go into my customize environment file and edit my version of terraform so that when my cloud shell is initiated terraform.13 can be installed and so i'm going to simply type in the command edit dot customize underscore environment and i'm back to my editor and i'm going to change the terraform version from dot 12 to dot 13 and then go over here to the lefthand menu click on file and then save and now i'm going to restart my machine again and come back when it's fully provisioned and i'm back again my machine has been provisioned and i'm going to go back to my terminal by clicking on the open terminal button and so i'm going to type in the command terraform dash dash version and as you can see i'm at version dot 13 and i'm going to run a simple terraform command to see if it's working and as you can see i am successful in running terraform on cloud shell now customizing the environment is not on the exam but it is such an amazing feature that i wanted to highlight it for you with a real world example like terraform in case you're away from your computer and you're logged into a browser and you need some special tools to use in cloud shell this is the best way to do it now as i mentioned before the cloud sdk is preinstalled on this and so everything that i've showed you in the last lesson with regards to cloud sdk can be done in the cloud shell as well so if i run the command gcloud beta interactive i'd be able to bring up the interactive cloud shell and i'll be able to run the same commands so now if i go ahead and run the command gcloud components list i'll be able to see all the components installed and as you can see with the cloud shell there are more components installed than what's installed on the default installation of the sdk i can also run the gcloud config list command to see all the properties in my active configuration and so this goes to show you that the sdk installation that's on cloud shell is just as capable as the one that you've installed on your computer the only difference here is that the sdk along with all the other tools that come installed in cloud shell is updated every week and so you can always depend that they're up to date and so moving on to a few more features of cloud shell i wanted to point out the obvious ones up here in the cloud shell toolbar right beside the open terminal i can open brand new tabs opening up different projects or even the same project but just a different terminal and moving over to the right hand menu of cloud shell this keyboard icon can send key combinations that you would normally not have access to moving on to the gear icon with this you're able to change your preferences and looking at the first item on the list when it comes to color themes you can go from a dark theme to a light theme or if you prefer a different color in my case i prefer the dark theme as well you have the options of changing your text size we can go to largest but i think we'll just keep things back down to medium and as well we have the different fonts the copy settings from which i showed you earlier as well as keyboard preferences you also have the option of showing your scroll bar now moving on to this icon right beside the gear is the web preview button and so the web preview button is designed so that you can run any web application that listens to http requests on the cloud shell and be able to view it in a new web browser tab when running these web applications web preview also supports applications run in app engine now mind you these ports are only available to the secure cloud shell proxy service which restricts access over https to your user account only and so to demonstrate this feature i am going to run a simple http server running a hello world page so first i'm going to clear my screen and then i'm going to exit the interactive shell and again i'm going to paste in for my clipboard a simple script that will run my simple http server and as you can see it's running on port 8080 and now i'm able to click on the web preview button and i'm able to preview it on port 8080 and a new web browser tab will open up and here i'll see my hello world page now this is just a simple example and so i'm sure that many of you can find great use for this and so i'm going to stop this http server now by hitting ctrl c and just as a quick note web preview can also run on a different port anywhere from port 2000 all the way up to 65 000. now moving on to the rest of the features hitting on the more button here with the three dots starting from the top we covered restart earlier when we had to restart our cloud shell you're able to both upload and download a file within cloud shell when the demands are needed as well if i have a misconfigured configuration i can boot into safe mode and fix the issue instead of having to start from scratch again moving on to boost cloud shell also known as boost mode is a feature that increases your cloud shell vm from the default e2 small to an e2 medium so in essence a memory bump from 2 gigabytes to 4 gigabytes and once it's activated all your sessions will be boosted for the next 24 hours and just as a quick note enabling boost mode restarts your cloud shell and immediately terminates your session but don't worry the data in your home directory will persist but any of the processes that you are running will be lost now when it comes to usage quota cloud shell has a 50 hour weekly usage limit so if you reach your usage limit you'll need to wait until your quota is reset before you can use cloud shell again so it's always good to keep your eyes on this in case you're a heavy user of cloud shell and moving back to the menu again you have your usage statistics which collects statistics on commands that come preinstalled in the vm and you can turn them on or off and as well help for cloud shell is available here as well if you wanted to give feedback to the google cloud team with regards to cloud shell this is the place to do it and so one last thing about cloud shell before we end this demo is that if you do not access cloud shell for 120 days your home disk will be deleted now don't worry you'll receive an email notification before its deletion and if you just log in and start up a session you'll prevent it being removed now moving ahead in this course i will be using cloud shell quite a bit and so feel free to use either cloud shell or the cloud sdk installed on your computer or feel free to follow along with me in the cloud shell within your google cloud environment and so if you are following along please make sure that you keep an eye on your quota and so i hope this demonstration has given you some really good insight as to what you can do with cloud shell and its limitations and so that's pretty much all i wanted to cover in this demonstration of cloud shell so you can now mark this as complete and let's move on to the next one welcome back in this lesson and demonstration i am going to go over limits and quotas and how they affect your cloud usage within google cloud i'm going to quickly go over some theory followed by a demonstration on where to find the quotas and how to edit them accordingly so google cloud enforces quotas on resource usage for project owners setting a hard limit on how much of a particular google cloud resource your project can use and so there are two types of resource usage that google limits with quota the first one is rate quota such as api requests per day this quota resets after a specified time such as a minute or a day the second one is allocation quota an example is the number of virtual machines or load balancers used by your project and this quota does not reset over time but must be explicitly released when you no longer want to use the resource for example by deleting a gke cluster now quotas are enforced for a variety of reasons for example they protect other google cloud users by preventing unforeseen usage spikes quotas also help with resource management so you can set your own limits on service usage within your quota while developing and testing your applications each quota limit is expressed in terms of a particular countable resource from requests per day to an api to the number of load balancers used by your application not all projects have the same quotas for the same services and so using this free trial account you may have very limited quota compared to a higher quota on a regular account as well with your use of google cloud over time your quotas may increase accordingly and so you can also request more quota if you need it and set up monitoring and alerts and cloud monitoring to warn you about unusual quota usage behavior or when you're actually running out of quota now in addition to viewing basic quota information in the console google cloud lets you monitor quota usage limits and errors in greater depth using the cloud monitoring api and ui along with quota metrics appearing in the metrics explorer you can then use these metrics to create custom dashboards and alerts letting you monitor quota usage over time and receive alerts when for example you're near a quota limit only your services that support quota metrics are displayed and so popular supported services include compute engine data flow cloud spanner cloud monitoring and cloud logging common services that are not supported include app engine cloud storage and cloud sql now as a note be aware that quota limits are updated once a day and hence new limits may take up to 24 hours to be reflected in the google cloud console if your project exceeds a particular quota while using a service the platform will return an error in general google cloud will return an http 429 error code if you're using http or rest to access the service or resource exhausted if you're using grpc if you're using cloud monitoring you can use it to identify the quota associated with the error and then create custom alerts upon getting a quota error and we will be going into greater depth with regards to monitoring later on in the course now there are two ways to view your current quota limits in the google cloud console the first is using the quotas page which gives you a list of all of your project's quota usage and limits the second is using the api dashboard which gives you the quota information for a particular api including resource usage over time quota limits are also accessible programmatically through the service usage api and so let's head into the console where i will provide a demonstration on where to look for quotas and how to increase them when you need to and so here we are back in the console and so as i explained before there are two main ways to view your current quota limits in the console and so the first one is using the quotas page and so in order to get to the quotas page i need to go to iam so i'm going to do that now by going up to the navigation menu in the top left hand corner i'm going to go to i am and admin and over to quotas and so here i am shown all the quotas of the current apis that i have enabled as you can see here it shows me the service the limit name the quota status and the details in this panel here on the right hand side shows me a little bit more information with regards to the service and the quota itself and so let's say i wanted to increase my quota on the compute engine api within networks so i'm going to select this service and over here on the right hand panel i'm going to tick the box that says global and i'm going to go back over here to the top left and click on the edit quotas button and a panel will pop up and i am prompted to enter a new quota limit along with a description explaining to google why i need this quota limit increase and so once i've completed my request i can click on done and then submit request and like i said before once the request has been submitted it will go to somebody at google to evaluate the requests for approval and don't worry these quota limit increases are usually approved within two business days and can often times be sooner than that also a great way to enter multiple quota changes is to click on the selected apis let's do bigquery api and cloud data store api and so i've clicked off three and now i can go back up to the top and click on the edit quotas button and as you can see in the panel i have all three apis that i want to increase my quotas on so i can enter all my new limit requests for each api and then i can submit it as a bulk request with all my new quota limit changes and so doing it this way would increase the efficiency instead of increasing the quotas for each service one by one and because i'm not going to submit any quota changes i'm going to close this panel and so again using the quotas page will give you a list of all your project quota usage and its limits and allow you to request changes accordingly and so now moving on to the second way which you can view your current quota limits i'm going to go to the api dashboard which will give me a more granular view including the resource usage over time so to get there i'm going to go back up to the left hand side to the navigation menu i'm going to go to apis and services and click on dashboard and here i will see all the names of the apis and i'm going to click on compute engine api for this demonstration and over here on the left hand menu you will see quotas and in here as i said before you can get some really granular data with regards to queries read requests list requests and a whole bunch of other requests i'm going to drill down into queries here and i can see my queries per day per 100 seconds per user and per 100 seconds and i can see here that my queries per 100 seconds is at a limit of 2 000 so if i wanted to increase that limit i can simply click on the pencil icon and a panel on the right hand side will prompt me to enter a new quota limit but i currently see that my quota limit is at its maximum and that i need to apply for a higher quota so when i click on the link it will bring me back to my iam page where my services are filtered and i can easily find the service that i was looking at to raise my quota limit and i can increase the quota by checking off this box and clicking on the edit quotas button at the top of the page and so as you can see the quotas page as well as the api dashboard work in tandem so that you can get all the information you need with regards to quotas and limits and to edit them accordingly and so i hope this gave you a good idea and some great insight on how you can view and edit your quotas and quota limits according to the resources you use and so that about wraps up this brief yet important demo on limits and quotas so you can now mark this as complete and let's move on to the next section welcome back and in this section we're going to be going through in my opinion one of the most important services in google cloud identity and access management also known as iam for short and i'll be diving into identities roles and the architecture of policies that will give you a very good understanding of how permissions are granted and how policies are inherited so before i jump into i am i wanted to touch on the principle of least privilege just for a second now the principle of least privilege states that a user program or process should have access to the bare minimum privileges necessary or the exact resources it needs in order to perform its function so for example if lisa is performing a create function to a cloud storage bucket lisa should be restricted to create permissions only on exactly one cloud storage bucket she doesn't need read edit or even delete permissions on a cloud storage bucket to perform her job and so this is a great illustration of how this principle works and this is something that happens in not only google cloud but in every cloud environment as well as any onpremises environment so note that the principle of least privilege is something that i have previously and will continue to be talking about a lot in this course and this is a key term that comes up quite a bit in any major exam and is a rule that most apply in their working environment to avoid any unnecessary granted permissions a wellknown and unsaid rule when it comes to security hence me wanting to touch on this for a brief moment so now with that out of the way i'd like to move on to identity and access management or i am for short so what is it really well with iam you manage access control by defining who the identity has what access which is the role for which resource and this also includes organizations folders and projects in iam permission to access a resource isn't granted directly to the end user instead permissions are grouped into roles and roles are then granted to authenticated members an iam policy defines and enforces what roles are granted to which members and this policy is attached to a resource so when an authenticated member attempts to access a resource iam checks the resources policy to determine whether the action is permitted and so with that being said i want to dive into the policy architecture breaking it down by means of components in this policy architecture will give you a better understanding of how policies are put together so now what is a policy a policy is a collection of bindings audit configuration and metadata now the binding specifies how access should be granted on resources and it binds one or more members with a single role and any contact specific conditions that change how and when the role is granted now the metadata includes additional information about the policy such as an etag and version to facilitate policy management and finally the audit config field specifies the configuration data of how access attempts should be audited and so now i wanted to take a moment to dive deeper into each component starting with member now when it comes to members this is an identity that can access a resource so the identity of a member is an email address associated with a user service account or google group or even a domain name associated with a g suite or cloud identity domains now when it comes to a google account this represents any person who interacts with google cloud any email address that is associated with a google account can be an identity including gmail.com or other domains now a service account is an account that belongs to your application instead of an individual end user so when you run your code that is hosted on gcp this is the identity you would specify to run your code a google group is a named collection of google accounts and can also include service accounts now the advantages of using google groups is that you can grant and change permissions for the collection of accounts all at once instead of changing access one by one google groups can help you manage users at scale and each member of a google group inherits the iam roles granted to that group the inheritance means that you can use a group's membership to manage users roles instead of granting iam roles to individual users moving on to g suite domains this represents your organization's internet domain name such as antonyt.com and when you add a user to your g suite domain a new google account is created for the user inside this virtual group such as antony antonyt.com a g suite domain in actuality represents a virtual group of all of the google accounts that have been created like google groups g suite domains cannot be used to establish identity but they simply enable permission management now a cloud identity domain is like a g suite domain but the difference is that domain users don't have access to g suite applications and features so a couple more members that i wanted to address is the all authenticated users and the all users members the all authenticated users is a special identifier that represents anyone who is authenticated with a google account or a service account users who are not authenticated such as anonymous visitors are not included and finally the all users member is a special identifier that represents anyone and everyone so any user who is on the internet including authenticated and unauthenticated users and this covers the slew of the different types of members now touching on the next component of policies is roles now diving into roles this is a named collection of permissions that grant access to perform actions on google cloud resources so at the heart of it permissions are what determines what operations are allowed on a resource they usually but not always correspond onetoone with rest methods that is each google cloud service has an associated permission for each rest method that it has so to call a method the caller needs that permission now these permissions are not granted to the users directly but grouped together within the role you would then grant roles which contain one or more permissions you can also create a custom role by combining one or more of the available iam permissions and again permissions allow users to perform specific actions on google cloud resources so you will typically see a permission such as the one you see here compute.instances.list and within google cloud iam permissions are represented in this form service.resource.verb so just as a recap on roles this is a collection of permissions and you cannot grant a permission directly to the user but you grant a role to a user and all the permissions that the role contains so an example is shown here where the compute instances permissions are grouped together in a role now you can grant permissions by granting roles to a user a group or a service account so moving up into a more broader level there are three types of roles in iam there are the primitive roles the predefined roles and the custom roles with the primitive roles these are roles that existed prior to the introduction of iam and they consist of three specific roles owner editor and viewer and these roles are concentric which means that the owner role includes the permissions in the editor role and the editor role includes the permissions in the viewer role and you can apply primitive roles at the project or service resource levels by using the console the api and the gcloud tool just as a note you cannot grant the owner role to a member for a project using the iam api or the gcloud command line tool you can only add owners to a project using the cloud console as well google recommends avoiding these roles if possible due to the nature of how much access the permissions are given in these specific roles google recommends that you use predefined roles over primitive roles and so moving into predefined roles these are roles that give granular and finergrained access control than the primitive roles to specific google cloud resources and prevent any unwanted access to other resources predefined roles are created and maintained by google their permissions are automatically updated as necessary when new features or services are added to google cloud now when it comes to custom roles these are user defined and allow you to bundle one or more supported permissions to meet your specific needs unlike predefined roles custom roles are not maintained by google so when new permissions features or services are added to google cloud your custom roles will not be updated automatically when you create a custom role you must choose an organization or project to create it in you can then grant the custom role on the organization or project as well as any resources within that organization or project and just as a note you cannot create custom roles at the folder level if you need to use a custom role within a folder define the custom role on the parent of that folder as well the custom roles user interface is only available to users who have permissions to create or manage custom roles by default only project owners can create new roles now there is one limitation that i wanted to point out and that is that some predefined roles contain permissions that are not permitted in custom roles so i highly recommend that you check whether you can use a specific permission when making a custom role custom roles also have a really cool feature that includes a launch stage which is stored in the stage property for the role the stage is informational and helps you keep track of how close each role is to being generally available and these launch stages are available in the stages shown here alpha which is in testing beta which is tested and awaiting approval and of course ga which is generally available and i'll be getting handson later with these roles in an upcoming demonstration so now moving on to the next component is conditions and so a condition is a logic expression and is used to define and enforce conditional attributebased access control for google cloud resources conditions allow you to choose granting resource access to identities also known as members only if configured conditions are met for example this could be done to configure temporary access for users that are contractors and have been given specific access for a certain amount of time a condition could be put in place to remove the access they needed once the contract has ended conditions are specified in the role bindings of a resources im policy so when a condition exists the access request is only granted if the condition expression is true so now moving on to metadata this component carries both e tags and version so first touching on e when multiple systems try to write to the same im policy at the same time there is a risk that those systems might overwrite each other's changes and the risk exists because updating an im policy involves multiple operations so in order to help prevent this issue iam supports concurrency control through the use of an etag field in the policy the value of this field changes each time a policy is updated now when it comes to a version this is a version number that is added to determine features such as a condition and for future releases of new features it is also used to avoid breaking your existing integrations on new feature releases that rely on consistency in the policy structure also when new policy schema versions are introduced and lastly we have the auditconfig component and this is used in order to configure audit logging for the policy it determines which permission types are logged and what identities if any are exempted from logging and so to sum it up this is a policy in all its entirety each component as you can see plays a different part and i will be going through policies and how they are assembled in statements in a later lesson and so there is one more thing that i wanted to touch on before ending this lesson and that is the policy inheritance when it comes to resource hierarchy and so as explained in an earlier lesson you can set an im policy at any level in the resource hierarchy the organization level the folder level the project level or the resource level and resources inherit the policies of all their parent resources the effective policy for a resource is the union of the policy set on that resource and the policies inherited from higher up in the hierarchy and so again i wanted to reiterate that this policy inheritance is transitive in other words resources inherit policies from the project which inherit policies from folders which inherit policies from the organization therefore the organization level policies also apply at the resource level and so just a quick example if i apply a policy on project x on any resources within that project the effective policy is going to be a union of these policies as the resources will inherit the policy that is granted to project x so i hope this gave you a better understanding of how policies are granted as well as the course structure and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i wanted to build on the last lesson where we went through iam and policy architecture and dive deeper into policies and conditions when it comes to putting them together in policy statements as cloud engineers you should be able to read and decipher policy statements and understand how they're put together by using all the components that we discussed earlier so just as a refresher i wanted to go over the policy architecture again now as i discussed previously a policy is a collection of statements that define who has what type of access it is attached to a resource and is used to enforce access control whenever that resource is accessed now the binding within that policy binds one or more members with a single role and any context specific conditions so in other words the member roles and conditions are bound together using a binding combined with the metadata and audit config we have a policy so now taking all of this and putting it together in a policy statement shown here you can see the bindings which have the role the members and conditions the first member being tony beauties gmail.com holding the role of storage admin and the second member as larkfetterlogin at gmail.com holding the role of storage object viewer now because lark only needs to view the files for this project in cloud storage till the new year a condition has been applied that does not grant access for lark to view these files after january the 1st an e tag has been put in and the version is numbered 3 due to the condition which i will get into a little bit later this policy statement has been structured in json format and is a common format used in policy statements moving on we have the exact same policy statement but has been formatted in yaml as you can see the members roles and conditions in the bindings are exactly the same as well as the etag and version but due to the formatting it is much more condensed so as you can see policy statements can be written in both json or yaml depending on your preference my personal preference is to write my policy statements in yaml due to the shorter and cleaner format so i will be moving ahead in this course with more statements written in yaml when you are looking to query your projects for its granted policies an easy way to do this would be to query it from the command line as shown here here i've taken a screenshot from tony bowtie ace in the cloud shell and have used the command gcloud projects get dash iam policy with the project id and this brought up all the members and roles within the bindings as well as the etag and version for the policy that has been attached to this project and as you can see here i have no conditions in place for any of my bindings and so again using the command gcloud projects get dash iam dash policy along with the project id will bring up any policies that are attached to this resource and the resource being the project id if the resource were to be the folder id then you could use the command gcloud resource dash manager folders get dash iampolicy with the folder id and for organizations the command would be gcloud organizations get dash iampolicy along with the organization id now because we don't have any folders or organizations in our environment typing these commands in wouldn't bring up anything and just as a note using these commands in the cloud shell or in the sdk will bring up the policy statement formatted in yaml so now i wanted to just take a second to dive into policy versions now as i haven't covered versions in detail i wanted to quickly go over it and the reasons for each numbered version now version one of the i am syntax schema for policies supports binding one role to one or more members it does not support conditional role bindings and so usually with version 1 you will not see any conditions version 2 is used for google's internal use and so querying policies usually you will not see a version 2. and finally with version 3 this introduces the condition field in the role binding which constrains the role binding via contact space and attributes based rules so just as a note if your request does not specify a policy version iam will assume that you want a version 1 policy and again if the policy does not contain any conditions then iam always returns a version one policy regardless of the version number in the request so moving on to some policy limitations each resource can only have one policy and this includes organizations folders and projects another limitation is that each iam policy can contain up to 1500 members and up to 250 of these members can be google groups now when making policy changes it will take up to seven minutes to fully propagate across the google cloud platform this does not happen instantaneously as iam is global as well there is a limit of 100 conditional role bindings per policy now getting a little bit deeper into conditions these are attributes that are either based on resource or based on details about the request and this could vary from time stamp to originating or destination ip address now as you probably heard me use the term earlier conditional role bindings are another name for a policy that holds a condition within the binding conditional role bindings can be added to new or existing iam policies to further control access to google cloud resources so when it comes to resource attributes this would enable you to create conditions that evaluate the resource in the access request including the resource type the resource name and the google cloud service being used request attributes allow you to manage access based on days or hours of the week a conditional role binding can be used to grant time bounded access to a resource ensuring that a user can no longer access that resource after the specified expiry date and time and this sets temporary access to google cloud resources using conditional role bindings in iam policies by using the date time attributes shown here you can enforce timebased controls when accessing a given resource now showing another example of a timebased condition it is possible to get even more granular and scope the geographic region along with the day and time for access in this policy lark only has access during business hours to view any objects within cloud storage lark can only access these objects from monday to friday nine to five this policy can also be used as a great example for contractors coming into your business yet only needing access during business hours now an example of a resourcebased condition shown here a group member has a condition tied to it where dev only access has been implemented any developers that are part of this group will only have access to vm resources within project cat bowties and tied to any resources that's name starts with the word development now some limitations when it comes to conditions is that conditions are limited to specific services primitive roles are unsupported and members cannot be of the all users or all authenticated users members conditions also hold a limit of 100 conditional role bindings per policy as well as 20 role bindings for the same role and same member and so for the last part of the policy statements i wanted to touch on audit config logs and this specifies the audit configuration for a service the configuration determines which permission types are logged and what identities if any are exempted from logging and when specifying audit configs they must have one or more audit log configs now as shown here this policy enables data read data write and admin read logging on all services while exempting tony bowtie ace gmail.com from admin read logging on cloud storage and so that's pretty much all i wanted to cover in this lesson on policies policy statements and conditions and so i highly recommend as you come across more policy statements take the time to read through it and get to know exactly what the statement is referring to and what type of permissions that are given and this will help you not only in the exam but will also help you in reading and writing policy statements in future and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this demonstration i'm going to do a handson tour working with iam here in the google cloud console we're going to go through the available services in the iam console as well as touching on the command line in the cloud shell to show how policies can be both added and edited we're also going to be bringing in another new user to really bring this demo to life and to show you how to edit existing policies so with that being said let's dive in so if i go over here to my user icon in the top right hand corner i can see that i am logged in as tony bowtie ace gmail.com and as you can see at the top i'm here in project tony so now to get to iam i'm going to go over to the navigation menu and i'm going to go to i am in admin and over to iam now moving over here to the menu on the left i wanted to go through the different options that we have in iam so under iam itself this is where you would add or edit permissions with regards to members and roles for the policy added to your given project which in my case is project tony and i'll be coming back in just a bit to go greater in depth with regards to adding and editing the policy permissions moving on to identity and organization now although we haven't touched on cloud identity yet i will be covering this in high level detail in a different lesson but for now know that cloud identity is google cloud's identity as a service solution and it allows you to create and manage users and groups within google cloud now if i was signed into cloud identity i would have a whole bunch of options here but since this is a personal account i cannot create or manage any users as well i do not have a domain tied to any cloud identity account as well as any g suite account so just know that if you had cloud identity or g suite set up you would have a bunch of different options to choose from in order to help you manage your users and groups and here under organization policies i'm able to manage organization policies but since i am not an organization policy administrator and i don't have an organization there's not much that i can do here just know that when you have an organization set up you are able to come here in order to manage and edit your organization policies now moving under quotas we went over this in a little bit of detail in a previous lesson and again this is to edit any quotas for any of your services in case you need a limit increase moving on to service accounts i will be covering this topic in great depth in a later lesson and we'll be going through a handson demonstration as well now i know i haven't touched much on labels as of yet but know that labels are a key value pair that helps you organize and then filter your resources based on their labels these same labels are also forwarded to your billing system so you can then break down your billing charges by label and you can also use labels based on teams cost centers components and even environments so for example if i wanted to label my virtual machines by environment i can simply use environment as the key and as the value i can use anything from development to qa to testing to production and i could simply add this label and add all the different environments and later i'd be able to query based on these specific labels now a good rule of thumb is to label all of your resources so that this way you're able to find them a lot easier and you're able to query them a lot easier so moving forward with any of your resources that you are creating be sure to add some labels to give you maximum flexibility so i'm going to discard these changes and we're going to move on to settings and we touched on settings in an earlier lesson with regards to projects and so here i could change the project name it'll give me the project id the project number and i'm able to migrate or shut down the project now when it comes to access transparency this provides you with logs that capture the actions that google personnel take when they're accessing your content for troubleshooting so they're like cloud audit logs but for google support now in order to enable access transparency for your google cloud organization your google cloud account must have a premium support plan or a minimum level of a 400 a month support plan and because i don't have this i wouldn't be able to enable access transparency now although access transparency is not on the exam this is a great feature to know about in case you are working in any bigger environments that have these support plans and compliance is of the utmost importance now moving into privacy and security this is where google supplies all of their clients of google cloud the compliance that they need in order to meet regulations across the world and across various industries such as health care and education and because google has a broad base in europe google provides capabilities and contractual commitments created to meet data protection recommendations which is why you can see here eu model contract clauses and eu representative contacts as well under transparency and control i'm able to disable the usage data that google collects in order to provide better data insights and recommendations and this is done at the project level and as well i have the option of going over to my billing account and i could select a different billing account that's linked to some other projects that you can get recommendations on and so continuing forward identity aware proxy is something that i will be covering in a later lesson and so i won't be getting into any detail about that right now and so what i really wanted to dig into is roles now this may look familiar as i touched on this very briefly in a previous lesson and here's where i can create roles i can create some custom roles from different selections and here i have access to all the permissions and if i wanted to i can filter down from the different types the names the permissions even the status so let's say i was looking for a specific permission and i'm looking all the permissions for projects this could help me find exactly what it is that i'm looking for and these filters allow me to get really granular so i can find the exact permission and so you can get really granular with regards to your permissions and create roles that are custom to your environment now moving on to audit logs here i can enable the auto logs without having to use a specific policy by simply clicking on default autoconfig and here i can turn on and off all the selected logging as well as add any exempted users now i don't recommend that you turn these on as audit logging can create an extremely large amount of data and can quickly blow through all of your 300 credit so i'm going to keep that off move back to the main screen of the audit logs and as well here i'm able to get really granular about what i want to log now quickly touching on audit logs in the command line i wanted to quickly open up cloud shell and show you an example of how i can edit the policy in order to enable audit logging just going to make this a little bit bigger and i'm going to paste in my command gcloud projects get dash iam dash policy with the project id which is project tony 286016 and i'm gonna just hit enter and as you can see here this is my current policy and as well as expected audit logs are not enabled due to the fact that the audit config field is not present so in order for me to enable the audit config logs i'm going to have to edit the policy and so the easiest way for me to do that is for me to run the same command and output it to a file where i can edit it and i'm going to call this new dash policy dot yaml and so now that my policy has been outputted to this file i'm going to now go into the editor and as you can see my new policy.yaml is right here and so for me to enable the autoconfig logs i'm going to simply append it to the file and then i'm going to go over here to the top menu and click on file and save and so now for me to apply this new policy i'm going to go back over to the terminal and now i'm going to paste in the command gcloud projects set dash iampolicy with the project id and the file name new dash policy dot yaml and i'm just going to hit enter and as you can see the audit log configs have been enabled for all services and because this may take some time to reflect in the console it will not show up right away but either way audit logs usually take up a lot of data and i don't want to blow through my 300 credit and so i'm going to disable them now the easiest way for me to do this is to output this policy to another file edit it and set it again and so i'm going to go ahead and do that i'm going to first clear the screen and then i'm going to paste in my command while outputting it to a new file called updated dash policy dot yaml and i'm gonna hit enter and now i'm gonna go into the editor so i can edit the file now the one thing i wanted to point out is that i could have overwritten the file new dash policy but if you look here in the updated policy the etag is different than the etag in the old policy and so this allowed me to highlight etags when it comes to editing and creating new policies and so when editing policies make sure that the etag is correct otherwise you will receive an error and not be able to set the new policy so going back to the updated policy file i'm going to take out the audit log configs and i'm going to leave the auto configs field there and i'm going to go to the menu click on file and then save now i'm going to go back to the terminal and i'm going to paste in the new command and this will update my policy and as you can see the audit config logs have been disabled and the policy has been updated now this is the same process that you can use when you want to update any parts of the policy when it comes to your members or roles and even adding any conditions so now moving on to the last item on the menu is groups and as you can see here because i do not have an organization i'm not able to view any groups and so if i did have an organization i could manage my groups right here in this page now moving back over to iam i wanted to dig into policies in a little bit of further detail now what we see here are the permissions and roles that have been granted to selected members in this specific project which is project tony now remember an im policy is a total collection of members that have roles granted to them in what's known as a binding and then the binding is applied to that layer and all other layers underneath it and since i'm at the project layer this policy is inherited by all the resources underneath it and so just to verify through the command line i'm going to open up cloud shell and i'm going to paste in the command gcloud projects get dash iampolicy with my project id and i'm going to hit enter and as you can see here the policy is a reflection of exactly what you see here in the console so as you can see here here's the service agent which you will find here and the other two service accounts which you will find above as well as tony bowtie ace gmail.com and all the other roles that accompany those members so as i mentioned earlier i've gone ahead and created a new user and so for those who are following along you can go ahead and feel free to create a new gmail user now going ahead with this demonstration the user i created is named laura delightful now tony needed an extra hand and decided to bring her onto the team from another department now unfortunately in order for laura to help tony on the project she needs access to this project and as you can see she doesn't have any access and so we're going to go ahead and change that and give her access to this project so i'm going to go back over to my open tab for tony bowtie ace and we're gonna go ahead and give laura permissions and so i'm gonna go ahead and click on this add button at the top of the page and the prompt will ask me to add a new member so i'm gonna add laura in here now and here she is and i'm going to select the role as project viewer i'm not going to add any conditions and i'm simply going to click on save and the policy has been updated and as you can see here laura has been granted the role of project viewer so i'm going to move over to the other open tab where laura's console is open and i'm going to simply do a refresh and now laura has access to view all the resources within project tony now laura is able to view everything in the project but laura isn't actually able to do anything and so in order for laura to get things done a big part of her job is going to be creating files with new ideas for the fall winter line of bow ties in 2021 and so because laura holds the project viewer role she is able to see everything in cloud storage but she is unable to create buckets to upload edit or delete any files or even folders and as you can see here there is a folder marked bowtie inc fallwinter 2021 ideas but laura cannot create any new buckets because she doesn't have the required permissions as well drilling down into this bucket laura is unable to create any folders as explained earlier and the same stands for uploading any files and so i'm going to cancel out of this and so in order to give laura the proper permissions for her to do her job we're going to give laura the storage admin role and so moving back over to the open console for tony bowtie i'm going to give laura access by using the command line so i'm going to go up to the top right and open up cloud shell and so the command i need to run to give laura the role of storage admin would be the following gcloud projects add dash iam dash policy dash binding with the project id dash dash member user followed by colon and then the user name which is laura delightful gmail.com dash dash role and the role which is storage admin and i'm going to go ahead and hit enter and as you can see it has been executed successfully so if i do a refresh of the web page here i'm going to be able to see the changes reflected in the console and after a refresh you can see here storage admin has been added to the role for laura delightful gmail.com and so if i go over to the open tab where laura has her console open i can simply do a refresh and if i go back to the home page for cloud storage you can see here that laura now has the permissions to create a bucket laura also now has permissions to create new folders create edit and delete new files on top of being able to create new storage buckets and so that about wraps up this demonstration on getting handson with iam in both the console and the command line and i also hope that this demo has given you a bit more confidence on working in the shell running the commands needed in order to create new bindings along with editing existing policies and this will get you comfortable for when you need to assign roles to new and existing users that are added to your gcp environment and so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to take a deep dive into service accounts now service accounts play a powerful part in google cloud and can allow a different approach for application interaction with the resources in google cloud now service accounts being both an identity and a resource can cause some confusion for some and so i really wanted to spend some time breaking it down for better understanding and so i'm first going to start off by explaining what exactly is a service account and so a service account is a special kind of account that is used by an application or a virtual machine instance and not a person an application uses the service account to authenticate between the application and gcp services so that the users aren't directly involved in short it is a special type of google account intended to represent a nonhuman user that needs to authenticate and be authorized to access data in google apis this way the service account is the identity of the service and the service accounts permissions control which resources the service can access and as a note a service account is identified by its email address which is unique to the account now the different service account types come in three different flavors user managed default and google managed service accounts when it comes to the user managed service accounts these are service accounts that you create you're responsible for managing and securing these accounts and by default you can create up to 100 user managed service accounts in a project or you can also request a quota increase in case you need more now when you create a user managed service account in your project it is you that chooses a name for the service account this name appears in the email address that identifies the service account which uses the following format seen here the service account name at the project id dot iam.gserviceaccount.com now moving on to the default service accounts when you use some google cloud services they create user managed service accounts that enable the service to deploy jobs that access other google cloud resources these accounts are known as default service accounts so when it comes to production workloads google strongly recommends that you create your own user managed service accounts and grant the appropriate roles to each service account when a default service account is created it is automatically granted the editor role on your project now following the principle of lease privilege google strongly recommends that you disable the automatic role grant by adding a constraint to your organization policy or by revoking the editor role manually the default service account will be assigned an email address following the format you see here project id at appspot.gserviceaccount.com for any service accounts created by app engine and project number dash compute at developer.gserviceaccount.com for compute engine and so lastly when it comes to google managed service accounts these are created and managed by google and they are used by google services the display name of most google managed service accounts ends with a gserviceaccount.com address now some of these service accounts are visible but others are hidden so for example google api service agent is a service account named with an email address that uses the following format project number at cloudservices.gerisa and this runs internal google processes on your behalf and this is just one example of the many google managed services that run in your environment and just as a warning it is not recommended to change or revoke the roles that are granted to the google api service agent or to any other google managed service accounts for that matter if you change or revoke these roles some google cloud services will no longer work now when it comes to authentication for service accounts they authenticate using service account keys so each service account is associated with two sets of public and private rsa key pairs that are used to authenticate to google they are the google manage keys and the user manage keys with the google manage keys google stores both the public and private portion of the key rotates them regularly and the private key is always held in escrow and is never directly accessible iam provides apis to use these keys to sign on behalf of the service account now when using user managed key pairs this implies that you own both the public and private portions of a key pair you can create one or more user managed key pairs also known as external keys that can be used from outside of google cloud google only stores the public portion of a user managed key so you are responsible for the security of the private key as well as the key rotation private keys cannot be retrieved by google so if you're using a user manage key please be aware that if you lose your key your service account will effectively stop working google recommends storing these keys in cloud kms for better security and better management user managed keys are extremely powerful credentials and they can represent a security risk if they are not managed correctly and as you can see here a user managed key has many different areas that need to be addressed when it comes to key management now when it comes to service account permissions in addition to being an identity a service account is a resource which has im policies attached to it and these policies determine who can use the service account so for instance lark can have the editor role on a service account and laura can have a viewer role on a service account so this is just like granting roles for any other google cloud resource just as a note the default compute engine and app engine service accounts are granted editor roles on the project when they are created so that the code executing in your app or vm instance has the necessary permissions now you can grant the service account user role at both the project level for all service accounts in the project or at the service account level now granting the service account user role to a user for a project gives the user access to all service accounts in the project including service accounts that may be created in the future granting the service account user role to a user for a specific service account gives a user access to only that service account so please be aware when granting the service account user role to any member now users who are granted the service account user role on a service account can use it to indirectly access all the resources to which the service account has access when this happens the user impersonates the service account to perform any tasks using its granted roles and permissions and is known as service account impersonation now when it comes to service account permissions there is also another method use called access scopes service account scopes are the legacy method of specifying permissions for your instance and they are used in substitution of iam roles these are used specifically for default or automatically created service accounts based on enabled apis now before the existence of iam roles access scopes were the only way for granting permissions to service accounts and although they are not the primary way of granting permissions now you must still set service account scopes when configuring an instance to run as a service account however when you are using a custom service account you will not be using scopes rather you will be using iam roles so when you are using a default service account for your compute instance it will default to using scopes instead of iam roles and so i wanted to quickly touch on how service accounts are used now one way of using a service account is to attach this service account to a resource so if you want to start a longrunning job that authenticates as a service account you need to attach a service account to the resource that will run the job and this will bind the service account to the resource now the other way of using a service account is directly impersonating a service account which i had explained a little bit earlier so once granted they require permissions a user or a service can directly impersonate the identity of a service account in a few common scenarios you can impersonate the service account without requiring the use of a downloaded external service account key as well a user may get artifacts signed by the google managed private key of the service account without ever actually retrieving a credential for the service account and this is an advanced use case and is only supported for programmatic access now although i'm going to be covering best practices at the end of this section i wanted to go over some best practices for service accounts specifically so you should always look at auditing the service accounts and their keys using either the service account dot keys dot list method or the logs viewer page in the console now if your service accounts don't need external keys you should definitely delete them you should always grant the service account only the minimum set of permissions required to achieve the goal service accounts should also be created for each specific service with only the permissions required for that service and finally when it comes to implementing key rotation you should take advantage of the iam service account api to get the job done and so that's all i have for this lesson on service accounts so you can now mark this lesson as complete and please join me in the next one where we go handson in the console welcome back so in this demonstration i'm going to take a handson tour diving through various aspects of working with both default and custommade service accounts we're going to start off fresh observing a new service account being automatically created along with viewing scopes observing how to edit them and creating custom service accounts that get a little bit more granular with the permissions assigned so with that being said let's dive in so as you can see here from the top right hand corner that i am logged in under tony bowtie ace gmail.com and looking over here from the top drop down menu you can see that i am in the project of cat bow ties fall 2021 and this is a brand new project that i had created specifically for this demo and so i currently have no resources created along with no apis enabled so now i want to navigate over to iam so i'm going to go up to the left hand corner to the navigation menu and i'm going to go to i am an admin and over to iam and as expected i have no members here other than myself tony bowtie ace gmail.com with no other members and if i go over here to the left hand menu under service accounts you can see that i have no service accounts created so now in order to demonstrate a default service account i'm going to go over to the navigation menu and go into compute engine and as you can see the compute engine api is starting up and so this may take a couple minutes to get ready okay and the compute engine api has been enabled so now if i go back over to iam to take a look at my service accounts as expected i have my compute engine default service account now again i did not create this manually this service account was automatically created when i had enabled the compute engine api along with the api's service agent and the compute engine service agent and the same would happen to other various apis that are enabled as well and so now that i have my default service account i want to go back over to compute engine and i'm going to go ahead and create a vm instance so i'm going to just click on create i'm going to keep everything as the default except i'm going to change the machine type from an e2 medium to an e2 micro and so now i'm going to scroll down to where it says identity and api access now here under service account you can see that the compute engine default service account has been highlighted and this is because i don't have any other service accounts that i am able to select from now when a default service account is the only service account you have access to access scopes are the only permissions that will be available for you to select from now remember access scopes are the legacy method of specifying permissions in google cloud now under access scopes i can select from the allow default access allow full access to all cloud apis and set access for each api and so i want to click on set access for each api for just a second and so as you can see here i have access to set permissions for each api the difference being is that i only have access to primitive roles and so now that i'm looking to grant access to my service account i'm going to grant access to cloud storage on a readonly capacity and so now that i have granted permissions for my service account i'm going to now create my instance by simply clicking on the create button and so now that my instance is created i want to head over to cloud storage to see exactly what my service account will have access to so i'm going to go over to my navigation menu and scroll down and click on storage and as you can see here i have created a bucket in advance called bow tie ink fall winter 2012 designs and this is due to bow tie ink bringing back some old designs from 2012 and making them relevant for today and within that bucket there are a few files of different design ideas that were best sellers back in 2012 that tony bowtie wanted to rerelease for the fall winter 2012 collection and so with the new granted access to my default service account i should have access to view these files so in order to test this i'm going to go back over to the navigation menu and go back to compute engine and i'm going to ssh into my instance and so now that i've sshed into my virtual machine i wanted to first check to see who is it that's running the commands is it my user account or is it my service account and so i'll be able to do this very easily by checking the configuration and i can do this by running the command gcloud config list and as you can see my current configuration is showing that my service account is the member that is being used to run this command in the project of cat bow ties fall 2021 now if i wanted to run any commands using my tony bowtie ace gmail.com user account i can simply run the command gcloud auth login and it will bring me through the login process that we've seen earlier on in the course for my tony bowtie ace gmail.com account but now since i'm running all my commands using my service account from this compute engine instance i'm using the permissions granted to that service account that we saw earlier and so since i set the storage scope for the service account to read only we should be able to see the cloud storage bucket and all the files within it by simply running the gsutil command so to list the contents of the bucket i'm going to type in the command gsutil ls for list and the name of the bucket and the syntax for that would be gs colon forward slash forward slash followed by the name of the bucket which would be bowtie inc fw2012 designs and as you can see we're able to view all the files that are in the bucket and so it is working as expected and so now because i've only granted viewing permissions for this service account i cannot create any files due to the lack of permissions so for instance if i was to create a file using the command touch file one i have now created that file here on the instance so now i want to copy this file to my bucket and so i'm going to run the gsutil command cp for copy file 1 which is the name of my file and gs colon forward slash forward slash along with the name of the bucket which is bow tie inc fw 2012 designs and as expected i am getting an access denied exception with a prompt telling me that i have insufficient permissions and so now that i've shown you how to create a default service account and give it permissions using access scopes let's now create a custom service account and assign it proper permissions to not only read files from cloud storage but be able to write files to cloud storage as well so i'm going to now close down this tab and i'm going to go back over to the navigation menu and go back to iam where we can go in and create our new service account under service accounts and so as you can see here this is the default service account and since we want to create a custom one i'm going to go ahead and go up to the top here and click on the button that says create service account and so now i'm prompted to enter some information with regards to details of this service account including the service account name the account id along with a description and so i'm going to call this service account sa hyphen bowtie hyphen demo and as you can see it automatically propagated the service account id and i'm going to give this service account a description storage read write access and i'm going to click on the button create and so now i've been prompted to grant permissions to the service account and i can do that by simply clicking on the drop down and selecting a roll but i'm looking to get a little bit more granular and so i'm going to simply type in storage and as you can see i'm coming up with some more granular roles as opposed to the primitive roles that i only had access to prior to the search so i'm going to click on storage object viewer for read access to cloud storage i'm not going to add any conditions and i'm going to add another role and this time i'm going to add storage object creator and so those are all the permissions i need for read write access to cloud storage and so now i can simply click on continue and so now i'm being prompted to add another user to act as a service account and this is what we discussed in the last lesson about service accounts being both a member and a resource now notice that i have an option for both the service account users role and the service account admins role now as discussed earlier the service account and men's role has the ability to grant other users the role of service account user and so because we don't want to do that i'm going to leave both of these fields blank and simply click on done now i know in the last lesson i talked about creating custom keys for authentication in case you're hosting your code on premise or on another cloud and so if i wanted to do that i can simply go to the actions menu and click on create key and it'll give me the option on creating a private key either using json or p12 format and because i'm not creating any keys i'm going to simply click on cancel and so in order for me to apply this service account to our vm instance i'm going to now go back over to the navigation menu and go back into compute engine and so now in order for me to change this service account that's currently assigned to this instance i'm going to go ahead and check off this instance and click on stop now please note that in order to change service accounts on any instance you must stop it first before you can edit the service account and so now that the instance has stopped i'm going to drill down into this instance one and i'm going to click on edit now i'm going to scroll down to the bottom and at the bottom you will find the service account field and clicking on the drop down i'll find my custom service account as a bow tie demo so i want to select this and simply click on save and so now that i've selected my new service account to be used in this vm instance i can now start up the instance again to test out the permissions that were granted and so just as a quick note here i wanted to bring your attention to the external ip whenever stopping and starting an instance with an ephemeral ip in other words it is not assigned a static ip your vm instance will receive a new ip address and i'll be getting into this in a lot deeper detail in the compute engine section of the course and so now i'm going to ssh into this instance now i'm going to run the same gsutil command that i did previously to list all the files in the bucket so i'm going to run the command gsutil ls for list and gs colon forward slash forward slash bow tie inc fw 2012 designs and as you can see i'm able to read all the files in the bucket now the difference in the permissions granted for the service account is that i'm able to write files to cloud storage and so in order to test that i'm going to use the touch command again and i'm going to name the file file2 and so now i'm going to copy this file to the cloud storage bucket by using the command gsutil cp file2 and the bucket name gs colon forward slash forward slash bow tie inc fw 2012 designs and as expected the file copied over successfully as we do have permissions to write to cloud storage and so before i end this demonstration i wanted to quickly go over exactly how to create service accounts using the command line and so i'm going to close down this tab and i'm going to head up to the top right hand corner and activate my cloud shell i'm going to make this window a little bit bigger and so now in order to view the service accounts i currently have i'm going to run the command gcloud iam service dash accounts list and so as expected the compute engine default service account along with the custom service account that i created earlier called sa bowtie demo is now displaying and in order to just verify that i'm going to go over to iam under service accounts and as you can see it is reflecting exactly the same in the console so now in order for me to create a new service account using the command line i'm going to run the command gcloud iam service accounts create and the name of the service account which i'm going to call satony bowtie along with the display name as essay tony bowtie as well and i'm going to hit enter and my service account has been created so now if i run the command gcloud i am service accounts list i should see my new service account and as well if i did a refresh here on the console i can see that it is reflecting the same so now that we've created our new service account we need to assign some permissions to it in order for us to be able to use it and so if i go over here to iam in the console i can see here that my service account has not been assigned any permissions and so in order to do that i am going to simply run the command gcloud projects add dash iampolicybinding so we're adding a policy binding and then the name of the project catbow ties fall 2021 we need to add the member which is the new service account email address along with the role of storage object viewer i'm going to hit enter and as you can see my member sa tony bowtie has been assigned the storage object viewer role and so if i wanted to grant some other roles to the service account i can do that as well and so if i did a refresh here i can see that the console reflects exactly the same and so in order for me to use this account in my instance i'm going to first have to stop my instance attach my service account and then start up my instance again so i'm going to go over to my cloud shell i'm just going to clear the screen and i'm going to paste in the command gcloud compute instances stop the name of the instance along with the zone and now that the instance has stopped i can now add my surface account to the instance and so i'm going to use the command gcloud compute instances set service account instance 1 along with the zone and the service account email address i'm going to go ahead and hit enter and it has now been successfully added and so now that that's done i can now start up the instance by using the command gcloud compute instances start along with the instance name and the zone and so now if i go over to my navigation menu and go over to compute engine and drill down on the instance if i scroll down to the bottom i'll be able to see that my new service account has been added and so this is a great demonstration for when you want to add different service accounts for your different applications on different instances or even on different resources and so that's pretty much all i wanted to cover in this demonstration so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to dive into cloud identity google's identity as a service offering for google cloud that maximizes end user efficiency protect company data and so much more now cloud identity as i said before is an identity as a service solution that centrally manages users and groups this would be the sole system for authentication and that provides a single signon experience for all employees of an organization to be used for all your internal and external applications cloud identity also gives you more control over the accounts that are used in your organization for example if developers in your organization use personal accounts such as gmail accounts those accounts are outside of your control so when you adopt cloud identity you can manage access and compliance across all the users in your domain now when you adopt cloud identity you create a cloud identity account for each of your users and groups you can then use iam to manage access to google cloud resources for each cloud identity account and you can also configure cloud identity to federate identities between google and other identity providers such as active directory and azure active directory and i'll be getting more into that a little bit later so now when it comes to cloud identity it gives you so much more than just user and group management it provides a slew of features such as device management security single signon reporting and directory management and i will be diving deeper into each one of these features of cloud identity now starting with device management this lets people in any organization access their work accounts from mobile devices while keeping the organization's data more secure in today's world employees want to access business applications from wherever they are whether at home at work or even traveling and many even want to use their own devices which is also known as bring your own device or byod for short using mobile device management there are several ways that you can provide the business applications employees need on their personal devices while implementing policies that keep the corporate data safe you can create a white list of approved applications where users can access corporate data securely through those applications you can enforce work profiles on android devices and requiring managed applications on ios devices policies can also be pushed out on these devices to protect corporate data and identities as well as keeping inventory of devices with corporate data present then when these devices are either no longer being used for corporate use or stolen the device can then be wiped of all its corporate data device management also gives organizations the power to enforce passcodes as well as auditing now moving into the security component of cloud identity this is where twostep verification steps in now as explained earlier twostep verification or to sv is a security feature that requires users to verify their identity through something they know such as a password plus something they have such as a physical key or access code and this can be anything from security keys to google prompt the authenticator app and backup codes so cloud identity helps by applying security best practices along with being able to deploy twostep verification for the whole company along with enforcement controls and can also manage passwords to make sure they are meeting the enforced password requirements automatically so single signon is where users can access many applications without having to enter their username and password for each application single signon also known as sso can provide a single point of authentication through an identity provider also known as idp for short you can set up sso using google as an identity provider to access a slew of thirdparty applications as well as any onpremise or custom inhouse applications you can also access a centralized dashboard for conveniently accessing your applications so now when lisa logs in with her employee credentials she will then have access to many cloud applications that bowtie inc it department has approved through a catalog of sso applications and this will increase both security and productivity for lisa and bowtie inc as lisa won't have to enter a separate username and password for separate applications now getting into reporting this covers audit logs for logins groups devices and even tokens you're even able to export these logs to bigquery for analysis and then you can create reports from these logs that cover security applications and activity now moving on to the last component of cloud identity is directory management and this provides profile information for users in your organization email and group addresses and shared external contacts in the directory using google cloud directory sync or gcds you can synchronize the data in your google account with your microsoft active directory or ldap server gcds doesn't migrate any content such as your email your calendar events or your files to your google account gcds is used to synchronize all your users groups and shared contacts to match the information in your ldap server which could be your active directory server or your azure active directory domain now getting deeper into google cloud directory sync i'd like to touch on active directory for just a minute now active directory is a very common directory service developed by microsoft and is a cornerstone in most big corporate onpremises environments it authenticates and authorizes all users and computers in a windows domain type network signing and enforcing security policies for all computers and installing or updating software as necessary now as you can see here in the diagram the active directory forest contains the active directory domain a bowtieinc.co and the active directory federation services of bowtieinc.co where the active directory forest is the hierarchical structure for active directory the active directory domain is responsible for storing information about members of the domain including devices and users and it verifies their credentials and defines their access rights active directory federation services or adfs is a single signon service where federation is the means of linking a person's electronic identity and attributes stored across multiple distinct identity management systems so you can think of it as a subset of sso as it relates only to authentication technologies used for federated identity include some common terms that you may hear me or others in the industry use from time to time such as saml which stands for security assertion markup language oauth open id and even security tokens such as simple web tokens json web tokens and saml assertions and so when you have identities already in your onpremises environment that live in active directory you need a way to tie these identities to the cloud and so here's where you would use google cloud directory sync to automatically provision users and groups from active directory to cloud identity or g suite google cloud directory sync is a free google provided tool that implements the synchronization process and can be run on google cloud or in your onpremises environment synchronization is one way so that active directory remains the source of truth cloud identity or g suite uses active directory federation services or adfs for single signon any existing corporate applications and other sas services can continue to use your adfs as an identity provider now i know this may be a review for some who are advanced in this topic but for those who aren't this is a very important topic to know as google cloud directory sync is a big part of cloud identity and is a common way that is used in many corporate environments to sync active directory or any other ldap server to google cloud especially when you want to keep your active directory as the single source of truth and so that's pretty much all i wanted to cover when it comes to cloud identity and google cloud directory sync so you can now mark this lesson as complete and let's move on to the next one welcome back now i wanted to close out this section by briefly going over the best practices to follow when working with identity and access management so the phrase that was discussed in the beginning of this lesson that will continuously come up in the exam is the principle of least privilege and again this is where you would apply only the minimal access level required for what is needed to be done and this can be done using predefined roles which is a more granular level role than using primitive roles which are very wide scoped roles that are applied to the whole project roles should also be granted at the smallest scope necessary so for instance when assigning somebody the permissions needed for managing preexisting compute instances assigning a compute instance admin role might be sufficient for what they need to do as opposed to assigning them the compute instance role that has full control of all compute engine instance resources now when it comes to child resources they cannot restrict access granted on its parent so always remember to check the policy granted on every resource and make sure you understand the hierarchical inheritance you also want to make sure that you restrict access to members abilities to create and manage service accounts as users who are granted the service account actor role for a service account can access all the resources for which the service account has access and granting someone with the owner role should be used with caution as they will have access to modify almost all resources projectwide including iam policies and billing granting an editor role might be more sufficient for the needs of most when using primitive roles now when dealing with resource hierarchy to make it easy on how to structure your environment you should look at mirroring your google cloud resource hierarchy structure to your organizational structure in other words the google cloud resource hierarchy should reflect how your company is organized you should also use projects to group resources that share the same trust boundary as well as setting policies at the organization level and at the project level rather than at the resource level now going back to what we discussed earlier about the principle of least privilege you should use this guideline to grant iam roles that is only give the least amount of access necessary to your resources and when granting roles across multiple projects it is recommended to grant them at the folder level instead of at the project level now diving back into service accounts a separate trust boundary should always be applied for any given application in other words create a new service account when multiple components are involved in your application you also want to make sure that you don't delete any service accounts that are in use by running instances as your application is likely to fail so you will want to schedule this during plan down time to avoid any outages now earlier on in this section we discussed service account keys and how they interact with google cloud and that is the main authentication mechanism used for keys so you want to make sure that any user managed keys are rotated periodically to avoid being compromised you can rotate a key by creating a new key switching applications to use the new key and then deleting the old key but be sure to create the new key first before deleting the old one as this will result in parts or even your entire application failing and also when working with service account keys it's always good practice to name your service keys and this will reflect your use for those keys and permissions for those keys so you know what they are used for when you're looking at them now when you are giving access to service accounts you want to make sure that only those who truly need access are the ones that have it others in your environment should be restricted to avoid any misuse now when it comes to keeping your service account keys safe i can't stress this enough you never want to check in these keys source code or leave them in your downloads directory as this is a prime way of not only getting your keys compromised but compromising your entire environment to be accessed publicly now we touched a bit on auditing but we haven't really gone into it in detail and we'll be going into it later on in the course but touching on best practices you want to be sure to check your cloud audit logs regularly and audit all i am policy changes whenever you edit any iam policies a log is generated that records that change and so you always want to periodically check these logs to make sure that there are no changes that are out of your security scope you also want to check to see who has editing permissions on these iam policies and make sure that those who hold them have the rights to do so point being is that you want to restrict who has the ability to edit policies and once these audit logs have been generated you want to export them to cloud storage so that you're able to store them for long term retention as these logs are typically held for weeks and not years getting back to service account keys service account key access should be periodically audited for viewing of any misuse or unauthorized access and lastly audit logs should also be restricted to only those who need access and others should have no permissions to view them and this can be done by adding a role to be able to view these logs now when touching on policy management you want to grant access to all projects in your organization by using an organization level policy you also want to grant roles to a google group instead of individual users as it is easier to add or remove members from a google group instead of updating an im policy and finally when you need to grant multiple roles to a task you should create a google group as it is a lot easier to grant the roles to that group and then add the users to that group as opposed to adding roles to each individual user and so that's all i wanted to cover on this short yet very important lesson on best practices when it comes to iam now i know this is not the most exciting topic but will become extremely necessary when you are dealing with managing users groups and policies in environments that require you to use iam securely and so please keep this in mind whenever you are working in any environment as it will help you grant the proper permissions when it comes to these different topics so now i highly recommend that you take a break grab a tea or coffee before moving on into the next section and so for now you can mark this lesson as complete and whenever you're ready please join me in the next section welcome back now i wanted to make this as easy as possible for those students who do not have a background in networking or any networking knowledge in general which is why i wanted to add this quick networking refresher to kick off the networking section of this course so with that being said let's dive in so before the internet computers were standalone and didn't have the capabilities to send emails transfer files or share any information fast forward some time people started to connect their computers together to share and be able to do the things that modern networks can do today part of being in this network is being able to identify each computer to know where to send and receive files this problem was solved by using an address to identify each computer on the network like humans use a street address to identify where they live so that mail and packages can be delivered to them an ip address is used to identify a computer or device on any network so communication between machines was done by the use of an ip address a numerical label assigned to each device connected to a computer network that uses the internet protocol for communication also known as ip for short so for this system to work a communication system was put in place that defined how the network would function this system was put together as a consistent model of protocol layers defining interoperability between network devices and software in layers to standardize how different protocols would communicate in this stack this stack is referred to as the open systems interconnection model or you may hear many refer to it as the seven layer osi model now this is not a deep dive networking course but i did feel the need to cover that which is necessary for the understanding of the elements taught in this course for those wanting to learn more about the osi model and the layers within it please check out the links that i have included in the lesson text below so for this lesson and the next i will be covering the specific layers with its protocols that are highlighted here and will help you understand the networking concepts in this course with a bit better clarity so i'll be covering a layer 3 being the network layer layer 4 being the transport layer and layer 7 being the application layer so first up i will be covering layer 3 which is the networking layer along with the internet protocol now there are two versions of the internet protocol and are managed globally by the regional internet registries also known as the rir the first one which is ipv4 is the original version of the internet protocol that first came on the scene in 1981 the second version is ipv6 which is a newer version designed in 2017 to deal with the problem of ipv4 address exhaustion meaning that the amount of usable ips were slowly being used up and i will be covering both versions of the internet protocol in a little bit of depth so let's first dive into ipv version 4. so ipv4 can be read in a human readable notation represented in dotted decimal notation consisting of four numbers each ranging from 0 to 255 separated by dots each part between the dots represents a group of 8 bits also known as an octet a valid range for an ip address starts from 0.0.0.0 and ends in 255.255.255.255. and this would give you a total number of over 4.2 billion ip addresses now this range was viewed as extremely large back then until the number of ip addresses available were quickly dwindling due to the many ipconnected devices that we have today and this is when a new addressing architecture was introduced called classful addressing where the address was split into smaller ranges and this was originally assigned to you when you needed an ip address by one of the registries noted before so for any given ip address they're typically made of two separate components the first part of the address is used to identify the network that the address is a part of the part that comes afterwards is used to specify a specific host within that network now the first part was assigned to you and your business by the registries and the second part was for you to do it as you'd like and so these ip addresses were assigned from the smaller ranges explained earlier called classes the first range of classes is class a and it started at 0.0.0.0 and ended at 127.255 and this would give a total number of over 2.1 billion addresses with 128 different networks class a ip addresses can support over 16 million hosts per network and those who were assigned addresses in this class had a fixed value of the first octet the second third and fourth octet was free for the business to assign as they choose class a ip addresses were to be used by huge networks like those deployed by internet service providers and so when ips started to dwindle many companies return these class a network blocks back to the registries to assist with extending addressing capacity and so the next range is class b and this is half the size of the class a network the class b network range started at one at 128.0.0.0 and end it at 191.255.255.255 and carries a total number of over 1 billion ip addresses with over 16 000 networks the fixed value in this class is of the first and second octet the third and fourth octet can be done with as you like ip addresses in this class were to be used for medium and large size networks in enterprises and organizations the next range is class c and this is half the size of the class b network the class c network range starts at 192 and ends at 223.255.255.255 and carries a total of over half a billion addresses with over two million networks and can support up to 256 hosts the fixed value of this class is the first second and third octet and the fourth can be done with as you like ip addresses in this class were the most common class and were to be used in small business and home networks now there's a couple more classes that were not commonly used called class d and class e and this is beyond the scope of this course so we won't be discussing this and so this was the way that was used to assign public ip addresses to devices on the internet and allowed communication between devices now the problem with classful addressing was that with businesses that needed larger address blocks than a class c network provided they received a class b block which in most cases was much larger than required and the same thing happened with requiring more ips than class b and getting a class a network block this problem introduced a lot of wasted ips as there was no real middle ground and so this was a way to address any publicly routable ips now there were certain ranges that were allocated for private use and were designed to be used in private networks whether onpremises or in cloud and again they are not designed for public use and also didn't have the need to communicate over the public internet and so these private ip address spaces were standardized using the rfc standard 1918 and again these ip addresses are designed for private use and can be used anywhere you like as long as they are still kept private chances are a network that you've come across whether it be a cloud provider your home network or public wifi will use one of these classes to define their network and these are split into three ranges first one being single class a with 10.0.0 ending in 10.255.255.255. the class b range ranging from 172.16.0.0 to 172.31 dot and lastly class c which was ranging from 192.168.0.0 to 192.168.255.255. now for those networks that use these private ips over the public internet the process they would use is a process called network address translation or nat for short and i will be covering this in a different lesson later on in the section this method of classful addressing has been replaced with something a bit more efficient where network blocks can be defined more granularly and was done due to the internet running out of ipv4 addresses as we needed to allocate these ips more efficiently now this method is called classless inter domain routing or cider for short now with cider based networks you aren't limited to only these three classes of networks class a b and c have been removed for something more efficient which will allow you to create networks in any one of those ranges cider ranges are represented by its starting ip address called a network address followed by what is called a prefix which is a slash and then a number this slash number represents the size of the network the bigger the number the smaller the network and the smaller the number the bigger the network given the example here 192.168.0.0 is the network address and the prefix is a slash 16. now at this high level it is not necessary to understand the math behind this but i will include a link in the lesson text for those of you who are interested in learning more about it all you need to keep in mind is as i said before the bigger the prefix number the smaller the network and the smaller the prefix number the bigger the network so just as an example the size of this slash 16 network is represented here by this circle its ip range is 192.168.0.0 ending in 192.168.255.255. and once you understand the math you will be able to tell that a slash 16 range means that the network is the fixed value in the first and second octet the hosts on the network or the range are the values of anything in the third or fourth octets so this network in total will provide us with 65 536 ip addresses now let's say you decided to create a large network such as this and you wanted to allocate part of it to another part of your business you can simply do so by splitting it in two and be left with two slash 17 networks so instead of one slash 16 network you will now have 2 17 networks and each network will be assigned 32 768 ip addresses so just to break it down the previous network which was 192.16 forward slash 16 with the first two octets being the network which is 192.168 it leaves the third and fourth octet to distribute as you like and these third and fourth octets are what you're having to create these two networks so looking at the blue half the address range will start at 0.0 and will end at 127.255. the green half will start halfway through the slash 16 network which will be 128.0 and end at 255.255. so now what if i was looking to break this network down even further and break it into four networks well using cider ranges this makes things fairly easy as i can have it again and as shown here i would split the two slash 17 networks to create four slash 18 networks so if i took the blue half circle and split it into two and then splitting the green half circle into this would leave me with four slash 18 networks as seen here the blue quarter would start from 192.168.0.0 ending with the last two octets of 63.255 and the red quarter which starts from where the blue left off starting at the last two octets of 64.0 and ending in 127.255. the green quarter again starting off with the previously defined 128.0 network which is where the red quarter left off and ending with the last two octets being 191.255 and lastly the yellow quarter starting off from where the green quarter left off at 192.0 with the last two octets ending with 255.255 and so this would leave us with four smaller slash 18 networks broken down from the previous two 17 networks with each of these networks consisting of 16 384 ip addresses and we can continue this process continuously having networks and breaking them down into smaller networks this process of dividing each network into two smaller networks is known as subnetting and each time you subnet a network and create two smaller networks the number in the prefix will increase and so i know this is already a lot to take in so this would be a perfect time for you to grab a coffee or a tea and i will be ending part one here and part two will be continuing immediately after part one so you can now mark this lesson as complete and i'll see you in the next one for part two welcome back and in this lesson i'm going to be covering the second part of the networking refresher now part two of this lesson is starting immediately from the end of part one so with that being said let's dive in now i know this network refresher has been filled with a ton of numbers with an underlying current of math but i wanted you to focus on the why so that things will make sense later i wanted to introduce the hard stuff first so that over the length of this course you will be able to digest this information and understand where this fits into when discussing the different network parts of google cloud this will also help you immensely in the real world as well as the exam when configuring networks and knowing how to do the job of an engineer so getting right into it i wanted to just do a quick review on classless interdomain routing or cider so as discussed in the first refresher an ipv4 address is referenced in dotted decimal notation alongside the slash 16 is the prefix and defines how large the network is and so before i move on i wanted to give you some references that i found helpful in order to determine the size of a network and so here i have referenced three of the most common prefixes that i continuously run into that i think would be an extremely helpful reference for you so if you look at the first i p address 192.168.0.0 with slash 8 as the prefix slash 8 would fall under a class a network 192 being the first octet as well as being the network part of the address would be fixed and so the host part of it would be anything after that so the address could be 192 dot anything and this cider range would give you over 16 million ip addresses the second most common network that i see is a slash 16 network and this would make this ip fall under a class b network making the first two octets fixed and being the network part meaning that anything after 192.168 would be the host part meaning that the address could be 192.168.anything and this would give you 65536 ip addresses and so for the third ip address which is probably the most common one that i see is a slash 24 network which falls under a class c network meaning that the first three octets are fixed and the fourth octet could be anything from zero to two five five and this would give you 256 ip addresses and another common one which is the smallest that you will see is a slash 32 prefix and this is one that i use constantly for white listing my ip address and because a slash 32 is one ip address this is a good one to know when you are configuring vpn for yourself or you're whitelisting your ip address from home or work and for the last reference as well as being the biggest network is the ip address of 0.0.0.1 forward slash 0 which covers all ip addresses and you will see this commonly used for the internet gateway in any cloud environment and so these are some common prefixes that come up very frequently and so i hope this reference will help you now moving back to the osi model i've covered ipv4 in the network layer and so now it's time to discuss ipv6 now as i noted earlier ipv4 notation is called dotted decimal and each number between the dots is an octet with a value of 0 to 255. now underneath it all each octet is made up of an 8bit value and having four numbers in an ip address that would make it a 32bit value ipv6 is a much longer value and is represented in hexadecimal and each grouping is two octets which is 16 bits and is often referred to as a hextet now as these addresses are very long as you can see you're able to abbreviate them by removing redundant zeros so this example shown here is the same address as the one above it so if there is a sequence of zeros you can simply replace them with one zero so in this address each grouping of four zeros can be represented by one zero and if you have multiple groups of zeros in one address you can remove them all and replace them with double colons so each of these ipv6 addresses that you see here are exactly the same now each ipv6 address is 128 bits long and is represented in a similar way to ipv4 starting with the network address and ending with the prefix each hextet is 16 bits and the prefix number is the number of bits that represent the network with this example slash 64 refers to the network address underlined in green which is 2001 colon de3 each hextet is 16 bits and the prefix is 64. so that's four groups of 16 and so this is how we know which part is the network part of the address and which is the host part of the address again notice the double colon here and as i explained previously any unneeded zeros can be replaced by a double colon and so this address would represent a slew of zeros and so adding in all the zeros the ipv6 starting network address would look like this now because the network address starts at 2001 colon de3 with another two hextets of zeros as the network address that was determined by the slash 64 prefix which is four hextets it means a network finishes at that network address followed by all fs and so that's the process of how we can determine the start and end of every ipv6 network now as i've shown you before with all ipv4 addresses they are represented with a 0.0.0.0.0 and because ipv6 addresses are represented by the same network address and prefix we can represent ipv6 addresses as double colon slash zero and you will see this frequently when using ipv6 and so i know this is really complicated but i just wanted to give you the exposure of ipv6 i don't expect you to understand this right away in the end it should become a lot clearer as we go through the course and i promise you it will become a lot easier i had a hard time myself trying to understand this network concept but after a few days i was able to digest it and as i went back and did some practice it started to make a lot more sense to me and so i know as we move along with the course that it will start making sense to you as well so now that we've discussed layer 3 in the osi model i wanted to get into layer 4 which is the transport layer with ip packets discussing tcp and udp and so in its simplest form a packet is the basic unit of information in network transmission so most networks use tcpip as the network protocol or set of rules for communication between devices and the rules of tcpip require information to be split into packets that contain a segment of data to be transferred along with the protocol and its port number the originating address and the address of where the data is to be sent now udp is another protocol that is sent with ip and is used in specific applications but mostly in this course i will be referring to tcpip and so as you can see in this diagram of the ip packet this is a basic datagram of what a packet would look like again with this source and destination ip address the protocol port number and the data itself now this is mainly just to give you a high level understanding of tcpip and udpip and is not a deep dive into networking now moving on to layer 7 of the osi model this layer is used by networked applications or applications that use the internet and so there are many protocols that fall under this layer now these applications do not reside in this layer but use the protocols in this layer to function so the application layer provides services for networked applications with the help of protocols to perform user activities and you will see many of these protocols being addressed as we go through this course through resources in google cloud like http or https for load balancing dns that uses udp on port 53 and ssh on port 22 for logging into hosts and so these are just a few of the many scenarios where layer 7 and the protocols that reside in that layer come up in this course and we will be diving into many more in the lessons to come and so that about wraps up this networking refresher lesson and don't worry like i said before i'm not expecting you to pick things up in this first go things will start to make more sense as we go through the course and we start putting these networking concepts into practice also feel free to go back and review the last couple of lessons again if things didn't make sense to you the first time or if you come across some networking challenges in future lessons and so that's everything i wanted to cover so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson we will be discussing the core networking service of gcp virtual private cloud or vpc for short it is the service that allows you to create networks inside google cloud with both private and public connectivity options both for incloud deployments and onpremise hybrid cloud deployments this is a service that you must know well as there are many questions that come up on the exam with regards to vpcs so with that being said let's dive in now vpcs are what manages the networking functionality for your google cloud resources this is a software defined network and is not confined to the physical limitations of networking in a data center this has been abstracted for you vpc networks including their associated routes and firewall rules are global resources they are not associated with any particular region or zone they are global resources and span all available regions across the globe as explained earlier vpcs are also encapsulated within projects projects are the logical container where your vpcs live now these vpcs do not have ip ranges but are simply a construct of all of the individual ip addresses and services within that network the ip addresses and ranges are defined within the subnetworks that i will be diving into a bit later as well traffic to and from instances can be controlled with network firewall rules rules are implemented on the vms themselves so traffic can be controlled and logged as it leaves or arrives at a vm now resources within a vpc network can communicate with one another by using internal or private ipv4 addresses and these are subject to applicable network firewall rules these resources must be in the same vpc for communication otherwise they must traverse the public internet with an assigned public ip or use a vpc peering connection or establish a vpn connection another important thing to note is that vpc networks only support ipv4 unicast traffic they do not support ipv6 traffic within the network vms in the vpc network can only send to ipv4 destinations and only receive traffic from ipv4 sources however it is possible to create an ipv6 address for a global load balancer now unless you choose to disable it each new project starts with a default network in a vpc the default network is an auto mode vpc network with predefined subnets a subnet is allocated for each region with nonoverlapping cider blocks also each default network has a default firewall rule these rules are configured to allow ingress traffic for icmp rdp and ssh traffic from anywhere as well as ingress traffic from within the default network for all protocols and ports and so there are two different types of vpc networks auto mode or custom mode an auto mode network also has one subnet per region the default network is actually an auto mode network as explained earlier now these automatically created subnets use a set of predefined ip ranges with a slash 20 cider block that can be expanded to a slash 16 cider block all of these subnets fit within the default 10.128.0.0 ford slash 9 cider block and as new gcp regions become available new subnets in those regions are automatically added to auto mode networks using an ip range on that block now a custom owned network does not automatically create subnets this type of network provides you with complete control over its subnets and ip ranges as well as another note an auto mode network can be converted to a custom mode network to gain more control but please be aware this conversion is one way meaning that custom networks cannot be changed to auto mode networks so when deciding on the different types of networks you want to use make sure that you review all of your considerations now custom mode vpc networks are more flexible and better suited to production and google recommends that you use custom mode vpc networks in production so here is an example of a project that contains three networks all of these networks span multiple regions across the globe as you can see here on the right hand side and each network contains separate vms and so this diagram is to demonstrate that vms that are in the same network or vpc can communicate privately even when placed in separate regions because vms in network a are in the same network they can communicate over internal ip addresses even though they're in different regions essentially your vms can communicate even if they exist in different locations across the globe as long as they are within the same network the vms in network b and network c are not in the same network therefore by default these vms must communicate over external ips even though they're in the same region as no internal ip communication is allowed between networks unless you set up vpc network peering or use a vpn connection now i wanted to bring back the focus to the default vpc for just a minute unless you create an organizational policy that prohibits it new projects will always start with a default network that has one subnet in each region and again this is an auto mode vpc network in this particular example i am showing a default vpc with seven of its default regions displayed along with their ip ranges and again i want to stress that vpc networks along with their associated routes and firewall rules are global resources they are not associated with any particular region or zone so the subnets within them are regional and so when an auto mode vpc network is created one subnet from each region is automatically created within it these automatically created subnets use a set of predefined ip ranges that fit within the cider block that you see here of 10.128.0.049 and as new google cloud regions become available new subnets in those regions are automatically added to auto mode vpc networks by using an ip range from that block in addition to the automatically created subnets you can add more subnets manually to auto mode vpc networks in regions that you choose by using ip ranges outside of 10.128.0.049 now if you're using a default vbc or have already created an auto mode vpc you can switch the vpc network from auto mode to custom mode and this is a oneway conversion only as custom mode vpc networks cannot be changed to auto mode vpc networks now bringing this theory into practice with regards to the default vpc i wanted to take the time to do a short demo so whenever you're ready join me in the console and so here we are back in the console and if i go here in the top right hand corner i am logged in as tony bowties at gmail.com and in the top drop down project menu i'm logged in under project tony and because this demo is geared around the default vpc i want to navigate to vpc networks so i'm going to go over here to the top left hand corner to the navigation menu and i'm going to click on it and scroll down to vpc network under networking and so as you can see here in the left hand menu there are a bunch of different options that i can choose from but i won't be touching on any of these topics as i have other lessons that will deep dive into those topics so in this demo i'd like to strictly touch on the default vpc and as you can see in project tony it has created a default vpc for me with a one subnet in every region having its own ip address range and so just as a reminder whenever you create a new project a default vpc will be automatically created for you and when these subnets were created each of them have a route out to the public internet and so the internet gateway is listed here its corresponding firewall rules along with global dynamic routing and flow logs are turned off and again i will be getting deeper into routing and flow logs in later lessons in the section now earlier i had pointed out that an auto mode vpc can be converted to a custom vpc and it's as simple as clicking this button but we don't want to do that just yet and what i'd like to do is drill down into the default vbc and show you all the different options as you can see here the dns api has not been enabled and so for most of you a good idea would be to enable it and so i'm going to go ahead and do that now as well you can see here that i can make adjustments to each of the different subnets or i can change the configuration of the vpc itself so if i click on this edit button here at the top i'm able to change the subnet creation mode along with the dynamic routing mode which i will get into in a later lesson and the same thing with the dns server policy and so to make this demo a little bit more exciting i want to show you the process on how to expand a subnet so i'm going to go into us central one i'm going to drill down here and here's all the configuration settings for the default subnet in the us central one region and so for me to edit this subnet i can simply click on the edit button up here at the top and so right below the ip address range i am prompted with a note saying that the ip ranges must be unique and nonoverlapping as we stated before and this is a very important point to know when you're architecting any vpcs or its corresponding sub networks and so i'm going to go ahead and change the subnet from a cider range of 20 and i'm going to change it to 16. i'm not going to add any secondary ip ranges i'm going to leave private google access off and so i'm going to leave everything else as is and simply click on save and so once this has completed i'll be able to see that my subnet range will go from a slash 20 to a slash 16. and so here you can see the ip address range has now changed to a slash 16. if i go back to the main page of the vpc network i can see that the ip address range is different from all the other ones now you're probably asking why can't i just change the ip address range on all the subnets at once and so even though i'd love to do that unfortunately google does not give you the option each subnet must be configured one by one to change the ipa address range now i wanted to quickly jump into the default firewall rules and as discussed earlier the rules for incoming ssh rdp and icmp have been prepopulated along with a default rule that allows incoming connections for all protocols and ports among instances within the same network so when it comes to routes with regards to the vpc network the only one i really wanted to touch on is the default route to the internet and so without this route any of the subnets in this vpc wouldn't have access to route traffic to the internet and so when the default vpc is created the default internet gateway is also created and so now going back to the main page of the vpc network i wanted to go through the process of making the ip address range bigger but doing it through the command line and so i'm going to go up to the right hand corner and open up cloud shell i'm going to make this a little bit bigger and so for this demo i'm going to increase the address range for the subnet in us west one from a slash 20 to a slash 16 and so i'm going to paste in the command which is gcloud compute networks subnets expand ip dash range and then the name of the network which is default as well as the region and i'm going to do uswest1 along with the prefix length which is going to be 16. so i'm going to hit enter i've been prompted to make sure that this is what i want to do and so yes i do want to continue so i'm going to type in y for yes and hit enter and so within a few seconds i should get some confirmation and as expected my subnet has been updated and so because i like to verify everything i'm going to now clear the screen and i'm going to paste in the command gcloud compute networks subnets describe and then the subnet name which is default along with the region which would be uswest1 i'm going to click on enter and as you can see here the ipsider range is consistent with what we have changed and if i do a quick refresh on the browser i'll be able to see that the console has reflected the same thing and as expected the ip address range here for us west one in the console reflects that which we see here in cloud shell and so now to end this demo i wanted to quickly show you how i can delete the default vpc and recreate it so all i need to do is to drill into the settings and then click on delete vpc network right here at the top i'm going to get a prompt to ask me if i'm sure and i'm going to simply click on delete now just as a note if you have any resources that are in any vpc networks you will not be able to delete the vpc you would have to delete the resources first and then delete the vpc afterwards okay and it has been successfully deleted and as you can see there are no local vpc networks in this current project and so i want to go ahead and recreate the default vpc so i'm going to simply click on create vpc network and so here i'm prompted to enter in a bunch of information for creating this new vpc network and so keeping with the spirit of default vpcs i'm going to name this vpc default i'm going to put default in the description and under subnet creation mode i'm going to click on automatic and as you can see a prompt came up telling me these ip address ranges will be assigned to each region in your vpc network and i'm able to review the ip address ranges for each region and as stated before the ip address ranges for each region will always be the same every time i create this default vpc or create a vpc in the automatic subnet creation mode now as a note here under firewall rules if i don't select these firewall rules none will actually be created so if you're creating a new default vpc be sure to check these off and so i'm going to leave everything else as is and i'm going to simply go to the bottom and click on the create button and within about a minute i should have the new default vpc created okay and we are back in business the default vpc has been recreated with all of these subnets in its corresponding regions all the ip address ranges the firewall rules everything that we saw earlier in the default vpc and so that's pretty much all i wanted to cover in this demo on the default vpc network along with the lesson on vpcs so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing vpc network subnets now the terms subnet and sub network are synonymous and are used interchangeably in google cloud as you'll hear me using either one in this lesson yet i am referring to the same thing now when you create a resource in google cloud you choose a network and a subnet and so because a subnet is needed before creating resources some good knowledge behind it is necessary for both building and google cloud as well as in the exam so in this lesson i'll be covering subnets at a deeper level with all of its features and functionality so with that being said let's dive in now each vpc network consists of one or more useful ip range partitions called subnets also known in google cloud as sub networks each subnet is associated with the region and vpc networks do not have any ip address ranges associated with them ip ranges are defined for the subnets a network must have at least one subnet before you can use it and as mentioned earlier when you create a project it will create a default vpc network with subnets in each region automatically auto mode will run under this same functionality now custom vpc networks on the other hand start with no subnets giving you full control over subnet creation and you can create more than one subnet per region you cannot change the name or region of a subnet after you've created it you would have to delete the subnet and replace it as long as no resources are using it primary and secondary ranges for subnets cannot overlap with any allocated range any primary or secondary range of another subnet in the same network or any ip ranges of subnets in peered networks in other words they must be a unique valid cider block now when it comes to ip addresses of a subnet google cloud vpc has an amazing feature that lets you increase the ip space of any subnets without any workload shutdown or downtime as demonstrated earlier in the previous lesson and this gives you the flexibility and growth options to meet your needs but unfortunately there are some caveats the new subnet must not overlap with other subnets in the same vpc network in any region also the new subnets must stay inside the rfc 1918 address space the new network range must be larger than the original which means the prefix length must be smaller in number and once a subnet has been expanded you cannot undo an expansion now auto mode network starts with a slash 20 range that can be expanded to a 16 ip range but not larger you can also convert the auto mode network to a custom mode network to increase the ip range even further and again this is a oneway conversion custom mode vpc networks cannot be changed to auto mode vpc networks now in any network that is created in google cloud there will always be some ip addresses that you will not be able to use and these are reserved for google and so every subnet has four reserved ip addresses in its primary ip range and just as a note there are no reserved ip addresses in the secondary ip ranges and these reserved ips can be looked at as the first two and the last two ip addresses in the cider range now the first address in the primary ip range for the subnet is reserved for the network the second address in the primary ip range for the subnet is reserved for the default gateway and allows you access to the internet the second to last address in the primary ip range for the subnet is reserved for google cloud for potential future use and the last address and the ip range for the subnet is for broadcast and so that about covers this short yet important lesson on vpc network subnets these features and functionalities of subnets that have been presented to you will help you make better design decisions that will give you a bit more knowledge and flexibility when it comes to assigning ipspace within your vpc networks and so that's all i have to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be going through routing and private google access now although routing doesn't really show up in the exam i wanted to give you an inside look on how traffic is routed so when you're building in google cloud you'll know exactly what you will need to do if you need to edit these routes in any way or if you need to build new ones to satisfy your particular need now private google access does pop its head in the exam but only at a high level but i wanted to get just a bit deeper with the service and get into the data flow of when the service is enabled so with that being said let's dive in now google cloud routes define the paths that network traffic takes from a vm instance to other destinations these destinations can be inside your google cloud vpc network for example in another vm or outside it in a vpc network a route consists of a single destination and a single next hop when an instance in a vpc network sends a packet google cloud delivers the packet to the route's next hop if the packet's destination address is within the route's destination range and so all these routes are stored in the routing table for the vpc now for those of you who are not familiar with a routing table in computer networking a routing table is a data table stored in a router or a network host that lists the routes to particular network destinations and so in this case the vpc is responsible for storing the routing table as well each vm instance has a controller that is kept informed of all applicable routes from the network's routing table each packet leaving a vm is delivered to the appropriate next hop of an applicable route based on a routing order now i wanted to take a couple minutes to go through the different routing types that are available on google cloud now in google cloud there are two types of routing there is the system generated which offers the default and subnet route and then there are the custom routes which support static routes and dynamic routes and so i first wanted to cover system generated routes in a little bit of depth and so every new network whether it be an automatic vpc or a custom vpc has two types of system generated routes a default route which you can remove or replace and one subnet route for each of its subnets now when you create a vpc network google cloud creates a system generated default route and this route serves two purposes it defines the path out of the vpc network including the path to the internet in addition to having this route instances must meet additional requirements if they need internet access the default route also provides a standard path for private google access and if you want to completely isolate your network from the internet or if you need to replace the default route with the custom route you can delete the default route now if you remove the default route and do not replace it packets destined to ip ranges that are not covered by other routes are dropped lastly the system generated default route has a priority of 1000 because its destination is the broadest possible which covers all ip addresses in the 0.0.0.0.0 range google cloud only uses it if a route with a more specific destination does not apply to a packet and i'll be getting into priorities in just a little bit and so now that we've covered the default route i wanted to get into the subnet route now subnet routes are system generated routes that define paths to each subnet in the vpc network each subnet has at least one subnet route whose destination matches the primary ip range of the subnet if the subnet has secondary ip ranges google cloud creates a subnet route with a corresponding destination for each secondary range no other route can have a destination that matches or is more specific than the destination of a subnet route you can create a custom route that has a broader destination range that contains the subnet route's destination range now when a subnet is created a corresponding subnet route for the subnet's primary and secondary ip range is also created auto mode vpc networks create a subnet route for the primary ip ranges of each of their automatically created subnets you can delete these subnets but only if you convert the auto mode vpc network to custom mode and you cannot delete a subnet route unless you modify or delete the subnet so when you delete a subnet all subnet routes for both primary and secondary ranges are deleted automatically you cannot delete the subnet route for the subnet's primary range in any other way and just as a note when networks are connected by using vpc network peering which i will get into a little bit later some subnet routes from one network are imported into the other network and vice versa and cannot be removed unless you break the peering relationship and so when you break the peering relationship all imported subnet routes from the other network are automatically removed so now that we've covered the system generated routes i wanted to get into custom routes now custom routes are either static routes that you can create manually or dynamic routes maintained automatically by one or more of your cloud routers and these are created on top of the already created system generated routes destinations for custom routes cannot match or be specific than any subnet route in the network now static routes can use any of the static route next hops and these can be created manually if you use the google cloud console to create a cloud vpn tunnel that uses policybased routing or one that is a route based vpn static routes for the remote traffic selectors are created for you and so just to give you a little bit more clarity and a little bit of context i've included a screenshot here for all the different routes that are available for the next hop we have the default internet gateway to define a path to external ip addresses specify an instance and this is where traffic is directed to the primary internal ip address of the vm's network interface in the vpc network where you define the route specify ip address is where you provide an internal ip address assigned to a google cloud vm as a next hop for cloud vpn tunnels that use policy based routing and routebased vpns you can direct traffic to the vpn tunnel by creating routes whose next hops refer to the tunnel by its name and region and just as a note google cloud ignores routes whose next hops are cloud vpn tunnels that are down and lastly for internal tcp and udp low balancing you can use a load balancer's ip address as a next hop that distributes traffic among healthy backend instances custom static routes that use this next hop cannot be scoped to specific instances by network tags and so when creating static routes you will always be asked for different parameters that are needed in order to create this route and so here i've taken a screenshot from the console to give you a bit more context with regards to the information that's needed so first up is the name and description so these fields identify the route a name is required but a description is optional and every route in your project must have a unique name next up is the network and each route must be associated with exactly one vpc network in this case it happens to be the default network but if you have other networks available you're able to click on the drop down arrow and choose a different network the destination range is a single ipv4 cider block that contains the ip addresses of systems that receive incoming packets and the ip range must be entered as a valid ipv4 cider block as shown in the example below the field now if multiple routes have identical destinations priority is used to determine which route should be used so a lower number would indicate a higher priority for example a route with a priority value of 100 has a higher priority than one with a priority value of 200 so the highest route priority means the smallest possible nonnegative number as well another great example is if you look back on your default routes all your subnet routes are of a priority of zero and the default internet gateway is of a priority of 1000 and therefore the subnet routes will take priority over the default internet gateway and this is due to the smaller number so remember a good rule of thumb is that the lower the number the higher the priority the higher the number the lower the priority now to get a little bit more granular you can specify a list of network tags so that the route only applies to instances that have at least one of the listed tags and if you don't specify any tags then google cloud applies the route to all instances in the network and finally next hop which was shown previously this is dedicated to static routes that have next hops that point to the options shown earlier so now that i've covered static routes in a bit of detail i want to get into dynamic routes now dynamic routes are managed by one or more cloud routers and this allows you to dynamically exchange routes between a vpc network and an onpremises network with dynamic routes their destinations always represent ip ranges outside of your vpc network and their next hops are always bgp peer addresses a cloud router can manage dynamic routes for cloud vpn tunnels that use dynamic routing as well as cloud interconnect and don't worry i'll be getting into cloud routers in a bit of detail in a later lesson now i wanted to take a minute to go through routing order and the routing order deals with priorities that i touched on a little bit earlier now subnet routes are always considered first because google cloud requires that subnet routes have the most specific destinations matching the ip address ranges of their respective subnets if no applicable destination is found google cloud drops the packet and replies with a network unreachable error system generated routes apply to all instances in the vpc network the scope of instances to which subnet routes apply cannot be altered although you can replace the default route and so just as a note custom static routes apply to all instances or specific instances so if the route doesn't have a network tag the route applies to all instances in the network now vpc networks have special routes that are used for certain services and these are referred to as special return paths in google cloud these routes are defined outside of your vpc network in google's production network they don't appear in your vpc network's routing table you cannot remove them or override them or if you delete or replace a default route in your vpc network although you can control traffic to and from these services by using firewall rules and the services that are covered are load balancers internet aware proxy or iap as well as cloud dns and so before i end this lesson i wanted to touch on private google access now vm instances that only have internal ip addresses can use private google access and this allows them to reach the external ip addresses of google's apis and services the source ip address of the packet can be the primary internal ip address of the network interface or an address in an alias ip range that is assigned to the interface if you disable private google access the vm instances can no longer reach google apis and services and will only be able to send traffic within the vpc network private google access has no effect on instances that have external ip addresses and can still access the internet they don't need any special configuration to send requests to the external ip addresses of google apis and services you enable private google access on a subnet by subnet basis and it's a setting for subnets in a vpc network and i will be showing you this in an upcoming demo where we'll be building our own custom vpc network now even though the next hop for the required routes is called the default internet gateway and the ip addresses for google apis and services are external requests to google apis and services from vms that only hold internal ip addresses in subnet 1 where private google access is enabled are not sent through the public internet those requests stay within google's network as well vms that only have internal ip addresses do not meet the internet access requirements for access to other external ip addresses beyond those for google apis and services now touching on this diagram here firewall rules in the vpc network have been configured to allow internet access vm1 can access google apis and services including cloud storage because its network interface is located in subnet 1 which has private google access enabled and because this instance only has an internal ip address private google access applies to this instance now with vm2 it can also access google apis and services including cloud storage because it has an external ip address private google access has no effect on this instance as it has an external ip address and private google access has not been enabled on that subnet and because both of these instances are in the same network they are still able to communicate with each other over an internal subnet route and so this is just one way where private google access can be applied there are some other options for private access as well you can use private google access to connect to google apis and services from your onpremises network through a cloud vpn tunnel or cloud interconnect without having any external ip addresses you also have the option of using private google access through a vpc network peering connection which is known as private services access and finally the last option available for private google access is connecting directly from serverless google services through an internal vpc connection now i know this has been a lot of theory to take in but i promise it'll become a lot easier and concepts will become less complicated when we start putting this into practice coming up soon in the demo of building our own custom vpc and so that's pretty much all i wanted to cover when it comes to routing and private google access so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing ip addressing now in the network refresher lesson i went into a bit of depth on how i p addresses are broken down and used for communication in computer networks in this lesson i'll be getting into the available types of ip addressing in google cloud and how they are used in each different scenario please note for the exam a high level overview will be needed to know when it comes to ip addressing but the details behind it will give you a better understanding on when to use each type of ip address so with that being said let's dive in now ip addressing in google cloud holds quite a few categories and really start by determining whether you are planning for communication internally within your vpc or for external use to communicate with the outside world through the internet once you determine the type of communication that you're looking to apply between resources some more decisions need to be made with regards to the other options and i will be going through these options in just a sec now in order to make these options a little bit more digestible i wanted to start off with the options available for internal ip addresses now internal ip addresses are not publicly advertised they are used only within a network now every vpc network or onpremises network has at least one internal ip address range resources with internal ip addresses communicate with other resources as if they're all on the same private network now every vm instance can have one primary internal ip address that is unique to the vpc network and you can assign a specific internal ip address when you create a vm instance or you can reserve a static internal ip address for your project and assign that address to your resources if you don't specify an address one will be automatically assigned to the vm in either case the address must belong to the ip range of the subnet and so if your network is an auto mode vpc network the address comes from the region subnet if your network is a custom mode vpc network you must specify which subnet the ip address comes from now all subnets have a primary sider range which is the range of internal ip addresses that define the subnet each vm instance gets its primary internal ip address from this range you can also allocate alias ip ranges from that primary range or you can add a secondary range to the subnet and allocate alias ip ranges from the secondary range use of alias ip ranges does not require secondary subnet ranges these secondary subnet ranges merely provide an organizational tool now when using ip aliasing you can configure multiple internal ip addresses representing containers or applications hosted in a vm without having to define a separate network interface and you can assign vm alias ip ranges from either the subnet's primary or secondary ranges when alias ip ranges are configured google cloud automatically installs vpc network routes for primary and alias ip ranges for the subnet of your primary network interface your container orchestrator or gke does not need to specify vpc network connectivity for these routes and this simplifies routing traffic and managing your containers now when choosing either an auto mode vpc or a custom vpc you will have the option to choose either an ephemeral ip or a static ip now an ephemeral ip address is an ip address that doesn't persist beyond the life of the resource for example when you create an instance or forwarding rule without specifying an ip address google cloud will automatically assign the resource an ephemeral ip address and this ephemeral ip address is released when you delete the resource when the ip address is released it is free to eventually be assigned to another resource so is never a great option if you depend on this ip to remain the same this ephemeral ip address can be automatically assigned and will be assigned from the selected region subnet as well if you have ephemeral ip addresses that are currently in use you can promote these addresses to static internal ip addresses so that they remain with your project until you actively remove them and just as a note before you reserve an existing ip address you will need the value of the ip address that you want to promote now reserving a static ip address assigns the address to your project until you explicitly release it this is useful if you are dependent on a specific ip address for a specific service and need to prevent another resource from being able to use the same address static addresses are also useful if you need to move an ip address from one google cloud resource to another and you also have the same options when creating an internal load balancer as you do with vm instances and so now that we've covered all the options for internal ip addresses i would like to move on to cover all the available options for external ip addresses now you can assign an external ip address to an instance or a forwarding rule if you need to communicate with the internet with resources in another network or need to communicate with a public google cloud service sources from outside a google cloud vpc network can address a specific resource by the external ip address as long as firewall rules enable the connection and only resources with an external ip address can send and receive traffic directly to and from outside the network and like internal ip addresses external ip addresses have the option of choosing from an ephemeral or static ip address now an ephemeral external ip address is an ip address that doesn't persist beyond the life of the resource and so follows the same rules as ephemeral internal ip addresses so when you create an instance or forwarding rule without specifying an ip address the resource is automatically assigned an ephemeral external ip address and this is something that you will see quite often ephemeral external ip addresses are released from a resource if you delete the resource for vm instances the ephemeral external ip address is also released if you stop the instance so after you restart the instance it is assigned a new ephemeral external ip address and if you have an existing vm that doesn't have an external ip address you can assign one to it forwarding rules always have an ip address whether external or internal so you don't need to assign an ip address to a forwarding rule after it is created and if your instance has an ephemeral external ip address and you want to permanently assign the ip to your project like ephemeral internal ip addresses you have the option to promote the ip address from ephemeral to static and in this case promoting an ephemeral external ip address to a static external ip address now when assigning a static ip address these are assigned to a project long term until they are explicitly released from that assignment and remain attached to a resource until they are explicitly detached for vm instances static external ip addresses remain attached to stopped instances until they are removed and this is useful if you are dependent on a specific ip address for a specific service like a web server or a global load balancer that needs access to the internet static external ip addresses can be either a regional or global resource in a regional static ip address allows resources of that region or resources of zones within that region to use the ip address and just as a note you can use your own publicly routable ip address prefixes as google cloud external ip addresses and advertise them on the internet the only caveat is that you must own and bring at the minimum a 24 cider block and so now that we've discussed internal and external ip addressing options i wanted to move into internal ip address reservations now static internal ips provide the ability to reserve internal ip addresses from the ip range configured in the subnet then assign those reserved internal addresses to resources as needed reserving an internal ip address takes that address out of the dynamic allocation pool and prevents it from being used for automatic allocations with the ability to reserve static internal ip addresses you can always use the same ip address for the same resource even if you have to delete and recreate the resource so when it comes to internal ip address reservation you can either reserve a static internal ip address before creating the associated resource or you can create the resource with an ephemeral internal ip address and then promote that ephemeral ip address to a static internal ip address and so just to give you a bit more context i have a diagram here to run you through it so in the first example you would create a subnet from your vpc network you would then reserve an internal ip address from that subnet's primary ip range and in this diagram is marked as 10.12.4.3 and will be held as reserved for later use with a resource and then when you decide to create a vm instance or an internal load balancer you can use the reserved ip address that was created in the previous step that i p address then becomes marked as reserved and in use now touching on the second example you would first create a subnet from your vpc network you would then create a vm instance or an internal load balancer with either an automatically allocated ephemeral ip address or a specific ip address that you've chosen from within that specific subnet and so once the ephemeral ip address is in use you can then promote the ephemeral ip address to a static internal ip address and would then become reserved and in use now when it comes to the external ip address reservation you are able to obtain a static external ip address by using one of the following two options you can either reserve a new static external ip address and then assign the address to a new vm instance or you can promote an existing ephemeral external ip address to become a static external ip address now in the case of external ip addresses you can reserve two different types a regional ip address which can be used by vm instances with one or more network interfaces or by network load balancers these ip addresses can be created either in the console or through the command line with the limitation that you will only be allowed to create ipv4 ip addresses the other type is a global ip address which can be used for global load balancers and can be created either in the console or through the command line as shown here the limitation here is that you must choose the premium network service tier in order to create a global ip address and after reserving the address you can finally assign it to an instance during instance creation or to an existing instance and so as you can see there is a lot to take in when it comes to understanding ip addressing and i hope this lesson has given you some better insight as to which type of ips should be used in a specific scenario now don't worry the options may seem overwhelming but once you start working with ip addresses more often the options will become so much clearer on what to use and when and as i said in the beginning only high level concepts are needed to know for the exam but knowing the options will allow you to make better decisions in your daily role as a cloud engineer and so that's pretty much all i wanted to cover when it comes to ip addressing in google cloud and so now that we've covered the theory behind ip addressing in google cloud i wanted to bring this into the console for a demo where we will get handson with creating both internal and external static ip addresses so as i explained before there was a lot to take in with this lesson so now would be a perfect opportunity to get up and have a stretch grab yourself a tea or a coffee and whenever you're ready join me back in the console so you can now mark this lesson as complete and i'll see you in the next welcome back in this demonstration i'm going to be going over how to create and apply both internal and external static ip addresses i'm going to show how to create them in both the console and the command line as well as how to promote ip addresses from ephemeral ips to static ips and once we're done creating all the ip addresses i'm going to show you the steps on how to delete them now there's a lot to get done here so let's dive in now for this demonstration i'm going to be using a project that has the default vpc created and so in my case i will be using project bowtieinc dev and so before you start make sure that your default vpc is created in the project that you had selected so in order to do that i'm going to head over to the navigation menu i'm going to scroll down to vpc network and we're going to see here that the default vpc has been created and so i can go ahead and start the demonstration and so the first thing i wanted to demonstrate is how to create a static internal ip address and so in order for me to demonstrate this i'm going to be using a vm instance and so i'm going to head over to the navigation menu again and i'm going to scroll down to compute engine and so here i'm going to create my new instance by simply clicking on create instance and so under name i'm going to keep it as instance 1. under region you want to select us east one and i'm going to keep the zone as the default selected under machine type i'm going to select the drop down and select e2 micro and i'm going to leave everything else as the default i'm going to scroll down here to management security disks networking and soul tenancy and i'm going to select the networking tab from there and so under here i'm going to select under network interfaces the default network interface and here is where i can create my static internal ip and so clicking on the drop down under primary internal ip you will see ephemeral automatic ephemeral custom and reserve static internal ip address and so you're going to select reserve static internal ip address and you'll get a popup prompting you with some fields to fill out to reserve a static internal ip address and so under name i'm going to call this static dash internal and for the purposes of this demo i'm going to leave the subnet and the static ip address as the currently selected if i wanted to select a specific ip address i can click on this drop down and select let me choose and this will give me the option to enter in a custom ip address with the subnet range that is selected for this specific sub network and so because i'm not going to do that i'm going to select assign automatically i'm going to leave the purpose as nonshared and i'm going to simply click on reserve and this is going to reserve this specific ip address and now as you can see here i have the primary internal ip marked as static internal and so this is going to be my first static internal ip address and so once you've done these steps you can simply click on done and you can head on down to the bottom and simply click on create to create the instance and when the instance finishes creating you will see the internal static ip address and as you can see here your static internal ip address has been assigned to the default network interface on instance 1. and so in order for me to view this static internal ip address in the console i can view this in vpc networks and drill down into the specific vpc and find it under static internal ip addresses but i wanted to show you how to view it by querying it through the command line and so in order to do this i'm going to simply go up to the menu bar on the right hand side and open up cloud shell and once cloud shell has come up you're going to simply paste in the command gcloud compute addresses list and this will give me a list of the internal ip addresses that are available and so now i'm going to be prompted to authorize this api call using my credentials and i definitely do so i'm going to click on authorize and as expected the static internal ip address that we created earlier has shown up it's marked as internal in the region of us east one in the default subnet and the status is in use and so as we discussed in the last lesson static ip addresses persist even after the resource has been deleted and so to demonstrate this i'm going to now delete the instance i'm going to simply check off the instance and go up to the top and click on delete you're going to be prompted to make sure if you want to delete this yes i do so i'm going to click on delete and so now that the instance has been deleted i'm going to query the ip addresses again by using the same command gcloud compute addresses list i'm going to hit enter and as you can see here the ip address static dash internal still persists but the status is now marked as reserved and so if i wanted to use this ip address for another instance i can do so by simply clicking on create instance up here at the top menu and then i can select static dash internal as my ip address so i'm going to quickly close down cloud shell and i'm going to leave the name as instance one the region can select us east one and we're going to keep the zone as the default selected under machine type you're going to select the e2 micro machine type going to scroll down to management security disks networking into soul tenancy and i'm going to select the networking tab from under here and under network interfaces i'm going to select the default network interface and under primary internal ip if i click on the drop down i have the option of selecting the static dash internal static ip address and so i wanted to move on to demonstrate how to promote an internal ephemeral ip address to an internal static ip address and so in order to do this i'm going to select on ephemeral automatic and i'm going to scroll down and click on done and i'm going to go ahead and create the instance and once the instance is ready i'll be able to go in and edit the network interface and so the instance is up and ready and so i'm going to drill down into the instance and i'm going to go up to the top and click on edit i'm going to scroll down to network interfaces and i'm going to edit the default network interface so i'm going to scroll down a little bit more and here under internal iptype i'm going to click on the drop down and i'm going to select static and so here you are taking the current ip address which is 10.142.0.4 and promoting it to a static internal ip address and so you're going to be prompted with a popup confirming the reservation for that static internal ip address and so notice that i don't have any other options and so all i'm going to do is type in a name and i'm going to call this promoted static and i'm going to click on reserve and this will promote the internal ip address from an ephemeral ip address to a static ip address and so now i'm just going to click on done and i'm going to scroll down and click on save and so now because i want to verify the ip address i'm going to go ahead and open up the cloud shell again and i'm going to use the same command that i used earlier which is gcloud compute addresses list and i'm going to hit enter as expected the promoted static ip address is showing as an internal ip address in the region of us east 1 in the default subnet and its status is in use and so just as a recap we've created a static internal ip address for the first instance and for the second instance we promoted an ephemeral internal ip address into a static internal ip address and we were able to verify this through cloud shell using the gcloud compute addresses list command and so this is the end of part one of this demo it was getting a bit long so i decided to break it up and this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready join me in part two where we will be starting immediately from the end of part one so you can now mark this as complete and i'll see you in the next one welcome back this is part two of the creating internal and external ip addresses demo and we will be starting immediately from the end of part one so with that being said let's dive in and so now that we've gone through how to both create static ip addresses and promote ephemeral ip addresses to static ip addresses for internal ips i want to go ahead and go through the same with external ips and so i'm going to first start off by deleting this instance i'm going to go ahead and click on delete and so instead of doing it through the compute engine interface i want to go into the external ip address interface which can be found in the vpc network menu so i'm going to go ahead up to the left hand corner click on the navigation menu and i'm going to scroll down to vpc network and from the menu here on the left hand side you can simply click on external ip addresses and here you will see the console where you can create a static external ip address and so to start the process you can simply click on reserve static address and so here you'll be prompted with a bunch of fields to fill out to create this new external static ip address and so for the name of this static ip address you can simply call this external dash static i'm going to use the same in the description now here under network service tier i can choose from either the premium or the standard and as you can see i'm currently using the premium network service tier and if i hover over the question mark over here it tells me a little bit more about this network service tier and as you can see the premium tier allows me higher performance as well as lower latency routing but this premium routing comes at a cost whereas the standard network service tier offers a lower performance compared to the premium network service tier and is a little bit more cost effective but still delivering performance that's comparable with other cloud providers and so i'm just going to leave it as the default selected and as we discussed in the previous lesson ipv6 external static ip addresses can only be used for global load balancers and so since we're only using it for an instance an ipv4 address will suffice and so just as a note for network service tier if i click on standard ipv6 is grayed out as well as the global selection and this is because in order to use global load balancing you need to be using the premium network service tier so whenever you're creating a global load balancer please keep this in mind as your cost may increase so i'm going to switch this back to premium and so under type i'm going to keep it as regional and under region i'm going to select the same region that my instance is going to be in which is us east 1 and because i haven't created the instance yet there is nothing to attach it to and so i'm going to click on the drop down and click on none and so just as another note i wanted to quickly highlight this caution point that the static ip addresses not attached to an instance or low balancer are still billed at an hourly rate so if you're not using any static ip addresses please remember to delete them otherwise you will be charged and so everything looks good here to create my external static ip address so i'm going to simply click on reserve and this will create my external static ip address and put the status of it as reserved so as you can see here the external static ip address has been created and you will find all of your external static ip addresses that you create in future right here in this menu and you will still be able to query all these external ip addresses from the command line and so now in order to assign this ip address to a network interface i'm going to go back over to the navigation menu and scroll down to compute engine and create a new instance so you can go ahead and click on create instance i'm going to go ahead and keep the name of this instance as instance one and in the region i'm going to select us east one i'm going to keep the zone as the selected default and under machine type i'm going to select the e2 micro machine type i'm going to scroll down to management security disks networking and soul tenancy and i'm going to select the networking tab and here under network interfaces i'm going to select the default network interface i'm going to scroll down a little bit here and under external ip ephemeral has been selected but if i click on the drop down i will have the option to select the ip that we had just created which is the external dash static ip and so i'm going to select that i'm going to click on done and you can go down and click on create and so now when the instance is created i will see the external ip address of external static as the assigned external ip and as expected here it is and because i always like to verify my work i'm going to go ahead and open up the cloud shell and verify it through the command line and so now i'm going to query all my available static ip addresses using the command gcloud compute addresses list i'm going to hit enter and as you can see here the external static ip address of 34.75.76 in the us east one region is now in use and this is because it is assigned to the network interface on instance one and so before we go ahead and complete this demo there's one more step that i wanted to go through and this is to promote an ephemeral external ip address to a static external ip address and so i'm going to go up here to the top menu and create a new instance i'm going to leave the name here as instance two under the region i'm going to select us east one i'm going to keep the zone as the selected default under machine type i'm going to select the e2 micro machine type i'm going to leave everything else as the default and i'm going to scroll down to management security disks networking and soul tenancy and select the networking tab and i'm going to verify that i'm going to be using an ephemeral external ip upon the creation of this instance if i scroll down here a little bit i can see that an external ephemeral ip address will be used upon creation and this will be the ip address that i will be promoting to a static ip through the command line so i'm going to go ahead and scroll down click on done and then i'm going to scroll down and click on create and once this instance is created then i can go ahead and promote the ephemeral external ip address okay and the instance has been created along with its external ephemeral ip address and so now i can go ahead and promote this ephemeral ip address so in order for me to do this i'm going to move back to my cloud shell and i'm going to quickly clear my screen and i'm going to use the command gcloud compute addresses create and then the name that we want to use for this static external ip address so i'm going to call this promoted external i'm going to use the flag dash dash addresses and so here i will need the external ip address that i am promoting which is going to be 104.196.219.42 and so i'm going to copy this to my clipboard and i'm going to paste it here in the command line and now i'm going to add the region flag along with the region of us east one and i'm going to go ahead and hit enter and success my ephemeral external ip address has been promoted to a static external ip address and of course to verify it i'm going to simply type in the gcloud compute addresses list command i'm going to hit enter and as expected here it is the promoted external ip of 104.196.219.42 marked as external in the u.s east one region and the status is marked as in use and so i wanted to take a moment to congratulate you on making it through this demonstration of creating internal and external ip addresses as well as promoting them so just as a recap you've created a static internal ip address in conjunction with creating a new instance and assigning it to that instance you then created another instance and used an ephemeral ip and then promoted it to a static internal ip address you then created an external static ip address using the console and assigned it to a brand new instance you then created another instance using an external ephemeral ip address and promoted it to a static external ip address and you did this all using both the console and the command line so i wanted to congratulate you on a great job now before we end this demonstration i wanted to go through the steps of cleaning up any leftover resources so the first thing you want to do is delete these instances so you can select them all and go up to the top and click on delete it's going to ask you if you want to delete the two instances yes we do click on delete and this will delete your instances and free up the external ip addresses so that you're able to delete them and so now that the instances have been deleted i'm going to go over to the vpc network menu and i'm going to head on over to the external ip address console and here i'm able to delete the external ip addresses and so i'm going to select all of them and i'm going to go up to the top menu and click on release static address and you should get a prompt asking you if you want to delete both these addresses the answer is yes click on delete and within a few seconds these external ip addresses should be deleted and so now all that's left to delete are the two static internal ip addresses and as i said before because there is no console to be able to view any of these static internal ip addresses i have to do it through the command line so i'm going to go back to my cloud shell i'm going to clear the screen and i'm going to list the ip addresses currently in my network and so here they are promoted static and static internal and so the command to delete any static ip addresses is as follows gcloud compute addresses delete the name of the ip address that i want to delete which is promoted static and then i will need the region flag and it'll be the region of us east one and i'm going to go ahead and hit enter it's going to prompt me if i want to continue with this and i'm going to type y for yes hit enter and success it has been deleted and so just a double check i'm going to do a quick verification and yes it has been deleted and so all that's left to delete is the static internal ip address and so i'm going to paste in the command gcloud compute addresses delete the name of the ip address that i want to delete which is static dash internal along with the region flag of us east one i'm going to go ahead and hit enter y for yes to continue and success and one last verification to make sure that it's all cleared up and as you can see i have no more static i p addresses and so this concludes this demonstration on creating assigning and deleting both static internal and static external ip addresses and so again i wanted to congratulate you on a great job and so that's pretty much all i wanted to cover in this demo on creating internal and external static ip addresses so you can now mark this as complete and i'll see you in the next one welcome back in this lesson i will be diving into some network security by introducing vpc firewall rules a service used to filter incoming and outgoing network traffic based on a set of userdefined rules a concept that you should be fairly familiar with for the exam and comes up extremely often when working as an engineer in google cloud it is definitely an essential security layer that prevents unwanted access to your cloud infrastructure now vpc firewall rules apply to a given project and network and if you'd like you can also apply firewall rules across an organization but i will be sticking to strictly vpc firewall rules in this lesson now vpc firewall rules let you allow or deny connections to or from your vm instances based on a configuration that you specify and these rules apply to either incoming connections or outgoing connections but never both at the same time enabled vpc firewall rules are always enforced regardless of their configuration and operating system even if they have not started up now every vpc network functions as a distributed firewall when firewall rules are defined at the network level connections are allowed or denied on a per instance basis so you can think of the vpc firewall rules as existing not only between your instances and other networks but also between individual instances within the same network now when you create a vpc firewall rule you specify a vpc network and a set of components that define what the rule does the components enable you to target certain types of traffic based on the traffic's protocol ports sources and destinations when you create or modify a firewall rule you can specify the instances to which it is intended to apply by using the target component of the rule now in addition to firewall rules that you create google cloud has other rules that can affect incoming or outgoing connections so for instance google cloud doesn't allow certain ip protocols such as egress traffic on tcp port 25 within a vpc network and protocols other than tcp udp icmp and gre to external ip addresses of google cloud resources are blocked google cloud always allows communication between a vm instance and its corresponding metadata server at 169.254 and this server is essential to the operation of the instance so the instance can access it regardless of any firewall rules that you configure the metadata server provides some basic services to the instance like dhcp dns resolution instance metadata and network time protocol or ntp now just as a note every network has two implied firewall rules that permit outgoing connections and block incoming connections firewall rules that you create can override these implied rules now the first implied rule is the allow egress rule and this is an egress rule whose action is allow and the destination is all ips and the priority is the lowest possible and lets any instance send traffic to any destination except for traffic blocked by google cloud the second implied firewall rule is the deny ingress rule and this is an ingress rule whose action is deny and the source is all ips and the priority is the lowest possible and protects all instances by blocking incoming connections to them now i know we touched on this earlier on in a previous lesson but i felt the need to bring it up as these are prepopulated rules and the rules that i'm referring to are with regards to the default vpc network and as explained earlier these rules can be deleted or modified as necessary the rules as you can see here in the table allow ingress connections from any source to any instance on the network when it comes to icmp rdp on port 3389 for windows remote desktop protocol and for ssh on port 22. and as well the last rule allows ingress connections for all protocols and ports among instances in the network and it permits incoming connections to vm instances from others in the same network and all of these have a rule priority of six five five four which is the second to lowest priority so breaking down firewall rules there are a few characteristics that google put in place that help define these rules and the characteristics are as follows each firewall rule applies to incoming or outgoing connections and not both firewall rules only support ipv4 connections so when specifying a source for an ingress rule or a destination for an egress rule by address you can only use an ipv4 address or ipv4 block insider notation as well each firewall rules action is either allow or deny you cannot have both at the same time and the rule applies to connections as long as it is enforced so for example you can disable a rule for troubleshooting purposes and then enable it back again now when you create a firewall rule you must select a vpc network while the rule is enforced at the instance level its configuration is associated with a vpc network this means you cannot share firewall rules among vpc networks including networks connected by vpc network peering or by using cloud vpn tunnels another major thing to note about firewall rules is that they are stateful and so that means when a connection is allowed through the firewall in either direction return traffic matching this connection is also allowed you cannot configure a firewall rule to deny associated response traffic return traffic must match the five tuple of the accepted request traffic but with the source and destination addresses and ports reversed so just as a note for those who may be wondering what a five tuple is i was referring to the set of five different values that comprise a tcpip connection and this would be source ip destination ip source port destination port and protocol google cloud associates incoming packets with corresponding outbound packets by using a connection tracking table google cloud implements connection tracking regardless of whether the protocol supports connections if a connection is allowed between a source and a target or between a target and a destination all response traffic is allowed as long as the firewalls connections tracking state is active and as well as a note a firewall rules tracking state is considered active if at least one packet is sent every 10 minutes now along with the multiple characteristics that make up a firewall rule there are also firewall rule components that go along with it here i have a screenshot from the console with the configuration components of a firewall rule and i wanted to take a moment to highlight these components for better clarity so now the first component is the network and this is the vpc network that you want the firewall rule to apply to the next one is priority which we discussed earlier and this is the numerical priority which determines whether the rule is applied as only the highest priority rule whose other components match traffic is applied and remember the lower the number the higher the priority the higher the number the lower the priority now the next component is the direction of traffic and these are the ingress rules that apply to incoming connections from specified sources to google cloud targets and this is where ingress rules apply to incoming connections from specified sources to google cloud targets and egress rules apply to connections going to specify destinations from targets and the next one up is action on match and this component either allows or denies which determines whether the rule permits or blocks the connection now a target is what defines which instances to which the rule applies and you can specify a target by using one of the following three options the first option are all instances in the network and this is the firewall rule that does exactly what it says it applies to all the instances in the network the second option is instances by target tags and this is where the firewall rule applies only to instances with a matching network tag and so i know i haven't explained it earlier but a network tag is simply a character string added to a tags field in a resource so let's say i had a bunch of instances that were considered development i can simply throw a network tag on them using a network tag of dev and apply the necessary firewall rule for all the development servers holding the network tag dev and so the third option is instances by target service accounts this is where the firewall rule applies only to instances that use a specific service account and so the next component is the source filter and this is a source for ingress rules or a destination for egress rules the source parameter is only applicable to ingress rules and it must be one of the following three selections source ip ranges and this is where you specify ranges of ip addresses as sources for packets either inside or outside of google cloud the second one is source tags and this is where the source instances are identified by a matching network tag and source service accounts where source instances are identified by the service accounts they use you can also use service accounts to create firewall rules that are a bit more granular and so one of the last components of the firewall rule is the protocols and ports you can specify a protocol or a combination of protocols and their ports if you omit both protocols and ports the firewall rule is applicable for all traffic on any protocol and any port and so when it comes to enforcement status of the firewall rule there is a drop down right underneath all the components where you can enable or disable the enforcement and as i said before this is a great way to enable or disable a firewall rule without having to delete it and is great for troubleshooting or to grant temporary access to any instances and unless you specify otherwise all firewall rules are enabled when they are created but you can also choose to create a rule in a disabled state and so this covers the vpc firewall rules in all its entirety and i will be showing you how to implement vpc firewall rules along with building a custom vpc custom routes and even private google access all together in a demo following this lesson to give you some handson skills of putting it all into practice and so that's pretty much all i wanted to cover when it comes to vpc firewall rules so you can now mark this lesson as complete and let's move on to the next one where we dive in and build our custom vpc so now is a perfect time to grab a coffee or tea and whenever you're ready join me in the console welcome back in this demonstration i want to take all the concepts that we've learned so far in this networking section and put it all into practice this diagram shown here is the architecture of exactly what we will be building in this demo we're going to start by creating a custom vpc and then we're going to create two subnets one public and one private in two separate regions we're then going to create a cloud storage bucket with some objects in it and then we will create some instances to demonstrate access to cloud storage as well as communication between instances and finally we're going to create some firewall rules for routing traffic to all the right places we're also going to implement private google access and demonstrate accessibility to the files in cloud storage from the private instance without an external ip so this may be a little bit out of your comfort zone for some but don't worry i'll be with you every step of the way and other than creating the instances all the steps here have been covered in previous lessons now there's a lot to get done here so whenever you're ready join me in the console and so here we are back in the console and as you can see up here in the right hand corner i am logged in as tony bowtie ace gmail.com and currently i am logged in under project tony and so in order to start off on a clean slate i'm going to create a new project and so i'm going to simply click on the project menu dropdown and click on new project i'm going to call this project bowtie inc and i don't have any organizations so i'm going to simply click on create and as well for those of you doing this lesson i would also recommend for you to create a brand new project so that you can start off anew again i'm going to go over to the project drop down and i'm going to select bow tie ink as the project and now that i have a fresh new project i can now create my vpc network so i'm going to go over to the left hand corner to the navigation menu and i'm going to scroll down to vpc network and so because vpc networks are tied in with the compute engine api we need to enable it before we can create any vpc networks so you can go ahead and enable this api so once this api has finished and is enabled we'll be able to create our vpc network ok and the api has been enabled and as you can see the default vpc network has been created with a subnet in every region along with its corresponding ip address ranges and so for this demo we're going to create a brand new vpc network along with some custom subnets and so in order to do that i'm going to go up here to the top and i'm going to click on create vpc network and so here i'm prompted with some fields to fill out so under name i'm going to think of a creative name that i can call my vpc network so i'm going to simply call it custom under description i'm going to call this custom vpc network and i'm going to move down here to subnets and because i'm creating custom subnets i'm going to keep it under custom under subnet creation mode and so i'm going to need a public subnet and a private subnet and you'll be able to get the values from the text file in the github repository within the sub networks folder under networking services and so i'm going to create my public subnet first and i'm going to simply call the public subnet public for region i'm going to use us east one and the ip address range will be 10.0.0.0 forward slash 24 and i'm going to leave private google access off and i'm going to simply click on done and now i can create the private subnet so underneath the public subnet you'll see add subnet you can simply click on that and the name of the new subnet will be as you guessed it private under region i'm going to use us east 4 and for the ip address range be sure to use 10.0.5.0.24 and we're going to leave private google access off for now and we'll be turning that on a little bit later in the demo and so you can now click on done and before we click on create we want to enable the dns api and clicking on enable will bring you to the dns api home page and you can click on enable to enable the api okay so now that we have our network configured along with our public and private subnets as well as dns being enabled we can now simply click on create but before i do that i wanted to give you some insight with regards to the command line so as i've shared before everything that can be done in the console can be done through the command line and so if ever you wanted to do that or you wanted to get to know the command line a little bit better after filling out all the fields with regards to creating resources in the console you will be given the option of a command line link that you can simply click on and here you will be given all the commands to create all the same resources with all the same preferences through the command line and i will be providing these commands in the lesson text so that you can familiarize yourself with the commands to use in order to build any networks using the command line but this is a great reference for you to use at any time and so i'm going to click on close and now i'm going to click on create and within a minute or two the custom vpc network will be created and ready to use okay and the custom vpc network has been created along with its public and private subnet and so just to get a little bit more insight with this custom vpc network i'm going to drill down into it and as you can see here the subnets are respectively labeled private and public along with its region ip address range the gateway and private google access the routes as you can see here are the system generated routes that i had discussed in an earlier lesson it has both the subnet routes to its respective ip range along with the default route with a path to the internet as well as a path for private google access now we don't have any firewall rules here yet but we'll be adding those in just a few minutes and so now that you've created the vpc network with its respective subnets we're going to head on over to cloud storage and create a bucket along with uploading the necessary files so i'm going to go again over to the navigation menu and i'm going to scroll down to storage and so as expected there are no buckets present here in cloud storage and so we're just going to go ahead and create our first bucket by going up here to the top menu and clicking on create bucket and so here i've been prompted to name my bucket and for those of you who are here for the first time when it comes to naming a storage bucket the name needs to be globally unique and this means that the name has to be unique across all of the google cloud platform now don't worry i'm going to get into further detail with this in the cloud storage lesson with all of these specific details when it comes to names storage classes and permissions and so in the meantime you can come up with a name for your bucket something that resonates with you and so for me i'm going to name my bucket bowtie inc dash file dash access and so now i'm going to simply click continue and so just as a note for those who are unable to continue through it is because the name for your bucket is not globally unique so do try to find one that is now when it comes to location type i'm just going to click on region and you can keep the default location as used one and i'm going to leave all the other options as default and i'm going to go down to the bottom and click create and so for those of you who have created your bucket you can now upload the files and those files can be found in the github repository in the cloud storage bucket folder under networking services and so now i'm going to click on upload files and under the networking services section under cloud storage bucket you will find these three jpeg files and you can simply select them and click on open and so they are now uploaded into the bucket and so now i'm ready to move on to the next step so you should now have created the vpc network with a private and public subnet along with creating your own bucket in cloud storage and have uploaded the three jpeg files so now that this is done we can now create the instances that will have access to these files and so again i will go over to the navigation menu in the top left hand corner and scroll down to compute engine and here i will click on create and so again i will be prompted with some fields to fill out and so for this instance i'm going to first create the public instance again i'm going to get really creative and call this public dash instance under labels i'm going to add a label under key i'm going to type environment and under value i'm going to type in public i'm going to go down to the bottom and click on save and under region i'm going to select us east1 and you can leave the zone as us east 1b moving down under machine type i'm going to select the e2 micro as the machine type just because i'm being cost conscious and i want to keep the cost down and so i'm going to scroll down to identity and api access and under service account you should have the compute engine default service account already preselected now under access scopes i want to be able to have the proper permissions to be able to read and write to cloud storage along with read and write access to compute engine and so you can click on set access for each api and you can scroll down to compute engine click on the drop down menu and select read write and this will give the public instance the specific access that it needs to ssh into the private instance and so now i'm going to set the access for cloud storage so i'm going to scroll down to storage i'm going to click on the drop down menu and select read write and this will give the instance read write access to cloud storage scrolling down a little bit further i'm going to go to management security disks networking and sold tenancy and i'm going to click on that scroll up here just a little bit and you can click on the networking tab which will prompt you for a bunch of options that you can configure for the networking of the instance so under network tags i want to type in public and you can click enter you can then scroll down to where it says network interfaces and click on the current interface which is the default and here it'll open up all your options and so under network you want to click on the drop down and set it from default to custom the public subnet will automatically be propagated so you can leave it as is and you also want to make sure that your primary internal ip as well as your external ip are set to ephemeral and you can leave all the other options as default and simply click on done and again before clicking on create you can click on the command line link and it will show you all the commands needed in order to create this instance through the command line so i'm going to go ahead and close this and so i'm going to leave all the other options as default and i'm going to click on create and so now that my public instance is being created i'm going to go ahead and create my private instance using the same steps that i did for the last instance so i'm going to go ahead and click on create instance here at the top and so the first thing i'm going to be prompted for is the name of the instance and so i'm going to call this instance private dash instance and here i'm going to add a label the key being environment and the value being private i'm going to go down here to the bottom and click on save and under region i'm going to select us east 4 and you can keep the zone as the default selected under machine type we're going to select the e2 micro and again scrolling down to the identity and api access under the access scopes for the default service account i'm going to click on the set access for each api and i'm going to scroll down to storage i'm going to click on the drop down menu and i'm going to select access for read write and for the last step i'm going to go into the networking tab under management security disks networking and soul tenancy and under network tags i'm going to give this instance a network tag of private and under network interfaces we want to edit this and change it from default over to the custom network and as expected it selected the private subnet by default and because this is going to be a private instance we are not going to give this an external ip so i'm going to click on the drop down and select none and with all the other options set as default i'm going to simply click on create and this will create my private instance along with having my public instance so just as a recap we've created a new custom vpc network along with a private and public subnet we've created a storage bucket and added some files in it to be accessed and we've created a private and public instance and assigning the service account on the public instance read write access to both compute engine and cloud storage along with a public ip address and assigning the service account on the private instance read write access only for cloud storage and no public ip and so this is the end of part one of this demo and this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready you can join me in part two where we will be starting immediately from the end of part one so you can go ahead and complete this video and i will see you in part two welcome back this is part two of the custom vpc demo and we will be starting exactly where we left off from part one so with that being said let's dive in and so now the last thing that needs to be done is to simply create some firewall rules and so with these firewall rules this will give me ssh access into the public instance as well as allowing private communication from the public instance to the private instance as well as giving ssh access from the public instance to the private instance and this will allow us to access the files in the bucket from the private instance and so in order to create these firewall rules i need to go back to my vpc network so i'm going to go up to the left hand corner again to the navigation menu and scroll down to vpc network over here on the left hand menu you'll see firewall i'm going to click on that and here you will see all the default firewall rules for the default network so for us to create some new ones for the custom vpc i'm going to go up here to the top and click on create firewall and so the first rule i want to create is for my public instance and i want to give it public access as well as ssh access and so i'm going to name this accordingly as public dash access i'm going to give this the same description always a good idea to turn on logs but for this demonstration i'm going to keep them off under network i'm going to select the custom network i'm going to keep the priority at 1000 the direction of traffic will be ingress and the action on match will be allow and so here is where the target tags come into play when it comes to giving access to the network so targets we're going to keep it as specified target tags and under target tags you can simply type in public under source filter you can keep it under ip ranges and the source ip range will be 0.0.0.0 forward slash 0. and we're not going to add a second source filter here so moving down to protocols and ports under tcp i'm going to click that off and add in port 22. and because i want to be able to ping the instance i'm going to have to add another protocol which is icmp and again as explained earlier the disable rule link will bring up the enforcement and as you can see it is enabled but if you wanted to create any firewall rules in future and have them disabled you can do that right here but we're gonna keep this enabled and we're gonna simply click on create and this will create the public firewall rule for our public instance in our custom vpc network and so we're going to now go ahead and create the private firewall rule and so i'm going to name this private dash access respectively i'm going to put the description as the same under network i'm going to select our custom network keep the priority at 1000 direction of traffic should be at ingress and the action on match should be allow for target tags you can type in private and then hit enter and because i want to be able to reach the private instance from the public instance the source ip range will be 10.0.0.1 forward slash 24. we're not going to add a second source filter and under protocols and ports we're going to simply add tcp port 22 and again i want to add icmp so that i'm able to ping the instance and i'm going to click on create and so we now have our two firewall rules private access and public access and if i go over to the custom vpc network and i drill into it i'll be able to see these selective firewall rules under the respective firewall rules tab and so now that we've created our vpc network along with the public and private subnet we've created the cloud storage bucket with the files that we need to access the instances that will access those files along with the firewall rules that will allow the proper communication we can now go ahead to test everything that we built and make sure that everything is working as expected so let's kick things off by first logging into the public instance so you can head on over to the navigation menu and scroll down to compute engine and you can ssh into the public instance by clicking on ssh under connect and this should open up a new tab or a new window logging you in with your currently authenticated credentials okay and we are logged into our instance and i'm going to zoom in for better viewing and so just to make sure that everything is working as expected we know that our firewall rule is correct because we are able to ssh into the instance and now i want to see if i have access to my files in the bucket and so in order to do that i'm going to run the gsutil command ls for list and then gs colon forward slash forward slash along with my bucket name which is bow tie inc hyphen file iphone access and i'm going to hit enter and as you can see i have access to all the files in the bucket and the last thing i wanted to check is if i can ping the private instance so i'm going to first clear my screen and i'm going to head on over back to the console i'm going to copy the ip address of the private instance to my clipboard and then i'm going to head back on over to my terminal and i'm going to type in ping i'm going to paste the ip address and success i am able to successfully ping the private instance from the public instance using the icmp protocol and you can hit control c to stop the ping so now that i know that my public instance has the proper permissions to reach cloud storage as well as being able to ping my private instance i want to be able to check if i can ssh into the private instance from my public instance and so i'm going to first clear my screen and next i'm going to paste in this command in order for me to ssh into the private instance g cloud compute ssh dash dash project and my project name which is bow tie inc dash dash zone and the zone that my instance is in which is us east 4c along with the name of the instance which is private dash instance and along with the flag dash dash internal dash ip stating that i am using the internal ip in order to ssh into the instance and i'm going to hit enter and so now i've been prompted for a passphrase in order to secure my rsa key pair as one is being generated to log into the private instance now it's always good practice when it comes to security to secure your key pair with a passphrase but for this demo i'm just going to leave it blank and so i'm just going to hit enter i'm going to hit enter again now i don't want to get too deep into it but i did want to give you some context on what's happening here so when you log into an instance on google cloud with os login disabled google manages the authorized keys file for new user accounts based on ssh keys in metadata and so the keys that are being generated that are being used for the first time are currently being stored within the instance metadata so now that i'm logged into my private instance i'm going to quickly clear my screen and just as a note you'll be able to know whether or not you're logged into your private instance by looking here at your prompt and so now i want to make sure that i can ping my public instance so i'm going to quickly type the ping command i'm going to head on over to the console i'm going to grab the ip address of the public instance i'm going to go back to my terminal and paste it in and as expected i'm able to ping my public instance from my private instance i'm just going to go ahead and hit control c to stop and i'm going to clear the screen so now we'd like to verify whether or not we have access to the files in the cloud storage bucket that we created earlier and so now i'm going to use the same command that i used in the public instance to list all the files in the cloud storage bucket so i'm going to use the gsutil command ls for list along with gs colon forward slash forward slash and the bucket name which is bow tie ink hyphen file if an access and i'm going to hit enter and as you can see here i'm not getting a response and the command is hanging and this is due to the fact that external access is needed in order to reach cloud storage and this instance only has an internal or private ip so accessing the files in the cloud storage bucket is not possible now in order to access cloud storage and the set of external ip addresses used by google apis and services we can do this by enabling private google access on the subnet used by the vms network interface and so we're going to go ahead and do that right now so i'm going to hit control c to stop and i'm going to go back into the console i'm going to go to the navigation menu and i'm going to scroll down to vpc network and then i'm going to drill down into the private subnet and i'm going to edit it under private google access i'm going to turn it on and i'm going to go down to the bottom and click on save and by giving this subnet private google access i will allow the private instance and any instances with private ip addresses to access any public apis such as cloud storage so now when i go back to my instance i'm going to clear the screen here and i'm going to run the gsutil command again and success we are now able to access cloud storage due to enabling private google access on the respective private subnet so i first wanted to congratulate you on making it to the end of this demo and hope that this demo has been extremely useful as this is a real life scenario that can come up and so just as a recap you've created a custom network with two custom subnets you've created a cloud storage bucket and uploaded some files to it you've created a public instance and a private instance and then created some firewall rules to route the traffic you then tested it all by using the command line for communication you also enable private google access for the instance with only the internal ip to access google's public apis so that it can access cloud storage and so again fantastic job on your part as this was a pretty complex demo and you can expect things like what you've experienced in this demo to pop up in your role of being a cloud engineer at any time so before you go be sure to delete all the resources you've created and again congrats on the great job so you can now mark this as complete and i'll see you in the next one welcome back in this lesson i will be going over vpc network peering and how you can privately communicate across vpcs in the same or different organization vpc network peering and vpc peering are used interchangeably in this lesson as they are used to communicate the same thing now for instances in one vpc to communicate with an instance in another vpc they would route traffic via the public internet however to communicate privately between two vpcs google cloud offers a service called vpc peering and i will be going through the theory and concepts of vpc peering throughout this lesson so with that being said let's dive in now vpc peering enables you to peer vpc networks so that workloads in different vpc networks can communicate in a private space that follows the rfc 1918 standard thus allowing private connectivity across two vpc networks traffic stays within google's network and never traverses the public internet vpc peering gives you the flexibility of peering networks that are of the same or different projects along with being able to peer with other networks in different organizations vpc peering also gives you several advantages over using external ip addresses or vpns to connect the first one is reducing network latency as all peering traffic stays within google's highspeed network vpc peering also offers greater network security as you don't need to have services exposed to the public internet and deal with greater risks of having your traffic getting compromised or if you're trying to achieve compliance standards for your organization vpc peering will allow you to achieve the standards that you need and finally vpc network peering reduces network costs as you save on egress costs for traffic leaving gcp so in a regular network google charges you for traffic communicating using public ips even if the traffic is within the same zone now you can bypass this and save money by using internal ips to communicate and keeping the traffic within the gcp network now there are certain properties or characteristics that peered vpcs follow and i wanted to point these out for better understanding first off peer vpc networks remain administratively separate so what exactly does this mean well it means that routes firewalls vpns and other traffic management tools are administered and applied separately in each of the vpc networks so this applies to each vpc independently which also means that each side of a peering association is set up independently as well so when you connect one vpc to the other you have to go into each vpc that you are connecting to both initiate and establish the connection peering becomes active only when the configuration from both sides match this also means that each vpc can delete the peering association at any given time now during vpc peering the vpc peers always exchange all subnet routes you also have the option of exchanging custom routes subnet and static routes are global and dynamic routes can be regional or global a given vpc network can peer with multiple vpc networks but there is a limit that you can reach in which you would have to reach out to google and ask the limit to be increased now when peering with vpc networks there are certain restrictions in place that you should be aware of first off a subnet cider range in one peered vpc network cannot overlap with a static route in another peered network this rule covers both subnet routes and static routes so when a vpc subnet is created or a subnet ip range is expanded google cloud performs a check to make sure that the new subnet range does not overlap with ip ranges of subnets in the same vpc network or in directly peered vpc networks if it does the creation or expansion will fail google cloud also ensures that no overlapping subnet ip ranges are allowed across vpc networks that have appeared network in common and again if it does the creation or expansion will fail now speaking of routing when you create a new subnet in appeared vpc network vpc network peering doesn't provide granular route controls to filter out which subnet cider ranges are reachable across pure networks these are handled by firewall rules so to allow ingress traffic from vm instances in a peer network you must create ingress allow firewall rules by default ingress traffic to vms is blocked by the implied deny ingress rule another key point to note is that transitive peering is not supported and only directly peered networks can communicate so they have to be peered directly in this diagram network a is peered with network b and network b is peered with network c and so if one instance is trying to communicate from network a to network c this cannot be done unless network a is directly peered with network c an extremely important point to note for vpc peering another thing to note is that you cannot use a tag or service account from one peered network in the other peered network they must each have their own as again they are each independently operated as stated earlier and so the last thing that i wanted to cover is that internal dns is not accessible for compute engine in peered networks as they must use an ip to communicate and so that about covers this short yet important lesson on the theory and concepts of vpc peering and so now that we've covered all the theory i'm going to be taking these concepts into a demo where we will be pairing two networks together and verifying the communication between them and so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this handson demonstration we're going to go through the steps to create a peering connection from two vpcs in two separate projects as shown here in the diagram and then to verify that the connection works we're going to create two instances one in each network and ping one instance from the other instance this demo is very similar to the custom vpc demo that you had done earlier but we are adding in another layer of complexity by adding in vpc network peering and so there's quite a bit to do here so let's go ahead and just dive in okay so here we are back in the console as you can see up in the top right hand corner i am logged in as tony bowties gmail.com and for this specific demo i will be using two projects both project tony and project bowtie inc and if you currently do not have two projects you can go ahead and create yourself a new project or the two projects if you have none and so i'm going to continue here with project tony and the first thing i want to do is create the two networks in the two separate projects so i'm going to go up to the navigation menu in the top left hand corner and i'm going to scroll down to vpc network here i'm going to create my first vpc network and i'm going to name this bowtie ink dash a i'm going to give it the same description and then under subnets i'm going to leave the subnet creation mode under custom under the subnet name you can call this subnet dash a i'm going to use the us east one region and for the ip address range i'm going to use 10.0 that's 0.0 forward slash 20. and i'm going to leave all the other options as default and i'm going to go down to the bottom and click on create now as this network is being created i'm going to go over to the project bowtie inc and i'm going to create the vpc network there so under name i'm going to call this bowtie inc b and under description i'm going to use the same under subnets i'm going to keep subnet creation mode as custom and under new subnet i'm going to call this subnet subnet b the region will be used 4 and the ip address range will be 10.4.0.0 forward slash 20. you can leave all the other options as default and scroll down to the bottom and click on create as this network is being created i'm going to go back to project tony and i'm going to create the firewall rule for bow tie ink dash a in this firewall rule as explained in the last lesson we'll allow communication from one instance to the other and so i'm going to click on create firewall and under name i'm going to call this project tony dash a under description i'm going to use the same under the network i'm going to choose the source network which will be bowtie inc dash a priority i'm going to keep at 1000 direction of traffic should be ingress and action on match should be allow under targets i'm going to select all instances in the network and under source filter i'm going to keep ip ranges selected and the source ip range specifically for this demo is going to be 0.0.0.0 forward slash 0. and again this is specifically used for this demo and should never be used in a productionlike environment in production you should only use the source ip ranges that you are communicating with and under protocols and ports because i need to log into the instance to be able to ping the other instance i'm going to have to open up tcp on port 22. under other protocols you can add icmp and this will allow the ping command to be used i'm going to leave all the other options as default and i'm going to click on create and now that this firewall rule has been created i need to go back over to project bowtie inc and create the firewall rule there as well i'm going to call this firewall rule bowtie inc dash b i'm going to give it the same description under network i'm going to select bow tie ink dash b i'm going to keep the priority as 1000 and the direction of traffic should be ingress as well the action on match should be allow scrolling down under targets i'm going to select all instances in the network and again under source filter i'm going to keep ip ranges selected and under source ip ranges i'm going to enter in 0.0.0.0 forward slash 0. and under protocols and ports i'm going to select tcp with port 22 as well under other protocols i'm going to type in icmp i'm going to leave everything else as default and i'm going to click on create now once you've created both networks and have created both firewall rules you can now start creating the instances so because i'm already in project bowtie inc i'm going to go to the lefthand navigation menu and i'm going to scroll down to compute engine and create my instance so i'm just going to click on create and to keep with the naming convention i'm going to call this instance instance b i'm not going to add any labels for now under region i'm going to choose us east 4 and you can leave the zone as the default selection and under machine type i'm going to select e2 micro and i'm going to scroll down to the bottom and i'm going to click on management security disks networking and sold tenancy so that i'm able to go into the networking tab to change the network on the default network interface so i'm going to click on the default network interface and under network i'm going to select bowtie inc b and the subnet has already been selected for me and then i'm going to scroll down click on done and i'm going to leave all the other options as default and click on create and so as this is creating i'm going to go over to project tony and i'm going to create my instance there and i'm going to name this instance instance a under region i am going to select us east1 you can leave the zone as the default selected under machine type i'm going to select e2 micro and scrolling down here to the bottom i'm going to go into the networking tab under management security disks networking and soul and here i'm going to edit the network interface and change it from the default network to bow tie ink dash a and as you can see the subnet has been automatically selected for me so now i can just simply click on done i'm going to leave all the other options as default and i'm going to click on create so just as a recap we've created two separate networks in two separate projects along with its corresponding subnets and the firewall rules along with creating an instance in each network and so now that we have both environments set up it's now time to create the vbc peering connection and so because i'm in project tony i'm going to start off with this project and i'm going to go up to the navigation menu and scroll down to vpc network and under vpc network on the left hand menu you're going to click on vpc network peering and through the interface shown here we'll be able to create our vpc network peering so now you're going to click on create connection and i'm prompted with some information that i will need and because we are connecting to another vpc in another project you're going to need the project id as well as the name of the vpc network you want to peer with and just as explained in the earlier lesson the subnet ip ranges in both networks cannot overlap so please make sure that if you are using ip ranges outside of the ones that are given for this demonstration the ip ranges that you are using do not overlap so once you have that information you can then click continue and so here you will be prompted with some fields to fill out with the information that you were asked to collect in the previous screen and so since we have that information already we can go ahead and start filling in the fields so i'm going to call this peering connection peering a b and under vpc network i'm going to select bow tie ink dash a under peered vpc network we're going to select the other project which should be bowtie inc and the vpc network name will be bow tie inc dash b and i'm going to leave all the other options as default and so under vpc network name you will see exchange custom routes and here i can select to import and export custom routes that i have previously created so any special routes that i have created before the actual peering connection i can bring them over to the other network to satisfy my requirements and so i'm not going to do that right now i'm going to close this up and i'm going to simply click on create and so this is finished creating and is marked as inactive and this is because the corresponding peering connection in project bowtie has yet to be configured the status will change to a green check mark in both networks and marked as active once they are connected if this status remains as inactive then you should recheck your configuration and edit it accordingly so now i'm going to head on over to project bowtie inc and i'm going to create the corresponding peering connection i'm going to click on create connection once you have your project id and the vpc network you can click on continue and for the name of this peering connection i'm going to call this peering dash ba respectively under vpc network i'm going to select bowtie inc b and under peered vpc network i'm going to select in another project here you want to type in your project id for me i'm going to paste in my project tony project id and under vpc network name i'm going to type in bowtie inc a and i'm going to leave all the other options as default and i'm going to click on create and so now that we've established connections on each of the peering connections in each vpc if the information that we've entered is correct then we should receive a green check mark stating that the peering connection is connected and success here we have status as active and if i head on over to project tony i should have the same green check mark under status for the peering connection and as expected the status has a green check mark and is marked as active so now in order to do the pairing connectivity test i'm going to need to grab the internal ip of the instance in the other network that resides in project bowtie and so because it doesn't matter which instance i log into as both of them have ssh and ping access i'm going to simply go over to the navigation menu i'm going to head on over to compute engine and i'm going to record the internal ip of instance a and now i'm going to head over to project bowtie and log into instance b and ping instance a and so in order to ssh into this instance i'm going to click on the ssh button under connect and it should open a new browser tab for me logging me into the instance okay i'm logged in here and i'm going to zoom in for better viewing and so now i'm going to run a ping command against instance a using the internal ip that i had copied earlier and i'm going to hit enter and as you can see ping is working and so now we can confirm that the vpc peering connection is established and the two instances in the different vpc networks are communicating over their private ips and you can go ahead and hit control c to stop the ping and so just as a recap you've created two separate vpc networks with their own separate subnets in two separate projects you've created the necessary firewall rules in each of these networks along with creating instances in each of those networks you then established a vpc peering connection establishing the configuration in each vpc you then did a connectivity test by logging into one of the instances and pinging the other instance and so i hope this helps cement the theory of vpc peering that you learned in the previous lesson and has given you some context when it comes to configuring each end of the peering connection so i wanted to take a moment to congratulate you on completing this demo and so all that's left now is to clean up all the resources that we created throughout this demo and you can start by selecting the instances and deleting them in each network as well as the firewall rules and the networks themselves i'm going to go over to project tony and i'm going to do the same thing there and so you can do exactly what you did with the last instance here you can select it click on delete and delete the instance and so next we're going to delete the peering connection so we're going to go up to the navigation menu we're going to scroll down to vpc network and on the left hand menu we're going to scroll down to vpc network peering and so we're going to select appearing connection we're going to go to the top and click on delete and then delete the peering connection and so now we're going to delete the firewall rule so we're going to go up to firewall we're going to select the firewall rule at the top we're going to click delete and then delete the firewall rule and last but not least we want to delete the vpc network that we created so we're going to go up to vpc networks we're going to drill down into the custom vpc up at the top we're going to click on delete vpc network and then we're going to click on delete and so now that we've deleted all the resources in project tony we're going to go back over to our second project project bowtie and do the same thing and so we're first going to start off with the vpc peering connection so we're going to go over to vpc network peering we're going to select the appearing connection we're gonna click on delete at the top and delete the peering connection next we're gonna go into firewall we're gonna select the firewall rule go up to the top and click on delete and then delete the firewall rule and finally we're gonna go over to vpc networks we're going to drill down into the custom network we're going to click on delete vpc network at the top and delete the vpc network and so now that you've successfully deleted all your resources you can now mark this lesson as complete and i'll see you in the next one and congrats again on the great job of completing this demo welcome back and in this lesson i'm going to be discussing the concepts and terminology of shared vpcs i'm also going to go into some detailed use cases and how shared vpcs would be used in different scenarios so with that being said let's dive in now when a vpc is created it is usually tied to a specific project now what happens when you want to share resources across different projects but still have separate billing and access within the projects themselves this is where shared vpcs come into play shared vpcs allow an organization to connect resources from multiple projects to a common vpc network so that way they can communicate with each other securely and efficiently using internal ips from that network when you use shared vpcs you designate a project as a host project and attach one or more other service projects to it the vpc networks in the host project are considered the shared vpc networks so just as a reminder a project that participates in a shared vpc is either a host project or a service project a host project can contain one or more shared vpc networks a service project is any project that has been attached to a host project by a shared vpc admin this attachment allows it to participate in the shared vpc and just as a note a project cannot be both a host and a service project simultaneously it has to be one or the other and you can create and use multiple host projects however each service project can only be attached to a single host project it is also a common practice to have multiple service projects administered by different departments or teams in the organization and so just for clarity for those who are wondering a project that does not participate in a shared vpc is called a standalone project and this is to emphasize that it is neither a host project or a service project now when it comes to administering these shared vpcs we should be adhering to the principle of least privilege and only assigning the necessary access needed to specific users so here i broken down the roles that are needed to enable and administer the shared vpcs a shared vpc admin has the permissions to enable host projects attach service projects to host projects and delegate access to some or all of the subnets in shared vpc networks to service project admins when it comes to a service project admin this is a shared vpc admin for a given host project and is typically its project owner as well although when defining each service project admin a shared vpc admin can grant permission to use the whole host project or just some subnets and so when it comes to service project admins there are two separate levels of permissions that can be applied the first is project level permissions and this is a service project admin that can be defined to have permission to use all subnets in the host project when it comes to subnet level permissions a service project admin can be granted a more restrictive set of permissions to use only some subnets now i wanted to move into some use cases which will give you a bit more context on how shared vpcs are used in specific environments illustrated here is a simple shared vpc scenario here a host project has been created and attached to service projects to it the service project admin in service project a can be configured to access all or some of the subnets in the shared vpc network service project admin with at least subnet level permissions to the 10.0.2.0 24 subnet has created vm1 in a zone located in the us west one region this instance receives its internal ip address 10.0.2.15 from the 10.0.2.0 24 cider block now service project admins in service project b can be configured to access all or some of the subnets in the shared vpc network a service project admin with at least subnet level permissions to the 10.10.4.0 forward slash 24 subnet has created vm2 in a zone located in the us central 1 region this instance receives its internal ip address 10.10.4.1 from the 10.10.4.0 forward slash 24 cider block and of course the standalone project does not participate in the shared vpc at all as it is neither a host nor a service project and the last thing to note instances in service projects attached to a host project using the same shared vpc network can communicate with one another using either ephemeral or reserve static internal ip addresses and i will be covering both ephemeral and static ip addresses in a later section under compute engine external ip addresses defined in the host project are only usable by resources in that project they are not available for use in service projects moving on to the next use case is a multiple hosts project for this use case an organization is using two separate host projects development and production and each host project has two service projects attached to them both host projects have one shared vpc network with subnets configured to use the same cider ranges both the testing and production networks have been purposely configured in the same way so this way when you work with resources tied to a subnet range it will automatically translate over from one environment to the other moving on to the next use case is the hybrid environment now in this use case the organization has a single host project with a single shared vpc network the shared vpc network is connected via cloud vpn to an onpremises network some services and applications are hosted in gcp while others are kept on premises and this way separate teams can manage each of their own service projects and each project has no permissions to the other service projects as well each service project can also be billed separately subnet level or project level permissions have been granted to the necessary service project admins so they can create instances that use the shared vpc network and again instances in these service projects can be configured to communicate with internal services such as database or directory servers located on premises and finally the last use case is a twotier web service here an organization has a web service that is separated into two tiers and different teams manage each tier the tier one service project represents the externally facing component behind an http or https load balancer the tier 2 service project represents an internal service upon which tier 1 relies on and it is balanced using an internal tcp or udp load balancer the shared vpc allows mapping of each tier of the web service to different projects so that they can be managed by different teams while sharing a common vpc network to host resources that are needed for both tiers now we cover quite a bit in this lesson when it comes to all the concepts of shared vpcs we covered both host and service projects and the roles that they play and their limitations we also went over the different roles that are needed to administrate these shared vpcs and we went over different use cases on how to use shared vpcs for different scenarios and so that about covers everything i wanted to discuss in this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing vpc flow logs flow logs is an essential tool for monitoring and analyzing traffic coming in and going out of vpcs from vm instances flow logs are essential to know for the exam as you should know the capabilities and use cases and so with that being said let's dive in so vpc flow logs records a sample of network flows sent from and received by vm instances including instances used as google kubernetes engine nodes these logs can be used for network monitoring forensics realtime security analysis and expense optimization when you enable vpc flow logs you enable for all vms in a subnet so basically you would be enabling vpc flow logs on a subnet by subnet basis flow logs are aggregated by connection from compute engine vms and exported in real time these logs can be exported to cloud logging previously known as stackdriver for 30 days if logs need to be stored for longer than 30 days they can be exported to a cloud storage bucket for longer term storage and then read and queried by cloud logging google cloud samples packets that leave and enter a vm to generate flow logs now not every packet is captured into its own log record about one out of every 10 packets is captured but this sampling rate might be lower depending on the vm's load and just as a note you cannot adjust this rate this rate is locked by google cloud and cannot be changed in any way and because vpc flow logs do not capture every packet it compensates for missed packets by interpolating from the captured packets now there are many different use cases for vpc flow logs and i wanted to take a quick minute to go over them the first one i wanted to mention is network monitoring vpc flow logs provide you with realtime visibility into network throughput and performance so you can monitor the vpc network perform network diagnostics understand traffic changes and help forecast capacity for capacity planning you can also analyze network usage with vpc flow logs and you can analyze the network flows for traffic between regions and zones traffic to specific countries on the internet and based on the analysis you can optimize your network traffic expenses now a great use case for vpc flow logs is network forensics so for example if an incident occurs you can examine which ips talked with whom and when and you can also look at any compromised ips by analyzing all the incoming and outgoing network flows and lastly vpc flow logs can be used for realtime security analysis you can leverage the realtime streaming apis using pub sub and integrate them with a sim or security information in event management system like splunk rapid7 or logarithm and this is a very common way to add an extra layer of security to your currently existing environment as well as a great way to meet any compliance standards that are needed for your organization now vpc flow logs are recorded in a specific format log records contain base fields which are the core fields of every log record and meta data fields that add additional information metadata fields may be omitted to save storage costs but base fields are always included and cannot be omitted some log fields are in a multifield format with more than one piece of data in a given field for example the connection field that you see from the base is of the ip details format which contains the source and destination ip address and port plus the protocol in a single field flows that have an endpoint in a gke cluster can be annotated with gke annotations which can include details of the cluster pod and service of the endpoint gke annotations are only available with a custom configuration of metadata fields now when you enable vpc flow logs you can set a filter based on both base and metadata fields that only preserves logs that match the filter all other logs are discarded before being written to logging which saves you money and reduces the time needed to find the information you're looking for shown here is a sample from the console in both the classic logs viewer as well as the logs viewer in preview and so in the classic logs viewer you can simply select the sub network from the first pull down menu and from the second pull down menu you can select the compute.googleapis.com forward slash vpc underscore flows and this will give you the information that you need to pull up all your vpc flow logs in the logs viewer preview it is done in a similar way but the query is shown here in the query builder and can be adjusted accordingly pulling up any vpc flow logs must be done within the console when viewing them in google cloud and so the last thing i wanted to show you before ending this lesson is a sample of the log itself the log shown here is a sample of what a vpc flow log looks like and as you can see here beside each field you will see a small arrow clicking on these arrows will expand the field and reveal many of the subfields that you saw on the last slide and will give you the necessary information you need to analyze your vpc flow logs in this example of the connection field it shows the five tuple that describes this connection which you can clearly see up here at the top and if i were to go further down and expand more of these fields i would find more information that could help me better analyze more logging info for my given problem that i am trying to solve now i didn't want to go too deep into logging as i will be diving into a complete section on its own in a later section of the course but i did want you to get a feel for what type of data vpc flow logs can give you and how it can help you in your specific use case as well as on the exam and so that's pretty much all i wanted to cover with regards to vpc flow logs so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to cover a highlevel overview of a basic foundational service that supports the backbone of the internet as we know it today this foundation is called dns or the domain name system dns is used widely in google cloud from mostly an infrastructure perspective and is used in pretty much any other cloud environment or computer network on the planet now there is quite a bit to cover in this lesson with regards to dns so with that being said let's dive in now dns or domain name system is a global decentralized distributed database that lets you store ip addresses and other data and look them up by name this system uses human readable names like google.com and translates it into a language that computers understand which are numeric ip addresses for example humans access information online through a domain name like google.com computers use ip addresses to access information online like 172.217. now whether you type google.com or the ip address into a web browser both will connect to google.com dns translates the domain name to an ip address so that the web browser knows where to connect to and we know what to enter into the web browser through dns you can connect a domain name to web hosting mail and other services now getting a bit deeper into it as ip addresses are at the core of communicating between devices on the internet they are hard to memorize and can change often even for the same service to get around these problems we gave names to ip addresses for example when it comes to our computer communicating with www.google.com it will use the dns system to do this now in the dns database contains the information needed to convert the www.google.com domain name to the ip address and this piece of information is stored in a logical container called a zone the way that the zone is stored is through what's commonly known as a zone file now within this zone file is a dns record which links the name www and the ip address that your laptop needs to communicate with the specific website and this zone file is hosted by what's known as a name server or ns server for short and i will be going into further detail on this in just a minute so in short if you can query the zone for the record www.google.com then your computer can communicate with the web server and dns is what makes it all happen now i wanted to go into a bit of history of how dns came about so in early computer networks a simple text file called a host file was created that mapped hostnames to ip addresses and this enabled people to refer to other computers by the name and their computers translated that name to an ip address when it needed to communicate with it the problem is as network sizes increased the host file approach became impractical due to the fact that it needed to be stored on each computer as each computer would have to resolve the same host names as well updates were difficult to manage as all of the computers would need to be given an updated file all in all this system was not scalable now to overcome these and other limitations the dns system was developed and the dns system essentially provided for a way to organize the names using a domain name structure it also provided a dynamic system for protocols services and methods for storing updating and retrieving ip addresses for host computers now that i've covered what dns is and why we use it i wanted to dive into the structure of the dns system now the structure all begins with a dot the root if you will and this can be found after every domain name that you type into your browser you will almost never see it and this is because your browser will automatically put it in without your knowing you can try it with any domain in any browser and you will almost always come up with the same result this dot is put in for you and will provide the route for you and this is where we start to break down the dns system now the domain name space consists of a hierarchical data structure like the one you have on your computer each node has a label and zero or more resource records which hold information associated with the domain name the domain name itself consists of the label concatenated with the name of its parent node on the right separated by a dot so when it comes to dns the domain name is always assembled from right to left this hierarchy or tree is subdivided into zones beginning at the root zone a dns zone may consist of only one domain or may consist of many domains and sub domains depending on the administrative choices of the zone manager now getting right into it the root server is the first step in translating human readable hostnames into ip addresses the root domain is comprised of 13 dns systems dispersed around the world known collectively as the dns root servers they are indicated by the letters a through m operated by 12 organizations such as verisign cogent and nasa while there are 13 ip addresses that represent these systems there are actually more than 13 servers some of the ip addresses are actually a cluster of dns servers and so each of these dns servers also consists of the root zone file which contains the address of the authoritative name server for each top level domain and because this is such a big undertaking to keep updated iana or the internet assigned numbers authority was appointed as the authority that manages and administrates this file and i will include a link in the lesson text for those of you who are looking to dive deeper into the contents of this root zone file as well as getting to know a little bit more about the iana organization now while the dns root servers establish the hierarchy most of the name resolution process is delegated to other dns servers so just below the dns route in the hierarchy are the top level domain servers also known as tld for short the top level domain takes the tld provided in the user's query for example www.google and provides details for the dotcom tld name server the companies that administer these domains are named registries and they operate the authoritative name servers for these top level domains for example verisign is the registry for the dot com top level domain over a hundred million domains have been registered in the dot com top level domain and these top level dns servers handle top level domains such as com dot org dot net and dot io and this can also be referred to as the gtld which is the general top level domains and the cctld which is the country code top level domain like dot ca for canada dot uk for the united kingdom and dot it for italy the top level dns servers delegate to thousands of second level dns servers now second level domain names are sold to companies and other organizations and over 900 accredited registrars register and manage the second level domains in the dot com domain for end users the second level of this structure is comprised of millions of domain names second level dns servers can further delegate the zone but most commonly store the individual host records for a domain name this is the server at the bottom of the dns lookup chain where you would typically find resource records and it is these resource records that maps services and host names to ip addresses and will respond with the queried resource record ultimately allowing the web browser making the request to reach the ip address needed to access a website or other web resources now there is one more concept that i wanted to cover before we move on and this is the sub domain now some of you have noticed and wondered where does the sub domain come into play with regards to the dns structure well this is a resource record that falls under the second level domain and in dns hierarchy a sub domain is a domain that is a part of another main domain but i wanted to put it in here just to give you an understanding of where subdomains would fall so now that we understand how dns is structured i wanted to go through the breakdown of the data flow of dns to give you some better contacts now there are eight steps in a dns lookup first we start off with the dns client which is shown here as tony bowtie's laptop and this is a client device which could also be a phone or a tablet and is configured with software to send name resolution queries to a dns server so when a client needs to resolve a remote host name into its ip address in most cases it sends a request to the dns recursive resolver which returns the ip address of the remote host to the client a recursive resolver is a dns server that is configured to query other dns servers until it finds the answer to the question it will either return the answer or an error message to the client if it cannot answer the query and the query will eventually be passed off to the dns client the recursive resolver in essence acts as the middle man between a client and a dns name server which is usually the internet service provider a service carrier or a corporate network now to make sure that a resolver is able to properly run dns a root hints file is supplied with almost every operating system and this file holds the ip addresses for the root name servers this also includes the dns resolver but in case it is unable to answer the query the client will be able to still make the query to the dns name servers now after receiving a dns query from a client this recursive resolver will either respond with cache data or send a request to a root name server and in this case the resolver queries a dns root name server the root server then responds to the resolver with the address of a top level domain or tld dns server such as com or dot net which stores the information for its domains now when searching for google.com the request is pointed towards the dotcom tld so naturally the resolver then makes a request to the com tld then the tld name server then responds with the ip address of the domain's name server google.com and lastly the resolver then sends a query to the domain's name server the ip address for google.com is then returned to the resolver from the name server this ip address is cache for a period of time determined by the google.com name server and this process is so that a future request for this hostname could be resolved from its cache rather than performing the entire process from beginning to end and so for those of you who are unaware cache is a component that stores data so that future requests for that data can be served faster the purpose of this caching is to temporarily store data in a location that results in improvements in performance and reliability for data requests dns caching involves storing the data closer to the requesting client so that the dns query can be resolved earlier and additional queries further down the dns lookup chain can be avoided and thus improving load times dns data can be cached in a variety of locations down the chain each of which will store dns records for a set amount of time determined by a time to live also known as ttl for short and this value is the time to live for that domain record a high ttl for a domain record means that local dns resolvers will cache responses for longer and give quicker responses however making changes to dns records can take longer due to the need to wait for all cash records to expire alternatively domain records with low ttls can change much more quickly but dns resolvers will need to refresh their records more often and so in this final step the dns resolver then responds to the web browser with the ip address of the domain requested initially and once these eight steps of the dns lookup have returned the ip address for www.google.com the browser is able to make the request for the webpage and so the browser will reach out to the ip address of the server and request the web page which will be loaded up in the browser now i know this probably has been a review for those who are a bit more advanced when it comes to understanding dns but for others who are fairly new to the underpinnings of dns i hope this has given you a basic understanding of what it is why we use it and how it works moving forward in the course i will be discussing dns with regards to different services and the needed resource records within zones that are used by these given services and so that's pretty much all i wanted to cover when it comes to the fundamentals of dns so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be diving into dns record types now dns resource records are the basic information elements of the domain name system they are entries in the dns database which provide information about hosts these records are physically stored in the zone files on the dns server this lesson will go through some of the most commonly used dns records that we will be coming across throughout this course so with that being said let's dive in now the first record that i wanted to touch on are the name server records also known as ns records for short this record identifies which dns server contains the current records for a domain these servers are usually found at a registrar internet service provider or hosting company ns records are created to identify the name server used for each domain name within a given zone in this example we have the dot co zone that will have multiple name server records for bowtieinc.co now these name server records are how the dot co delegation happens for bowtieinc.co and they point at servers that host the inc.co zone that is managed by bowtie inc and the flow shown here of the query starts from the root zone going to the dot co zone where the record lies for the name servers for bowtieinc.com and flows down to the bowtieinc.cozone that contain all the necessary records for bowtieinc.co the next record that i wanted to touch on are the a and aaa records and this is short for address records for ipv4 and ipv6 ip addresses respectively and this record points a domain name to an ip address for example when you type wwe in a web browser the dns system will translate that domain name to the ip address of 52.54.92.195 using the a record information stored in the bowtieinc.co dns zone file the a record links a website's domain name to an ipv4 address that points to the server where the website's files live now when it comes to an aaa record this links a website's domain to an ipv6 address that points to the same server where the website's files live a records are the simplest type of dns records and one of the primary records used in dns servers you can do a lot with a records including using multiple a records for the same domain in order to provide redundancy the same can be said for aaa records additionally multiple domains could point to the same address in which case each would have its own a or aaa record pointing to that same ip address moving on to cname records a c name record short for canonical name record is a type of resource record that maps one domain name to another this can be really convenient when running multiple services like an ftp server and an ecommerce server each running on different ports from a single ip address you can for example point ftp ftp.bowtieinc.co and shop.bowtieinc.co to the dns entry for bowtieinc.co which in turn has an a record which points to the ip address so if the ip address ever changes you only have to change the record in one place in the dns a record for bow tie inc dot co and just as a note cname records must always point to another domain name and never directly to an ip address next up are txt records a text record or txt for short is a type of resource record that provides text information to sources outside your domain that can be used for a number of arbitrary purposes the records value can be either human or machine readable text in many cases text records are used to verify domain ownership or even to provide human readable information about a server a network or a data center it is also often used in a more structured fashion to record small amounts of machine readable data into the dns system a domain may have multiple tax records associated with it provided the dns server implementation supports this each record can in turn have one or more character strings in this example google wants to verify the bowtieinc.co domain so that g suite can be set up and needs verification through the domain to google through creating a text record and adding it to the zone google will then supply a text verification record to add to the domain host's dns records and start to scan for the text record to verify the domain the supplied text record is then added by the domain administrator and behind the scenes google is doing a verification check at timed intervals when google finally sees the record exists the domain ownership is confirmed and g suite can be enabled for the domain and this is a typical example of how tax records are used now moving on to mx records a dns mx record also known as the mail exchange record is the resource record that directs email to a mail server the mx record indicates how email messages should be routed and to which server mail should go to like cname records an mx record must always point to another domain now mx records consist of two parts the priority and the domain name the priority are the numbers before the domains for these mx records and indicate the preference of the order in which the mail server should be used the lower the preference number the higher the priority so in this example laura is emailing tony bowtie at tony at bowtieinc.co the mx records are part of this process as dns needs to know where to send the mail to and we'll look at the domain attached to the email address which is bowtieinc.co so the dns client will run a regular dns query by first going to the root then to the cotld and finally to bowtieinc.co it will then receive the mx record which in this example is two of them the first one being mail representing mail.bowtieinc.co and then the second one is a different mail server outside the current domain and in this case is a google mail server of aspmx.l.google.com and this is a fully qualified domain name as the dot on the right of this record suggests so here the server will always try mail.bowtieinc.co first because 5 is lower than 10. and this will give mail.bowtieinc.co the higher priority in the result of a message send failure the server will default to aspmx.l.google.com if both values are the same then it would be low balanced across both servers whichever is used the server gets the result of the query back and it uses this to connect to the mail server for bowtieinc.co via the smtp protocol and it uses this protocol to deliver all email and this is how mx records are used for email the next record i wanted to cover are the pointer records also known as ptr records for short and this provides the domain name associated with an ip address so a dns pointer record is exactly the opposite of the a record which provides the ip address associated with the domain name dns pointer records are used in reverse dns lookups as we discussed earlier when a user attempts to reach a domain name in their browser a dns lookup occurs matching the domain name to the ip address a reverse dns lookup is the opposite of this process and it is a query that starts with the ip address and looks up the domain name while dnsa records are stored under the given domain name dns pointer records are stored under the ip address reverse and ending in dot i n a d d r dot arpa so in this example the pointer record for the iap address 52.54.90 would be stored under 195.92.54.52 dot in addr dot arpa ipv6 addresses are constructed differently from ipv4 addresses and ipv6 pointer records exist in a different namespace within.arpa ipv6 pointer records are stored under the ipv6 address reversed and converted into 4bit sections as opposed to 8bit sections as in ipv4 and as well the domain.ip6.arpa is added at the end pointer records are used most commonly in reverse dns lookups for antispam troubleshooting email delivery issues and logging and so the last record that i wanted to cover are the soa records also known as the start of authority records and this resource record is created for you when you create your managed zone and specifies the authoritative information including global parameters about a dns zone the soa record stores important information about a domain or zone such as the email address of the administrator when the domain was last updated and how long the server should wait between refreshes every dns zone registered must have an soa record as per the rfc 1035 and there is exactly one soa record per zone the soa record contains the core information about your zone so it is not possible for your zone to work without that information and i will include a link in the lesson text for those who are interested in diving deeper and understanding all the information that is covered under these soa records a properly optimized and updated soa record can reduce bandwidth between name servers increase the speed of website access and ensure the site is alive even when the primary dns server is down and so that about covers everything that i wanted to discuss when it comes to resource records within dns so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be covering network address translation also known as nat for short this is a common process used in home business and any cloud networks that you will encounter knowing and understanding that will help you achieve why you would use it and what makes it such a necessary process now there's quite a bit to cover here so with that being said let's dive in now at a high level nat is a way to map multiple local private ip addresses to a public ip address before transferring the information this is done by altering the network address data in the ip header of the data packet while traveling through a network towards the destination as packets pass through a nat device either the source or destination ip address is changed then packets returning in the other direction are translated back to the original addresses and this is a process that is typically used in most home routers that are provided by your internet service provider now originally nat was designed to deal with the scarcity of free ipv4 addresses increasing the number of computers that can operate off a single publicly routable ip address and so because devices in the private ip space such as 192.168.0.0 cannot traverse the public internet that is needed for those devices to communicate with the public internet now ipv6 was designed to overcome the ipv4 shortage and has tons of available addresses and therefore there is no real need for nat when it comes to ipv6 now nat has an additional benefit of adding a layer of security and privacy by hiding the ip address of your devices from the outside world and only allowing packets to be sent and received from the originating private device and so this is a high level of what nat is now there are multiple types of not that i will be covering which at a high level do the same thing which is translate private i p addresses to public ip addresses yet different types of nat handles the process differently so first we have static nat which maps a single private ip address to a public ip address so a onetoone mapping that gives the device with the private ip address access to the public internet in both directions this is commonly used where one specific device with a private address needs access to the public internet the next type of nat is dynamic nan and this is similar to static nat but doesn't hold the same static allocation a private ip address space is mapped to a pool of public ip addresses and are allocated randomly as needed when the ip address is no longer needed the ip address is returned back to the pool ready to be used by another device this method is commonly used where multiple internal hosts with private i p addresses are sharing an equal or fewer amount of public i p addresses and is designed to be an efficient use of public ips and finally there is port address translation or pat where multiple private ip addresses are translated using a single public ip address and a specific port and this is probably what your home router is using and will cover all the devices you use in your home network this method uses ports to help distinguish individual devices and is also the method that is used for cloudnat in google cloud which i will be covering in a later lesson and so i wanted to get into a bit more detail on how these methods work starting with static not now to set the stage for static not i'm going to start off with a private network here on the left and the public ip space here on the right and the router or not device in the middle in this example there is a server on the left that needs access to external services and for this example the external service we are using is the bowtress service an image sharing site for all sorts of awesome bow ties so the server on the left is private with a private ip address of 192.168.0.5 and this means it has an address in the ip version 4 private address space meaning that it cannot route packets over the public internet because it only has a private ip the beautress service on the other hand has a public ip address which is 54.5.4.9 so the issue we run into is that the private address can't be routed over the public internet because it's private and the public address of the beau trust service can't directly communicate with any private address because public and private addresses can communicate over the public internet what we need is to translate the private address that the server on the left has to a public ip that can communicate with the service on the right and vice versa now then that device will map the private ip to public ip using and maintaining a nat table and in this case of static nat the nat device will have a onetoone mapping of the private ip address to a public ip address and can be allocated to the device specified which in this case is the server marked as 192.168.0.15 and so in order for the server on the left to communicate with the beautress service the server will generate a packet as normal with the source ip of the packet being the server's private ip address and the destination ip of the packet being the ip of the bowtrust service now the router in the middle is the default gateway for any destination so any ip packets which are destined for anything but the local network are sent to the router so as you can see here with the entry in the table it will contain the private i p address of 192.168.0.15 and mapped to the public address which in this case is 73.6.2.33 and these are statically mapped to one another and so as the packet passes through the nat device the source address of the packet is translated from the private address to the mapped public address and this results in a new packet so this new packet still has beautrest as the destination but now it has a valid public ip address as the source and so this is the translation that happens through nat now this process works in a similar way in the other direction so when the beautress service receives the packet it sees the source as this public ip so when it responds with data its packet has its ip address as the source and the previous server's public ip address as the destination so it sends this packet back to this public ip so when the packet arrives at the nat device the table is checked it recognizes then that the ip is for the server and so this time for incoming traffic the destination ip address is updated to the corresponding private ip address and then the packet is forwarded through to the private server and this is how static nat works the source i p address is translated from the mapped private ip to public ip and for incoming traffic the destination i p address is translated from the allocated public ip to the corresponding private ip all without having to configure a public ip on any private device as they always hold their private ip addresses now i wanted to supply an analogy for nat and so a very common analogy that is used is that of a phone service so in this example laura is the new manager of bow tie inc new location in montreal and has put in a new public phone number of 5145558437 although as you can see here laura also has a private extension of one three three seven now if george called laura at that public phone number he would reach laura without ever knowing her private extension so the private extension acts as that private ip address and the public phone number would act as the public ip address and this would be the telephone analogy for static nat and so this is the end of part one of this lesson it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or a tea and whenever you're ready you can join me in part two where we will be starting immediately from the end of part one so you can go ahead and complete this video and i will see you in part two welcome back this is part two of the network address translation lesson and we will be starting exactly where we left off from part 1. so with that being said let's dive in now moving on to dynamic nat this method is similar to static nat except that devices are not allocated a permanent public ip a public ip address is allocated from a pool of ip addresses as they are needed and the mapping of public to private is allocation base in this example there are two devices on the left and according to the nat table there are two public ip addresses available for use 73.6.2.33 and 73.6.2.34 so when the laptop on the left is looking to access the beautress service it will generate a packet where the source ip is the private address of 192.168.0.13 and the destination ip is 54.5.4.9 so it sends this packet and again the router in the middle is the default gateway for anything that isn't local as the packet passes through the router or the nat device it checks if the private ip has a current allocation of public addressing from the pool and if it doesn't and one is available it allocates one dynamically and in this case 73.6.2.34 is allocated so the packet's source i p address is translated to this address and the packets are sent to the beautress service and so this process is the same as static not thus far but because dynamic nat allocates these ip addresses dynamically multiple private devices can share a single public ip as long as the devices are not using the same public ip at the same time and so once the device is finished communication the ip is returned back to the pool and is ready for use by another device now just as a note if there's no public ip addresses available the router rejects any new connections until you clear the nat mappings but if you have as many public ip addresses as hosts in your network you won't encounter this problem and so in this case since the lower server is looking to access the fashion tube service there is an available public ip address in the pool of 73.6.2.33 thus giving it access to the public internet and access to fashion tube so in summary the nat device maps a private ip with the public ip in a nat table and public ips are allocated randomly and dynamically from a pool now this type of knot is used where multiple internal hosts with private ip addresses are sharing an equal or fewer amount of public ip addresses when all of those private devices at some time will need public access now an example of dynamic nat using the telephone analogy would be if laura and two other bow tie inc employees lisa and jane had private phone numbers and this would represent your private ips in this example bowtie inc has three public phone numbers now when any employee makes an outbound call they are routed to whichever public line is open at the time so the caller id on the receiver's end would show any one of the three public phone numbers depending on which one was given to the caller and this would represent the public ips in the public ip pool now the last type of nat which i wanted to talk about is the one which you're probably most familiar with and this is port address translation which is also known as not overload and this is the type of not you likely use on your home network port address translation is what allows a large number of private devices to share one public ip address giving it a many to one mapping architecture now in this example we'll be using three private devices on the left all wanting to access fashiontube on the right a popular video sharing website of the latest men's fashions shared by millions across the globe and this site has a public ip of 62.88.44.88 and accessed using tcp port 443 now the way that port address translation or pat works is to use both the ip addresses and ports to allow for multiple devices to share the same public ip every tcp connection in addition to a source and destination ip address has a source and destination port the source port is randomly assigned by the client so as long as the source port is always unique then many private clients can use the same public ip address and all this information is recorded in the nat table on the nat device in this example let's assume that the public ip address of this nat device is 73.6.2.33 so when the laptop in the top left generates a packet and the packet is going to fashion tube its destination ip address is 62.80 and its destination port is 443. now the source ip of this packet is the laptop's private ip address of 192.168.6 and the source port is 35535 which is a randomly assigned ephemeral port so the packet is routed through the nat device and in transit the nat device records the source ip and the original source private port and it allocates a new public ip address and a new public source port which in this case is 8844 it records this information inside the not table as shown here and it adjusts the pocket so that its source ip address is the public ip address that the nat device is using and the source port is this newly allocated source port and this newly adjusted packet is forwarded on to fashiontube now the process is very similar with the return traffic where the packet will verify the recorded ips and ports in the nat table before forwarding the packet back to the originating source now if the middle laptop with the ip of 192.168.0.14 did the same thing then the same process would be followed all of this information would be recorded in the nat table a new public source port would be allocated and would translate the packet adjusting the packet's source ip address and source port as well the same process would happen for the laptop on the bottom generating a packet with the source and destination ip with the addition of the source and destination ports and when routed through the nat device goes through its translation recording the information in the nat table and reaching its destination again return traffic will be verified by the recorded ips and ports in the nat table before forwarding the packet back to its originating source and so just as a summary when it comes to port address translation the nat device records the source ip and source port in a nat table the source ip is then replaced with a public ip and public source port and are allocated from a pool that allows overloading and this is a manytoone architecture and so for the telephone analogy for pat let's use a phone operator example so in this instance george is trying to call laura now george only knows lark laura's executive admin and only has lark's phone number george does not have laura's private line lark's public phone number is the equivalent to having a public ip address george calls lark who then connects george to laura the caveat here is that lark never gives out laura's phone number in fact laura doesn't have a public phone number and can only be called by lark and here's where nat can add an extra layer of security by only allowing needed ports to be accessed without allowing anyone to connect to any port now i hope this has helped you understand the process of network address translation how the translation happens and the process of using a nat table to achieve packet translation along with its destination this is so common in most environments that you will encounter and it's very important to fully understand the different types of not and how it can be used in these types of environments and so that's pretty much all i wanted to cover on this lesson of network address translation so you can now mark this lesson as complete and let's move on to the next one welcome back so now that we've covered the fundamentals of dns along with the different record types i wanted to focus in on google cloud's dns service called cloud dns now cloud dns is a fully managed service that manages dns servers for your specific zones and since cloud dns shows up on the exam only on a high level i will be giving an overview of what this service can do so with that being said let's dive in now cloud dns acts as an authoritative dns server for public zones that are visible to the internet or for private zones that are visible only within your network and is commonly referred to as google's dns as a service cloud dns has servers that span the globe making it a globally resilient service now while it is a global service there is no way to select specific regions to deploy your zones and dns server policies you simply add your zones records and policies and it is distributed amongst google's dns servers across the globe cloud dns is also one of the few google cloud services that offers 100 availability along with low latency access by leveraging google's massive global network backbone now in order to use cloud dns with a specific publicly available domain a domain name must be purchased through a domain name registrar and you can register a domain name through google domains or another domain registrar of your choice cloud dns does not provide this service and just as a note that to create private zones the purchasing of a domain name is not necessary now as stated earlier cloud dns offers the flexibility of hosting both public zones and privately managed dns zones now public zones are zones that are visible to the public internet and so when cloud dns is managing your public domain it has public authoritative name servers that respond to public zone dns queries for your specific domain now when it comes to private zones these enable you to manage custom domain names for your google cloud resources without exposing any dns data to the public internet a private zone can only be queried by resources in the same project where it is defined and as we discussed earlier a zone is a container of dns records that are queried by dns so from a private zone perspective these can only be queried by one or more vpc networks that you authorize to do so and just as a note the vpc networks that you authorize must be located in the same project as the private zone to query records hosted in manage private zones in other projects the use of dns peering is needed now i don't want to get too deep into dns peering but just know that vpc network peering is not required for the cloud dns peering zone to operate peering zones do not depend on vpc network peering now each managed zone that you create is associated with a google cloud project and once this zone is created it is hosted by google's managed name servers now these zones are always hosted on google's manage name servers within google cloud so you would create records and record sets and these servers would then become allocated to that specific zone hosting your records and record sets and just as a quick reminder a record set is the collection of dns records in a zone that have the same name and are of the same type most records contain a single record but it's not uncommon to see record sets a great example of this are a records or ns records which we discussed earlier and these records can usually be found in pairs and so now to give you a practical example of cloud dns i wanted to bring the theory into practice through a short demo where i'll be creating a managed private zone so whenever you're ready join me in the console and so here we are back in the console and i'm logged in as tonybowties gmail.com and i'm currently in project bowtie inc so now to get to cloud dns i'm going to go over to the navigation menu i'm going to scroll down to network services and go over to cloud dns and because i currently don't have any zones i'm prompted with only one option which is to create a zone and so i'm going to go ahead and create a zone and so here i've been prompted with a bunch of different options in order to create my dns zone and so the first option that i have is zone type and because i'm creating a private zone i'm going to simply click on private and i need to provide a zone name which i'm going to call tony bowtie next i'm going to have to provide a dns name which i will call tony bowtie dot private and under the description i'm just going to type in private zone for tony bowtie and so the next field i've been given is the options field where it is currently marked as default private and so if i go over here to the right hand side and open up the drop down menu i'm given the options to forward queries to another server dns peering manage reverse lookup zones and use a service directory namespace and so depending on your type of scenario one of these five options in most cases will suffice so i'm going to keep it under default private and under networks it says your private zone will be visible to the selected networks and so i'm going to click on the drop down and i'm giving only the option of the default network because it's the only network that i have and so i'm going to select it and i'm going to click on the white space and if i feel so inclined i can simply click on the shortcut for the command line and here i'm given this specific commands if i was to use the command line in order to create this dns zone so i'm going to click on close here and i'm going to click on create and as you can see here my zone has been created along with a couple of dns records the first one being my name server records as well as my start of authority records and so as a note to know for the exam when creating a zone these two records will always be created both the soa record and the ns record and moving on to some other options here i can add another record set if i choose to again the dns name the record type which i have a whole slew of record types to choose from it's ttl and the ip address but i'm not going to add any records so i'm just going to cancel and by clicking in use by i can view which vpc network is using this zone and as expected the default network shows up and i also have the choice of adding another network but since i don't have any other networks i can't add anything so i'm going to simply cancel i also have the option of removing any networks so if i click on this i can remove the network or i can also remove the network by clicking on the hamburger menu and so as you can see i have a slew of options to choose from when creating zones and record sets and so that about covers everything that i wanted to show you here in cloud dns but before i go i'm going to go ahead and clean up and i'm just going to click on the garbage can here on the right hand side of the zone and i'm going to be prompted if i want to delete the zone yes i do so i'm going to click on delete and so that pretty much covers everything that i wanted to show you with regards to cloud dns so you can now mark this lesson as complete and let's move on to the next one welcome back now before we step into the compute engine section of the course i wanted to cover a basic foundation of what makes these vms possible and this is where a basic understanding of virtualization comes into play now this is merely an introductory lesson to virtualization and i won't be getting too deep into the underpinnings it serves as just a basic foundation as to how compute engine gets its features under the hood and how they are possible through the use of virtualization for more indepth understanding on virtualization i will be including some links in the lesson text for those who are looking to learn more but for now this will provide just enough theory to help you understand how compute engine works so with that being said let's dive in so what exactly is virtualization well virtualization is the process of running multiple operating systems on a server simultaneously now before virtualization became popular a standard model was used where an operating system would be installed on a server so the server would consist of typical hardware like cpu memory network cards and other devices such as video cards usb devices and storage and then the operating system would run on top of the hardware now there is a middle layer of the operating system a supervisor if you will that is responsible for interacting with underlying hardware and this is known as the kernel the kernel manages the distribution of the hardware resources of the computer efficiently and fairly among all the various processes running on the computer now the kernel operates under what is called kernel mode or privilege mode as it runs privileged instructions that interacts with the hardware directly now the operating system allows other software to run on top of it like an application but cannot interact directly with the hardware it must interact with the operating system in user mode or nonprivileged mode so when lark decides to do something on an application that needs to use the system hardware that application needs to go through the operating system it needs to make what's known as a system call and this is the model of running one operating system on a single server now when passed servers would traditionally run one application on one server with one operating system in the old system the number of servers would continue to mount since every new application required its own server and its own operating system as a result expensive hardware resources were purchased but not used and each server would use approximately under 20 of its resources on average server resources were then known as underutilized now there came a time when multiple operating systems were installed on one computer isolated from each other with each operating system running their own applications this was a perfect model to consolidate hardware and keep utilization high but there is a major issue that arose each cpu at this given moment in time could only have one thing running as privileged so having multiple operating systems running on their own in an unmodified state and expecting to be running on their own in a privileged state running privileged instructions was causing instability in systems causing not just application crashes but system crashes now a hypervisor is what solved this problem it is a small software layer that enables multiple operating systems to run alongside each other sharing the same physical computing resources these operating systems come as virtual machines or vms and these are files that mimic an entire computing hardware environment in software the hypervisor also known as a virtual machine monitor or vmm manages these vms as they run alongside each other it separates virtual machines from each other logically assigning each its own slice of the underlying computing cpu memory and other devices like graphics network and storage this prevents the vms from interfering with each other so if for example one operating system suffers a crash or a security compromise the others will survive and continue running now the hypervisor was never as efficient as how you see it here it went through some major iterations that gave its structure as we know it today initially virtualization had to be done in software or what we now refer to as the host machine and the operating system with its applications put in logical containers known as virtual machines or guests the operating system would be installed on the host which included additional capabilities called a hypervisor and allowed it to make the necessary privileged calls to the hardware having full access to the host the hypervisor exposed the interface of the hardware device that is available on the host and allowed it to be mapped to the virtual machine and emulated the behavior of this device and this allowed the virtual machine using the operating system drivers that were designed to interact with the emulated device without installing any special drivers or tools as well as keeping the operating system unmodified the problem here is that it was all emulated and so every time the virtual machines made calls back to the host each instruction needed to be translated by the hypervisor using what's called a binary translation now without this translation the emulation wouldn't work and would cause system crashes bringing down all virtual machines in the process now the problem with this process is that it made the system painfully slow and it was this performance penalty that caused this process to not be so widely adopted but then another type of virtualization came on the scene called para virtualization now in this model a modified guest operating system is able to speak directly to the hypervisor and this involves having the operating system kernel to be modified and recompiled before installation onto the virtual machine this would allow the operating system to talk directly with the hypervisor without any performance hits as there is no translation going on like an emulation para virtualization replaces instructions that cannot be virtualized with hyper calls that communicate directly with the hypervisor so a hypercall is based on the same concept as a system call privileged instructions that accept instead of calling the kernel directly it calls the hypervisor and due to the modification in this guest operating system performance is enhanced as the modified guest operating system communicates directly with the hypervisor and emulation overhead is removed the guest operating system becomes almost virtualization aware yet there is still a process whereby software was used to speak to the hardware the virtual machines could still not access the hardware directly although things changed in the world of virtualization when the physical hardware on the host became virtualization aware and this is where hardware assisted virtualization came into play now hardware assisted virtualization is an approach that enables efficient full virtualization using help from hardware capabilities on the host cpu using this model the operating system has direct access to resources without any hypervisor emulation or operating system modification the hardware itself becomes virtualization aware the cpu contains specific instructions and capabilities so that the hypervisor can directly control and configure this support it also provides improved performance because the privileged instructions from the virtual machines are now trapped and emulated in the hardware directly this means that the operating system kernels no longer need to be modified and recompiled like in para virtualization and can run as is at the same time the hypervisor also does not need to be involved in the extremely slow process of binary translation now there is one more iteration that i wanted to discuss when it comes to virtualization and that is kernel level virtualization now instead of using a hypervisor kernel level virtualization runs a separate version of the linux kernel and sees the associated virtual machine as a user space process on the physical host this makes it easy to run multiple virtual machines on a single host a device driver is used for communication between the main linux kernel and the virtual machine every vm is implemented as a regular linux process scheduled by the standard linux scheduler with dedicated virtual hardware like a network card graphics adapter cpu memory and disk hardware support by the cpu is required for virtualization a slightly modified emulation process is used as the display and execution containers for the virtual machines in many ways kernel level virtualization is a specialized form of server virtualization and this is the type of virtualization platform that is used in all of google cloud now with this type of virtualization because of the kernel acting as the hypervisor it enables a specific feature called nested virtualization now with nested virtualization it is made possible to install a hypervisor on top of the already running virtual machine and so this is what google cloud has done now you're probably wondering after going through all the complexities involved with previous virtualization models what makes this scenario worthwhile well using nested virtualization it makes it easier for users to move their onpremises virtualized workloads to the cloud without having to import and convert vm images so in essence it eases the use when migrating to cloud a great use case for many but wouldn't be possible on google cloud without the benefit of running kernel level virtualization now this is an advanced concept that does not show up on the exam but i wanted you to understand virtualization at a high level so that you can understand nested virtualization within google cloud as it is a part of the feature set of compute engine and so that's pretty much all i wanted to cover when it comes to virtualization so you can now mark this lesson as complete and let's move on to the next one welcome back now earlier on in the course i discussed compute engine at a high level to understand what it is and what it does the goal for this section is to dive deeper into compute engine as it comes up heavily on the exam and so i want to make sure i expose all the nuances as well it is the goto service offering from google cloud when looking to solve any general computing needs with this lesson specifically i will be going into what makes up an instance and the different options that are available when creating the instance so with that being said let's dive in now compute engine lets you create and run virtual machines known as instances and host them on google's infrastructure compute engine is google's infrastructure as a service virtual machine offering so it being an is service google takes care of the virtualization platform the physical servers the network and storage along with managing the data center and these instances are available in different sizes depending on how much cpu and memory you might need as well compute engine offers different family types for the type of workload you need it for each instance is charged by the second after the first minute as this is a consumption based model and as well these instances are launched in a vpc network in a specific zone and these instances will actually sit on hosts in these zones and you will be given the option of using a multitenant host where the server that is hosting your machine is shared with others but please note that each instance is completely isolated from the other so no one can see each other's instances now you're also given the option of running your instance on a sole tenant node whereby your instance is on its own dedicated hosts that is reserved just for you and you alone you don't share it with anyone else and this is strictly for you only now although this option may sound really great it does come at a steep cost so only if your use case requires you to use a sole tenant node for security or compliance purposes i recommend that you stick with a multitenant host when launching your instances and this is usually the most common selection for most now compute engine instances can be configured in many different ways and allow you the flexibility to fulfill the requests for your specific scenario and as you can see here there are four different base options when it comes to configuration of the instance that you are preparing to launch and so i wanted to take time to go through them in just a bit of detail for context starting first with the machine type which covers vcpu and memory now there are many different predefined machine types that i will be covering in great depth in a different lesson but for now just know that they are available in different families depending on your needs and can be chosen from the general compute optimize and memory optimize machine types they are available in intel or amd flavors and if the predefined options doesn't fit your need you have the option of creating a custom machine that will suit your specific workload now when creating a vm instance on compute engine each virtual cpu or vcpu is implemented as a single hardware hyper thread on one of the available cpu processors that live on the host now when choosing the amount of vcpus on an instance you must take into consideration the desired network throughput as the amount of vcpus will determine this throughput as the bandwidth is determined per vm instance not per network interface or per ip address and so the network throughput is determined by calculating 2 gigabits per second for every vcpu on your instance so if you're looking for greater network throughput then you may want to select an instance with more vcpus and so once you've determined a machine type for your compute engine instance you will need to provide it an image with an operating system to boot up with now when creating your vm instances you must use an operating system image to create boot disks for your instances now compute engine offers many preconfigured public images that have compatible linux or windows operating systems and these operating system images can be used to create and start instances compute engine uses your selected image to create a persistent boot disk for each instance by default the boot disk for your instance is the same size as the image that you selected and you can use most public images at no additional cost but please be aware that there are some premium images that do add additional cost to your instances now moving on to custom images this is a boot disk image that you own and control access to a private image if you will custom images are available only to your cloud project unless you specifically decide to share them with another project or another organization you can create a custom image from boot disks or other images then use the custom image to create an instance custom images that you import to compute engine add no cost to your instances but do incur an image storage charge while you keep your custom image in your project now the third option that you have is by using a marketplace image now google cloud marketplace lets you quickly deploy functional software packages that run on google cloud you can start up a software package without having to manually configure the software the vm instances the storage or even the network settings this is a allinone instance template that includes the operating system and the software preconfigured and you can deploy a software package whenever you like and is by far the easiest way to launch a software package and i will be giving you a run through on these marketplace images in a later demo now once you've decided on your machine type as well as the type of image that you wanted to use moving into the type of storage that you want would be your next step now when configuring a new instance you will need to create a new boot disk for it and this is where performance versus cost comes into play as you have the option to pay less and have a slower disk speed or lower iops or you can choose to have fast disk speed with higher iops but pay a higher cost and so the slowest and most inexpensive of these options is the standard persistent disk which are backed by standard hard disk drives the balance persistent disks are backed by solid state drives and are faster and can provide higher iops than the standard option and lastly ssd is the fastest option which also brings with it the highest iops available for persistent disks now outside of these three options for persistent disks you also have the option of choosing a local ssd and these are solid state drives that are physically attached to the server that hosts your vm instances and this is why they have the highest throughput and lowest latency than any of the available persistent disks just as a note the data that you store on a local ssd persists only until the instance is stopped or deleted which is why local ssds are suited only for temporary storage such as caches or swap disk and so lastly moving into networking each network interface of a compute engine instance is associated with a subnet of a unique vpc network as you've seen in the last section you can do this with an auto a default or a custom network each network is available in many different regions and zones within that region we've also experienced routing traffic for our instance both in and out of the vpc network by use of firewall rules targeting ip ranges specific network tags or by instances within the network now load balancers are responsible for helping distribute user traffic across multiple instances either within the network or externally using a regional or global load balancer and i will be getting into low balancing in another section of the course but i wanted to stress that load balancers are part of instance networking that help route and manage traffic coming in and going out of the network and so this is a high level overview of the different configuration types that go into putting together an instance and i will be diving deeper into each in this section as well i will be putting a handson approach to this by creating an instance in the next lesson and focusing on the different available features that you can use for your specific use case and so this is all i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back now i know in previous demonstrations we've built quite a few compute engine instances and have configured them accordingly in this demonstration we're going to go through a build of another instance but i wanted to dig deeper into the specific configurations that are available for compute engine so with that being said let's dive in and so i am now logged in under tony bowties gmail.com as well i am logged in under the bowtie inc project so in order to kick off this demo i'm going to head on over to the compute engine console so i'm going to go over to the navigation menu and i'm going to scroll down to compute engine and so here i'm prompted to either create or import a vm instance as well as taking the quick start and so i'm not going to import or take the quick start so i'm going to simply click on create and so i want to take a moment here to focus on the left hand menu where there are a bunch of different options to create any given instance so the first and default option allows me to create the instance from scratch choosing the new vm instance from template option allows me to create a new instance from an instance template and because i don't have any instance templates i am prompted here with the option to create one and so for those of you who are unfamiliar with instance templates templates are used in managed instance groups and define instance properties for when instances are launched within that managed instance group but don't worry i will be covering instance groups and instant templates in a later lesson the next option that's available is new vm instance from machine image and an image is a clone or a copy of an instance and again i will be covering this in a separate lesson and going through all the details of machine images but if i did have any machine images i would be able to create my instance from here but since i do not i am prompted with the option to create a new machine image now the last option that i wanted to show you is the marketplace and so the marketplace has existing machine images that are all preconfigured with its proper operating system as well as the software to accompany it so for instance if i'm looking to create a vm with a wordpress installation on it i can simply go up to the top to the search bar type in wordpress and i will be presented with many different options and i'm just going to choose the one here at the top and i am presented with 49 results of virtual machines with different types of wordpress installations on them and these are all different instances that have been configured specifically for wordpress by different companies like lightspeed analog innovation and cognosis inc and so for this demonstration i'm going to choose wordpress on centos 7 and here i'm giving an overview about the software itself i'm also given information about the company that configured this as well at the top i'm given a monthly estimated cost for this specific instance and if i scroll down the page i can get a little bit more information with regards to this image and as shown here on the right i can see my pricing the usage fee will cost me 109 a month along with the vm instance type that the software is configured for the amount of disk space and the sustained use discount i've also been given some links here for tutorials and documentation and i've also been given instructions for maintenance and support i've been given both an email and a link to live support and of course at the bottom we have the terms of service and this is a typical software package amongst many others that's available in the google cloud marketplace now i can go ahead and launch this if i choose but i'm going to choose not to launch this and i'm going to back out and so just to give you some context with regards to enterprise software software packages like f5 and jenkins are also available in the google cloud marketplace and again when i click on the first option it'll give me a bunch of available options on jenkins and its availability from different companies on different platforms now just as a note to update your existing deployment of a software package you have to redeploy the software package from marketplace in order to update it but other than that caveat the easiest way to deploy a software package is definitely through the marketplace and so now that we've gone through all the different options on how to create an instance i'm gonna go back and select new vm instance so i can create a new vm from scratch and so i am prompted here at the top with a note telling me that there was a draft that was saved from when i started to create in my new instance but i navigated away from it and i have the option to restore the configuration i was working on and so just know that when you are in the midst of creating an instance google cloud will automatically save a draft of your build so that you are able to continue working on it later now i don't really need this draft but i will just hit restore and for the name i'm going to keep it as instance 1 and for the sake of this demo i'm going to add a label the key is going to be environment and the value will be testing i'm going to go down to the bottom click save now when it comes to the geographic location of the instance using regions i can simply click on the drop down and i will have access to deploy this instance in any currently available region as regions are added they will be added here as well and so i'm going to keep it as us east one and under zone i have the availability of putting it in any zone within that region and so i'm going to keep it as us east 1b and just as another note once you've deployed the instance in a specific region you will not be able to move that instance to a different region you will have to recreate it using a snapshot in another region and i will be going over this in a later lesson now scrolling down to machine configuration there are three different types of families that you can choose from when it comes to machine types the general purpose the compute optimized and the memory optimized the general purpose machine family has a great available selection of different series types that you can choose from and is usually the go to machine family if you're unsure about which machine type to select so for this demo i'm going to keep my selection for series type as e2 and under machine type i'm given a very large selection of different sizes when it comes to vcpu and memory and so i can select from a shared core a standard type a high memory type or a high cpu type and i will be going over this in greater detail in another lesson on machine types now in case the predefined machine types do not fit my needs or the scope for the amount of vcpus and memory that i need fall in between those predefined machine types i can simply select the custom option and this will bring up a set of sliders where i am able to select both the amount of vcpus and amount of memory that i need for the instance that i am creating now as i change the course slider to either more vcpus or less my core to memory ratio for this series will stay the same and therefore my memory will be adjusted automatically i also have the option to change the memory as i see fit to either add more memory or to remove it and so this is great for when you're in between sizes and you're looking for something specific that fits your workload and so i'm going to change back the machine type to an e2 micro and as you can see in the top right i will find a monthly estimate of how much the instance will cost me and i can click on this drop down and it will give me a breakdown of the cost for vcpu in memory the cost for my disks as well as my sustained use discount and if i had any other resources that i was consuming like a static ip or an extra attached disk those costs would show up here as well and so if i went to a compute optimized you can see how the price has changed but i'm given the breakdown so that i know exactly what i'm paying for so i'm going to switch it back to general purpose and i wanted to point out here the cpu platform and gpu as you can add gpus to your specific machine configuration and so just as another note gpus can only be added to an n1 machine type as any other type will show the gpu selection as grayed out and so here i can add the gpu type as well as adding the number of gpus that i need but for the sake of this demonstration i'm not going to add any gpus and i'm going to select the e2 series and change it back to e2 micro scrolling down a little bit here when it comes to cpu platform depending on the machine type you can choose between intel or amd if you are looking for a specific cpu but just know that your configuration is permanent now moving down a little bit more you will see here display device now display device is a feature on compute engine that allows you to add a virtual display to a vm for system management tools remote desktop software and any application that requires you to connect to a display device on a remote server this is an especially great feature to have for when your server is stuck at boot patching or hardware failure and you can't log in and the drivers are already included for both windows and linux vms this feature works with the default vga driver right out of the box and so i'm going to keep this checked off as i don't need it and i'm going to move down to confidential vm service now confidential computing is a security feature to encrypt sensitive code and data that's in memory so even when it's being processed it is still encrypted and is a great use case when you're dealing with very sensitive information that requires strict requirements now compute engine also gives you the option of deploying containers on it and this is a great way to test your containers instead of deploying a whole kubernetes cluster and may even suffice for specific use cases but just note that you can only deploy one container per vm instance and so now that we've covered most of the general configuration options for compute engine i wanted to take a minute to dive into the options that are available for boot disk so i'm going to go ahead and click on change and here i have the option of choosing from a bunch of different public images with different operating systems that i can use for my boot disk so if i wanted to load up ubuntu i can simply select ubuntu and i can choose from each different version that's available as well i'm shown here the boot disk type which is currently selected as the standard persistent disk but i also have the option of selecting either a balanced persistent disk or ssd persistent disk and i'm going to keep it as standard persistent disk and if i wanted to i can increase the boot disk size so if i wanted 100 gigs i can simply add it and if i select it and i go back up to the top right hand corner i can see that my price for the instance has changed now i'm not charged for the operating system due to it being an open source image but i am charged more for the standard persistent disk because i'm no longer using 10 gigs but i'm using 100 gigabytes now let's say i wanted to go back and i wanted to change this image to a windows image i'm going to go down here to windows server and i want to select windows server 2016 i'm going to load up the data center version and i'm going to keep the standard persistent disk along with 100 gigabytes i'm going to select it if i scroll back up i can see that i'm charged a licensing fee for windows server and these images with these licensing fees are known as premium images so please make sure that you are aware of these licensing fees when launching your instances and because i want to save on money just for now i'm going to scroll back down to my boot disk and change it back to ubuntu and i'm going to change the size back down to 10 gigabytes as well before you move on i wanted to touch on custom images and so if i did have any custom images i could see them here and i would be able to create instances from my custom images using this method i also have the option of creating an instance from a snapshot and because i don't have any nothing shows up and lastly i have the option of using existing disks so let's say for instance i had a vm instance and i had deleted it but i decided to keep the attached boot disk it would show up as unattached and i am able to attach that to a new instance and so now that i've shown you all the available options when it comes to boot disk i'm going to go ahead and select the ubuntu operating system and move on to the next option here we have identity and api access which we've gone through in great depth in a previous demo as well i'm given an option to create a firewall rule automatically for http and https traffic and as for networking as we covered it in great depth in the last section i will skip that part of the configuration and simply launch it in the default vpc and so just as a quick note i wanted to remind you that down at the bottom of the page you can find the command line shortcut and when you click on it it will give you the gcloud command to run that you can use in order to create your instance and so i want to deploy this as is so i'm going to click here on close and i'm going to click on create and so i'm just going to give it a minute now so the instance can be created and it took a few seconds but the instance is created and this is regarded as the inventory page to view your instance inventory and to look up any correlating information on any of your instances and so this probably looks familiar to you from the previous instances that you've launched so here we have the name of the instance the zone the internal ip along with the external ip and a selection to connect to the instance as well i'm also given the option to connect to this instance in different ways you also have the option of adding more column information to your inventory dashboard with regards to your instance and you can do this by simply clicking on the columns button right here above the list of instances and you can select from creation time machine type preserve state and even the network and this may bring you more insight on the information available for that instance or even grouping of instances with common configurations this will also help you identify your instances visually in the console and so i'm just going to put the columns back to exactly what it was and so now i want to take a moment to dive right into the instance and have a look at the instance details so as you remember we selected the machine type of e2 micro which has two vcpus and one gigabyte of memory here we have the instance id as well scrolling down we have the cpu platform we have the display device that i was mentioning earlier along with the zone the labels the creation time as well as the network interface and scrolling down i can see here the boot disk with the ubuntu image as well as the name of the boot disk so there are quite a few configurations here and if i click on edit i can edit some of these configurations on the fly and with some configurations i need to stop the instance before editing them and there are some configurations like the network interface where i would have to delete the instance in order to recreate it so for instance if i wanted to change the machine type i need to stop the instance in order to change it and the same thing goes for my display device as well the network interface in order for me to change it from its current network or subnetwork i'm going to have to stop the instance in order to change it as well and so i hope this general walkthrough of configuring an instance has given you a sense of what can be configured on launch and allowed you to gain some insight on editing features of an instance after launch a lot of what you've seen here in this demo will come up in the exam and so i would recommend that before going into the exam to spend some time launching instances knowing exactly how they will behave and what can be edited after creation that can be done on the fly edits that need the instance to be shut down and edits that need the instance to be recreated and so that's pretty much all i wanted to cover when it comes to creating an instance so you can now mark this as complete and let's move on to the next one welcome back now in this lesson i'm going to be discussing compute engine machine types now a machine type is a set of virtualized hardware resources that's available to a vm instance including the system memory size virtual cpu count and persistent disks in compute engine machine types are grouped and curated by families for different workloads you must always choose a machine type when you create an instance and you can select from a number of predefined machine types in each machine type family if the predefined machine types don't meet your needs then you can create your own custom machine types in this lesson i will be going through all the different machine types their families and their use cases so with that being said let's dive in now each machine type family displayed here includes different machine types each family is curated for specific workload types the following primary machine types are offered on compute engine which is general purpose compute optimized and memory optimized and so i wanted to go through each one of these families in a little bit of detail now before diving right into it defining what type of machine type you are running can be overwhelming for some but can be broken down to be understood a bit better they are broken down into three parts and separated by hyphens the first part in this example shown here is the series so for this example the series is e2 and the number after the letter is the generation type in this case it would be the second generation now the series come in many different varieties and each are designed for specific workloads now moving on to the middle part of the machine type this is the actual type and types as well can come in a slew of different flavors and is usually coupled with a specific series so in this example the type here is standard and so moving on to the third part of the machine type this is the amount of vcp use in the machine type and so with vcpus they can be offered anywhere from one vcpu up to 416 vcpus and so for the example shown here this machine type has 32 vcpus and so there is one more aspect of a machine type which is the gpus but please note that gpus are only available for the n1 series and so combining the series the type and the vcpu you will get your machine type and so now that we've broken down the machine types in order to properly define them i wanted to get into the predefined machine type families specifically starting off with the general purpose predefined machine type and all the general purpose machine types are available in the standard type the high memory type and the high cpu type so the standard type is the balance of cpu and memory and this is the most common general purpose machine type general purpose also comes in high memory and this is a high memory to cpu ratio so very high memory a lower cpu and lastly we have the high cpu machine type and this is a high cpu to memory ratio so this would be the opposite of the high memory so very high cpu to lower memory so now digging into the general purpose machine family i wanted to start off with the e2 series and this is designed for daytoday computing at a low cost so if you're looking to do things like web serving application serving back office applications small to medium databases microservices virtual desktops or even development environments the e2 series would serve the purpose perfectly now the e2 machine types are cost optimized machine types that offer sizing between 2 to 32 vcpus and half a gigabyte to 128 gigabytes of memory so small to medium workloads that don't require as many vcpus and applications that don't require local ssds or gpus are an ideal fit for e2 machines e2 machine types do not offer sustained use discounts however they do provide consistently low ondemand and committed use pricing in other words they offer the lowest ondemand pricing across the general purpose machine types as well the e2 series machines are available in both predefined and custom machine types moving on i wanted to touch on all the machine types available in the nseries and these are a balanced machine type with price and performance across a wide range of vm flavors and these machines are designed for web servers application servers back office applications medium to large databases as well as caching and media streaming and they are offered in the standard high memory and high cpu types now the n1 machine types are compute engines first generation general purpose machine types now this machine type offers up to 96 vcpus and 624 gigabytes of memory and again as i mentioned earlier this is the only machine type that offers both gpu support and tpu support the n1 type is available as both predefined machine types and custom machine types and the n1 series offers a larger sustained use discount than n2 machine types speaking of which the n2 machine types are the second generation general purpose machine types and these offer flexible sizing between two 280 vcpus and half a gigabyte of memory to 640 gigabytes of memory and these machine types also offer an overall performance improvement over the n1 machine types workloads that can take advantage of the higher clock frequency of the cpu are a good choice for n2 machine types and these workloads can get higher per thread performance while benefiting from all the flexibility that a general purpose machine type offers and two machine types also offer the extended memory feature and this helps control per cpu software licensing costs now getting into the last n series machine type the n2d machine type is the largest general purpose machine type with up to 224 vcpus and 896 gigabytes of memory this machine type is available in predefined and custom machine types and this machine type as well has the extended memory feature which i discussed earlier that helps you avoid per cpu software licensing the n2d machine type supports the committed use and sustain use discounts now moving on from the general purpose machine type family i wanted to move into the compute optimize machine family now this series offers ultra high performance for compute intensive workloads such as high performance computing electronic design automation gaming and single threaded applications so anything that is designed for compute intensive workloads this will definitely be your best choice now compute engine optimized machine types are ideal for as i said earlier compute intensive workloads and these machine types offer the highest performance per core on compute engine compute optimized types are only available as predefined machine types and so they are not available for any custom machine types the c2 machine types offer a maximum of 60 vcpus and a maximum of 240 gigabytes of memory now although the c2 machine type works great for compute intensive workloads it does come with some caveats and so you cannot use regional persistent disks with compute optimized machine types and i will be getting into the details of persistent disks in a later lesson and they are only available in select zones and regions on select cpu platforms and so now moving into the last family is the memory optimize machine family and this is for ultra high memory workloads this family is designed for large in memory databases like sap hana as well as in memory analytics now the m series comes in two separate generations m1 and m2 the m1 offering a maximum of 160 vcpus and a maximum memory of 3844 gigabytes whereas the m2 offering again a maximum of 160 vcpus but offering a whopping 11 776 gigabytes of maximum memory and as i said before these machine types they're ideal for tasks that require intensive use of memory so they are suited for inmemory databases and in memory analytics data warehousing workloads genomics analysis and sql analysis services memory optimized machine types are only available as predefined machine types and the caveats here is that you cannot use regional persistent disks with memory optimized machine types as well they're only available in specific zones now i wanted to take a moment to go back to the general purpose machine type so that i can dig into the shared cord machine type and this is spread amongst the e2 and n1 series and these shared core machine types are used for burstable workloads are very cost effective as well they're great for nonresource intensive applications shared core machine types use context switching to share a physical core between vcpus for the purpose of multitasking different shared core machine types sustain different amounts of time on a physical core which allows google cloud to cut the price in general share core instances can be more cost effective for running small nonresource intensive applications than standard high memory or high cpu machine types now when it comes to cpu bursting these shared core machine types offer bursting capabilities that allow instances to use additional physical cpu for short periods of time bursting happens automatically when your instance requires more physical cpu than originally allocated during these spikes your instance will take advantage of available physical cpu in bursts and the e2 shared core machine type is offered in micro small and medium while the n1 series is offered in the f1 micro and the g1 small and both of these series have a maximum of two vcpus with a maximum of four gigabytes of memory now i wanted to take a moment to touch on custom machine types and these are available for any general purpose machine and so this is customer defined cpu and memory designed for custom workloads now if none of the general purpose predefined machine types cater to your needs you can create a custom machine type with a specific number of vcpus and amount of memory that you need for your instance these machine types are ideal for workloads that are not a good fit for the predefined machine types that are available they're also great for when you need more memory or more cpu but the predefined machine types don't quite fit exactly what you need for your workload just as a note it costs slightly more to use a custom machine type than a predefined machine type and there are limitations in the amount of memory and vcpu you can select and as i stated earlier when creating a custom machine type you can choose from the e2 n2 and 2d and n1 machine types and so the last part i wanted to touch on are the gpus that are available and these are designed for the graphic intensive workloads and again are only available for the n1 machine type and gpus come in five different flavors from nvidia showing here as the tesla k80 the tesla p4 the tesla t4 the tesla v100 and the tesla p100 and so these are all the families and machine types that are available for you in google cloud and will allow you to be a little bit more flexible with the type of workload that you need them for and so for the exam you won't have to memorize each machine type but you will need to know an overview of what each machine type does now i know there's been a lot of theory presented here in this lesson but i hope this is giving you a better understanding of all the available predefined machine types in google cloud and so that's pretty much all i wanted to cover in this lesson on compute engine machine types so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be reviewing managing your instances now how you manage your instances is a big topic in the exam as well it's very useful to know for your work as a cloud engineer in the environments you are responsible for knowing both the features that are available as well as the best practices will allow you to make better decisions with regards to your instances and allow you to keep your environment healthy this lesson will dive into the many features that are available in order to better manage your instances using the specific features within google cloud so with that being said let's dive in now i wanted to start off this lesson discussing the life cycle of an instance within google cloud every instance has a predefined life cycle from its starting provisioning state to its deletion an instance can transition through many instant states as part of its life cycle when you first create an instance compute engine provisions resources to start your instance next the instance moves into staging where it prepares the first boot and then it finally boots up and is considered running during its lifetime a running instance can be repeatedly stopped and restarted or suspended and resumed so now i wanted to take a few minutes to go through the instance life cycle in a bit of detail starting with the provisioning state now this is where resources are being allocated for the instance the instance is not yet running and the instance is being allocated its requested amount of cpu and memory along with its root disk any additional disks that are attached to it and as well some additional feature sets that are assigned to this instance and when it comes to the cost while in the provisioning state there are no costs that are being incurred moving right along to the staging state after finishing the provisioning state the life cycle continues with the staging state and this is where resources have been acquired and the instance is being prepared for first boot both internal and external ips are allocated and can be either static or ephemeral in the system image that was originally chosen for this instance is used to boot up the instance and this can be either a public image or a custom image costs in the state are still not incurred as the instance is still in the preboot state now once the instance has left staging it will move on to the running state and this is where the instance is booting up or running and should allow you to log into the instance either using ssh or rdp within a short waiting period due to any startup scripts or any boot maintenance tasks for the operating system now during the running state you can reset your instance and this is where you would wipe the memory contents of the vm instance and reset the virtual machine to its initial state resetting an instance causes an immediate hard reset of the vm and therefore the vm does not do a graceful shutdown for the guest operating system however the vm retains all persistent disk data and none of the instance properties change the instance remains in running state through the reset now as well in the running state a repair can happen due to the instance encountering an internal error or the underlying machine is unavailable due to maintenance during this time the instance is unusable and if the repair is successful the instance returns back to the running state paying attention to costs this state is where the instance starts to occur them and is related to the resources assigned to the instance like the cpu and memory any static ips and any disks that are attached to the instance and i will be going into a bit of detail in just a bit with regards to this state and finally we end the life cycle with the stopping suspended and terminated states now when you are suspending an instance it is like closing the lid of your laptop suspending the instance will preserve the guest operating system memory and application state of the instance otherwise it'll be discarded and from this state you can choose either to resume or to delete it when it comes to stopping either a user has made a request to stop the instance or there was a failure and this is a temporary status and the instance will move to terminated touching on costs for just a second when suspending or stopping an instance you pay for resources that are still attached to the vm instance such as static ips and persistent disk data you do not pay the cost of a running vm instance ephemeral external ip addresses are released from the instance and will be assigned a new one when the instance is started now when it comes to stopping suspending or resetting an instance you can stop or suspend an instance if you no longer need it but want to keep the instance around for future use compute engine waits for the guest to finish shutting down and then transitions the instance to the terminated state so touching on the terminated state this is where a user either shuts down the instance or the instance encounters a failure you can choose to restart the instance or delete it as well as holding some reset options within the availability policy in this state you still pay for static ips and disks but like the suspending or stopping state you do not pay for the cpu and memory resources allocated to the instance and so this covers a high level overview of the instance lifecycle in google cloud and all of the states that make up this lifecycle now to get into some detail with regards to some feature sets for compute engine i wanted to revisit the states where those features apply now when creating your instance you have the option of using shielded vms for added security and when using them the instance would instantiate them as the instance boots and enters into the running state so what exactly is a shielded vm well shielded vms offer verifiable integrity of your compute engine vm instances so you can be sure that your instances haven't been compromised by boot or kernel level malware or rootkits and this is achieved through a fourstep process which is covered by secure boot virtual trusted platform module also known as vtpm measure boot which is running on vtpm and integrity monitoring so i wanted to dig into this for just a sec to give you a bit more context now the boot process for shielded vms start with secure boot and this helps ensure that the system only runs authentic software by verifying the digital signature for all boot components and stopping the boot process if signature verification fails so shielded vm instances run firmware that's signed and verified using google's certificate authority and on each and every boot any boot component that isn't properly signed or isn't signed at all is not allowed to run and so the first time you boot a vm instance measure boot creates the integrity policy baseline from the first set of these measurements and then securely stores this data each time the vm instance boots after that these measurements are taken again and stored in secure memory until the next reboot having these two sets of measurements enables integrity monitoring which is the next step and allows it to determine if there have been changes to a vm instance's boot sequence and this policy is loaded onto a virtualized trusted platform module again known as the vtpm for short which is a specialized computer chip that you can use to protect objects like keys and certificates that you use to authenticate access to your system with shielded vms vtpm enables measured boot by performing the measurements needed to create a known good boot baseline and this is called the integrity policy baseline the integrity policy baseline is used for comparison with measurements from subsequent vm boots to determine if anything has changed integrity monitoring relies on the measurements created by measured boot for both the integrity policy baseline and the most recent boot sequence integrity monitoring compares the most recent boot measurements to the integrity policy baseline and returns a pair of pass or failed results depending on whether they match or not one for the early boot sequence and one for the late boot sequence and so in summary this is how shielded vms help prevent data exfiltration so touching now on the running state when you start a vm instance using google provided public images a guest environment is automatically installed on the vm instance a guest environment is a set of scripts daemons and binaries that read the content of the metadata server to make a virtual machine run properly on compute engine a metadata server is a communication channel for transferring information from a client to the guest operating system vm instances created using google provided public images include a guest environment that is installed by default creating vm instances using a custom image will require you to manually install the guest environment this guest environment is available for both linux and windows systems and each supported operating system that is available on compute engine requires specific guest environment packages either google or the owner of the operating system builds these packages now when it comes to the linux guest environment it is either built by google or the owner of the operating system and there are some key components that are applicable to all builds which can be found in the link that i have included in the lesson text the base components of a linux guest environment is a python package that contains scripts daemons and packages for the supported linux distributions when it comes to windows a similar approach applies where a package is available with main scripts and binaries as a part of this guest environment now touching back on the metadata server compute engine provides a method for storing and retrieving metadata in the form of the metadata server this service provides a central point to set metadata in the form of key value pairs which is then provided to virtual machines at runtime and you can query this metadata server programmatically from within the instance and from the compute engine api this is great for use with startup and shutdown scripts or gaining more insight with your instance metadata can be assigned to projects as well as instances and project metadata propagates to all instances within the project while instance metadata only impacts that instance and you can access the metadata using the following url with the curl command you see here on the screen so if you're looking for the metadata for a project you would use the first url that ends in project and for any instance metadata you can use the second url that ends in instance now please note that when you make a request to get information from the metadata server your request and the subsequent metadata response never leaves the physical host running the virtual machine instance now once the instance has booted and has gone through the startup scripts you will then have the ability to login to your instance using ssh or rdp now there are some different methods that you can use to connect and access both your linux instances and your windows instances that i will be going over now when it comes to linux instances we've already gone through accessing these types of instances in previous lessons and demos but just as a refresher you would typically connect to your vm instance via ssh access on port 22. please note that you will require a firewall rule as we have done in previous demos to allow this access and you can connect to your linux instances through the google cloud console or the cloud shell using the cloud sdk now i know that the use of ssh keys are the defacto when it comes to logging into linux instances now in most scenarios on google cloud google recommends using os login over using ssh keys the os login feature lets you use compute engine iam roles to manage ssh access to linux instances and then if you'd like you can add an extra layer of security by setting up os login with twostep verification and manage access at the organization level by setting up organizational policies os login simplifies ssh access management by linking your linux user account to your google identity administrators can easily manage access to instances at either an instance or project level by setting iam permissions now if you're running your own directory service for managing access or are unable to set up os login you can manually manage ssh keys and local user accounts in metadata by manually creating ssh keys and editing the public ssh key metadata now when it comes to windows instances you would typically connect to your vm instance via rdp access on port 3389 and please note that you will also require a firewall rule as shown here to allow this access you can connect to your windows instances through the rdp protocol or through a powershell terminal now when logging into windows this requires setting a windows password and can be done either through the console or the gcloud command line tool and then after setting your password you can then log in from the recommended rdp chrome extension or using a thirdparty rdp client and i will provide a link to this rdp chrome extension in the lesson text now once the instance has booted up and your instance is ready to be logged into you always have the option of modifying your instance and you can do it manually by either modifying it on the fly or you can take the necessary steps to edit your instance like i showed you in a previous lesson by stopping it editing it and then restarting it although when it comes to google having to do maintenance on a vm or you merely want to move your instance to a different zone in the same region this has all become possible without shutting down your instance using a feature called live migration now when it comes to live migration compute engine migrates your running instances to another host in the same zone instead of requiring your vms to be rebooted this allows google to perform maintenance reliably without interrupting any of your vms when a vm is scheduled to be live migrated google provides a notification to the guest that a migration is coming soon live migration keeps your instances running during compute engine hosts that are in need of regular infrastructure maintenance and upgrades replacement of failed hardware and system configuration changes when google migrates a running vm instance from one host to another it moves the complete instance state from the source to the destination in a way that is transparent to the guest os and anyone communicating with it google also gives you the option of doing live migration manually from one zone to another within the same region either using the console or running the command line you see here gcloud compute instances move the name of the vm with the zone flag and the zone that it's currently in and then the destination zone flag with the zone that you wanted to go to and just as a note with some caveats instances with gpus attached cannot be live migrated and you can't configure a preemptable instance to live migrate and so instance lifecycle is full of different options and understanding them can help better coordinate moving editing and repairing vm instances no matter where they may lie in this life cycle now i hope this lesson has given you the necessary theory that will help better use the discuss feature sets and giving you some ideas on how to better manage your instances now there is a lot more to know than what i've shown you here to manage your instances but topics shown here are what shows up in the exam as well are some really great starting points to begin managing your instances and so that's pretty much all i wanted to cover when it comes to managing instances so you can now mark this lesson as complete and join me in the next one where i will cement the theory in this lesson with the handson demo welcome back in this demonstration i'm going to be cementing some of the theory that we learned in the last lesson with regards to the different login methods for windows and linux instances how to implement these methods are extremely useful to know both for the exam and for managing multiple instances in different environments now there's a lot to cover here so with that being said let's dive in so as you can see i am logged in here under tony bowtie ace gmail.com as well i am in the project of bowtie inc and so the first thing that i want to do is create both a linux instance and a windows instance and this is to demonstrate the different options you have for logging into an instance and so in order for me to do that i need to head on over to compute engine so i'm going to go over to the navigation menu and i'm going to scroll down to compute engine and so just as a note before creating your instances please make sure that you have a default vpc created before going ahead and creating these instances if you've forgotten how to create a default vpc please go back to the networking services section and watch the vpc lesson for a refresher and so i'm going to go ahead and create my first instance and i'm going to start with the windows instance so i'm going to simply click on create and so for the name of this instance you can simply call this windows dash instance and i'm not going to add any labels and for the region you should select us east1 and you can keep the zone as the default for us east 1b and scrolling down to the machine configuration for the machine type i'm going to keep it as is as it is a windows instance and i'm going to need a little bit more power scrolling down to boot disk we need to change this from debian over to windows so i'm going to simply click on the change button and under operating system i'm going to click on the drop down and select windows server for the version i'm going to select the latest version of windows server which is the windows server 2019 data center and you can keep the boot disk type and the size as its default and simply head on down and click on select and we're going to leave everything else as the default and simply click on create and success our windows instance has been created and so the first thing that you want to do is you want to set a windows password for this instance and so i'm going to head on over to the rdp button and i'm going to click on the dropdown and here i'm going to select set windows password and here i'm going to get a popup to set a new windows password the username has been propagated for me as tony bowties i'm going to leave it as is and i'm going to click on set and i'm going to be prompted with a new windows password that has been set for me so i'm going to copy this and i'm going to paste it into my notepad so be sure to record it somewhere either write it down or copy and paste it into a text editor of your choice i'm going to click on close and so now for me to log into this i need to make sure of a couple things the first thing is i need to make sure that i have a firewall rule open for port 3389 the second is i need to make sure that i have an rdp client and so in order to satisfy my first constraint i'm going to head on over to the navigation menu and go down to vpc network here i'm going to select firewall and as expected the rdp firewall rule has been already created due to the fact that upon creation of the default vpc network this default firewall rule is always created and so now that i've gotten that out of the way i'm going to head back on over to compute engine and what i'm going to do is i'm going to record the external ip so that i'll be able to log into it now i'm going to be logging into this instance from both a windows client and a mac client so starting with windows i'm going to head on over to my windows virtual machine and because i know windows has a default rdp client already built in i'm going to simply bring it up by hitting the windows key and typing remote desktop connection i'm going to click on that i'm going to paste in the public ip for the instance that i just recorded and i'm going to click on connect you should get a popup asking for your credentials i'm going to type in my username as tony bowtie ace as well i'm going to paste in the password and i'm going to click on ok i'm prompted to accept the security certificate and i'm going to select yes and success i'm now connected to my windows server instance and it's going to run all its necessary startup scripts you may get a couple of prompts that come up asking you if you want to connect to your network absolutely i'm going to close down server manager just for now and another thing that i wanted to note is that when you create a windows instance there will automatically be provisioned a google cloud shell with the sdk preinstalled and so you'll be able to run all your regular commands right from this shell without having to install it and this is due to the guest environment that was automatically installed on the vm instance upon creation and this is a perfect example of some of the scripts that are installed with the guest environment i'm going to go ahead and close out of this and i'm going to go ahead and close out of my instance hit ok and so being here in windows i wanted to show you an alternate way of logging into your instance through powershell so for those of you who are quite versed in windows and use powershell in your daytoday there is an easy way to log into your instance using powershell now in order for me to do that i need to open another firewall rule covering tcp port 5986 so i'm going to head on over back to the google cloud console i'm going to head over to the navigation menu and i'm going to scroll down to vpc network i'm going to go into firewall and i'm going to create a new firewall rule and under name i'm going to name this as allow powershell i'm going to use the same for the description i'm going to scroll down to targets and i'm going to select all instances in the network and under source ip ranges for this demonstration i'm going to use 0.0.0.0 forward slash 0. and again this should not be used in a production environment but is used merely for this demo i'm going to leave everything else as is and i'm going to go down to protocols and ports i'm going to click on tcp and i'm going to type in 5986 for the port and i'm going to click on create i'm going to give it a second just to create and it took a couple seconds but our firewall rule is now created and so now i'm gonna head over to my windows vm and i'm gonna open up a powershell command prompt and hit the windows key and type in powershell and so in order for me to not get constantly asked about my username and password i'm going to use a variable that will keep my password for me and so every time i connect to my windows instance i won't need to type it in all the time and so the command for that is dollar sign credentials equals get dash credential i'm going to hit enter and i'm going to get a prompt to type in my username and password so i'm going to simply type that in now along with my password and hit ok and if you don't get a prompt with any errors then chances are that you've been successful at entering your credentials and so now in order to connect to the instance you're going to need the public ip address again so i'm going to head on over back to the console i'm going to head on over to the navigation menu and back to compute engine here i'm going to record the external ip and i'm going to head on over back to my windows virtual machine and so you're going to enter this command which i will include in the lesson text and you'll also be able to find it in the github repository beside computer name you're going to put in your public ip address of your windows instance and make sure at the end you have your credentials variable i'm going to simply click enter and success i'm now connected to my windows instance in google cloud so as you can see here on the left is the public ip of my windows instance and so these are the various ways that you can connect to your windows instance from a windows machine and so now for me to connect to my windows instance on a mac i'm going to head on over there now and like i said before i need to satisfy the constraint of having an rdp client unfortunately mac does not come with an rdp client and so the recommended tool to use is the chrome extension but i personally like microsoft's rdp for mac application and so i'm going to go ahead and do a walkthrough of the installation so i'm going to start off by opening up safari and i'm going to paste in this url which i will include in the lesson text and microsoft has made available a microsoft remote desktop app available in the app store i'm going to go ahead and view it in the app store and i'm going to simply click on get and then install and once you've entered your credentials and you've downloaded and installed it you can simply click on open i'm going to click on not now and continue and i'm going to close all these other windows for better viewing i'm going to click on add pc i'm going to paste in the public ip address of my windows instance and under user account i'm going to add my user account type in my username paste in my password you can add a friendly name here i'm going to type in windows dash gc for google cloud and i'm going to click on add and then once you've pasted in all the credentials and your information you can then click on add and i should be able to connect to my windows instance by double clicking on this window it's asking me for my certificates i'm going to hit continue and success i'm connected to my windows instance and so this is how you would connect to a windows instance from a windows machine as well as from a mac as well there are a couple of other options that i wanted to show you over here on the drop down beside rdp i can download an rdp file which will contain the public ip address of the windows instance along with your username if i need to reset my password i can view the gcloud command to do it or i can set a new windows password if i forgotten my old one and so that's everything i had to show you with regards to connecting to a windows instance and so since this demo was getting kind of long i decided to split it up into two parts and so this is the end of part one of this demo and this would be a great opportunity to get up and have a stretch grab yourself a tea or a coffee and whenever you're ready you can join me in part two where we will be starting immediately from the end of part 1 so you can complete this video and i'll see you in part 2. welcome back this is part 2 of the connecting to your instances demo and we will be starting exactly where we left off in part one so with that being said let's dive in and so now that we've created our windows instance and went through all the methods of how to connect to it let's go ahead and create a linux instance i'm going to go up to the top menu here and click on create instance and i'm going to name this instance linux instance i'm not going to give it any labels under region i'm going to select the us east one region and the zone i'm going to leave it as its set default as us east 1b the machine configuration i'm going to leave it as is under boot disk i'm going to leave this as is with the debian distribution and i'm going to go ahead and click on create okay and our linux instance has been created and in order for me to connect to it i am going to ssh into it but first i need to satisfy the constraint of having a firewall rule with tcp port 22 open so i'm going to head on over to the navigation menu and i'm going to scroll down to vpc network i'm going to head on over to firewall and as expected the allow ssh firewall rule has been created alongside the default vpc network and so since i've satisfied that constraint i can head back on over to compute engine and so here i have a few different options that i can select from for logging into my linux instance i can open in a browser window if i decided i wanted to put it on a custom port i can use this option here if i provided a private ssh key to connect to this linux instance i can use this option here i have the option of viewing the gcloud command in order to connect to it and i've been presented with a popup with the command to use within the gcloud command line in order to connect to my instance i can run it now in cloud shell but i'm going to simply close it and so whether you are on a mac a windows machine or a linux machine you can simply click on ssh and it will open a new browser window connecting you to your instance now when you connect to your linux instance for the first time compute engine generates an ssh key pair for you this key pair by default is added to your project or instance metadata and this will give you the freedom of not having to worry about managing keys now if your account is configured to use os login compute engine stores the generated key pair with your user account now when connecting to your linux instance in most scenarios google recommends using os login this feature lets you use iam roles to manage ssh access to linux instances and this relieves the complexity of having to manage multiple key pairs and is the recommended way to manage many users across multiple instances or projects and so i'm going to go ahead now and show you how to configure os login for your linux instance and the way to do this will be very similar on all platforms so i'm going to go ahead and go back to my mac vm and i'm going to open up my terminal make this bigger for better viewing and i'm going to start by running the gcloud init command in order to make sure i'm using the right user and for the sake of this demonstration i'm going to reinitialize this configuration so i'm going to click on one hit enter number two for tony bowtie ace and i'm going to use project bow tie ink so 1 and i'm not going to configure a default compute region in zone and so if i run the gcloud config list command i can see that the account that i'm using is tony bowties gmail.com in project bowtie inc and so because os login requires a key pair i'm going to have to generate that myself so i'm going to go ahead and clear the screen and i'm going to use the command ssh keygen and this is the command to create a public and private key pair i'm going to use the default path to save my key and i'm going to enter a passphrase i'm going to enter it again and i recommend that you write down your passphrase so that you don't forget it as when you lose it you will be unable to use your key pair and so if i change directory to dot ssh and do an ls for list i can see that i now have my public and private key pair the private key lying in id underscore rsa and the public key lying in id underscore rsa.pub and so another constraint that i have is i need to enable os login for my linux instance so i'm going to go ahead and go back to the console and i'm going to go ahead and go into my linux instance i'm going to click on edit and if you scroll down you will come to some fields marked as custom metadata and under key you will type in enable dash os login and under value you will type in all caps true now i wanted to take a moment here to discuss this feature here under ssh keys for block project wide ssh keys now project wide public ssh keys are meant to give users access to all of the linux instances in a project that allow project projectwide public ssh keys so if an instance blocks projectwide public ssh keys as you see here a user can't use their projectwide public ssh key to connect to the instance unless the same public ssh key is also added to the instance metadata this allows only users whose public ssh key is stored in instance level metadata to access the instance and so this is an important feature to note for the exam and so we're going to leave this feature checked off for now and then you can go to the bottom and click on save now if i wanted to enable os login for all instances in my project i can simply go over to the menu on the left and click on metadata and add the metadata here with the same values so under key i type in enable dash os login and under value i type in in all caps true but i don't want to enable it for all my instances only for that one specific instance so with regards to projectwide public keys these keys can be managed through metadata and should only be used as a last resort if you cannot use the other tools such as ssh from the console or os login these are where the keys are stored and so you can always find them here when looking for them here as you can see there are a couple of keys for tony bowtie ace that i have used for previous instances and so i'm going to go back to metadata just to make sure that my key value pair for os login has not been saved and it is not and i'm going to head back on over to my instances and so now that my constraint has been fulfilled where i've enabled the os login feature by adding the unnecessary metadata i'm going to head on over back to my mac vm i'm going to go ahead and clear the screen so now i'm going to go ahead and log into my instance using os login by using the command gcloud compute os dash login ssh dash keys add and then the flag key dash file and then the path for my public key which is dot ssh forward slash id underscore rsa.pub i'm gonna hit enter and so my key has been successfully stored with my user account i'm gonna go ahead and make this a little bigger for better viewing and so in order to log into my instance i'm going to need my username which is right up here under username i'm going to copy that and i'm just going to clear my screen for a second here for better viewing and so in order for me to ssh into my instance i'm going to type in the command ssh minus i i'm going to have to provide my private key which is in dot ssh forward slash id underscore rsa and then my username that i had recorded earlier at and then i'm going to need my public ip address of my linux instance so i'm going to head back over to the console for just a sec i'm going to copy the ip address head back over to my mac vm paste it in and hit enter it's asking if i want to continue yes i do enter the passphrase for my key and success i am connected and so there is one caveat that i wanted to show you with regards to permissions for os login so i'm going to head back over to the console and i'm going to go up to the navigation menu and head over to i am an admin now as you can see here tony bowties gmail.com has the role of owner and therefore i don't need any granular specific permissions i have the access to do absolutely anything now in case i was a different user and i didn't hold the role of owner i would be looking for specific permissions that would be under compute os login and this would give me permissions as a standard user now if i wanted super user access or root access i would need to be given the compute os admin login role and as you can see it would allow me administrator user privileges so when using os login and the member is not an owner one of these two roles are needed so i'm going to exit out of here i'm going to hit cancel and so that about covers everything that i wanted to show you with regards to all the different methods that you can use for connecting to vm instances for both windows and linux instances now i know this may have been a refresher for some but for others knowing all the different methods of connecting to instances can come in very useful especially when coordinating many instances in bigger environments i want to congratulate you on making it to the end of this demo and gaining a bit more knowledge on this crucial part of managing your instances so before you go be sure to delete any resources that you've created and again congrats on the great job so you can now mark this as complete and i'll see you in the next one welcome back in this demonstration i'll be discussing metadata and how it can pertain to a project as well as an instance as well i'm going to touch on startup and shutdown scripts and it's real world use cases in the last lesson we touched the tip of the iceberg when it came to metadata and wanted to go a bit deeper on this topic as i personally feel that it holds so much value and give you some ideas on how you can use it i'm also going to combine the metadata using variables in a startup script and i'm going to bring to life something that's dynamic in nature so with that being said let's dive in so i am currently logged in as tony at bowtie ace gmail.com under the project of bow tie inc and so in order to get right into the metadata i'm going to head on over to my navigation menu and go straight to compute engine and over here on the left hand menu you will see metadata and you can drill down into there now as i explained in a previous lesson metadata can be assigned to both projects and instances while instance metadata only impacts a specific instance so here i can add and store metadata which will be used on a projectwide basis as well as mentioned earlier metadata is stored in key value pairs and can be added at any time now this is a way to add custom metadata but there is a default set of metadata entries that every instance has access to and again this applies for both project and instance metadata so here i have the option of setting my custom metadata for the entire project and so i'm going to dive into where to store custom metadata on an instance and so in order for me to show you this i'm going to first head over to vm instances and create my instance and so just as a note before creating your instance make sure that you have the default vpc created and so because i like to double check things i'm going to head over to the navigation menu i'm going to scroll down to vpc network and as expected i have the default vpc already created and so this means i can go ahead and create my instance so i'm going to head back on over to compute engine and i'm going to create my instance and i'm going to name this instance bowtie dash web server i'm not going to add any labels and under the region i'm going to select us east one and you can keep the zone as the default as us east 1b under machine type i want to keep things cost effective so i'm going to select the e2 micro i'm going to scroll down and under identity and api access i want to set access for each api and scroll down to compute engine i want to select it and i want to select on read write and i'm going to leave the rest as is and scrolling down to the bottom i want to click on management security disks networking and sold tenancy and under here you will find the option to add any custom metadata and you can provide it right here under metadata as a key value pair but we're not going to add any metadata right now so i'm just going to scroll down to the bottom i'm going to leave everything else as is and simply click on create and it should take a few moments for my instance to be created okay and now that my instance is up i want to go ahead and start querying the metadata now just as a note metadata must be queried from the instance itself and can't be done from another instance or even from the cloud sdk on your computer so i'm going to go ahead and log into the instance using ssh okay and now that i'm logged into my instance i want to start querying the metadata now normally you would use tools like wget or curl to make these queries in this demo i will use curl and for those who don't know curl is a command line tool to transfer data to or from a server using supported protocols like http ftp scp and many more this tool is fantastic for automation since it's designed to work without any user interaction and so i'm going to paste in the url that i am going to use to query the instance metadata and this is the default url that you would use to query any metadata on any instance getting a little deeper into it a trailing slash shown here shows that the instance value is actually a directory and will have other values that append to this url whether they are other directories or just endpoint values now when you query for metadata you must provide the following header in all of your requests metadata dash flavor colon google and should be put in quotations if you don't provide this header the metadata server will deny your request so i'm going to go ahead and hit enter and as you can see i've been brought up a lot of different values that i can choose from in order to retrieve different types of metadata and as stated before anything with a trailing slash is actually a directory and will have other values underneath it so if i wanted to query the network interfaces and because it's a directory i need to make sure that i add the trailing slash at the end and as you can see here i have the network interface of 0 and i'm going to go ahead and query that and here i will have access to all the information about the network interface on this instance so i'm going to go ahead and query the network on this interface and as expected the default network is displayed i'm going to quickly go ahead and clear my screen and i'm going to go ahead and query some more metadata this time i'm going to do the name of the server and as expected bowtie dash web server showed up and because it's an endpoint i don't need the trailing slash at the end i'm going to go ahead and do one more this time i'm going to choose machine type and again as expected the e2 micro machine type is displayed and so just as a note for those who haven't noticed any time that you query metadata it will show up to the left of your command prompt now what i've shown you here is what you can do with instance metadata and so how about if you wanted to query any project metadata well instead of instance at the end you would use project with the trailing slash i'm going to simply click on enter and as you can see here project doesn't give me a whole lot of options but it does give me some important values like project id so i'm going to simply query that right now and as expected bowtie inc is displayed and so this is a great example of how to query any default metadata for instances and for projects now you're probably wondering how do i query my custom metadata well once custom metadata has been set you can then query it from the attributes directory in the attributes directory can be found in both the instance and project metadata so i'm going to go ahead and show you that now but first i wanted to add some custom metadata and this can be set in either the console the gcloud command line tool or using the api and so i'm going to run the command here gcloud compute instances add dash metadata the name of your instance and when you're adding custom metadata you would add the flag dash dash metadata with the key value pair which in this example is environment equals dev and then i'm also going to add the zone of the instance which is us east 1a and i'm going to hit enter and because i had a typo there i'm going to go ahead and try that again using us east 1b i'm going to hit on enter and success and so to verify that this command has worked i'm going to go ahead and query the instance and i'm going to go under attributes i'm going to hit on enter and as you can see here the environment endpoint has been populated so i'm going to query that and as expected dev is displaying as the environment value now if i wanted to double check that in the console i can go over to the console i can drill down into bowtie web server and if i scroll down to the bottom under custom metadata you can see the key value pair here has m as the key and dev being the value and so these are the many different ways that you can query metadata for any instances or projects now i wanted to take a quick moment to switch gears and talk about startup and shutdown scripts now compute engine lets you create and run your own startup and shutdown scripts on your vm instance and this allows you to perform automation that can perform actions when starting up such as installing software performing updates or any other tasks that are defined in the script and when shutting down you can allow instances time to clean up on perform tasks such as exporting logs to cloud storage or bigquery or syncing with other systems and so i wanted to go ahead and show you how this would work while combining metadata into the script so i'm going to go ahead and drill down into bow tie web server i'm going to click on edit and i'm going to scroll down here to custom metadata i'm going to click on add item and under key i'm going to type in startup dash script and under value i'm going to paste in my script i'm going to just enlarge this here for a second and i will be providing the script in the github repository now just to break it down this is a bash script i'm pulling in a variable called name which will query the instance name as well i have a variable called zone which will query the instance zone i'm going to be installing an apache web server and it's going to display on a web browser both the server name and the zone that it's in and so in order for me to see this web page i also need to open up some firewall rules and so an easy way to do this would be to scroll up to firewalls and simply click on allow http and allow https traffic this will tag the instance with some network tags as http server and https server and create two separate firewall rules that will allow traffic for port 80 and port 443 so i'm going to leave everything else as is i'm going to scroll down to the bottom and click on save okay and it took a few seconds there but it did finish saving i'm going to go ahead and go up to the top and click on reset and this will perform a hard reset on the instance and will allow the startup script to take effect so i'm going to click on reset it's going to ask me if i really want to do this and for the purposes of this demonstration i'm going to click on reset please note you should never do this in production as it doesn't do a clean shutdown on the operating system but as this is an instance with nothing on it i'm going to simply click on reset now i'm going to head on back to the main console for my vm instances and i'm going to record my external ip i'm going to open up a new browser i'm going to zoom in for better viewing and i'm going to paste in my ip address and hit enter and as you can see here i've used my startup script to display not only this web page but i was able to bring in metadata that i pulled using variables and was able to display it here in the browser and so before i end this demonstration i wanted to show you another way of using a startup script but being able to pull it in from cloud storage so i'm going to go back to the navigation menu and i'm going to scroll down to storage here i will create a new bucket and for now find a globally unique name to name your bucket and i'm going to call my bucket bowtie web server site and i'm going to leave the rest as its default and i'm going to simply click on create and if you have a globally unique name for your bucket you will be prompted with this page without any errors and i'm going to go ahead and upload the script and you can find this script in the github repository so i'm going to go into my repo and i'm going to look for bow tie start up final sh i'm going to open it and now that i have the script uploaded i'm going to drill into this file so i can get some more information that i need for the instance and what i need from here is to copy the uri so i'm going to copy this to my clipboard and i'm going to head back on over to compute engine i'm going to drill down into my instance i'm going to click on edit at the top and i'm going to scroll down to where it says custom metadata and here i'm going to remove the startup script metadata and i'm going to add a new item and i'm going to be adding startup dash script dash url and in the value i'm going to paste in the uri that i had just copied over and this way on startup my instance will use this startup script that's in cloud storage so i'm going to scroll down to the bottom click on save and now i'm going to click on reset i'm going to reset here i'm going to go back to the main page for my vm instances and i can see that my external ip hasn't changed so i'm going to go back to my open web browser and i'm going to click on refresh and success and as you can see here i've taken a whole bunch of different variables including the machine name the environment variable the zone as well as the project and i've displayed it here in a simple website and although you may not find this website specifically useful in your production environment this is just an idea to get creative using default and custom metadata along with a startup script i've seen in some environments where people have multiple web servers and create a web page to display all the specific web servers in their different environments along with their ips their data and their configurations and so just as a recap we've gone through the default and custom metadata and how to query it in an instance we also went through startup scripts and how to apply them both locally and using cloud storage and so i hope you have enjoyed having fun with metadata and using them in startup scripts such as this one i also hope you find some fascinating use cases in your current environments and so before you go just a quick reminder to delete any resources that you've created to not incur any added costs and so that's pretty much all i wanted to cover with this demonstration so you can now mark this as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing compute engine billing now when it comes to pricing with regards to compute engine i've only gone over the fact that instances are charged by the second after the first minute but i never got into the depths of billing and the various ways to save money when using compute engine in this lesson i will be unveiling how both costs and discounts are broken down in google cloud as it refers to the resource based billing model and the various savings that can be had when using compute engine so with that being said let's dive in now each vcpu and each gigabyte of memory on compute engine is built separately rather than as part of a single machine type you are still creating instances using predefined machine types but your bill shows them as individual cpus and memory used per hour and this is what google refers to as resourcebased billing which i will get into in just a bit the billing model applies to all vcpus gpus and memory resources and are charged a minimum of one minute for example if you run your virtual machine for 30 seconds you will be billed for one minute of usage after one minute instances are charged in one second increments instance up time is another determining factor for cost and is measured as the number of seconds between when you start an instance and when you stop an instance in other words when your instance is in the terminated state if an instance is idle but still has a state of running it will be charged for instance uptime but again you will not be charged if your instance is in a terminated state now getting into reservations these are designed to reserve the vm instances you need so after you create a reservation the reservation ensures that those resources are always available for you to use during the creation process you can choose how a reservation is to be used for example you can choose for a reservation to be automatically applied to any new or existing instances that match the reservation's properties which is the default behavior or you can specify that reservation to be consumed by a specific instance in all cases a vm instance can only use a reservation if its properties exactly match the properties of the reservation after you create a reservation you begin paying for the reserved resources immediately and they remain available for your project to use indefinitely until the reservation is deleted reservations are great to ensure that your project has resources for future increases in demand including planned or unplanned spikes backup and disaster recovery or for a buffer when you're planning growth when you no longer need a reservation you can simply delete the reservation to stop incurring charges each reservation like normal vms are charged based on existing ondemand rates which include sustained use discounts and are eligible for committed use discounts which i will be getting into in just a bit now purchasing reservations do come with some caveats reservations apply only to compute engine data proc and google kubernetes engine as well reservations don't apply to shared core machine types preemptable vms sole tenant nodes cloud sql and data flow now as i explained before each vcpu and each gigabyte of memory on compute engine is built separately rather than as a part of a single machine type and is billed as individual cpus and memory used per hour resourcebased pricing allows compute engine to apply sustained use discounts to all of your predefined machine type usage in a region collectively rather than to individual machine types and this way vcpu and memory usage for each machine type can receive any one of the following discounts sustained use discounts committed use discounts and preemptable vms and i'd like to take a moment to dive into a bit of detail on each of these discount types starting with sustained use discounts now sustained use discounts are automatic discounts for running specific compute engine resources a significant portion of the billing month for example when you run one of these resources for more than 25 percent of a month compute engine automatically gives you a discount for every incremental minute that you use for that instance now the following tables show the discounts applied for the specific resources described here now for the table on the left for general purpose n2 and n2d predefined and custom machine types and for compute optimized machine types you can receive a discount of up to 20 percent the table on the right shows that for general purpose n1 predefined and custom machine types as well as sole tenant nodes and gpus you can get a discount of up to 30 percent sustained use discounts are applied automatically to usage within a project separately for each region so there is no action required on your part to enable these discounts now some notes that i wanted to cover here is that sustained use discounts automatically apply to vms created by both google kubernetes engine and compute engine as well they do not apply to vms created using the app engine flexible environment as well as data flow and the e2 machine types sustained use discounts are applied on incremental use after you reach certain usage thresholds this means that you pay only for the number of minutes that you use an instance and compute engine automatically gives you the best price google truly believes that there's no reason to run an instance for longer than you need it now sustained use discounts are applied on incremental use after you reach certain usage thresholds this means that you pay only for the number of minutes that you use an instance and compute engine automatically gives you the best price now consider a scenario where you have two instances or sole tenant nodes in the same region that have different machine types and run at different times of the month compute engine breaks down the number of vcpus and amount of memory used across all instances that use predefined machine types and combines the resources to qualify for the largest sustained usage discounts possible now in this example assume you run the following two instances in the us east one region during a month for the first half you run an n1 standard four instance with four vcpus and 15 gigabytes of memory for the second half of the month you run a larger and one standard 16 instance with 16 vcpus and 60 gigabytes of memory in this scenario compute engine reorganizes these machine types into individual vcpu and memory resources and combines their usage to create the following resources for vcpus so because four vcpus were being used for the whole month the discount here would be thirty percent the additional twelve vcpus were added on week two in the month and so for those 12 vcpus they would receive a 10 discount and this is how discounts are applied when it comes to sustained use discounts now moving on to the next discount type is committed use discounts so compute engine lets you purchase committed use contracts in return for deeply discounted prices for vm usage so when you purchase a committed use contract you purchase compute resource which is comprised of vcpus memory gpus and local ssds and you purchase these resources at a discounted price in return for committing to paying for those resources for one year or three years committed use discounts are ideal for workloads with predictable resource needs so if you know exactly what you're going to use committed use discounts would be a great option for this and the discount is up to 57 for most resources like machine types or gpus when it comes to memory optimized machine types the discount is up to 70 percent now when you purchase a committed use contract you can purchase it for a single project and applies to a single project by default or you can purchase multiple contracts which you can share across many projects by enabling shared discounts once purchased your billed monthly for the resources you purchased for the duration of the term you selected whether you use the services or not if you have multiple projects that share the same cloud billing account you can enable committed use discount sharing so that all of your projects within that cloud billing account share all of your committed use discount contracts your sustained use discounts are also pooled at the same time now some caveats when it comes to committed use discounts shared core machines are excluded on this as well you can purchase commitments only on a per region basis if a reservation is attached to a committed use discount the reservation can't be deleted for the duration of the commitment so please be aware now to purchase a commitment for gpus or local ssds you must purchase a general purpose and one commitment and lastly after you create a commitment you cannot cancel it you must pay the agreed upon monthly amount for the duration of the commitment now committed use discount recommendations give you opportunities to optimize your compute costs by analyzing your vm spending trends with and without a committed use discount contract by comparing these numbers you can see how much you can save each month with a committed use contract and this can be found under the recommendations tab on the home page in the console and so i wanted to move on to the last discount type which are preemptable vms now preemptable vms are up to eighty percent cheaper than regular instances pricing is fixed and you never have to worry about variable pricing these prices can be found on the link to instance pricing that i have included in the lesson text a preemptable vm is an instance that you can create and run at a much lower price than normal instances however compute engine might stop or preempt these instances if it requires access to those resources for other tasks as preemptable instances our access compute engine capacity so their availability varies with usage now generally compute engine avoids preempting instances but compute engine does not use an instant cpu usage or other behavior to determine whether or not to preempt it now a crucial characteristic to know about preemptable vms is that compute engine always stops them after they run for 24 hours and this is something to be aware of for the exam preemptable instances are finite compute engine resources so they might not always be available and if you happen to accidentally spin up a preemptable vm and you want to shut it down there is no charge if it's running for less than 10 minutes now another thing to note is that preemptable instances can't live migrate to a regular vm instance or be set to automatically restart when there is a maintenance event due to the limitations preemptable instances are not covered by any service level agreement and when it comes to the google cloud free tier credits for compute engine this does not apply to preemptable instances so you're probably asking when is a great time to use preemptable vms well if your apps are fault tolerant and can withstand possible instance preemptions then preemptable instances can reduce your compute engine costs significantly for example batch processing jobs can run on preemptable instances if some of those instances stop during processing the job slows down but does not completely stop preemptable instances create your batch processing tasks without placing any additional workload on your existing instances and without requiring for you to pay full price for additional normal instances and since containers are naturally stateless and fault tolerant this makes containers an amazing fit for preemptable vms so running preemptable vms for google kubernetes engine is another fantastic use case now it's really critical that you have an understanding for each different discount type and when is a good time to use each as you may be presented different costeffective solutions in the exam and understanding these discount types will prepare you to answer them understanding the theory behind this resourcebased pricing model all the available discount types along with the types of workloads that are good for each will guarantee that you will become familiar with what types of questions are being asked in the exam and will also make you a better cloud engineer as you will be able to spot where you can save money and be able to make the appropriate changes and so that's pretty much all i wanted to cover when it comes to compute engine billing and its discount types so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be covering the fundamentals as it pertains to storage these concepts are needed to know in order to fully understand the different google cloud storage options that i will be diving into later as well the exam expects that you know the different types of storage that's available for all the various services and so before i get into the different types of storage i wanted to cover the underlying theory behind it so with that being said let's dive in so i wanted to start off by going through the three types of storage and how data is presented to a user or to the server there is block storage file storage and object storage these types of storage tie into the available services that are available in google cloud and they offer different options for different types of workloads and i will be going over each of these in a bit of depth and so the first one i wanted to touch on is block storage now block storage is sometimes referred to as block level storage and is a technology that is used to store data files on storage systems or cloudbased storage environments block storage is the fastest available storage type and it is also efficient and reliable with block storage files are split into evenly sized blocks of data each with its own unique identifier it is presented to the operating system as structureless raw data in the form of a logical volume or a hard drive and the operating system structures it with a file system like ext3 or ext4 on linux and ntfs for windows it would then mount this volume or drive as the root volume in linux or a c or d drive in windows block storage is usually delivered on physical media in the case of google cloud it is delivered as either spinning hard drives or solid state drives so in google cloud you're presented with block storage that consists of either persistent disks or local ssd which can both be mountable and bootable block storage volumes can then be used as your boot volumes for compute instances in google cloud installed with your operating system of choice and structured so that your operating system database or application will then be able to consume it now moving on to the second type of storage is file storage now file storage is also referred to as file level or file based storage and is normally storage that is presented to users and applications as a traditional network file system in other words the user or application receives data through directory trees folders and files file storage also allows you to do the same this functions similarly to a local hard drive however a structure has already been applied and cannot be adjusted after the fact this type of structure only has the capabilities of being mountable but not bootable you cannot install an operating system on file storage as i said before the structure has already been put in place for you and is ready for you or your application to consume due to this structure the service that is serving the file system has some underlying software that can handle access rights file sharing file locking and other controls related to file storage in google cloud this service that serves this type of storage is known as cloud file store and is usually presented over the network to users in your vpc network using the nfs protocol or in this case nfs version 3. but i'll be diving into that a little bit later and the last storage type that i wanted to cover is object storage now object storage also referred to as objectbased storage is a general term that refers to the way in which we organize and work with units of storage called objects and this is a storage type that is a flat collection of unstructured data and this type of storage holds no structure like the other two types of storage and is made up of three characteristics the first one is the data itself and this could be anything from movies songs and even photos of men in fancy bow ties the data could also be binary data as well the second characteristic is the metadata and this is usually related to any contextual information about what the data is or anything that is relevant to the data and the third characteristic is a globally unique identifier and this way it's possible to find the data without having to know the physical location of the data and this is what allows object storage to be infinitely scalable as it doesn't matter where the object is stored this type of storage can be found in google cloud and is known as cloud storage cloud storage is flat storage with a logical container called a bucket that you put objects into now although this type of storage is not bootable using an open source tool called fuse this storage type can be mounted in google cloud and i will be covering that a little bit later in the cloud storage lesson but in most cases object store is designed as the type of storage that is not bootable or mountable and because of the characteristics of this storage it allows object storage again to be infinitely scalable and so these are the three main types of storage that you will need to know and understand as each has its use cases so if you're looking for high performance storage you will always look to block storage to satisfy your needs if you're looking to share files across multiple systems or have multiple applications that need access to the same files and directories then file storage might be your best bet if you're looking to store terabytes of pictures for a web application and you don't want to worry about scaling object storage will allow you to read and write an infinite amount of pictures that will meet your requirements so now that we've covered these storage types let's take a few moments to discuss storage performance terms now when discussing storage performance there are some key terms to understand that when used together define the performance of your storage first there is io which stands for input output and is a single read write request and can be measured in block size and this block size can vary anywhere from one kilobyte to four megabytes and beyond depending on your workload now q depth when it comes to storage is the number of pending input output requests waiting to be performed on a disk io requests become queued when reads or writes are requested faster than they can be processed by the disk when io requests are queued the total amount of time it takes to read or write data to disk becomes significantly higher this is where performance degradation can occur and queue depth must be adjusted accordingly now the next term is a common touch point when it comes to discussing storage performance on gcp and on the exam which is iops and this is a metric that stands for input output operations per second this value indicates how many different input or output operations a device or group of devices can perform in one second more value in the iops signifies the capability of executing more operations per second and again this is a common touch point that i will be diving into a little bit later now next up is throughput and this is the speed at which the data is transferred in a second and is most commonly measured in megabytes per second this is going to be another common topic that comes up frequently when discussing storage on gcp as well latency is the measurement of delay between the time data is requested when the data starts being returned and is measured in milliseconds so the time each io request will take to complete results in being your average latency and the last two terms i wanted to bring up is sequential and random access sequential would be a large single file like a video and random access would be loading an application or an operating system so lots of little files that are all over the place it's obvious that accessing data randomly is much slower and less efficient than accessing it sequentially and this can also affect performance now why i bring up all these terms is not about calculating the average throughput but to give you a holistic view on storage performance as all these characteristics play a part in defining the performance of your storage there is not one specific characteristic that is responsible for disk performance but all have a role in achieving the highest performance possible for your selected storage now i know this is a lot of theory to take in but this will all start to make more sense when we dive into other parts of the course where we will discuss disk performance with all these characteristics as it relates to compute engine and other services that use storage it is crucial to know the storage types as well as the performance characteristics as it will bring clarity to questions in the exam and also give you a better sense on how to increase your storage performance in your work environment and so that's pretty much all i wanted to cover when it comes to storage types and storage performance as it pertains to storage as a whole so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be covering persistent disks and local ssds i'm going to be getting into the detail with the most commonly used storage types for instances which are both persistent disks and local ssds this lesson will sift through all the different types of persistent disks and local ssds along with the performance of each knowing what type of disk to use for your instance and how to increase disk performance shows up on the exam and so i want to make sure to cover it in detail and leave no stone unturned so with that being said let's dive in now persistent disks and local ssds are the two available types of block storage devices available in google cloud and the determining factor of what you will use for your particular scenario will depend on your use case and the specific characteristics that you require from each storage medium now by default each compute engine instance has a single boot persistent disk that contains the operating system when you require additional storage space you can add one or more additional persistent disks or local ssds to your instance and i will be going through these storage options along with their characteristics now as you can see here persistent disks and local ssds come in a slew of different types as well with persistent disks they are available in both zonal and regional options so starting off with persistent disks you have three different types you can choose from as well you have the flexibility of choosing from two different geographic options when it comes to the redundancy of your persistent disks and i will be covering the zonal and regional options in detail in just a bit now persistent disks are durable network storage devices that your instances can access like physical disks in a computer so these are not physically attached disks but network disks that are connected over google's internal network persistent disks are independent of your instance and can persist after your instance has been terminated and this can be done by turning on this flag upon creation you can even detach your disk and move it to other instances when you need to scaling persistent disks can be done automatically and on the fly by using the disk resize feature and this gives you the flexibility to resize your current persistent disks with no downtime and even add additional disks to your instance for additional performance and storage persistent disks are also encrypted by default and google also gives you the option of using your own custom keys each persistent disk can be up to 64 terabytes in size and most instances can have up to 128 persistent disks and up to 257 terabytes of total persistent disk space attached and just as a note share core machine types are limited to 16 persistent disks and 3 terabytes of total persistent disk space and so now that i've gone through the details of persistent disks i wanted to dive into the two geographic options that's available for persistent disks first starting with zonal now zonal persistent disks are disks that are available in one zone in one region these disks are the most commonly used persistent disks for general daytoday usage and used for those whose workloads are not sensitive to specific zone outages they are redundant within the zone you've created them in but cannot survive an outage of that zone and may be subjected to data loss if that specific zone is affected and this is where snapshots should be a part of your high availability strategy when using zonal persistent disks snapshots are incremental and can be taken even if you snapshot disks that are attached to running instances and i'll be going into detail about snapshots in a later lesson zonal persistent disks can also be used with any machine type including predefined shared core and custom machine types now when it comes to regional persistent disks they have storage qualities that are similar to zonal persistent disks however regional persistent disks provide durable storage and replication of data between two zones in the same region if you are designing systems that require high availability on compute engine you should use regional persistent disks combined with snapshots for durability regional persistent disks are also designed to work with regional managed instance groups in the unlikely event of a zonal outage you can usually fail over your workload running on regional persistent disks to another zone by simply using the force attached flag regional persistent disks are slower than zonal persistent disks and should be taken into consideration when write performance is less critical than data redundancy across multiple zones now noting a couple of caveats here when it comes to disk limits regional persistent disks are similar to zonal persistent disks however regional standard persistent disks have a 200 gigabyte size minimum and may be a major factor when it comes to cost so please be aware as well you can't use regional persistent disks with memory optimized machine types or compute optimized machine types now these two geographic options are available for all three persistent disk types whose characteristics i will dive into now starting off with the standard persistent disk type also known in google cloud as pd standard now these persistent disks are backed by standard hard disk drives and these are your standard spinning hard disk drives and allows google cloud to give a cost effective solution for your specific needs standard persistent disks are great for large data processing workloads that primarily use sequential ios now as explained earlier sequential access would be accessing larger files and would require less work by the hard drive thus decreasing latency as there are physical moving parts in this hard drive this would allow the disc to do the least amount of work as possible and therefore making it the most efficient as possible and therefore sequential ios are best suited for this type of persistent disk and again this is the lowest price persistent disks out of all the persistent disk types now stepping into the performance of standard persistent disks for just a second please remember that iops and throughput performance depends on disk size instance vcpu count and i o block size among other factors and so this table here along with the subsequent tables you will see later are average speeds that google has deemed optimum for these specific disk types they cover the maximum sustained iops as well as the maximum sustained throughput along with the granular breakdown of each here you can see the differences between both the zonal and regional standard pd and as you can see here in the table the zonal standard pd and the regional standard pd are pretty much the same when it comes to most of these metrics but when you look closely at the read iops per instance this is where they differ where the zonal standard pd has a higher read iops per instance than the regional standard pd and this is because the regional standard pd is accessing two different disks in two separate zones and so the latency will be higher the same thing goes for right throughput per instance and so this would be a decision between high availability versus speed moving on to the next type of persistent disk is the balanced persistent disk in google cloud known as pd balance this disk type is the alternative to the ssd persistent disks that balance both performance and cost as this disk type has the same maximum iops as the ssd persistent disk type but holds a lower iops per gigabyte and so this disk is designed for general purpose use the price for this disk also falls in between the standard and the ssd persistent disks so this is basically your middle of the road disk when you're trying to decide between price and speed moving straight into performance i put the standard pd metric here so that you can see a sidebyside comparison between the balance pd and the standard pd and as you can see here when it comes to the metrics under the maximum sustained iops the balance pd is significantly higher than the standard pd in both the zonal and regional options as well looking at the maximum sustained throughput the read write throughput per gigabyte is a little over two times faster and the right throughput per instance is three times faster so quite a bit of jump from the standard pd to the balance pd and moving on to the last persistent disk type is the ssd persistent disk type also known in google cloud as a pd ssd and these are the fastest persistent disks that are available and are great for enterprise applications and high performance databases that demand lower latency and more iops so this would be great for transactional databases or applications that require demanding and near realtime performance the pd ssds have a single digit millisecond latency and because of this comes at a higher cost and therefore is the highest price persistent disk moving on to the performance of this persistent disk this disk type is five times faster when it comes to read iops per gigabyte than the balance pd as well as five times faster for the right iops per gigabyte and so the table here on the left shows the performance for the pd ssd and the table on the right shows the performance of both the standard pd and the balance pd and so here you can see the difference moving from the standard pd over to the ssd pd the read write throughput per instance stays the same from the standard pd all the way up to the ssd pd but where the ssd outperforms all the other ones is through the read write throughput per gigabyte it's one and a half times faster than the balance pd and four times faster than the standard pd and again you will also notice a drop in performance from the zonal option to the regional option and so this is the end of part one of this lesson as it started to get a little bit long and so whenever you're ready you can join me in part two where i will be starting immediately from the end of part one so you can complete this video and i will see you in the next welcome back this is part two of the persistent disks and local ssds lesson and we will be starting exactly where we left off in part one so with that being said let's dive in and so now that i've covered all the persistent disk types i wanted to move into discussing the characteristics of the local ssd local ssds are physically attached to the server that hosts your vm instance local ssds have higher throughput and lower latency than any of the available persistent disk options and again this is because it's physically attached and the data doesn't have to travel over the network now the crucial thing to know about local ssds is that the data you store on a local ssd persists only until the instance is stopped or deleted once the instance is stopped or deleted your data will be gone and there is no chance of getting it back now each local ssd is 375 gigabytes in size but you can attach a maximum of 24 local ssd partitions for a total of 9 terabytes per instance local ssds are designed to offer very high iops and very low latency and this is great for when you need a fast scratch disk or a cache and you don't want to use instance memory local ssds are also available in two flavors scuzzy and mvme now for those of you who are unaware scuzzy is an older protocol and made specifically for hard drives it also holds the limitation of having one queue for commands nvme on the other hand also known as nonvolatile memory express is a newer protocol and is designed for the specific use of flash memory and designed to have up to 64 000 qs as well each of those queues in turn can have up to 64 000 commands running at the same time and thus making nvme infinitely faster now although nvme comes with these incredible speeds it does come at a cost and so when it comes to the caveats of local ssd although compute engine automatically encrypts your data when it's written to local ssd storage space you can't use customer supplied encryption keys with local ssds as well local ssds are only available for the n1 n2 and compute optimized machine types now moving on to the performance of local ssds throughput is the same between scuzzy and nvme but the read write iops per instance is where nvme comes out on top and as you can see here the read iops per instance is a whopping two million four hundred thousand read iops per instance as well the right iops per instance is 1.2 million over the 800 000 for local ssd now before i end this lesson i wanted to cover a few points on performance scaling as it pertains to block storage on compute engine now persistent disk performance scales with the size of the disk and with the number of vcpus on your vm instance persistent disk performance scales linearly until it reaches either the limits of the volume or the limits of each compute engine instance whichever is lower now this may seem odd that the performance of your disk scales with cpu count but you have to remember persistent disks aren't physically attached to your vm they are independently located as such i o on a pd is a network operation and thus it takes cpu to do i o which means that smaller instances run out of cpu to perform disk io at higher rates so in order for you to get better performance you can increase the iops for your disk by resizing them to their maximum capacity but once that size has been reached you will have to increase the number of cpus on your instance in order to increase your disk performance a recommendation by google is that you have one available vcpu for every 2000 to iops of expected traffic so to sum it up performance scales until it reaches either the limits of the disk or the limits of the vm instance to which the disk is attached the vm instance limits are determined by the machine type and the number of vcpus of the instance now if you want to get more granular with regards to disk performance i've included a few links in the lesson text that will give you some more insight but for most general purposes and for the exam remember that persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vcpus that the instance has and so that's pretty much all i wanted to cover when it comes to persistent disks and local ssds so you can now mark this lesson as complete and let's move on to the next one welcome back in this demo i'm going to be covering how to manage and interact with your disks on compute engine this demo is designed to give you both experience and understanding on working with persistent disks and how you would interact with them we're going to start the demo off by creating an instance we're then going to create a separate persistent disk and attach it to the instance we're going to then interact with the disk and then resize the disk while afterwards we will delete it and we're going to do this all by both using the console and the command line so with that being said let's dive in so here i am in the console i'm logged in as tony bowties gmail.com and i am in project bowtie inc and so the first thing we need to do to kick off this demo is to create an instance that we can attach our disk to but first i always like to make sure that i have a vpc to deploy my instance into with its corresponding default firewall rules so i'm going to head on over to the navigation menu and i'm going to go down to vpc network and as expected my default vpc has been created and just to make sure that i have all my necessary firewall rules i'm going to drill down into the vpc and head on over to firewall rules i'm going to click on firewall rules and the necessary firewall rule that i need for ssh is created and so i can go ahead and create my instance so i'm going to go back up to the navigation menu and i'm going to go over to compute engine so i'm going to go ahead and click on create and i'm going to name this instance bowtie dash instance and for the sake of this demo i'll add in a label here the key is going to be environment and the value will be testing i'm going to go down to the bottom click on save with regards to the region i'm going to select us east 1 and i'm going to keep the zone as the default for us east 1b and under machine type to keep things cost effective i'm going to use an e2 micro shared core machine and i'm going to scroll down to service account and under service account you want to select the set access for each api you want to scroll down to compute engine and here you want to select read write and this will give us the necessary permissions in order to interact with our disk that we will be creating later so i'm going to scroll down to the bottom here and i'm going to leave everything else set at its default and just before creating the instance please do remember you can always click on the command line link where you can get the gcloud command to create this instance through the command line i'm going to close this up and i'm going to simply click on create i'm just going to wait a few seconds here for my instance to come up okay and my instance is up and so now what we want to do is we want to create our new disk so i'm going to go over here to the left hand menu and i'm going to click on disks and as you can see here the disk for the instance that i had just created has 10 gigabytes in us east 1b and we want to leave that alone and we want to create our new disk so i'm going to go up to the top here and simply click on create disk and so for the name of the disk i'm going to call this disk new pd for persistent disk and i'm going to give it the same description i'm going to keep the type as standard persistent disk and for the region i want to select us east one i'm going to keep the zone as its default in us east 1b and as the disk is in us east 1b i'll be able to attach it to my instance and so just as a note here there is a selection where you can replicate this disk within the region if i click that off i've now changed this from a zonal persistent disk to a regional persistent disk and over here in zones it'll give me the option to select any two zones that i prefer and so if you're looking at creating some regional persistent disks these are the steps you would need to take in order to get it done in the console now in order to save on costs i'm going to keep this as a zonal persistent disk so i'm going to click on cancel i'm going to uncheck the option and make sure your region is still set at us east 1 and your zone is selected as us east 1b we're going to leave the snapshot schedule alone and i'll be diving into snapshot schedules in a later lesson i'm going to scroll down here to source type i'm going to keep it as blank disk and the size here is set at 500 gigabytes and we want to set it to 100 gigabytes but before we do that i wanted to bring your attention to the estimated performance here you can see the sustain random iops limits as well as the throughput limit and so depending on the size of the disk that you want to add these limits will change accordingly so if i change this to 100 my sustained random iops limit on read went from 375 iops to 75 iops and so this is a great demonstration that the larger your disc the better your performance and so this is a great way to figure out on what your performance will be before you create your disk and i've also been prompted with a note here saying that because my disk is under 200 gigabytes that i will have reduced performance and so for this demo that's okay i'm going to keep my encryption as the google manage key and under labels i will add environment as the key and value is testing and so now that i've entered all my options i'm going to simply click on create and i'm going to give it a few seconds and my new disk should be created okay and my new disk has been created and you can easily create this disk through the command line and i will be supplying that in the lesson text i merely want to go through the console setup so that you are aware of all the different options and so now that i've created my disk and i've created my instance i want to now log into my instance and attach this new disk so i'm going to go back to vm instances and here i want to ssh into the bowtie instance and i'm going to give it a few seconds here to connect and i'm going to zoom in for better viewing i'm going to clear my screen and so the first thing i want to do is i want to list all my block devices that are available to me on this instance and the linux command for that is ls blk and as you can see my boot disk has been mounted and is available to me and so now i want to attach the new disk that we just created and just as a note i could as easily have done this in the console but i wanted to give you an idea of what it would look like doing it from the command line and so i'm going to paste in the command to attach the disk which is gcloud compute instances attach dash disk the name of the instance which is bow tie dash instance along with the flag dash dash disk the disk name which is new pd and the zone of the disk using the zone flag with us east 1b so i'm going to go ahead and hit enter and no errors came up so i'm assuming that this had worked and so just to double check i'm gonna run the lsblk command again and success as you can see here my block device sdb has been attached to my instance and is available to me with the size of 100 gigabytes and so now i want to look at the state that this roblox device is in and so the command for that will be sudo file dash s followed by the path of the block device which is forward slash dev forward slash sdb i'm going to hit on enter and as you can see it is showing data which means that it is just a raw data device and so in order for me to interact with it i need to format the drive with a file system that the operating system will be able to interact with and so the command to format the drive would be sudo mkfs which is make file system i'm going to use ext4 as the file system minus capital f along with the path of the new disk so i'm going to hit on enter and no errors so i'm assuming that it was successful so just to verify i'm going to run the sudo file minus s command and as you can see here because the disk now has a file system i've been given the information with regards to this disk whereas before it was simply raw data and so now that we've created our disk and we've formatted our disk to a file system that the operating system is able to read we need to now mount the disk and so in order to do that we need to create a mount point so i'm going to first clear the screen and i'm going to run the command sudo mkdir and the new mount point i'm going to call it slash new pd i'm going to hit enter and now i'm going to mount the disk and the command for that is sudo mount the path for the block device which is forward slash dev forward slash sdb and then the mount point which is forward slash new pd i'm going to hit enter no errors so i'm assuming that it had worked but just to verify i'm going to run the command lsblk and success as you can see sdb has now been mounted as new pd and so now i can interact with this disk so the first thing i want to do is i want to change directories to this mount point i'm in now new pd i'm going to do an ls and so just as a note for those of you who are wondering the lost and found directory is found on each linux file system and this is designed to place orphaned or corrupted files or any corrupted bits of data from the file system to be placed here and so it's not something that you would interact with but always a good to know so i'm going to now create a file in new pd so i'm going to run the command sudo nano file a bow ties dot text so file a bow ties is the file that i'm going to create nano is my text editor and so i'm going to hit on enter and so in this file i'm going to type in bow ties are so classy because after all they are i'm going to hit ctrl o to save i'm going to hit enter to verify it and ctrl x to exit so if i do another ls i can see the file of bow ties has been created also by running the command df minus k i'll be able to see the file system here as well and so this is the end of part one of this demo it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up have a stretch get yourself a coffee or tea and whenever you're ready you can join me in the next one where part two will be starting immediately from the end of part one welcome back this is part two of this demo and we're gonna continue immediately from the end of part one so with that being said let's dive in and so what i want to do now is i want to reboot the instance in order to demonstrate the mounting of this device and i'm going to do that by using the command sudo reboot it's going to disconnect me i'm going to click on close and i'm going to wait about a minute for it to reboot okay and it's been about a minute so i'm going to now ssh into my instance okay and here i am back again logged into my instance i'm going to quickly clear the screen and i'm going to run the lsblk command now what i wanted to demonstrate here is that although i mounted the new device it did not stay mounted through the reboot and this is because there is a configuration file in linux that points to which partitions get mounted automatically upon startup that i need to edit in order to make sure that this device is mounted every time the instance reboots and so in order to do that i need to edit a file called fstab and i'm going to have to add the unique identifier for this partition also known as the device sdb and this will mount the partition automatically every time there happens to be a reboot so in order to do that i'm going to run the command sudo blk id and the path of the block device forward slash dev forward slash sdb i'm going to hit on enter and here is the identifier also known as the uuid that i need to append to the fstab file so i'm going to copy the uuid and i'm going to use the command sudo nano etc fs tab and i'm going to hit on enter and here you will find the uuid for your other partitions and so you're going to be appending a line here right at the end so i'm going to move my cursor down here i'm going to type in uuid equals and then the uuid that i had copied earlier the amount point which is going to be forward slash new pd the type of file system which is ext4 along with defaults comma no fail i'm going to hit control o to save hit enter to verify and control x to exit and so now i'm going to mount this device by running the command sudo mount dash a and hit enter and this command will mount all the partitions that are available in the fstab file and so when i run a lsblk i can see here that my block device sdb is now mounted on forward slash new pd now i know this may be a refresher for some but this is a perfect demonstration of the tasks that need to be done when creating and attaching a new disk to an instance and is a common task for many working on linux instances and working in cloud this can definitely be scripted but i wanted to show you the steps that need to be taken in order to get a new disk in a usable state okay so great we have created a new disk we had attached the disk created a file system and had mounted the disk along with editing the configuration file to make sure that the device mounts whenever the instance starts up so now that we've done all that i wanted to demonstrate resizing this disk from 100 gigabytes to 150 gigabytes and so just to show you where it is in the console i'm going to quickly go back to my console tab and so here i'm going to go to the left hand menu i'm going to click on disks i'm going to drill down into new pd and at the top i'm going to click on edit and so here i'm able to adjust the disk space size and simply click on save not much that i really need to do here but i did want to show you how to do this in the command line so i'm going to go back to the tab of my instance and i'm going to quickly clear the screen and i'm going to paste in the command gcloud compute disks resize the name of the disk which is new pd and the new size in gigabytes using the dash dash size flag 150 which is the new size of the disc along with the dash dash zone flag of us east 1b i'm going to hit enter it's going to ask me if i want to do this as this is not reversible and please remember when you resize a disk you can only make it bigger and never smaller so i'm going to hit y to continue and it took a few seconds there but it was successful so if i run a df minus k you can see here that i only have 100 gigabytes available to me and this is because i have to extend the file system on the disk so i've made the disk larger but i haven't allocated those raw blocks to the file system so in order for the file system to see those unallocated blocks that's available to it i need to run another command so i'm going to quickly clear my screen again and i'm going to run the command sudo resize to fs along with the block device i'm going to hit enter and as you can see it was successful showing the old blocks as 13 and the new blocks as 19. so if i run a df minus k i can now see my 150 gigabytes that's available to me and so just to demonstrate after resizing the disk along with mounting and then remounting the disk that the file that i've created still exists i'm going to run an ls minus al but first i will need to change directories into new pd clear my screen and run an ls and phyla bow ties is still there and so this is a great example demonstrating how the data on persistent disks persist through the lifetime of a disk even when mounting unmounting rebooting and resizing and so as you can see we've done a lot of work here and so just as a recap where we've created a new disk we attached this disk to an instance we formatted the disk into an ext4 file system we've mounted this disk we've written a file to it added its unique identifier to the configuration file so that it mounts on startup and then we've resized the disk along with extending the file system on the disk and so this is the end of the demo and i wanted to congratulate you on making it to the end and i hope this demo has been extremely useful and again fantastic job on your part now before you go i wanted to quickly walk through the steps of deleting all the resources you've created and so the first thing that i want to do is delete the disk that was created for this demo and so before i can delete the disk i'm going to first detach the disk from the instance and the easiest way to do that is through the command line so i'm going to quickly clear my screen and so i'm going to show you how to detach the disk from the instance and so i'm going to paste in this command gcloud compute instances detach disk the instance name which is bow tie dash instance along with the disc with the flag dash dash disc the name of the disc which is new pd along with the zone i'm going to hit enter and it's been successfully detached and so now that it's detached i can actually delete the disk and so i'm going to head on over back to the console and i'm going to go ahead and delete the new pd disk i'm going to click on delete i'm going to get a prompt asking me if i'm sure yes i am if i go back to the main menu for my disks and this should just take a moment and once it's deleted you will no longer see it here and i'm going to go back over to vm instances and i'm going to delete this as well and so there's no need to delete your default vpc unless you'd like to recreate it again but don't worry for those who decide to keep it you will not be charged for your vpc as we will be using it in the next demo and so that's pretty much all i wanted to cover when it comes to managing disks with compute engine so you can now mark this as complete and let's move on to the next one welcome back in this lesson i'll be discussing persistent disk snapshots now snapshots are a great way to backup data from any running or stopped instances from unexpected data loss snapshots are also a great strategy for use in a backup plan for any and all instances no matter where they are located and so as cloud engineers and architects this is a great tool for achieving the greatest uptime for your instances so diving right into it snapshots as i mentioned before are a great way for both backing up and restoring the data of your persistent disks you can create snapshots from disks even while they are attached to running instances snapshots are global resources so any snapshot is accessible by any resource within the same project you can also share snapshots across projects as well snapshots also support both zonal and regional persistent disks snapshots are incremental and automatically compressed so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of a disk now when you create a snapshot you have the option of choosing a storage location snapshots are stored in cloud storage and can be stored in either a multiregional location or a regional cloud storage bucket a multiregional storage location provides higher availability but will drive up costs please be aware that the location of a snapshot affects its availability and can incur networking costs when creating the snapshot or restoring it to a new disk if you do not specify storage location for a snapshot google cloud uses the default location which stores your snapshot in a cloud storage multiregional location closest to the region of your source disk if you store your snapshot in the same region as your source disk there is no network charge when you access that snapshot from the same region if you access the snapshot from a different region you will incur a network cost compute engine stores multiple copies of each snapshot across multiple locations as well you cannot change the storage location of an existing snapshot once a snapshot has been taken it can be used to create a new disk in any region and zone regardless of the storage location of the snapshot now as i explained earlier snapshots are incremental and i wanted to take a moment to dive into that for just a minute so when creating snapshots the first successful snapshot of a persistent disk is a full snapshot that contains all the data on the persistent disk the second snapshot only contains any new data or modify data since the first snapshot data that hasn't changed since snapshot 1 isn't included instead snapshot 2 contains references to snapshot 1 for any unchanged data as shown here snapshot 3 contains any new or changed data since snapshot 2 but won't contain any unchanged data from snapshot 1 or 2. instead snapshot 3 contains references to blocks in snapshot 1 and snapshot 2 for any unchanged data this repeats for all subsequent snapshots of the persistent disk snapshots are always created based on the last successful snapshot taken and so now you're probably wondering what happens when you decide to delete a snapshot are they dependent on each other well when you delete a snapshot compute engine immediately marks the snapshot as deleted in the system if the snapshot has no dependent snapshots it is deleted outright however if the snapshot does have dependent snapshots then there are some steps that happen behind the scenes so shown here in this diagram snapshot 2 is deleted the next snapshot from the full snapshot no longer references the snapshot for deletion in this example snapshot 1 then becomes the reference for snapshot 3 and any data that is required for restoring other snapshots is moved into the next snapshot increasing its size shown here blocks that were unique to snapshot 2 are moved to snapshot 3 and the size of snapshot 3 increases any data that is not required for restoring other snapshots is deleted so in this case blocks that are already in snapshot 3 are deleted from snapshot 2 and the size of all snapshots are lower now because subsequent snapshots might require information stored in a previous snapshot please be aware that deleting a snapshot does not necessarily delete all the data on the snapshot if you're looking to make sure that your data has indeed been deleted from your snapshots you should delete all snapshots if your disk has a snapshot schedule you must detach the snapshot schedule from the disk before you can delete the schedule removing the snapshot schedule from the disk prevents further snapshot activity from occurring now touching on the topic of scheduled snapshots by far the best way to backup your data on compute engine is to use scheduled snapshots this way you will never have to worry about manually creating snapshots or even worry about using other tools to kick off those snapshots you can simply use this builtin tool by google which is why snapshot schedules are considered best practice to backup any compute engine persistent disks now in order to create any snapshot schedules you must create your snapshot schedule in the same region where your persistent disk resides now there are two ways to create a snapshot schedule the first one is to create a snapshot schedule and then attach it to an existing persistent disk the other way is to create a new persistent disk with a snapshot schedule you also have the option of setting up a snapshot retention policy that defines how long you want to keep your snapshots some options when creating snapshot schedules are both retention policies and source disk deletion rules now if you choose to set up a snapshot retention policy you must do it as part of your snapshot schedule when you create a snapshot schedule is when you can also set a source disk deletion rule the source disk deletion rule controls what happens to your snapshots if the source disk is deleted now a few caveats here on the scheduled snapshots is that a persistent disk can only have one snapshot schedule attached to it at a time also you cannot delete a snapshot schedule if it is attached to a disk you must detach the schedule from all disks then delete the schedule as well after you create a snapshot schedule you cannot edit it to update a snapshot schedule you must delete it and create a new one now before i end this lesson i wanted to touch on managing snapshots for just a minute so when managing snapshots there's a few things to remember in order to use snapshots to manage your data efficiently you can snapshot your disks at most once every 10 minutes you are unable to snapshot your disks at intervals less than 10 minutes so please keep that in mind when creating your schedules also you should create snapshots on a regular schedule to minimize data loss if there was an unexpected failure if you have existing snapshots of a persistent disk the system automatically uses them as a baseline for any subsequent snapshots that you create from that same disk so in order to improve performance you can eliminate excessive snapshots by creating an image and reusing it using this method would not only be ideal for storage and management of snapshots but also help to reduce costs and if you schedule regular snapshots for your persistent disks you can reduce the time that it takes to complete each snapshot by creating them during offpeak hours when possible and lastly for those of you who use windows for most situations you can use the volume shadow copy service to take snapshots of persistent disks that are attached to windows instances you can create vss snapshots without having to stop the instance or detach the persistent disk and so that's pretty much all i wanted to cover when it comes to the theory of persistent disk snapshots their schedules and how to manage them in the next lesson i'll be doing a handson demo demonstrating snapshots and putting this theory into practice and get a feel for how snapshots work and how they can be applied to persistent disks so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this demonstration we're going to dive into snapshots and snapshot schedules this demo will give you the handson knowledge you need to create and delete snapshots along with how to manage snapshot schedules we're going to start the demo off by creating an instance we're going to interact with it and then take a snapshot of the disk we're going to then create another instance from the snapshot and then create some snapshot schedules for both of these instances by using both the console and the command line so there's a lot to do here so with that being said let's dive in and so i'm currently logged in as tony bowties gmail.com as well i'm in project bowtie inc so the first thing that we need to do to kick off this demo is to create an instance but first as always i like to make sure that i have a vpc to deploy my instance into with its corresponding default firewall rules and so i'm going to head on over to the navigation menu and scroll down to vpc network and because i didn't delete my default vpc from the last demo i still have it here i'm just going to drill down and make sure that i have my firewall rules i'm gonna go over to firewall rules and as expected the ssh firewall rule that i need has already been created and so now that i have everything in order i'm gonna go back over to the navigation menu and head on over to compute engine to create my instance now i figure for this demo i'd switch it up a little bit and create the instance by the command line so i'm going to head on over to cloud shell i'm going to open that up and it took a minute to provision and so what i'm going to do now is i'm going to open it up in a new tab i'm going to zoom in for better viewing and i'm going to paste in my command to create my instance and this gcloud command to create these instances will be available in the github repository and you will find all the instructions and the commands under managing snapshots in compute engine so i'm going to hit enter and you may get a prompt to authorize this api call and i'm going to click on authorize and success our instance has been created and is up and running and so now what i want to do is ssh into the instance and so i'm just going to run the command from here which is gcloud compute ssh dash dash zone the zone that i'm in which is used 1b and the instance which is bowtie dash instance i'm going to hit enter it's going to prompt me if i want to continue i'm going to say yes and i'm going to enter my passphrase and enter it again it's going to update my metadata and it's going to ask me again for my passphrase and i'm in so i'm going to just quickly clear my screen and so the first thing i want to do is i want to verify the name of my instance so i'm going to type in the command hostname and as expected bowtie dash instance shows up and so now i want to create a text file and so i'm going to run the command sudo nano file a text i'm going to hit enter and it's going to open up my nano text editor and you can enter a message of any kind that you'd like for me i'm going to enter more bow tie needed because you can never get enough bow ties i'm going to hit ctrl o to save press enter to verify the file name to write and then ctrl x to exit i'm going to run the command ls space minus al to list my files so i can verify that my file has been created and as you can see here file a bowties.txt has been created and so now that i've created my instance and i've written a file to disk i'm going to now head on over to the console and take a snapshot of this disk and because my session was transferred to another tab i can now close the terminal and you want to head over to the lefthand menu and go to disks and so now i want to show you two ways on how you can create this snapshot the first one is going to disks and choosing the disk that you want for me it's bowtie instance and under actions i'm going to click on the hamburger menu and here i can create snapshot and this will bring me straight to my snapshot menu but for this demo i'm going to go over to the left hand menu and i'm going to click on snapshots and here i'm going to click on create snapshot and so for the name of the snapshot i'm going to type in bowtie snapshot and i'm going to use the same for the description moving down on the source disk the only one that i can select is bow tie instance and that's the one that i want anyways so i'm going to click on that the location in order to cut down on costs we don't need multiregional we're going to just select regional and if you select on the location i'm able to select any other locations like tokyo and i can create my snapshot in tokyo but i want to keep my snapshot in the same region so i'm going to go back and select us east one where it is based on the source disk location and i'm going to add a label here with the key environment and the value of testing i'm going to leave my encryption type as google managed and i'm going to simply click on create and this will create a snapshot of the boot disk on bow tie instance and that took about a minute there and so just as a note if you have any bigger discs they will take a little bit longer to snapshot okay and now that i've created my snapshot i'm going to go back up to vm instances and i'm going to create a new instance from that snapshot and so i'm going to name this instance bowtie dash instance dash 2 and i'm going to give this a label i'm going to add a label here the key of environment and the value of testing and hit save the region is going to be used 1 and you can leave the zone as its default as us east 1b and under machine type you can select the e2 micro and you want to go down to boot disk and select the change button and here i'm going to select snapshots instead of using a public image so i'm going to click on snapshots and if i select the snapshot drop down menu i will see here my bowtie snapshot so i'm going to select this i'm going to leave the rest as default and i'm going to go down to select and i'm going to leave everything else as its default and i'm going to click on create i'm going to just give it a minute here so bowtie instance 2 can be created okay and it took a minute there so now i'm going to ssh into this instance and i'm going to zoom in for better viewing and even though i know the instance is named bowtie.instance2 i'm still going to run the hostname command and as expected the same name pops up but what i was really curious about is if i run the command ls space dash al i can see here my file of file of bowties.text and if i cat the file i'll be able to see the text that i inputted into that file and so although it was only one file and a text file at that i was able to verify that my snapshot had worked as there will be times where your snapshot can get corrupted and so doing some various spot checks on your snapshots is some good common practice and so now i want to create a snapshot schedule for both of these instances and so i'm going to go back to the console and on the left hand menu i'm going to head down to snapshots and if i go over to snapshot schedules you can see that i have no snapshot schedules so let's go ahead and create a new one by clicking on create snapshot schedule and so as mentioned in the last lesson we need to create this schedule first before we can attach it to a disk and so i'm going to name this snapshot schedule as bow tie dash disk schedule i'm going to use the same for the description the region i'm going to select it as us east one and i'm going to keep the snapshot location as regional under us east one you scroll down here and under schedule options you can leave the schedule frequency as daily and just as a note for start time this time is measured in utc so please remember this when you're creating your schedule in your specific time zone and so i'm going to put the start time as o 600 and this will be 1 am eastern standard time as backups are always best done when there is the least amount of activity and i'm going to keep the auto delete snapshots after 14 days i'm going to keep the deletion rule as keep snapshots as well i can enable the volume shadow copy service for windows but since we're running linux i don't need to enable this and since we labeled everything else i might as well give this a label i'm going to use the key as environment and the value of testing and once you've filled everything out then you can simply click on create and it took a minute there but the schedule was created and so now that i have my snapshot schedule i need to attach it to a disk so i'm going to head on over to the left hand menu and click on disks and here i'm going to drill down into bow tie instance i'm going to go up to the top and click on edit and under snapshot schedule i'm going to click on the drop down and here i will find bow tie disk schedule i'm going to select that i'm going to click on save and so now that i have my snapshot schedule attached to my disk for the bowtie instance instance i now want to create a snapshot schedule for my other instance and so instead of using the console i'm going to go ahead and do it through the command line so i'm going to go up to the top to my open shell and i'm going to quickly clear the screen and so in order to create my schedule i'm going to run this command gcloud compute resource policies create snapshot schedule the name of the snapshot schedule which is bow tie disk schedule 2 the region the maximum retention days the retention policy and the schedule followed by the storage location and like i said before these commands you will find in the github repository so i'm going to go ahead and hit enter and so i wanted to leave this error in here to show you that i needed the proper permissions in order to create this snapshot schedule a great reminder to always check if you have the right role for the task at hand and so i have two options i can either change users from my service account user to tony bowtie or i can simply head on over to my instance and edit the service account permissions and so the easiest way to do it would be to just switch users and so i'm going to go ahead and do that so i'm going to go ahead and run the command gcloud auth login and remember that this is something that you don't have to do i merely wanted to show you that you require the proper permissions on creation of specific resources okay and i quickly went through the authentication process i'm gonna just clear my screen and i'm going to go ahead and run the command again and as expected the snapshot schedule was created with no errors and so now that my schedule has been created i can now attach it to the disk so i'm going to run the command gcloud compute disks add resource policies the instance name which is bowtie instance 2 and the resource policy which is the snapshot schedule named as bowtie disk schedule 2 in the zone of us east 1b i'm going to hit enter and success and so just to verify that the snapshot schedule has been attached to my disk i'm going to go back to the console i'm going to head back on over to the main page of disks i'm going to drill down into bow tie instance 2 and here it is the snapshot schedule has been attached and so i want to congratulate you on making it to the end of this demo and i hope this demo has been useful as snapshots in the role of an engineer is a common task that can save you from any data loss once set into place and so just as a recap you've created an instance you created a file on that instance and then you've created a snapshot of the disk of that instance and used it to create another instance you then verified the snapshot and then created a snapshot schedule for both boot disks of the instances using the console and the command line well done on another great job now before you go i wanted to take a moment to clean up any resources we've used so we don't accumulate any costs and so the first thing we want to do is we want to detach the snapshot schedules from the disks and so since we're in bow tie instance 2 i'm going to go ahead and click on edit under snapshot schedule i'm going to select the no schedule hit save and i'm going to do the same thing with my other disk now i'm going to head back on over to snapshots i'm going to delete this snapshot and i'm going to head back on over to snapshot schedules i'm going to select all the snapshot schedules and i'm going to click on delete and now that everything's cleaned up with regards to snapshots and snapshot schedules i can now go over to vm instances and delete the instances i'm going to select them all and simply click on delete and so that's pretty much all i wanted to cover in this demo when it comes to snapshots and snapshot schedules so you can now mark this as complete and let's move on to the next one welcome back in this lesson we're going to switch gears and take an automated approach to deployment by diving into google's tool for infrastructure as code called deployment manager now deployment manager allows you to deploy update and tear down resources from within google cloud using yaml jinja and python code templates it allows you to automate the deployment of all the resources that are available in google cloud and deploy it in a fast easy and repeatable way for consistency and efficiency in this lesson we're going to explore the architecture of deployment manager and dive into all the different components that gives it its flexibility and the features that make this tool an easy solution for deploying complex environments so with that being said let's dive in now breaking down the components that i mentioned earlier i wanted to start off with the first component being the configuration now a configuration defines the structure of your deployment as you must specify a configuration to create a deployment a configuration describes all the resources you want for a single deployment and is written in yaml syntax that lists each of the resources you want to create and its respective resource properties a configuration must contain a resources section followed by the list of resources to create and so each resource must contain these three components the name the type and properties without these three components a deployment will not instantiate and so i wanted to take a moment to go over these three components in a bit of depth so the first component of the configuration is the name and the name is a user defined string to identify this resource and can be anything you choose from names like instance one myvm bowtie dash instance and you can even go as far to use larks dash instance dash don't dash touch and the syntax can be found here and must not contain any spaces or invalid characters next component in a configuration is type and there are a couple of different types that you can choose from a type can represent a single api source known as a base type or a set of resources known as a composite type and either one of these can be used to create part of your deployment the type of the resource being deployed here in this diagram is shown as a base type of compute.v1.instance and there are many other api resources that can be used such as compute.v1.disk app engine dot v1 as well as bigquery.v2 and the syntax is shown here as api dot version dot resource now a composite type contains one or more templates that are preconfigured to work together these templates expand to a set of base types when deployed in a deployment composite types are essentially hosted templates that you can add to deployment manager the syntax is shown here as gcp dash types forward slash provider colon resource and to give you an example of what a composite type looks like here is shown the creation of a reserved ip address using the compute engine v1 api and you could also use composite types with other apis in the same way such as gcp dash types forward slash app engine dash v1 colon apps or bigquery v2 colon data sets and for the last component in a configuration is properties and this is the parameters for the resource type this includes all the parameters you see here in this example including the zone machine type the type of disk along with its parameters pretty much everything that gives detail on the resource type now just as a note they must match the properties for this type so what do i mean by this so let's say you entered a zone but that particular zone doesn't exist or that compute engine machine type doesn't exist in that zone you will end up getting an error as deployment manager will not be able to parse this configuration and thus failing deployment so make sure when you add your properties that they match those of the resource now a configuration can contain templates which are essentially parts of the configuration file that have been abstracted into individual building blocks a template is a separate file that is imported and used as a type in a configuration and you can use as many templates as you want in a configuration and allow you to separate your configuration out into different pieces that you can use and reuse across different deployments templates can be as generalized or specific as you need and they also allow you to take advantage of features like template properties environment variables and modules to create dynamic configuration as shown here templates can be written in a couple of different ways they can be written in either ginger 2.1 or python 3. the example shown on the left has been written in ginger and is very similar to the yaml syntax so if you're familiar with yaml this might be better for you the example on the right has been written in python and is pretty amazing as you can take advantage of programmatically generating parts of your templates if you are familiar with python this might be a better format for you now one of the advantages of using templates is the ability to create and define custom template properties template properties are arbitrary variables that you define in template files any configuration file or template file that uses the template in question can provide a value for the template property without changing the template directly this lets you abstract the property so that you can change the property's value for each unique configuration without updating the underlying template and just as a note deployment manager creates predefined environment variables that you can use in your deployment in this example the project variable will use the project id for this specific project and so combining all these components together will give you a deployment and so a deployment is a collection of resources that are deployed and managed together using a configuration you can then deploy update or delete this deployment by merely changing some code or at the click of a button now when you deploy you provide a valid configuration in the request to create the deployment a deployment can contain a number of resources across a number of google cloud services when you create a deployment deployment manager creates all of the described resources to deploy a configuration it must be done through the command line and cannot be done through the console you can simply use the syntax shown here and a deployment will be instantiated from the configuration file that you have entered where bow tie deploy is the name of the deployment and the file after the dash dash config is your configuration file google cloud also offers predefined templates that you can use to deploy from the gcp marketplace and can be found right in the console of deployment manager this way all the configuration and template creation is handled for you and you just deploy the solution through the console now after you've created a deployment you can update it whenever you need to you can update a deployment by adding or removing resources from a deployment or updating the properties of existing resources in a deployment a single update can contain any combination of these changes so you can make changes to the properties of existing resources and add new resources in the same request you update your deployment by first making changes to your configuration file or you can create a configuration file with the changes you want you will then have the option to pick the policies to use for your updates or you can use the default policies and finally you then make the update request to deployment manager and so once you've launched your deployment each deployment has a corresponding manifest as the example shown here a manifest is a readonly property that describes all the resources in your deployment and is automatically created with each new deployment manifests cannot be modified after they have been created as well it's not the same as a configuration file but is created based on the configuration file and so when you delete a deployment all resources that are part of the deployment are also deleted if you want to delete specific resources from your deployment and keep the rest delete those resources from your configuration file and update the deployment instead and so as you can see here deployment manager gives you a slew of different options to deploy update or delete resources simultaneously in google cloud now like most services in gcp there are always some best practices to follow note that there are many more best practices to add to this and can be found in the documentation which i will be providing the link to in the lesson text but i did want to point out some important ones to remember so the first one i wanted to bring up is to break your configurations up into logical units so for example you should create separate configurations for networking services security services and compute services so this way each team will be able to easily take care of their own domain without having to sift through a massive template containing the code to the entire environment another best practice to follow is to use references and references should be used for values that are not defined until a resource is created such as resources selflink ip address or system generated id without references deployment manager creates all resources in parallel so there's no guarantee that dependent resources are created in the correct order using references would enforce the order in which resources are created the next one is to preview your deployments using the preview flag so you should always preview your deployments to assess how making an update will affect your deployment deployment manager does not actually deploy resources when you preview a configuration but runs a mock deployment of those resources instead this gives you the opportunity to see the changes to your deployment before committing to it you also want to consider automating the creation of projects as well as automating the creation of resources contained within the projects and this enables you to adopt an infrastructure as code approach for project provisioning this will allow you to provide a series of predefined project environments that can be quickly and easily provisioned it will also allow you to use version control to manage your base project configuration and it will also allow you to deploy reproducible and consistent project configurations and lastly using a version control system as part of the development process for your deployments is a great best practice to follow as it allows you to fall back to a previous known good configuration it provides an audit trail for changes as well it uses the configuration as part of a continuous deployment system now as you've seen here in this lesson deployment manager can be a powerful tool in your tool belt when it comes to implementing infrastructure as code and it has endless possibilities that you can explore on your own it can also provide a massive push towards devops practices and head down the path of continuous automation through continuous integration continuous delivery and continuous deployment and so that's pretty much all i wanted to cover when it comes to deployment manager and so whenever you're ready join me in the next one where we will go handson in a demonstration to deploy a configuration in deployment manager so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this demonstration we're gonna go handson with deployment manager and deploy a small web server we're gonna first use the google cloud editor to copy in our code and we're gonna then do a dry run and then finally deploy our code we're gonna then do a walkthrough of deployment manager in the console and go through the manifest as well as some of the other features we're then going to verify all the deployed resources and we get to do an easy cleanup in the end by hitting the delete button and taking care of removing any resources that were created so there's quite a bit to go through here and so with that being said let's dive in and so as you can see here i am logged in as tonybowties gmail.com in the project of bowtie inc now since we're going to be doing most of our work in code the first thing that we want to do is go to the google cloud editor so i'm going to go up here to the top and open up cloud shell and i'm going to then click on the button open editor i'm going to make this full screen for better viewing and so in order to get the terminal in the same viewing pane as the editor i'm going to simply go up to the top menu and click on terminal and select new terminal now for better viewing and this is totally optional for you i'm going to change the color theme into a dark mode and so i'm going to go up to the menu click on file go down to settings and go over to color theme and i'm going to select dark visual studio and for those of you who are working in visual studio code this may look very familiar to you and i'm also going to increase the font size by again going back up to file over to settings and then over to open preferences here under workspace and then scroll down to terminal and if you scroll down to integrated font size i'm going to adjust the font size to 20 for better viewing and my cloud shell font size is a little bit easier to see and so once you've done that you can then close the preferences tab and we're now ready to create files in our editor okay so next up i want to create a folder for all my files to live in so i'm going to go up to the menu here i'm going to select on file and select new folder and i'm going to rename this folder as templates and hit ok and so now that we have the folder that all of our files are going to live in the next step is to open up the github repository in your text editor and have your files ready to copy over and so just as a note for those who are fluent in how to use git you can use this new feature in the cloud shell editor to clone the course repo without having to recreate the files so i'm going to go over my text editor and make sure that you've recently done a git pull we're going to open up the files under compute engine deployment manager and you'll see templates with a set of three files and i've already conveniently opened them up i'm going to go up to bow tie deploy.yaml and this is going to be the configuration file that i'm going to be copying over and once i finish copying all these files over i'll be going through this in a little bit of detail just so you can understand the format of this configuration and so i'm going to select all of this i'm going to copy this head back on over to the editor and here i'm going to select file new file so i'm going to rename this as bow tie dash deploy dot yaml hit okay and i'm going to paste in my code and so this configuration file is showing that i'm going to be importing two templates by the name of bowtie.webserver.jinja as well as bowtie.network.jinja so i'm going to have a template for my web server and a template for the network and under resources as you can see this code here will create my bow tie dash web server the type is going to be the template the properties will have the zone the machine type as well as a reference for the network as well underneath the bowtie web server is the bowtie network and again this is pulling from type bowtie.network.jinja so this is a another template file and under the properties we have the region of us east one and so we're going to copy over these two templates bowtie web server and bowtie network as we need both of these templates in order to complete this deployment and so i'm going to go ahead and do that now head back on over to my code editor i'm going to go to bowtie web server i'm going to copy everything here back to my editor and i'm going to create the new file called bowtie web server it's going to be dot jinja hit enter i'm going to paste the code in and just to do a quick run through of the template the instance name is going to be bow tie dash website the type is compute.v1.instance and as you can see here we are using a bunch of different properties here under zone we have property zone which is going to reference back to the yaml template here under zone you will see us east 1b and so this way if i have to create another web server i can enter whatever zone i like here in the configuration file and leave the bow tie dash web server template just the way it is under machine type i have variables set for both the zone and machine type under disks i'm going to have the device name as an environment variable and it's going to be a persistent disk and the source image is going to be debian9 i also put in some metadata here that will bring up the web server and lastly i have a network tag of http server as well as the configuration for the network interface the network referring to bowtie dash network and a sub network called public which i will be showing to you in just a moment and as well the access configs of the type one to one nat and this will give the instance a public ip address and so now that we've gone through that template we need to create one last template which is the bowtie dash network so i'm going to head back on over to my code editor and open up bowtie network select the code copy it back over to cloud editor and i'm going to create a new file call this bowtie network dot jinja hit enter paste in my code and to quickly walk you through this we're going to be creating a new custom network called bow tie dash network the type is going to be compute.v1.network as the vpc uses the compute engine api it's going to be a custom network so the value of the auto create sub networks is going to be false the name is going to be public here we have the custom ipcider range and you can also use this as a variable but for this demo i decided to just leave it under network i have a reference to the bowtie network the value for private google access is false and the region variable is fulfilled through the configuration file moving right along i have two firewall rules here one for ssh access and the other for web server access one opening up port 22 to the world as well as port 80. as well the web server access firewall rule has a target tag of http server referencing back to the network tag of the bowtie web server instance okay and so now we've finished creating the configuration file along with the templates so i'm going to head back on up to the menu click on file and select save all and since we've finished creating all of our files the next thing to do is to execute a mock deploy using the bowtie deploy configuration but first i know that we haven't used deployment manager before and so i need to go in and turn on the api and so i'm just going to go up here to the top to the search bar and i'm going to type in deployment and you should see deployment manager as the first result and bring this down a little bit and as expected the deployment manager api has not been enabled yet so i'm going to click on enable and after a few moments we should be good to go okay and as you can see here deployment manager is pretty empty as most of it is done through the command line but if you're looking to deploy a marketplace solution you can do that right here at the top and this will bring you right to the marketplace and will allow you to deploy from a large selection of preconfigured templates but i don't want to do that and so i'm just going to bring this up a little bit and i'm going to head on over to the terminal i'm going to run an ls i'm going to run the command ls and you should be able to see the templates folder i'm going to change my directory into the templates folder do another ls and here are all my files and so before we do a mock deploy of this configuration we want to make sure that we're deploying to the correct project i can see here that i am currently in bow tie inc but if you are ever unsure about the project that you're in you can always run the gcloud config list command in order to confirm so i'm going to quickly clear my screen and i'm going to run the command gcloud config list it's going to prompt me to authorize this api call and i'm going to authorize and as expected my project is set to deploy in project bowtie inc and so now that i've verified it i'm going to quickly clear my screen again and so i'm going to paste in my command gcloud deployment dash manager deployments create bowtie deploy which is the name of the deployment along with the configuration file flag dash dash config and then the name of the configuration file which is bowtie deploy.yaml and the preview flag as we're only doing a mock deploy and so if there are any errors i'll be able to see this before i actually deploy all the resources so i'm going to go ahead and hit enter and in just a minute we'll find out exactly what happens and as you can see here the mock deployment was a success and there are no errors and if i do a quick refresh up here in the console i'll be able to see my deployment which i can drill down into and here i will see my manifest file with my manifest name and i can view the config as well as my templates that it imported the layout as well as the expanded config so if i click on view of the config it'll show me here in the right hand panel exactly what this deployment has used for the config and i can do the same thing with my template files so i'm going to open up my network template and i can quickly go through that if i'd like as well i also have the option to download it and if i really want to get granular i can go over here to the left hand pane i can select on vm instance and it'll show me all the resource properties everything from the disks to the machine type to the metadata the network interfaces the zone that it's in and the network tag same thing if i go over here to the network and again because this is a custom network the value for the autocreate subnetworks is false i can check on the public sub network as well as the firewall rules and so because this is a preview it has not actually deployed anything now taking a look at compute engine instances in a new tab you can see here that i have no instances deployed and so the same goes for any of the other resources and so what we want to do now is we want to deploy this deployment and we can do that one of two ways we can simply click on the button here that says deploy or we can run the command in the command line and so i'm looking to show you how to do it in the command line so i'm going to move down to the command line i'm going to quickly clear my screen i'm going to paste in the code which is gcloud deployment dash manager deployments update bowtie deploy now you're probably wondering why update and this is because the configuration has been deployed even though it's a preview deployment manager still sees it as a deployment and has created what google cloud calls a shell and so by using update you can fully deploy the configuration using your last preview to perform that update and this will deploy your resources exactly how you see it in the manifest and so anytime i make an adjustment to either the configuration or the templates i can simply run the update command instead of doing the whole deployment again so i want to get this deployed now and so i'm going to hit enter and i'll be back in a minute once it's deployed all the resources and success my deployment is successful and as you can see here there are no errors and all the resources are in a completed state so i'm going to select my bow tie website in my manifest and i'll have access to the resource with a link up here at the top that will bring me to the instance as well i can ssh into the instance and i have all the same options that i have in the compute engine console and so in order to verify that all my resources have been deployed i'm going to go back over to the tab that i already have open and as you can see my instance has been deployed and i want to check to see if my network has been deployed so i'm going to go up to the navigation menu and i'm going to head on down to vpc network and as you can see here bowtie network has been deployed with its two corresponding firewall rules i'm going to drill down into bowtie network and check out the firewall rules and as you can see here ssh access and web server access have been created with its corresponding protocols and ports and so now that i know that all my resources have been deployed i want to head back on over to compute engine to see if my instance has been configured properly so i'm going to click on ssh to see if i can ssh into the instance and success with ssh so i know that this is working properly and so i'm going to close this tab down and i also want to see whether or not my web server has been configured properly with the metadata that i provided it and so i can directly open up the webpage by simply clicking on this link and success my you look dapper today why thank you tony bowtie and so as you can see the web server has been configured properly using the metadata that i provided so i wanted to congratulate you on making it to the end of this demo and hope it has been extremely useful and gave you an understanding of how infrastructure is code is used in google cloud using their native tools i hope this also triggered some possible use cases for you that will allow you to automate more resources and configurations in your environment and allow you to start innovating on fantastic new ways for cicd for those of you who are familiar with infrastructure as code this may have been a refresher but will give you some insight for questions on the exam that cover deployment manager and just as a quick note for those of you who are looking to learn more about infrastructure as code i have put a few links in the lesson text going into depth on deployment manager and another tool that google recommends called terraform and so now before you go we want to clean up all the resources that we've deployed to reduce any incurred costs and because deployment manager makes it easy we can do it in one simple step so i'm going to head back on over to my open tab where i have my console open to deployment manager and i'm going to head on over to the delete button and simply click on delete now deployment manager gives me the option of deleting all the resources it created or simply deleting the manifest but keeping the resources untouched and so you want to select delete bowtie deploy with all of its resources and simply click on delete all and this will initiate the teardown of all the resources that have been deployed from the bowtie deploy configuration and this will take a few minutes to tear down but if you ever have a larger configuration to deploy just as a note it may take a little bit longer to both deploy and to tear down and so just as a recap you've created a configuration file and two templates in the cloud shell editor you then deployed your configuration using deployment manager through the command line in cloud shell you then verified each individual resource that was deployed and verified the configuration of each resource congratulations again on a job well done and so that's pretty much all i wanted to cover in this demo when it comes to deploying resources using deployment manager so you can now mark this as complete and let's move on to the next one welcome back and in this lesson we're going to learn about google cloud load balancing and how it's used to distribute traffic within the google cloud platform google cloud load balancing is essential when using it with instance groups kubernetes clusters and is pretty much the defacto when it comes to balancing traffic coming in as well as within your gcp environment knowing the differences between the types of load balancers and which one to use for specific scenarios is crucial for the exam as you will be tested on it and so there's a lot to cover here so with that being said let's dive in now i wanted to start off with some basics with regards to what is low balancing and so when it comes to the low balancer itself a low balancer distributes user traffic across multiple instances of your application so by spreading the load you reduce the risk of your applications experiencing performance issues a load balancer is a single point of entry with either one or multiple back ends and within gcp these back ends could consist of either instance groups or negs and i'll be getting into any g's in just a little bit low balancers on gcp are fully distributed and software defined so there is no actual hardware load balancer involved in low balancing on gcp it is completely software defined and so there's no need to worry about any hardware any prewarming time as this is all done through software now depending on which low balancer you choose google cloud gives you the option of having either a global load balancer or a regional load balancer the load balancers are meant to serve content as close as possible to the users so that they don't experience increased latency and gives the users a better experience as well as reducing latency on your applications when dealing with low balancers in between services google cloud also offers auto scaling with health checks in their load balancers to make sure that your traffic is always routed to healthy instances and by using auto scaling able to scale up the amount of instances you need in order to handle the load automatically now as there are many different low balancers to choose from it helps to know what specific aspects you're looking for and how you want your traffic distributed and so google has broken them down for us into these three categories the first category is global versus regional global load balancing is great for when your back ends are distributed across multiple regions and your users need access to the same applications and content using a single anycast ip address as well when you're looking for ipv6 termination global load balancing will take care of that now when it comes to regional load balancing this is if you're looking at serving your back ends in a single region and handling only ipv4 traffic now once you've determined whether or not you need global versus regional low balancing the second category to dive into is external versus internal external load balancers are designed to distribute traffic coming into your network from the internet and internal load balancers are designed to distribute traffic within your network and finally the last category that will help you decide on what type of load balancer you need is the traffic type and shown here are all the traffic types that cover http https tcp and udp and so now that we've covered the different types of load balancing that's available on google cloud i wanted to dive into some more depth on the low balancers themselves here you can see that there are five load balancers available and i will be going through each one of these in detail now before diving into the low balancers themselves i wanted to introduce you to a concept using gcp for all load balancers called back end services how a low balancer knows exactly what to do is defined by a backend service and this is how cloud load balancing knows how to distribute the traffic the backend service configuration contains a set of values such as the protocol used to connect to back ends various distribution in session settings health checks and timeouts these settings provide fine grain control over how your load balancer behaves an external http or https load balancer must have at least one backend service and can have multiple backend services the back ends of a backend service can be either instance groups or network endpoint groups also known as negs but not a combination of both and so just as a note you'll hear me refer to negs over the course of this lesson and so a network endpoint group also known as neg is a configuration object that specifies a group of backend endpoints or services and a common use case for this configuration is deploying services into containers now moving on to the values themselves i wanted to first start with health checks and google cloud uses the overall health state of each back end to determine its eligibility for receiving new requests or connections back ends that respond successfully for the configured number of times are considered healthy backends that fail to respond successfully for a separate number of times are considered unhealthy and when a backend is considered unhealthy traffic will not be routed to it next up is session affinity and session affinity sends all requests from the same client to the same back end if the back end is healthy and it has capacity service timeout is the next value and this is the amount of time that the load balancer waits for a backend to return a full response to a request next up is traffic distribution and this comprises of three different values the first one is a balancing mode and this defines how the load balancer measures backend readiness for the new requests or connections the second one is target capacity and this defines a target maximum number of connections a target maximum rate or target maximum cpu utilization and the third value for traffic distribution is capacity scalar and this adjusts overall available capacity without modifying the target capacity and the last value for backend services are backends and a backend is a group of endpoints that receive traffic from a google cloud load balancer and there are several types of backends but the one that we are concentrating on for this section and for the exam is the instance group now backend services are not critical to know for the exam but i wanted to introduce you to this concept to add a bit more context for when you are creating low balancers in any environment and will help you understand other concepts in this lesson and so this is the end of part one of this lesson it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready join me in part two where we will be starting immediately from the end of part one so you can now complete this video and i will see you in part two this is part two of the cloud load balancers lesson and we'll be starting exactly where we left off in part one so with that being said let's dive in now before jumping right into the first load balancer that i wanted to introduce which is http and https low balancer there's a couple of different concepts that i wanted to introduce and these are the methods of how an http and https load balancer distributes traffic using forwarding rules and these are cross region low balancing and content based load balancing now touching on cross region load balancing when you configure an external http or https load balancer in premium tier it uses a global external ip address and can intelligently route requests from users to the closest backend instance group or neg based on proximity for example if you set up instance groups in north america and europe and attach them to a low balancers backend service user requests around the world are automatically sent to the vms closest to the users assuming that the vms pass health checks and have enough capacity if the closest vms are all unhealthy or if the closest instance group is at capacity and another instance group is not at capacity the load balancer automatically sends requests to the next closest region that has available capacity and so here in this diagram a user in switzerland hits the low balancer by going to bowtieinc.co and because there are vms that are able to serve that traffic in europe west 6 traffic is routed to that region and so now getting into content based load balancing http and https low balancing supports content based load balancing using url maps to select a backend service based on the requested host name request path or both for example you can use a set of instance groups or negs to handle your video content and another set to handle static as well as another set to handle any images you can also use http or https low balancing with cloud storage buckets and then after you have your load balancer set up you can add cloud storage buckets to it now moving right along when it comes to http and https load balancer this is a global proxy based layer 7 low balancer which is at the application layer and so just as a note here with all the other low balancers that are available in gcp the http and https low balancer is the only layer 7 load balancer all the other low balancers in gcp are layer 4 and will work at the network layer and so this low balancer enables you to serve your applications worldwide behind a single external unicast ip address external http and https load balancing distributes http and https traffic to back ends hosted on compute engine and gke external http and https load balancing is implemented on google front ends or gfes as shown here in the diagram gfes are distributed globally and operate together using google's global network and control plane in the premium tier gfes offer crossregional low balancing directing traffic to the closest healthy backend that has capacity and terminating http and https traffic as close as possible to your users with the standard tier the load balancing is handled regionally and this load balancer is available to be used both externally and internally that makes this load balancer global external and internal this load balancer also gives support for https and ssl which covers tls for encryption in transit as well this load balancer accepts all traffic whether it is ipv4 or ipv6 traffic and just know that ipv6 traffic will terminate at the low balancer and then it will forward traffic as ipv4 so it doesn't really matter which type of traffic you're sending the load balancer will still send the traffic to the back end using ipv4 this traffic is distributed by location or by content as shown in the previous diagram forwarding rules are in place to distribute defined targets to each target pool for the instance groups again defined targets could be content based and therefore as shown in the previous diagram video content could go to one target whereas static content could go to another target url maps direct your requests based on rules so you can create a bunch of rules depending on what type of traffic you want to direct and put them in maps for requests ssl certificates are needed for https and these can be either google managed or selfmanaged and so just as a quick note here the ports used for http are on 80 and 8080 as well on https the port that is used is port 443 now moving into the next low balancer is ssl proxy an ssl proxy low balancing is a reverse proxy load balancer that distributes ssl traffic coming from the internet to your vm instances when using ssl proxy load balancing for your ssl traffic user ssl connections are terminated at the low balancing layer and then proxied to the closest available backend instances by either using ssl or tcp with the premium tier ssl proxy low balancing can be configured as a global load balancing service with the standard tier the ssl proxy load balancer handles low balancing regionally this load balancer also distributes traffic by location only ssl proxy low balancing lets you use a single ip address for all users worldwide and is a layer 4 load balancer which works on the network layer this load balancer shows support for tcp with ssl offload and this is something specific to remember for the exam this is not like the http or https load balancer where we can use specific rules or specific configurations in order to direct traffic ssl proxy low balancer supports both ipv4 and ipv6 but again it does terminate at the load balancer and forwards the traffic to the back end as ipv4 traffic and forwarding rules are in place to distribute each defined target to its proper target pool and encryption is supported by configuring backend services to accept all the traffic over ssl now just as a note it can also be used for other protocols that use ssl such as web sockets and imap over ssl and carry a number of open ports to support them moving on to the next load balancer is tcp proxy now the tcp proxy load balancer is a reverse proxy load balancer that distributes tcp traffic coming from the internet to your vm instances when using tcp proxy load balancing traffic coming over a tcp connection is terminated at the load balancing layer and then forwarded to the closest available backend using tcp or ssl so this is where the low balancer will determine which instances are at capacity and send them to those instances that are not like ssl proxy load balancing tcp proxy load balancing lets you use a single ip address for all users worldwide the tcp proxy load balancer automatically routes traffic to the back ends that are closest to the user this is a layer 4 load balancer and again can serve traffic both globally and externally tcp proxy distributes traffic by location only and is intended for specifically nonhttp traffic although you can decide if you want to use ssl between the proxy and your back end and you can do this by selecting a certificate on the back end again this type of load balancer supports ipv4 and ipv6 traffic and ipv6 traffic will terminate at the low balancer and forwards that traffic to the back end as ipv4 traffic now tcp proxy low balancing is intended for tcp traffic and supports many wellknown ports such as port 25 for simple mail transfer protocol or smtp next up we have the network load balancer now the tcp udp network load balancer is a regional passthrough load balancer a network load balancer distributes tcp or udp traffic among instances in the same region network load balancers are not proxies and therefore responses from the back end vms go directly to the clients not back through the load balancer the term known for this is direct server return as shown here in the diagram this is a layer 4 regional load balancer and an external load balancer as well that can serve to regional locations it supports either tcp or udp but not both although it can low balance udp tcp and ssl traffic on the ports that are not supported by the tcp proxy and ssl proxy ssl traffic can still be decrypted by your back end instead of the load balancer itself traffic is also distributed by incoming protocol data this being protocols scheme and scope there is no tls offloading or proxying and forwarding rules are in place to distribute and define targets to their target pools and this is for tcp and udp only now with other protocols they use target instances as opposed to instance groups lastly a network load balancer can also only support selfmanaged ssl certificates as opposed to the google managed certificates as well and so the last low balancer to introduce is the internal load balancer now an internal tcp or udp load balancer is a layer 4 regional load balancer that enables you to distribute traffic behind an internal load balancing ip address that is accessible only to your internal vm instances internal tcp and udp load balancing distributes traffic among vm instances in the same region this load balancer supports tcp or udp traffic but not both and as i said before this type of load balancer is used to balance traffic within gcp across instances this low balancer cannot be used for balancing internet traffic as it is internal only traffic is automatically sent to the back end as it does not terminate client connections and for forwarding rules this load balancer follows specific specifications where you need to specify at least one and up to five ports by number as well you must specify all to forward traffic to all ports now again like the network load balancer you can use either tcp or udp and so that's pretty much all i had to cover with this lesson on low balancing please remember that for the exam you will need to know the differences between them all in my experience there are a few questions that come up on the exam where you will need to know what low balancer to use and so a good idea might be to dive into the console and have a look at the options as well as going back through this lesson as a refresher to understand each use case this is also a crucial component in any environment that is used especially when serving applications to the internet for any threetier web application or kubernetes cluster and so that pretty much sums up this lesson on low balancing so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i will be going into depth on instance groups along with instance templates instance groups are a great way to set up a group of identical servers used in conjunction with instance groups instance templates handles the instance properties to deploy the instance groups into your environment this lesson will dive into the details of the features use cases and how instance groups and instance templates work together to create a highly scalable and performing environment now there's a lot to cover here so with that being said let's dive in now an instance group is a collection of vm instances that you can manage as a single entity compute engine offers two kinds of vm instance groups managed and unmanaged manage instance groups or migs let you operate applications on multiple identical vms you can make your workload scalable and highly available by taking advantage of automated mig services like auto scaling auto healing regional and zonal deployments and automatic updating and i'll be getting into these services in just a sec now when it comes to unmanaged instance groups they also let you low balance across a fleet of vms but this is something that you need to manage and i'll be going deeper into unmanaged instance groups a bit later right now i wanted to take some time to go through the features and use cases of migs in a bit more detail for some more context starting off with its use cases now migs are great for stateless serving workloads such as website front ends web servers and website applications as the application does not preserve its state and saves no data to persistent storage all user and session data stays with the client and makes scaling up and down quick and easy migs are also great for stateless batch workloads and these are high performance or high throughput compute workloads such as image processing from a queue and lastly you can build highly available stateful workloads using stateful managed instance groups or stateful migs stateful workloads include applications with stateful data or configuration such as databases legacy monolith type applications and long running batch computations with checkpointing you can improve uptime and resiliency of these types of applications with auto healing controlled updates and multizone deployments while preserving each instance's unique state including instance names persistent disks and metadata now that i've covered the type of workloads that are used with migs i wanted to dive into the features starting with auto healing now when it comes to auto healing managed instance groups maintain high availability of your applications by proactively keeping your instances in a running state a mig automatically recreates an instance that is not running and managed instance groups also take care of applicationbased auto healing and this improves application availability by relying on a health check that detects things like freezing crashing or overloading if a health check determines that an application has failed on a vm the mig auto healer automatically recreates that vm instance the health check used to monitor the migs are similar to the health checks used for low balancing with a few little differences low balancing health checks help direct traffic away from unresponsive instances and towards healthy ones these health checks cannot recreate instances whereas mig health checks proactively signal to delete and recreate instances that become unhealthy moving on to managed instance groups regional or multizone feature now you have the option of creating regional migs or zonal migs regional migs provide higher availability compared to zonal migs because the instances in a regional mig are spread across multiple zones in a single region google recommends regional migs over zonal migs as you can manage twice as many migs as zonal migs so you can manage 2 000 migs instead of 1000 you can also spread your application load across multiple zones instead of a single zone or managing multiple zonal migs across different zones and this protects against zonal failures and unforeseen scenarios where an entire group of instances in a single zone malfunctions in the case of a zonal failure or if a group of instances in a zone stops responding a regional mig continues supporting your instances by continuing to serve traffic to the instances in the remaining zones now cloud low balancing can use instance groups to serve traffic so you can add instance groups to a target pool or to a back end an instance group is a type of back end and the instances in the instance group respond to traffic from the load balancer the back end service in turn knows which instances it can use and how much traffic they can handle and how much traffic they are currently handling in addition the backend service monitors health checking and does not send new connections to unhealthy instances now when your applications require additional compute resources migs support auto scaling that dynamically add or remove instances from the mig in response to an increase or decrease in load you can turn on auto scaling and configure an auto scaling policy to specify how you want the group to scale not only will auto scaling scale up to meet the load demands but will also shrink and remove instances as the load decreases to reduce your costs auto scaling policies include scaling based on cpu utilization load balancing capacity and cloud monitoring metrics and so when it comes to auto updating you can easily and safely deploy new versions of software to instances in a mig the rollout of an update happens automatically based on your specifications you can also control the speed and scope of the deployments in order to minimize disruptions to your application you can optionally perform rolling updates as well as partial rollouts for canary testing and for those who don't know rolling updates allow updates to take place with zero downtime by incrementally updating instances with new ones as well canary testing is a way to reduce risk and validate new software by releasing software to a small percentage of users with canary testing you can deliver to certain groups of users at a time and this is also referred to as stage rollouts and this is a best practice in devops and software development now there are a few more things that i wanted to point out that relate to migs you can reduce the cost of your workload by using preemptable vm instances in your instance group and when they are deleted auto healing will bring the instances back when preemptable capacity becomes available again you can also deploy containers to instances in managed instance groups when you specify a container image in an instance template and is used to create a mig each vm is created with the container optimized os that includes docker and your container starts automatically on each vm in the group and finally when creating migs you must define the vpc network that it will reside in although when you don't define the network google cloud will attempt to use the default network now moving on into unmanaged instance groups for just a minute unmanaged instance groups can contain heterogeneous instances and these are instances that are of mixed sizes of cpu ram as well as instance types and you can add and remove these instances from the group whenever you choose there's a major downside to this though unmanaged instance groups do not offer auto scaling auto healing rolling update support multizone support or the use of instance templates and are not a good fit for deploying highly available and scalable workloads you should only use unmanaged instance groups if you need to apply load balancing to groups of these mixed types of instances or if you need to manage the instances yourself so unmanaged instance groups are designed for very special use cases where you will need to mix instance types in almost all cases you will be using managed instance groups as they were intended to capture the benefits of all the features they have to offer now in order to launch an instance group into any environment you will need another resource to do this and this is where instance templates come into play an instance template is a resource that you can use to create vm instances and managed instance groups instance templates define the machine type boot disk image or container image as well as labels and other instance properties you can then use an instance template to create a mig or vm instance instance templates are an easy way to save a vm instances configuration so you can use it later to recreate vms or groups of vms an instance template is a global resource that is not bound to a zone or region although you can restrict a template to a zone by calling out specific zonal resources now there is something to note for when you are ever using migs if you want to create a group of identical instances you must use an instance template to create a mig and is something you should always keep in the front of mind when using migs these two resources both instance templates and managed instance groups go hand in hand now some other things to note is that instance templates are designed to create instances with identical configurations so you cannot update an existing instance template or change an instance template after you create it if you need to make changes to the configuration create a new instance template you can create a template based on an existing instance template or based on an existing instance to use an existing vm to make a template you can save the configuration using the gcloud command gcloud instance dash templates create or to use the console you can simply go to the instance templates page click on the template that you want to update and click on create similar the last thing that i wanted to point out is that you can use custom or public images in your instance templates and so that's pretty much all i had to cover when it comes to instance groups and instance templates managed instance groups are great for when you're looking at high availability as a priority and letting migs do all the work of keeping your environment up and running and so you can now mark this lesson as complete and whenever you're ready join me in the next one where we go handson with instance groups instance templates and load balancers in a demo welcome back in this demo we're going to put everything that we've learned together in a handson demo called managing bow ties we're going to create an instance template and next we're going to use it to create an instance group we're then going to create a low balancer with a new back end and create some health checks along the way we're then going to verify that all instances are working by browsing to the load balancer ip and verifying the website application we're then going to stress test one of the instances to simulate a scale out using auto scaling and then we're going to simulate scaling the instance group back in now there's quite a bit to do here so with that being said let's dive in so here i am logged in as tony bowties at gmail.com under project bowtie inc and so the first thing that you want to do is you want to make sure that you have a default vpc network already created and so just to double check i'm going to go over to the navigation menu i'm going to scroll down to vpc network and yes i do have a default vpc network so i'm going to go ahead and start creating my resources and so now what i want to do is i want to create my instance template and so in order to do that i'm going to go back up to the navigation menu i'm going to go down to compute engine and go up to instance templates as you can see i currently have no instance templates and yours should look the same and so you can go ahead and click on create instance template and so just as a note there are no monthly costs associated with instance templates but this estimate here on the right is to show you the cost of each instance you will be creating with this template okay so getting right into it i'm going to name this instance template bowtie template and since we're spinning up a lot of vms you want to be conscious on costs and so under series you're going to click on the drop down and you're going to select n1 and under machine type you're going to select f1 micro and this is the smallest instance type as well as the cheapest within google cloud you can go ahead and scroll down right to the bottom here under firewall you want to check off allow http traffic next you want to select management security disks networking and sold tenancy you scroll down a little bit and under startup script you're going to paste in the script that's available in the repo and you will find a link to this script and the repo in the lesson text and so you can leave all the other options as its default and simply click on create it's going to take a couple minutes here okay and the instance template is ready and so the next step that you want to do is create an instance group and as i said in a previous lesson in order to create an instance group you need an instance template hence why we made the instance template first okay and our instance template has been created and so now that you've created your instance template you can head on over to instance groups here in the left hand menu and as expected there are no instance groups and so you can go ahead and click on the big blue button and create an instance group you're going to make sure that new managed instance group stateless is selected and here you have the option of choosing a stateful instance group as well as an unmanaged instance group and so we're going to keep things stateless and so for the name of the instance group you can simply call this bowtie group i'm going to use the same name in the description and under location you want to check off multiple zones in under region you want to select us east one and if you click on configure zones you can see here that you can select all the different zones that's available in that region that you choose to have your instances in and so i'm going to keep it under all three zones i'm going to scroll down here a little bit and under instance template you should see bow tie template you can select that you can scroll down a little bit more and here under minimum number of instances you want to set the minimum number of instances to 3 and under maximum number of instances you want to set that to 6 and so this is going to be double the amount of the minimum number of instances so when you're scaled out you should have a maximum of 6 instances and when you're scaled in or you have very low traffic you should only have three instances so you can scroll down some more and under auto healing you want to select the health check and you're going to go ahead and create a new health check under name you can call this healthy bow ties i'm going to use the same for the description and i'm going to leave the rest as its default and go down and click on save and continue i'm going to scroll down some more and i'm going to leave the rest as is and simply click on create and it's going to take a couple minutes here and so i'm going to pause the video and i'll be back in a flash okay and my instance group has been created and so to get a better look at it i'm going to click on bow tie group and i can see here that three instances have been created if i go up to vm instances you can see here that i have three instances but under instance groups because i have health check enabled it shows that my instances are unhealthy and this is because i still need to create a firewall rule that will allow google's health check probes to reach my vm instances and so you're going to go ahead and create that firewall rule so you can bring the health check status up to healthy so i'm going to go over to the navigation menu and scroll down to vpc network and go over to firewall here under firewall as expected you have the default firewall rules from the default created vpc network and so i'm going to go up to create firewall and you can name this firewall rule allow health check i'm going to use the same for the description i'm going to scroll down here a little bit and under targets i'm going to select all instances in the network source filter i'm going to leave as i p ranges and so here under source i p ranges i want to enter in the ip addresses for the google cloud health check probes and you can find these in the documentation and i will also be supplying them in the instructions and there are two sets of ip addresses that need to be entered and just as a note you don't need to know this for the exam but it's always a good to know if you're ever adding health checks to any of your instances i'm going to scroll down a little bit to protocols and ports and under tcp i'm going to check it off and put in port 80. that's pretty much all you have to do here so whenever you entered all that information in you can simply click on create and so now i have a firewall rule that will allow health checks to be done and so it may take a minute or two but if i head back on over to my compute engine instances and go over to my instance groups i'll be able to see that all my instances are now healthy and so whenever you're creating instance groups and you're applying health checks this firewall rule is necessary so please be aware okay so now that we've created our instance templates we've created our instance groups and we created a firewall rule in order to satisfy health checks we can now move on to the next step which is creating the load balancer so i'm going to go back up to the navigation menu and i'm going to scroll down to network services and over to load balancing and as expected there are no load balancers created and so whenever you're ready you can click on the big blue button and create a new low balancer here you have the option of creating an http or https load balancer along with a tcp load balancer or a udp load balancer and because we're serving external traffic on port 80 we're going to use the http load balancer so you can click on start configuration and i'm being prompted to decide between internet facing or internal only and you're going to be accepting traffic from the internet to your load bouncer so make sure that from internet to my vms is checked off and simply click continue and so next you will be prompted with a page with a bunch of configurations that you can enter and so we'll get to that in just a second but first we need to name our load balancer and so i'm going to call this bowtie dash lb for low balancer and so next step for your load balancer is you need to configure a back end so you can click on back end configuration and here you have the option of selecting from backend services or backend buckets so you're going to go ahead and click on backend services and create a backend service and here you will be prompted with a bunch of fields to fill out in order to create your backend service and you can go ahead and name the backend service as bowtie backend service backend type is going to be instance group and you can leave the protocol named port and timeout as is as we're going to be using http under instance group in new backend if you select the dropdown you should see your available bow tie group instance group select that scroll down a little bit and under port numbers you can enter in port 80 and you can leave all the other options as default and simply click on done and so if you're ever interested you can always add a cache using cloud cdn now i know we haven't gone through cloud cdn in this course but just know that this is google's content delivery network and it uses google's global edge network to serve content closer to users and this accelerates your websites and your applications and delivers a better user experience for your user okay and moving on here under health check if i click on the drop down you should see healthy bow ties you can select that for your health check and so just as a note here under advanced configurations you can set your session affinity your connection draining timeout as well as request and response headers and so we don't need any of that for this demo and so i'm going to go ahead and collapse this and once you've finished filling in all the fields you can simply click on create okay and so you should now have your back end configuration and your host and path rules configured and so the only thing that's left to configure is the front end so you can go up and click on frontend configuration and you can name your frontend bowtie frontend service gonna keep the protocols http and here is where you would select the network service tier choosing either premium or standard and if you remember in the load balancing lesson in order to use this as a global load balancer i need to use a premium tier okay and we're going to keep this as ipv4 with an ephemeral ip address on port 80 so once you've finished configuring the front end you can simply click on done and you can go and click on review and finalize and this will give you a summary on your configuration and so i'm happy with the way everything's configured and if you are as well you can simply click on create and this may take a minute or two but it will create your low balancer along with your back end and your front end so again i'm going to pause the video here for just a minute and i'll be back before you can say cat in the hat okay and my load balancer has been created and to get a little bit more details i'm going to drill down into it and i can see here the details of my load balancer along with my monitoring and any caching but i don't have any caching enabled and therefore nothing is showing so going back to the details i can see here that i have a new ip address for my load balancer and i'll be getting into that in just a minute i'm going to go back here and i'm going to check out my back ends click on bow tie back end service and here i can see the requests per second as well as my configuration and if you do see this caution symbol here showing that some of your instances are unhealthy it's only because the low balancer needs time to do a full health check on all the instances in the instance group and so this will take some time okay and so i'm going to go back over and check out my front end and there's nothing to drill down into with the front end service but it does show me my scope the address the protocol network tier and the low balancer itself so this is the end of part one of this demo it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up have a stretch get yourself a coffee or tea and whenever you're ready part two will be starting immediately from the end of part one so you can now mark this as complete and i'll see you in part two this is part two of the managing bow ties demo and we will be starting exactly where we left off in part one so with that being said let's dive in and so before you move forward you want to make sure that all your instances are considered healthy by your load balancer and as i can see here all my instances in my instance group are considered healthy by the load balancer and so just to verify this i'm going to go ahead and copy the i p address and you can open up a new tab in your browser and simply paste it in and success as you can see here managing the production of many bow ties can be automated but managing the wearer of them definitely cannot another fine message from the people at bow tie inc now although this is a simple web page i used a couple variables just to show you the low balancing that happens in the background and traffic will be load balanced in between all of the instances in the instance group so if you click on refresh then you should see the machine name and the data center change so every time i click refresh the traffic will be routed to a different instance in a different zone and so a simple simulation on how traffic is low balance between the different instances in their different zones okay so now that we've verified the website application i'm going to close down this tab and so now that we've created our instance template we've created our instance group and we've created our low balancer with the back end and front end service and it looks like everything seems to be working together nicely we're going to go ahead and simulate a scale out using auto scaling and so in order to simulate this we're going to do a stress test on one of the instances so i'm going to head back on over to the navigation menu scroll down to compute engine and here you can ssh into any one of these instances and run the stress test from there so i'm going to pick here the one at the top and so whenever you're logged in you can simply paste in the command that i've included in the instructions that will run the stress test and so this is a stress test application called stress that was included in the startup script and this again will put stress on the server itself and trigger a scale out to handle the load and it'll do this for 30 seconds so you can go ahead and hit enter and head back over to the console and in about a minute or two you should see some new instances that will be created by your instance group in order to handle the load okay and after about a couple minutes it's showing here that instances are being created and it will be scaling out to the maximum amount of instances that i've set it to which is six i'm going to drill down into this and yes a scale out is happening and some new instances are being created to handle the load so i'm going to give it just a minute here okay and as you can see here all the instances have been created they've been added to the instance group and all of them are marked as healthy and so just to verify that all the instances are working i'm going to go ahead and open up a new tab i'm going to plug in the ip address on my load balancer and i'm going to simply cycle through all these instances to make sure that all them are working and it looks like i have no issues and so now that you've simulated a scale out i wanted to go ahead and run a scale in and so i'm first going to close up these tabs now with regards to scaling there is a 10 minute stabilization period that cannot be adjusted for scaling and this is a builtin feature into google cloud now because i respect your time as a student i'm going to show you a work around to trigger a scale in sooner strictly for this demo and i also wanted to caution that this should never be done in a production or productionlike environment you should always wait for the scaling to happen on its own and never force it this method is being used strictly for learning purposes to save you some time and so i'm going to go ahead to the top menu and click on rolling restart and replace and this will bring up a new page where you will have the option to either restart or replace any instances in your instance group and so for your purposes under operation make sure that you have restart checked off and this will restart all of your instances and only bring up the ones that are needed so i'm going to go ahead and click on restart i'm going to go back to my instance group console and i'm just going to give this a few minutes to cook and i'll be right back in a flash okay so it looks like the instance group has scaled in and we are now down left to three instances the minimum that we configured for our instance group and so that pretty much covers the managing bow ties demo so i wanted to congratulate you on making it through this demo and i hope that this has been extremely useful in excelling your knowledge on managing instance templates managed instance groups and creating load balancers with backend and frontend services now this was a jampacked demo and there was a lot to pack in with everything you've learned from the last few lessons and so just as a recap you created an instance template with your startup script you then created a new instance group with a health check to go with it configuring auto scaling for a minimum of three instances you then created a firewall rule so that the health check probes were able to connect to the application and you then created a load balancer with its back end and frontend service and verified that the website application was indeed up and running you then ran a stress test to allow a simulation of a scale out of your instance group and then simulated a scale in of your instance group great job and so now that we've completed this demo you want to make sure that you're not accumulating any unnecessary costs and so i'm going to go ahead and walk you through the breakdown of deleting all these resources so first you're going to go ahead and delete the load balancer go back up to the navigation menu and scroll down to network services and go over to load balancing so i'm going to go ahead and check off bow tie lb and simply go up to the top and click on delete it's going to ask me if i'm sure i want to do this i'm also going to select bow tie back end service and i can delete my load balancer and my back end service all at once i'm going to go ahead and delete load balancer and the selected resources and this should clear up within a few seconds okay and our load balancer has been deleted i'm going to just go up here to the back end make sure everything's good yeah we're all clean same thing with front end and so now you can move on to instance groups so i'm going to head back up to the navigation menu go down a compute engine and go up to instance groups and here you can just simply check off bow tie group and simply click on delete you're going to be prompted with a notification to make sure you want to delete bow tie group yes i want to delete and again this should take about a minute okay it actually took a couple minutes but my instance group has been deleted and so now i'm going to go over to instance templates and i'm going to delete my template and check off bow tie template and simply click delete you're going to get a prompt to make sure you want to delete your instance template yes you want to delete and success you've now deleted all your resources although there is one more resource that you will not be billed for but since we're cleaning everything up we might as well clean that up as well and this is the firewall rule that we created and go over to the navigation menu and scroll down to vpc network i'm going to go to firewall here on the left hand menu and here i'm going to check off the allow health check firewall rule and simply click on delete i'm going to get a prompt to make sure that i want to delete it yes you want to delete i'm going to quickly hit refresh and yes we've deleted it and so this concludes the end of this demo so you can now mark this as complete and i'll see you in the next one welcome back in this next section we will be focusing on google cloud's premier container orchestration service called kubernetes but before we can dive right into kubernetes and the benefits that it gives to containers you'll need an understanding as to what containers are and what value containers provide in this lesson i will be covering the difference between virtual machines and containers what containers are how they work and the value proposition they bring so with that being said let's dive in now for those of you who didn't know container technology gets its name from the shipping industry products get placed into standardized shipping containers which are designed to fit into the ship that accommodates the container's standard size instead of having various sizes of packaging now by standardizing this process and keeping the items together the container can be moved as a unit and it costs less to do it this way as well the standardization allows for consistency when packing and moving the containers placing them on ships and docks as well as storage no matter where the container is it always stays the same size and the contents stay isolated from all the other containers that they are stacked with and so now before we get into the details of containers i wanted to cover how we got here and why so a great way to discuss containers is through their comparison to virtual machines now as we discussed in a previous lesson when it comes to vms the systems are virtualized through a hypervisor that sits on top of the underlying host infrastructure the underlying hardware is virtualized so that multiple operating system instances can run on the hardware each vm runs its own operating system and has access to virtualized resources representing the underlying hardware due to this process vms come with the cost of large overhead in cpu memory and disk as well can be very large due to the fact that each vm needs its own individual operating system there also lacks standardization between each vm making them unique due to the os configuration the software installed and the software libraries thus not making it very portable to be able to run in any environment now when dealing with containers things are run very differently the underlying host infrastructure is still there but instead of just using a hypervisor and abstracting the underlying hardware containerization takes it one step further and abstracts the operating system thus leaving the application with all of its dependencies in a neatly packaged standardized container this is done by installing the operating system on top of the host infrastructure and then a separate layer on top of the host operating system called the container engine now instead of having their own operating system the containers share the operating system kernel with other containers while operating independently running just the application code and the dependencies needed to run that application this allows each container to consume very little memory or disk making containers very lightweight efficient and portable containerized applications can start in seconds and many more instances of the application can fit onto the machine compared to a vm environment this container can now be brought over to other environments running docker and able to run without having the worries of running into issues of compatibility now although there are a few different container engines out there the one that has received the most popularity is docker and this is the engine that we will be referring to for the remainder of this course now a docker image is a collection or stack of layers that are created from sequential instructions on a docker file so each line in the dockerfile is run line by line and a unique readonly layer is written to the image what makes docker images unique is that each time you add another instruction in the docker file a new layer is created now going through a practical example here shown on the right is a docker file and we will be able to map each line of code to a layer shown on the docker image on the left the line marked from shows the base image that the image will be using the example shown here shows that the ubuntu image version 12.04 will be used next the run instruction is used which will perform a general update install apache 2 and output a message to be displayed that is written to the index.html file next up is the working directories and these are the environment variables set by using an env instruction and this will help run the apache runtime next layer is the expose instruction and this is used to expose the container's port on 8080 and lastly the command layer is an instruction that is executing the apache web server from its executable path and so this is a great example of how a docker file is broken down from each line to create the layers of this image and so just as a note here each docker image starts with a base image as well each line in a docker file creates a new layer that is added to the image and finally all the layers in a docker image are read only and cannot be changed unless the docker file is adjusted to reflect that change so now how do we get from a docker image to a container well a running docker container is actually an instantiation of an image so containers using the same image are identical to each other in terms of their application code and runtime dependencies so i could use the same image for multiple copies of the same container that have different tasks what makes each individual container different is that running containers include a writable layer on top of the readonly content runtime changes including any rights and updates to data and files are saved in this read write layer so in this example when using the command docker run fashionista a docker container will be instantiated from the docker image and a read write layer is always added on top of the readonly layers when a container is created writing any necessary files that's needed for the application and so just as a note here docker containers are always created from docker images and containers can use the same image yet will always have a different read write layer no matter the amount of containers running on a given host so now when your containers have been created you need a place to store them and so this is where a container registry comes into play now a container registry is a single place for you to store and manage docker images now when you create your docker file and then build your image you want to store that image in a central image repository whether it be a private one or a public one a popular public container registry is docker hub and this is a common registry where many open source images can be found including those used for the base layer images like the ubuntu example that i showed you earlier and so once you have your containers in a container registry you need to be able to run these containers so in order to run these containers you need docker hosts and these can consist of any machine running the docker engine and this could be your laptop server or you can run them in provided hosted cloud environments now this may have been a refresher for some but for those of you who are new to containers i hope this has given you a lot more clarity on what containers are what they do and the value that they bring to any environment and so that's pretty much all i wanted to cover on this short lesson of an introduction to containers so you can now mark this lesson as complete and let's move on to the next one welcome back so now that you've gotten familiar with what containers are and how they work i wanted to dive into google cloud's platform as a service offering for containers called google kubernetes engine also known as short as gke now although the exam goes into a more operational perspective with regards to gke knowing the foundation of kubernetes and the different topics of kubernetes is a must in order to understand the abstractions that take place with gke from regular kubernetes in this lesson i will be getting into key topics with regards to kubernetes and we'll be touching on the architecture components and how they all work together to achieve the desired state for your containerized workloads now there's a lot to get into so with that being said let's dive in now before i can get into gke i need to set the stage on explaining what kubernetes is put simply kubernetes is an orchestration platform for containers which was invented by google and eventually open source it is now maintained by the cncf short for the cloud native computing foundation and has achieved incredible widespread adoption kubernetes provides a platform to automate schedule and run containers on clusters of physical or virtual machines thus eliminating many of the manual processes involved in deploying and scaling containerized applications kubernetes manages the containers that run the applications and ensure that there is no downtime in a way that you the user can define for example if you define that when a container goes down and another container needs to start kubernetes would take care of that for you automatically and seamlessly kubernetes provides you with the framework to run distributed systems resiliently it takes care of scaling and failover for your application provides deployment patterns and allows you to manage your applications with tons of flexibility reliability and power it works with a range of container tools including docker now although this adoption was widespread it did come with its various challenges this included scaling at cd load balancing availability auto scaling networking rollback on faulty deployments and so much more so now google cloud has since developed a managed offering for kubernetes providing a managed environment for deploying managing and scaling your containerized applications using google infrastructure the gke environment consists of compute engine instances grouped together to form a cluster and it provides all the same benefits as onpremises kubernetes yet has abstracted the complexity of having to worry about the hardware and to top it off it has the benefits of advanced cluster management features that google cloud provides with things like cloud load balancing and being able to spread traffic amongst clusters and nodes node pools to designate subnets of nodes within a cluster for additional flexibility automatic scaling of your cluster's node instance count and automatic upgrades for your clusters node software it also allows you to maintain node health and availability with node auto repair and takes care of logging and monitoring with google cloud's operation suite for visibility into your cluster so as you can see here gke holds a lot of benefits when it comes to running kubernetes in google cloud so i wanted to take a moment now to dive into the cluster architecture and help familiarize you with all the components involved in a cluster so a cluster is the foundation of google kubernetes engine and kubernetes as a whole the kubernetes objects that represent your containerized applications all run on top of the cluster in gke a cluster consists of at least one control plane and multiple worker machines called nodes the control plane and node machines run the kubernetes cluster the control plane is responsible to coordinate the entire cluster and this can include scheduling workloads like containerized applications and managing the workload's life cycle scaling and upgrades the control plane also manages network and storage resources for those workloads and most importantly it manages the state of the cluster and make sure it is at the desired state now the nodes are the worker machines that run your containerized applications and other workloads the nodes are compute engine vm instances that gke creates on your behalf when you create a cluster each node is managed from the control plane which receives updates on each node's selfreported status a node also runs the services necessary to support the docker containers that make up your cluster's workloads these include the docker runtime and the kubernetes node agent known as the cubelet which communicates with the control plane and is responsible for starting and running docker containers scheduled on that node now diving deeper into the architecture there are components within the control plane and nodes that you should familiarize yourself with as these components are what ties the cluster together and helps manage the orchestration as well as the state now the control plane is the unified endpoint for your cluster the control plane's components make global decisions about the cluster for example scheduling as well as detecting and responding to cluster events all interactions with the cluster are done via kubernetes api calls and the control plane runs the kubernetes api server process to handle those requests you can make kubernetes api calls directly via http or grpc or can also be done indirectly by running commands from the kubernetes command line client called cubectl and of course you can interact with the ui in the cloud console the api server process is the hub for all communications for the cluster moving on to the next component is cube scheduler the cube scheduler is a component that discovers and assigns newly created pods to a node for them to run on so any new pods that are created will automatically be assigned to its appropriate node by the cube scheduler taking into consideration any constraints that are in place next up is the cube controller manager and this is the component that runs controller processes and is responsible for things like noticing and responding when nodes go down maintaining the correct number of pods populating the services and pods as well as creating default accounts and api access tokens for new namespaces it is these controllers that will basically look to make changes to the cluster when the current state does not meet the desired state now when it comes to the cloud controller manager this is what embeds cloudspecific control logic the cloud controller manager lets you link your cluster into any cloud providers api and separates out the components that interact with that cloud platform from components that just interact with your cluster the cloud controller manager only runs controllers that are specific to your cloud provider in this case google cloud and lastly we have fcd and this component is responsible to store the state of the cluster at cd is a consistent and highly available key value store that only interacts with the api server it saves all the configuration data along with what nodes are part of the cluster and what pods they are running so now the control plane needs a way to interact with the nodes of the cluster thus the nodes having components themselves for this communication to occur this component is called a cubelet and this is an agent that runs on each node in the cluster that communicates with the control plane it is responsible for starting and running docker containers scheduled on that node it takes a set of pod specs that are provided to it and ensures that the containers described in those pod specs are running and healthy and i will be diving into pod specs in a later lesson next up is cube proxy and this is the component that maintains network connectivity to the pods in a cluster and lastly the container runtime is the software that is responsible for running containers kubernetes supports container runtimes like docker and container d and so these are the main components in a cluster covering the control plane and nodes with regards to communication within the cluster now before i end this lesson there is one more topic i wanted to touch on with regards to the architecture of a gke cluster and that is the abstraction that happens and what exactly does gke manage with regards to kubernetes well gke manages all the control plane components the endpoint exposes the kubernetes api server that cubectl uses to communicate with your cluster control plane the endpoint exposes the kubernetes api server that cubectl uses to communicate with your cluster control plane the endpoint ip is displayed in cloud console and this ip will allow you to interact with the cluster when you run the command gcloud container clusters get dash credentials you see that the command gets the cluster endpoint as part of updating cubeconfig an ip address for the cluster is then exposed to interact with and is responsible for provisioning and managing all the infrastructure that is needed for the control plane gke also automates the kubernetes nodes by launching them as compute engine vms under the hood but still allows the user to change the machine type and access upgrade options by default google kubernetes engine clusters and node pools are upgraded automatically by google but you can also control when auto upgrades can and cannot occur by configuring maintenance windows and exclusions and just as a note a clusters control plane and nodes do not necessarily run the same version at all times and i will be digging more into that in a later lesson and so i know this is a lot of theory to take in but is as i said before a necessity to understanding kubernetes and gke and as we go further along into kubernetes and get into demos i promise that this will start to make a lot more sense and you will start becoming more comfortable with gke and the underlying components of kubernetes knowing kubernetes is a must when working in any cloud environment as it is a popular and growing technology that is not slowing down so knowing gke will put you in a really good position for your career as an engineer in google cloud as well will give you a leg up on diving into other cloud vendors implementation of kubernetes and so that's pretty much all i wanted to cover when it comes to google kubernetes engine and kubernetes so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i will be covering cluster and node management in gke as it refers to choosing different cluster types for your workloads cluster versions node pools as well as upgrades and the many different options to choose from it is good to familiarize yourself with these options as they may be the deciding factor of having to keep your workloads highly available and your tolerance to risk within your environment so with that being said let's dive in now in the last lesson we touched on nodes and how they are the workers for the kubernetes cluster so now that you are familiar with nodes i wanted to touch on a concept that builds on it called node pools now a node pool is a group of nodes within a cluster that all have the same configuration and using node config specification to achieve this a node pool can also contain one or multiple nodes when you first create a cluster the number and type of nodes that you specify becomes the default node pool as shown here in the diagram then you can add additional custom node pools of different sizes and types to your cluster all nodes in any given node pool are identical to one another now custom node pools are really useful when you need to schedule pods that require more resources than others such as more memory more disk space or even different machine types you can create upgrade and delete node pools individually without affecting the whole cluster and just as a note you cannot configure a single node in any node pool any configuration changes affect all nodes in the node pool and by default all new node pools run the latest stable version of kubernetes existing node pools can be manually upgraded or automatically upgraded you can also run multiple kubernetes node versions on each node pool in your cluster update each node pool independently and target different node pools for specific deployments in that node now with gke you can create a cluster tailored to your availability requirements and your budget the types of available clusters include zonal both single zone or multizonal and regional zonal clusters have a single control plane in a single zone depending on what kind of availability you want you can distribute your nodes for your zonal cluster in a single zone or in multiple zones now when you decide to deploy a single zone cluster it again has a single control plane running in one zone this control plane manages workloads on nodes running in the same zone a multizonal cluster on the other hand has a single replica of the control plane running in a single zone and has nodes running in multiple zones during an upgrade of the cluster or an outage of the zone where the control plane runs workloads still run however the cluster its nodes and its workloads cannot be configured until the control plane is available multizonal clusters are designed to balance availability and cost for consistent workloads and just as a note the same number of nodes will be deployed to each selected zone and may cost you more than budgeted so please be aware and of course when you're looking to achieve high availability for your cluster regional clusters are always the way to go a regional cluster has multiple replicas of the control plane running in multiple zones within a given region nodes also run in each zone where a replica of the control plane runs because a regional cluster replicates the control plane and nodes it consumes more compute engine resources than a similar single zone or multizonal cluster the same number of nodes will be deployed to each selected zone and the default when selecting regional clusters is three zones now if you're dealing with more sensitive workloads that require more strict guidelines private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet this isolation is achieved as the nodes have internal ip addresses only if you want to provide outbound internet access for certain private nodes you can use cloudnat or manage your own nat gateway by default private google access is enabled in private clusters and their workloads with limited outbound access to google cloud apis and services over google's private network in private clusters the control plane's vpc network is connected to your clusters vpc network with vpc network peering your vpc network contains the cluster nodes and a separate google cloud vpc network contains your cluster's control plane the control plane's vpc network is located in a project controlled by google traffic between nodes and the control plane is routed entirely using internal ip addresses the control plane for a private cluster has a private endpoint in addition to a public endpoint the control plane for a nonprivate cluster only has a public endpoint the private endpoint is an internal ip address in the control plane's vpc network the public endpoint is the external ip address of the control plane and you can control access to this endpoint using authorized networks or you can disable access to the public endpoint as shown here in the diagram you can disable the public endpoint and connect to your network using an internal ip address using cloud interconnect or cloud vpn and you always have the option of enabling or disabling this public endpoint now when you create a cluster you can choose the cluster specific kubernetes version or you can mix the versions for flexibility on features either way it is always recommended that you enable auto upgrade for the cluster and its nodes now when you have auto upgrade enabled you are given the choice to choose from what are called release channels when you enroll a new cluster in a release channel google automatically manages the version and upgrade cadence for the cluster and its node pools all channels offer supported releases of gke and are considered in general availability you can choose from three different release channels for automatic management of your cluster's version and upgrade cadence as shown here the available release channels are rapid regular and stable release channels the rapid release channel gets the latest kubernetes release as early as possible and be able to use new gka features the moment that they go into general availability with the regular release channel you have access to gke and kubernetes features reasonably soon after they are released but on a version that has been qualified two to three months after releasing in the rapid release channel and finally we have the stable release channel where stability is prioritized over new functionality changes and new versions in this channel are rolled out last after being validated two to three months in the regular release channel and so if you're looking for more direct management of your cluster's version choose a static version when you enroll a cluster in a release channel that cluster is upgraded automatically when a new version is available in that channel now if you do not use a release channel or choose a cluster version the current default version is use the default version is selected based on usage and real world performance and is changed regularly while the default version is the most mature one other versions being made available are generally available versions that pass internal testing and qualification changes to the default version are announced in a release note now if you know that you need to use a specific supported version of kubernetes for a given workload you can specify it when creating the cluster if you do not need to control the specific patch version you use consider enrolling your cluster in a release channel instead of managing its version directly now when it comes to upgrading the cluster please be aware that control plane and nodes do not always run the same version at all times as well a control plane is always upgraded before its nodes when it comes to zonal clusters you cannot launch or edit workloads during that upgrade and with regional clusters each control plane is upgraded one by one as well with control planes auto upgrade is enabled by default and this is google cloud's best practice now again if you choose you can do a manual upgrade but you cannot upgrade the control plane more than one minor version at a time so please be aware as well with any cluster upgrades maintenance windows and exclusions are available and so this way you can choose the best times for your upgrades and so like cluster upgrades by default a clusters nodes have auto upgrade enabled and it is recommended that you do not disable it again this is best practice by google cloud and again like the cluster upgrades a manual upgrade is available and maintenance windows and exclusions are available for all of these upgrades now when a no pool is upgraded gke upgrades one node at a time while a node is being upgraded gke stops scheduling new pods onto it and attempts to schedule its running pods onto other nodes the node is then recreated at the new version but using the same name as before this is similar to other events that recreate the node such as enabling or disabling a feature on the node pool and the upgrade is only complete when all nodes have been recreated and the cluster is in the desired state when a newly upgraded node registers with the control plane gke marks the node as schedulable upgrading a no pool may disrupt workloads running in that pool and so in order to avoid this you can create a new node pool with the desired version and migrate the workload then after migration you can delete the old node pool now surge upgrades let you control the number of nodes gke can upgrade at a time and control how disruptive upgrades are to your workloads you can change how many nodes gke attempts to upgrade at once by changing the surge upgrade parameters on a no pool surge upgrades reduce disruption to your workloads during cluster maintenance and also allow you to control the number of nodes upgraded in parallel surge upgrades also work with the cluster auto scaler to prevent changes to nodes that are being upgraded now surge upgrade behavior is determined by two settings max surge upgrade and max unavailable upgrade now with max surge upgrade this is the number of additional nodes that can be added to the no pool during an upgrade increasing max surge upgrade raises the number of nodes that can be upgraded simultaneously and when it comes to the max unavailable upgrade this is the number of nodes that can be simultaneously unavailable during an upgrade increasing max unavailable upgrade raises the number of nodes that can be upgraded in parallel so with max surge upgrade the higher the number the more parallel upgrades which will end up costing you more money with max unavailable upgrade the higher the number the more disruptive it is and so the more risk you are taking and so during upgrades gke brings down at most the sum of the max surge upgrade added with the max unavailable upgrade so as you can see here there are a slew of options when it comes to deciding on the type of cluster you want as well as the type of upgrades that are available along with when you want them to occur and so your deciding factor in the end will be the workload that you are running and your risk tolerance and this will play a big factor in keeping up time for your cluster as well as saving money in any type of environment and so that's pretty much all i wanted to cover when it comes to gke cluster and node management so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i will be diving into some more theory within kubernetes and gke this time touching on objects and how objects are managed pods are only one type of object but there are many other parts that are involved in the management of these objects and this is what this lesson is set out to teach you now there's quite a bit to cover here so with that being said let's dive in now kubernetes objects are persistent entities in kubernetes kubernetes uses these entities to represent the state of your cluster for example it can describe things like what containerized applications are running and on which nodes and what resources are available to those applications a kubernetes object is a record of intent once you create the object kubernetes will constantly work to ensure that object exists by creating an object you're effectively telling kubernetes what you want your cluster's workload to look like and this is your cluster's desired state and you've heard me speak about this many times before and this is what i was referring to now almost every kubernetes object includes two nested object fields that govern the object's configuration the object spec and the object's status for objects that have a spec you have to set this when you create the object providing a description of the characteristics you want the resource to have its desired state the status describes the current state of the object supplied and updated by kubernetes and its components the kubernetes control plane continually and actively manages every object's actual state to match the desired state you supplied now each object in your cluster has a name that is unique for that type of resource every kubernetes object also has a uid that is unique across your whole cluster only one object of a given kind can have a given name at a time however if you delete the object you can make a new object with that same name every object created over the whole lifetime of a kubernetes cluster has a distinct uid these distinct uids are also known as uuids which we discussed earlier on in the course now when creating updating or deleting objects in kubernetes this is done through the use of a manifest file where you would specify the desired state of an object that kubernetes will maintain when you apply the manifest each configuration file can contain multiple manifests and is common practice to do so when possible a manifest file is defined in the form of a yaml file or a json file and it is recommended to use yaml now in each yaml file for the kubernetes object that you want to create there are some required values that need to be set the first one is the api version and this defines which version of the kubernetes api you're using to create this object the kind described in this example as a pod is the kind of object you want to create next up is the metadata and this is the data that helps uniquely identify the object including a string name a uid and an optional namespace and the last required value is the spec and this is what state you desire for the object and the spec in this example is a container by the name of bow tie dash web server and is to be built with the latest nginx web server image as well as having port 80 open on the container now when it comes to objects pods are the smallest most basic deployable objects in kubernetes a pod represents a single instance of a running process in your cluster pods contain one or more containers such as docker containers and when a pod runs multiple containers the containers are managed as a single entity and share the pods resources which also includes shared networking and shared storage for their containers generally one pod is meant to run a single instance of an application on your cluster which is selfcontained and isolated now although a pod is meant to run a single instance of your application on your cluster it is not recommended to create individual pods directly instead you generally create a set of identical pods called replicas to run your application a set of replicated pods are created and managed by a controller such as a deployment controllers manage the life cycle of their pods as well as performing horizontal scaling changing the number of pods is necessary now although you might occasionally interact with pods directly to debug troubleshoot or inspect them it's recommended that you use a controller to manage your pods and so once your pods are created they are then run on nodes in your cluster which we discussed earlier the pod will then remain on its node until its process is complete the pot is deleted the pod is evicted from the node due to lack of resources or the node fails if a node fails pods on the node are automatically scheduled for deletion now a single gke cluster should be able to satisfy the needs of multiple users or groups of users and kubernetes namespaces help different projects teams or customers to share a kubernetes cluster you can think of a namespace as a virtual cluster inside of your kubernetes cluster and you can have multiple namespaces logically isolated from each other they can help you and your teams with organization and security now you can name your namespaces whatever you'd like but kubernetes starts with four initial namespaces the first one is the default namespace and this is for objects with no other namespace so when creating new objects without a namespace your object will automatically be assigned to this namespace cube dash system is the next one and these are for objects created by kubernetes cubepublic is created automatically and is readable by all users but is mostly reserved for cluster usage in case that some resources should be visible and readable publicly throughout the whole cluster and finally cube node lease is the namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales and so like most resources in google cloud labels are key value pairs that help you organize your resources in this case kubernetes objects labels can be attached to objects at creation time and can be added or modified at any time each object can have a set of key value labels defined and each key must be unique for a given object and labels can be found under metadata in your manifest file and so the one thing to remember about pods is that they are ephemeral they are not designed to run forever and when a pod is terminated it cannot be brought back in general pods do not disappear until they are deleted by a user or by a controller pods do not heal or repair themselves for example if a pod is scheduled on a node which later fails the pod is deleted as well if a pod is evicted from a node for any reason the pod does not replace itself and so here is a diagram of a pod life cycle that shows the different phases of its running time to give you some better clarity of its ephemeral nature when first creating the pod the pod will start impending and this is the pod's initial phase and is waiting for one or more of the containers to be set up and made ready to run this includes the time a pod spends waiting to be scheduled as well as the time spent downloading container images over the network once the pod has completed the pending phase it is moved on to be scheduled and once it is scheduled it will move into the running phase and this is the phase where the pod has been bound to a node and all of the containers have been created the running phase has at least one container in the pod running or is in the process of starting or restarting and once the workload is complete the pod will move into the succeeded phase and this is where all the containers in the pod have terminated in success and will not be restarted now if all the containers in the pod have not terminated successfully the pod will move into a failed phase and this is where all the containers in the pod have terminated and at least one container has terminated in failure now there's one more phase in the pod life cycle that i wanted to bring up which is the unknown phase and this is the state of the pod that could not be obtained this phase typically occurs due to an error in communicating with the node where the pod should be running so now when you're creating pods using a deployment is a common way to do this a deployment runs multiple replicas of your application and automatically replaces any instances that fail or become unresponsive deployments help ensure that one or more instances of your application are available to serve user requests deployments use a pod template which contains a specification for its pods the pod specification determines how each pod should look like for instance what applications should run inside its containers which volumes the pods should mount its labels and more and so when a deployments pod template is changed new pods are automatically created one at a time now i wanted to quickly bring up replica sets for just a moment you'll hear about replica sets and i wanted to make sure that i covered it replica sets ensures that a specified number of pod replicas are running at any given time however a deployment is a higher level concept that manages replica sets and provides updates to pods along with other features and so using deployments is recommended over using replica sets unless your workload requires it and i will be including a link to replica sets in the lesson text so speaking of workloads in kubernetes workloads are objects that set deployment rules four pods based on these rules kubernetes performs the deployment and updates the workload with the current state of the application workloads let you define the rules for application scheduling scaling and upgrading now deployments which we just discussed is a type of workload and as we've seen a deployment runs multiple replicas of your application and automatically replaces any instances that fail or become unresponsive deployments are best used for stateless applications another type of workload is stateful sets and in contrast to deployments these are great for when your application needs to maintain its identity and store data so basically any application that requires some sort of persistent storage daemon sets is another common workload that ensures every node in the cluster runs a copy of that pod and this is for use cases where you're collecting logs or monitoring node performance now jobs is a workload that launches one or more pods and ensures that a specified number of them successfully terminate jobs are best used to run a finite task to completion as opposed to managing an ongoing desired application state and cron jobs are similar to jobs however cron jobs runs to completion on a cronbased schedule and so the last workload that i wanted to cover are config maps and these store general configuration information and so after you upload a config map any workload can reference it as either an environment variable or a volume mount and so just as a note config maps are not meant to store sensitive data if you're planning to do this please use secrets now i know this lesson has been extremely heavy in theory but these are fundamental concepts to know when dealing with kubernetes and gke as well as the objects that it supports so i recommend that if you need to go back and review this lesson if things aren't making sense so that you can better understand it as these concepts all tie in together and will come up in the exam and so that's pretty much all i wanted to cover in this lesson on pods and object management within gke so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be diving into kubernetes services now services are a major networking component when it comes to working in kubernetes and can play a major factor when it comes to deciding on how you want to route your traffic within your kubernetes cluster as well in my experience services show up on the exam and so an understanding of how they work and the different types to use are essential to understanding the big picture of kubernetes this lesson will cover an overview on what services are what they do and the different types that are available along with their use cases now there's a lot to cover here so with that being said let's dive in now as i had discussed earlier kubernetes pods are ephemeral pods are created and destroyed to match the state of your cluster so these resources are never permanent a perfect example of this is by using a deployment object so you can create and destroy pods dynamically now when it comes to networking in kubernetes each pod gets its own ip address however in a deployment a pod that is running once destroyed will be recreated with a new ip address and there is no real way to keep track of these i p addresses for communication as they change very frequently and this is where services come into play now a service is an abstraction in the sense that it is not a process that listens on some network interface a service can be defined as a logical set of pods an abstraction on top of the pod which provides a single persistent ip address and dns name by which pods can be accessed it allows for routing external traffic into your kubernetes cluster and used inside your cluster for more intelligent routing with services it is also very easy to manage load balancing configuration for traffic between replicas it helps pods scale quickly and easily as the service will automatically handle the recreation of pods and their new ip addresses the main goal of services in kubernetes is to provide persistent access to its pods without the necessity to look for a pod's ip each time when the pod is recreated and again services also allow for external access from users to the applications inside the cluster without having to know the ip address of the individual pod in order to reach that application now in order for a service to route traffic to the correct pod in the cluster there are some fields in the manifest file that will help determine the end points on where traffic should be routed shown here on the right is the deployment manifest for reference and on the left is the services manifest now as you can see here in the service manifest on the left the kind is clearly defined as service under metadata is the name of the service and this will be the dns name of the service when it is created so when it comes to the spec there is a field here called a selector and this is what defines what pods should be included in the service and it is the labels under the selector that define which pods and labels are what we discussed in the last lesson as arbitrary key value pairs so any pod with these matching labels is what will be added to the service as shown here in the deployment file this workload will be a part of the service and its labels match that of the selector in the services file for type this is the type of service that you will want to use in this example type cluster ip is used but depending on the use case you have a few different ones to choose from now at the bottom here is a list of port configurations protocol being the network protocol to use with the port port being the port that incoming traffic goes to and finally the target port which is the port on the pod that traffic should be sent to and this will make more sense as we go through the upcoming diagrams so touching on selectors and labels for a moment kubernetes has a very unique way of routing traffic and when it comes to services it's not any different services select pods based on their labels now when a selector request is made to the service it selects all pods in the cluster matching the key value pair under the selector it chooses one of the pods if there are more than one with the same key value pair and forwards the network request to it and so here in this example you can see that the selector specified for the service has a key value pair of app inventory you can see the pod on node 1 on the left holds the label of app inventory as well which matches the key value pair of the selector and so traffic will get routed to that pod because of it if you look at the label for the pod in node 2 on the right the label does not match that of the selector and so it will not route traffic to that pod and so to sum it up the label on the pod matching the selector in the service determines where the network request will get routed to and so now i will be going through the many different service types that are available for routing network traffic within gke starting with cluster ip now a cluster ip service is the default kubernetes service it gives you a service inside your cluster that other apps inside your cluster can access the service is not exposed outside the cluster but can be addressed from within the cluster when you create a service of type cluster ip kubernetes creates a stable ip address that is accessible from nodes in the cluster clients in the cluster call the service by using the cluster ip address and the port value specified in the port field of the service manifest the request is forwarded to one of the member pods on the port specified in the target port field and just as a note this ip address is stable for the lifetime of the service so for this example a client calls the service at 10.176 on tcp port 80. the request is forwarded to one of the member pods on tcp port 80. note that the member pod must have a container that is listening on tcp port 80. if there is no container listening on port 80 clients will see a message like fail to connect or this site can't be reached think of the case when you have a dns record that you don't want to change and you want the name to resolve to the same ip address or you merely want a static ip address for your workload this would be a great use case for the use of the cluster ip service now although the service is not accessible by network requests outside of the cluster if you need to connect to the service you can still connect to it with the cloud sdk or cloud shell by using the exposed ip address of the cluster and so i wanted to take a moment to show you what a cluster ip manifest actually looks like and i will be going through the manifest for each service type for you to familiarize yourself with we first have the name of the service which is cluster ip dash service we then have the label used for the selector which is the key value pair of app inventory and then we have the service type which is cluster ip and we have the port number exposed internally in the cluster which is port 80 along with the target port that containers are listening on which again is port 80. and so the next service type we have is node port so when you create a service of type node port you specify a node port value the node port is a static port and is chosen from a preconfigured range between 30 000 and 32 760 you can specify your own value within this range but please note that any value outside of this range will not be accepted by kubernetes as well if you do not choose a value a random value within the range specified will be assigned once this port range has been assigned to the service then the service is accessible by using the ip address of any node along with the no port value the service is then exposed on a port on every node in the cluster the service can then be accessed externally at the node ip along with the node port when using node port services you must make sure that the selected port is not already open on your nodes and so just as a note the no port type is an extension of the cluster i p type so a service of type node port naturally has a cluster i p address and so this method isn't very secure as it opens up each node to external entry as well this method relies on knowing the ip addresses of the nodes which could change at any time and so going through the manifest of type node port service we start off with the name of the service which is node port dash service the label used for the selector which uses the key value pair of app inventory the type which is node port and notice the case sensitivity here which you will find in most service types along with the port number exposed internally in the cluster which is port 80 and again the port that the containers are listening on which is the target port which is port 80 as well and lastly and most importantly we have the no port value which is marked as you saw in the diagram earlier as port 32002 the next service type we have up is low balancer and this service is exposed as a load balancer in the cluster low balancer services will create an internal kubernetes service that is connected to a cloud provider's load balancer and in this case google cloud this will create a static publicly addressable ip address and a dns name that can be used to access your cluster from an external source the low balancer type is an extension of the no port type so a service of type load balancer naturally has a cluster ip address if you want to directly expose a service this is the default method all traffic on the port you specify will be forwarded to the service there is no filtering or routing and it means you can send many different types of traffic to it like http https tcp or udp and more the downside here is that for each service you expose with a low balancer you pay for that load balancer and so you can really rack up your bill if you're using multiple load balancers and shown here is the manifest for type load balancer it shows the name of the service load balancer dash service the label which is used for the selector which is the key value pair of app inventory the service type which is low balancer again notice the case sensitivity along with the port and the target port which are both port 80. and so this is the end of part one of this lesson it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready part two will be starting immediately from the end of part one so go ahead and mark this as complete and i'll see you in the next one welcome back this is part two of the kubernetes services lesson and we're going to continue immediately from the end of part one so whenever you're ready let's dive in and so the next service type we have is multiport services now for some services there is the need to expose more than one port kubernetes lets you configure multiple port definitions on a service object so when using multiple ports for a service you must give all your ports names and if you have multiple service ports these names must be unique in this example if a client calls the service at 10.176.1 on tcp port 80 the request is forwarded to a member pod on tcp port 80 on either node 1 or node 2. but if a client calls the service at 10.176.133.7 on tcp port 9752 the request is forwarded to the pod on tcp port 9752 that resides on node 1. each member pod must have a container listening on tcp port 80 and a container listening on tcp port 9752 this could be a single container with two threads or two containers running in the same pod and of course as shown here is a manifest showing the multiport services the name of the service the label used for the selector as well as the service type the port node exposed internally for each separate workload as well as the port that containers are listening on for each workload as well and as you saw before nginx was using target port 80 where appy was using port 9752 moving on to another service type is external name now a service of type external name provides an internal alias for an external dns name internal clients make requests using the internal dns name and the requests are redirected to the external name when you create a service kubernetes creates a dns name that internal clients can use to call the service in this example the internal dns name is bowtie.sql when an internal client makes a request to the internal dns name of bowtie.sql the request gets redirected to bowtie.sql2 dot bow tie inc dot private the external name service type is a bit different than other service types as it's not associated with a set of pods or an ip address it is a mapping from an internal dns name to an external dns name this service does a simple cname redirection and is a great use case for any external service that resides outside of your cluster and again here is a view of a manifest for type external name here showing the internal dns name along with the external dns name redirect and moving on to the last service type we have the headless service type now sometimes you don't need or want low balancing and a single service ip in this case you can create headless services by specifying none as the service type in the manifest file this option also allows you to choose other service discovery mechanisms without being tied to kubernetes implementation applications can still use a selfregistration pattern with this service and so a great use case for this is when you don't need any low balancing or routing you only need the service to patch the request to the back end pod no ips needed headless service is typically used with stateful sets where the name of the pods are fixed this is useful in situations like when you're setting up a mysql cluster where you need to know the name of the master and so here is a manifest for the headless service again the service type is marked as none and so to sum it up kubernetes services provides the interfaces through which pods can communicate with each other they also act as the main gateway for your application services use selectors to identify which pods they should control they expose an ip address and a port that is not necessarily the same port at which the pod is listening and services can expose more than one port and can also route traffic to other services external ip addresses or dns names services make it really easy to create network services in kubernetes each service can be backed with as many pods as needed without having to make your code aware of how each service is backed also please note that there are many other features and use cases within the services that have been mentioned that i've not brought up i will also include some links in the lesson text for those who are interested in diving deeper into services this lesson was to merely summarize the different service types and knowing these service types will put you in a great position on the exam for any questions that cover services within gke now i know this has been another lesson that's been extremely heavy in theory and has been a tremendous amount to take in but not to worry next up is a demo that will put all this theory into practice and we'll be going ahead and building a cluster along with touching on much of the components discussed within the past few lessons and so that's pretty much all i wanted to cover when it comes to kubernetes service types so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this lesson i'll be going over ingress for gke an object within gke that defines rules for routing traffic to specific services ingress is a wellknown topic that comes up in the exam as well as being a common resource that is used in many gke clusters that you will see in most environments something that you will get very familiar with while diving deeper into more complex environments so whenever you're ready let's dive in now in gke an ingress object defines rules for routing http and https traffic to applications running in a cluster an ingress object is associated with one or more service objects each of which is associated with a set of pods when you create an ingress object the gke ingress controller creates a google cloud http or https load balancer and configures it according to the information in the ingress and its associated services gke ingress is a builtin and managed ingress controller this controller implements ingress resources as google cloud load balancers for http and https workloads in gke also the load balancer is given a stable ip address that you can associate with a domain name each external http and https load balancer or internal http or https load balancer uses a single url map which references one or more backend services one backend service corresponds to each service referenced by the ingress in this example assume that you have associated the load balancers ip address with the domain name bowtieinc.co when a client sends a request to bowtieinc.co the request is routed to a kubernetes service named products on port 80. and when a client sends a request to bowtieinc.co forward slash discontinued the request is routed to a kubernetes service named discontinued on port 21337 ingress is probably the most powerful way to expose your services but can also be very complex as there are also many types of ingress controllers to choose from along with plugins for ingress controllers ingress is the most useful and cost effective if you want to expose multiple services under the same ip address as you only pay for one load balancer if you are using the native gcp integration and comes with a slew of features and so shown here is the ingress manifest which is a bit different from the other manifest that you've seen as it holds rules for different paths explain in the previous diagram in the manifest shown here one path directs all traffic to the product's service name while the other path redirects traffic from discontinued to the back end service name of discontinued and note that each of these service names have their own independent manifest as it is needed to create the service and are referenced within the ingress manifest so the more rules you have for different paths or ports the more services you will need now i wanted to touch on network endpoint groups or any g's for short for just a second now this is a configuration object that specifies a group of backend endpoints or services negs are useful for container native load balancing where each container can be represented as an endpoint to the load balancer the negs are used to track pod endpoints dynamically so the google low balancer can route traffic to its appropriate back ends so traffic is low balanced from the load balancer directly to the pod ip as opposed to traversing the vm ip and coupe proxy networking in these conditions services will be annotated automatically indicating that a neg should be created to mirror the pod ips within the service the neg is what allows compute engine load balancers to communicate directly with pods the diagram shown here is the ingress to compute engine resource mappings of the manifest that you saw earlier where the gke ingress controller deploys and manages compute engine low balancer resources based on the ingressed resources that are deployed in the cluster now touching on health checks for just a minute if there are no specified health check parameters for a corresponding service using a backend custom resource definition a set of default and inferred parameters are used health check parameters for a backend service should be explicitly defined by creating a backend config custom resource definition for the service and this should be done if you're using anthos a backend config custom resource definition should also be used if you have more than one container in the serving pods as well if you need control over the port that's used for the low balancers health checks now you can specify the backend services health check parameters using the health check parameter of a backend config custom resource definition referenced by the corresponding service this gives you more flexibility and control over health checks for a google cloud external http or https load balancer or internal http or https load balancer created by an ingress and lastly i wanted to touch on ssl certificates and there are three ways to provide ssl certificates to an http or https load balancer the first way is google managed certificates and these are provisioned deployed renewed and managed for your domains and just as a note managed certificates do not support wildcard domains the second way to provide ssl certificates is through selfmanaged certificates that are shared with google cloud you can provision your own ssl certificate and create a certificate resource in your google cloud project you can then list the certificate resource in an annotation on an ingress to create an http or https load balancer that uses the certificate and the last way to provide ssl certificates is through selfmanaged certificates as secret resources so you can provision your own ssl certificate and create a secret to hold it you can then refer to the secret as an ingress specification to create an http or https load balancer that uses this certificate and just as a note you can specify multiple certificates in an ingress manifest the load balancer chooses a certificate if the common name in the certificate matches the host name used in the request and so that pretty much covers all the main topics in this short lesson on ingress for gke so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'll be going over gke storage options now kubernetes currently offers a slew of different storage options and is only enhanced by the added features available in google cloud for gke we'll also be getting into the different abstractions that kubernetes offers to manage storage and how they can be used for different types of workloads now there's quite a bit to go over here so with that being said let's dive in now as i stated before there are several storage options for applications running on gke the choices vary in terms of flexibility and ease of use google cloud offers several storage options that can be used for your specific workload kubernetes also provides storage abstractions which i will be getting into in just a bit the easiest storage options are google cloud's managed storage products if you need to connect a database to your cluster you can consider using cloud sql datastore or cloud spanner and when it comes to object storage cloud storage would be an excellent option to fill the gap file store is a great option for when your application requires managed network attached storage and if your application requires block storage the best option is to use persistent disks and can be provisioned manually or provisioned dynamically through kubernetes now i wanted to first start off with kubernetes storage abstractions but in order to understand kubernetes storage abstractions i wanted to take a moment to explain how storage is mounted in the concept of docker now docker has a concept of volumes though it is somewhat looser and less managed than kubernetes a docker volume is a directory on disk or in another container docker provides volume drivers but the functionality is somewhat limited a docker container has a writable layer and this is where the data is stored by default making the data ephemeral and so data is not persisted when the container is removed so storing data inside a container is not always recommended now there are three ways to mount data inside a docker container the first way is a docker volume and sits inside the docker area within the host's file system and can be shared amongst other containers this volume is a docker object and is decoupled from the container they can be attached and shared across multiple containers as well bind mounting is the second way to mount data and is coming directly from the host's file system bind mounts are great for local application development yet cannot be shared across containers and the last way to mount data is by using tempfs and is stored in the host's memory this way is great for ephemeral data and increases performance as it no longer lies in the container's writable layer now with kubernetes storage abstractions file system and block based storage are provided to your pods but are different than docker in nature volumes are the basic storage unit in kubernetes that decouples the storage from the container and tie it to the pod and not the container like in docker a regular volume simply called volume is basically a directory that the containers in a pod have access to the particular volume type used is what will determine its purpose some volume types are backed by ephemeral storage like empty dir config map and secrets and these volumes do not persist after the pod ceases to exist volumes are useful for caching temporary information sharing files between containers or to load data into a pod other volume types are backed by durable storage and persist beyond the lifetime of a pod like persistent volumes and persistent volume claims a persistent volume is a cluster resource that pods can use for durable storage a persistent volume claim can be used to dynamically provision a persistent volume backed by persistent disks persistent volume claims can also be used to provision other types of backing storage like nfs and i will be getting more into persistent volumes and persistent volume claims in just a bit now as you saw in docker on disk files in a container are the simplest place for an application to write data but files are lost when the container crashes or stops for any other reason as well as being unaccessible to other containers running in the same pod in kubernetes the volume source declared in the pod specification determines how the directory is created the storage medium used and the directory's initial contents a pod specifies what volumes it contains and the path where containers mount the volume ephemeral volume types live the same amount of time as the pods they are connected to these volumes are created when the pod is created and persist through container restarts only when the pod terminates or is deleted are the volumes terminated as well other volume types are interfaces to durable storage that exist independently of a pod like ephemeral volumes data in a volume backed by durable storage is preserved when the pod is removed the volume is merely unmounted and the data can be handed off to another pod now volumes differ in their storage implementation and their initial contents you can choose the volume source that best fits your use case and i will be going over some common volume sources that are used and you will see in many gke implementations the first volume that i want to bring up is empty dir now an empty dir volume provides an empty directory that containers in the pod can read and write from when the pod is removed from a node for any reason the data in the empty dir is deleted forever an empty dir volume is stored on whatever medium is backing the node which might be a disk ssd or network storage empty der volumes are useful for scratch space and sharing data between multiple containers in a pod the next type of volume that i wanted to go over is config map and config map is a resource that provides a way to inject configuration data into pods the data stored in a config map object can be referenced in a volume of type config map and then consumed through files running in a pod the next volume type is secret and a secret volume is used to make sensitive data such as passwords oauth tokens and ssh keys available to applications the data stored in a secret object can be referenced in a volume of type secret and then consumed through files running in a pod next volume type is downward api and this volume makes downward api data available to applications so this data includes information about the pod and container in which an application is running in an example of this would be to expose information about the pods namespace and ip address to applications and the last volume type that i wanted to touch on is persistent volume claim now a persistent volume claim volume can be used to provision durable storage so that they can be used by applications a pod uses a persistent volume claim to mount a volume that is backed by this durable storage and so now that i've covered volumes i wanted to go into a bit of detail about persistent volumes persistent volume resources are used to manage durable storage in a cluster in gke a persistent volume is typically backed by a persistent disk or file store can be used as an nfs solution unlike volumes the persistent volume life cycle is managed by kubernetes and can be dynamically provisioned without the need to manually create and delete the backing storage persistent volume resources are cluster resources that exist independently of pods and continue to persist as the cluster changes and as pods are deleted and recreated moving on to persistent volume claims this is a request for and claim to a persistent volume resource persistent volume claim objects request a specific size access mode and storage class for the persistent volume if an existing persistent volume can satisfy the request or can be provisioned the persistent volume claim is bound to that persistent volume and just as a note pods use claims as volumes the cluster inspects the claim to find the bound volume and mounts that volume for the pod now i wanted to take a moment to go over storage classes and how they apply to the overall storage in gke now these volume implementations such as gce persistent disk are configured through storage class resources gke creates a default storage class for you which uses the standard persistent disk type of ext4 as shown here the default storage class is used when a persistent volume claim doesn't specify a storage class name and can also be replaced with one of your choosing you can even create your own storage class resources to describe different classes of storage and is helpful when using windows node pools now as i stated before persistent volume claims can automatically provision persistent disks for you when you create this persistent volume claim object kubernetes dynamically creates a corresponding persistent volume object due to the gke default storage class this persistent volume is backed by a new empty compute engine persistent disk you use this disk in a pod by using the claim as a volume when you delete a claim the corresponding persistent volume object and the provision compute engine persistent disk are also deleted now to prevent deletion you can set the reclaim policy of the persistent disk resource or its storage class resource to retain now deployments as shown here in this diagram are designed for stateless applications so all replicas of a deployment share the same persistent volume claim which is why stateful sets are the recommended method of deploying stateful applications that require a unique volume per replica by using stateful sets with persistent volume claim templates you can have applications that can scale up automatically with unique persistent volume claims associated to each replica pod now lastly i wanted to touch on some topics that will determine the storage access that is available for any gke cluster in your environment now i first wanted to start off with access modes and there are three supported modes for your persistent disks that allow read write access and are listed here read write once is where the volume can be mounted as read write by a single node read only many is where the volume can be mounted as a read only by many nodes and lastly read write many is where the volume can be mounted as read write by many nodes and just as a note read write once is the most common use case for persistent disks and works as the default access mode for most applications next i wanted to touch on the type of persistent disks that are available and the benefits and caveats of access for each now going through the persistent disks lesson of this course you probably know by now about the available persistent disks when it comes to zonal versus regional availability and so this may be a refresher for some now going into regional persistent disks these are multizonal resources that replicate data between two zones in the same region and can be used similarly to zonal persistent disks in the event of a zonal outage kubernetes can fail over workloads using the volume to the other zone regional persistent disks are great for highly available solutions for stateful workloads on gke now zonal persistent disks are zonal resources and so unless a zone is specified gke assigns the disk to a single zone and chooses the zone at random once a persistent disk is provisioned any pods referencing the disk are scheduled to the same zone as the disk and just as a note using antiaffinity on zones allows stateful set pods to be spread across zones along with the corresponding disks and the last point that i wanted to cover when it comes to persistent volume access is the speed of access now as stated in an earlier lesson the size of persistent disks determine the iops and throughput of the disk gke typically uses persistent disks as boot disks and to back kubernetes persistent volumes so whenever possible use larger and fewer disks to achieve higher iops and throughput and so that pretty much covers everything that i wanted to go over in this lesson on gke storage options so you can now mark this lesson as complete and let's move on to the next one welcome back in these next few demos i'm going to be doing a complete walkthrough and putting all the theory we learned into practice through building and interacting with gke clusters and you'll be building and deploying your own containerized application on this cluster called box of bowties so in this demo we're going to be setting up our own gke cluster in the console along with going through all the options that are available when deploying it we're also going to use the command line to configure the cubectl command line tool so that we can interact with the cluster so with that being said let's dive in and so here in the console i am logged in as tonybowties gmail.com under the project of bow tie inc and so before launching the cluster i need to make sure that my default vpc has been created so i'm going to go over to the navigation menu and i'm going to scroll down to vpc network and as expected the default network is here so i can go ahead and create my cluster and so in order to get to my kubernetes engine console i'm going to go up to the navigation menu and i'm going to scroll down under compute and you will find here kubernetes engine and you'll see a few different options to choose from and over here on the left hand menu i will be going through these options in the upcoming demos but for now i want to concentrate on creating our cluster now gk makes things pretty easy as i have the option to create a cluster to deploy a container or even taking the quick start and so we're going to go ahead and click on create our cluster and so here we are prompted with our cluster basics now if i really wanted to i can simply fill out all the fields that you see here and click on create and it will use all the defaults to build my cluster but we're going to customize it a little bit so we're going to go ahead and go through all these options so first under name we're going to name this cluster bowtie dash cluster and so under location type we want to keep things as zonal and if i check off the specify default node locations i'll be able to make this a multizonal cluster as i have the option of selecting from multiple zones where i can situate my nodes and so i can select off a bunch of different zones if i choose but we want to keep it as a single zonal cluster and so i'm going to check these all off and under zone i'm going to click on the drop down menu and i'm going to select us east 1b and just as a note for each zone that you select this is where the control plane will live so if i was to create a multizonal cluster as you can see the master zone is the zone where the control plane will be created and is selected as us east 1b as that is the zone that i had selected and so if i change this to let's say us east 1d you can see that the control plane will change with it so i'm going to change it back to us east 1b and you also have the option of creating a regional cluster and the location selection will change from zone to region and here you will have to specify at least one zone to select but please also remember that the same number of nodes will be deployed to each selected zone so if i have three nodes in this cluster and i decide to select three zones then i will have nine nodes in this cluster and so doing something like this could get quite pricey when you're looking to be cost conscious okay so moving on i'm going to uncheck specify default node locations i'm going to change the location type back to zonal and make sure that my zone is at us east 1b moving down to the master version this is where we would select either a static version or optin to a release channel for the version of kubernetes that you want for your cluster and so with the static version i can choose from a bunch of different versions here all the way back from 1.14.10 all the way to the latest version and so with the release channel i have the release channel selection here and i can choose from the rapid channel the regular channel or the stable channel and so i'm going to keep things as the default with the regular channel as well i'm going to keep the default version as the version of my choice now i could go ahead and simply click on create here but as this demo is a walkthrough i'm going to go ahead and go through all the available options so i'm going to start by going over to the left hand menu and clicking on default pool under no pools now here i have one node pool already with three nodes and this is the default node pool that comes with any cluster but if i was doing something specific i could add another node pool and configure it from here but because i don't have a need for two node pools i'm gonna go ahead and remove nodepool1 so i'm going to go up here to remove nodepool and as you can see gke makes it really easy for me to add or remove node pools so i'm going to go back to the default pool and i'm going to keep the name as is i'm gonna keep my number of nodes as three and if i wanted to change the number of nodes i can simply select this i can choose six or however many nodes you need for your workload and so because we're not deploying a large workload i'm gonna keep this number at 3 and moving right along we do want to check off enable auto scaling and so this way we don't have to worry about scaling up or scaling down and here i'm going to put the minimum number of nodes as one and i'm going to keep my maximum number of nodes at 3. and so here i'm given the option to select the zone location for my nodes but again for each zone that i select it will run the same amount of nodes so basically i have another option in order to choose from having a zonal or multizonal cluster and because we're creating our cluster in a single zone i'm going to uncheck this and under automation as you can see enable auto upgrade and enable auto repair are both checked off and this is due to the fact that the auto upgrade feature is always enabled for the release channel that i selected but as i pointed out in a previous lesson that this is google's best practice to have auto upgrade and auto repair enabled and so moving down to the bottom are some fields to change the surge upgrade behavior and so just as a refresher surge upgrades allow you to control the number of nodes gke can upgrade at a time and control how disruptive those upgrades are to your workloads so max surge being the number of additional nodes that can be added to the node pool during an upgrade and max unavailable being the number of nodes that can be simultaneously unavailable during that upgrade and because we're not worried about disruptions we'll just leave it set as the default and so moving on we're going to move back over to the left hand menu and under no pools we're going to click on nodes and here is where i can choose the type of instance that i want to be using for my nodes and so i'm going to keep the image type as container optimize os and this is the default image type but i also have the option of choosing from others like ubuntu or windows and so i'm going to keep it as the default and under machine configuration i'm going to keep it under general purpose with series e2 but i do want to change the machine type to e2 micro just to be cost conscious and under boot disk size i want to keep it as 10 gigabytes as we don't really need 100 gigabytes for what we're doing here and you also have the option of choosing from a different boot disk type you can change it from standard persistent disk to ssd but i'm going to keep things as standard as well i also have the option here to use customer manage keys for encryption on my boot disk as well as selecting from preemptable nodes for some cost savings and so i'm going to now move down to networking and here if i wanted to get really granular i can add a maximum pods per node as well as some network tags but our demo doesn't require this so i'm going to leave it as is and i'm going to go back over to the left hand menu and click on security and under node security you have the option of changing your service account along with the access scopes and so for this demo we can keep things as the default service account and the access scopes can be left as is i'm going to go back over to the left hand menu and click on metadata and here i can add kubernetes labels as well as the instance metadata and so i know i didn't get into node taints but just to fill you in on no taints when you submit a workload to run in a cluster the scheduler determines where to place the pods associated with the workload and so the scheduler will place a pod on any node that satisfies the resource requirements for that workload so no taints will give you some more control over which workloads can run on a particular pool of nodes and so they let you mark a node so that the scheduler avoids or prevents using it for certain pods so for instance if you had a node pool that is dedicated to gpus you'd want to keep that node pool specifically for the workload that requires it and although it is in beta this is a great feature to have and so that pretty much covers no pools as we see it here and so this is the end of part one of this demo it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or a tea and whenever you're ready part two will be starting immediately from the end of part one so you can now mark this as complete and i'll see you in the next one this is part two of creating a gke cluster part 2 will be starting immediately from the end of part 1. so with that being said let's dive in and so i'm going to go back over to the left hand menu and under cluster i'm going to click on automation and here i have the option of enabling a maintenance window for aligning times when auto upgrades are allowed i have the option of adding the window here and i can do it at specified times during the week or i can create a custom maintenance window and so we don't need a maintenance window right now so i'm going to uncheck this and as well you have the option of doing maintenance exclusions for when you don't want maintenance to occur ngk gives you the option of doing multiple maintenance exclusions for whenever you need them and because we don't need any maintenance exclusions i'm going to delete these and here you have the option to enable vertical pod auto scaling and this is where gke will automatically schedule pods onto other nodes that satisfy the resources required for that workload as well here i can enable my node auto provisioning and enabling this option allows gke to automatically manage a set of node pools that can be created and deleted as needed and i have a bunch of fields that i can choose from the resource type the minimum and maximum for cpu and memory the service account as well as adding even more resources like gpus but our workload doesn't require anything this fancy so i'm going to delete this and i'm going to uncheck enable auto provisioning and lastly we have the auto scaling profile and i have the option from choosing the balance profile which is the default as well as the optimize utilization which is still in beta and so i'm going to keep things as the default and i'm going to move back on over to the left hand menu over to networking and so here i can get really granular with my cluster when it comes to networking i have the option of choosing from a public or a private cluster as well i can choose from a different network and since we only have the default that's what shows up but if you had different networks here you can choose from them as well as the subnets i can also choose from other networking options like pod address range maximum pods per node and there's a bunch of other options which i won't get into any detail with but i encourage you if you're very curious to go through the docs and to check out these different options now the one thing that i wanted to note here is the enable http low balancing and this is a addon that is required in order to use google cloud load balancer and so as we discussed previously in the services lesson when you enable service type load balancer a load balancer will be created for you by the cloud provider and so google requires you to check this off so that a controller can be installed in the cluster upon creation and will allow a load balancer to be created when the service is created and so i'm going to leave this checked as we will be deploying a load balancer a little bit later and so moving back over to the left hand menu i'm going to now click on security and there are many options here to choose from that will allow you to really lock down your cluster and again this would all depend on your specific type of workload now i'm not going to go through all these options here but i did want to highlight it for those who are looking to be more security focused with your cluster and so moving down the list in the menu i'm going to click on metadata and so here i can enter a description for my cluster as well as adding labels and so the last option on the cluster menu is features and here i have the option of running cloud run for anthos which will allow you to deploy serverless workloads to anthos clusters and runs on top of gke and here you can enable monitoring for gke and have it be natively monitored by google cloud monitoring and if i was running a thirdparty product to monitor my cluster i can simply uncheck this and use my thirdparty monitoring and there's a whole bunch of other features that i won't dive into right now but if you're curious you can always hover over the question mark and get some more information about what it does and so now i've pretty much covered all the configuration that's needed for this cluster and so now i'm going to finally head down to the bottom and click on create and so it may take a few minutes to create this cluster so i'm going to go ahead and pause this video here and i'll be back faster than you can say cat in the hat okay and the cluster has been created as you can see it's in the location of us east 1b with three nodes six vcpus and three gigabytes of memory and i can drill down and see exactly the details of the cluster as well if i wanted to edit any of these options i can simply go up to the top click on edit and make the necessary changes and so now you're probably wondering what will i need to do in order to create this cluster through the command line well it's a bit simpler than what you think and i'm going to show you right now i'm going to simply go over to the right hand menu and activate cloud shell and bring this up for better viewing and i'm going to paste in my command gcloud container clusters create bow tie dash cluster with the flag num nodes and the number of nodes that i choose which is three and so like i said before if i wanted to simply create a simple cluster i can do so like this but if i wanted to create the cluster exactly how i built my last cluster then i can use this command which has all the necessary flags that i need to make it customize to my liking a not so very exciting demonstration but at the same time shows you how easy yet powerful gke really is and so i'm not going to launch this cluster as i already have one and so now i wanted to show you how to interact with your new gke cluster so i'm going to simply clear my screen and so now in order for me to interact with my cluster i'm going to be using the cube ctl command line tool and this is the tool that is used to interact with any kubernetes cluster no matter the platform now i could use the gcloud container commands but they won't allow me to get very granular as the cubectl tool and so a caveat of creating your cluster through the console is that you need to run a command in order to retrieve the cluster's credentials and configure the cubectl command line tool and i'm going to go ahead and paste that in now and the command is gcloud container clusters get dash credentials and the name of my cluster which is bow tie dash cluster along with the zone flag dash dash zone followed by the zone itself which is us east 1b i'm going to go ahead and hit enter and as you can see cubectl has now been configured and so now i'm able to interact with my cluster so just to verify i'm going to run the command cubectl getpods and naturally as no workloads are currently deployed in the cluster there are no pods so i'm going to run the command cube ctl get nodes and as you can see the cubectl command line tool is configured correctly and so now this cluster is ready to have workloads deployed to it and is also configured with the cubectl command line tool so that you're able to manage the cluster and troubleshoot if necessary now i know that there has been a ton of features that i covered but i wanted to give you the full walkthrough so that you are able to tie in some of the theory from the last few lessons and get a feel for the gke cluster as we will be getting more involved with it over the next couple of demos and so that's pretty much all i wanted to cover when it comes to creating and setting up a gke cluster so you can now mark this as complete and whenever you're ready join me in the console in the next one where you will be building your box a bow ties container to deploy to your new cluster but if you are not planning to go straight into the next demo i do recommend that you delete your cluster to avoid any unnecessary costs and recreate it when you are ready to go into the next demo welcome back now in the last lesson you built a custom gke cluster and configured the cube ctl command line tool to interact with the cluster in this lesson you're going to be building a docker image for a box of bow ties using cloud build which will then be pushed over to google cloud container registry so that you can deploy it to your current gke cluster and so as you can see there's a lot to do here so with that being said let's dive in so now the first thing that you want to do is to clone your repo within cloud shell so you can run the necessary commands to build your image so i'm going to go up here to the top right and i'm going to open up cloud shell i'm going to make sure that i'm in my home directory so i'm going to run the command cd space tilde hit enter and i'm in my home directory if i run the command ls i can see that i only have cloud shell.txt and so now i'm going to clone my github repository and i'll have a link in the instructions in the github repo as well as having it in the lesson text below and so the command would be git clone along with the https address of the github repo and i'm going to hit enter and it's finished cloning my repo i'm going to quickly clear my screen and i'm going to run the command ls and i can see my repo here and now i'm going to drill down into the directory by running cd google cloud associate cloud engineer if i run an ls i can see all my clone files and folders and so now the files that we need are going to be found in the box of bowties folder under kubernetes engine and containers so i'm going to change directories to that location and run ls and under box of bow ties is a folder called container which will have all the necessary files that you need in order to build your image we have the jpeg for box of bow ties we have the docker file and we have our index.html and so these are the three files that we need in order to build the image and so as i said before we are going to be using a tool called cloud build which we have not discussed yet cloudbuild is a serverless ci cd platform that allows me to package source code into containers and you can get really fancy with cloud build but we're not going to be setting up any ci cd pipelines we're merely using cloud build to build our image and to push it out to container registry as well container registry is google cloud's private docker repository where you can manage your docker images and integrates with cloud build gke app engine cloud functions and other repos like github or bitbucket and it allows for an amazing build experience with absolutely no heavy lifting and because you're able to build images without having to leave google cloud i figured that this would be a great time to highlight these services so getting back to it we've cloned the repo and so we have our files here in cloud shell and so what you want to do now is you want to make sure the cloud build api has been enabled as this is a service that we haven't used before now we can go through the console and enable the api there but i'm going to run it here from cloud shell and i'm going to paste in the command gcloud services enable cloudbuild.googleapis.com i'm going to hit enter and you should get a prompt asking you to authorize the api call you definitely want to authorize should take a few seconds all right and the api has been enabled for cloud build so now i'm going to quickly clear my screen and so because i want to show you exactly what cloud build is doing i want to head on over there through the console and so i'm going to go over to the navigation menu and i'm going to scroll down to tools until you come to cloud build and as expected there is nothing here in the build history as well not a lot here to interact with and so now you're going to run the command that builds the image and so you're going to paste that command into the cloud shell which is gcloud builds submit dash dash tag gcr.io which is the google cloud container registry our variable for our google cloud project along with the image name of box bow ties version 1.0.0 and please don't forget the trailing dot at the end i'm going to go ahead and hit enter cloud build will now compress the files and move them to a cloud storage bucket and then cloud build takes those files from the bucket and uses the docker file to execute the docker build process and so i'm going to pause the video here till the build completes and i'll be back in a flash okay and the image is complete and is now showing up in the build history in the cloud build dashboard and so if i want to drill down into the actual build right beside the green check mark you will see the hot link so you can just simply click on that and here you will see a build summary with the build log the execution details along with the build artifacts and as well the compressed files are stored in cloud storage and it has a hot link right here if i wanted to download the build log i can do so here and i conveniently have a hot link to the image of box of bow ties and this will bring me to my container registry so you can go ahead and click on the link it should open up another tab and bring you right to the page of the image that covers a lot of its details now the great thing i love about container registry is again it's so tightly coupled with a lot of the other resources within google cloud that i am able to simply deploy right from here and i can deploy to cloud run to gke as well as compute engine now i could simply deploy this image right from here but i wanted to do it from gke so i'm going to go back over to gke in the other tab i'm going to go to the navigation menu go down to kubernetes engine and i'm going to go up to the top menu and click on deploy it's going to ask for the image you want to deploy and you want to click on select to select a new container image and you should have a menu pop up from the right hand side of your screen and under container registry you should see box of bow ties you can expand the node here and simply click on the image and then hit select and so now the container image has been populated into my image path and you want to scroll down and if i wanted to i could add another container and even add some environment variables and so we're not looking to do that right now so you can simply click on continue and you're going to be prompted with some fields to fill out for your configuration on your deployment and so the application name is going to be called box of bow ties i'm going to keep it in the default namespace as well i'm going to keep the key value pair as app box of bow ties for my labels and because this configuration will create a deployment file for me you can always have a look at the manifest by clicking on the view yaml button before it's deployed and this is always good practice before you deploy any workload so as you can see here at the top i have the kind as deployment the name as well as the namespace my labels replicas of three as well as my selector and my spec down here at the bottom as well this manifest also holds another kind of horizontal pod auto scaler and is coupled with the deployment in this manifest due to the reference of the deployment itself and so it's always common practice to try and group the manifest together whenever you can and so this is a really cool feature to take advantage of on gke so i'm going to close this now and i'm actually going to close cloud shell as i don't need it right now as well you can see here that it's going to deploy to my kubernetes cluster of bow tie cluster in us east 1b and if i wanted to i can deploy it to a new cluster and if i had any other clusters in my environment they would show up here and i'd be able to select from them as well but bow tie cluster is the only one that i have and so now that you've completed your configuration for your deployment you can simply click on deploy this is just going to take a couple minutes so i'm just going to pause the video here and i'll be back as soon as the deployment is done okay the workload has been deployed and i got some default messages that popped up i can set an automated pipeline for this workload but we're not going to do that for this demo but feel free to try it on your own later if you'd like and we will want to expose our service as we want to see if it's up and running and we're going to take care of that in just a bit and so if i scroll through some of the details here i can see that i have some metrics here for cpu memory and disk the cluster namespace labels and all the pods that it's running on basically a live visual representation of my deployment if i scroll back up to the top i can dive into some details events and even my manifest i can also copy my manifest and download it if i'd like so as you can see a lot of different options and so now i want to verify my deployment and so i'm going to use the cube ctl command line tool to run some commands to verify the information so i'm going to open back up my cloud shell and make this a little bit bigger for better viewing and i'm going to run the command cubectl get all and as you can see here i have a list of all the pods that are running the name of the service the deployment the replica set everything about my cluster and my deployment and you should be seeing the same when running this command and so next you want to pull up the details on your deployments in the cluster and so the command for that is cube ctl get deployments and it came out kind of crammed at the bottom so i'm going to simply clear my screen and run that command again and as you can see the box of bowties deployment is displayed how many replicas that are available how many of those replicas achieve their desired state and along with how long the application has been running and so now i want to dive into my pods and in order to do that i'm going to run the command cube ctl get pods and here i can see all my pods now if i wanted to look at a list of events for a specific pod the command for that would be cubectl describe pod and then the name of one of the pods so i'm going to pick this first one copy that i'm going to paste it and i'm going to hit enter and here i can see all the events that have occurred for this pod as well i also have access to some other information with regards to volumes conditions and even the container and image ids and this is a great command to use for when you're troubleshooting your pods and you're trying to get to the bottom of a problem and so now the final step that you want to do is you want to be able to expose your application so you can check to see if it's running properly and so we're going to go ahead and do that through the console so i'm going to close down cloud shell and i'm going to go to overview and scroll down to the bottom click on the button that says expose and if i wanted to i can do it from up here in the top right hand corner where it says expose deployment so i'm going to click on expose and this probably looks very familiar to you as this is a graphical representation of the services manifest and so the port mapping here will cover the ports configuration of the services manifest starting here with port target port as well as protocol for target port i'm going to open up port 80. here under service type you have the option of selecting cluster ip node port or load balancer and the service type you want to use is going to be low balancer and we can keep the service name as box of bowties service and again you can view the manifest file for this service and you can copy or download it if you need to but we don't need this right now so i'm going to close it in a pretty simple process so all i need to do is click on expose and within a minute or two you should have your service up and running with your shiny new low balancer okay and the service has been created and as you can see we're under the services and ingress from the left hand menu and if i go back to the main page of services in ingress you can see that box a bow tie service is the only one that's here i also have the option of creating a service type ingress but we don't want to do that right now so i'm going to go back to services and here you will see your endpoint and this is the hot link that should bring you to your application so you can click on it now you'll get a redirect notice as it is only http and not https so it's safe to click on it so i'm going to click on it now and success and here is your box of bow ties what were you expecting and so i wanted to congratulate you on deploying your first application box of bow ties on your gke cluster and so just as a recap you've cloned your repo into your cloud shell environment you then built a container image using cloud build and pushed the image to container registry you then created a deployment using this image and verified the deployment using the cube ctl command line tool you then launched a service of type low balancer to expose your application and verified that your application was working so fantastic job on your part and that's pretty much all i wanted to cover in this part of the demo so you can now mark this as complete and whenever you're ready join me in the console for the next part of the demo where you will manage your workload on the gke cluster so please be aware of the charges incurred on your currently deployed cluster if you plan to do the next demo at a later date again you can mark this as complete and i'll see you in the next welcome back in the last couple of demo lessons you built a custom gke cluster and deployed the box of bowties application in this lesson you will be interacting with this workload on gke by scaling the application editing your application and rebuilding your docker image so you can do a rolling update to the current workload in your cluster now there's a lot to do here so with that being said let's dive in so continuing where we left off you currently have your box of bow ties workload deployed on your gke cluster and so the first thing you want to do is scale your deployment and you are looking to scale down your cluster to one pod and then back up again to three and this is just to simulate scaling your workload so whether it be ten pods or one the action is still the same so now we can easily do it through the console by drilling down into the box of bowties workload going up to the top menu and clicking on actions and clicking on scale and here i can indicate how many replicas i'd like and scale it accordingly and so i wanted to do this using the command line so i'm going to cancel out of here and then i'm going to open up cloud shell instead okay and now that you have cloud shell open up you want to run the command cube ctl get pods to show the currently running available pods for the box of bowties workload and you may get a popup asking you to authorize the api call using your credentials and you definitely want to authorize and here you will get a list of all the pods that are running your box of bow ties workload and so now since you want to scale your replicas down to one you can run this command cube ctl scale deployment and your workload which is box of bowties dash dash replicas is equal to one you can hit enter and it is now scaled and in order to verify that i'm going to run cube ctl get pods and notice that there is only one pod running with my box of bow ties workload and in order for me to scale my deployment back up to three replicas i can simply run the same command but change the replicas from 1 to 3. hit enter it's been scaled i'm going to run cube ctl get pods and notice that i am now back up to 3 replicas and so as you can see increasing or decreasing the number of replicas in order to scale your application is pretty simple to do okay so now that you've learned how to scale your application you're gonna learn how to perform a rolling update but in order to do that you need to make changes to your application and so what you're going to do is edit your application then rebuild your docker image and apply a rolling update and in order to do that we can stay here in cloud shell as you're going to edit the file in cloud shell editor i'm going to first clear my screen i'm going to change directory into my home directory and now you want to change directories to your container folder where the files are that i need to edit i'm going to run ls and here's the files that i need and so what you're going to do now is edit the index.html file and the easiest way to do that is to simply type in edit index.html and hit enter and this will open up your editor so you can edit your index.html file and if you remember when we launched our application it looked exactly like this and so instead of what were you expecting we're going to actually change that text to something a little different and so i'm going to go back to the editor in my other tab and where it says what were you expecting i'm going to actually change this to well i could always use something to eat then i'm going to go back up to the menu click on file and click on save and so now in order for me to deploy this i need to rebuild my container and so i'm going to go back to my terminal i'm going to clear the screen and i'm going to run the same command that i did the last time which is gcloud build submit dash dash tag gcr dot io with the variable for your google cloud project followed by the image box of bowties colon 1.0.1 and so this will be a different version of the image also don't forget that trailing dot at the end and you can hit enter and again this is the process where cloud build compresses the files moves them to a cloud storage bucket and then takes the files from the bucket and uses the docker file to execute the docker build process and this will take a couple minutes so i'm going to pause the video here and i'll be back before you can say cat in the hat okay and my new image has been created and so i want to head over to cloud build just to make sure that there are no errors so i'm going to close down cloud shell because i don't need it right now i'm going to head back up to the navigation menu and scroll down to cloud build and under build history you should see your second build and if you drill down into it you will see that the build was successful and heading over to build artifacts you should now see your new image as version 1.0.1 and so now i'm going to head over to the registry and verify the image there and it seems like everything looks okay so now i'm gonna head back on over to my gke cluster i'm gonna go to the navigation menu down to kubernetes engine and here i'm gonna click on workloads i'm gonna select box of bowties and up at the top menu you can click on actions and select a rolling update and here you are prompted with a popup where you can enter in your minimum seconds ready your maximum search percentage as well as your maximum unavailable percentage and so here under container images i am prompted to enter in the sha256 hash of this docker image now a docker image's id is a digest which contains a sha256 hash of the image's configuration and if i go back over to the open tab for container registry you can see here the digest details to give you a little bit more context along with the sha 256 hash for the image that i need to deploy and so you can copy this digest by simply clicking on the copy button and then you can head back on over to the gke console head over to the container images highlight the hash and paste in the new hash and so when you copy it in make sure it's still in the same format of gcr dot io forward slash your project name forward slash box of bow ties the at symbol followed by the hash and so once you've done that you can click on the update button and this will schedule an update for your application and as you can see here at the top it says that pods are pending as well if i go down to active revisions you can see here that there is a summary and the status that pods are pending and so just as a note rolling updates allow the deployments update to take place with zero downtime by incrementally updating pods instances with new ones so the pods will be scheduled on nodes with available resources and if the nodes do not have enough resources the pods will stay in a pending state but i don't think we're going to have any problems with these nodes as this application is very light in resources and if i open up cloud shell and run a cube ctl get pods command you will see that new pods have started and you can tell this by the age of the pod as well if you ran the command keep ctl describe pod along with the pod name you could also see the event logs when the pod was created and if i close cloud shell i can see up here at the top of my deployment details it shows that my replicas have one updated four ready three available and one unavailable and if i click on refresh i can see now that my replicas are all updated and available and so now in order to check your new update you can simply go down to exposing services and click on the endpoints link you'll get that redirect notice you can simply click on the link and because the old site may be cached in your browser you may have to refresh your web page and success and you have now completed a rolling update in gke so i wanted to congratulate you on making it to the end of this multipart demo and hope that it's been extremely useful in excelling your knowledge in gke and so just as a recap you scaled your application to accommodate both less and more replicas you edited your application in the cloud shell editor and rebuilt your container image using cloud build you then applied the new digest to your rolling update and applied that rolling update to your deployment while verifying it all in the end fantastic job on your part as this was a pretty complex and long multipart demo and you can expect things like what you've experienced in this demo to pop up in your role of being a cloud engineer when dealing with gke and so that's pretty much all i wanted to cover with this multipart demo working with gke so before you go i wanted to take a few moments to delete all the resources you've created one by one so i'm going to go up to the top i'm going to close all my tabs i'm going to head on over to clusters and so i don't want to delete my cluster just yet but the first thing that i want to do is delete my container images so i'm going to head up to the top and open up cloud shell and i'm going to use the command gcloud container images delete gcr dot io forward slash your google cloud project variable forward slash along with your first image of box of bow ties colon 1.0.0 hit enter it's going to prompt you if you want to continue you want to hit y for yes and it has now deleted the image as well you want to delete your latest image which is 1.0.1 so i'm going to change the zero to one hit enter it's going to ask if you want to continue yes and so the container images have now been deleted and so now along with the images you want to delete the artifacts as well and those are stored in cloud storage so i'm going to close down cloud shell i'm going to head on up to the navigation menu and i'm going to head down to storage and you want to select your bucket that has your project name underscore cloud build select the source folder and click on delete and you're going to get a prompt asking you to delete the selected folder but in order to do this you need to type in the name of the folder so i'm going to type it in now you can click on confirm and so now the folder has been deleted along with the artifacts and so now that we've taken care of the images along with the artifacts we need to clean up our gke cluster so i'm going to head back on up to the navigation menu and i'm going to head on over to kubernetes engine and the first thing that i want to delete is the low balancer so i'm going to head on up to services and ingress and you can select box of bow tie service and go up to the top and click on delete you're going to get a confirmation and you want to click on delete and it's going to take a couple minutes you do quick refresh and the service has finally been deleted i now want to delete my workload so i'm going to go over to the left hand menu click on workloads select the workload box of bowties and go up to the top and click on delete and you want to delete all resources including the horizontal pod auto scaler so you can simply click on delete and it may take a few minutes to delete gonna go up to the top and hit refresh and my workload has been deleted and so now all that's left to delete is the gke cluster itself so i'm going to go back to clusters so you're going to select the cluster and go up to the top and click on delete and you're going to get a prompt asking you if you want to delete these storage pods and these are default storage pods that are installed with the cluster as well you can delete the cluster while the workload is still in play but i have this habit of being thorough so i wanted to delete the workload before deleting the cluster and so you want to go ahead and click on delete and so that's pretty much all i have for this demo and this section on google kubernetes engine and again congrats on the great job you can now mark this as complete and i'll see you in the next one welcome back and in this lesson i will be covering the features of cloud vpn an essential service for any engineer to know about when looking to connect another network to google cloud whether it be your onpremises network another cloud provider or even when connecting to vpcs this service is a must know for any engineer and for the exam so with that being said let's dive in now cloudvpn securely connects your peer network to your vpc network through an ipsec vpn connection when i talk about a peer network this is referring to an onpremises vpn device or vpn service a vpn gateway hosted by another cloud provider such as aws or azure or another google cloud vpn gateway and so this is an ipsec or encrypted tunnel from your peer network to your vpc network that traverses the public internet and so for those who don't know ipsec being short for internet security protocol and this is a set of protocols using algorithms allowing the transport of secure data over an ip network ipsec operates at the network layer so layer 3 of the osi model which allows it to be independent of any applications although it does come with some additional overhead so please be aware and so when creating your cloud vpn traffic traveling between the two networks is encrypted by one vpn gateway and then decrypted by the other vpn gateway now moving on to some details about cloud vpn this is a regional service and so please take that into consideration when connecting your onpremises location to google cloud for the least amount of latency it also means that if that region were to go down you would lose your connection until the region is back up and running now cloud vpn is also a sitetosite vpn only and therefore it does not support sitetoclient so this means that if you have a laptop or a computer at home you cannot use this option with a vpn client to connect to google cloud cloudvpn can also be used in conjunction with private google access for your onpremises hosts so if you're using private google access within gcp you can simply connect to your data center with vpn and have access as if you were already in gcp so if you're looking to extend private google access to your onpremises data center cloud vpn would be the perfect choice and so when it comes to speeds each cloud vpn tunnel can support up to three gigabits per second total for ingress and egress as well routing options that are available are both static and dynamic but are only available as dynamic for aha vpn and lastly cloudvpn supports ik version 1 and ike version 2 using shared secret and for those of you who are unaware ike stands for internet key exchange and this helps establish a secure authenticated communication channel by using a key exchange algorithm to generate a shared secret key to encrypt communications so know that when you choose cloudvpn that your connection is both private and secure so now there are two types of vpn options that are available in google cloud one being the classic vpn and the other being h a vpn and i'm going to take a moment to go through the differences now with classic vpn this provides a service level agreement of 99.9 percent also known as an sla of three nines while h a vpn provides a four nines sla when configured with two interfaces and two external ips now when it comes to routing classic vpn supports both static and dynamic routing whereas havpn supports dynamic routing only and this must be done through bgp using cloud router classic vpn gateways have a single interface and a single external ip address and support tunnels using static routing as well as dynamic routing and the static routing can be either route based or policy based whereas with havpn it can be configured for two interfaces and two external ips for true ha capabilities and as mentioned earlier when it comes to routing for havpn dynamic routing is the only available option now the one thing about classic vpn is that google cloud is deprecating certain functionality on october 31st of 2021 and is recommending all their customers to move to h a vpn and so know that this has not been reflected in the exam and not sure if and when it will be but know that when you are creating a cloud vpn connection in your current environment h a vpn is the recommended option and so now i wanted to dive into some architecture of how cloud vpn is set up for these two options starting with classic vpn now as i said before classic vpn is a cloud vpn solution that lets you connect your peer network to your vpc network through an ipsec vpn connection in a single region now unlike h a vpn classic vpn offers no redundancy out of the box you would have to create another vpn connection and if the connection were to go down you would have to manually switch over the connection from one to the other now as you can see here when you create a vpn gateway google cloud automatically chooses only one external ip address for its interface and the diagram shown here shows that of a classic vpn network connected from the bowtie dash network vpc in bowtie project to an onpremises network configured using a static route to connect now moving on to havpn again this is a highly available cloud vpn solution that lets you connect your peer network to your vpc network using an ipsec vpn connection in a single region exactly like classic vpn where havpn differs is that it provides four nines sla and as you can see here it supports double the connections so when you create an h a vpn gateway google cloud automatically chooses two external ip addresses one for each of its fixed number of two interfaces each ip address is automatically chosen from a unique address pool to support high availability each of these ha vpn gateway interfaces supports multiple tunnels which allows you to create multiple h a vpn gateways and you can configure an h a vpn gateway with only one active interface and one public ip address however this configuration does not provide a four nines sla now for h a vpn gateway you configure an external peer vpn gateway resource that represents your physical peer gateway in google cloud you can also create this resource as a standalone resource and use it later in this diagram the two interfaces of an h a vpn gateway in the bowtie network vpc living in bowtie project are connected to two peer vpn gateways in an onpremises network and this connection is using dynamic routing with bgp connecting to a cloud router in google cloud now when it comes to the times when using cloudvpn makes sense one of the first things you should think about is whether or not you need public internet access so when you're sharing files or your company needs a specific sas product that's only available on the internet vpn would be your only option as well when you're looking to use interconnect and your peering location is not available so you're not able to connect your data center to the colocation facility of your choice vpn would be the only other option that you have as well if budget constraints come into play when deciding on connecting to your peer network vpn would always be the way to go as cloud interconnect is going to be the more expensive option and lastly if you don't need a high speed network and low latency is not really a concern for you and you only have regular outgoing traffic coming from google cloud then vpn would suffice for your everyday needs and so the options shown here are also the deciding factors to look for when it comes to questions in the exam that refer to cloudvpn or connecting networks and so that's pretty much all i have for this short lesson on cloudvpn so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to go over another connection type that allows for onpremises connectivity to your google cloud vpcs which is cloud interconnect other than vpn this is the other connection type that allows connectivity from your onpremises environment to your google cloud vpc cloud interconnect is the most common connection for most larger organizations and are for those that demand fast low latency connections this lesson will cover the features of cloud interconnect and the different types that are available so with that being said let's dive in so getting right into it cloud interconnect is a low latency highly available connection between your onpremises data center and google cloud vpc networks also cloud interconnect connections provide internal ip address connection which means internal ip addresses are directly accessible from both networks and so on premises hosts can use internal ip addresses and take advantage of private google access rather than external ip addresses to reach google apis and services traffic between your onpremises network and your vpc network doesn't traverse the public internet traffic traverses a dedicated connection or through a service provider with a dedicated connection your vpc network's internal ip addresses are directly accessible from your onpremises network now unlike vpn this connection is not encrypted if you need to encrypt your traffic at the ip layer you can create one or more selfmanaged vpn gateways in your vpc network and assign a private ip address to each gateway now although this may be a very fast connection it also comes with a very high price tag now unlike vpn this connection type is not encrypted if you need to encrypt your traffic at the ip layer you can create one or more selfmanaged vpn gateways in your vpc network and assign a private ip address to each gateway now although this may be a very fast connection it also comes with a very high price tag and is the highest price connection type cloud interconnect offers two options for extending your onpremises network dedicated interconnect which provides a direct physical connection between your onpremises network and google's network as well as partner interconnect which provides connectivity between your onpremises and vpc networks through a supported service provider and so i wanted to take a moment to highlight the different options for cloud interconnect starting with dedicated interconnect now dedicated interconnect provides a direct physical connection between your onpremises network and google's network dedicated interconnect enables you to transfer large amounts of data between your network and google cloud which can be more cost effective than purchasing additional bandwidth over the public internet for dedicated interconnect you provision a dedicated interconnect connection between the google network and your own router in a common location the following example shown here shows a single dedicated interconnect connection between a vpc network and an onpremises network for this basic setup a dedicated interconnect connection is provisioned between the google network and the onpremises router in a common colocation facility when you create a vlan attachment you associate it with a cloud router this cloud router creates a bgp session for the vlan attachment and its corresponding onpremises peer router these routes are added as custom dynamic routes in your vpc network and so for dedicated interconnect connection capacity is delivered over one or more 10 gigabits per second or 100 gigabits per second ethernet connections with the followon maximum capacity supported per interconnect connection so with your 10 gigabit per second connections you can get up to eight connections totaling a speed of 80 gigabits per second with the 100 gigabit per second connection you can connect two of them together to have a total speed of 200 gigabits per second and so for dedicated interconnect your network must physically meet google's network in a supported colocation facility also known as an interconnect connection location this facility is where a vendor the colocation facility provider provisions a circuit between your network and a google edge point of presence also known as a pop the setup shown here is suitable for noncritical applications that can tolerate some downtime but for sensitive production applications at least two interconnect connections in two different edge availability domains are recommended now partner interconnect provides connectivity between your onpremises network and your vpc network through a supported service provider so this is not a direct connection from your onpremises network to google as the service provider provides a conduit between your onpremises network and google's pop now a partner interconnect connection is useful if a dedicated interconnect colocation facility is physically out of reach or your workloads don't warrant an entire 10 gigabit per second connection for partner interconnect 50 megabits per second to 50 gigabits per second vlan attachments are available with the maximum supported attachment size of 50 gigabits per second now service providers have existing physical connections to google's network that they make available for their customer to use so in this example shown here you would provision a partner interconnect connection with a service provider and connecting your onpremises network to that service provider after connectivity is established with the service provider a partner interconnect connection is requested from the service provider and the service provider configures your vln attachment for use once your connection is provisioned you can start passing traffic between your networks by using the service providers network now there are many more detailed steps involved to get a connection established along with traffic flowing but i just wanted to give you a high level summary of how a connection would be established with a service provider now as well to build a highly available topology you can use multiple service providers as well you must build redundant connections for each service provider in each metropolitan and so now there's a couple more connection types that run through service providers that are not on the exam but i wanted you to be aware of them if ever the situation arises in your role as a cloud engineer so the first one is direct peering and direct peering enables you to establish a direct peering connection between your business network and google's edge network and exchange high throughput cloud traffic this capability is available at any of more than 100 locations in 33 countries around the world when established direct peering provides a direct path from your onpremises network to google services including google cloud products that can be exposed through one or more public ip addresses traffic from google's network to your onpremises network also takes that direct path including traffic from vpc networks in your projects now you can also save money and receive direct egress pricing for your projects after they have established direct peering with google direct peering exists outside of google cloud unless you need to access google workspace applications the recommended methods of access to google cloud are dedicated interconnect or partner interconnect establishing a direct peering connection with google is free and there are no costs per port and no per hour charges you just have to meet google's technical peering requirements and can then be considered for the direct peering service and moving on to the last connection type is cdn interconnect now i know we haven't gotten into cdns in the course as the exam does not require you to know it but cdn standing for content delivery network is what caches content at the network edge to deliver files faster to those requesting it one of the main ways to improve website performance now moving on to cdn interconnect this connection type enables select thirdparty cdn providers like akamai and cloudflare along with others to establish and optimize your cdn population costs by using direct peering links with google's edge network and enables you to direct your traffic from your vpc networks to the provider's network and so your egress traffic from google cloud through one of these links benefits from the direct connectivity to the cdn provider and is billed automatically with reduced pricing typical use cases for cdn interconnect is if you're populating your cdn with large data files from google cloud or you have frequent content updates stored in different cdn locations and so getting into the use cases of when to use cloud interconnect a big purpose for it would be to prevent traffic from traversing the public internet it is a dedicated physical connection right to google's data centers so when you need an extension of your vpc network to your onpremises network interconnect is definitely the way to go now in speed and low latencies of extreme importance interconnect is always the best option and will support up to 200 gigabits per second as well when you have heavy outgoing traffic or egress traffic leaving google cloud cloud interconnect fits the bill perfectly and lastly when it comes to private google access this travels over the backbone of google's network and so when you are connected with interconnect this is an extension of that backbone and therefore your onpremises hosts will be able to take advantage of private google access and so i hope this has given you some clarity on the differences between the different connection types and how to extend your google cloud network to a peer or onpremises network so that's pretty much all i had to cover when it comes to cloud interconnect so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be covering an overview of app engine now this is not a deep dive lesson for app engine as there is so much to cover with this service but i will be listing a lot of the features of app engine to give you a good feel for what it can do and what you will need to know for the exam so with that being said let's dive in now app engine is a fully managed serverless platform for developing and hosting web applications at scale this is google's platform as a service offering that was designed for developers so that they can develop their application and let app engine do all the heavy lifting by taking care of provisioning the servers and scaling the instances needed based on demand app engine gives you the flexibility of launching your code as is or you can launch it as a container and uses runtime environments of a variety of different programming languages like python java node.js go ruby php or net applications deployed on app engine that experience regular traffic fluctuations or newly deployed applications where you're simply unsure about the load are auto scaled accordingly and automatically your apps scale up to the number of instances that are running to provide consistent performance or scale down to minimize idle instances and reduces costs app engine also has the capabilities of being able to deal with rapid scaling for sudden extreme spikes of traffic having multiple versions of your application within each service allows you to quickly switch between different versions of that application for rollbacks testing or other temporary events you can route traffic to one or more specific versions of your application by migrating or splitting traffic and you can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service and allows you to do a b testing or blue green deployment between your versions when rolling out new features app engine supports connecting to backend storage services such as cloud firestore cloud sql and cloud storage along with connecting to onpremises databases and even external databases that are hosted on other public clouds app engine is available in two separate flavors standard and flexible environments and each environment offers their own set of features that i will get into in just a sec now as i mentioned before app engine is available in standard and flexible environments and depending on your application needs either one will support what you need for your workload or you could even use both simultaneously the features shown here will give you a feel for both types of environments and i'm going to be doing a quick run through summarizing the features of each starting with the standard environment now with the standard environment applications run in a secure sandboxed environment allowing app engine standard to distribute requests across multiple servers and scaling servers to meet traffic demands your application runs with its own secure reliable environment that is independent of the hardware operating system or physical location of the server the source code is written in specific versions of the supported programming languages and with app engine standard it is intended to run for free or at a very low cost where you pay only for what you need and when you need it with app engine standard your application can scale to zero instances when there is no traffic app engine standard is designed for sudden and extreme spikes of traffic which require immediate scaling and pricing for standard app engine is based on instance hours and so when it comes to features for app engine flexible the application instances run within docker containers that includes a custom runtime or source code written in other programming languages these docker containers are then run on compute engine vms app engine flexible will run any source code that is written in a version of any of the supported programming languages for app engine flexible and unlike the standard environment unfortunately there is no free quota for app engine flexible as well app engine flexible is designed for consistent traffic or for applications that experience regular traffic fluctuations and pricing is based on the vm resources and not on instance hours like app engine standard and so where app engine flexible really shines over app engine standard are how the vms are managed so instances are health checked healed as necessary and colocated with other services within the project the vm's operating system is updated and applied automatically as well vms are restarted on a weekly basis to make sure any necessary operating system and security updates are applied ssh along with root access are available to the vm instances running your containers now deploying applications to app engine is as simple as using the gcloud app deploy command this command automatically builds a container image from your configuration file by using the cloud build service and then deploys that image to app engine now an app engine application is made up of a single application resource that consists of one or more services each service can be configured to use different runtimes and to operate with different performance settings services and app engine are used to factor your large applications into logical components that can securely share app engine features and communicate with one another these app engine services become loosely coupled behaving like microservices now within each service you deploy versions of that service and each version then runs within one or more instances depending on how much traffic you configured it to handle having multiple versions of your application within each service allows you to quickly switch between different versions of that application for rollbacks testing or other temporary events you can route traffic to one or more specific versions of your application by migrating traffic to one specific version or splitting your traffic between two separate versions and so the versions within your services run on one or more instances by default app engine scales your application to match the load your applications will scale up the number of instances that are running to provide consistent performance or scale down to minimize idle instances and reduce costs now when it comes to managing instances app engine can automatically create and shut down instances as traffic fluctuates or you can specify a number of instances to run regardless of the amount of traffic you can also configure how and when new instances are created by specifying a scaling type for your application and how you do this is you specify the scaling type in your application's app.yaml file now there are three different types of scaling choices to choose from and the first one being automatic scaling and this scaling type creates instances based on request rate response latencies and other application metrics you can specify thresholds for each of these metrics as well as a minimum number instances to keep running at all times if you use automatic scaling each instance in your application has its own queue for incoming requests before the queues become long enough to have a visible effect on your app's latency app engine automatically creates one or more new instances to handle the load the second type is basic scaling and this creates instances when your application receives requests each instance is shut down when the application becomes idle basic scaling is fantastic for intermittent workloads or if you're looking to drive your application by user activity app engine will try to keep your costs low even though it might result in higher latency as the volume of incoming requests increase and so the last scaling type is manual scaling and this is where you specify the number of instances that continuously run regardless of the load so these are instances that are constantly running and this allows complex startup tasks on the instances to have already been completed when receiving requests and applications that rely on the state of the memory over time so this is ideal for instances whose configuration scripts require some time to fully run their course so now that i've gone over managing the instances i wanted to take a few moments to go over how app engine manages traffic starting with traffic migration now traffic migration switches the request routing between the versions within a service of your application moving traffic from one or more versions to a single new version so when deploying a new version with the same name of an existing version it causes an immediate traffic migration all instances of the old version are immediately shut down in app engine standard you can choose to route requests to the target version either immediately or gradually you can also choose to enable warmup requests if you want the traffic gradually migrated to a version gradual traffic migration is not supported in app engine flexible and traffic is migrated immediately now one thing to note is that when you immediately migrate traffic to a new version without any running instances then your application will have a spike in latency for loading requests while instances are being created and so another way to manage traffic on app engine is through traffic splitting now you can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service so in this example if i'm deploying a new version of my service i can decide on how i want to distribute traffic to each version of my application and so i decide that i want to keep my current version in play but roll out the new version of my application to 10 of my users leaving the old version was still 90 of the traffic going to that version and so splitting traffic allows you to conduct a b testing between your versions and provides control over the pace when rolling out features and just as a note when you've specified two or more versions for splitting you must choose whether to split traffic by either by either ip address http cookie or do it randomly now again this has not been a deep dive lesson on app engine but i hope this has given you an overview of the features that are available as the exam touches on these features i also wanted to give you some familiarity with the service itself as coming up next i will be going into a demo where we will be launching an application using app engine and trying on some of these features for yourself and so that's pretty much all i wanted to cover when it comes to app engine so you can now mark this lesson as complete and whenever you're ready join me in the console where you will deploy an application on app engine and try out some of these features for yourself welcome back and in this demo you're going to build another application to deploy on app engine called serverless bowties this demo will run you through the ins and outs of deploying a website application on app engine along with managing it while experiencing no downtime so there's quite a bit of work to do here so with that being said let's dive in and so here in my console i am logged in as tonybowtieace gmail.com under project bowtie inc and so the first thing i want to do here is i want to head on over to app engine so in order to do that i'm going to go to the top lefthand navigation menu and i'm going to go down to app engine and because i haven't created any applications i'm going to be brought to this splash page now in order to deploy this application we're not going to be doing it through the console but we will be doing it through the command line and so to get started with that i'm going to go up to the top and open up cloud shell i'm going to make this bigger for better viewing and so in order for me to get the code to launch this application i'm going to be cloning my github repository into cloud shell and so for those of you who haven't deleted your repository from the last demo you can go ahead and skip the cloning step for those of you who need to clone your repository you will find a link to the instructions in the lesson text and there you'll be able to retrieve the command which will be git clone along with the address of the repo i'm going to hit enter and because i've already cloned this repo i'm receiving this error i'm going to do an ls and as you can see here the google cloud associate cloud engineer repo has already been cloned so i'm going to cd into that directory and in order to get the code i'm going to simply run the command git pull to get the latest and i'm going to simply clear my screen and so now that i've retrieved all the code that i need in order to deploy it i need to go to that directory and that directory is going to be 11 serverless services forward slash 0 1 serverless bowties and hit enter you're going to run ls and here you will find two versions of the website application site v1 and site v2 along with the instructions if you want to follow straight from here and so i want to go ahead and deploy my first website application so i'm going to cd into site v1 ls and here you will see the app.yaml which is the configuration file that you will need in order to run the application on app engine and so before i go ahead and deploy this i wanted to take a moment to show you the application configuration so i'm going to go ahead and open it up in cloud shell editor so i'm going to type in edit app.yaml enter and as you can see here my runtime is python 3.7 and as you can see i have a default expiration of two seconds along with an expiration underneath each handler and this is due to the caching issue that happens with app engine and so in order to simulate traffic splitting between the two website applications in order to make things easy i needed to expire the cash and this is an easy way to do it now there may be applications out there that do need that caching and so the expiration may be a lot higher but for the purposes of this demo two seconds expiration should suffice as well explain the two handlers here the first one showing the files that will be uploaded to the cloud storage bucket as well as the second stating what static files will be presented and so i'm going to go ahead back over to my terminal and i'm going to go ahead and clear my screen and i'm going to go ahead and run the command gcloud app deploy with the flag dash dash version and this is going to be version one so i'm going to go ahead and hit enter and you may get a popup asking you to authorize this api call using your credentials and you want to click on authorize and you're going to be prompted to enter in a region that you want to deploy your website application to we want to keep this in us east one so i'm going to type in 15 hit enter and you're going to be prompted to verify your configuration for your application before it's deployed you're also going to be prompted if you want to continue definitely yes so i'm going to hit y enter and so now as you've seen the files have been uploaded to cloud storage and app engine is going to take a few minutes to create the service along with the version so i'm going to let it do the needful and i'll be back before you know it okay and my application has been deployed now although you don't see it here in the console it has been deployed all i need to do is refresh my screen but i wanted to just point out a couple things that is shown here in the terminal the first one being the default service now the first time you deploy a version of your application it will always deploy to the default service initially and only then will you be able to deploy another named service to app engine now here where it says setting traffic split for service this is referring to the configuration for traffic splitting being applied in the background which i will be getting into a little bit later and lastly the url shown for the deployed service will always start with the name of your project followed by.ue.r.appspot.com which is why in production google recommends to run app engine in a completely separate project before this demo running it in the same project that we've been using will suffice okay so let's go ahead and take a look at the application so i'm going to go back up to the top here to the navigation menu and i'm gonna go down to app engine and go over to services and so here you will see the default service with version one and if i go over to versions i will see here my version the status the traffic allocation along with any instances that it needs the run time the specific environment and i'll have some diagnostic tools here that i could use and so because this is a static website application we won't be using any instances and so this will always show a zero so now i want to head back on over to services and i'm going to launch my application by simply clicking on this hot link and success serverless bow ties for all and so it looks like my application has been successfully deployed so i'm going to close down this tab now there's a couple of things that i wanted to run through here on the left hand menu just for your information so here i can click on instances and if i was running any instances i am able to see a summary of those instances and i can click on the drop down here and choose a different metric and find out any information that i need as well i can click on this drop down and select a version if i had multiple versions which i do not clicking on task queues here is where i can manage my task queues but this is a legacy service that will soon be deprecated clicking on cron jobs here i can schedule any tasks that i need to run at a specific time on a recurring basis i can edit or add any firewall rules if i need to and as you can see the default firewall rule is open to the world now you probably noticed memcache as being one of the options here in the menu but this is a legacy service that will soon be deprecated memcache is a distributed inmemory data store that is bundled into the python to runtime acting as a cache for specific tasks and google recommends moving to memory store for redis if you're planning on applying caching for your app engine application and so i'm not sure how much longer this will be here and lastly under settings here is where you can change your settings for your application i can add any custom domains any ssl certificates as well as setting up email for any applications that want to send email out to your users okay and now that we've done that walkthrough i want to go ahead and deploy my second version of the application and so i'm going to go ahead back down to cloud shell i'm going to quickly clear my screen and i want to move into the site v2 directory so i'm going to hit cd dot dot which will bring you back one directory you do an ls and i'm going to change directories into site v2 and do an ls just to verify and yes you will see serverless bow ties too i'm going to quickly clear my screen and i'm going to run the same command as before which is gcloud app deploy with the version flag dash dash version and instead of one i'm going to launch version 2. so i'm going to hit enter i'm going to be prompted if i want to continue yes i do and as you can see the files have been uploaded to cloud storage for version 2 of the website application and app engine is going to take a few minutes to create the service along with the version so i'm going to let it cook here for a couple minutes and i'll be back before you can say cat in the hat okay so version 2 has been deployed and so if i go up here to the console and i click on refresh you should see version 2 of your service and as you can see 100 of the traffic has been allocated to version 2 automatically and this is the default behavior for whenever you launch a new version of your service the only way to avoid this is to deploy your new version with the no promote flag and so if i go back to services here on the left and i click on the default service you should see success for version two and so i know that my website application for version 2 has been deployed successfully so i'm going to close down this tab again and i'm going to go back to versions and so what i want to do now is i want to simulate an a b test or blue green deployment by migrating my traffic back to the old version in this case being version one so in production let's say that you would release a new version and the version doesn't go according to plan you can always go back to the previous version and app engine allows you to do that very easily and so i'm going to click on version 1 and i'm going to go up to the top menu and click on migrate traffic you'll be prompted if you want to migrate traffic yes i do so i'm going to click on migrate and it should take a minute here and traffic should migrate over to version one and success traffic has been migrated and so we want to verify that this has happened i'm gonna go back to services i'm gonna click on the default service and yes the traffic has been allocated to version one okay so i'm going to shut down this tab i'm going to go back to versions and so now what i want to do is i want to simulate splitting the traffic between the two versions and so in order for you to do this you can go up to the top menu click on split traffic and you'll be prompted with a new menu here and here i can choose from different versions and because i only have two versions i'm going to add version 2 and in order to allocate the traffic between the two i can either use this slider and as you can see the allocation percentage will change or i can simply just type it in and so i'm going to leave this at 50 percent so fifty percent of version one fifty percent of version two i'm going to split traffic randomly i'm gonna move this down just a little bit and so that's exactly how you wanna allocate your traffic and so once you've completed that you can simply click on save it's going to take a moment to update the settings and it's been successful so if i head back on over to the previous page you can see here that traffic has been allocated to both versions and so now in order to verify this what you're going to do is go over to services and click on the default hot link and you'll see version one but if i continuously refresh my screen i can see that here i have version two so because it's random i have a 50 chance of getting version 1 and a 50 chance of getting version 2. and so this is a simulation of splitting traffic to different versions and usually with a b testing only a small percentage of the traffic is routed to the new version until verification can be made that the new version deployed has indeed been successful and this can be done by receiving feedback from the users and so now i wanted to take a quick moment to congratulate you on making it through this demo and hope that it has been extremely useful in excelling your knowledge in deploying and managing applications on app engine so just as a recap you've cloned the repo to cloud shell you then deployed version one of your application into app engine you verified its launch and then you deployed version two of the application and verified its launch as well you then migrated traffic from version two over to version one and then you went ahead and split traffic between both versions and allotted 50 of the traffic allocation to each version and so now before you go i want to make sure that we clean up any resources that we've deployed so that we don't incur any unnecessary costs and so the way to do this is very simple so first step you want to go over to the left hand menu and click on settings and simply click on disable application you're going to be prompted to type in the app's id for me it's bowtie inc so i'm going to type that in and i'm going to click on disable now unfortunately with app engine you can't actually delete the application it can only be disabled and so now here i'm going to hit the hot link to go over to the cloud storage bucket and as you can see here i have no files but i'm going to move back to my buckets and i'm going to move into the staging bucket which is appended with your project id.appspot.com and as you can see here there's a whole bunch of different files as well if i drill down into the directory marked as ae for app engine i can see here that i have some more directories along with the manifest and so now if you want to keep your application in order to run it later you don't need to delete this bucket but because i don't need it i'm going to go ahead and delete the bucket hit delete paste in my bucket name hit delete as well under us.artifacts you will find a directory called containers and as explained in the last lesson code build builds a container for your application before deploying it to app engine so i'm going to drill down into images so here's all the container digests and i don't need any of these so i'm gonna go ahead and delete this bucket as well and so this is the last step in order to delete all the directories and files that we use to deploy our application in an app engine okay and so i'm gonna head back on over to app engine and so now that cleanup has been taken care of that's pretty much all i wanted to cover in this demo for deploying and managing applications on app engine so you can now mark this as complete and i'll see you in the next one and again congrats on a job well done welcome back in this lesson i will be diving into another serverless product from google cloud by the name of cloud functions an extremely useful and advanced service that can be used with almost every service on the platform now there's quite a bit to cover here so with that being said let's dive in now cloud functions as i said before are a serverless execution environment and what i mean by this is like app engine there is no need to provision any servers or updating vms as the infrastructure is all handled by google but unlike app engine you will never see the servers so the provisioning of resources happens when the code is executed now cloud functions are a function as a service offering and this is where you upload code that is purposefully written in a supported programming language and when your code is triggered it is executed in a fully managed environment and your billed for when that code is executed cloud functions run in a runtime environment and support many different runtimes like python java node.js go and net core cloud functions are event driven so when something happens in your environment you can choose whether or not you'd like to respond to this event if you do then your code can be executed in response to the event these triggers can be one of a few different types such as http pub sub cloud storage and now firestore and firebase which are in beta and have yet to be seen in the exam cloud functions are priced according to how long your function runs and how many resources you provision for your function if your function makes an outbound network request there are also additional data transfer fees cloud functions also include a perpetual free tier which allows you 2 million invocations or executions of your function now cloud functions themselves are very simple but have a few steps to execute before actually running so i wanted to give you a walkthrough on exactly how cloud functions work now after selecting the name and region you want your function to live in you would then select the trigger you wish to use and you can choose from the many i listed earlier being http cloud storage pub sub cloud firestore and firebase a trigger is a declaration that you are interested in a certain event or set of events binding a function to a trigger allows you to capture and act on these events authentication configuration is the next step and can be selected with public access or configured through iam now there are some optional settings that can be configured where you would provide the amount of memory the function will need to run networking preferences and even selection for a service account now once all the settings have been solidified your written code can then be put into the function now the functions code supports a variety of languages as stated before like python java node.js or go now when writing your code there are two distinct types of cloud functions that you could use http functions and background functions with http functions you invoke them from standard http requests these http requests wait for the response and support handling of common http request methods like get put post delete and options when you use cloud functions a tls certificate is automatically provisioned for you so all http functions can be invoked via a secure connection now when it comes to background functions these are used to handle events from your gcp infrastructure such as messages on a pub sub topic or changes in a cloud storage bucket now once you have put all this together you are ready to deploy your code now there are two things that will happen when deploying your code the first one is the binding of your trigger to your function once you bind a trigger you cannot bind another one to the same function only one trigger can be bound to a function at a time now the second thing that will happen when you deploy your function's source code to cloud functions is that source code is stored in a cloud storage bucket as a zip file cloud build then automatically builds your code into a container image that pushes that image to container registry cloud functions accesses this image when it needs to run the container to execute your function the process of building the image is entirely automatic and requires no manual intervention and so at this point of the process the building of your function is now complete now that the function has been created we now wait for an event to happen and events are things that happen within your cloud environment that you might want to take action on these might be changes to data in cloud sql files added to cloud storage or a new vm being created currently cloud functions supports events from the same services used for triggers that i have just mentioned including other google services like bigquery cloud sql and cloud spanner now when an event triggers the execution of your cloud function data associated with the event is passed via the functions parameters the type of event determines the parameters that are passed to your function cloud functions handles incoming requests by assigning them to instances of your function now depending on the volume of requests as well as the number of existing function instances cloud functions may assign a request to an existing instance or create a new one so the cloud function will grab the image from cloud registry and hand off the image along with the event data to the instance for processing now each instance of a function handles only one concurrent request at a time this means that while your code is processing one request there is no possibility of a second request being routed to the same instance thus the original request can use the full amount of resources that you requested and this is the memory that you assign to your cloud function when deploying it now to allow google to automatically manage and scale the functions they must be stateless functions are not meant to be persistent nor is the data that is passed on to the function and so once the function has run and all data has been processed by the server it is then passed on to either a vpc or to the internet now by default functions have public internet access unless configured otherwise functions can also be private and used within your vpc but must be configured before deployment now there are so many use cases for cloud functions and there are many that have already been created by google for you to try out and can be located in the documentation that i've supplied in the lesson text below now the exam doesn't go into too much depth on cloud functions but i did want to give you some exposure to this fantastic serverless product from google as it is so commonly used in many production environments in a simple and easy way to take in data process it and return a result from any event you are given and i have no doubt that once you get the hang of deploying them that you will be a huge fan of them as well and so that's pretty much all i had to cover when it comes to cloud functions so you can now mark this lesson as complete and whenever you're ready join me in the next one where we go handson in the console creating and deploying your very first function welcome back and in this demo we will be diving into creating and deploying our very first cloud function we're going to take a tour of all the options in the console but we're going to do most of the work in cloud shell to get a good feel for doing it in the command line so with that being said let's dive in and so i'm logged in here as tony bowties gmail.com and i'm in the project of bowtie inc and so the first thing i want to do is head on over to cloud functions in the console so i'm going to go up to the top left to the navigation menu and i'm going to scroll down to cloud functions and as you can see here cloud functions is getting ready and this is because we've never used it before and the api is being enabled okay and the api has been enabled and we can go ahead and start creating our function so you can go ahead and click create function and you will be prompted with some fields to fill out for the configuration of your cloud function and so under basics for function name i'm going to name this hello underscore world for region i'm going to select us east one and under trigger for trigger type we're gonna keep this as http although if i click on the drop down menu you can see that i will have options for cloud pub sub cloud storage and the ones that i mentioned before that are in beta so we're going to keep things as http and here under url is the url for the actual cloud function under authentication i have the option of choosing require authentication or allow unauthenticated invocations and as you can see this is clearly marked saying that check this if you are creating a public api or website which we are and so this is the authentication method that you want to select and so now that we have all the fields filled out for the basic configuration i'm going to go ahead and click on save and just to give you a quick run through of what else is available i'm going to click on the drop down here and this will give me access to variables networking and advanced settings the first field here memory allocated i can actually add more memory depending what i am doing with my cloud function but i'm going to keep it as the default if you have a cloud function that runs a little bit longer and you need more time to run the cloud function you can add additional time for the timeout and as well i have the option of choosing a different service account for this cloud function and so moving on under environment variables you will see the options to add build environment variables along with runtime environment variables and the last option being connections here you can change the different networking settings for ingress and egress traffic under ingress settings i can allow all traffic which is the default i can allow internal traffic only as well i can allow internal traffic and traffic from cloud low balancing now as well when it comes to the egress settings as i said before by default your cloud function is able to send requests to the internet but not to resources in your vpc network and so this is where you would create a vpc connector to send requests from your cloud function to resources in your vpc so if i click on create a connector it'll open up a new tab and bring me to vpc network to add serverless vpc access and so i don't want to do that right now so i'm going to close down this tab and i'm going to go ahead and leave everything else as is and click on next and so now that the configuration is done i can dive right into the code and so google cloud gives you a inline editor right here along with the different runtime environments so if i click on the drop down menu you can see i have the options of net core go java node.js and python 3.7 and 3.8 and so for this demo i'm going to keep it as node.js 10. the entry point will be hello world and i'm going to keep the code exactly as is and this is a default cloud function that is packaged with any runtime whenever you create a function from the console and so if i had any different code i can change it here but i'm not going to do that i'm going to leave everything else as is and click on deploy and it'll take a couple minutes here to create my cloud function and so i'm going to pause the video here for just a quick sec and i'll be back in a flash okay and my cloud function has been deployed and i got a green check mark which means that i'm all good and so i want to dive right into it for just a second so i can get some more details here i have the metrics for my cloud function the invocations per second execution time memory utilization and active instances i have my versions up here at the top but since i only have one version only one version shows up if i click on details it'll show me the general information along with the networking settings the source will show me the code for this cloud function as well as the variables the trigger permissions logs and testing and here i can write in some code and test the function and so in order for me to invoke this function i can simply go to trigger and it'll show me the url but a quick way to do this through the command line is to simply open up cloud shell and make this a little bigger for better viewing and i'm going to paste in the command gcloud functions describe along with the function name which is hello underscore world along with the region flag dash dash region with the region that my cloud function has been deployed in which is us east one and i'm going to hit enter it's going to ask me to authorize my api call yes i want to authorize it and this command should output some information on your screen and so what we're looking for here is the http trigger which you will find here under https trigger and it is the same as what you see here in the console and so just know if you want to grab the http url trigger you can also do it from the command line and so i'm going to now trigger it by going to this url and you should see in the top left hand side of your screen hello world not as exciting as spinning bow ties but this example gives you an idea of what an http function can do and so i'm going to close down this tab and so now what i want to do is i want to deploy another function but i want to do it now through the command line and so i'm going to now quickly clear my screen and so since i've already uploaded the code to the repo i'm going to simply clone that repo and run it from here so i'm going to simply do a cd tilde to make sure i'm in my home directory for those of you who haven't deleted the directory you can simply cd into it so i'm going to run cd google cloud associate cloud engineer hit enter and i'm going to run a get pull command and it pull down all the files that i needed i'm going to quickly clear my screen and so i'm going to change directories into the directory that has my code and so you're going to find it under 11 serverless services under zero to you called hit enter and again i will have a link in the lesson text for the full instructions on this demo and it will list the directory where you can find this code okay so moving forward i'm going to run ls and you should see three files here main.py requirements.txt and the text file with the instructions and so now that i have everything in place in order to deploy my code i'm going to paste in the command to actually deploy my function which is gcloud functions deploy the name of the function which is you underscore called the flag for the runtime dash dash runtime and the runtime is going to be python 3.8 the flag for the trigger which is going to be http and because i'm a nice guy and i want everyone to have access to this i'm going to tag it with the flag dash dash allow unauthenticated so i'm going to hit enter okay and this function should take a couple minutes to deploy so i'm going to sit here and let it cook and i'll be back before you can say cat in the hat okay and our function has been deployed i'm going to do a quick refresh here in the console and it deployed successfully as you can see the green check mark is here okay and so now that it's been deployed we want to trigger our function and so because i just deployed this function the url trigger is conveniently located here in my screen so you can go ahead and click on it and hello lover of bow ties you called now although this may be similar to the hello world demo but i did add a small feature that might spice things up and so if you go up to the url and you type in question mark name equals and your name and since my name is anthony i'm going to type in anthony hit enter and hello anthony you called and so this is a perfect example of the many different ways you can use functions and although i've only highlighted some very simple demonstrations there are many different ways that you can use functions such as running pipelines running batch jobs and even event driven security now although the exam doesn't go into too much depth on cloud functions it's always good to know its use cases and where its strengths lie for when you do decide to use it in your role as a cloud engineer now before you go be sure to delete all the resources you've created by deleting the functions and the storage buckets that house the code for the cloud functions and i will walk you through the steps right now okay so first i'm going to close down this tab and next you're going to select all the functions and you're going to simply click on delete you're going to get a prompt to delete the functions you're going to click on delete and it's going to take a minute or two and the functions are deleted i'm going to close down my cloud shell and i'm going to head over to cloud storage and as you can see here both these buckets that start with gcf standing for google cloud functions can be safely deleted as inside them are the files that were used for the cloud function so i'm going to go back out i'm going to select both of these and i'm going to click on delete you get a prompt to delete two buckets you can simply type in delete and click on delete and the buckets have now been deleted and you've pretty much finished your cleanup and so just as a recap you created a default cloud function that was available from the console and then verified it by triggering the http url you then deployed another function from the command line by pulling the code from the repo and using it for deployment and then you verified that function by triggering it using the http url as well and then you modify the url for a different output great job on another successful demo so you can now mark this as complete and let's move on to the next one welcome back in this lesson we're going to dive into cloud storage the go to storage service from google cloud if you're an engineer working in google cloud you've probably used this many times as a storage solution and if you haven't this is definitely a service that you will need to know for both the exam and your daytoday role as a cloud engineer now there's quite a bit to cover here so with that being said let's dive in now cloud storage is a consistent scalable large capacity highly durable object storage and this is unlimited storage for objects with no minimum object size but please remember that this is object storage and is not designed to store an operating system on but to store whole objects like pictures or videos cloud storage has worldwide accessibility and worldwide storage locations so anywhere that there is a region or zone cloud storage is available from there and can be accessed at any time through an internet connection cloud storage is great for storing data from data analytics jobs text files with code pictures of the latest fashion from paris and videos of your favorite house dj at the shelter cloud storage excels for content delivery big data sets and backups and are all stored as objects in buckets and this is the heart of cloud storage that i will be diving into so starting with buckets these are the basic containers or construct that holds your data everything that you store in cloud storage must be contained in a bucket you can use buckets to organize your data and control access to your data but unlike directories and folders you cannot nest buckets and i'll get into that in just a minute now when you create a bucket you must specify a globally unique name as every bucket resides in a single cloud storage namespace as well as a name you must specify a geographic location where the bucket and its contents are stored and you have three available geography choices to choose from from region dual region and multiregion and so just as a note choosing dual region and multiregion is considered georedundant for dual region georedundancy is achieved using a specific pair of regions for multiregion georedundancy is achieved using a continent that contains two or more geographic places basically the more regions your data is available in the greater your availability for that data after you've chosen a geographic location a default storage class must be chosen and this applies to objects added to the bucket that don't have a storage class explicitly specified and i'll be diving into storage classes in just a bit and so after you create a bucket you can still change its default storage class to any class supported in the buckets location with some stipulations you can only change the bucket name and location by deleting and recreating the bucket as well once dual region is selected it cannot be changed to multiregion and when selecting multiregion you will not be able to change the bucket to be dual region and lastly you will need to choose what level of access you want others to have on your bucket whether you want to apply permissions using uniform or fine grained access uniform bucket level access allows you to use iam alone to manage permissions iam applies permissions to all the objects contained inside the bucket or groups of objects with common name prefixes the find green option enables you to use iam and access control lists or acls together to manage permissions acls are a legacy access control system for cloud storage designed for interoperability with amazon s3 for those of you who use aws you can specify access and apply permissions at both the bucket level and per individual object and i will also be diving more into depth with access control in just a bit and just as a note labels are an optional item for bucket creation like every other resource creation process in gcp now that we've covered buckets i wanted to cover what is stored in those buckets which is objects and objects are the individual pieces of data or data chunks that you store in a cloud storage bucket and there is no limit on the number of objects that you can create in a bucket so you can think of objects kind of like files objects have two components object data and object metadata object data is typically a file that you want to store in cloud storage and in this case it is the picture of the plaid bow tie and object metadata is a collection of name value pairs that describe the various properties of that object an object's name is treated as a piece of object metadata in cloud storage and must be unique within the bucket cloud storage uses a flat namespace to store objects which means that cloud storage isn't a file system hierarchy but sees all objects in a given bucket as independent with no relationship towards each other for convenience tools such as the console and gsutil work with objects that use the slash character as if they were stored in a virtual hierarchy for example you can name one object slash bow ties slash spring 2021 slash plaid bowtie.jpg when using the cloud console you can then navigate to these objects as if they were in a hierarchical directory structure under the folders bow ties and spring 2021 now i mentioned before that the part of the bucket creation is the selection of a storage class the storage class you set for an object affects the object's availability and pricing model so when you create a bucket you can specify a default storage class for the bucket when you add objects to the bucket they inherit this storage class unless explicitly set otherwise now i wanted to touch on these four storage classes now to give you a better understanding of the differences between them the first one is standard storage and is considered best for hot data or frequently accessed data and is best for shortterm use as it does not have any specified storage duration and this is excellent for use in analytical workloads and transcoding and the price for this storage class comes in at two cents per gigabyte per month next up is near line storage and this is considered hot data as well and is a lowcost storage class for storing in frequently accessed data nearline storage has a slightly lower availability a 30day minimum storage duration and comes with the cost for data access nearline storage is ideal if you're looking to continuously add files but only plan to access them once a month and is perfect for data backup and data archiving the price for this storage class comes in at a penny per gigabyte per month now cold line storage is considered cold data as it enters into more of the longer term storage classes and is a very low cost storage class for storing and frequently accessed data it comes with slightly lower availability than nearline storage a 90day minimum storage duration and comes with the cost for data access that is higher than the retrieval cost for nearline storage coldline storage is ideal for data you plan to read or modify at most once a quarter and is perfect for data backup and data archiving the price for this storage class comes in at less than half of a penny per gigabyte per month and finally archive storage is the lowest cost highly durable storage service for data archiving online backup and disaster recovery and even coming in at a lowest cost the data access is still available within milliseconds archive storage comes in at a higher cost for data retrieval as well as a day minimum storage duration and is the best choice for data that you plan to access less than once a year archive storage also comes with the highest price for data retrieval and it is ideal for archive data storage that's used for regulatory purposes or disaster recovery data in the event that there is an oopsies in your environment the price of the storage class comes in at a ridiculously low price per gigabyte per month at a fraction of a penny per gigabyte per month now when it comes to choosing your geographic location this will determine the availability of your data here as you can see the highest availability is the standard multiregion whereas archive has the lowest availability when stored in a regional setting now when it comes to the durability of your data meaning the measurement of how healthy and resilient your data is from data loss or data corruption google cloud boasts 11 9's durability annually on all data stored in any storage class on cloud storage so know that your data is stored safely and will be there holding the same integrity from the day you stored it now when it comes to granting permissions to your cloud storage buckets and the objects within them there are four different options to choose from the first is iam permissions and these are the standard permissions that control all your other resources in google cloud and follow the same topdown hierarchy that we discussed earlier the next available option are access control list or acls and these define who has access to your buckets and objects as well as what type of access they have and these can work in tandem with im permissions moving on to sign urls these are time limited reader write access urls that can be created by you to give access to the object in question for the duration that you specify and lastly is sign policy documents and these are documents to specify what can be uploaded to a bucket and i will be going into each one of these in a bit of detail now cloud storage offers two systems for granting users permission to access your buckets and objects iam and access control lists these systems act in parallel in order for a user to access a cloud storage resource only one of the systems needs to grant the user permission im is always the recommended method when it comes to giving access to buckets and the objects within those buckets granting roles at the bucket level does not affect any existing roles that you granted at the project level and vice versa giving you two levels of granularity to customize your permissions so for instance you can give a user permission to read objects in any bucket but permissions to create objects only in one specific bucket the roles that are available through iam are the primitive standard storage roles or the legacy roles which are equivalent to acls now acls are there if you need to customize access and really get granular with individual objects within a bucket and are used to define who has access to your buckets and objects as well as what level of access they have each acl consists of one or more entries and gives a specific user or group the ability to perform specific actions each entry consists of two pieces of information a permission which defines what actions can be performed and a scope which defines who can perform the specified actions now acls should be used with caution as iam roles and acls overlap cloud storage will grant a broader permission so if you allow specific users access to an object in a bucket and then an acl is applied to that object to make it public then it will be publicly accessible so please be aware now a signed url is a url that provides limited permission and time to make a request sign urls contain authentication information allowing users without credentials to perform specific actions on a resource when you generate a signed url you specify a user or service account which must have sufficient permission to make the request that the sign url will make after you generate a signed url anyone who possesses it can use the sign url to perform specified actions such as reading an object within a specified period of time now if you want to provide public access to a user who doesn't have an account you can provide a signed url to that user which gives the user read write or delete access to that resource for a limited time you specify an expiration date when you create the sign url so anyone who knows the url can access the resource until the expiration time for the url is reached or the key used to sign the url is rotated and the command to create the sign url is shown here and as you can see has been assigned for a limited time of 10 minutes so as you've seen when it comes to cloud storage there are so many configuration options to choose from and lots of different ways to store and give access and this makes this resource from google cloud such a flexible option and full of great potential for many different types of workloads this is also a service that comes up a lot in the exam as one of the many different storage options to choose from and so knowing the features storage classes pricing and access options will definitely give you a leg up when you are presented with questions regarding storage and so that's pretty much all i wanted to cover when it comes to this overview on cloud storage so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i will be covering object versioning and life cycle management a feature within cloud storage that is used to manage and sort through older files that need to be deleted along with files that are not in high need of regular access knowing the capabilities of these two features can really help organize accumulated objects in storage buckets and cut down on costs so without further ado let's dive in now to understand a bit more about objects i wanted to dive into immutability and versioning now objects are immutable which means that an uploaded object cannot change throughout its storage lifetime an object's storage lifetime is the time between a successful object creation or upload and successful object deletion this means that you cannot edit objects in place instead objects are always replaced with a new version so after the upload of the new object completes the new version of the object is served to readers this replacement marks the end of one object's life cycle and the beginning of a new one now to support the retrieval of objects that are deleted or replaced cloud storage offers the object versioning feature object versioning retains a noncurrent object version when the live object version gets replaced or deleted enabling object versioning increases storage costs which can be partially mitigated by configuring object lifecycle management to delete older object versions but more on that in just a bit cloud storage uses two properties that together identify the version of an object the generation which identifies the version of the object's data and the meta generation which identifies the version of the object's metadata these properties are always present with every version of the object even if object versioning is not enabled these properties can be used to enforce ordering of updates so in order to enable object versioning you would do that by enabling it on a bucket once enabled older versions remain in your bucket when a replacement or deletion occurs so by default when you replace an object cloud storage deletes the old version and adds a new version these older versions retain the name of the object but are uniquely identified by their generation number when object versioning has created an older version of an object you can use the generation number to refer to the older version this allows you to restore a replaced object in your bucket or permanently delete older object versions that you no longer need and so touching back on cost for just a minute these versions can really add up and start costing you some serious money if you have thousands of files with hundreds of versions and this is where life cycle management comes into play now cloud storage offers the object lifecycle management feature in order to support some common use cases like setting a time to live or ttl for objects retaining noncurrent versions of objects or downgrading storage classes of objects to help manage costs now in order to apply this feature to your objects you would assign a lifecycle management configuration to a bucket the configuration contains a set of rules which apply to current and feature objects in the bucket when an object meets the criteria of one of the rules cloud storage automatically performs the specified action on the object and so some example use cases are shown here so if you're looking to downgrade the storage class of objects older than 365 days to cold line storage for compliance purposes along with saving money life cycle management is perfect for this another use case is when you want to delete objects created before january 1st of 2020 and this is another great use case to save money as well with keeping only the three most recent versions of each object in a bucket with versioning enabled to keep from version objects building up object lifecycle management has so many other use cases across a myriad of industries and when used correctly is a great way to achieve object management along with saving money now i wanted to take a moment to dive into the lifecycle management configuration each lifecycle management configuration contains a set of components these are a set of rules conditions and the action when the conditions are met rules are any set of conditions for any action conditions is something an object must meet before the action defined in the rule occurs on the object and there are various conditions to choose from that allows you to get pretty granular and finally the action which is where you would have the option to delete or set storage class now when you delete current versions this will move the current version into a noncurrent state and when you delete a noncurrent version you will permanently delete the version and cannot get it back and so when you set the storage class it will transition the object to a different storage class so when defining a rule you can specify any set of conditions for any action if you specify multiple conditions in a rule an object has to match all of the conditions for the action to be taken so if you have three conditions and one of those conditions have not been met then the action will not take place if you specify multiple rules that contain the same action the action is taken when an object matches the conditions in any of these rules now if multiple rules have their conditions satisfied simultaneously for a single object cloud storage will either perform the delete action as it takes precedence over the set storage class action or the set storage class action that switches the object to the storage class with the lowest at rest storage pricing takes precedence so for example if you have one rule that deletes an object and another rule that changes the object storage class but both rules use the exact same condition the delete action always occurs when the condition is met or if you have one rule that changes the object storage class to near line storage and another rule that changes the object storage class to cold line storage but both rules use the exact same condition the object storage class always changes to cold line storage when the condition is met and so some considerations that i wanted to point out when it comes to cloud storage is that when it comes to object life cycle management changes are in accordance to object creation date as well once an object is deleted it cannot be undeleted so please be careful when permanently deleting a version as well life cycle rules can take up to 24 hours to take effect so be aware when setting them and always be sure to test these life cycle rules in development first before rolling them out into production and so that's pretty much all i had to cover when it comes to versioning and object life cycle management and so you can now mark this lesson as complete and whenever you're ready join me in the console where we go handson with versioning object life cycle management and cloud storage as a whole welcome back in this demo we're going to cement the knowledge that we learned from the past couple lessons on cloud storage and really dive into the nitty gritty when it comes to the features and configuration you're first going to create a cloud storage bucket and upload some files to it and then interact with the bucket and the files using the console as well you're going to get your hands dirty using the gsutil command line tool and this is the tool for managing cloud storage from the command line now there's quite a bit of work to do here so with that being said let's dive in and so i am logged in here as tony bowties at gmail.com along with being in project bowtie inc and so the first thing i want to do is i want to create a cloud storage bucket so in order for me to do that i'm going to head over to the navigation menu and i'm going to scroll down to storage and here i already have a couple of buckets that i created from earlier lessons and you may have a couple buckets as well but you're going to go ahead and create a new bucket by going up to the top here and click on create bucket now i know that we've gone through this before in previous lessons but this time i wanted to go through all the configuration options that are available and so the first thing that you're prompted to do here is to name your bucket as explained in an earlier lesson it needs to be a globally unique name and so you can pick any name you choose and so for me i'm going to call this bucket bowtie inc dash 2021 i'm going to hit continue and if it wasn't a globally unique name it would error out and you would have to enter in a new name but since this bucket name is globally unique i'm able to move forward for location type you can select from region dual region and multi region with multi region under location you can select from either the americas europe or asia pacific and under dual region you have the options of again choosing from america's europe and asia pacific and you will be given the regions for each and so for this demo we're going to go ahead and choose region and we're going to keep the location as u.s east one and once you've selected that you can go ahead and hit continue and you're going to be prompted to choose a default storage class and here you have the option of selecting from the four storage classes that we discussed in an earlier lesson and so for this demo you can keep it as standard and simply click on continue and so here you're prompted to choose access control and because we're going to be diving into acls you can keep this as the default fine grain access control you can go ahead and click continue and under encryption you can keep it as the default google manage key but know that you always have the option of choosing a customer manage key and once you've uploaded your customer manage key you can select it from here and because i have no customer managed keys no other keys show up so i'm going to click on google manage keys and here under retention policy i know i haven't touched into that but just to give you some context when placing a retention policy on a bucket it ensures that all current and future objects in the bucket can't be deleted or replaced until they reach the age that you define in the retention policy so if you try to delete or replace objects where the age is less than the retention period it will obviously fail and this is great for compliance purposes in areas where logs need to be audited by regulators every year or where government required retention periods apply as well with the retention policy you have the option of locking that retention policy and when you lock a retention policy on a bucket you prevent the policy from ever being removed or the retention period from ever being reduced and this feature is irreversible so please be aware if you're ever experimenting with lock retention policies so if i set a retention policy here i can retain objects for a certain amount of seconds days months and years and for this demo we're not going to set any retention policies so i'm going to check that off and i'm going to go ahead and add a label with the key being environment and the value being test and just as a note before you go ahead and click on create over on the right hand side you will see a monthly cost estimate and you will be given an estimate with storage and retrieval as well as how much it costs for operations your sla and your estimated monthly cost and so before creating any buckets you can always do a price check to see how much it'll cost for storage size retrieval to get a good idea of how much it'll cost you monthly okay so once you're all done here you can simply click on create and it'll go ahead and create your bucket and so now that your bucket is created we want to add some files and so we first want to go into copying files from an instance to your cloud storage bucket and so in order to do that we need to create an instance and so we're gonna go back over to the navigation menu we're gonna scroll down to compute engine and we're gonna create our instance and for those who do not have your default vpc set up please be sure to create one before going ahead and creating your instance i'm going to go ahead and click on create i'm going to name this instance bowtie instance going to give it a label of environment test click on save the region is going to be east one and you can keep the default zone as us east 1b the machine type we're going to change it to e2micro and you're going to scroll down to access scopes and here your instance is going to need access to your cloud storage bucket and so it's going to need cloud storage access so you're going to click on set access for each api scroll down to storage and for this demo we'll select full gonna leave everything else as the default and simply click on create and so we'll give it a couple minutes here for instance to create okay and my instance has been created and so now i want to create some files and copy them over to cloud storage so i'm going to first navigate over to cloud storage and into my bucket and this way you can see the files that you upload and so next you're going to open up cloud shell and make this a little bigger for better viewing and so now you're going to ssh into your instance by using the command gcloud compute ssh along with your instance name the zone flag dash dash zone with the zone of us east 1b i'm going to go ahead and hit enter and you may be prompted with a message asking to authorize this api call and you want to hit authorize and you're going to be prompted to enter a passphrase for your key pair enter it in again and one more time and success we're logged into the instance i'm going to quickly clear my screen and so i know i could have sshed into the instance from the compute engine console but i wanted to display both the console and the shell on the same screen to make viewing a bit easier as i add and remove files to and from the bucket okay and so now that you're logged in you want to create your first file that you can copy over to your bucket so you can enter in the command sudo nano file a bow ties dot text hit enter and this will allow you to open up the nano editor to edit the file of bowties.txt and here you can enter in any message that you'd like for me i'm going to enter in learning to tie a bow tie takes time okay and i'm going to hit ctrl o to save hit enter to verify the file name to right and ctrl x to exit and so now i want to copy this file up to my bucket and so here is where i'm going to use the gsutil command so i'm going to type in gsutil cp for copy the name of the file which is file of bowties text along with gs colon forward slash forward slash and the name of your bucket which in my case is bow tie ink dash 2021 and this should copy my file file a bowties.txt up to my bucket of bow tie inc 2021 i'm gonna hit enter okay and it's finished copying over and if i go up here to the top right and click on refresh i can see that my file successfully uploaded and this is a great and easy method to upload any files that you may have to cloud storage okay and so now that you've copied files from your instance to your bucket you're going to now copy some files from the repo to be uploaded to cloud storage for our next step so you're gonna go ahead and exit out of the instance by just simply typing in exit i'm gonna quickly clear the screen and so here i need to clone my repo if you already have clone the repo then you can skip this step i'm going to cd tilde to make sure i'm in my home directory i'm going to do an ls and so i can see here that i've already cloned my repo so i'm going to cd into that directory and i'm going to run the command git pull to get the latest files fantastic i'm going to now clear my screen and i'm going to cd back to my home directory and so now i want to copy up the files that i want to work with to my cloud storage bucket and they are two jpegs by the name of pink elephantbowtie as well as plaid bowtie and these files can be found in the repo marked 12 storage services under zero one cloud storage management and i will be providing this in the lesson text as well as can be found in the instructions and so i'm going to simply cd into that directory by typing in cd google cloud associate cloud engineer 12 storage services and 0 1 cloud storage management i'm going to list all the files in the directory and as you can see here pink elephant dash bow tie and plaid bow tie are both here and so i'm going to quickly clear my screen and so now for me to copy these files i'm going to use the command gsutil cp for copy star.jpg which is all the jpegs that are available along with gs colon forward slash forward slash and the bucket name which is bow tie inc dash 2021 i'm going to hit enter and it says that it's successfully copied the files i'm going to simply go up to the top right hand corner and do another refresh and success the files have been successfully uploaded another perfect example of copying files from another source to your bucket using the gsutil command line tool and so this is the end of part one of this demo it was getting a bit long so i decided to break it up and this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready part two will be starting immediately from the end of part one so you can complete this video and i will see you in part two this is part two of the managing cloud storage access demo and we'll be starting exactly where we left off in part 1. so with that being said let's dive in and so now that we've uploaded all these files we next want to make this bucket publicly available now please know that leaving a bucket public is not common practice and should only be used on the rare occasion that you are hosting a static website from your bucket and should always be kept private whenever possible especially in a production environment so please note that this is only for the purposes of this demo and so i'm going to quickly show this to you in the console so i'm going to shut down the cloud shell for just a minute and i'm going to go to the top menu and click on permissions and under permissions i'm going to click on add here you can add new members and because you want to make it publicly available you want to use the all users member so you type in all and you should get a popup bringing up all users and all authenticated users you want to click on all users and the role that you want to select for this demo is going to be storage object viewer so i'm going to type in storage object viewer and here it should pop up and select that and then you can click on save you're going to be prompted to make sure that this is what you want to do that you want to make this bucket public and so yes we do so you can simply click on allow public access and you will get a banner up here at the top saying that this bucket is public to internet and is a great fail safe to have in case you were to ever mistakenly make your bucket public and if i head back over to objects you can see that public access is available to all the files in the bucket and so just to verify this i'm going to copy the public url for pink elephant dash bowtie i'm going to open up a new tab paste in the url hit enter and as you can see i have public access to this picture and close this tab and so now that we've done our demo to make the bucket publicly accessible we should go ahead and remove public access so in order to remove public permissions i can simply go up to permissions and simply click on remove public permissions i'm going to get a prompt to make sure this is exactly what i want to do and yes it is so you can click on remove public permissions a very simple and elegant solution in order to remove public access from your bucket and if you go back to objects you'll see that all the public access has been removed from all the files and so now that you've experienced how to add public access to a bucket i wanted to get a little bit more granular and so we're going to go ahead and apply acl permissions for one specific object and because i like pink elephants let's go ahead and select pink elephant dash bow tie and so here i can go up to the top menu and click on edit permissions and i'll be prompted with a new window for permissions that are currently available for this object you can click on add entry click on the drop down and select public from the dropdown and it will automatically auto populate the name which is all users and the access which will be reader i'm going to go ahead and click on save and a public url will be generated and so just to verify this i'm going to click on the public url and success i now have public access to this picture yet once again i'm going to close down this tab and so now that you've configured this object for public access i want to show you how to remove public access using the command line this time so you're going to go up to the top right hand corner and open up cloud shell i'm going to quickly clear my screen and i'm going to paste in the command here which is gsutil acl ch for change minus d which is delete the name of the user which is all users and if this was a regular user you could enter in their email address along with gs colon forward slash forward slash the bucket name which in my case is bow tie ink dash 2021 and the name of the file which is pink elephant bow tie dot jpeg i'm going to hit enter and it says that it's been successfully updated and so if i go back up here to the console and i back out and go back into the file i can see here that the public url has been removed okay and now there's one last step that we need to do before ending this demo and this is to create a signed url for the file so in order to create a signed url we first need to create a private key and so we're gonna do this using a service account and so i'm gonna head on over to iam so i'm going to go up to the navigation menu i'm going to go to i am an admin and here with the menu on the left i'm going to click on service accounts here up at the top menu you're going to click on create service account and under service account name you can enter in any name but for me i'm going to enter in signed url i'm going to leave everything else as is i'm going to simply click on create i'm going to close down cloud shell because i don't really need it right now just select a role and i'm going to give it the role of storage object viewer i'm going to click on continue and i'm going to leave the rest blank and simply click on done and you should see a service account with the name of signed url and so in order to create a key i'm going to simply go over to actions and i'm going to click on the three dots and i'm going to select create key from the drop down menu and here i'm going to be prompted with what type of key that i want to create and you want to make sure that json is selected and simply click on create and this is where your key will be automatically downloaded to your downloads folder i'm going to click on close and so once you have your key downloaded you're able to start the process of generating a signed url and so i'm going to go ahead and use cloud shell in order to generate this signed url so i'm going to go ahead back up to the top and open up cloud shell again and then you can open up the cloud shell editor going to go up to the top menu in editor and click on file and you're going to select upload files and here's where you upload your key from your downloads folder and i can see my key has been uploaded right here and you can rename your key file to something a little bit more human readable so i'm going to right click i'm going to click on rename and you can rename this file as privatekey.json hit ok and so once you have your key uploaded and renamed you can now go back into the terminal to generate a signed url i'm going to quickly clear the screen i'm going to make sure that the private key is in my path by typing in ls and as you can see here privatekey.json is indeed in my path and so before i generate this key i'm going to head back on over to cloud storage i'm going to drill down into my bucket and as you can see here pink elephant dash bow tie does not have a public url and so when the sign url is generated you will get a public url that will not be shown here in the console and will be private to only the user that generated it and the users that the url has been distributed to okay and once you have everything in place you can then go ahead and paste in the command gsutil sign url minus d the allotted time which is 10 minutes the private key which is private key dot json along with gs colon forward slash forward slash your bucket name which in my case is bow tie ink dash 2021 along with the file name of pinkelephantbowtie.jpg i'm going to hit enter and so i purposely left this error here so you can see that when you generate a signed url you need pi open ssl in order to generate it and so the caveat here is that because python 2 is being deprecated the command pip install pi openssl will not work pi open ssl needs to be installed with python3 and so to install it you're going to run the command pip3 install pi open ssl and hit enter and so once it's finished installing you can now generate your signed url i'm going to quickly clear my screen paste in the command again hit enter and success you've now generated a sign url for the object pink elephant bowtie.jpg and because this is a signed url you will see under public url there is no url there available even though it is publicly accessible and so just to verify this i'm going to highlight the link here i'm going to copy it i'm going to open up a new tab i'm going to paste in this url hit enter and success this sign url is working and anyone who has access to it has viewing permissions of the file for 10 minutes and so again this is a great method for giving someone access to an object who doesn't have an account and will give them a limited time to view or edit this object and so i wanted to congratulate you on making it through this demo and hope that it has been extremely useful in excelling your knowledge on managing buckets files and access to the buckets and files in cloud storage and so just as a recap you created a cloud storage bucket you then created an instance and copied a file from that instance to the bucket you then clone your repo to cloud shell and copy two jpeg files to your cloud storage bucket you then assigned and then removed public access to your bucket and then applied an acl to a file in the bucket making it public as well as removing public access right after you then created a service account private key and generated a signed url to an object in that bucket congratulations again on a job well done and so that's pretty much all i wanted to cover in this demo on managing cloud storage access so you can now mark this as complete and let's move on to the next one welcome back in this demo we're going to be getting into the weeds with object versioning and life cycle management using both the console and the command line we're going to go through how versioning works and what happens when objects get promoted along with creation configuration and editing these life cycle policies and so with that being said let's dive in so we're going to be starting off from where we left off in the last demo with all the resources intact that we created before and we're going to go ahead and dive right into versioning and so the first thing that you want to do is turn on versioning for your current bucket so in my case for bow tie ink dash 2021 and we're going to do this through the command line so i'm going to first go up to the top right hand corner and open up cloud shell and so you first want to see if versioning is turned on for your bucket and you can do this by using the command gsutil versioning get along with gs colon forward slash forward slash with your bucket name and hit enter and you may be prompted with a message asking you to authorize this api call you definitely want to authorize and as expected versioning is not turned on on this bucket hence the return of suspended and so in order to turn versioning on we're going to use a similar command gsutil versioning and instead of get we're going to use set on gs colon forward slash forward slash and the bucket name and hit enter and versioning has been enabled and so if i run the command gsutil version in get again i'll get a response of enabled okay great now that we have versioning enabled we can go ahead with the next step which is to delete one of the files in the bucket and so you can go ahead and select plaid bowtie.jpg and simply click on delete you can confirm the deletion and the file has been deleted now technically the file has not been deleted it is merely been converted to a noncurrent version and so in order to check the current and noncurrent versions i'm going to use the command gsutil ls minus a along with the bucket name of g s colon forward slash forward slash bow tie inc dash 2021 i'm gonna hit enter and as you can see here plaid bow tie still shows up the ls minus a command is a linux command to show all files including the hidden files and so what's different about these files is right after the dot text or dot jpg you will see a hashtag number and this is the generation number and this determines the version of each object and so what i want to do now is bring back the noncurrent version and make it current so i'm going to promote the noncurrent version of plaid bowtie.jpg to the current version and so in order to do this i'm going to run the command gsutil and v for move along with the bucket of gs colon forward slash forward slash bowtie inc hyphen 2021 and the name of the file of plaid bow tie dot jpeg along with the generation number and i'm going to copy it from the currently listed i'm going to paste it in and so now we need to put in the target which is going to be the same without the generation number and paste that in then hit enter okay operation completed and so if i go up to the top right hand corner and click on refresh i can see that now there is a current version for plaid bow tie now just know that using the move command actually deletes the noncurrent version and gives the new current version a new generation number and so in order to verify this i'm going to quickly clear my screen and i'm going to run the command gsutil ls minus a along with the bucket name a bow tie inc dash 2021 and the generation number here is different than that of the last now if i use the cp or copy command it would leave the noncurrent version and create a new version on top of that leaving two objects with two different generation numbers okay so with that step being done you now want to log into your linux instance and we're going to be doing some versioning for file of bowties.text so i'm going to go ahead and clear my screen again and i'm going to run the command gcloud compute ssh bowtie instance which is the name of my instance along with the zone flag dash dash zone of the zone us east 1b i'm going to hit enter and you should be prompted for the passphrase of your key and i'm in and so here you want to edit file a bowties.txt to a different version so you can go ahead and run the command sudo nano file a bow ties dot text and hit enter and you should have learning to tie a bow tie takes time and what you want to do is append version 2 right at the end ctrl o to save enter to verify the file name to right and control x to exit and so now we want to copy file a bow ties dot text to your current bucket mine being bow tie ink dash 2021 so i'm going to go ahead and run the command gsutil cp the name of the file which is file of bowties dot text and the target which is going to be bowtie inc 2021 and hit enter and it's copied the file to the bucket and so if i hit refresh in the console you can see that there is only one version of file of bowties.text and so to check on all the versions that i have i'm going to go back to my cloud shell i'm going to quickly clear my screen and i'm going to run the command gsutil ls minus a along with the target bucket hit enter and as you can see here there are now two versions of file of bowties.text and if i quickly open this up i'm gonna click on the url you can see here that this is version two and so this should be the latest generation of file of bowties.txt that you edited over in your instance i'm going to close this tab now and so what i want to do now is i want to promote the noncurrent version to be the current version in essence making version 2 the noncurrent version and so i'm going to run the command gsutil cp and i'm going to take the older generation number and i'm going to copy it and paste it here and the target is going to be the same without the generation number and paste it and hit enter okay and the file has been copied over so i'm going to do a quick refresh in the console i'm going to drill down into file a bowties.txt and when i click on the url link it should come up as version 1. and so this is a way to promote noncurrent versions to current versions using the gsutil copy command or the gsutil move command i'm going to close on this tab now i'm going to quickly clear my screen and if i run the command gsutil ls minus a again you can see that i have even more files and so these files and versions of files will eventually accumulate and continuously take up space along with costing you money and so in order to mitigate this a good idea would be to put life cycle policies into place and so you're gonna go ahead now and add a life cycle policy to the bucket and this will help manage the evergrowing accumulation of files as more files are being added to the bucket and more versions are being produced something that is very common that is seen in many different environments and so we're going to go ahead and get this done in the console so i'm going to close down cloud shell and i'm going to go back to the main page of the bucket and under the menu you can click on lifecycle and here you'll be able to add the lifecycle rules and so here you're going to click on add a rule and the first thing that you're prompted to do is to select an action and so the first rule you're going to apply is to delete noncurrent objects after seven days so you're gonna click on delete object you're gonna be prompted with a warning gonna hit continue and you'll be prompted to select object conditions and as discussed in an earlier lesson there are many conditions to choose from and multiple conditions can be selected so here you're going to select days since becoming noncurrent and in the empty field you're going to type in 7. you can click on continue and before you click on create i wanted just to note that any life cycle rule can take up to 24 hours to take effect so i'm going to click on create and here you can see the rule has been applied to delete objects after seven days when object becomes noncurrent and so now that we added a delete rule we're going to go ahead and add another rule to move current files that are not being used to a storage class that can save the company money and so let's go ahead and create another lifecycle rule but this time to use this set storage class action and so the files that accumulate that have been there for over 90 days you want to set the storage class the cold line so this way it'll save you some money and so you're going to click on add a rule you're going to select set storage class to cold line and as a note here it says archive objects will not be changed to cold line so you can move forward with the storage class but you can't move backwards in other words i can't move from cold line to near line or archive the cold line i can only move from near line to cold line or cold line to archive so i'm going to go ahead and click continue for the object conditions you want to select age and in the field you want to enter 90 days and here you want to hit continue and finally click on create and so in order to actually see these rules take effect like i said before it'll take up to 24 hours and so before we end this demo i wanted to show you another way to edit a life cycle policy by editing the json file itself so you can head on up to the top right and open up cloud shell i'm going to bring this down a little bit and you're going to run the command gsutil lifecycle get along with the bucket name and output it to a file called lifecycle.json and hit enter and no errors so that's a good sign next i'm going to run the command ls and as you can see here the lifecycle.json file has been written and so i'd like to edit this file where it changes the set to cold line rule from 90 days to 120 days as tony bowtie's manager thinks that they should keep the files a little bit longer before sending it to coldline and so in order to edit this file you're going to run the command sudo nano along with the name of the file of lifecycle.js you hit enter and it's going to be a long string but if you use your arrow keys and move down and then back you'll see the set to cold line rule with the age of 90 days so i'm going to move over here and i'm going to edit this to 120 and i'm going to hit ctrl o to save enter to verify file name to write and ctrl x to exit and just know that you can also edit this file in cloud shell editor and so in order for me to put this lifecycle policy in place i need to set this as the new lifecycle policy and so in order for me to do that i'm going to run the command gsutil lifecycle set along with the name of the json file which is lifecycle.json along with the bucket name and hit enter and it looks like it said it and i'm going to do quick refresh in the console just to verify and success the rule has been changed from 90 days to 120 days congratulations on completing this demo now a lot of what you've experienced here is more of what you will see in the architect exam as the cloud engineer exam focuses on more of the high level theory of these cloud storage features but i wanted to show you some real life scenarios and how to apply the theory that was shown in previous lessons into practice and so just as a recap you set versioning on the current bucket that you are working in and you deleted a file and made it noncurrent you then brought it back to be current again you then edited a file on your instance and copied it over to replace the current version of that file in your bucket you then promoted the noncurrent version as the new one and moved into lifecycle rules where you created two separate rules you created a rule to delete files along with the rule to set storage class after a certain age of the file and the last step you took was to copy the lifecycle policy to your cloud shell and edited that policy and set it to a newer edited version and so that pretty much covers this demo on object versioning and lifecycle management congratulations again on a job well done and so before you go make sure you delete all the resources you've created for the past couple of demos as you want to make sure that you're not accumulating any unnecessary costs and so i'm going to do a quick run through on deleting these resources and so i'm going to quickly close down cloud shell and i'm going to head on over to the navigation menu go to compute engine i'm going to delete my instance and i'm going to head back on over to cloud storage and delete the bucket there i'm going to confirm the deletion i'm going to click on delete and so that covers the deletion of all the resources so you can now mark this as complete and i'll see you in the next one welcome back and in this lesson i'm going to be covering cloud sql one of google cloud's many database offerings that offers reliable secure and scalable sql databases without having to worry about the complexity to set it all up now there's quite a bit to cover here so with that being said let's dive in now cloud sql is a fully managed cloud native relational database service that offers mysql postgres and sql server engines with builtin support for replication cloud sql is a database as a service offering from google where google takes care of all the underlying infrastructure for the database along with the operating system and the database software now because there are a few different types of database offerings from google cloud sql was designed for low latency transactional and relational database workloads it's also available in three different flavors of databases mysql postgres and the newest edition is sql server and all of them support standard apis for connectivity cloud sql offers replication using different types of read replicas which i will get into a little bit later and offers capabilities for high availability for continuous access to your data cloud sql also offers backups in two different flavors and allows you to restore your database from these backups with the same amount of ease now along with your backups comes point in time recovery for when you want to restore a database from a specific point in time cloud sql storage relies on connected persistent disks in the same zone that are available in regular hard disk drives or ssds that currently give you up to 30 terabytes of storage capacity and because the same technologies lie in the background for persistent disks automatic storage increase is available to resize your disks for more storage cloud sql also offers encryption at rest and in transit for securing data entering and leaving your instance and when it comes to costs you are billed for cpu memory and storage of the instance along with egress traffic as well please be aware that there is a licensing cost when it comes to windows instances now cloud sql instances are not available in the same instance types as compute engine and are only available in the shared core standard and high memory cpu types and when you see them they will be clearly marked with a db on the beginning of the cpu type you cannot customize these instances like you can with compute engine and so memory will be predefined when choosing the instance type now storage types for cloud sql are only available in hard disk drives and ssds you are able to size them according to your needs and as stated earlier can be sized up to 30 terabytes in size and when entering the danger zone of having a full disk you do have the option of enabling automatic storage increase so you never have to worry about filling up your disk before that 30 terabyte limit now when it comes to connecting to your cloud sql instance you can configure it with a public or private ip but know that after configuring the instance with a private ip it cannot be changed although connecting with the private ip is preferred when connecting from a client on a resource with access to a vpc as well it is always best practice to use private i p addresses for any database in your environment whenever you can now moving on to authentication options the recommended method to connecting to your cloud sql instance is using cloud sql proxy the cloud sql proxy allows you to authorize and secure your connections using iam permissions unless using the cloud sql proxy connections to an instance's public ip address are only allowed if the connection comes from an authorized network authorized networks are ip addresses or ranges that the user has specified as having permission to connect once you are authorized you can connect to your instance through external clients or applications and even other google cloud services like compute engine gke app engine cloud functions and cloud run now i wanted to focus a moment here on the recommended method for connecting to your instance which is cloud sql proxy now as mentioned before the cloud sql proxy allows you to authorize and secure your connections using iam permissions the proxy validates connections using credentials for a user or service account and wrapping the connection in an ssl tls layer that is authorized for a cloud sql instance using the cloud sql proxy is the recommended method for authenticating connections to a cloud sql instance as it is the most secure the client proxy is an open source library distributed as an executable binary and is available for linux macos and windows the client proxy acts as an intermediary server that listens for incoming connections wraps them in ssl or tls and then passes them to a cloud sql instance the cloud sql proxy handles authentication with cloud sql providing secure access to cloud sql instances without the need to manage allowed ip addresses or configure ssl connections as well this is also the best solution for applications that hold ephemeral eyepiece and while the proxy can listen on any port it only creates outgoing connections to your cloud sql instance on port 3307 now when it comes to database replication it's more than just copying your data from one database to another the primary reason for using replication is to scale the use of data in a database without degrading performance other reasons include migrating data between regions and platforms and from an onpremises database to cloud sql you could also promote a replica if the original instance becomes corrupted and i'll be getting into promoting replicas a little bit later now when it comes to a cloud sql instance the instance that is replicated is called a primary instance and the copies are called read replicas the primary instance and read replicas all reside in cloud sql read replicas are readonly and you cannot write to them the read replica processes queries read requests and analytics traffics thus reducing the load on the primary instance read replicas can have more cpus in memory than the primary instance but they cannot have any less and you can have up to 10 read replicas per primary instance and you can connect to a replica directly using its connection name and ip address cloud sql supports the following types of replicas read replicas cross region read replicas external read replicas and cloud sql replicas when replicating from an external server now when it comes to read replicas you would use it to offload work from a cloud sql instance the read replica is an exact copy of the primary instance and data and other changes on the primary instance are updated in almost real time on the read replica a read replica is created in a different region from the primary instance and you can create a cross region read replica the same way as you would create an inregion replica this improves read performance by making replicas available closer to your application's region it also provides additional disaster recovery capability to guard you against a regional failure it also lets you migrate data from one region to another with minimum downtime and lastly when it comes to external read replicas these are external mysql instances that replicate from a cloud sql primary instance for example a mysql instance running on compute engine is considered an external instance and so just as a quick note here before you can create a read replica of a primary cloud sql instance the instance must meet the following requirements automated backups must be enabled binary logging must be enabled which requires pointintime recovery to be enabled and at least one backup must have been created after binary logging was enabled and so when you have read replicas in your environment it gives you the flexibility of promoting those replicas if needed now promoting replicas is a feature that can be used for when your primary database becomes corrupted or unreachable now you can promote an inregion read replica or crossregion rereplica depending on where you have your read replicas hosted so when you promote a read replica the instance stops replication and converts the instance to a standalone cloud sql primary instance with read and write capabilities please note that this cannot be undone and also note that when your new primary instance has started your other read replicas are not transferred over from the old primary instance you will need to reconnect your other read replicas to your new primary instance and as you can see here promoting a replica is done manually and intentionally whereas high availability has a standby instance that automatically becomes the primary in case of a failure horizontal outage now when it comes to promoting crossregion replicas there are two common scenarios for promotion regional migration which performs a planned migration of a database to a different region and disaster recovery and this is where you would fail over a database to another region in the event that the primary instances region becomes unavailable both use cases involve setting up crossregion replication and then promoting the replica the main difference between them is whether the promotion of the replica is planned or unplanned now if you're promoting your replicas for a regional migration you can use a cross region replica to migrate your database to another region with minimal downtime and this is so you can create a replica in another region wait until the replication catches up promote it and then direct your applications to the newly promoted instance the steps involved in promotion are the same as for promoting an inregion replica and so when you're promoting replicas for disaster recovery crossregion replicas can be used as part of this disaster recovery procedure you can promote a crossregion replica to fail over to another region should the primary instances region become unavailable for an extended period of time so in this example the entire u.s east 1 region has gone down yet the reed replica in the europe region is still up and running and although there may be a little bit more latency for your customers in north america i'm able to promote this read replica connect it to the needed resources and get back to business now moving along to high availability cloud sql offers aha capabilities out of the box the aha configuration sometimes called a cluster provides data redundancy so a cloud sql instance configured for ha is also called a regional instance and is located in a primary and secondary zone within the configured region within a regional instance the configuration is made up of a primary instance and a standby instance and through synchronous replication to each zone's persistent disk all rights made to the primary instance are also made to the standby instance each second the primary instance writes to a system database as a heartbeat signal if multiple heartbeats aren't detected failover is initiated and so if an haconfigured instance becomes unresponsive cloud sql automatically switches to serving data from the standby instance and this is called a failover in this example the primary instance or zone fails and failover is initiated so if the primary instance is unresponsive for approximately 60 seconds or the zone containing the primary instance experiences an outage failover will initiate the standby instance immediately starts serving data upon reconnection through a shared static ip address with the primary instance and the standby instance now serves data from the secondary zone and now when the primary instance is available again a fail back will happen and this is when traffic will be redirected back to the primary instance and the standby instance will go back into standby mode as well the regional persistent disk will pick up replication to the persistent disk in that same zone and with regards to billing an ha configured instance is charged at double the price of a standalone instance and this includes cpu ram and storage also note that the standby instance cannot be used for read queries and this is where it differs from read replicas as well a very important note here is that automatic backups and point in time recovery must be enabled for high availability and so the last topic that i wanted to touch on is backups and backups help you restore lost data to your cloud sql instance you can also restore an instance that is having problems from a backup you enable backups for any instance that contains necessary data backups protect your data from loss or damage enabling automated backups along with binary logging is also required for some operations such as clone and replica creation by default cloud sql stores backup data in two regions for redundancy one region can be the same region that the instance is in and the other is a different region if there are two regions in a continent the backup data remains on the same continent cloud sql also lets you select a custom location for your backup data and this is great if you need to comply with data residency regulations for your business now cloud sql performs two types of backups ondemand backups and automated backups now with ondemand backups you can create a backup at any time and this is useful for when you're making risky changes that may go sideways you can always create ondemand backups for any instance whether the instance has automatic backups enabled or not and these backups persist until you delete them or until their instance is deleted now when it comes to automated backups these use a four hour backup window these backups start during the backup window and just as a note when possible you should schedule your backups when your instance has the least activity automated backups occur every day when your instance is running at any time in the 36 hour window and by default up to seven most recent backups are retained you can also configure how many automated backups to retain from 1 to 365. now i've touched on this topic many times in this lesson and i wanted to highlight it for just a second and this is pointintime recovery so pointintime recovery helps you recover an instance to a specific point in time for example if an error causes a loss of data you can recover a database to its state before the error happened a point in time recovery always creates a new instance and you cannot perform a point in time recovery to an existing instance and point in time recovery is enabled by default when you create a new cloud sql instance and so when it comes to billing by default cloud sql retains seven days of automated backups plus all ondemand backups for an instance and so i know there is a lot to retain in this lesson on cloud sql but be sure that these concepts and knowing the difference between them as well as when to use each feature will be a sure help in the exam along with giving you the knowledge you need to use cloud sql in your role as a cloud engineer and so that's pretty much all i had to cover when it comes to cloud sql so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i wanted to touch on google cloud's global relational database called cloud spanner now cloud spanner is the same in some ways as cloud sql when it comes to asset transactions sql querying and strong consistency but differs in the way that data is handled under the hood than cloud sql and so knowing this database only at a high level is needed for the exam but i'll be going into a bit more detail just to give you a better understanding on how it works so with that being said let's dive in now cloud spanner is a fully managed relational database service that is both strongly consistent and horizontally scalable cloud spanner is another database as a service offering from google and so it strips away all the headaches of setting up and maintaining the infrastructure and software needed to run your database in the cloud now being strongly consistent in this context is when data will get passed on to all the replicas as soon as a write request comes to one of the replicas of the database cloud spanner uses truetime a highly available distributed atomic clock system that is provided to applications on all google servers it applies a time stamp to every transaction on commit and so transactions in other regions are always executed sequentially cloud spanner can distribute and manage data at a global scale and support globally consistent reads along with strongly consistent distributed transactions now being fully managed cloud spanner handles any replicas that are needed for availability of your data and optimizes performance by automatically sharding the data based on request load and size of the data part of why cloud spanner's high availability is due to its automatic synchronous data replication between all replicas in independent zones cloud spanner scales horizontally automatically within regions but it can also scale across regions for workloads that have higher availability requirements making data available faster to users at a global scale along with node redundancy quietly added for every node deployed in the instance and when you quickly add up all these features of cloud spanner it's no wonder that it's available to achieve five nines availability on a multiregional instance and four nines availability on a regional instance cloud spanner is highly secure and offers data layer encryption audit logging and iam integration cloud spanner was designed to fit the needs of specific industries such as financial services ad tech retail and global supply chain along with gaming and pricing for cloud spanner comes in at 90 cents per node per hour with the cost of storage coming in at 30 cents per gigabyte per month definitely not cheap but the features are plentiful now this isn't in the exam but i did want to take a moment to dive into the architecture for a bit more context as to why this database is of a different breed than the typical sql database now to use cloud spanner you must first create a cloud spanner instance this instance is an allocation of resources that is used by cloud spanner databases created in that instance instance creation includes two important choices the instance configuration and the node count and these choices determine the location and the amount of the instances cpu and memory along with its storage resources your configuration choice is permanent for an instance and only the node count can be changed later if needed an instance configuration defines the geographic placement and replication of the database in that instance either regional or multiregion and please note that when you choose a multizone configuration it allows you to replicate the databases data not just in multiple zones but in multiple zones across multiple regions and when it comes to the node count this determines the number of nodes to allocate to that instance these nodes allocate the amount of cpu memory and storage needed for your instance to either increase throughput or storage capacity there is no instance types to choose from like cloud sql and so when you need more power you simply add another node now for any regional configuration cloud spanner maintains exactly three read write replicas each within a different zone in that region each read write replica contains a full copy of your operational database that is able to serve rewrite and read only requests cloud spanner uses replicas in different zones so that if a single zone failure occurs your database remains available in a multiregion instance configuration the instance is allotted a combination of four read write and read only replicas and just as a note a three node configuration minimum is what is recommended for production by google and as cloud spanner gets populated with data sharding happens which is also known as a split and cloud spanner creates replicas of each database split to improve performance and availability all of the data in a split is physically stored together in a replica and cloud spanner serves each replica out of an independent failure zone and within each replica set one replica is elected to act as the leader leader replicas are responsible for handling rights while any read write or read only replica can serve a read request without communicating with the leader and so this is the inner workings of cloud spanner at a high level and not meant to confuse you but to give you a better context of how cloud spanner although it is a relational sql database is so different than its cloud sql cousin now before ending this lesson i wanted to touch on node performance for a quick moment and so each cloud spanner node can provide up to 10 000 queries per second or qps of reads or 2000 qps of writes each node provides up to two terabytes of storage and so if you need to scale up the serving and storage resources in your instance you add more nodes to that instance and remember as noted earlier that adding a node does not increase the number of replicas but rather increases the resources each replica has in the instance adding nodes gives each replica more cpu and ram which increases the replicas throughput and so if you're looking to scale up automatically you can scale the numbers of nodes in your instance based on the cloud monitoring metrics on cpu or storage utilization in conjunction with using cloud functions to trigger and so when you are deciding on a relational database that provides global distribution and horizontally scalable that handles transactional workloads in google cloud cloud spanner will always be the obvious choice over cloud sql and so that's pretty much all i have to cover when it comes to this overview on cloud spanner so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson we will be going over the available nosql databases available in google cloud this lesson is meant to be another overview just to familiarize you with the nosql database options as they show up in the exam this lesson is not meant to go in depth on databases but an overview and will give you a good understanding on what features are available for each and their use cases so with that being said let's dive in now there are four managed nosql databases available in google cloud and i will be briefly going over them and i'll be starting this off by discussing bigtable now cloud bigtable is a fully managed wide column nosql database designed for terabyte and petabyte scale workloads that offers low latency and high throughput bigtable is built for realtime application serving workloads as well as largescale analytical workloads cloud bigtable is a regional service and if using replication a copy is stored in a different zone or region for durability cloud bigtable is designed for storing very large amounts of single keyed data while still being able to provide very low latency and because throughput scales linearly you can increase the queries per second by adding more bigtable nodes when you need them bigtable throughput can be dynamically adjusted by adding or removing cluster nodes without restarting meaning you can increase the size of a bigtable cluster for just a few hours to handle a large load and then reduce the cluster size again and do it all without any downtime bigtable is an ideal source for map reduce operations and integrates easily with all the existing big data tools such as hadoop dataproc and dataflow along with apache hbase and when it comes to price bigtable is definitely no joke pricing for bigtable starts at 65 cents per hour per node or over 450 dollars a month for a one node configuration with no data now you can use bigtable to store and query all of the following types of data such as cpu and memory usage over time for multiple servers marketing data such as purchase histories and customer preferences financial data such as transaction histories stock prices and currency exchange rates iot data or internet of things such as usage reports from energy meters and home appliances and lastly graph data such as information about how users are connected to one another cloud bigtable excels as a storage engine as it can batch mapreduce operations stream processing or analytics as well as being used for storage for machine learning applications now moving on to the next nosql database is cloud datastore and cloud datastore is a highly scalable nosql document database built for automatic scaling high performance and ease of application development datastore is redundant within your location to minimize impact from points of failures and therefore can offer high availability of reads and rights cloud datastore can execute atomic transactions where a set of operations either all succeed or none occur cloud datastore uses a distributed architecture to automatically manage scaling so you never have to worry about scaling manually as well what's very unique about cloud datastore is that it has a sqllike query language that's available called gql also known as gql gql maps roughly to sql however a sql role column lookup is limited to a single value whereas in gql a property can be a multiple value property this consistency model allows an application to handle large amounts of data and users while still being able to deliver a great user experience data is automatically encrypted before it is written to disk and automatically decrypted when read by an authorized user now this does not reflect in the exam as of yet and i will be updating this lesson if and when it happens but firestore is the newest version of datastore and introduces several improvements over datastore existing datastore users can access these improvements by creating a new firestore database instance in datastore mode and in the near future all existing datastore databases will be automatically upgraded to firestore in datastore mode now moving right along cloud datastore holds a really cool feature for developers that's called datastore emulator and this provides local emulation of the production datastore environment so that you can use to develop and test your application locally this is a component of the google cloud sdks gcloud tool and can be installed by using the gcloud components install command that we discussed earlier on in the course and so moving on to use cases for datastore it is ideal for applications that rely on highly available structured data at scale you can use datastore for things like product catalogs that provide realtime inventory and product details for a retailer user profiles that deliver a customized experience based on the user's past activities and preferences as well as transactions based on asset properties for example transferring funds from one bank account to another next up we have firestore for firebase and so this is a flexible scalable nosql cloud database to store and sync data for client and server side development and is available for native c plus unity node.js java go and python sdks in addition to rest and rpc apis pretty much covering the gamut of most major programming languages now with cloud firestore you store data in documents that contain fields mapping to values these documents are stored in collections which are containers for your documents that you can use to organize your data and build queries documents support many different data types as well you can also create sub collections within documents and build hierarchical data structures cloud firestore is serverless with absolutely no servers to manage update or maintain and with automatic multiregion replication and strong consistency google is able to hold a five nines availability guarantee and so when it comes to querying in cloud firestore it is expressive efficient and flexible you can create shallow queries to retrieve data at the document level without needing to retrieve the entire collection or any nested subcollections cloud firestore uses data synchronization to update data in real time for any connected device as well it also caches data that your application is actively using so that the application can write read listen to and query data even if the device is offline when the device comes back online cloud firestore synchronizes any local changes back to cloud firestore you can also secure your data in cloud firestore with firebase authentication and cloud firestore security rules for android ios and javascript or you can use iam for server side languages and when it comes to costs firestore falls into the always available free tier where you can use one database holding five gigabytes or if you need more you can move into their paid option now firebase also has another database sharing similar features like having no servers to deploy and maintain realtime updates along with the free tier in this database is called real time database and is used for more basic querying simple data structure and keeping things to one database it's something i like to call firestore lite real time database does not show up in the exam but i wanted to bring it to light as it is part of the firebase family just know that you can use both databases within the same firebase application or project as both can store the same types of data client libraries work in a similar manner and both hold realtime updates now although firebase is a development platform and not a database service i wanted to give it a quick mention for those of you who are unfamiliar with the tiein to firestore with firebase firebase is a mobile application development platform that provides tools and cloud services to help enable developers to develop applications faster and more easily and since it ties in nicely with firestore it becomes the perfect platform for mobile application development okay so moving on to our last nosql database is memorystore and memorystore is a fully managed service from google cloud for either redis or memcached in memory datastore to build application caches and this is a common service used in many production environments specifically when the need for caching arises memory store automates the administration tasks for redis and memcached like enabling high availability failover patching and monitoring so you don't have to and when it comes to memory store for redis instances in the standard tier these are replicated across zones monitored for health and have fast automatic failover standard tier instances also provide an sla of three nines availability memory store for redis also provides the ability to scale instant sizes seamlessly so that you can start small and increase the size of the instance as needed memory store is protected from the internet using vpc networks and private ip and also comes with iam integration systems are monitored around the clock ensuring that your data is protected at all times and know that the versions are always kept up to date with the latest critical patches ensuring your instances are secure now when it comes to use cases of course the first thing you will see is caching and this is the main reason to use memory store as it provides low latency access and high throughput for heavily accessed data compared to accessing the data from a disk common examples of caching is session management frequently accessed queries scripts or pages so when using memory store for leaderboards and gaming this is a common use case in the gaming industry as well as using it for player profiles memory store is also a perfect solution for stream processing combined with data flow memory store for redis provides a scalable fast in memory store for storing intermediate data that thousands of clients can access with very low latency and so when it comes to nosql databases these are all the available options on google cloud and as i said before it will only show up on the exam at merely a high level and so knowing what each of these databases are used for will be a huge benefit along with being an entry to diving deeper into possibly using these services within your daytoday job as a cloud engineer and so that's pretty much all i wanted to cover when it comes to nosql databases available in google cloud so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson we'll be going over the big data ecosystem in an overview just to familiarize you with the services that are available in google cloud and are the services that will show up in the exam this lesson is not meant to go in depth but is an overview and will give you a good understanding on what these services can do and how they all work together to make sense of big data as a whole so getting right into it i wanted to first ask the question what is big data i mean many people talk about it but what is it really well big data refers to massive amounts of data that would typically be too expensive to store manage and analyze using traditional database systems either relational or monolithic as the amount of data that we have been seeing over the past few years has started to increase these systems have become very inefficient because of their lack of flexibility for storing unstructured data such as images text or video as well as accommodating high velocity or realtime data or scaling to support very large petabyte scale data volumes for this reason the past few years has seen the mainstream adoption of new approaches to managing and processing big data including apache hadoop and nosql database systems however those options often prove to be complex to deploy manage and use in an onpremises situation now the ability to consistently get business value from data fast and efficiently is now becoming the de facto of successful organizations across every industry the more data a company has access to the more business insights and business value they're able to achieve like gain useful insights increase revenue get or retain customers and even improve operations and because machine learning models get more efficient as they are trained with more data machine learning and big data are highly complementary all in all big data brings some really great value to the table that is impossible for any organization to turn down and so now that we've gone through that overview of what big data is i wanted to dive into some shorter overviews of the services available for the big data ecosystem on google cloud and so the first service that i'd like to start with is bigquery now bigquery is a fully managed serverless data warehouse that enables scalable analysis over petabytes of data this service supports querying using sql and holds builtin machine learning capabilities you start by ingesting data into bigquery and then you are able to take advantage of all the power it provides so big data would ingest that data by doing a batch upload or by streaming it in real time and you can use any of the currently available google cloud services to load data into bigquery you can take a manual batch ingestion approach or stream using pub sub etl data and with bigquery data transfer service you can automatically transfer data from external google data sources and partner sas applications to bigquery on a scheduled and fully managed basis and the best part is batch and export is free bigquery's highspeed streaming api provides an incredible foundation for realtime analytics making business data immediately available for analysis and you can also leverage pub sub and data flow to stream data into bigquery bigquery transparently and automatically provides highly durable replicated storage in multiple locations for high availability as well as being able to achieve easy resource bigquery keeps a seven day history of changes in case something were to go wrong bigquery supports standard sql querying which reduces the need for code rewrites you can simply use it as you would for querying any other sql compliant database and with dataproc and dataflow bigquery provides integration with the apache big data ecosystem allowing existing hadoop spark and beam workloads to read or write data directly from bigquery using the storage api bigquery also makes it very easy to access this data by using the cloud console using the bq command line tool or making calls to the bigquery rest api using a variety of client libraries such as java.net or python there are also a variety of thirdparty tools that you can use to interact with bigquery when visualizing the data or loading the data bigquery provides strong security and governance controls with finegrained controls through integration with identity and access management bigquery gives you the option of geographic data control without the headaches of setting up and managing clusters and other computing resources in different zones and regions bigquery also provides fine grain identity and access management and rest assured that your data is always encrypted at rest and in transit now the way that bigquery calculates billing charges is by queries and by storage storing data in bigquery is comparable in price with storing data in cloud storage which makes it an easy decision for storing data in bigquery there is no upper limit to the amount of data that can be stored in bigquery so if tables are not edited for 90 days the price of storage for that table drops by 50 percent query costs are also available as ondemand and flat rate pricing and when it comes to ondemand pricing you are only charged for bytes read not bytes returned in the end bigquery scales seamlessly to store and analyze petabytes to exabytes of data with ease now there are so many more features to list but if you are interested feel free to dive into the other features with the supplied link in the lesson text now moving on to the next service is pub sub and pub sub is a fully managed realtime messaging service that allows you to send and receive messages between independent applications it acts as messaging oriented middleware or event ingestion and delivery for streaming analytics pipelines and so a publisher application creates and send messages to a topic subscriber applications create a subscription to a topic and receives messages from it and so i wanted to take a moment to show you exactly how it works so first the publisher creates messages and sends them to the messaging service on a specified topic a topic is a named entity that represents a feed of messages a publisher application creates a topic in the pub sub service and sends messages to that topic a message contains a payload and optional attributes that describe the content the service as a whole ensures that published messages are retained on behalf of subscriptions and so a published message is retained for a subscription in a message queue shown here as message storage until it is acknowledged by any subscriber consuming messages from that subscription pub sub then forwards messages from a topic to all of its subscriptions individually a subscriber then receives messages either by pub sub pushing them to the subscriber's chosen endpoint or by the subscriber pulling them from the service the subscriber then sends an acknowledgement to the pub sub service for each received message the service then removes acknowledged messages from the subscriptions message queue and some of the use cases for pub sub is balancing large task queues distributing event notifications and realtime data streaming from various sources and so the next service that i wanted to get into is composer now composer is a managed workflow orchestration service that is built on apache airflow this is a workflow automation tool for developers that's based on the open source apache airflow project similar to an onpremises deployment cloud composer deploys multiple components to run airflow in the cloud airflow is a platform created by the community to programmatically author schedule and monitor workflows the airflow scheduler as you see here executes the tasks on an array of workers while following the specified dependencies and storing the data in a database and having a ui component for easy management now breaking down these workflows for just a sec in data analytics a workflow represents a series of tasks for ingesting transforming analyzing or utilizing data in airflow workflows are created using dags which are a collection of tasks that you want to schedule and run and organizes these tasks to ensure that each task is executed at the right time in the right order or with the right issue handling now in order to run the specialized workflows provision environments are needed and so composer deploys these selfcontained environments on google kubernetes engine that work with other google cloud services using connectors built into airflow the beauty of composer is that you can create one or more of these environments in a single google cloud project using any supported region without having to do all the heavy lifting of creating a fullblown apache airflow environment now when it comes to data flow dataflow is a serverless fully managed processing service for executing apache beam pipelines for batch and realtime data streaming the apache beam sdk is an open source programming model that enables you to develop both batch and streaming pipelines using one of the apache beam sdks you build a program that defines the pipeline then one of apache beam's supported distributed processing backends such as data flow executes that pipeline the data flow service then takes care of all the lowlevel details like coordinating individual workers sharding data sets auto scaling and exactly once processing now in its simplest form google cloud data flow reads the data from a source transforms it and then writes the data back to a sink now getting a bit more granular with how this pipeline works data flow reads the data presented from a data source once the data has been read it is put together into a collection of data sets called a p collection and this allows the data to be read distributed and processed across multiple machines now at each step in which the data is transformed a new p collection is created and once the final collection has been created it is written to async and this is the full pipeline of how data goes from source to sync this pipeline within data flow is called a job and finally here is a highlevel overview of what a data flow job would look like when you involve other services within google cloud and put together in an endtoend solution from retrieving the data to visualizing it and finally when it comes to pricing data flow jobs are billed in per second increments so you're only charged for when you are processing your data now moving on to data proc this is a fast and easy way to run spark hadoop hive or pig on google cloud in an onpremises environment it takes 5 to 30 minutes to create spark and hadoop clusters data proc clusters take 90 seconds or less on average to be built in google cloud dataproc has builtin integration with other google cloud platform services and use spark and hadoop clusters without any admin assistance so when you're done with the cluster you can simply turn it off so you don't spend money on an idle cluster as well there's no need to worry about data loss because data proc is integrated with cloud storage bigquery and cloud bigtable the great thing about dataproc is you don't need to learn new tools or apis to use it spark hadoop pig and hive are all supported and frequently updated and when it comes to pricing you are billed at one cent per vcpu in your cluster per hour on top of the other resources you use you also have the flexibility of using preemptable instances for even lower compute cost now although cloud data proc and cloud data flow can both be used to implement etl data warehousing solutions they each have their strengths and weaknesses and so i wanted to take a quick moment to point them out now with dataproc you can easily spin up clusters through the console the sdk or the api and turn it off when you don't need it with dataflow it is serverless and fully managed so there are never any servers to worry about and when it comes to having any dependencies to tools in the hadoop or spark ecosystem data proc would be the way to go but if you're looking to make your jobs more portable across different execution engines apache beam allows you to do this and is only available on data flow moving on to the next service is cloud data lab now cloud data lab is an interactive developer tool created to explore analyze transform and visualize data and build machine learning models from your data data lab uses open sourced jupyter notebooks a wellknown format used in the world of data science it runs on compute engine and connects to multiple cloud services easily so you can focus on your data science tasks it also integrates with all of the google services that help you simplify data processing like bigquery and cloud storage cloud data lab is packaged as a container and run in a vm instance cloud data lab uses notebooks instead of text files containing code notebooks bring together code documentation written as markdown and the results of code execution whether it's text image or html or javascript like a code editor or ide notebooks help you write code and they allow you to execute code in an interactive and iterative manner rendering the results alongside the code cloud data lab notebooks can be stored in google cloud source repository this git repository is cloned onto persistent disk when attached to the vm now when it comes to prepping your data before consumption whether it be data cleansing cleaning prepping or alteration this is where data prep hits it out of the park dataprep is a serverless intelligent data service for visually exploring cleaning and preparing structured and unstructured data for analysis reporting and machine learning it automatically detects schemas data types possible joins and anomalies such as missing values outliers and duplicates so you don't have to the architecture that i'm about to show you is how data prep shines the raw data that's available from various different sources is ingested into cloud data prep to clean and prepare the data data prep then sends the data off to cloud data flow to refine that data and then sent off to cloud storage or bigquery for storage before being analyzed by one of the many available bi tools now these big data services are used by many data analysts in the field and it's great to know what services that can be used to help process the data needed for their specific job as well for the exam you only need to know these services at a high level and not to know them in depth but if you seem interested in diving into any of these services to know more about them i highly encourage you to dive in after the course and really take a look at them and that's pretty much all i have to cover in this lesson on the services that are available for the big data ecosystem in google cloud so you can now mark this lesson as complete and let's move on to the next one welcome back this lesson is going to be based on the foundation of machine learning i'm going to go over what machine learning is what it can do for us the machine learning ecosystem on google cloud and hopefully answer any questions along the way this lesson will be a high level overview of the services available on google cloud yet these services that are available are a need to know as they come up in the exam and hopefully will give you some really cool ideas on the possibilities of building something truly fantastic on google cloud so what is machine learning well machine learning is functionality that helps enable software to perform tasks without any explicit programming or rules traditionally considered a subcategory of artificial intelligence machine learning involves statistical techniques such as deep learning also known as neural networks that are inspired by theories about how the human brain processes information it is trained to recognize patterns in collected data using algorithmic models and this collected data includes video images speech or text and because machine learning is very expensive to run onpremises is an efficient place for machine learning due to the use of massive computation at scale and as explained before machine learning is always better with big data so now i wanted to touch on what can machine learning do for us well it can categorize images such as photos faces or satellite imagery it can look for keywords in text documents or emails it can flag potentially fraudulent transactions when it comes to credit cards or debit cards it can enable software to respond accurately to voice commands it can also translate languages in text or audio and these are just some of the common functions that machine learning can do for us so getting into google's machine learning platform itself machine learning has been a cornerstone of google's internal systems for years primarily because their need to automate datadriven systems on a massive scale and doing this has provided unique insight into the right techniques infrastructure and frameworks that help their customers get optimal value out of machine learning the originally developed open source framework for use inside of google called tensorflow is now the standard in the data science community in addition to heavily contributing to the academic and open source communities google's machine learning researchers helped bring that functionality into google products such as g suite search and photos in addition to google's internal operations when it comes to data center automation now here is an overview of all the machine learning services that we will be covering and that you will need to know only at a high level for the exam and we'll start off with the site api services starting with the vision api the vision api offers powerful pretrained machine learning models that allow you to assign labels to images and quickly classify them into millions of predefined categories vision api can read printed and handwritten text it can detect objects and faces and build metadata into an image catalog of your choice now when it comes to video intelligence it has pretrained machine learning models that automatically recognizes more than 20 000 objects places and actions in stored and streaming video you can gain insights from video in near real time using the video intelligence streaming video apis and trigger events based on objects detected you can easily search a video catalog the same way you search text documents and extract metadata that can be used to index organize and search video content now moving on to the language apis we start off with the natural language api and this uses machine learning to reveal the structure and meaning of text you can extract information about people places and events and better understand social media sentiment and customer conversations natural language enables you to analyze text and also integrate it with your document storage on cloud storage now with the translation api it enables you to dynamically translate between languages using google's pretrained or custom machine learning models translation api instantly translates text into more than 100 languages for your website and apps with optional customization features following another grouping of machine learning is the conversation apis first up we have dialog flow dialog flow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your application or device it could be a mobile app a web application a bot or an interactive voice response system using dialogflow you can provide new and engaging ways for users to interact with your product dialogflow can analyze multiple types of input from your customers including text or audio inputs like from a phone or voice recording and it can also respond to your customers in a couple of ways either through text or with synthetic speech now with the speechtotext api this api accurately converts speech into text it can transcribe content with accurate captions and deliver better user experience in products through voice commands going the other way from text to speech this api enables developers to synthesize natural sounding speech with over a hundred different voices available in multiple languages and variants text to speech allows you to create lifelike interactions with their users across many applications and devices and to finish off our machine learning segment i wanted to touch on auto ml automl is a suite of machine learning products that enables developers with very limited machine learning expertise to train high quality models specific to their business needs in other words using automl allows making deep learning easier to use and relies on google's stateoftheart transfer learning and neural architecture search technology so you can now generate high quality training data and be able to deploy new models based on your data in minutes automl is available for vision video intelligence translation natural language tables inference and recommendation apis now i know this has been a lot to cover for this machine learning lesson and the ecosystem around it but is a necessity for the exam and will also help you build really cool products when it comes to your role as an engineer again all the services that i have discussed in this lesson should be known at a high level only although my recommendation would be to dive deeper into these services by checking out the links in the lesson text below and having some fun with these products getting to know these services will really help up your game when it comes to getting to know these services a little bit more in depth and will really help you gain more momentum when it comes to building any applications or applying them to any currently running applications i personally found it extremely valuable and really cemented my knowledge when it came to machine learning i also had a ton of fun doing it and so that's all i have for this lesson on machine learning so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson we'll be diving into a suite of tools used on the google cloud platform that allow you to operate monitor and troubleshoot your environment known as operation suite and previously known as stackdriver this lesson will be mostly conceptual and gear more towards what the suite of tools do as it plays a big part not only in the exam but for the needs of gaining insight from all the resources that exist in your environment now there are a few tools to cover here so with that being said let's dive in now the operation suite is a suite of tools for logging monitoring and application diagnostics operation suite ingests this data and generates insights using dashboards charts and alerts this suite of tools are available for both gcp and aws you can connect to aws using an aws role and a gcp service account you can also monitor vms with specific agents that again both run on gcp for compute engine and aws ec2 operation suite also allows the added functionality of monitoring any applications that's running on those vms operation suite is also available for any onpremises infrastructure or hybrid cloud environments operation suite has a native integration within gcp out of the box so there's no real configurations that you need to do and integrates with almost all the resources on google cloud such as the previously mentioned compute engine gke app engine and bigquery and you can find and fix issues faster due to the many different tools an operation suite can reduce downtime with realtime alerting you can also find support from a growing partner ecosystem of technology integration tools to expand your operations security and compliance capabilities now the operation suite comprises of six available products that covers the gamut of all the available tools you will need that allows you to monitor troubleshoot and improve application performance on your google cloud environment and i will be going over these products in a bit of detail starting with monitoring now cloud monitoring collects measurements or metrics to help you understand how your applications and system services are performing giving you the information about the source of the measurements time stamped values and information of those values that can be broken down through time series data cloud monitoring can then take the data provided and use predefined dashboards that require no setup or configuration effort cloud monitoring also gives you the flexibility to create custom dashboards that display the content you select you can use the widgets available or you can install a dashboard configuration that is stored in github now in order for you to start using cloud monitoring you need to configure a workspace now workspaces organize monitoring information in cloud monitoring this is a single pane of glass where you can view everything that you're monitoring in your environment it is also best practice to use a multiproject workspace so you can monitor multiple projects from a single pane of glass now as i mentioned earlier cloud monitoring has an agent and this gathers system and application metrics from your vm and sends them to cloud monitoring you can monitor your vms without the agent but you will only get specific metrics such as cpu disk traffic network traffic and uptime using the agent is optional but is recommended by google and with the agent it allows you to monitor many thirdparty applications and just as a note cloud logging has an agent as well and works well together with cloud monitoring to create visualize and alert on metrics based on log data but more on that a little bit later cloud monitoring is also available for gke and this will allow you to monitor your clusters as it manages the monitoring and logging together and this will monitor clusters infrastructure its workloads and services as well as your nodes pods and containers so when it comes to alerting this is defined by policies and conditions so an a learning policy defines the conditions under which a service is considered unhealthy when these conditions are met the policy is triggered and it opens a new incident and sends off a notification a policy belongs to an individual workspace and each workspace can contain up to 500 policies now conditions determine when an alerting policy is triggered so all conditions watch for three separate things the first one is a metric the second one is a behavior in some way and the third one is for a period of time describing a condition includes a metric to be measured and a test for determining when the metric reaches a state that you want to know about so when an alert is triggered you could be notified using notification channels such as email sms as well as third party tools such as pagerduty and slack now moving on to cloud logging cloud logging is a central repository for log data from multiple sources and as described earlier logging can come not just from google but with aws as well as onpremises environments cloud logging handles realtime log management and analysis and has tight integration with cloud monitoring it collects platform system and application logs and you also have the option of exporting logs to other sources such as longterm storage like cloud storage or for analysis like bigquery you can also export to thirdparty tools as well now diving into the concepts of cloud logging these are associated primarily with gcp projects so logs viewer only shows logs from one specific project now when it comes to log entries log entry records a status or an event a project receives log entries when services being used produce log entries and to get down to the basics logs are a named collection of log entries within a google cloud resource and just as a note each log entry includes the name of its log logs only exist if they have log entries and the retention period is the length of time for which your logs are kept so digging into the types of logs that cloud logging handles there are three different types of logs there are audit logs transparency logs and agent logs now with audit logs these are logs that define who did what where and when they also show admin activity and data access as well as system events continuing on to access transparency logs these are logs for actions taken by google so when google staff is accessing your data due to a support ticket the actions that are taken by the google staff are logged within cloud logging now when it comes to agent logs these are the logs that come from agents that are installed on vms the logging agent sends system and thirdparty logs on the vm instance to cloud logging moving on to error reporting this looks at realtime error monitoring and alerting it counts analyzes and aggregates the errors that happen in your gcp environment and then alerts you when a new application error occurs details of the error can be sent through the api and notifications are still in beta error reporting is integrated into cloud functions and google app engine standard which is enabled automatically error reporting is in beta for compute engine kubernetes engine and app engine flexible as well as aws ec2 air reporting can be installed in a variety of languages such as go java.net node.js python php and ruby now moving into debugger this tool debugs a running application without slowing it down it captures and inspects the call stack and local variables in your application this tool debugs a running application without slowing it down it captures and inspects the call stack and local variables in your application this is also known as taking a snapshot once the snapshot has been taken a log point can be injected to allow you to start debugging debugger can be used with or without access to your application source code and if your repo is not local it can be hooked into a remote git repo such as github git lab or bitbucket debugger is integrated with google app engine automatically and can be installed on google compute engine gke and google app engine debugger is integrated with google app engine automatically and can be installed on gke debugger is integrated with google app engine automatically and can be installed on google compute engine google kubernetes engine google app engine and cloud run and just as a note installation on these products is all dependent on the library and again debugger can be installed like trace on nongcp environments and is available to be installed using a variety of different languages next up is trace and trace helps you understand how long it takes your application to handle incoming requests from users and applications trace collects latency data from app engine https load balancers and applications using the trace api this is also integrated with google app engine standard and is applied automatically so you would use trace for something like a website that is taking forever to load to troubleshoot that specific issue trace can be installed on google compute engine google kubernetes engine and google app engine as well it can also be installed on nongcp environments and it can be installed using a variety of different languages as shown here and coming up on the last tool of the bunch is profiler now profiler gathers cpu usage and memory allocation information from your applications continuously and this helps you discover patterns of resource consumption to help you better troubleshoot profiler is low profile and therefore won't take up a lot of memory or cpu on your system as well in order to use profiler an agent needs to be installed profiler can be installed on compute engine kubernetes engine and app engine as well and of course it can be installed on nongcp environments and profiler can be installed using the following languages just go java node.js and python and so just as a note for the exam only a high level overview of these tools are needed and so this concludes this lesson on a high level overview of operation suite so you can now mark this lesson as complete and let's move on to the next one