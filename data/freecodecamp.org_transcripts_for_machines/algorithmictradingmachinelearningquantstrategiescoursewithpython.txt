in this comprehensive course on algorithmic trading you will learn about three cuttingedge trading strategies to enhance your financial toolkit latchezar teaches this course he is an experienced quantitative researcher and data scientist in the first module you'll explore the unsupervised learning trading strategy utilizing SP 500 stocks data to master features indicators and portfolio optimization next you'll leverage the power of social media with the Twitter sentiment investing strategy ranking NASDAQ stocks based on engagement and evaluating performance against the QQQ return lastly the intraday strategy will introduce you to the gar model combining it with technical indicators to capture both daily and intraday signals for potential lucrative positions hello and welcome to this free code Camp course on algorithmic trading machine learning and Quant strategy with python my name is Lazar and I'll be your instructor toout the course and in this course we are going to develop three big quantitative projects from start to end and the course overview would be the following first we're going to talk about algorithmic trading Basics then we're going to talk about machine learning in trading some obstacles and challenges we may face while using machine learning in trading then we are going to develop the first project which would be an unsup rised learning trading strategy using stocks from S&P 500 the next project would be Twitter sentiment and it would be using data from NASDAQ 100 stocks and the third one would be focusing on one asset it would be an intraday strategy using a gar model to predict the volatility it would use simulated data and after that we are going to have a quick wrap up and with that we'll finish the tutorial but before we continue I would like to mention that this tutorial should not be ConEd as a financial advice it is for educational and entertainment purposes only we are going to develop some Concepts and come up with strategies in the end but it's not a financial advice and you shouldn't make any decisions based on it as well for this course I would assume that you have at least some python knowledge and understanding because down the road will deal with some complex problems and if you're new to python you may get B down nevertheless you learn some very interesting Concepts so stay along and let's get into it okay algorithmic trading Basics so what is algorithmic trading it is trading on predefined set of rules which are combined into a strategy or a system it is developed by a programming language and it is run by the computer it can be used for both manual and automated trading by manual what I mean is that you may have a screener which comes up with a set of stock stocks you want to trade on a given day or you may have a algorithmic strategy which is developed into an alert system and whenever the conditions are triggered you get an alert but you execute it manually on the other hand you may have a completely automated complex system which does a lot of calculation comes up with uh positions and sizing and then executes the trade automatically okay so what is the role of python in algorithmic trading python is the most popular language used in Alor trading quantitative finance and data science and this is mainly due to the vast amount of libraries that are developed in python as well the ease of use of uh python it is mainly used for data pipelines research back testing strategies as well it can be used to automate strategies but python is a slow language and it can be used to automate low complexity systems if you have a really highend system which is really complicated and it needs to execute trades really quickly you would use Java or C++ for those strategies our gmic trading is a great career opportunity it's a huge industry there are a lot of jobs with hedge funds Banks Prop Shops and I've just checked the average yearly base salary for Quant researcher is around $173,000 and this is not including the yearly bonus it is a great career opportunity and if you're interested into it the main things you need to know are python you need to know how to back test strategies you need to know how to replicate papers and you need to know machine learning in trading if you're interested into it I'll definitely advise you to go for it okay so let's move on and talk a little bit about machine learning in trading and some use cases of machine learning when we talk about supervised learning we can use it for Signal generation through prediction for example we can come up with buy or sell signals on a given stock or a given asset based on predicting the return or the sign of the return of that asset we can as well use it in risk management for example we may use a prediction to determine the position sizing or the weight of a given stock in our portfolio or to predict where exactly should our stop loss be and with unsup learning we can use it to extract insights from the data for example we can discover patterns relationships or structures within the data for example clusters and use it in this way to uh help our decisions what are some of the challenges that we may face while trying to apply machine learning in trading and the first theoretical challenge we may face is the so called reflexivity feedback loop and it is referring to the phenomenon that if we for example have a machine learning model that is uh predicting that a stock is going to go up each Friday and we can form a strategy around to profit from that phenomenon for example we are going to buy each Thursday and then sell on Friday to capture that price increase move if we find this strategy throughout predictions and start trading it with time other Market participants as well are going to find this Market phenomenon and start to exploiting it as well which would cause the price to start going up on Thursday because everybody's buying now on Thursday instead on Friday and then this strategy is going to be Arbitrage away so it's this reflexivity feedback loop which is making predictions quite hard what is most hard while applying machine learning the most hard thing is to predict returns and predict prices the next quite hard thing to do is to predict return signs or the direction of a given asset is it going to to go up or down the next thing is to predict an economic indicator for example it is quite hard to predict nonfarm payrolls or weekly jobless claims and a thing which is not that hard or quite straightforward is to predict the volatility of a given asset furthermore there are some technical challenges like overfitting a model or generalization overfitting is that the model is learned the train data too well and it fails on the test data and generalization is that the model is not performing the same as on the real data as well we may have nonstationarity in our training data and regime shifts which may ruin the the performance of the model and the last thing is that if we have a really complicated model or um neuron Network it is like a black box and we are not able to interpret it correctly what is the usual workflow process in algorithmic trading and machine learning that would be to collect and prepare the the data then develop a hypothesis for a strategy then you have to code the model and train the model and then finally back test the strategy okay guys and some key takeaways from this course so you learn high level Concepts in quantitative Finance as well practical machine learning in trading you develop a project from idea to back test final results however we will not automate or execute any trades we'll just develop a strategy this course is all about developing a strategy from start to end so you see the workflow and this is just a purely research project for educational purposes I repeat it should not be construed as any Financial advice whatsoever and with that we can move to the first project and let's get into it okay so first project is about unsupervised machine learning trading strategy we're going to use data from S&P 5 500 stocks let's talk a little bit about unsupervised learning in trading so it involves machine learning techniques to analyze financial data and discover patterns relationships and structures within this data without predefined labels or Target variable unlike supervised learning where the model is trained to make predictions unsupervised learning focusing on extracting insides from the data and some use cases would be clustering which we are going to use in the first project dimensionality reduction anomaly detection Market regime detection and portfolio optimization in this project what we are going to do is first we're going to download all the prices data for all S&P 500 stocks then we're going to calculate different technical indicators and features for each stock next we're going to aggregate on a monthly level and filter only the top50 most liquid stocks from the S&P 500 for each month next we are going to calculate PL monthly returns for different time Horizons to add up to the features and the next step would be to download the F French factors and calculate rolling Factor betas for each stock as well to add to the feature set and at this point we'll have enough features to fit the model and either make predictions or in our case we're going to fit a c's clustering algorithm an unsupervised learning model and we will use it to group group the stocks into similar assets into clusters and from those clusters then we will be able to for each month select stocks from a given cluster and we are going to analyze the Clusters and select a particular cluster and then for each month we are going to select those stocks within this cluster and form portfolios however these portfolios will be optimized so the weights of the stocks within the portfolio we're going to find them by using the efficient Frontier Max Sharpie ratio isue portfolio weights and then we are going to form the portfolio hold for one month and rebalance at the end of the month and you know form another Max sharp ratio portfolio in the end we'll have the strategy returns for each day and we will be able to compare our portfolio strategy returns to the S&P 500 returns themselves actually not small limitation is that we're going to use the most recent S&P 500 stocks list which means that there may be a survivor buyers in this list this is a huge issue actually in reality you should always uh back test strategies using survivorship free bias data what is survivorship bias it is the condition when a stock which have actually went out of the S&P 500 because it was failing is currently not in the list right so last year for example there was a stock which was failing and going down down down so at some point in December last year they removed it from S&P 500 and they included a new stock so if we have made the optimization last November we could end up with having this stock into our portfolio and actually it affecting our portfolio results but if we use the most recent S&P 500 symbols list this stock would not be there so that's survivorship bias and for the given project we are not going to deal with this survivorship bu so the list we are going to work with has survivorship bus most probably so yeah that is limitation you need to know in the second project we are going to develop a Twitter sentiment based investing strategy we are going to use the NASDAQ 100 stocks and Twitter sentiment data what is sentiment investing this approach focuses on analizing how people feel about certain stocks Industries or the overall Market it assumes that the public sentiment can impact stock prices and for example if many people are positive about a particular company on Twitter it might indicate potential for that company stock to perform well what we are going to do first we're going to load the NASDAQ stocks Twitter sentiment data then we're going to calculate a quantitative feature of the engagement ratio in Twitter for each stock after that we're going to rank all the stocks crosssectionally for each month and create an equal weight portfolio in the end we're going to compare the return of this portfolio to the NASDAQ itself the second strategy is much smaller than the first strategy there is no machine learning modeling in this strategy but the idea here is to show you how alternative or different data in this case sentiment data can help us to create a Quant feature and then create potential strategy out of it that's the idea of the second project and the third project is about an intraday strategy using a gar model in this one we are going to focus on a single asset and we'll be using simulated data actually we'll have daily data and intraday 5 minute data but what does an intraday strategy means this approach involves buying and selling Financial assets within the same trading day to profit from the shortterm price movements intraday Traders usually use technical analysis real time data and different risk management techniques to make decisions and profit from the strategies what exactly we're going to do in this project first we are going to load the simulated daily data and the simulated 5 minute data then we are going to define a function which would fit in a rolling window a gar model to predict one day ahead volatility of the asset after we have that we'll calculate prediction premium so we'll predict the volatility and we'll calculate the prediction premium and form a daily signal from it after we have uh calculate that then we'll merge the daily data with the intraday data and calculate intraday technical indicators to form intraday signal so we will have a daily signal and then on top an intraday signal so two signals and after we have that we'll generate position entry and hold until the end of the day in the end we're going to calculate the final strategy returns and this is it for the third project the idea is to show you how predicting volatility Works in uh intraday strategies and yeah with that guys we are ready to jump into the first project so let's get into it okay let's start with the first project the unup rised learning trading strategy but before we continue with the coding guys the first step would be for you to pause the video and install all the needed packages for this project so I prepared a small list up here with all the packages we are going to need for the project so those are pandas numai MP lip stats models pandas data reader daytime wi Finance SK learn and buy portfolio op how you can install the packages you can do it the following way you can open on Ana prompt like that right and then just write pip install and the package name pip install pandas for example and do that for each package that's one way another way to do it is through the notebook itself you can uh type the following so pip install and then the package name so pip install pandas you do control shift and run the cell and it will install the package pause the video take your time install all the packages and let's continue okay so the first step would be to download the S&P 500 constituents prices data but before that we'll have to import all the packages we use throughout this project I've already prepared that those are all the packages we've just installed and now we are importing into the Jupiter notebook The Next Step would be to download S&P 500 constituents data to do that we can go to this link on Wikipedia and up here you can see that they have a table containing a S&P 500 component stocks like the symbol the the name of the security sector industry date added as well so quite some data uh so we would like to load this table into our Jupiter notebook how we can do that we just call a pandas read HTML function and let's see what this would return us okay so this is returning a list containing two elements it looks like two data frames so actually we are interested in the first frame yeah exactly we will assign that to an object called S&P 500 and the next step would be to grab this symbol column and extract all the symbols in a list but before that I think we would have to make a little cleaning on some of the symbols because I know that one or two of the symbols contain a DOT and this would give us an error while we download data from wi Finance so we have to actually replace all dots with Dash and that would do the job next step would be to you know just grab list with all the stocks so yeah we can just use symbol. unique to list we will assign that to a object called symbols list and yeah that would be our symbols list of all S&P 500 stocks as we talked uh in the beginning this list of stocks is not Survivor ship bias free so you need to know that it's uh quite some of limitation all right so we would like to download the data up to a few days ago let's define an object which is end dat and we'll use 2023 September 27th and the start date would be P to date time and date it would be exactly 8 years ago so we can just use the end date and we will substract eight years out of it how we can do that we just say pandas to datetime we'll convert to daytime the end date and then we'll just substract pandas date of set and then Supply 365 * 8 all right so now we have the end date which is yeah 27th of September which is a string and the start date would be a time stamp but that's all right because we are going to download data from y finance and this function we use from y Finance download function which takes tickers as an argument so that would be our symbols list then start which would be our start date and end which would be our end date and now this will download all the S&P 500 constituents if we run it yeah it will take some time all right so we've downloaded all the data for the S&P 500 stocks we got it up here so the next step would be to actually yeah we'll comment out everything and have uh this object printed out as you can see up here we have the column and then we have like a multi index column so first we have the adjusted close and then we have the adjusted close for each stock that's not really convenient to work with the whole data frame at the moment we have 2012 rows and 3,8 columns that's really inefficient how we can overcome that we would just use the stock method which now creates a m index the first level would be the date and then for each date we have the corresponding 500 stocks adjusted close close high low so that's much more convenient we have six columns and almost 1 million rows so we have to change the datea frame to be stocked or actually we can move this method right up here after the download directly and yeah now we have our data frame stacked Next Step would be I would always when I have a multiindex I would always want to have labels on both of the index levels so we'll say index. names and we'll assign the new name so date and thicker that will be our new multiindex names as you see they change step here next step would be to fix the columns a little bit I I would like to fix the column names to be not as titles but with a lower letters so that would be DF do columns DF do columns do string lower and that's pretty much it with the downloading and fixing data a little bit before we move to the next step which would be to start calculating technical indicators and features of all those 53 stocks okay so in the second step we can start calculating the features and Technical indicators for each stock we are going to calculate the garm class volatility RSI Ballinger bands ATR macd and dollar volume for each stock let's first start with the garm class volatility what is garm class volatility it is volatility measure usually used in Forex Trading but it works for stocks as well it is approximation to measure the intraday volatility of a given asset and that is the formula right here so what do we do we Define a new column called garmon class V and it would be the following so log from the high minus log from the low so that would be yeah from the low this whole thing is squared and then it is divided by two then from this we substract 2 * log 2us one multiplied by subtraction of log from the adjusted close yeah minus log from the open again the hold think is squared and we just close another bracket and that should be it that should be the gar class volatility yes so we now have calculated the gar class volatility for each stock it is calculated on a given role we don't need to do any fancy calculations for this one the next one is RSI so how do we calculate the RSI on each stock what we're going to do we are going to group Buy on the thicker level so level one the multi index has level zero and level one which is the thicker level zero is the date level one is the thicker so we grew by on level one then we are selecting the column which would be adjusted close and apply the transform method and within the transform method we just apply a Lambda function which would be so now we have grouped grouped by on each thicker and what do we want to do we want to calculate the RSI to calculate the RSI we are going to use the pandascore TA package which is the package to calculate pretty much all of the needed technical indicator so we use the pandascore TA package and from the pandascore we use the RSI function the side function we have to supply the close price which would be X and the length would be 20 and yeah if we do that you see now we have the RSI for each stock how we can double check our work we select apple and then RSI and then we'll plot it and yeah as you can see the r side goes up and down up and down so we have worked correctly the next indicator we would like to calculate is Ballinger bands and actually we would want to have the lower band the middle band and the upper band but there is one specification for each indicator from now on we would like to normalize and scale the indicator itself so for the Binger BNS will will supply the log from the close price first the function we are going to use is from Panda CA B bands and it is taking the clothes just for presentational purposes we Supply a upper adjusted close and the length will be 20 when you run this function it returns five columns so the first one is the lower band the second one is the middle band and the third one is the upper band we have to take this into account and what we are going to do is pretty much you know Define a new column BBL low Ballinger band low and we use the the same idea as for the RSI we are going to group by each thicker select the adjusted close column and then we use the transform method Lambda function and in the Lambda function we say pandas ta B bands close would be equal to X actually it will be equal to log of x so MP log 1 p and then the length would be again 20 when we run that it will return those five columns what we actually want to do is assign to B below the First Column which we know is the B the lower Ballinger band we can say iock all the rows and the First Column we can repeat the same operation for the midband and for the upper band okay but this happens when I forget something and I think I have forgot I forgot the second curly bracket to close the curly brackets let's see what this would return yeah guys after you return and calculate something you can just select all the the code you've used and comment it out okay we forgot to change the names that would be BB mid and first index that would be BB High the second index okay let's run that again and see what we get awesome now we have the lower bager Bond Middle Ballinger Bond and upper Ballinger Bond and we have the data scaled and normalized next step is to calculate the ATR for each stock however the ATR function needs three inputs so three columns not only one column and when we use transform method in pandas it is actually working when you select only one column it would not work if you have three columns as an input so we would have to use another approach more specifically that would be a group by apply and to do that we need to Define our own custom function to calculate the ATR we can uh double check what the ATR function from pandas ta requires as input and it is the high so we have to supply the high then we have to supply the low and the close price furthermore we can supply the length for example 14 and yeah it will mess mess up with the data if we run it like that because we have to select the data for a given stock but yeah you see that it requires three columns so here we we going to define a function called compute ATR it will take stock data and here we will just calculate at which would be pandas ta. high would be stock data High the low then we have we need a close the length would be 14 all right and a little detail we are going to add of here is we are going to normalize the data while we calculate it so that would be ATR do substract first first we are going to the mean and then we are going to divide it by the standard deviation so ATR do standard deviation and yeah that will be our ATR indicator function now we can just say ATR create a new column called ATR and group by level one again so we are applying that for each stock however up here when we use the group by apply we need to add an additional argument to the group bio which is group key is equal to false because if we don't do that it will double the the date column so it will return another date colum and we have a triple multiindex with two date columns we don't want that so we just say group Cas equals to false and then apply this function and this will now calculate the ATR index normalized for each stock the next indicator we are going to calculate is the macd indicator and for the macd we're going to follow the same logic as as for the ATR indicator we're going to Define our own custom function to compute the macd so it will be called compute macd it will take the close price and up here we are going to say magd is equals to pandas ta. magd close equal to close length is 20 and then we would like to get the First Column which would be returned however in the end as well we are going to normalize the dat we're going to the meain the series and then we're going to divide by the standard deviation why do we do that right away we're normalizing the data because we are going to use it into a machine learning model we are going to Cluster the data we want to do that straight away and don't think about it later in the future so here we are going to do again group buy level one each scker on each stock group Keys equal to false and then apply compute Mark D this would calculate calate the mag the indicator for each stock all right we have an error why do we have this error that's quite strange oh okay I'm sorry I'm sorry I forgot to add the adjusted close column up here and this is driving the eror all right so we will return back I lock in the first corn that should be guys all right yeah now we have the macd as well calculated and normalized as you can see we have the data looks pretty good so far the only indicator we are not going to normalize is the RSI and there is a particular reason for that but you uh understand more about it when we come to the clustering part all the other indicators we are going to normalize and the final one is the dollar volume so we're going to create a new column dollar volume which would be equal to the adjusted close multiplied by the volume however we may want to actually divide that by 1 million for each stock because we know that millions of shares are traded each day and this would make sense as you can see now the data looks much better for the dollar volume right and that's pretty much it with uh calculating the first batch of features our technical indicators now we have a really beautiful data frame for each day we have all the 500 stocks we have the close price low open volume garment class volatility RSI Ballinger band ATR macd and the dollar volume for each stock and we are now ready to move to the next and the third step in the third step what we want to do is to aggregate on a monthly level the data and filter the top 150 most liquid stocks for each month why do we do that we do that to reduce training time for any potential machine learning model and experiment with features and strategies what is my idea here I would like to aggregate all the indicators so those five I would like to take the end value the end of the last value for the month as well the same for the adjusted close price and for the dollar volume I would like to get the average dollar volume for the whole month for each stock we can start actually what we can do first is uh take the data frame un stock the thicker level so we'll unstack thicker and then we're going to select the dollar volume column and if we run that we have the dollar volume for each day for each stock now right and what we can do is just resample to monthly and take the mean this should resample to monthly now as you can see we have monthly index end of each month and we have the average dollar volume for the month what we can do now is just stack it back into a MTI index like that and we can say two frame to make it uh a data frame with one column two frame dollar volume beautiful that would be the first step however for the indicators what we can do is we can follow the same logic but we need to select the exact columns actually we may create a list of columns so last call and this would be our list of columns for which we want to do the same operation however we would use the last method up here instead of mean and those columns would be C for C in DF do columns. unique and yeah the first element C for C in do in DF columns. unique if C is not in the following columns list so if the column is not dollar volume it is not volume it is not open it is not high low or close pretty much we want to do that only for the technical indicators columns so for those columns we don't want to use these columns for our aggregation we want just the Fe we are creating the features data frame in the end right we would use the dollar volume to filter out the most liquid stocks the dollar volume would not be featured in our model as well NE neither the volume or open low high close after we have defined the last columns uh we can actually proceed with the next aggregation we have done the dollar volume aggregation the next one would be for DF last C we are going to unstack actually we are going to un stack before that so we are going to unstack we're going to unstack and select those columns and then resample to monthly and then just use the last volume like that and again we're going to stack backwards into a multiindex voila and now what we can do is concut those two together we can say the following pandas cona and that would be axis axis one and boom we have the dollar volume the average dollar volume and the last value for adjusted close ATR and other technical indicators for each month now we have aggregated the data to monthly level for the features we would need actually what we can uh what we can add up here is a small drop and that looks much more beautiful and we can call that data and let's visualize what we have yeah that's our data all right and that's the first step with the aggregating to a monthly level The Next Step would be to calculate the fiveyear rolling average dollar volume for each stock and then you use this uh aggregated dollar volume to filter out only the top 150 most liquid stocks for each month how do we approach that first we can start by selecting the dollar volume like that so what we can do select the dollar volume and stack the thicker level and now we can use a rolling function with a window of 5 * 12 so 5 years and then we can calculate the mean as you can see now we have the rolling average mean for each stock rolling average 5year dollar volume for each stock and again we can just stack backwards and that's pretty much our dollar volume column what we can do now is we can assign it to the dollar volume column update to The fiveyear Rolling average for each stock awesome after we have that the next step is to calculate the dollar volume rank cross section for each month how do we do that we can say data Group by level zero or date so we can Group by on date for each month we're going to select the dollar volume and just rank ascending equal to false let's see what this would give us ascending and now as you can see we have all the stocks ranked by Dollar volume and the the ones the the guys who have the smallest dollar volume have the the highest rank so we want the top 150 and from here we can pretty much very easily select the stocks which are below 150 for each month and those would be the top50 most liquid stocks for each month after we have selected them we can just drop the two columns we can drop the dollar volume and the dollar volume rank because we are not going to need them anymore access equals to one and that's pretty much it for our we can assign that to data and that's pretty much it with our third step where now we have aggregated monthly data for all the features we would need plus the adjusted close price and we can move move on with the next step the fourth step and that would be calculating monthly returns for different time Horizons and add them as additional features to the ones we already have here okay so let's move to that step I'll just cut those sales in the middle all right why do we want to calculate the monthly returns for different time Horizons and uh add them to the feature set because we may want to capture time series dynamics that reflect for example the momentum patterns for each stock to do that we can just use the pandas data frame method uh percent underscore change and Supply the different logs my Approach would be to use logs for 1 month two months 3 months 6 months 9 months and 12 months that's uh like six different uh LS to really capture the momentum patterns how do we approach that let's first start by for example selecting the Apple stock I just want to make a little example so we'll select the Apple stock and let's see what we have here all right now we would like to calculate the returns for the following lcks for one month two months 3 months 6 months 9 months and 12 months right as well we may want to have an outlier cut off because we are dealing with a lot of stocks there will definitely be outlier values in the returns of those stocks what do we want to do we want to with them by clipping them what clipping does is that for all values which are above the outlier threshold they will just be assigned the threshold of that percent up the cut off value we may want to have it as a 0.05 which means the 99.5 percenti that would be our outlier cut off and now what we do is just for each log so for log in logs for each log we are going to create a column which would be the return for the given log for the given log month right that would be the column and then we grab the adjusted close then we just do percent change Lo so for each lck we'll calculate the following column which would calculate the given return and then we want to deal with the outliers right so that would be pipe Lambda so far we have the adjusted close and then we calculate the return for the given L and then we input that into the pipe Lambda X we can clip now we clip the return and we can clip the lower band the lower cut off we'll use x quantile and we supply for the lower cut off just outl cut off and for the upper one we Supply x quantile one minus outline cut off then we add one to the power of 1 / by the log and we substract one in the end and that should be it guys now we can see that for our Apple stock which is G right we have the one month return two months return 3 months 6 months 9 months 12 months Etc how we can extend that we just use the same approach we use for the at and the macd indicators we will create our own custom function and then we you just use the group by apply methodology for the bound data frame so calculate returns it takes DF for example and just move that a little bit return DF in the end however up here we have to change that to DF this one to DF as well and that's pretty much it our function and now we can say the following data equals to data. group by level one because we Group by on the thicker level so we can say uh Group by ticker or level one let's say level one and then we want Group keys to be false so we don't have two uh date indexes assigned to the new date frame so Group keys false then apply we apply the calculator T function and in the end we may want to drop the na values and yeah this would take some time and yeah that's pretty much it I mean now we have added the return features as well which we would use to capture momentum patterns for each stock and that's pretty much it with the fourth step the calculating the monthly returns for different on Horizons and we can move to the next step which is really interesting actually adding even more features to our data set that would be to download the F French factors and we are going to calculate the rolling Factor better for each stock in our data all right let's move to it okay so in this step we're going to download the farm French factors data and calculate the rolling Factor betas for each stock in our current data set so we want to introduce the F French data to estimate the exposure of our assets to commonly known risk factors and we're going to do that using a regression rolling OS model the five round French factors namely Market risk size value profitability and invest M have been shown to empirically explain asset returns in the past and are commonly used in the asset management industry to assess the risk return profile of different portfolios so it kind of makes sense to include them in our current feature data set how we can do that we can use the pandas data reader package we which we imported as web we can use this package to download the F French Factor models but before that we may want to to take a look at the we might want to take a look at the data so we can Google it Farm French factors and up here you can find the canid French data Library just click on it and this is the part we are interested in in FAL French 5 research factors 2 * three so those five factors that's the data we are interested in if you scroll down a little bit you can find the daily data right here you can download it as txt or CSV file as well you can check the details they have monthly returns and annual returns we are interested in the monthly returns as our data is already on a monthly level right so what we can do is uh we can say the following web. data reader data reader and up here we have to supply the name of the exact Factor so the exact file so I have prepared it already that's the name then we say F French then the start date we want 2010 this is returning a dictionary with two keys the first one is the monthly factors and the second one is the yearly factors that's awesome so we want the monthly factors only and that's the data we have 164 uh months up to August 2023 that's pretty good however I think we don't even though the riskfree return is pretty solid right now it is out of the scope of this tutorial so we'll just drop it we say drop RF AIS equal to one and and that's perfect we can call this uh assign it to factor data and yeah that's our Factor data let's check the index the index is uh monthly yeah all right so I think we have to fix the index as well so we can call of pandas to daytime and Supply the index let's see if this would work okay we have to use to time stamp all right okay so now we have fixed the index as well as you can see now it's it has the year the month and the beginning of month date however our data is end of month that's one thing we have to fix another one is that I see that the factors are in percentages so we would have to divide them by 100 how we can fix that we can just say resum P to monthly and get the last value which would fix immediately the issue yeah with the beginning of mandate next we can say divide by 100 and that's perfect we just assign that to factor data comment it out yeah next we want to fix the name of the index to be just date and the next step would be to join with the pretty much join with the one month return why would we want to do that because at the beginning of each month we have the factors and then we have the return of each stock at the end of the same month so now we have fixed the date of the factors we can just join with the end of month return and then we can can regress them and take the beta right if the factor is predictive we have it at the beginning of the month and we will regress it with the return of the end of the month so we'll get the the the beta how we can do that factor data join and then from our data we can select the one return column let's see what this would give is that's perfect now we can sort the index and just assign to factor data what we may want to do here is to double check our work we can select two stocks for example apple and let's say Microsoft and we can double check the return yeah the return is different but the factors stay the same looks like we' worked correctly and we are ready to move to the next step in this step we are going to filter out stocks that have less than 10 month data why are we doing that because we are going to use rolling window for the regression of around 2 years 24 months and stocks that don't have enough data would actually break our function so we have to remove them from the data set how we can do that we can say Factor data. groupby level one and then just call the size method and now we can see how many months of data we have for each stock okay guys and I just realized that we have made a small mistake somewhere because we have only 23 months of data for each stock and I had to go back through the code and actually f figure out that on this step uh this part was missing which was the data dolog selecting all the rows and then the dollar volume column you can just rerun this sale and you can see that here our our first month is in 205 November and after that it was 2020 so when you add the dot loog and selecting the all the rows and the dollar volume column you get the fixed data then we can rerun as you can see up here it's 2021 the first date if we run it it should be much backwards in the past yeah so 2017 31st of October and we have to do that again for the factor data two and if we run now yeah now we have 71 months right and the idea here is uh that we remove all the stocks that have less than 10 months of data how do we do that we can just assign that to observations and then we can save valid stocks that would be observations that would be all the stocks that have more than 10 months of data observations yeah those are our valid stocks and now we can just use that as a filter so Factor data would be Factor data again Factor data. index do get level values we get the thicker values maybe I can show you what this is returning this is returning uh yeah an object with all the the stocks we have in the thicker index part of our M index and then we can just say is in the valid stock stocks and this would filter out pretty much everything we have to add this part okay okay so I think oh yeah I missed this part should be all stock index those are the stocks we are going to remove so if we take that out yes so now as you can see we hit before we hit 10,21 rows now we have 10,250 we've removed around 51 rows and yeah with this step we are now ready to calculate the rolling Factor betas we would like to do that simultaneously for all the stocks in our Factor data we can just use the same methodology with Group by and apply a function that would be Factor data. Group by and then we're grouping by level one by ticker Group keys should be false and we can put that into brackets so we can continue on the next row so apply Lambda X and now we want to use the rolling regression actually we may want to explore the rolling regression rolling ORS python it takes the endog and exog all right we here Supply endog and exog so the endog would be our return column right so X return one month and our xog would be everything else what we can do here is say x do drop the return one month column and this would return all the other this would give all the other columns without the return and actually we can add a constant here on the spot so we add a constant and I think the next one we have to supply the window we decided the window to be 2 years right so that would be 24 right 24 months and there is another mean observation the minimum number of observation required to estimate the model we have to supply this one as well this one is a little bit more tricky we have to supply here to have at least the total number of columns plus one so that would be Lan x. columns plus one and this should be our model now we have to say fit and then params and then we can drop the we can drop the constant because it will return constant as we added a constant up here right and I think this should pretty much work except that maybe sometimes we would not have exactly 24 months of observations but we still may want to run the regression so what we can do is use as a window the value which is smaller than two Val so it either 24 months or we can use the number of rows we have in the for the given stock right and we know that we have stocks with more than 10 months of data so if one of the stocks have like 15 months of data you just use the 15 months as a window instead of 24 and yeah I think that should pretty much be it right okay series object doesn't have fit why right maybe okay so that's pretty much that's pretty much uh our rolling Factor betas we have calculated them now we can assign them to betas and that's our rolling Factor BS guys in The Next Step what we want to do is to join them to our current features and with that we have our Full Features data set but before we join them we have to think about a little bit now we have the rolling Factor betas where we used the factor at the beginning of the month and the return at the end of the month so this beta we would actually know at the next month right at the end of the month we go we'll be able to run the regressions and have the betas but we'll have them in the next month so we cannot just blindly join them to the features data we have so far what we have to do is to shift them with one month forward before we join them to the data because these values we would have no not known in the same month we would know uh we would know the mon the the rolling Factor better for example for the end of October we would know them in November what we have to do is we have to shift with one month forward on the ticker level so for each ticker not like the whole data frame if we just say betas do shift it will run it will shift with one row downwards and for example the value of Verizon will come up up here right just see that so Verizon is 0.3 if we apply shift it's now here so that's obviously not correct so what we have to do is we have to first group by and we can do that group by thicker and then we can shift that would now uh do it correctly we can grab that and Supply it here and data will be equal to this and the next step that we want is to impute the missing values of each factor with the average for that factor beta how we we can do that we can say first we can create a a list with the factor columns factors would be just those five columns we can say data. log all the rows factors we'll just select all the factors and all the rows and then we can say the following data. Group by ticker for each ticker we'll select the factors again and then we'll just apply a Lambda function Lambda X where we will F all the missing values with the mean of this Factor but I think because we are doing grp Group by apply we have to add the group keys to be false and now we should have fixed our missing vales issue voila so now we don't have any missing Valu so all all the nas are imputed by the average for this factor and yeah now we can just say drop a if there are any and we can say data. info to see our final result and that is now beautiful guys this is our features data set however I see a column we don't need here which is the adjusted close we just have to drop it data is equal to data drop adjusted close axis equal to one and and this is our features data set guys so we have 18 features at this moment we are now ready to apply machine learning models from here on what we have to decide usually is for each new month we have to form a portfolio with some of the stocks from our uh data set so as we know we have for each month we have the top 150 most liquid stocks and now now at the end of each month we have to decide the stocks we want to have in our portfolio for the next month that's where we can use a machine learning model first we can use a machine learning model to predict which stocks to include in the portfolio as well if we have a long short portfolio we have we can predict which stocks to be long and which stocks to be short but in this course we are just focusing on Long Port foros so we can use a machine learning model to predict which stocks as well we can use machine learning model to predict the magnitude of the position in each stock so what is the weight in the portfolio and the other way is to use a machine learning model in our case a nonsupervised model to decide which stocks to use in the portfolio based on grouping that's why we are going to use a clustering algorithm a KES clustering to keep things simple because from this point on things can get really complicated but yeah that's so far was the preparation to get the data to fit into a machine learning model guys and yeah I'm really excited in the next step we are going to fit a c's clustering algorithm and split the data in in a few clusters each stock will be assigned a cluster and then we'll uh be able to analyze the Clusters and decide what we do further let's move to this step okay so in this step we're going to fit the C clustering algorithm for each month and split the stock stocks into four different groups based on their features y four I've already did a a little work beforehand and I've estimated that the optimal number of clusters roughly on average for each month is around four so we'll use four clusters for each month uh from now on the specifications about the K's clustering algorithm is that it uh it may assign the centroids of the Clusters around randomly and then it assigns a given point to the cluster based on the distance from the centrate to that point to uh help you understand a little bit more so this is an example with the same data but different number of clusters specified when fitting the model so in the first one we have two clusters as you can see really well defined three four and five whenever we initialize the model we have to specify how many clusters we want the data to be grouped in and the algorithm would go in assign random points and then the closest points to the Cent read so Random Cent the closest point to the Cent would be assigned to that cluster and until uh that will be repeated until all points have an assigned cluster how do we start here let's first import the C's class from sklearn do cluster we are going to import K means and now what we want to do right is fit uh K means model for each month and assign a cluster to each stock so get the label we can do that at once using the group by apply logic we've already used for ATR and macd we can define a function which should be get clusters to take data or DF it's up to you and then we'll create a new column called cluster and here we'll fit the CES model so the CES class takes first argument number of clusters that would be four as we already know I've done my research beforehand and then we'll use the random State argument to be zero this would ensure it works like random seed this would sure that we have the same results through different calls and there is another really important argument here they need this is the initialization method of the centroids actually here for now we'll use a random initialization but we can actually Supply here the initial Central points for the Clusters and we may use that in the future so let's see the first result we fit the model follow to the data frame and then we'll get the labels we will return the labels and we will assign them to the cluster column and we just return the data frame in the end and now we can say the following data drop na we will ensure that we don't have any na in the data Group by level one or date and then Group keys equals to false because we do group by apply and then apply get clusters this will take some time but it will now go through each month and fit the K clustering algor with four clusters and assign the given cluster to each stock let's wait for this to run okay so the clustering is finished let's take a look at the results now we have the cluster column and in it we have the assigned values from 0 to three so four clusters 0 1 2 three for each stock and that is done every month now the next step would be to visualize actually our clustering job which is pretty hard when we use more than two features and in our case we have 18 but that's why we didn't apply we didn't apply normalization to the RSI because now we can use it to visualize the clustering job in Better Way all right so I've already wrote down a small function for that up here this is a plot function uh we just select the stocks for the first the second the third and the fourth cluster and then we just visualize them with a with a scatter plot and my idea here is to do that for each month so we can use this function it will select for the scattering it will select the first column which is the 8 R and the seventh column which is the RSI and let's do that now p. style. use we use the ggplot style always and now for I in data. index get level values and we'll get all all the months so unique to list for each month we are going to select pretty much this month right so data. x uh Xs and then I level zero so we are going to select the month and we're going to use this function so we for each month we will just plot the Clusters however we may want to add the month to the title so we can do that in the following way date all let's run this thing and check our clustering job all right guys so now for each month we have uh our clustering job so those are all the stocks and on the Y label we have the RSI values and here we have the ATR values and those are our clusters as you can see they're pretty well defined throughout the different months and if you go through it the first observation is that actually the clustering is working in such a way that all the stocks which are around 60 to 70 RSI are in one cluster then you have the two middle two midles from 50 to 60 and from 50 to 40 and then you have the down cluster here around 35 and what we can see is that this assumption or this observation holds throughout each month of clustering however the problem is that for example in this month cluster zero is around 30 35 RSI cluster one is around 40 to 50 right but then uh this is the same for the next month but at some point cluster zero is around 50 and cluster one is around 60 because the Cent RS are random our clustering gets random and my idea for the strategy we're going to apply is that we would like to follow stocks momentum by stocks momentum I would use the RSI as an as the main indicator right the stocks which are around 70 RSI are in an upward momentum and I would like our strategy to invest every month in the stocks that have have the highest upward momentum throughout the previous month for that job I would like to focus on the stocks that are clustered around RSI of 65 to 7075 but using the random initialization of the Clusters would not work and as we can see so what we have to do now is to help a little bit the CIS clustering Alm by supplying the initial centroids but how we can do that let's check out the K's documentation okay so that's the init argument so the default is C++ we used random but if an array is passed it should be of shape so this shape and clusters and features and gives the initial centers of the Cent that's exactly what we want to do and we want the initial centers to be based on our RSI indicator pretty much what we want is that all the stocks which are around 70 RSI to be for each month to be in the same cluster then 55 the same cluster then 45 the same cluster and 30 the same cluster obviously that would be a little bit better than just deciding those thresholds ourselves for example if you want to select all the stocks above 70 every month we'll have a different result than using this clustering algorithm to do the clustering around 70 for US based on all the features we have in the data set now we have to supply to the clustering algorithm this this array in in this shape with the initial centers okay so let's do that so first we would like to have the target RSI values and that would be a list with the first Target RSI value would be 30 45 55 and 70 then our initial centroids would be n Pi zeros and we were we are going to use the target Val so the length to the Target values which is four and then the number of features right four and 18 because we know that we have 18 columns right 18 features and that would be our initial sent rats however let's run that yeah I think I forgot that part yeah so that's our array however up here we have to supply the sent R what we were going to do is the following all rows and this column as we know that the sixth the the seventh column so the sixth index the seventh column is our RSI column in the features data set we're going to just change that to the RS Target RSI values and we'll get the following here we have the initial sent RS and that's our array we're going to use for the clustering job we have to supply it here but before that we would have to first drop the cluster colum and rerun the whole clustering so data is equal to data drop this will now drop the cluster column and rerun the the clustering and assign the centroids as we have supplied this will ensure that throughout each month we'll have the same cluster label assigned to the stocks corresponding to the the same cluster as you can see again in the first month in October we have assigned cluster two to all stocks with RSI around 70 in the next month it's cluster one we want that to be always cluster two that's why we are doing it okay let's run it we have to wait again for the model to fit okay and it's done we have fit now the model with our initial centroids and we can just now rerun the visualization let's do that now okay so now cluster three is assigned for stocks uh with our around 70 and this is consistent for each month it's again cluster three cluster three cluster three cluster three and now we can use that down the road to select every month the stocks which are for example in cluster three to form our portfolio site and we know that cluster 3 is corresponding to stocks that have had a good momentum through the previous month with that we are ready to move to the next step which is at the beginning of each new month we're going to select the stocks we want to invest in for the given for that month okay guys so we are at the most exciting part now we are going to select stocks based on our clustering and then we are going to form a portfolio using an efficient Frontier optimized Max Sharpie weights for those those stocks we want to choose a cluster based on our hypothesis and as we talked earlier my hypothesis here is that stocks which had an RSI around 70 are having a good momentum and my idea is that this momentum should keep outperforming in the next month and for this job I've analyzed all the Clusters throughout the months and I saw that cluster 3 is always stocks around 70 R side and in this case I would like to select those stocks for every month for my portfolio how do I do that we just select stocks corresponding to Cluster I'm sorry to Cluster three for each month okay so here we can call that filtered data frame and assign that okay so now after we have our filter data frame now the first step is to actually get so at the end of October right on 31st of October we had those stocks that's our list of stocks which we would like to use to invest in November my idea here is to create a dictionary with the first day of the next month and all the stocks for the for the next month in a list to do that we have to do the following so filter DF we first reset the index we reset actually only the first level okay all right so we've done that and now now we can do the following we can just use the index and add one date to it so pandas date of set one which would move each index with one day in the future as we know all the indexes are the last day of the month so we'll have the beginning of the next month like this and the next step would be to do the following so we we can reset the index and then we can set the index again to be a multiindex so we have date and thicker let's see all right perfect if that doesn't work to you you can do unstuck and then stuck but it's not necessary at the moment we do this step and then the next step would be to create the dates object so dates would be index. get level values we get all the months and then what we have to do is to create this dictionary which is having a key the date so the beginning of the new month and value would be a list of all the stocks for that month we can call that fixed dates it would be a dictionary and then for D in dates we do the following fix dates do D here we'll fix it to be string have time in our format year month day and this would be equal to filtered data frame pretty much we will select each month right we select each month and then we get the index after we select the month the index will be the tickers and then we call it to list and that's pretty much it this should give us this should create for us this dictionary and voila we have it so now we have the first day of the next month right and the stocks list we the stocks we want to invest for the next month into a list and actually we can now really easily use that to create our portfolios and with that we are ready for this step so the next step is to Define find the portfolio optimization function we're going to use the portfolio opt package and the efficient Frontier to have on a portfolio which maximizes the Sharpie ratio so okay let's move to define the portfolio optimization function we'll use to do that first we will import from pfolio op package a few different classes so from P PF op do effici efficient Frontier we import the efficient Frontier class then from ppf opt we import risk models and again from ppf op we will import the expected returns object okay so now what do we want to do so at the beginning of each month we have the list of stocks we are going to invest and those stocks we have to assign weights to them we have to find those weights we are going to use this package to optimize and find those weights for every month we'll Define this optimize weights function which will take two argument prices actually one argument for now and and uh we'll add another one in the later on so first we'll the the function will take only the the prices of all the stocks we want the weights to be optimized for first we have to calculate the returns of those stocks so we will use the expected returns from expected returns we use a method called mean historical return which take an argument price PR and we will supply the prices as well it takes an argument frequency and that would be 252 days so one year of trading data then we have to calculate the co variance and this time we use the risk models so from risk models sample Co variance again we have to supply the prices and the frequency again one year of data after we have calculated the returns and the co variance we can now initialize our efficient Frontier object efficient Frontier and now we can take a look by portfolio op efficient from here let's read the docs okay but that's the general we want the mean variance optimization okay so the efficient Frontier takes the following argents so expected Returns the covariance Matrix weight bounce and then the so all right the expected returns maybe we can just c those the expected returns would be our just calculated Returns the C coverance Matrix would be C and we will keep this weight bounce for now I will show you why and we use the S CS store and after we have that we can just calculate the weights weights would be from efficient Frontier we will use the max Sharpie method that should give us the weights and then in the end we can get the clean weights which will be round the weights and Clips near zeros so we we want the the clean weights we will just return ef. clean weights and that this our portfolio optimization function so obviously we have to supply the onee prices to this function and it will calculate the returns of those stocks so we have a data frame with one year prices of all the stocks for a given month this function will calculate the returns it will calculate the co variance then it will fit the efficient Frontier optimization and come up with the optimized Max Sharpie ratio weights and return the weights however there is a small specification and that is the weight bounce pretty much this is the bounce for a single stock what can be the constraint for weight of a given stock at the moment it is from zero to one so after the optimization we may have some stocks with zero weight and we may have a single stock with 100% weight obviously we don't want that so we will come back and fix it but yeah that's uh pretty much our function for diversification purposes we would like to actually have a maximum weight of 10% of our portfolio in a single stock so we can already assign that to be 0.1 but for the lower bound we may use a more Dynamic metric so we'll create a argument here lower bound which would be equal to zero but for example for the lower bound when we do the optimization we may use half the weight of an equally weighted portfol for example for a given month we have 20 stocks an equally weighted portfolio would have 5% in each stock so we may assign half of that weight let's say with 20 stocks we may have um the smallest weight to be 2.5% half of equal weight and the maximum to be 10% and with this we will ensure that we have a diversified and well balance portfolio and yeah guys with that we're ready for the next step and in The Next Step we're going to download fresh daily prices of all the stocks that may end up in our portfolios as we know the first portfolio we are going to form at 1st of November 2017 and from our optimized weights function we know that we need at least one year of data prior to the optimization so the starting date of our uh of the download should be 2016 1st of November at least and to download the prices we can just use wi Finance package the first step would be to create the stocks list actually we can go back and use the data object that would be the 150 most liquid stocks because any one of them may end up in our portfolio we want the data for them so data. index. get level values get level values that will be thicker right or thicker values unique and then to list so we have the stocks list and then we can create the new data frame uh we can yeah the new data frame we download from y finance thers will be our stocks and now the start date yeah as we know the first date we want at least one year before that so we can actually to make things Dynamic we can use the data do index get level values but here we we'll get the the the months then we take the first value so the first month that would be October 2017 but we can just say minus Panda's date of set and then months equal to one so minus one month this would return us exactly end of September 2016 actually I went to 2017 okay so we want 12 months we want one whole year I'm sorry my bad guys that's exactly what we want so that would be our start date and our end date would be pretty much the same however that would be the final value from our unique month index of the day date object and that's pretty much it this will download the data we need for our optimization and after this is downloaded we are ready for the next step where the most interesting part is coming we are going to Loop over the fixed dates dictionary and for each new month we're going to get the stocks optimize the weights calculate the daily portfolio return and then uh calculate the whole time period portfolio return by rebalancing and optimizing every new month the download is done and we're ready to move to the next step first we are going to calculate the returns the daily returns for each stock from the fresh new data we just downloaded and then again we're going to Loop over each month select the stocks for the month calculate the weights form the portfolio and calculate the daily return for our portfolio and for our strategy all right so let's start first you want to create the returns data frame would be we calculate log returns so from new data frame we'll select the adjusted Clause column take a log and then the difference that would give us the returns data frame awesome after we have the returns we'll create portfolio DF which would be an empty data frame and then we will Loop over so for start date in fixed dates. keys so we are going to Loop over our dictionaries for start date let's so for each date we are going to first get the end date so the end date would be should be the end of the month right we're investing for one month and then we rebalancing so at the beginning of uh November 2017 we want to select the returns from this return data frame we want to select the returns for the next month right so the end date would be the end of November 2017 that will be P to date time start date Plus pandas date offsets actually I think here we'll use offsets and then month end and we can call the string F time method so we have the right format and we can print that now actually we can print both the start date and we can print the end date for each month we have the start of the month and the end of the month and now the next step is to get the columns for the month to do that we can just say the following so columns would be equal to fixed dates and we are just going to select the star date and those are going to be our columns right so those are going to be the stocks for each month that uh we are going to form portfolio portfolios with after we have the start date and date and the stocks for the given month now we want to calculate the weights but to calculate the weights we need to do the optimization and as we already know to find the optimized weights we need a onee data prior to the start date and it has to be daily so for that data we'll use the new data frame that we've just downloaded which contains the prices and pretty much what do what do we want so to calculate the weights for all stocks at the beginning of November 2017 so for all those stocks we would have to input the prices for one year prior of those stocks into this function which will calculate the weights and then we'll have the weights for uh for the first month so for the November 2017 in this case we would have to create an optimization data frame first we have an optimization optimization start date and it would be pandas to date time start date it would be the given start date 1st of November 2017 minus Panda's date off set and we will supply months minus 12 months exactly so exactly 12 months ago would be the optimization start date and we will create an optimization end date as well so the end date would be actually again start dat minus however one date and now we can print to visualize everything we here so far so we have the start date we have we have the start date then we have the end date then we have the stocks that would be included in a given month then we have the optimization start date and optimization end date date of set my bad so date of set and now for this month so for 2017 from 1st of uh November until the thir 30th of November that would be the stocks in our portfolio and we'll have an optimization so to calculate the weights for those stocks we have an optimization data frame which would be for exactly one year prior to the starting date so from uh 1st of November 2016 up to 31st of October 2017 and now the next step is to calculate the weights for the weights we use we are going to use that was my point one so we're going to use our optimize weight function and and here we have to supply the prices of the optimized weight and the lower bound right the prices would be actually an optimization data frame this optimization data frame we can use the new data frame which contains prices right and we can select the following dates from the optimization start date to optimization and date then we will select the adjusted close and we want only the columns that we are going to use for a portfolio for this month and that would be pretty much our optimization dat frame so in this case yeah when I run it so that's that because in the for Loop the the last values for the start and end date would be the last values based on the last value of the fixed date key so it it's fter out from uh 3rd of October 2022 up to 29th of September 2023 that's around almost one year but what we are going to do for the sake of Simplicity because our first start date is 200 yeah 2017 1st of November we are going to select the following date 2016 1st of November until 2017 30 to October and this is going to be our optimization data frame to calculate the weights we are going to supply that up here as well we have to supply the lower bound that for each stock what would be the lower bound the upper bound we know it's 10% maybe we want to make it this one dynamic as well but for now let's keep it 10% but the lower bound we want to be half of an equally weighted portfolio so a equally weighted portfolio is going to have weights which are pretty much equal to one divided by the number of Assets in the portfolio so the number of columns in this case we can use we can so we would have 10 columns so 10 assets for a given month so the equally weighted portfolio would have 10% for each stock half of that that would be the number of stocks multiplied by two I mean one divided by the number of stocks multiplied by two and that would be 0. uh 0.05 and now we can wrap that around in a round function so we are always sure we have around weight and now we can run it to optimize our weights however I think that up here our columns are actually not the right columns because we would have to select the first optimization date so that would be 11 this is the list of our columns and our optimization data frame will be the following and now we can input it here and check out the weights this now looks quite interesting first let's take a look at the lower bound so the lower bount is 0.01 if we apply it will be 0.012 okay maybe we can use a rounding of three instead of two okay now this looks better those will be the weights of our port portfolio in November 2017 as you can see for stocks they have a weight of 0.01 so 1.2% of our total portfolio and then we have this stock which has 10% of our portfolio this one has seven this one has 6.8% 5.9 3% 4% 4% that's what we are going to do at the beginning of each month we are going to optimize and find the weights for our stocks for for that month okay now we we can Implement that into our optimization so up here we are going to provide the weights and Next Step would be to create the returns date frame but actually before that I think we have we can turn that into a data frame so we can say the following weight equals to pandas data frame frame and then weights and index is uh Panda series and now we have the weights as a data frame and we can now potentially use that with the returns data frame to multiply the way to Fe stock for the given month by the return for that stock for every day and this way we'll be able to calculate the portfolio return for each day okay next step would be to create a temporary datea frame which would be equal to returns data frame and here we'll use the start date up to the end date we are going to filter the beginning of month so that would be 2017 first until 2017 13th that will be our temporary data frame let's see what we have here yeah so we pretty much selected the stocks and the the returns of the stocks for the given month the selected stocks and now what we would like to do is just merge with the weights data frame to get the stock return for each day and the weight for that day we can do the following first stock we are going to stock and then we're going to say to frame to frame return now we have for each day all the stocks and then the return column and now we want to reset the index we are going to reset the date index and next step is to merge with the weights are looking something like that we can say stock to frame weight and we can remove we have now a multi index up here so level zero level one so we want to remove the zero we can say reset index level zero drop equals to true because we want to just drop it directly and we are going to merge those two left by index so left index true right index true as well and we'll get something which looks like that the next step would be to reset the index and set a new multiindex date and I think this one doesn't have a name right now so it would be just called index and now we have almost what we want so we can say unstack and then stock and now we have the data frame with the return for the given date and the weight of the stock for the whole month of our stock and we can call this our temp dat frame the temp dat frame was initially the start date to the end date we can grab that put it up over here and now we can grab this part as well put it over here and let's take a look again now we can just simply use a vectorized operation we can multiply the return column by the weight column and then we can get the waited return for each stock for every day of the given month and let's do that but before maybe we would like to fix the index so the index to be the index names actually so date and thicker to keep things tidy names and now now we would like to calculate the weighted return for each stock which would be just the return return column multiplied by the weight column and now we have the weighted return for each day for each stock and the next step would be just to calculate the daily the sum of the daily weighted return which would be our portfolio return for the given day and we can do that by saying the following Group by level one or date level zero or date whatever you prefer so awaited return do sum that will be the portfolio return for every day and then we can just say to frame and and call it strategy return and that will be our strategy return for the for the selected month obviously we are looping over each month and we are doing the the operations for every month we can assign that to the temp DF and move it over here and the next operation would be just to you know concut this to the portfolio data frame so portfolio data frame will be to pandas concut portfolio data frame with temporary date frame axis equal to zero or the default axis and that should be it guys we have our so we Loop over each month we select the optimization dat frame we are doing the optimization calculating the weights then we are merging the weights with the returns of those stocks for the given month and we're uh calculating the waited return for each day for our portfolio which is our strategy return for every day let's run that and see if it's going to work out okay so we have an error which is return okay so I found how to overcome the error we'll just Implement and try and accept cloud so try and we'll try to run that except exception s e and we'll just print the exception as well I was digging through the code and I figured out guys that we have forgot a really important part of it and up here we don't specify the optimization data frame we would have to add that the optimization data frame was pretty much our new data frame where we select the optimization start date and the optimization end date and then we select the adjusted close and the columns for the given month and with that I think now the code the optimization part should be pretty much fixed and we have something so please check your objective constraint or use different so so status invasible okay yeah so uh this is actually happening from time to time and pretty much the solver is not able to optimize the max Sharpie ratio weights and instead it's failing if we visualize the portfolio returns you can see those periods where there is no data so pretty much the so over has not optimized the portfolio we didn't have any weights and we end up with no investment for those months a work around would be actually to do the following I've really spent some time on that part and unfortunately there is no easy solution for this uh optimization failing for the max sharp ratio the only workaround I figure out is to implement the following whenever the optimization fails and we don't have the max Sharpie ratio weights we will use equal weights for our portfolio as simple as that we'll do the following so we'll try to calculate the weights and then if this doesn't work out we'll do accept and then we'll print that Max sharp optimization failed for start date continuing with equal weights and let's see if this is the issue if we run that now it should print yeah exactly so for those months the max Sharpie ratio optimization failed and we have to continue with equal weights so how do we do that we'll do that the following way we'll create a new variable which is success and it will be equal to false however if the weights have been optimized it will be equal to true all right and then we can just do the following so if success is false our weights would be pretty much we would create a pandas data frame with our weight so pandas data frame but let me take this out of here let's put it down here we'll create a pandas data frame and then we want equal weights so we can just use this part right here we want equal weights so that would be the equal weight right one weight but we want it for for the number of stocks in our portfolio so it will be for I in range L optimization DF do columns yeah that would be our equal weights and then we have to supply the index and the index would be just the columns is a list so columns to list and the columns would be a pandas series zero and this would give us a data frame containing the stocks for the given month with uh their weight assigned as an equal uh equal weight and we can just transpose that and have it like that so this would be our equal weights data frame if our optimization fails and we'll have only equal weights whenever the optimization fails and after implementing that so yeah pretty much you can Implement a print for print statements throughout the code to double check if uh we work correctly but I can assure you that it's working correctly and now if you want to visualize the returns just do a simple plot you see that we have the returns throughout the whole time frame and with that pretty much guys we are ready with our strategy portfolio building we may want to apply to drop duplicates up here if we have any duplicates I think we don't but just in case and yeah with that guys we are ready for the next step and in The Next Step we're going to visualize the returns of our portfolio and compare it to a benchmark which in this case would be the S&P 500 Index itself it will be really interesting to see the results of all that hard work so far so let's move to that part and to compare our portfolio to the S&P 500 first thing would be to just download the returns for S&P 500 so we can say for so spy and we have uh we download the from wi Finance stickers AR tier would be spy the ATF on S&P 500 the start date we can just Supply like 2015 1st of January and the end date we can supply daytime date today and now we would pretty much create a new object let's call it spycor red so spy return and here will just take the log return of spy and just close then difference then drop an name rename and yeah first let's check spirate before okay so this is the log return of spy for each day and now I'll would just like to rename the adjusted close to Benchmark return or spy Buy and Hold would be adjusted close to spy Buy and Hold AIS equ one and then we can use the portfolio data frame to merge the Spy return to the port for dat FR so left index would be true right index would be true by the 4 is an inner joint so we don't have to worry about uh okay so we'll just you know we'll just rerun this sell and then we'll run rerun this one and now we have the strategy return and the Spy Buy and Hold return for the same dates and we can move to the next step where we would like to you know calculate the cumulative return for both the S&P 500 and for our strategy and plot them to compare them in time we'll do the following M lip style. use ggplot I think we've done that previously but just in case and now we want to calculate the portfolio cumulative return cumulative return and that would be from npy expanding window so NY XP log from portfolio data frame and then after that we call the and this should calculate the cumulate is starting from one but we have to say minus one and now we would have the cumulative return for our strategy and the S&P 500 itself now what we may want to do do is just to select for example that we want the portfolio return up to a given point let's say we want it up to 29th of uh September so we just say up to 0.9 20 29 and then we can plot that we can say fix size 16x 6 and and and that's uh pretty much uh that's our comparison guys we can add a title so p. title unsupervised learning trading strategy returns over time as well we may want to PTY label return return and maybe we would like to fix a little bit the axis up here to be in percentages so we can call the following GCA do ya axis set major formatter and here we can supply a formatter like there is a format from MTI but I think we have to first sent format there I think we have to import it up here so we have to import from modb forat B Li thicker same thck uh so import map yeah as MTI and this would do it uh Y axis set major format and that is pretty much our strategy guys up here we have selected the 150 most liquid stocks for each month we have calculated 18 different features on each stock then we used a c's clustering algorithm to assign a cluster for every stock we supplied our custom sent we initialized our sent with a custom array and then for each month we optimized the portfolio to have the maximum Sharpie portfolio weights for every new month and form this strategy using S&P 500 stocks and now we compare to the return of the S&P 500 nevertheless this is not a financial advice you shouldn't use that anyway if you come up here and just change the logic for uh selecting the stocks from third cluster to zero uh to Cluster zero everything will be changed on the spot and you see that the results would change dramatically as well that's just example of how we can Implement machine learning into trading strategies and forming portfolios Etc so that's pretty much it for this strategy and into the next one we're going to form portfolios using Twitter sentiment data I'm looking forward to that and let's move to the second project in the second project we are going to build a Twitter sentiment investing strategy for this project we are going to load a data set which will be available for you to download from the description of the video there is a link where you can download the data set used in this project and in the third project as well the notebook or all three projects would be available on that link too so we would be loading this data set containing Twitter sentiment data on stocks from NASDAQ 100 this strategy would be focused on NASDAQ 100 stocks and we are using Twitter sentiment data the idea here is that I would like to to it will be a smaller project than the first one but I would like to show you how we can create a derivative feature from the Twitter sentiment data to create potential value for our investments first we'll start by importing the packages those are the packages we're going to use panda snai m clip datetime wi finance and the operational system package we import daytime is DT that's the I think we're going to change and again we are going to use ggplot as our mpot lip style first let's define our data folder so this would be the path to the sentiment data file you're going to download so it would be a CSV file this is my folder which contains the data and you would have to assign the path to your folder up here let's load the data so sentiment data frame would be uh pandas read CSV and here we have to supply the puff os. puff. jooin data folder and the file name so the file name is sentiment data.csv and let's run that and see what we have here that is the the sentiment data we have we got the the date the symbol and the corresponding Twitter posts Twitter comments Twitter likes Twitter Impressions and the Twitter sentiment uh the Twitter sentiment is calculated uh by the methodology used by the data provider we just use it as it is here the first thing we want to fix is the date we want the date column to be date time so pandas to date time and next step is to set the indexes again we are going to work with the multiindex in two levels so sentiment dat data frame set index this is going to be the date and the symbol date and symbol so far so good now I would like to calculate a Quant feature out of this data and it would be called actually I'm interested in the engage the Twitter engage each stock here yes not only just the raw sentiment or the RO Impressions likes and comments but I'm interested in the engagement that people have on the posts on Twitter for this company because we know that there are a lot of bots on Twitter and they are messing up the data skewing the data on some post or some symbols and actually I would like to have engagement ratio which may be more informative than having just the live likes or the comments or the sentiment on its own and this engagement ratio will be pretty much the Twitter comments divided by the Twitter likes if a given company has a lot of likes but no comments we know that this is Bots activity but if there are quite some a lot of comments and a lot of likes as well and actually the comments are as much as the likes are even more we know that people are engaged on this stock let's calculate the engagement ratio engagement ratio and this engagement ratio will be pretty much Twitter comments divided by Twitter likes so how much are the comments compared to the likes and that's our engagement ratio for we have this engagement ratio for each stock for every day and now we may just want to filter out the data frame to to contain only stocks that have for example more than u a given amount of likes so let's say we want stocks that have more than 20 likes for a given day and more than 10 comments on a given day because otherwise it's just random noise so Twitter likes we would like to have more than 20 likes and You' like stock to have more than 10 columns to be kept into our investment universe and with that we are pretty much ready with the first step we are following pretty much the similar logic to the previous project however here we're going to create an equally weighted portfolio we are not going to implement the efficient Frontier optimization ation every month we would decide an investment Universe of top five stocks and just invest into them and now after we have uh calculated the engagement ratio now we want to pretty much calculate the average engagement ratio for each stock for every month and we can do that the following way so we the reset the symbol index the symbol part of the index we reset symbol and then we'll Group by we'll group byy for every month and every stock we'll Group by on the monthly level and the symbol level and then we are going to maybe we can put that into brackets then we are going to select the engagement ratio column and calculate that the average engagement ratio for the whole month pretty much and now we can assign that to uh new variable new data frame called aggregated DF and now we can calculate the crosssectionally the rank for each month of all the stocks based on this engagement ratio how do we do that we say a new column rank we are going to group by level zero or date so we are grouping by each month because already we have uh aggregated to the monthly level right so we group up group on each month and then we select the engagement ratio and then we are doing the following so on the next row we're going to say transform Lambda but maybe we can put that into brackets and move that on the next row so Lambda X and here we'll just say x. rank ascending equals to false and this would create a new column called rank crosssectionally for each month based on the engagement ratio and the stocks with the highest engagement ratio would have the highest rank and we can obviously see that already so now we want to pretty much select for each month the top five stocks for each month based on this Rank and grab them to use similarly to what we did in the previous project grab the start date and the stocks that we want to include in the next month and then calculate the returns but this time in an equally weighted fashion so let's do that I'll cut those celles and yeah now we'll just select the top five stocks for each month so we'll do the following aggregated data frame rank where is below six and now we have the stocks with the highest engagement ratio for each month we can assign that to filtered DF data frame and after we have the filter DF data frame we can just reset the symbols part of the index so reset index level equals to one and then we just fix the index again the same way we did last time so we'll just add one day to your index so plus day time uh Panda's date of set one day let's see what this would give us now we have the beginning of the next month and the stocks we want to invest in for the next month so those five stocks for each month and yeah pretty much next step is to uh reset the index and set again the multi index we'll set the multi index again date symbol and visualize top 20 rows so far so good now for for the beginning of each new month we have the top five stocks by engagement ratio and now we would like to create the fixed dates object like we did in the first project from dates would be good to filter data frame index get level values date and here yeah unique and to list so we make a list and fixed dates would be again a dictionary empty dictionary for D dates pretty much we are following the exact same logic we did in the previous projects so it's quite straightforward I hope this stuff will guys however uh yeah in this project we are not doing the efficient front here optimization just rather use a weekly weighted portfolio the idea here is to show you how we can extract value from different uh data sets and yeah as you can see now for each new month we have the list of five stocks we are going to invest in and the next step would be to download the fresh prices to do that we can first create our stocks list it would be equal to sentiment data frame do index get level values symbol and again unique to list this is our stock list and we can create a new data frame pric DF where we are going to download again from yaho final our start date we can use as you you can see the we can use just the 1st of January 2021 to be sure we have enough prices and the end date would be 2023 1st of March so 2023 1 of March awesome after we have our our list of stocks for each month and the prices we can move to the next step where we are going to calculate the portfolio returns following similar logic to what we did in the last section so from prices DF we can select the adjusted close column and calculate the log returns for each stock and yeah pretty much call that returns date frame so after and we can drop the na values in the end after we have the returns we can now create portfolio data frame which is just an empty data frame and for start date in fixed dates. Keys print start date just to visualize what we have so far so for each for each month we have the start date and now we want the end date for our portfolio so end date would be pandas to datetime start date plus pandas offsets month end which will give us the which should give us which should give us the end date of the same month uh okay pretty much we have that however we would have to add the string F time method in the end like that that's pretty awesome so what was the next step The Next Step was to select the columns or for the given month that will be the stocks we are going to invest for that month and these will be fixed dates start date so for each month the stocks we have to invest for that month and now we can create our temp data frame where we would be calculating the equally weighted return for our portfolio for every day this would be returns date frame here we'll save from start date to the end date and then for the selected columns we're going to calculate the mean the average by columns so the average daily return and we call that in the end we say to frame and then portfolio return portfolio return next step would be just for each month to concut this portfolio return and with that we have calculated the equally weighted portfolio return for every month for those stocks we have selected using our methodology about aggregated uh mon monthly average engagement ratio for every stock and then ranked crosssectionally and now we are ready to pretty much visualize the return the cumulative return of our portfolio and compare it for example to a benchmark like NASDAQ right those stocks are constituents of NASDAQ so we may want to compare it to the NASDAQ indexes a benchmark again guys I've said it in the beginning of the tutorial and the course we're using stock lists which are not survivorship bias free and that is really important and most probably skewing the results upwards so you should never use you should always use survival ship bias free stock list when you're researching strategies so now what do we want we want to just download the prices for the NASDAQ or the NASDAQ ETF which is QQQ so we would use start date to be 2021 1st of January and the end date would be pretty much the same as for our portfolio so first of March and now we can calculate the NASDAQ return so QQQ return that would be log from the adjusted close differen and we may want to rename it to Mazda return and with that we're ready to merge together the can return to our portfolio date frame so we merge by index by theault the merge is always using uh in our joint so we are not obliged to specify that or maybe we are no we are not and now we have the portfolio return for every day and the NASDAQ return for every day and the next step would be to just visualize so we can say the following portfolios cumulative return will be equal to uh we'll calculate that in a n expanding window where we take uh log from the return of both portfolios and then we apply the cumulative some over the whole expanding window so for for each new uh each new addition We Run The accumulator some in the end we have to substract one and that's pretty much ready for plotting so we create a plot with a fix size equal to 16 and six and we can set a title like Twitter engagement ratio Str return over time as well P GCA Y axis set major format that would set the the y axis to be a percentage so we'll use the percent format as well we give a name to the Y uh label and we just show and that's pretty much our second project guys about the Twitter sentiment investing strategy with monthly rebalancing of an equally weighted portfolio our uh stock selection criteria is based on the engagement ratio which we calculators the Twitter comments divided by the Twitter likes for each stock and we select the top stocks which had the highest average engagement ratio for every month if you want to change the selection criteria for example if you want to select the stocks which have on average the most likes for every month you just we can just simply do that so cop this com put it up here then put it up here and now we have the new we have pretty much the ranking done by average T likes through throughout the month and if we run the symbols down the sales down below it will be redone using the likes and yeah as you can see when we use the likes our strategy underperforms however if you use the comments maybe we can try with the comments to see what what would be the results with the coms similar results similar results however a little bit um higher performance so the idea here is to show you that if we have alternative data we may need to implement some calculations to get a derivative features which then may be valuable for our research and for our strategies that is the main idea here and if we return the engagement ratio let run the whole thing we are able to see the difference in the results obviously the engagement ratio by using the engagement ratio we have some we've created a little bit of value above the NASDAQ itself and that is pretty much it about the second project and we are able to now move to the third project where we are going to create intraday trading strategy on a simulated one asset data using a gar model so a model to predict the one day ahead volatility and it will be a really interesting one so I'm really excited to move to the third project and let's get into it in the third project we are creating an intraday strategy using gar model we're going to use simulated daily data and simulated intraday 5 minute data on a single asset and we are going to create two signals on two different time frames on the daily level and on the intraday level so on The Daily level our signal would be driven by the gar model volatility prediction we are going to fit in a rolling window for every day a gar model and predict the volatility for the next day and from that we're going to calculate the prediction premum premium and derive our daily signal out of that on the intraday level we are going to calculate technical indicators and create a signal following a intraday price action pattern and merge with the daily signal and calculate our final signal intraday only for those days where simultaneously we have the intraday and the daily signal and whenever we have the the intraday signal we are going either into a long or into a short position and we're holding until the end of the day and that's pretty much the strategy guys let's start by first loading the data we are going to use so the data again it will be available for you to download from the link down in the video description first we start by importing all the packages we'll be using for this project they are MP lip Arc we'll use the arc project for the gar model then uh Panda snai and the operation system package again our uh data folder you have to provide the path to your data folder and we can start by loading the data so daily uh data frame would be pandas read CSV all spot join and then the daily date frame which is simulated daily data. CSV and this should be our daily data here we would have to fix the date we use pandas to daytime and we Supply the date as well then we are going to set the index for to the Daily date frame to be the date pretty much standard stuff and our I have already calculated the log return but to repeat that I'll just calculate the log return which would be n log from the adjusted close and then we just do the dot difference method and so far so good that's our daily data and our intraday data would be intraday 5 minutes DF again we are going to load it using pandas stre CSV so p. join and the name of the file is simulated 5 minutes data. CS we can commment out this code this one as well and here first first step we need to fix the daytime column to be date time so pandas to date time that's the first step then we want to assign it as an index and the next step would be to calculate this date column so we can just redo that real quick or we just cast it to data we already have it so we'll cast it to daytime but if you're wondering what it is it's just selecting the index and then date which which would give us the date column and with that we are pretty much ready with loading our daily and intraday 5 minute data and we're ready to move to the second step so in this step we are going to define a function which would fit a gar model and predict the volatility for one day ahead in a rolling window we'll use a six months rolling window for that but the specification here is that whenever you fit a gar model you need to supply the auto regressive and moving moving average orders you need to actually find those orders and to do that I've used a Brute Force approach where I fit 16 models with a combination for the outo aggressive and moving a average orders from one to four and combinations between them so I fit 16 models then I've extracted the mean square error and the beian information Criterion and pretty much I chose the best model to be the one who is minimizing the beian information Criterion so I found that the best G model on this data is having Auto regressive order of one and moving average order of three so P = to 1 and Q = to 3 so that's a really important um clarification first you start by calculating on The Daily data frame the variance or actually the rolling six months variance of the log return that would be the variance and we may want to visualize that this is the variance of our data the variance of the log return of our data through time and now we would actually want to for the sake of Simplicity I'll just filter out the date to be be from 2020 onwards and now I'm going to create a predict volatility function so we are going to predict the volatility for every day so so far we have the Vance pretty much the volatility is predicting the Vance with a G model when you fit a g model You can predict the variance or the uh the mean of the series we're going to predict the variance of our asset for every day and we would have to do that in a rolling window so first we create the best model which would be where you going to use the arc model and here we have to supply Y which will be our X or the series then p is the order the regressive order the moving average order Q so those values we have to find with the Brute Force approach as I did I mean fitting 16 or 25 models collecting the metric the mean squar error the be information Criterion after you have run the model and choosing the one which is minimizing the for example the bean information criteria that's the best model so after we have supplied arguments to the arc model next we are going to fit the model here we would say the update frequency to be every five steps and this position to be off and this is our best model and now we can forecast right VAR forecast so from the best model we are going to forecast Horizon equals to 1 so for one day ahead our data is daily so for one day ahead and then after the forecast is done for next day we have two columns or two vales the mean or the Vance so we choose the Vance and and we're going to choose the last value so last row of the First Column we may want to print the date so x. index last date the one we are going to forecast for then we'll just return the VAR forecast and this is our prediction function guys now we'll just say the following predictions and our predictions are going to be based on the log return we're going to apply a function in a rolling window of 180 days the same way as we did for the Varan 180 days and here we say Lambda x equal to predict volatility X and we can now pretty much run that okay yeah my bad there is no do up here what is the problem okay I think yeah it's iock here and this will now just run in a rolling window fit the best G model we know that the best G model for our data is auto regressive order of one moving average order of three and yeah we just have to wait for this thing to run and predict the volatility for the whole data set after we are done with that we will move to the next uh step where we are going to calculate the prediction premium and form a daily signal out of it okay so the model is done fitting and predicting the Vance and we can now take a look we have the V and the predictions actually if you look closer then the Val are pretty close and similar actually we may want to visualize them if you're interested guys let's see what we have here so we have the Vance and the predictions and yeah maybe we can just plot them so the Vance is more stable than the predictions and as you can see the predictions are exploding here or there but we may deal with that if we apply a rolling average or a CA filter model on the predictions they will smooth them out considerably however for the sake of this strategy and this course I'm not going to implement that but usually you may think about it you know like signal reduction techniques okay so the next step we are going to calculate feature called prediction premium and after we have the prediction premium we'll calculate the six months rolling standard deviation of that uh prediction premium and from there we'll be able to calculate and create our daily signal the prediction premium would be a very simple formula it would be equal to predictions minus so from the predictions we are going to substract the Varan and then we're going to scale the whole thing by the varas itself it's a simple formula then we're going to calculate the premium standard deviation premium standard deviation which would be just a rolling 6 months standard deviation and let's run that and see what we have now we have the prediction premium let's visualize it real quick so this is the prediction premium and we have the standard deviation of this prediction premium awesome from now on we can create our daily signal right so our signal daily will be the following our daily signal will calculate by doing apply Lambda over each roll we have one if prediction premium is higher than the premium standard deviation multiplied by 1.5 so 1.5 if the prediction premium is higher than 1.5 standard deviations then we have long signal or one else we would have minus one if the prediction premium is smaller than minus 1.5 standard deviations and here we have to say the following else MP n and then we'll do that for axis equal to one we're doing that for each row right let's see what this would give us this would create this signal daily column so whenever you know the prediction premium is higher than 1.5 standard deviations or lower than 1.5 standard deviations we get the signal we may want to explore that a little bit equals to one we have quite some signals for long but actually you know what we may do the following we may just plot a histogram instead of uh you know obing the we want to understand how much short and how much long signals we have so kind histogram and then p. show this would create histogram and we'll be able to understand how many okay so what is the problem funny okay so now we can see that we have around 50 long signals and then around 35 short signals and yeah with that we are ready with our daily signal guys and we can move to the intraday part of the strategy and here we are going to merge our daily data with the intraday data based on the based on the column on the date column have created actually to do that we need to shift the daily data one day ahead right because at the end of the 29th for example we have the daily data but we'll be using it on the 30th so we need to do the following up here we need to say signal daily will be just shifted one day forward that's pretty much what we have to to say and we can move to forming the final data frame here we can use our intraday data frame we're going to reset the index and then we're going to merge so do merge actually we may move that on the next row so we are going to merge with the daily date frame and actually only with signal column because we are not interested in the other daily columns we interested only in the signal column we're going to reset the index on The Daily data frame as well and here we'll be using the left on so left on date left on this this column and right on date as well or on the index column pretty much this one after the reset it will be available as a column and that's pretty much our join now we have the five uh five minute intraday data right and we have the daily signal derived by the gar volatility prediction now we may just want to set the index to be the DAT time again and that's pretty much our final data frame we can call it final data frame comment that out okay so now we want to drop those two columns we don't need the those two date columns so we'll say the following final DF do drop and yeah we'll drop date and date and we are ready now to calculate all the technical indicators potentially we can use for this strategy to do that we we will need the pandas ta package which I forgot to import so we are going to import the pandas ta package and the first indicator I would like to calculate is the RSI indicator from Pand ta we are going to use the RSI my to and we'll just apply the close price and the length would be 20 next one is uh the Ballinger bands I would like to calculate so b or lower band first here it's B bands again it takes the close price as an argument however the B bands are returning five columns instead of one we have to specify that we want the First Column to be returned so iock all the rows and the First Column that would be our lower band and for our upper band it would be the third column lower middle upper those are going to be Ballinger band lower upper band and now I would like to have potentially to use the ls of the lower and over band but let's continue with d and for after we have calculated the indicators we can just run that part check out the indicators and now we can move on to calculate the intraday signal so signal intraday would be the followings so we have a intraday signal whenever the RSI is above 70 and the close price closes above the upper Ballinger band so whenever the RSI is above 70 and and the price closes above the upper boringer band we have a burst of momentum upwards or downwards that's my strategy idea and this would be our Buy Signal and the sell would be when at RS SI is below 30 and simultaneously we have a close below the lower bar this price action pattern is a pure momentum pattern and we can again uh calculate using the applied Lambda function so one if RSI is above 70 and if the close price is above the upper band else we would like minus one if the opposite pretty much right R side below 30 and close below the lower band else we would like to have MP n if those conditions are not met and the axis will be equal to one we can run that and see what we call we have a small Arrow up here why okay so there's something wrong with our data we can call the info and everything is float what is the problem here guys I found the error guys I have forgot to add the X up here for both the upper and the lower bound and that was doing the error it's going to calculate the intraday strategy signal as well now and with that we are actually pretty much ready with our intraday signal and our daily signal and we can move on now to generating our entry points and our final signal the final signal that we are going to use to calculate our daily returns from to do that we are going to do the following so final DF return signal or return sign that will be the of our return and actually the direction of the position it would be the following we would have a short position whenever the daily signal is one so the prediction premium is above one standard 1.5 standard deviations High prediction premium let me grab that signal daily so whenever the signal daily is equal to one and the signal intraday is equal to one as well the intraday situation is that the price is super overextended our RSI above 70 and simultaneously the close is above the upper Binger band so I assume a mean reversion and on the day we know that the prediction premium the volatility is above the rolling standard deviation or 1.5 1.5 rolling standard deviations whenever we have those conditions met we are going into a short position else we're going into a long position whenever we have the opposite so both the daily signal and the intraday signal are minus one and else we have MP none and the axis would be equal to one as well and this would be our intraday signal to get into a long or a short position invalid syntax okay I forgot the com up here that's enough okay I'm not sure guys what's the problem here but all right I think I had forgotten this something you know okay now we have the intraday signal for each position my idea here is the following so whenever intraday whenever in the in a given day we have this signal the first trigger of that position we are going into a position and we'll be holding for the whole day so the first time it can be in 1000 but we can have another signal in the same day uh at 500 we don't care about the second one we are going into full position position on the first one and we are going to hold for the whole day in this return sign column whenever we have the first occurrence of the signal what do we want to do we want to forward feel until the end of the day the same position sign to do that we can use the following trick return sign would be first let's explore what we have I'm sorry let's explore what we have so far so the return sign is our intraday final signal uh wow okay so this one should be one all right so whenever we have to go into a long position we have long signal right here the return sign we can observe that this happened at uh in 2021 18th of October at 605 then we had another signal on the 23rd of October then on the 28th on the 29th etc etc so in some days we may have only one signal but in some days we may have two or more we don't care about the second or the third signal we care only about the first one so at this point in time we had the signal and on the next bar so on the next 5 minutes on the open price we are going to go into a position and we want to hold it until the end of the day at the end of the day we close the position an easy way to calculate the strategy return would be to use just the vectorizer return right so we are going to forward f for every 5 minutes after the entry time we're going to forward f with the same sign the return sign column how do we do that final DF do group by pandas grouper we are going to group for every day we are going to group on a daily basis we are going to select the return sign and now we'll use the transform method and because we have supplied one or MP9 we are able to use a transform method and do Lambda X forward F and this would now forward F this value until the end of the day for every day where we have a Val it will be done by the first occurrence of the Val after we run that we can now select where the return is equal to one where we have the entry signal and as you can see now we have the first signal at 605 6010 615 Etc ET until the end of the day we can now calculate the 5 minutes returns shift them backwards with one row for every day and multiply the return sign column by the signal intraday let's do that right now that would be The Following return percent change and we would like to get the forward return right so forward return and this would be equal to our return column shifted we don't need to apply Group by daily here because our data is continuous it's 5 minutes data but it's uh from the beginning of the day until the end of the day it is continuous for every day including the weekends and that's why we we can just calculate the 5 minute return and then shift it backwards with one row and our final strategy return will be the following so strategy return would be equal to final data frame forward return multiplied by the return sign that would be our strategy return but it will be on the 5 minute level so we want to get the daily strategy return so we'll calculate daily return data frame and it would be the final data frame do groupby we're grouping on the daily basis so for every day we're going to group by select the strategy return column and just Summit after we have the daily return data frame we can move on to the final step and we'll just calculate the strategy cumulative return that would be equal to expanding window so numi expand log and then cumulate sum and we just substract one in the end and we can plot the strategy return we set title to be intraday strategy returns we fix the yaxis uh values to be percentages again so set major format to be from anti percent format one the Y label we set it to return and we're pretty much ready with our graph and this is our strategy G as you can see we have an inaday uh strategy we may not trade for quite some time up here we haven't trade for almost six months but then we had some trades and yeah whenever we have uh the match of the daily signal and the intraday signal that's when we are going into trades and we have the returns that's a strategy using the machine learning model the gar model to predict one day ahead volatility in combination with simulated intraday data and with that we're finishing the third project and I really really hope you've enjoyed it guys you can find the data and the notebook Down Below in the link if you're interested to learn more how to back test Quant strategies how to do algorithmic trading there will be a link in the description ion to my website where you can learn more about algorithmic trading and quad trading with python