learn about event driven architecture with this comprehensive course event driven architecture is a software design pattern where the flow of the program is determined by events such as user actions sensor outputs or messages passing between processes join Matt Marts and Matt Morgan as they unravel the secrets of transforming traditional apps into Cutting Edge eventdriven powerhouses Welcome to our Deep dive into event driven architecture on AWS in this course we'll be going over what eventdriven architecture is along with migrating an example Inventory management app to event driven architecture we also have a companion blog post that is linked throughout the course at mars. codde I'm Matt Marts I've been an AWS Community Builder since the first cohort in October of 2020 the AWS Community Builder program is an incred incredible platform for networking with AWS professionals both inside and outside of AWS along with a few other perks if that sounds interesting to you their application cycle tends to open at the beginning of the calendar year so keep an eye out for that I'm also a principal software architect at definitive at definitive we have an eventdriven architecture backed IOS app called Savvy spin smart that helps people save money in this course we'll dive into HandsOn coding for event driven architecture using AWS cdk if you're new to cdk you might find my cdk crash course with free code Camp particularly useful for deeper insights into Dev tools event driven architecture and server lless you could also check out my blog at matt. mars. codes where I share Advanced knowledge and tips uh thanks Matt I'm also Matt I'm Matt Morgan I'm also AWS Community Builder joined at the same time as Matt Mart I've been in the program for about three years uh I'm currently employed as a director of engineering at comold which is a multi Channel live selling platform uh I've done a lot of writing uh I coauthored a book called the typescript workshop and uh I have a lot of other blog posts and things and everything is linked uh as well as some talks linked from my website which is at Matt morgan. Cloud over the course of this video we'll take a look at Eda Basics such as terminology item potency and why you need to car about it and synchronous and asynchronous response models then we'll build an Eda app using cdk making sure to add in observability and things like that and we'll wrap the course up by going over some Advanced practices such as considerations for how to format your events along with some neat tools that AWS provides so to help demonstrate the value of event rate architecture we came up with a little sample application uh and a problem to solve in the application so so the application is a warehouse Inventory management system called whims because we've got a warehouse full of McGuffin and the McGuffin is a very popular item and so we need software to help us manage uh things like Inventory management uh fulfillment uh Payment Processing all those kinds of things whims is was V1 is built as uh a microservice uh rest API systems everything is a synchronous rest API call um and this is uh this is fine at a smaller scale but as as uh the system begins to grow up we find that um some of the uh some of the Lambda functions inside the architecture uh are doing too many different things and we start to see some some weaknesses in the architecture so we're going to go into um into this application get a little bit deeper into it and uh examine some of those problems and then examine the solution uh that we solved with event DP so let's look ahead at what we're going to be building by the end of this course uh what we want to do is we want to take our a tightly coupled uh entirely rest based microservice and change it into an Aventure an architecture where uh things happen are eventually consistent and have a single responsibility for component uh the way this will work is that um when an order is submitted via the API Gateway that's going to be persisted in a dynb table and then we'll use Dynamo DB streams and event Bridge pipes in order to transform that to transform that event uh and then uh and then emit it across event bridge where we can create rules to capture the events and then trigger other things so in the diagram you can see that we're triggering a number of things off the event Bridge rule uh we can have our low inventory warning there uh we can eventually build an external fulfillment system off of that we have event logs and we're going to trigger a step function that is going to handle things like adjusting inventory and processing payments and we know that one of the weaknesses of our payment system is that we sometimes get rate limited uh by the payment system so we're going to use sqs to slow down uh the rate of uh events going into the payment system so that everything can be processed in due time a few reminders before we get started there's a companion blog post that reinforces what we'll be going over here the video is split up into chapters and we'll have timestamped links in the description and the sections are colorcoded the intro and outro are blue the Eda basic section will be green the demo will be orange and the advanced or best practices section will be purple now that that's out of the way let's get some backend on what event driven architecture is Eric Johnson is a renowned principal developer Advocate at AWS he's widely recognized for his expertise and insightful talks on a venten architecture in his various presentations Eric distills Eda to its core and he simp simplifies it brilliantly as something happens and then you react this succinct phrase captures the essence of event driven architecture but what does it mean in practice it's helpful to think of in terms of our daily experiences as humans we're surrounded by a continuous stream of events these could be anything from a phone notification to a sudden change in the weather interestingly we don't respond to every single event we selectively react based on relevancy urgency or our current focus and this selective reaction is a key aspect of event driven architecture it's about systems intelligently responding to specific events events that matter in the realm of software and Cloud architecture this translates to applications and services reacting to certain triggers or changes in the environment just like us these systems are designed to respond selectively ensuring efficiency and relevance in their operations Eric Johnson's analogy beautifully aligns with this principle making Eda not just a technical concept but a relatable natural process so why do you think Eda is becoming so popular well I think that uh you know in many of the tools that we like to work with increase developer productivity when you increase developer productivity uh you build more and when you build more you end up with more features and when you end up with more features then uh you start complexity really starts to creep into the system uh and um we've all seen like um you know a thousand functions that try to do way too many things and uh are really hard to to modify so I think it's natural to say um that my system is getting really big and complex so I want to start to make it into a distributed system I want to start to to break Services out I want to start having things happen uh when something else happens and decouple uh these services in a way that uh makes it easier to build a wide broad system that has lots of complexity but that the complexity can be siloed and contained within Services yeah that plus like fa tolerance and you get a highly reactive system that it's easy to extend like as a you can throw an intern at a problem and say Hey you know add this feature to it you don't have to worry about the intern breaking the entire monolith Lambda that's handling your order system right yeah I mean if if the intern is breaking it that's probably my fault anyway for um asking someone like that but but yeah I mean it's um I think that isolating failure to one service as opposed to my whole system Falls over is a really really important design consideration I also think that um you know we we find uh opportunities where we want systems to scale independently uh one system uh you know maybe uh can't take too much traffic and another system can can go very quickly we have different levels of importance on on some of the capabilities that we do some some some capabilties we might say oh eventually consistency is fine like that can happen overnight or you know it can happen later other things we may need an immediate response it's very hard to build systems that way when everything is sort of contained within one function everything just has to kind of happen see synchronously one after another but if you can emit an event that says uh you know something that an order has come in or a new customer signed up or a payment is whatever it is and and then uh hook into that and say okay and then I want this to happen uh that can be a really powerful way to build and the other thing that I think um is really important here is that a lot of us who've been in the business for a while have seen systems that sort of start with you know lots of rest interactions and things like that and then eventually we say we can't do everything inside of uh one rest call so we need something that happens some kind of process that will happen offline or in the background and often the solution to that has been CW so like every five minutes a process wakes up and says do I have anything to do and maybe it has to query a database to do that uh or maybe there's you know some other way that that it can do that and and that kind of system can be um you know it's first of all if uh if it wakes up and says any work to do right now oh there isn't and then a job comes in right after that it's going to wait until the next interval so it's slower uh also it's frequently waking up and saying do I have any work to do and finding out the answer is no so it's inefficient uh if you can instead say uh here's an event react to the event and then have a system that's kind of standing by ready to do that uh you're going to be in a much better place and and the tools that we have now to do that build that kind of architecture uh things like Lambda step functions event Bridge those are great tools and they do a really good job for it so by switching to from like a cron based server full like Legacy PHP system running once a day or something like that to more of an eventdriven architecture it enables your teams to more quickly react to things and actually process data closer to On Demand that's right yeah and and um I think that um part of this is is that it's important to embrace eventual consistency and to say that things are not happening within that um you know the the 29 second API Gateway uh timeout or or um whatever it is you're working with um but on the other hand if you build uh event driven architecture in a really good way that you can often find that things are happening very quickly uh and and it almost seems like it's happening in real time yeah that's a good point and I don't think we haven't gone over eventual consistency yet in this course and maybe maybe editor match should go back and add some slides on it but uh what what is involved in getting into that eventual consistency mindset you know I don't think I think I think it's just just thinking about it really because um because the web is already prepped us for that in fact even before the web was um such a big part of our Lives we were um you know it was part of our lot it was part of um it was part of culture it it was something it was understood I'm waiting for a check to clear for example right uh I received a payment but that payment is eventually consistent and the funds are not available for me to use I mean that's that's been uh you know around longer than I have uh so you know and I think that um I think payments are a great example of that but but I also feel like if you just understand that that the web is by by its nature eventually consistent of course you can you can contrive of of uh uh conditions where uh you don't want that eventual consistency and there are tools for that too but but as by taking uh eventual consistency I know you we're also talking about item potency here which I think I think kind of goes along with that uh and saying that that's the default starting place that's where I want to be uh when I'm designing my system and anything uh that uh isn't uh living up to those principles is an exception do you find it difficult to maybe do you have to push back on maybe product people about the whole eventual consistency aspect I've never really had a problem with that um I think that uh I I I don't I don't want to I don't want to beat up on devs at all really uh because devs are great people but most of the time in my career I found that the reason that a system is uh was built in immediately consistent way it's just because of we just built it as rest and then that became of the expectation um when I when I go and say um you know hey we're going to put a spinner on the page and then then poll until this report is ready or something like that to a product person I've never had any push back on that I I think that um you know as I said in the first place uh it's it's very normal for uh many things in our lives to have that kind of eventual consistency mailing a letter I mean like uh you know that's that's been around for hundreds of years and uh and everyone's had that expectation and I think that um you know that that should just be part of our uh expectation when it comes to development too great all right let's uh let's move on and start talking about uh what an actual event is and item potency and all those things we just discussed all right thanks Matt to fully grasp eventdriven architecture it's essential to understand its core components so let's dive into four key terms that are fundamental to event Ren architecture first up an event you can think of an event as a record of something that has already happened it's a historical fact immutable and unchangeable an event is not about the current state of the system but rather a specific occurrence or action that has taken place in the past next we have a producer a producer is the source of events it generates and sends out events to signify that something has happened this could be anything from a user action like clicking a button to a system change like a completed transaction now let's talk about the consumer a consumer is the recipient of events it's designed to react or respond to the event it receives this reaction could be anything from updating a database sending a notification or triggering a new process or workflow finally we have the channel this is the conduit through which events are transmitted from producer users to Consumers the channel ensures that events reach their intended destinations and it's the infrastructure that supports the flow and management of events acting like a bridge between producers and consumers now let's unpack the concept of item potency in event driven architecture item potency ensures that we that even if the same event is processed multiple times it won't lead to duplicate or unintended effects on the system this characteristic is crucial for preventing creation of duplicate records or actions which can occur in complex systems item potency also enhances the overall reliability of the system by ensuring consistent outcomes regardless of the number of times an event is processed typically item potency is implemented using unique identifiers for each event ensuring each event is recognized and processed only once in a meaningful way a real world analogy for item potent systems is pressing an elevator button once you press an elevator button once you press an unlit elevator button it changes its state if you press it a second time it doesn't unpress itself nor can it do anything to speed the system up so building on our understanding of Eda terminology let's now look at five key response models these models dictate how systems communicate and handle Events first first we'll go through the synchronous model familiar in traditional apis where responses are immediate and direct and essential for realtime interactions then the async Q where events are queed for later processing allowing the producer to move on without waiting for the consumer's response the broadcast model disseminates events to multiple consumers at once ideal for widespread event notification next the async bus Which acts like a central Highway for events supporting flexible multipoint Communication in complex systems and finally we have the async router which intelligently directs events based on specific rules inside of the producer's code ensuring they reach the appropriate destination the synchronous response model is more like a traditional API interaction a sender emits a request which is received and process by the receiver which then sends the response back to the sender this model is widely used and understood in traditional application architectures making it a familiar choice for many developers it allows for immediate error detection as the sender can quickly know if something goes wrong thanks to the immediate response the direct back and forth communication typically ensures low latency which is crucial for realtime processing needs but in case in cases of long processing times there's a risk of timeout which can disrupt the flow of information as the system scales managing and balancing the load across servers can become increasingly complex this model often leads to tight coupling making the system Le less flexible and more prone to failure if any single component fails ensuring that operations can safely can be safely repeated without Crea in duplicates falls on the receiver adding complexity to its design a very common channel for synchronous response model is the API Gateway with an HTTP connection this is a pivotal component in managing synchronous interactions API Gateway seamlessly integrates with various AWS services and it supports multiple authorization mechanisms because of this even in the subsequent asyn response models that will go through more often than not you'll still be proxying those requests via API Gateway because of the multiple authorization mechanisms and the service Integrations that it provides for synchronous responses the channel also allows for request and response Transformations this feature is crucial for adapting and formatting the data as needed integration with Cloud watch also enables realtime monitoring a critical as aspect to note is that the channel requires responses within 30 seconds this constraint is essential for maintaining system efficiency and reliability while the synchronous model offers familiarity and immediate feedback its limitations and scalability and flexibility and complexity are important considerations in your system design to put this into the context of our upcoming demo app let's talk about a real world scenario imagine a customer walks into a store to check if our McGuffin is in stock this customer represents the sender in our model the store worker akin to the API Gateway is approached with the query this worker has the responsibility to provide immediate information just as the API Gateway handles and routes the requests so the warehouse worker checks the inventory system and provides an immediate response to the customer about the items availability this interaction is synchronous mirroring the direct and immediately response expected in the synchronous model however if the warehouse is huge or the worker is busy it may take time to get the response highlighting the synchronous model's potential for timeouts and real world scenarios next let's discuss the async Q response model here a producer sends messages to a que and receives an acknowledgement in return separately a consumer pulls this Q to ret retrieve messages also acknowledging once the messages are processed this model allows the receiver to control the rate at which it processes messages preventing overload it accommodates processes requiring longer compute times as messages can wait in the queue in the case of failures the model also facilitates recovery as messages remain in the queue until successfully processed services like Amazon sqs can offer item potency preventing duplicate processing of messages too but the model only provides acknowledgement of responses which can limit the information available about message processing and while it Rec while it enables recovery the time to recover can be significant depending on the Q size and processing speed typically a single consumer processes the que which can be a bottleneck in high volume scenarios this could be a single Lambda function spread across multiple parallel invocations but you would not have different Lambda functions reading from a que despite being asynchronous there's still a degree of tight coupling as the consumer is directly dependent on the q's structure and format it has to be created and exist in the context of async Q's our main channel is sqs or simple Q service simple Q service plays a crucial role in scaling and fortifying the architecture efficiently managing message q and ensuring smooth processing however with standard cues be mindful of potential duplication and ordering issues where messages might not always be processed in the order they were sent sqs uses a polling model which can introduce latency as consumers periodically check for messages instead of receiving them in real time also there's a size limitation to consider each message in sqs can be up to 256 kiloby monitoring sqs is relying on cloudwatch which provides insights into the q's performance and message flow now talking about a real world analogy let's imagine our fictional apps Warehouse has to send out a McGuffin the warehouse producer sends the McGuffin the message to the shipping company the Q from there the shipping company manages the storage and eventual delivery to the destination it receives one McGuffin and delivers one McGuffin it doesn't copy the McGuffin it's one one next we'll explore the broadcast response model also known as publish subscribe PBS up the arc diagram here illustrates a producer sending a message to a topic and receiving an acknowledgement in return this message is then distributed from the topic to two separate consumers showcasing the multi receiver capability of this model in this case one message is copied and sent to each consumer in advantage of this model is its ability to handle processes that require extended computation without burdening the producer the model excels in scenarios where one event needs to reach multiple consumers as demonstrated in the diagram it supports a variety of communication protocols offering adaptability to diverse system requirements a challenge in this model though is the potential for receiver failure if a consumer fails it might miss out on important broadcasted events despite its broadcast nature there's a level of tight coupling as consumers need to to subscribe to specific topics to receive messages ensuring that a are processed uniquely and not duplicated falls on the consumer adding an extra layer of complexity in the broadcast response model our main channel is SNS or simple notification service SNS excels in one Dem many messaging enabling a single message to reach a wide array of recipients simultaneously it operates on realtime push base notifications ensuring immediate dissemination of information SNS uses a topic based Pub sub model where messages are published to topics and then pushed to subscribers the service supports a variety of subscribers in communication protocols catering to different system needs however it's important to note that SNS does not inherently provide message ordering or duplication handling which might require additional handling in the consumer's logic for a real world analogy consider our McGuffin Warehouse Inventory management suppose the warehouse must quickly update various departments sales shipping customer service about changes in the MCU and stock levels when a new match of McGuffin arrives or stock levels change a single announcement is broadcasted to all relevant departments for event bus routing our architecture diagram has a producer that emits two typ types of messages new me new user and updated user these messages are sent to the event Bridge bus where rules on each of the types forward them to their type specific consumers there could potentially be a third rule that forwards both event types to another Lambda similar to SNS this is a many to many situation where many types of events can go on the bus and each of those events can be D directed to multiple consumers a key advantage of this model is its loose coupling producers and consumers operate independently enhancing system flexibility and maintainability it's well suited for processes requiring extended computation as message processing is not tied to the producer's timeline like other async response models one limitation is that the response model primarily provides acknowledgement responses which might not offer detailed feedback on the message processing recovery from failures or message losses is managed by the consumer adding complexity to its design and ensuring item potency or preventing duplicate processing is also up to the receiver which can be challenging in complex event scenarios for the async bust response model our primary channel is Amazon event Bridge event Bridge excels in handling complex event routing directing specific events to the right destinations based on detailed criteria it offers seamless integration with a wide range of AWS Services enhancing the model's connectivity and utility users can create custom event buses tailored to their specific application events offering greater control and customization the schema registry feature AIDS in managing event models ensuring consistency and Clarity in event handling one notable limitation is the lack of direct support for message queuing within eventbridge which may require additional considerations for message handling a real world analogy for the async bus is very similar to the broadcast scenario for stock levels event Bridge routes specific events based on Def defined criteria when stock levels change in event could trigger updates to the inventory team sales department but not the sh shipping dock because it didn't hit their the threshold in their rule each department is a consumer that receives events based on their own set of rules so it's a much more flexible system than SNS finally we have the async router a multimodel event routing approach the architecture diagram here illustrates a producer that's not just emitting events but also handling the routing logic it routes events to the various response models we've been discussing based on its own internal business logic a significant advantage of this model is the greater control it provides over the event routing the producer directly manages where and how events are routed allowing for specific event handling having said that its Advantage is also a sign significant disadvantage due to the extra complexity that you have to add to the producer's code base the model also introduced is tight coupling as the producer is in intricately linked with the routing logic this type of system could be exceptionally difficult to maintain so Matt after having gone through those the five different response models like I don't know we've already discussed going from kind of the the synchronous mindset we've talked about the flexibility moving to Adventure bin architecture like should teams just use event bridge by default for everything as their starting point in Eda or what what do you think uh well I think event Bridge is a fantastic service and I'm a huge fan of uh it's many capabilities uh I I was a little bit late to the pipes game actually and and I'm now I'm just really impressed with what you can do with pipes and I can't wait to see what else you can do with that in the future uh that said uh there are number of other tools that I think are really useful here too and um so to say Let's uh throw everything over event Bridge I think sometimes um what's so so definitely do use a vent Bridge people should use a vent Bridge uh when building a vent driven architecture uh it's right there in the name but but more than that it's it's a really great service uh I think that that um there needs to be room here for step functions as well uh step functions can be a target for an event Bridge pipe or just just a rule uh and uh but sometimes you've got things that you want to happen in a certain sequence and if you're just firing Events off uh you're just saying here are here here's an event and then I've got seven different listeners and uh you know all those listeners are going to do their own work right but maybe I want to know when are all those listeners done right well they can all fire events too but that doesn't it doesn't kind of come back together and say okay everyone's done here is the uh you know uh top level completionist I would have to build something to do that right I would have to track all of those events coming back in the end and then saying okay now I'm finally done I finally reached the end of this uh it's much easier to do something like that with step function so you can fire the event you can have a step function that says okay I'm going to do all these different things uh but I'm going to do it within a container and then I'm going to understand that that job has finished and that then I can have my reporting I can have my final status whatever whatever needs to be done at the end of that so I think those two things uh they work really great together so I always think about event Bridge as being like if I if if system a needs to talk to system B uh or something like that or system a just needs to say hey something happened who cares and then you can have rules that that pick that up and do things with that but if you've got uh I need these seven things to happen and I need them to happen in a certain sequence I wouldn't just build that over vent bridge I would definitely leverage step functions for yeah that's a good point and that reminds me of uh like a Blog series or a blog post by yanu the burning monk about orchestration versus choreography which basically covers the whole like should you use event Bridge or should you use step functions to depending on the task yeah that's a great look at the question I think you can mix and match the two and I've written about this too and I think Yan has also but uh like you can combine the two by adding put event notifications as part of your stuff step functions flow too so you understand as certain sections of the step function completes you can maybe trigger things yeah I don't I don't think we're getting into that in this course uh maybe a future one but but uh using past tokens and stuff functions and sending uh those out of vent bridge and then eventually getting uh a response back to restart your state machine uh it's a great pattern I love using that yeah awesome okay so uh our next section we're going to switch over to demo mode so we're going to go over uh migrating our Legacy app into uh cdk well not into cdk into event driven architecture all right let's do it so now that we know a bit more about event driven architecture Basics let's get into the actual demonstration to do that we're going to be using AWS cdk AWS cdk is a cloud development kit it's an opensource software it it's where you can write imperative code to to generate declarative cloud formation templates now we just learned about the concept of item potency and cdk is item potent the same inputs that you give to a cdk stack will yield the same exact cloud formation yl if you somehow get into a scenario where it's not and it's not deterministic then you're doing something wrong with cdk so you should take a look at that um if you'd like to if you haven't been exposed to cdk before I've made the cdk crash course for free code Camp before and linked at mar. codv cdk all right so let's get back to our whim system and look at uh the version one of that system and the kinds of problems that we had with it so uh so it is a API Gateway Lambda Dynamo DB system uh we got a couple of Lambda functions here just just two for for demo purposes but in the real world example you probably have more uh and um we got to check inventory function which is you know pretty straightforward uh that one's probably fine but but our um our create order lamba function um it has a few problems and the problems it um really is is that it's trying to do too much it's trying to do too much and it's got um of course Lambda has a 15 minute timeout nobody wants to wait 15 minutes but we can anyway because um uh API Gateway has a 29se second timeout so that means that it has to do all of its work within window uh or that the user is going to get a like a 504 error back so um it has a couple of things that it's doing uh in a Dynamo DB table it's saving in order and it's updating the inventory uh and those those ones are probably okay um we'll come back to that in a second like you can probably do do uh two um updates to uh um calls to Dynamo DB inside the Lambda function uh but the the process payment one is one one that we're really concerned about so let's say our system you know it it it does this thing it is um uh we are calling our payment microservice which is a separate service that you know maybe another team maintains and uh we know we understand that that's an eventually consistent thing it's not going to do all the the the payment processing uh within the uh uh request that the Lambda function is making to it but uh it is uh let's say it's uh not a a perfect uh microservice it's a little bit slow sometimes uh and sometimes we get rate limited sometimes it just can't handle that the uh amount of traffic that we send to it so this is a really big problem now because our Lambda function is going to try to call the payment processing system and sometimes it'll get rate limited so what can we do we could we can do some retries within the Lambda function we have a very limited time window for those retries and if we can't complete the retries within the 30 seconds then what's going to happen is that that our user is going to get an error and they're not going to know that this was successful okay so that's a big problem we have to fix that problem there's a lot of different ways to fix that problem uh but it is a it is a big problem the other problem that we're facing here is that um we're saving the order we're updating inventory and then we get a requirement that you know uh we didn't restock our inventory in time uh because we did know how low it gotten because we had you know just a door buster of a day of um selling the guffins uh so what can we do we need some kind of notification system that lets us know when our inventory gets low well one way we could do that is we could say um after we do the adjustment we could check the current level against whatever you know our water mark is for low inventory and if it were below that we could uh post a message to SNS that's going to send us an email to let us know that our inventory is low that's um we it could work but our Lambda function was doing three things now it's going to be doing five things uh which just means that there's more things that could go wrong but the other problem that H is what if our inventory gets adjusted by something else maybe there's like an admin override or something like that or maybe uh there's other sales channels or or something like that and if the logic to send that notification that we're low on inventor is only in this function then any of those other channels that that uh take our inventory to that low level are not going to send the notification so so basically that's just really not a good solution and then finally if you see the bottom of the the diagram here uh we're thinking about building this fulfillment system um but how do we know when there's a new order uh based on the system we're going to send another uh request out um you know to notify that we're gonna have a six thing that this Lambda function does we're we're kind of um collapsing under the weight of all the different things that we're expecting to do here and we really need to start sending us some events because having an event that says hey there's a new order you want to do something with that is going to be much more robust architecture all right so let's look at the uh at at the adventure of inversion of this so we made a few changes here first of all um and we'll see this when we look at the code uh we've gotten rid of some Lambda functions we brought in another Lambda function because you know we kind of needed to um we we switched to some uh direct Integrations between API Gateway and Dynamo DB and that's really because uh the uh putting a Dynamo put statement or or a a get item statement or something like that in um in an API Gateway uh template is really pretty easy and it's not very much different from putting it in land that you avoid the cold starts uh and um you know we we're we're basically stripping down our system to make it much simpler some people may not be comfortable with that you can still use Lambda it's not like a bad solution or something like that but here we're just trying to strip it down to the the uh bare minimum now what we're doing is when we create an order the first thing that we do is we're going to persist that in the database this is a little bit flipped around from the way um many systems work which is which is that okay an order comes in uh we have a bunch of business logic that we need to execute against it and then we finally save it uh in a table somewhere uh here what we're going to do is we're going to say Okay order comes in we immediately save it so we're basically capturing ing the uh the user intent in in what needs to happen later on and then we're going to emit events based on that right we don't have any any additional logic at that stage we just say store it send the event okay so Dynamo DB streams are are a great solution to this and a lot of this architecture is based off that if you happen to be using Dynamo DB uh it's a good way to go um if you're not using Dynamo DB you know you can emit events in many other ways this is just a great way to get started so those who have used Dynamo DB streams before know that uh it's recommended that you only have two consumers of a stream um and that has to do with polling and and some some internal concerns there uh a good solution to uh or I guess a good pairing with Dynamo D streams is to use a vent Bridge pipes because you can subscribe the pipe to the stream and then from there you can you can the the data that's coming through you can you can transform it in different ways you can do that with a Lambda function or a step function you can do that with um just a a very simple mapping template if your need isn't that complex and then uh that can be reposted to event bridge and now you can create rules to send it different places so in our architecture uh we have the stream coming through we're going to enrich the the data in that stream so basically what we're going to do is we're going to um unmarshal the uh Dynamo DB tax to make it a little bit easier to work with later in the uh process um and then we're going to repost that event back to to uh event Bridge from there we can create a rule that is going to give us that low inventory notification we'll show how that works a little bit later it's very cool I think uh we're going to store event logs this is kind of a new thing that wasn't in the old system but anytime we have a change we're just going to throw that into cloudwatch so that we can uh see what are all the different change events that are coming through uh and of course Very importantly here we still need to adjust inventory and we still need to process the payment we're going to use a step function for that uh because we want both of those things to happen and they're kind of related U you know pertinent to the order so uh the uh step function is going to directly uh again with with just a direct integration it's going to make the uh inventory adjustment um into Dynamo DB you can do that with a condition statement to make sure we don't end up with negative inventory or something like that and um and notice that that feeds back in we get another change uh event which is going to then uh be um encounter that rule that will notify us if their inventory got too low the last piece of this uh is that we have put an uh sqsq in between uh our order process and the payment processing so that uh if uh if we get rate limited uh the message should stay on the Queue and and then then getss consumed a little bit later uh that's that's done by means of another event Bridge pipe one thing I'll mention is that that particular piece of this architecture of of saying my Downstream service might be rate limiting me so let me uh put sqs and aent Bridge pipes in between that it's not something I've done production before we might find that that doesn't work as well as we thought it might we might need a more robust solution that could be swapped for a step function later on and the rest of the architecture doesn't have to change so that's that's a really great thing about uh uh event driven architecture is that you can uh replace components that aren't quite living up to what you expected them to do and the rest of the the architecture doesn't have to change on this diagram just wanted to show uh that um you know the motion here where we had a Lambda function that was doing uh check inventory in the V1 architecture in V2 uh we're doing that as a direct integration it's almost the same thing uh but that save order it was doing multiple things before now we have single responsibility components so we have a direct integration that that processs the order in our table uh and then we've got the Dynamo stream that eventually uh eventually but very quickly actually uh uh will trigger that stuff function that is going to do the inventory adjustment um that we've got a um some new features here we got we got our low inventory notification that's very easy to implement uh and uh we can build that external a fulfillment system based on the um on the event we've got event logs uh and we've got that cue for our payment processing and while we're at it we could think about additional events that we could add to this system what if we wanted a high inventory warning that that says that we've overstocked and our sales team needs to get active what if we want an out of stock uh notific if that that then can can flag that in our catalog is uh on back order there there's lots of different uh things that we could conceive of and they're all going to be fairly easy to implement and um and and fit this model all right everybody let's look at some code so what I have here is the uh whims V1 system that's the the rest API based system uh and um this is the cdk code so I'm just going to walk through this really quickly there's not a lot here um and you'll be able to do the GitHub later on I suggest you check that out might a little bit different by then but it's going to be mostly the same thing so uh we've got a table construct we're going to create uh our Dynamo DB table uh that's that's pretty straightforward um i' I've uh winged and moaned a little bit about the payments API uh for demo purposes what I did is I just created a lock integration in a separate rest API uh that always returns a 200 uh actually that seems like a pretty nice service but it doesn't actually do a lot I I added some throttling to that just for fun uh so we can Benchmark that um then I created a couple Lambda functions there's the one to get inventory and the one to uh create orders we look at the source for those get inventory um is super simple um so I'm using the uh Dynamo DB document client and um I'm just going and getting a specific item so there's one item in the table uh for each inventory now probably I would do something a little we probably have more than one thing that we can sell uh and there'd probably be some kind of uh query pram or something like that that includes the the um skew or something and we we use that to uh actually generate a query but in this case just to keep it really simple there's exactly one thing and so we're just going to go get that and it's going to tell us what that inventory number is all right uh create ORD is a little bit more complex um we still got Dynamo DB document client here and uh first of all we're going to toss that guy in our table uh we're going to generate a uh um partition key and sort key for that we're also going to set the status of it to pending uh you won't actually see that status change in this demo but you can imagine that some other system later on would update that then we also have to have an update command uh that's going to go and uh reduce the inventory for that um and finally we're going to make a uh fetch call uh using native node fetch to uh post our order over to the payment system and uh and and get that payment process started then we return order created and uh everybody's happy right uh the only other things that you would see in this stack are uh just a couple really quick things we've got our our rest API for the orders API uh we've got a couple of resources there to connect to the Lambda integration and then finally there's this AWS custom resource which is uh is just to seed the database so that when I deploy my stack I'm going to have it preceded with that LX model McGuffin uh with a quantity of 1 million items all right so let's walk through the modern architecture uh it's going to be pretty different here um first of all what I did is um the V1 version of whims uh had everything just inside of that whims stack class uh our V2 version has a few more things going on and so what I did is I I built some L3 constructs uh L3 constructs um combine uh various things uh other resources that I want to create uh and and kind of put them behind my own uh vision of of what the application actually looks like so the the so those are just really in this case uh I'm not anticipating any reuse it's just sort of an organizational uh construct so I still left the um the table here I could have created uh an L3 construct just for the table but it would really be exactly the same as this L2 construct so it's just fine to do that the only thing I might uh save a few lines here of uh of config hardly worth it as um you can see that I've only got a little over 40 lines of uh total code here so let's look at some of these individual um sections of it the first is CDC which is uh course stands for change data capture and we've got several things happening here um I'm declaring a node.js function uh which is called CDC enrichment let's just check that one out uh really quickly and um basically what this is doing is it's um we're borrowing the um unmarshal utility from util Dynamo DB and we're using that to uh to transform our Dynamo DB record into something that's a little bit easy into pure Json because Dynamo DB always specifies the type uh in a way that is a little obtuse to work with later on so that's that's really all this is doing we could do other things here if we wanted to this is just code and we could do anything that we want uh um we are uh adding a meta tag that that lets us know what process this um we're adding this data key and so then we're kind of putting this in in a a format that's going to make sense to us in a schema that will uh be happening later on so that's um that's the function that is going to be enriching what happens in the pipe let's let's look at what the pipe is here so so um Matt for the enrichment function is that what's the output of that going to like what is so it's outputting a list right it's processing a bunch of Records what does that list end up becoming uh so the output of the enrichment function uh is sent to the Target of the pipe and for this specific pipe the target's event Bridge right so the the response list is going to end up being the details of all the events that will be going out that's right that's a really good point to make all right so let's go back and look at our pipe construct uh at the time that we're making this recording uh there is not an L2 pipe construct so we are forced to fall back to CFM we're not actually forc because there is an RFC for an L2 pipe construct it's still in uh RFC status uh you can use it uh I have used it it's it's kind of good uh I went with CFM pipe um just because it's not actually part of the cdk just yet uh and CFN pipe is okay to use you have to do a little bit more manual stuff for example I have to create a role explicitly uh because CFN pipe is going to want the Arn of that role just the thing that it does uh I added a bunch of log configuration here it's a little bit optional but it's really good especially when it comes to debugging as I had to do uh to do that um you can see that there's an enrichment key here um which that's going to expect uh the function Arn um the L2 construct would just take a function as an argument in this case because this is uh the uh corresponds directly to Cloud information it's going to want the Arn um we set the source here so the source is going to be that table stream uh this subscribes the pipe to the Dynamo DB stream uh which is configured here in our table V2 construct we're setting some parameters here a batch size of 10 which means that we'll we will not process more than 10 times uh 10 items at a time and our starting position will be the most recent change uh we are targeting this back to the same uh event bus which is going to be our default bus uh as discussed previously and uh we're adding some additional parameters that this will help us to Route the event when we create a rule for this our change data capture all it's doing is it's it's saying there's a stream of data coming in it could be uh a um a creation event it could be an update it could be a deletion uh and um we're going to use our Lambda function to enrich that just because the uh the uh transform capabilities and the vent Bridge pipes isn't really up to what we wanted to do for this uh for our purposes here and then we're going to send that back to to uh event bridge if we never created a rule for it it would just kind of never go anywhere from here but our other components can start to build those rules so let's look at those here we have the our order processor construct um we're going to grab that uh default event bu and um we're declaring a machine I kind of um there's a few different ways to do this in in cdk I I like to use the constructs uh and um so we've got a parallel construct here not much to say about it and then we've got an update uh Dynamo update item construct here now this is really not very different from uh using the SDK and typescript so I'm pretty comfortable doing it this could be a Lambda function that does this instead uh and um in this case uh this was actually missing from the V1 architecture so uh that's another little Improvement that you get here there's a condition expression here that makes sure that we're not uh uh committing more quantity than we actually have uh so this would fail if we had 99 stock and tried to sell 100 uh and um assuming that condition is successful then we are going to reduce uh the quantity uh our other Branch here is that is is basically we're just going to take that uh that message body that came in and we're going to replay that on SQ we're going to stick stick that in the queue whoever subscribes to that queue we don't know at this point uh we're just um sending it off and then uh how does this thing get triggered in the first place well here's the rule that does that now we're uh we've got that source and event type that we set uh in our CDC construct and here we are going to um uh create this rule uh make sure that we're only getting insert uh event types and that our comption key begins with customer uh and uh we could we could uh further refine that if we wanted to and then we're going to Target our state machine so the event now becomes the input to the state machine let's look at the our payments API so I moved that um I moved that mocked integration API here um as as well as the uh the throttle plan this is really unchanged uh just wanted to make sure that it had its own construct the payments process there is the um piece that's a little bit more interesting here uh because we got another CFN pipe uh with its own role and its own uh logging solution uh this is this is the queue that uh is going to um consume messages uh the messages go into the uh into the pipe uh but this pipe instead of replaying an event over event Bridge which actually come to think of it is probably the way I would like for uh our payment system to work but somebody else is working on that or nobody's working on that so it's payment system isn't going to change much uh in this cut so what we're going to do is we're going to Target that API Gateway uh with this so we can actually have our pipe connect sqs messages come off the queue and we we pull them in we're going to replay that to API Gateway and uh that way we can manage our our rate limit of couple more constructs here to go through uh our inventory monitor uh this one is is uh is really nice so all we have to do is is um we're grabbing our bus again uh it's just the default bus that uh already exists in the account we are um creating SNS topic with an email subscription so that our team can be notified when when this event goes out and then we're going to create another rule here but this rule has a numeric uh comparator so we say if our inventory is is less than or equal to 100 items 100 units uh then we're going to trigger this rule finally we've got uh this observability construct and um this creates another event rule uh event Bridge rule uh that is going to um store all of our events in uh a cloudwatch log group and it's going to create an event Bridge archive uh and um uh and schema registry for all the events that we're sending yes so I just want to um I've got the view here of my Dynamo DB table in my account where I've deployed the whim system and I just want to kind of give a really quick demo here of how it can work so um uh you can see that I have uh preseeded my database I've got I've got my LX McGuffin uh ready to roll and I've got a thousand uh items in stock over here is um my so here over here in Postman I'm um going to uh create a post uh to the orders API in order to create uh a new order so I've got set my customer ID to 123 and the quantity is three and presumably there' be some more interesting information here um so if I send that over and then I flip back to my table you can see that um I have uh registered my order and my quantity has already been decremented so so it was that quickly it was even faster than that that that um my state machine ran and all the other pieces uh uh happened and uh so it almost seems like it's uh probably even as fast as it was before so what happened behind the scenes well one of the things we know it was uh that my my step function ran um and so this is this is the step function you can see that it's um got uh parallel two branches in the parallel step here one of them is adjust inventory and the other one's in key payment uh they were both successful um what I think is really cool about this is is is look at how fast uh this thing was it was uh 136 milliseconds uh from start to end uh and this thing just just really blazed uh it it got the work done very very quickly and um because this is an Express uh step machine step function uh that means that it's going to be very very cheap in fact really the um unless I'm I'm doing this quite a lot which would be a good problem to have let's face it because that means I'm selling quite a lot it's the the cost of running this is going to be negligible an xray uh we have a little bit of a problem here which I hope is is a problem that service teams ultimately solve for us and that is that uh you actually don't get to trace over a Dynamo DB stream so um in this case you can you can do traces over event Bridge um but the the stream kind of ends the trace so so we see the initial Trace here which is that our client has placed an order and that's going to hit the Dynamo DB we'd really love to see a line that comes from Dynamo DB over to the uh which really the stream and the pipe here is is the client we triggering the state machine and we can see that we're um uh doing another operation on our whims table uh we're putting something in sqs uh but there's obviously some missing pieces here so that means that uh really it uh in the end here we're going to have to kind of uh put together some uh some kind of additional observability elements that will let us know hey we had uh you know X orders come in and uh y ended up in this status and and Z ended up in this status because we don't get the full Trace here uh we do get good traces for you know if we want to uh uh do it dive down and uh look at okay just just that initial save or or something like that like if we start seeing error or something in this area this is a great tool for that but unfortunately it doesn't yet give us the whole end end view so Matt if I was a team implementing this what approach should we take to iteratively do it should we maybe start with the observability construct and the change data capture or is there a better approach so those are both uh you know the great thing about those constructs is that they're completely nondestructive right they're just adding additional information the observability construct is going to start registering events and and uh give you some additional logging and and the CDC right until you subscribe to that event and do something meaningful with it uh you could you could Implement that and then just see what it does and see how you like it and see whether it makes sense to you you data capture stuff uh independently of everything get this get those events flowing and then even skip the stuff and like automate the Fulfillment part or automate the low inventory notification part without even touching the existing V1 code and then absolutely or you spread it out and have multiple teams you know one adding the Fulfillment one adding the inventory another an intern you know adding the step function or whatever that's correct I guess calling out one more thing the the distributed nature the like decoupled nature of this if ultimately business comes back and doesn't want to know about low inv low inventory notifications for example you can just delete that construct and you don't have to go in and modify the original Lambda the order processing Lambda like that it's right right I mean I think I think that's um you know to to think of your overall application platform as a a series of components all of which are are replaceable is a great architectural mindset to have because it just makes it makes pivots it makes new requirements it makes you know maybe the the state of play has changed in some way uh and it's and you know you have to respond to uh new new business pressures and if your architecture is uh is modular enough then that's going to be a lot easier to do than if uh you've just got sort of like this uh you know sprawling classes and functions uh that are much harder to iterate on now let's move on to Advanced practices for event architecture proper event formatting is crucial for maintaining system integrity scalability and interoperability enforce strict backward compatibility in your event structures this approach reduces the risk of breaking changes and ensures smoother evolution of your systems keep your event properties consistent across all events consistency AIDS in predictability and eases integration across different parts of the system avoid optional properties in your event schemas ensuring that all properties are mandatory simplifies processing logic and reduces ambiguity ensure that event sources are consistent within a single code Repository this consistency is key to traceability and manageability of events different code repositories should not emit the same Event Source unique sources prevents conflicts and improves Clarity in event handling build a standard metadata object for all events this metadata should include essential information like the event type the Tim stamp and source providing a uniform context for each event place your actual event payload under a dedicated data object this separation ensures that the payload is clearly distinguishable from the metadata let's talk about what should be in the metadata of an event detail include item potency IDs in the metadata these unique identifiers ensure that each event is processed only once preventing duplicate processing and enhancing reliability it's essential to track where events originate you can use identifiers like the default Lambda environment variables the AWS lamed a function name or things like the step function execution ID and store those in the metadata object this tracking provides Clarity on the event journey and AIDS in debugging and monitoring for more insights into effective event payload patterns you could check out David Bo's blog post which offers valuable perspectives I also wrote a blog post not long ago delving into inferring architecture by using the metadata from event details a as an example of a well structured detail object The Meta object here includes incoming details like the account source and detail type as well as the function name and state machine or job identifiers this structure AIDS in tracking The Event Source and path under the data object we would place the actual payload of the event and this clear separation between the metadata and the payload simplifies processing and interpretation when producing events there are two primary types full and sparse understanding the differences in applications of each is key to efficient event management sparse events carry minimal information about what occurred they're lightweight and typically include just enough data to identify the event and its basic context the advantage of sparse events is their Simplicity and reduce load on the system making them efficient for high volume or realtime scenarios in contrast full events are more detailed including extra information that might minimize the need for subsequent queries while they can be more convenient by providing comprehensive data up front they can also increase the computation burden on the producer and can be heavier on the network there's a spectrum between full and sparse events the choice of which you to use uh depends on the specific requirements of your system such as response time Network bandwidth and data processing needs remember with extra information comes more risk more data can lead to increased complexity potential privacy concerns and a higher chance of data becoming stale or or irrelevant regardless of the type chosen it's crucial to avoid breaking changes in your event schema ensure backward compatibility to maintain system stability and reliability when designing events in event driven architecture it's crucial to consider the nature and handling of the data within your payloads as a general rule avoid including sensitive information in event payloads exposing such data can pose security risks and compliance issues if sensitive data must be included consider encrypting it at the field level this approach helps preserve the ability to perform pattern matching while safeguarding the sensitive data be mindful of payload limits for instance Amazon event Bridge has a payload limit of 256 kiloby if your event data exed ceeds this limit an alternative strategy is to upload the larger data to S3 and then emit an event containing the bucket and key combination this method not only circumvents the payload limit but also leverages the robust storage capabilities of S3 remember if you're using S3 buckets consumers of your events will need independent access permissions to read from these buckets ensuring appropriate access controls and permissions is critical to maintain security and functionality change data capture events are a pivotal aspect of event driven architecture particularly when dealing with database changes these events are produced in response to modifications in a database providing a realtime data stream reflecting these changes key to change data capture is creating a consistent event interface these interfaces typically include information like the columns that were changed and stringified representations of data values before and after the change the diffs such structured information helps in understanding the nature and impact of changes made to the database I've written some blog posts that can help with this on Aurora if you have an aurora mySQL database you could use binlog streaming as a source for your change data capture events or for MySQL or postgress you could also use the database migration service and pipe the events to Kinesis and from there go to event Bridge or wherever fortunately Dynamo DB is much simpler and you can just leverage Dynamo DB streams to create events these can be efficiently integrated with event Bridge pipes for seamless event handling to the right is is an example of a change data capture structure in my example I made the detail type CDC update and in the details data portion I included the information about the source of the change the database and table along with a list of columns changed and before and after attributes with stringified Json we use stringified Json for the before and after fields to keep the interface consistent if the event bus has schema registry enabled and the before and after Fields were not strings a new version in the schema registry would get created for every combination of field change but with the stringified format we have the information we need between the combination of columns changed and before and after and we maintain a a common interface I'll note that a CDC insert event would not have the before attribute and a CDC delete event would not have an after attribute they aren't optional in these cases because they are different detail types they're different interfaces hey editor Matt here I just want to point out that stringified before and after bodies aren't necessarily for everyone it's a consideration if you want to have maintained consistent interfaces but what it does do is it breaks event pattern matching inside of the Deltas so for example in our demo app we actually didn't stringify the data and we used the change values inside of the change data cap capture event to do the low inventory notification so that's an important consideration for how you want to manage your change data capture events and whether or not you care about the schema registry schema registry incrementing the version every time at the end of the day it's probably not a big deal I I did want to point it out event pattern filtering plays a critical role in efficiently routing and handling events event Bridge off highly flexible event pattern rules these can be these can include various patterns such as using a prefix suffix Wild Card anything but in numeric and others such versatility allows for precise event matching according to specific criteria best practices indicate that you should be cautious to avoid writing patterns that could create infinite Loops in event triggering another event in a cyclical manner that could lead uh to system overload and unpredict ible changes you also want to make sure your event patterns are as precise as possible vague patterns may lead to unintentional matching causing noise and inefficiencies in event processing always specify the Event Source and detail type in your filters this practice Narrows down the event scope ensuring only relevant events are processed include the account and region as filters for enhanced security too utilize Conta filters to further refine event selection focusing on specific event attributes or values within the event payload a schema registry can play an important role in managing and maintaining the structure of event data in event driven architectures it ensures a consistency Clarity and compatibility across different parts of the system start by defining clear and concise schemas at the beginning of your project early definition helps in establishing a strong foundation for your event your event data structures an Implement Version Control for your schemas as your system evolves Version Control ensures that changes are tracked and managed systematically regularly enforce schema validation this practice helps in Catching inconsistencies and errors early maintaining the Integrity of your data keep a detailed record of any changes made to your schemas proper documentation is crucial for understanding the evolution of your event structures and for troubleshooting Implement notifications for schema changes this keeps all the stakeholders informed about modifications ensuring that everyone is aligned with the latest schema updates and finally consider using tools like event catalog by David Bo for managing your event schemas the async API initiative is another valuable resource resource it provides tools and standards for managing asynchronous apis many AWS Services emit events to your account's default event bus and event Bridge these events can range from changes in service states to specific actions executed within the services you can create powerful automation within your AWS accounts tapping into these default events this enables realtime responsive actions based on service activities to effectively document and manage these previously I wrote a threepart blog series on event driven documentation where I tied into these AWS Service events to automatically update my org's documentation anytime a cloud information stack deployed with event Bridge rules or API gateways in event Bridge there are two types of buses the default bus that is included in every AWS account and custom named buses understanding their difference is key to effective event management the default bus is always available ensuring a foundation for loose coupling and better collaboration it receives AWS Service events automatically making it ideal for integration across multiple services and teams Custom Custom named buses offer more control tailored to specific requirements while they provide tighter Access Control they are more challenging to share making them better suited for single teams or projects with specific access needs while custom named buses have their place I recommend prioritizing the use of the default bus wherever possible for its EAS of collaboration and Broad integration capabilities event Bridge pipes is a feature in AWS that simplifies pointtopoint Integrations between event sources and targets it is designed to reduce the complexity and coding requirements in developing eventdriven architectures you can create a pipe to receive events from supported sources and optionally add filters for specific event processing further you can Define enrichments to enhance event Data before sending it to your chosen Target pipes support various Integrations including Lambda functions step functions and API calls allowing for versatile and dynamic event handling for example a pipe can link an Amazon sqs message CU to an AWS step function State machine with an event Bridge API destination for data enrichment this setup exemplifies efficient Automation in a e ecommerce context so Matt now that we've come to the end and learned so much about event driven architecture what's next how do people get started doing this so I I think uh kind of as we talked about it in the demo a little bit like an easy way is to just start producing events even if you don't have consumers to to use them um you can do that by adding and change data capture depending on what system your your database is on or you could if you have uh a server full or a server L system both could just start putting events on the event rout bus right um and I think it Segways really easily uh to once you get towards like decoupling your system by emitting events like you can really segue into domain driven development and really break things up into like teens um there's the there's a concept called event storming also that you could kind of approach with your team to see how you can break things up into domain driven development have have you used uh event storming or domain driven development in your workplaces sure yeah um I mean I think we we definitely uh you know need to think about the the business domains uh that drive our applications in order to to make sure that we're working on the most important uh things so um so so considering uh the business domains that we have to work with you know like oftentimes uh that becomes a discussion about microservices or uh banded contexts but I think that um you know thinking thinking that in terms of events because whether or not we're using event Bridge whether or not we're using event driven architecture of course we have lots of events right like uh you know click streams like you know different things that happen in the system identifying and naming business events that we care about as things that uh you know become features of the system as opposed to sort of like a side effect uh that of some code I think is a really strong pattern uh to start to make sense of your system like some of us uh have uh work on systems that have been around for a little while and and sometimes they sort of grow up in strange different ways and you end up with feature sets that uh don't exactly make perfect sense if you start creating events and you start saying this is an order event or or this is a uh you know application submission event or whatever your business may be uh that um I I think that's a great way to start making sense of things and then you can start pulling you know you may have had a function that does nine different things uh because that's the way the system was built once you have that event you can start taking some of those things out of that big function and have them instead triggered by the event awesome but it also sounds like we're touching on our next uh free code Camp course if we make a domain driven one all right let's do it and I think that about wraps it up so again uh We've linked it throughout the course but there's a companion blog post at mars. codde um there's also a lot of really great resources at serverless land.com including different serverless patterns that you can use with eventdriven architecture and other serverless concept Concepts event driven architecture isn't specific to serverless but it certainly helps uh there's an awesome talk from Eric Johnson at rein from from reinvent and this short code links to that march. Cod EJ and David Bo does a lot of blog posts about avender and architecture he has some really awesome stuff about like different patterns with it and different tool sets he's made things like the event catalog to help with documentation and the event the event Cannon um that can help you you know test your own event structures and everything like that so he also has a reinvent talk about his journey to event driven architecture and that's I have a short link to that at mar. codes SL DB and then my own blog is at matt. mars. codes and Matt Morgan's links to his talks and all his blog posts and everything is at Matt morgan. Cloud so thanks to free code Camp uh for hosting uh this video and um and thanks to you Matt Mars for inviting me to join you uh and especially thanks for doing all the editing because I would be bad at that it would take me a really long time so I'm glad else is doing that uh it's been fun um uh anyone who enjoyed this and wants to continue your learning find me on LinkedIn uh find my website uh happy to engage with you and uh help continue your learning yeah likewise everything Matt said uh I'm on socials I'm Mart's codes on pretty much most of them including LinkedIn so feel free to reach out to me there I think we'll try to monitor the comments once the video is out and definitely check out the companion blog post that we'll be hosting alongside this U that will also have the code references and everything too so yeah thanks Matt thanks for code Camp hope everyone uh learned a bit about event driven architecture bye bye and and learn so much about event Bridge driven architecture that was stupid I said event Bridge driven architecture you should see my blooper real for enhanced security and context specific specif Eric Johnson is a renowned principal developer at Eric Johnson is a renowned principal developer advate wow I can't say that Eric Johnson is a renowned prent bloop real Eric Johnson is a renowned look at the camera Eric Johnson is a renowned P all right it's just the producer directly manages where and how events are routed allowing for specific event handling let's see if I can edit that a signic uh okay here Postman I'm going to make a uh a post to my API Gateway uh to create a new payment and sorry cut that event Bridge offers not sure what that's supposed to say