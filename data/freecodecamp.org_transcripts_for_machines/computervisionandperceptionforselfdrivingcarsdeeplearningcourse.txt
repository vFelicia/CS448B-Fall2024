An important use case for computer vision and machine learning is self driving cars. In this course, Akshay will teach you how to implement many techniques related to perception of self driving cars, and engineers am SEC sec. Welcome to this course on perception for self driving cars. In this course, we will go over a number of different projects that are related to the perception module of the self driving car. This course is meant for an intermediate level of audience, you are required to know the basic concepts of Python, as well as some basic concepts regarding deep learning. This course is meant for you if you are looking to get into the field of self driving cars, or if you are interested to do projects that are related to computer vision using deep learning. Before starting, if you like the content of this course, you can also check out my channel robotics with Akshay where I post videos related to robotics and artificial intelligence. So now let's get started. Our first project would be road segmentation. In this project, we will be doing segmentation of road images using a deep learning technique called fully convolutional networks. In the next project, we will be doing 2d object detection on various different traffic scenarios using a deep learning technique called the Yolo algorithm. Next, we will be tracking those object detections using a technique called Deep sought. After we are done with the 2d part in these three projects, we will be moving on to the 3d part, we will start our 3d part with 3d data visualization where we will visualize camera images and LIDAR data. Next, we will be discussing multi task learning based on two tasks depth estimation and semantic segmentation. Then we are going to move on to 3d object detection. Then move on to our final project the bird's eye view visualization using an advanced computer vision technique called transformers, all the data set and the core notebooks are provided, so that you can follow along with this course. Alright, so what is the road segmentation problem, you would say, indoor segmentation given an input image, we are required to identify where are the roads present in this image. Now we as humans can see that road is present right over here in somewhat in the middle of the image. So what we are required is to output an image something that looks like this. Now here, we have imaged the road that is present in that image. Now, we aren't going to exactly output this, but what we require is that we somehow mark this road in a separate image. One way to do that would be to output a black and white image where white represents wherever the old is present, and zero or black represents the other background in that image. Let's have a look at another image as well. Now this is another road image that we can see now. Now as humans, it is easy to see where is the road present in this image. For a computer, what it would see is something like this. So the computer has to output a segmentation mask that looks something like this. Now these two images that you're seeing over here, are part of the GT road data set. Now a data set is a collection of such input images and desired output images, which we use to baseline or check how our model is performing. If our model is able to perform well on these images, we can be kind of sure that they are going to perform well in real world as well. But there are some complications that are required as well. Okay, so by now you would have understood what is the problem of road segmentation and the data set that we're working on, which is the kitty robot data set. Now for this task of road segmentation, there exists a number of computer vision techniques that we can apply. The older computer vision techniques required us the programmers to actually hard code the different values in order to segment the road that is present in the image. Now this technique is prone to a lot of errors and usually doesn't generalize very good. Like this technique would work for one image very good but would completely fail for another image. For instance, if we develop a technique to segment a road on an image, then even a little shadow on the road would completely ruin our parameter tuning and we will require to tune the parameters again. Then with the advent of deep learning, we got rid of such high grafting techniques where in the computer would automatically learn these parameters given a diverse set of data. So, one such technique that we are going to discuss today is fully convolutional networks. Now, fully convolutional networks were one of the first techniques that introduced their method of segmentation end to end. What I mean by end to end is that given an input image, if we feed the input image to an FCN model, it will directly output the segmentation that it is trained for, no other pre processing or post processing would be required on that image as such. Now, we are applying the fully convolutional network technique on the road segmentation task, which is we can say a subset of the segmentation task as well. So how the complete picture looks like is that we have an image input image that has a road present in it, we pass that input image to the FCN model that we have, and towards the output, we get a 01 image binary image where one is present wherever the lane was present, or the road was present. Otherwise, we have zero in the complete image. Now, in order to understand fully convolutional networks, we first need to understand an image operation called up sampling. So let's have a look at that. So in simple words, up sampling is an operation where we increase the size of a given input. For instance, if we are given an image like this, a two cross two image, then upsampling to x is going to double the height and the width, therefore, we will be having four cross four output, the complete problem of of sampling is we need to decide what are going to be the values in this whole cross code metrics when we would be given the values in this to cross to given metrics. So there exists a number of techniques that we can employ, let's discuss a few of them first, the first technique for up sampling we have is called bed of nails, what we do in bed of nails, whatever the values present in this matrix, we are going to assign that value to this block over here. So the three values that are related to this block which are 123, these three blocks are assigned a value of zero. Similarly from this block, we can take a value and we can assign it over here, the rest three related values are assigned zero, these, which are these three. Similarly, we do the same thing for these two cell values as well. Now this is a really simplistic technique where we are going to have a lot of zeros in our output image, a better method than this is the nearest neighbor method. In nearest neighbor method, what we do is we take the value that is present over here, and in all the four related cells to this cell, we assign the same value. For instance, if we have one over here, we are going to assign the value of one to all the four blocks over here. Similarly, if we have a value of two over here, then we're going to assign the value of two to these four blocks here. And we do this similarly for the complete metrics over here. Now, an even better method than nearest neighbor is called interpolation. Now, interpolation is more like an average finding technique where we assign values based on a weighted average, how we do that, in interpolation, we try to work our way backwards, if we want to calculate the value of this cell over here, then we approximately try to overlap this matrix with this matrix, what we are going to see is that this block is going to lie somewhere over here. Now this cell over here is close to four values in this matrix, which are all the four values over here. Now based on the distance from these four values, we are going to assign an average value to this cell over here. Similarly, we are going to do this for this cell as well, this is going to lie somewhere over here, and average value will be assigned to this cell based on the distance from the four cells that are present over here. Likewise, if we want to assign the value of this cell over here, then this cell is going to lie somewhere over here. Now, we are going to take the average of these two cells only because these are the nearest neighbors present to this cell. So this is how the interpolation method works. Now interpolation method is one of the best technique that we could apply, but there exists an even better technique that we can apply as well, which is transposed convolutions. In all the techniques that we discussed before, we were assigning the values based on some heuristic that we hardcourt. Instead in transpose convolution, we assign the values based on a learnable weight filter, we define a filter of some size with some random weight values. We then use those weight values to assign the value of All these cells that are present in this matrix and pass this to the network, when the network is going to do a backpropagation, we are going to back propagate the loss of the weights of those filter as well and they are going to be learned as well. For the purpose of this project, I tried two methods which were interpolation and transposed convolutions, from what I saw transpose convolutions did not work very well. But the interpolation method worked really well for our case. Now, since we have understood what the upsampling operation is all about. So let's now discuss the architecture of the fully convolutional network. So for the fully convolutional network, we require a backbone network, which is an image classification network called VGG. Net. Now, what is image classification and what is VGG net? Now image classification is a problem where we are given an input image and we are required to decide what is present in that image. Or we can say the computer is required to decide what is present in that image. Now, before doing anything, we already define the classes before sending an input image. For instance, if in our image, we know that there are going to be three entities that are present, airplane bicycle and a person, then we are going to assign a value zero to the airplane, one to the bicycle and to to the person. Then we pass an image through the image classification network, and it outputs 01 Or two, representing what it saw in the image. Now, image classification is kind of a solved problem where we have models which give very good results on some baseline datasets. One such good network is VGG. Net. Now since VGG, network takes in an image input and outputs a single number output, we need to convert it for our use case, which is the segmentation problem. In the segmentation problem, the input is an image and the output is also an image that is of the same dimensions. So let's see the changes that we require to make to this VGG net in order to output such images. Now, VGG network is composed of a number of convolutional blocks, it's placed sequentially to each other. Each convolution block consists of two convolutional layers, followed by a max pooling layer. Now what we do is we extract the output of last three convolutional blocks, which are pool three, pool four and pool five. So what we do first, we upsample, this pool five times two, so we have an bigger image over here, then we add this output to the output of pool four, then what we get over here, we again upsample, this output two times to get an output over here. And finally again, add this pool three layer, which then we get the output over here. Finally, we upsample this output eight times. And the final output that we get is the segmentation mask that we require. Now, this long sequence is what constitutes the FCN architecture, or particularly this is the FCN eight architecture. There exist to other FCN architectures as well FCN 32, and FCN 16. What we do in FCN 32, we don't go through all these operations that we have done over here, instead, we simply upsample, this pool five layer 32 times and we get the output in FCN 16, we again, don't do all these operations, we simply go to this step over here where we calculate the sum till pool four, and then we assemble it 16 times to get the output since we are performing all the operations that are present over here, and finally upsample eight times therefore, this architecture is called FCN eight. In one way, we can say that FCN eight is a combination of FCN 32 and FCN 16. What this means is that FCN 32 identifies coarser structures in the image FCN 16 identifies more finer structures in the image and FCN eight even more finer structures in the image using these addition operations that we have over here we are combining the knowledge of these three networks to get the final output. Now these are the complete details of the architecture and how to architecture is somewhat working on the inside. Alright, so now our discussion for this is complete. Now let's have a look at how this model was implemented and what are the different results that we got. So let's have a look at that the data set and the code both are present on Kaggle. The link for both of these is in the description you can have a look at them as well. Okay. So this is the implementation First we define an input layer with a parameter input shape. Now input shape is a tuple that contains image size, image size, common number of channels, image size we have set over here is 128, cross 128, and number of channels is three the RGB channels That way define the VGG network, but that is VGG 16. TensorFlow or Kara's already provides us with the implementation of VGG 16, which is pre trained on the image net data set. Then we extract these three blocks, which are the pool three, pool four and pull five layers. For the first step, we have sample two times that we can see over here, the tool five layer, then we add the sub sample layer to the pool for and we apply a one cross one convolution over here, as you can see, now this convolution is more like a placeholder to get the dimensions in the right size, as such, using them or not using them doesn't have a big impact on the performance as such. Then we apply the second up sampling the second two acts of sampling and add that upsampled layer with the pool three layer, again, we apply a convolution to D which is again as kind of a placeholder overhead. Finally, for the output, we calculate the upsampling eight times this final convolution is a required one. Now this n classes is said to be one over here, this is done because we want to set the output of the network to be a single channel image. Now that single channel is going to contain zero and one as we discussed before, one is going to be the mask where the road is present and zero otherwise. We also apply a sigmoid activation over here in order to get the final values between zero and one. And then we return the model with the inputs and outputs as such set and we have named the model FCN eight likewise now let's have a look at the results that we got his images were held out from the model and the model is seeing them from for the first time this is a second image. Now as we can see the model is working really well. Now let's apply some changes to the baseline network that we have and see how the results change. Now this is one change that we do. Instead of the add function we are using the concatenate function over here in adding what we do is we add the corresponding values that are present in the matrix of two images. However in concatenate, we just add that image to the back of the image and increase the number of channels of the input. So now let's see what are the results after we apply this concatenate instead of ad? So we can see that concatenate is also performing good, but add is a little better than this concatenate function. Now, let's change that upsampling today with convolutional 2d transpose, which we can see over here, so instead of upsampling, here we are using transpose convolution where we are going to learn the weights of upsampling as well. And we are using the add function over here. So let's see how the results for this look like So we can clearly see that transpose convolution is not getting that good of results, although it has learned the task, but the output is kind of pixelated, and doesn't really look good on the segmentation as well. Alright, so these are the results of the network and some of its extensions as well. So, before discussing what is to do object detection problem, let us first see, what is the data set that is provided to us in this project. So, Lyft is a self driving cars company that develops self driving car software. In order to further the research in self driving cars. Lyft announced two competitions that were held previously, these two competitions were 3d object detection and motion prediction, these two are separate. The data set that we're going to use for this project is the one from the left 3d object detection challenge. So what was given in the object detection challenge. So, what Lyft did is it drove a self driving car around the streets of Palo Alto, and they recorded the camera data as well as the LIDAR sensor data. Now, Lidar is a sensor that works on the principle of reflection of laser lights and lasers starting from the sensor goes and hits an obstacle returns back to the sensor and the sensor detects the distance of that object. So in this data set, we are given the camera data as well as the LIDAR data or the same trip. As we can see, these are some sample pictures. So this is one back image. As we can see, we have certain cars over here we have the front image, we have certain cars over here as well. Now we can compare the LIDAR data with the camera data. So we saw our car is localized at this cross and we can see that there is one car that is present over here that is trying to enter the lane besides us as we can see we also have a car over here and we can also see that we have two cars behind our car which are present over here. These blue bounding boxes represent pedestrians so we also have front left image and right image back right image and back left image we can also visualize the data in a video format so let's play this video similarly, we can also visualize the LIDAR data in a video format let's play this video so, the code for this data visualization will be present in the description box below. We are specifically going to work on the camera images as such. So now let us discuss what is this object detection problem and what is the difference between 2d and 3d object detection. So in object detection, the complete problem is framed as such. That in a given image, we are required to find the location of an object present in that image as well as also tell what is that object. For instance, in this example, we can see over here, there is one dog that is present and there are two cats. So given this image, we need to output where is the dog present in the form of bounding box and where are the cats present. Now in a 2d object detection case, we would be given a 2d image and we are required to give the bounding boxes in 2d. However, in 3d object detection, apart from a 2d image, we would also be given some other data representation as well. Using that image and the additional data, we are required to return a 3d bounding box of that object along with its label. A very good example is in the 3d object detection data as well. As we can see, we are given 3d bounding boxes of the various cars in this image. And we are also given an additional data which is the LIDAR data And we can also see that we are predicting pedestrians as well as well as the cars. So one cool thing that we can also notice is that we are also able to detect objects that are included in the given image. How we are able to do that is because of the additional data that is provided by the LIDAR. Alright, so this is all for 3d and 2d object detection. In this video, we are only going to focus on 2d object detection. So we are going to run an algorithm on these given images and predict 2d bounding boxes. So let's discuss that algorithm in detail. As you may have seen in object detection, we have to localize the object as well as tell what that object is. Now in case of self driving cars, as we can see over here, if we are given two cars, then we also need to find the position of those two cars as well as decide are the cars are the pedestrians are their bicycles, and so on. Now, the algorithm that we're going to discuss for 2d object detection is yellow, y O L O. And this stands for view only local ones. Now giving an overall picture, we can divide the Yolo algorithm into three stages, which are the anchor boxes, the intersection over union and the bounding box predictions. So how does this work? Now YOLO is also a deep learning technique that we use for object detection. In this we are given an input image, we pass this input image through a convolutional neural network. And as an output, we get another metrics. Usually, for the Yolo algorithm, this metric size is taken to be 19 Cross 19. What does this input to output mapping represent? If we consider this matrix as a 19 cross 19 matrix, if we take our input image as such, and overlap this 1919 matrix over it, then the 19 cross 19 boxes that we can see over our image, all those individual boxes are going to tell what is present in that box. It can either be a car, a truck, or even a motorcycle. And it is also possible that there is nothing useful present in that particular box. For instance, in our case, it would be we can say Road, for each cell, we are going to have this vector in place, this thing can also be visualized as this matrix is actually a volume. So 19 Cross 19 Are the width and the height of this matrix. And in the depth perspective, we have this complete vector. Now let's see what this vector describes. This vector that we discussed, describes the information that is present in that particular cell, the PC value is going to tell whether something interesting is present in that cell or not, is something interesting is not present in that cell, then the other values are just going to be undefined, or we are not going to consider them in the calculations as such. If we assume that PC is one and there is something interesting present in that cell, then x and y are going to describe the position of the center of that object with respect to a given cell. So x and y are going to tell if we start from this upper left corner of this cell, how much we need to go right and how much we need to go down in order to reach the midpoint of that object, W and H describe the width and height again with respect to that cell. Now these C one C two c three variables describe what object is present in that cell. For instance, if we have three objects, then one of these three is going to be one, the other ones are going to be zero. If we consider that we have four objects in our consideration, then we're going to have four variables over here c one, c two, C three and C four. Now we can see what is going to be present in the output. Some of the output cells, as we can see there, red over here, are going to describe that something special is present in that particular cell. So the output over here gives us what the object is going to be and what is going to be the bounding box of that particular object. One certain example is we can see over here for this particular car, we can see that there are multiple bounding boxes that have been generated by this output. And in order to differentiate I have also drawn one car in the corner over here as well, that also has multiple bounding boxes describing it. Along with these bounding boxes and the labels, we also compute a probability. This probability also describes how much confident our our bounding box predictions. For instance, this bounding box over here, the outer one can have a lower probability. However, the one that is inside is going to have a higher probability. So this is how one part the bounding box prediction part of Ulo works. In order to separate these bounding boxes and get a single bounding box for each object, we use intersection over union. And this technique of getting a single bounding box and separating different bounding boxes is called non maximum suppression. Okay. So first let us see what is intersection over union or we can say I owe you. So, IOU is used to compute how much are two bounding boxes different, you will be able to see that in its mathematical description. So if we have two bounding boxes as we have given over here, we first calculate their intersection and divide that by their union. So, this gives us a measure of how two bounding boxes are similar to each other. So, we can use IOU to separate the bounding boxes of two different objects. For instance, these two, we calculate the value of the bounding boxes with respect to the maximum probability bounding box. So, what this means, if we consider again that the inside bounding box has the highest probability, and we calculate the IOU score for the other four bounding boxes with respect to this maximum probability bounding box, we can see that these two bounding boxes are going to have a very high IOU value. Since they have a very high IOU value, this means that they are actually representing the same object. Therefore, we discard these other two bounding boxes of these two other bounding boxes, they have a very low IOU value, this depicts that they represent some other object, and we are going to apply the non maximum suppression separately on these bounding boxes. And this is how the intersection over union part of you know works, or there can be a scenario in which a single cell over here represents multiple different objects. One small example would be two cars that are parked side by side. So, in one of these cells, we are going to capture both of those guys simultaneously. In order to deal with that problem, we use this concept of anchor boxes, these anchor boxes represent a rough shape for the different objects that are going to be present in our image. These anchor boxes can be designed by us humans, or it can be decided by another machine learning algorithm as well. So how does this anchor box help in detection of multiple objects, so each anchor box describes what shape of the object they are looking for. For instance, this anchor box is looking for a square object. This anchor box is looking for a rectangular object with a greater height than width. And this anchor box is looking for an object which is greater in width, but less in height, each anchor box is going to predict a certain specific object, and how this is going to change our output. For each cell, instead of having this single vector, we are going to extend this vector in our case three times since we have three anchor boxes, this enables us to predict multiple objects that will be present in a single cell. So, all these techniques combined give us the Yolo algorithm YOLO algorithm is a really fast and accurate object detection technique, which makes it extremely useful for perception in self driving cars, there is a lot of work that has been done on this YOLO algorithm too much so that that we have multiple variants of YOLO algorithm with each variant of the Yolo algorithm, there has been an improvement in the speed as well as the accuracy. Okay, so now let's see this YOLO algorithm in action YOLO is a very complex algorithm that would take quite a lot of time to develop. Hence, it is neither feasible nor an intelligent way that we implement your law from scratch. Hence, we are going to use this Charis YOLO v3 implementation provided by experience or apologies if I pronounced that incorrectly. This is an open source implementation, I would provide the link to the repository in the description section below. This is the notebook that we have the link for this notebook would also be present in the description section below. Since there is nothing much to discuss with respect to implementation, we will directly get to the results that we have on this dataset. So I have zoomed into show the image is better as we can see the camera was attached on a car so we are also detecting our portion of the car as well. We are detecting other cars as well. However, we can see that this bus is getting detected as a truck. We have predictions on car as well over here. Cars over here. Ours so we can see we also have one truck detection over Here we have the detection of a bus over here as well. And we also have a truck that is much behind Alright, so these were the results that we got on running the Yolo v3 algorithm on the left 3d object detection data set. Now, 2d object detection is pretty much a solved problem. And we already have very good algorithms like YOLO that do our tasks. However, there is some research that is still required on 3d object detection, which was also the motivation behind why Lyft conducted this competition as well. Alright, so this is it for this video. For this particular project, we are going to use several videos that were recorded by a camera that was placed in front of an autonomous car. So let's have a look at one of the videos. In this video, we can see that there are two cars that are moving. And this is another video of the same highway scenario. Saw data set has six videos that contain this highway scenario. For object tracking, we need to keep track of both of these cars that are moving in front of us. If we assign an ID to both of these cars, if we say this is the black car, and this is the white car, then in each frame of the video, we need to keep detecting where is the black car and where is the white car. This has to be done for the complete duration of the video. In Object Tracking, we are not only required to detect where the car is present in this particular frame. But we also need to remember that this is the car that has been assigned the ID black car. And this is the car that has been assigned the ID white car, the link to this dataset would be present in the description box below. Okay, so now we have defined object detection problem and had a look at the dataset. Now let us understand how this deep sort algorithm works. As you may have seen before, in object tracking, not only do we need to detect the objects, but also track where they are going. So we consider this collection of images, or we can see a video, we can see that we have two objects that are moving in this video, it is this round object that is going in the horizontal direction and one object that is going in the vertical direction. In Object Tracking, we are not only detecting where are the objects present in this image, but we are also tracking where the objects I did one and two are going. So this is the whole problem of object tracking. In our case, it would be traffic tracking, we need to keep a track of the different cars that are moving around our autonomous vehicle. So the algorithm that we're going to use to solve this object tracking problem is called the deep sort algorithm. Before discussing the deep sort algorithm, we would first start our discussion with the simple sort algorithm. So R stands for Simple online real time tracking. And the simple sort algorithm uses bounding box prediction, Kalman filters and IOU matching techniques. First let us start with bounding box predictions. The first step in this algorithm is to generate bounding boxes or detect where are the objects present in the image. Now, this can be accomplished using any CNN architecture it can be YOLO it can be our CNN or it can even be a simple computer vision handcrafted model. For our project we have used the Yolo v3 algorithm briefly what you know v3 algorithm does given an input image it is going to detect all the objects that are present in that image, draw a bounding box around them, and also tell us what that object is. After we generated the bounding box predictions we get to the next step, which is the Kalman filter. Now Kalman filter is a very broad topic, which would take an entire video to explain, but still we would be left with a lot of ground to cover. So to briefly discuss Kalman filter, we can say in very simple terms that Kalman filter is a linear approximation. So what is the role of kalman filter over here? Kalman filter needs to predict what is going to be the future location of a given detected object. Using these future predictions we can determine whether the object that we were tracking isn't the same object or not. Along with that, using prediction, we can also deal with the problem of occlusion. So what is occlusion? In this example above, we can see between these these two frames, the two balls are going to overlap each other. And whichever object is in front is going to occlude the object that is present behind it. In order to deal with this problem in object tracking, predictions are useful. So how does Kalman filter works? Kalman filter assumes a linear velocity model and generates the predictions according to it. Once we get the real data where the object is, we input that again to the Kalman filter. The Kalman filter improves its predictions based on the real data that it got, and again generates a new set of predictions. In this way Kalman filter works in an iterative manner and keeps improving its predictions. As an output, the Kalman filter does not output a single number, actually, it outputs a probability distribution of where the object can be in a given set of locations. If we take the maximum value of that probability, we can in some way approximate where the object is going to be. This is the work of the Kalman filter, it generates future predictions for the objects. The next step we have is called IO U matching. Io U stands for intersection over Union. In brief terms, we can say IOU gives us a quantitative score to determine how much two bounding boxes are similar to each other based on their location in the image as well as the size, but is this IOU matching in the context of sort algorithm, let us consider this case, we have n different cards that we are detecting in a particular frame. And we have n IDs that we need to assign to these particular cards, like in this case, IDs are useful to determine which object we are tracking. So we need to assign these n detections to these n IDs. For each of the n IDs, we have IOU score corresponding to each detection in a descriptive way, we have the detected objects in the row, and we have the IDs in the columns. So for each of the ID and the GS detection, we have an IOU score. Now we want to assign the IOU scores to these n detections in such a way that the total IOU score is maximized. solving this problem using brute force approach takes order complexity n factorial, which is very large. In order to solve this, the sort algorithm uses the Hungarian algorithm, the Hungarian algorithm solves this linear assignment problem in order complexity n cubed, which is a great improvement over the n factorial algorithm. Once we have assigned these n predictions to N IDs, we have in a sense solved the tracking problem for the AI X frame, then we can run this algorithm again in a loop and again, keep track of the objects that are in the next frame of the video. So this is how the simple salt algorithm works. Not deep salt is an extension of the simple sort algorithm. Now that deep in the deep salt comes from this step called the deep appearance descriptor step. Along with that, we also have a cascade matching step that is added to the simple sort algorithm. So deep appearance descriptor is a convolutional neural network, which is trained to detect a similar object in different images. What this means this, this network can tell given a number of images of the same person in different views, whether it is the same person or not. So as an input, the deep descriptor receives a cropped image of the object detected. And as an output, we try to receive a vector that encodes the information that is present in that crowd image, these encoded vectors would allow us to compare different objects. So now we have two models that are describing us about similar objects in different frames of a video, we combined the score given by these two models in a linear fashion. The score by the deep descriptor is given using the cosine distance metric, and the score by the Kalman filter is given by the melanomas distance before discussing how these metrics are computed. Let us first discuss the notion of a distance metric. A distance metric is a score that we assign to two different entities A and B to tell how similar they are. For instance, if we consider it to deep plane on that plane, we have two points A and B. We compute the distance from the origin. If the two entities A and B are close to each other then distance from the origin is also going to be similar in value. If the two entities A and B are very dissimilar to each other, then these two entities are going to have a Euclidean distance that is very dissimilar, we can say one is going to have a negative value, the other one is going to have a positive value if they are lying on the two sides of the Y axis. In these two cases, we need to use another notion of distance metric. For the deep descriptors, we use the cosine distance metric, how do we compute this? If we are given two entities A and B. And from the origin, we draw a vector joining these and then we calculate the angle between these two vectors, the cosine value of this angle is going to give us the cosine distance metric. Let us consider an example. If we have two vectors that are overlapping each other, the angle between them is going to be zero, and the cosine value of zero is going to be one. This means that these two vectors are very similar to each other. However, if two vectors are perpendicular to each other, the cosine value of 90 is going to be zero. This would mean that these two vectors are very dissimilar to each other. In this way, we would be able to determine whether the two objects that we detected how much similar they are to each other. For the Kalman filter case, we are not using the cosine similarity. A reason for this is the Kalman filter is going to output not a single point, but a probability distribution. In order to compare the similarity between a point and a probability distribution, we use melanomas distance. If we consider that this is the distribution that is output by the Kalman filter, this implies that the probability of the object being over here is very high compared to the values outwards. And this is the point that we want to compare with this probability distribution. In very simple terms, what melanomas distance does, for a 2d version, we compute the two directions in which the data spread is the highest. For this case, we can see the data spread is the highest in this direction and a perpendicular this direction. Then we normalize and transform the coordinate axes in such a way that these two axes become the coordinate axes. In that transformed space, we calculate the distance of this point from the origin, a value of that distance is the melanomas distance. Using this metric, we decide how much a given prediction by the Kalman filter matches an object that we just detected. Combining these two, we get an overall scalar score value that we can use to assign detections to their ids using the N Galeon algorithm. However, in the implementation, there is a slight catch, the authors of the deep sort algorithm noticed that the predictions by the Kalman filter are not very useful, and the score for these predictions can be neglected. However, you will see that in our project, this doesn't work very well. The next step that we have is cascade matching. Gaskin matching is an extension to the IOU matching algorithm for the assignment problem, the cascade matching takes into account the temporal dimension as well, in order to reduce the time complexity for the Hungarian algorithm cascade matching tries and match the latest detections with the latest IDs and the later or old detections with the old IDs. All of this process combined gives us the deep salt algorithm. And we can again run this algorithm in a loop to track objects in a complete video. Alright, so this is all for the explanation part of the deep sort algorithm. Now let's have a brief look at the code and the results and see how it worked for our project. So let's do this. The link to this code or notebook will be provided in the description box below as well. So, these are the different sub parts of the complete deep sort algorithm. The first part is the Yolo v3 network that is used to generate the bounding boxes. The second step is the detections part where we encode the given cropped image. The third part is the Kalman filter, where we use the Kalman filter to generate the predictions. The next part is the IOU matching in this part, we solve the linear assignment problem and the Cascade matching that we discussed before. Next up is the nearest neighbor matching. This contains the code for the Euclidean distance as well as the cosine distance. The next part is tracking. This combines all the different modes To do that we developed before to create a final object that does the complete tracking. Finally, we have the object tracking where we run the complete algorithm. This file over here contains the pre trained network for deep appears descriptive II have this function object tracking that takes in the video path input, and also takes the output path there to save the output video. Now let's have a look at the results of how the final video looks like. So this is the first video, as we can see this one and two, these are the IDs that are assigned to these two cars respectively. This is the second video. As we can see, the car over here was assigned the ID nine. And even in some frames, the yellow algorithm was not able to detect that car. Even after those Miss detections, the algorithm was able to determine that this was the car labeled ID nine l now we had a look at the videos where the algorithm was performing really well. Now let's have a look at the video where the algorithm didn't perform that well. In this video, we can see that we have this car labeled 12. And we have this new car that is labeled 35. Now you will see what is the problem that this algorithm is facing. Due to the occlusion that was given by this black car. This car is now the identified as the ID 36 and not the ID 12 That it was assigned before. So this is one problem that deep sort algorithm faced due to a longer occlusion period, the algorithm was not able to detect the correct ID for the car. Even after checking different hyper parameter values, I was not able to get rid of this ID assignment problem. This I believe would be due to assigning an negligible amount of weight to the Kalman filter predictions. Possibly if we increase the weight for the Kalman filter predictions, this problem would be solved. But that is an extension to the project that we developed in this video, and will be looked as a future work to this project. Alright, so this is it for this video. Okay, so these are the images that are part of the TT 3d object detection data set as we can see. And these are the LIDAR data provided in the data set as well. For instance, we can see a wall in this image over here, the same wall, it has a correspondence in the LIDAR scan as well as we can see over here. These curvy lines are the projections of the LIDAR that fell on the road itself. All also if we consider another example, we can see this car in front of the image. This same car can be seen over here, which is blocking the LIDAR signals. Another cool visualization that we have is the LIDAR data presented in the form of a camera as we can see over here as we saw in the image earlier, we had a car in front of us, we can see that car similarly in these LIDAR scans as well. Another example is this car over here. This car, this car as well as the traffic signal can be seen in this LIDAR scan as well. And this is another visualization of the LIDAR data as well as the bounding box that are given in that 3d object detection data set. Alright, so these are all the visualizations. Now let us understand what is the theory behind how we generate these visualizations. Okay, now in order to understand how are we generating these data visualizations, first step, we need to understand the concept of homogeneous transformations. In a general self driving car setting. We usually have two main sensors which are Lidar and the camera. Typically, these Lidar and camera sensors are not going to be placed at the same location. They are going to have different locations turns on a car. For instance, in this case, the Lidar is on top of the car, and the camera is on the front head of the car. Camera, as we all know, is a visual sensor, we use it to capture a 2d scene of a given 3d environment. Lidar, on the other hand is a perception sensor. It is used to map a 3d environment using a rotating laser. LIDAR generates what are called the point clouds. Point Clouds provide a very good representation of a given 3d environment. Now, this is a very general use case that we want to see the data that was captured by a lidar in the frame of a camera. What I mean by that, if through our eyes, we are seeing how the environment is looking like from here, we also need to see how the same environment at the same time is going to look like when we see it from over here at the camera location. In order to convert between these given reference points, we use what are called homogeneous transformations. If we consider that we are given two coordinate axes, the blue one over here, and the red one over here, and we have this object, this black dot that we want to see through both of these coordinate axes. Mathematically, this problem translates to, if we are given the vector from the origin of this coordinate axis to this point over here, which we call x, we need to find x dash, which is a vector starting from the origin of this coordinate axis to the same point given this coordinate axis is called transformation. Now, any general transformation can be divided into two subproblems, which are translation and rotation. Translation is simply moving the origin of this coordinate axis to some other location in space. If we have a coordinate axis like this, and we simply move this to a new location, then we have what is called a translation. In rotation, however, we would like to change the direction in which these axes are pointing. However, the origin is going to remain at the same place. In order to understand rotation better, we can divide the rotation operation into three sub operations, which are rotation about the x axis, rotation about the y axis and rotation about the z axis. A combination of all these three rotations gives us the actual rotation that is going to finally take place. Without going into much depth, let us see how the mathematical representations of these look like. In order to get the translated vector, we simply need to add another vector to our original vector. In this case, we are adding this ABC vector to our original x to get x t, which is the translated vector, the rotation operation can be understood as a matrix multiplication. If we consider this three cross three matrix, and we multiply this matrix with our original vector x, we get a rotated vector XR, however, we need to keep this in mind, the values of this translation vector can be taken anything, but the parameters of this matrix are bounded between a range combining these two sub operations of translation and rotation, we get what is called a homogeneous transformation. What homogeneous transformation does, it simply reduces translation and rotation into a single matrix multiplication, as we can see over here. So how we derive this matrix multiplier, first up, we take the rotation matrix and place it in the upper left corner over here, we take this translation vector, we place it in the upper right corner over here, and these remaining part, this last row is filled with three zeros and a one. This will look something like this, this three cross three matrix is the rotation matrix. And this three dimensional vector is the translation vector, rest zeros and ones are placed over here. Now following the rules of matrix multiplication, given this is a full Cross for matrix, it can only be multiplied with a vector of dimensionality for in order to make this three dimensional vector, a four dimensional vector, what we do is place a one over here. Hence, our complete operation looks like this. We'll take this matrix, we multiply this matrix with this extended vector. And what we get as an output is this extended vector over here, because of the structure of this matrix over here, this one over here is a guaranteed number. Using this operation, we Finally get a vector x dash representing the point x in a new frame of access. Now, in order to convert this given 3d data into a 2d representative screen, just like we have in a camera, we use the operations that we will discuss over here. Now mathematically, how does a camera work? If we consider this red coordinate axis as the camera axis, we have this point in the 3d scene that we can see through this camera axis. Now we define a new point which is called the camera Center, which is taken to be the origin of this coordinate axis over here. Now all the points that are situated at a distance of one unit from this camera center in the direction of z axis, is what we call the image plane. Now this image plane is a plane to the surface, which has defined its x&y coordinates in this direction, the x direction is taken upward and y direction is taken along this direction. We can also understand this image plane as sitting at the camera center and watching a cinema screen that is one unit away from us. Now this complete problem boils down to if we are given this 3d point, we want to project this 3d point to this image. That is, how would this 3d Point look like if we were to see it in a 2d projected image plane. And this is exactly what a camera does converts a 3d scene into a 2d image. We accomplish this using the similar homogeneous transformations. If we are given a transformation matrix M, we can simply multiply that with a 3d object vector x dash and get a new vector, which is x double dash. Also, in order to get how these points would look like in this image plane, we need to divide the X and Y coordinates of this newly x double dash vector with its Z coordinate, which we can see over here. Solving both of these operations, we would get the transformation of a 3d point on a 2d image plane. In our case, how that works out is we are given the point cloud that is generated by the LIDAR using these operations. We can see how that LiDAR point cloud is going to look like when we put it on an image that is generated by a camera. Now this complete concept of homogeneous Transformations is very fundamental to robotics as well as computer vision. It is used in self driving cars, mobile robots, and even robot manipulators in computer vision. Apart from this application, it is also used in 3d scene reconstruction. Now the parameters used for the operations that we did on this side of the board are called extrinsic parameters. And the parameters that we used for this side of the board are called intrinsic parameters. Extrinsic parameters are external to the camera and depend on how the camera is placed. Intrinsic parameters depend on the internals of a camera, for instance, the focal length of the camera lens, there are a number of different ways we can use to determine what are the extrinsic and the intrinsic parameters of a given camera. One very popular way to determine these is the checkerboard method. In this method, we take a printed photo of a chest or a checkerboard. And we click its images in different positions and angles. We apply a number of different operations to those images to determine the extrinsic and intrinsic parameters of the camera. This is all the theory that we needed to discuss for the data visualization. Now let's have a brief look at the code of how this data visualization is working. So let's get to it. All the code that we are using to generate these 3d visualizations will be present in the description box below as well. Alright, so now first, let us discuss how we generate this LIDAR data in the form of a camera. In order to convert we have this points array over here. This array contains all the points of the point cloud that were captured by the LIDAR. We pass this points array to a function called Velo to Cam. Along with that, we also pass the extrinsic camera metrics. Well to Cam over here stands for velodyne to camera velodyne is the LIDAR sensor that was used over here. All in all what this Vallo two cam function does, it takes this cloud array and it takes this v two C matrix, and it simply does a matrix multiplication on them. Once we get the points in the camera reference, we then calculate a matrix multiplication of those points with the intrinsic camera matrix. And we do the division operation over here. And finally, we fill in different colors using this expression over here. And calling this function on different set of point clouds gives us this the following visualization that we see over here. generating this view of the LIDAR also called the bird's eye view, is relatively simple. We simply take the points of the point cloud and scale them in such a way that they occupy the complete frame of a given image. And then finally, fill the different colors that are present in over here. And this is how we get the bird's eye view of the LIDAR data. Getting this image visualization is pretty simple. And this data is already provided. And we do not need to do any processing over here as such. Getting the boxes to print over here is also simple, just like how we did for the LiDAR, we use the extrinsic and intrinsic camera parameters to accomplish the same and this is just a Combined view of the Lidar and the boxes. Alright, so this is all for this video. Alright, so the problem of multi task learning is such that we want to derive a single network that can be used for various different computer vision tasks. The data set that we are going to use for this particular project is the cityscapes dataset, the link for this data scraped would be present in the description box below. The cityscape data set consists of different Street View images, that is images taken from a car that is being driven on the road. This data set is particularly used for depth estimation and semantic segmentation. Let's have a look at the images present in the dataset and what these two tasks are. So these are the different images that are present in the data set. Okay, now, let us discuss what is the depth estimation task. As the name implies in this task, we want to calculate the depth of any object that is present in the image. Let us have a look at some of the example images to make this point more clear. In this image, we have this struct that is front of our car. And through the color gradient we can see that the depth is almost constant in front of this. We can also observe other such features from this gradient color. The areas close to the car are slightly darker in color and as we move away from the car, the colors get slightly lighter. As we can see over here. A similar look we can have over here, there is a car present near our car, and we can see a very dark color for that car. And as the objects go away, the color gets lighter and lighter. Let's have a look at some more example images. So the overall work in this task is to calculate depth maps, and the depth maps without the underlying image looks something like this. For the image segmentation tasks, we need to segment different objects that are present in the image. Let's have a look at some example images. In this example all the cars and vehicles are segmented in a yellow color. And the trees and background are segmented in red color. Something similar can be observed in this image as well. Let's have a look at another example images So, the overall work in this task is to calculate segmentation labels which we can see in the example images over here. Alright, so, this is the visualization of the data that we are going to use. Now, let us have a look at the technique that we are going to use to solve the multitask learning problem. Alright, so the problem of multitask learning is as follows. We want to derive a network that can be used to solve different tasks, these different tasks can be object detection, object classification, and Object segmentation. The constraint in this problem is that we cannot have different networks for these different tasks, we have to use a single network that is run once and trained once to solve all the different tasks. The advantage it has over distributed networks is that we don't have to train each and every network individually for the different tasks resulting in saving some time. So the technique that we're going to discuss for this multitask learning is m 10. M 10 stands for multi task attention network. And this is the architecture that M tan follows. Given an input image, the first step we do is calculate the shared features of that given input image. Then we have task specific modules that use those shared features to solve their given task. For n number of tasks, we are going to have n specific attention modules. These attention modules take some data from the shared features and augment it in such a way that is useful for their particular task. The word attention over here refers to the attention that each module gives do its particular task. For the intent technique, we can use any convolutional network to calculate the shared features for our particular project, we are going to use set net to calculate the shared features. Now signal is a network that is used to solve the image segmentation task in the image segmentation task, we would be given an input image and the task is to segment the different objects that are present in that image. For this image segmentation task, the networks that are used have a general structure and general structure is such that we are given an input image the first step we do is downsample the image and derive the features from that image once we downsample the image and reach the bottleneck, we have sample the image to the same dimension as the given input image. And with the help of the features derived in the downsampling task output a segmentation mask. The downsampling part of the network is also called the encoder and the upsampling part of the network is also called the decoder. The only difference between Cygnet and fully convolutional neural networks is the operation that we use for upsampling. So the Signet architecture has different convolution blocks. As you can see over here, these convolution blocks comprise of convolution layers followed by some pooling layers. Now, the M 10 architecture is in such a way that for each of these convolution blocks, we have attention some modules, these sub modules are for the blue tasks and these sub modules are for the red task. The arrows over here show the data flow that is happening between the overall network. Now there are some differences in the upsampling attention modules and the downsampling attention modules. Let us discuss them in slight detail. For the upsample attention module, we get the input from the previous attention module which we can see over here. And we take another input that is derived from the first layer of the convolutional block as we can see over here, these two matrices are merged the convolution operation on them is computed twice and then we calculate the element wise multiplication of the output generated over here and the input of one of the filters of these convolution blocks as we can see over here, then this element wise multiplication is once again sent through a convolution block and the output is returned to the next attention module. For the downsampling module, we we take the input from the previous attention module, which we can see over here This input is sent through a convolution block which is then merged with the input of the first layer of the convolution block which is present over here. This merge filter is passed twice through convolution blocks, then an element wise multiplication is calculated for this output, and the input that is taken from one of the layers of convolution block as we can see over here. And this final output is sent to the next attention module, the attention module is only different for the up sampling and down sampling phase of the shared feature network. For the different tasks, the attention module architecture is exactly the same. And this is the complete network architecture of the M 10 network. One additional technique that M 10 proposes is what is called dynamic weighted averaging. For any multi task learning network, we calculate the loss by a weighted average of the loss calculated for the different tasks as you can see over here, the adjustment of the weights of these losses is very important for the network to actually converge and learn the different tasks. Dynamic weight averaging is a technique that is used to determine these weights based on the rate of change of the gradient loss. Now, what is the rate of change of gradient loss, if we consider two training batches of images input to the network at time t and time t plus one the difference between the loss that is back propagated at time t plus one and time t is what is called the rate of change or gradient loss, we calculate these weight values by taking a softmax kind of average using the rate of change of loss. And this technique gives us the optimal value of the weights that we should have for these individual losses. So, these are the two main contributions of the multitask attention network technique. So, this is all the theory that we need to cover for the M 10 technique. Now, let us have a brief look at the code and the results generated using this technique. Okay, so, the code or notebook for this m 10 would be present in the description box below as well. So, the first thing that we have done over here is implement the data set loader class which we can see over here. Now, this data set loader class is specific to the cityscapes data set, the architecture that we discussed can be applied to a number of different data sets. And for those data sets, we would have to implement those specific classes. Then, we have defined the utility functions model fit which is used to train the model. We also have the different metrics, the confusion matrix and the depth error. And then we have this function called multi task trainer, which handles the complete training of the network. We have also defined the network in this Cygnet class. Finally, we load the data set and train it on the dataset. One thing to note is that the authors of this technique trained the network on 200 epochs with a learning rate of 10 raise to power minus four. However, due to the constraints imposed by candle on the training time doesn't allow us to experiment with these values. Hence, for this particular experiment, the epochs are taken to be 100 and the learning rate is taken to be 10 raised to one minus three. The results of the training are decent and good. However, it would have been better if we use the parameters that were provided by the authors. Let us have a look at some of the results. The left one is the original segmentation map and the right one is the predicted segmentation map. As we can see, there are slight issues that this network faces while segmenting by cycles. But the overall results don't look good. These are the segmentation results overlaid on the image. Now let's have a look at the depth maps the left one is the original depth map and the right one is the predicted Depth Map. For the depth map we can also say there are slight errors here and there but the results overall do seem good. Now, these are the depth maps overlaid on the image All right, so this is it for this video. Alright, so the data set that we're going to use for this video is the kit a 3d object detection data set, we will quickly go over this data set in this video as well. This data set has images that are taken from the front camera of a self driving car as we can see them over here. We are also provided with a LIDAR data that was captured by LIDAR present on the self driving car as we can see them over here. And these are the LIDAR data projected in the view of a camera. So this is the dataset that we are going to work with. Now the cool thing about SFA 3d technique is that this network only uses the LIDAR data to do the 3d object detection task. Hence, the images that present in this data set are not used at all. Another thing to note as well. This data that we're visualizing over here is a processed form of the city 3d object detection dataset. However, for generating the final output of that technique, we are going to use the raw form of the KT data set that is provided in the form of a video feed, the real time video feed and the LIDAR data feed would be used for the final prediction tasks. Alright, so this is all for the data set. Now let us discuss the technical details of this asset with 3d technique. Okay, so as we discussed, we are going to use SFA 3d for the 3d object detection task SFA. 3d stands for super fast and accurate 3d object detection. So SFA 3d as a complete technique does 3d object detection using only the LIDAR data? As an input we provide SFA 3d as a bird's eye view input image to the network. The network predicts the different objects that are present in that image and also classifies them as well. The data collected using a lidar is a 3d data. When we view that 3d data from a top view that top view is also called the bird's eye view of that given scene. Once you provide the network with the bird's eye view image of the LIDAR seen as an output, we get seven outputs. These seven outputs are heat maps of the different classes that are present in the image, the distance of the center of other objects from our ego car or the self driving car, the angles in which the other objects are facing the dimensions of the objects that we have detected, the length width and the height and the z coordinate of the center of that object as we can see over here. So now we have discussed the input as well as the list of outputs that sma 3d provides us now SFA 3d mainly consists of three components, which are key point FPN, the different losses that we're going to use for this network and the learning rate scheduling technique that we employ for the training. Let us discuss these techniques one by one. The first component that we're going to discuss is the key point SPN before discussing key point FPN let us first discuss what is FPN SBN stands for feature pyramid network feature pyramid network is a feature feature extraction technique that given a single input image outputs different feature maps that vary in size. It's something as you can see over here, given a single input image, the outputs are different feature maps of different sizes. So what is feature extraction and feature maps. All the important data that is contained inside an image is what are called features. They can be points, edges, designs and patterns or even human faces. In a convolutional neural network, we have what are called convolution blocks. These convolution blocks consist of different convolution layers and pooling layers. We provide an input image to these convolution blocks, and we get an output image or we can say a matrix. This output image or metrics are what are called feature maps. The process of extracting features out of a given image is what is called Feature Extraction. Now, feature pyramid networks is not a network by name, it's actually a general technique that we can employ on a number of different neural networks. For instance, we can apply this feature pyramid network technique to generate feature maps on a ResNet architecture as well. By post processing these output feature maps, we can do a number of different tasks like object detection, the feature pyramid network technique works in this way. Given this input image, we pass it through a number of different convolution blocks, and ensure that these images get smaller in size. As we can see, over here, the input image is getting smaller in size as we approach the tip of the pyramid. Then we employ successive iterations of convolutional blocks, ensuring that they up sample the given image as we can see over here, the image gets larger in size. As we approach the base of the pyramid network, along with the convolutions that we apply in this part of the network. We also provide information to this part of the network from the previous part of the network, as we can see through the skip connections over here, and we generate the output feature maps from this part of the network as we can see over here. Now, feature pyramid networks are a great technique to employ if we want to create networks that are scale invariant. So what is scale invariant network? Let us assume that we want to predict a ball that is present on an image using a simple convolutional network. If the ball that is present in that image of large size or that ball is of small size, the neural network should be able to detect the ball that is present in that image. This property of detecting irrespective of the size of that object is what is called scale invariance. for tasks like object detection, we need to predict a bounding box that tells us where that object is present in that image. For tasks like these scaling variants is an important property that neural networks are supposed to have. However, for detecting points in a given image, scaling variants does not play a major role. If we want to detect a single point in a given image, that point is going to remain at the same location, irrespective of the scaling that we apply on that image. Hence, directly applying feature pyramid networks to detect points in an image is not an efficient technique. Key Point feature pyramid networks are an extension to the given SPM networks that help us detect points in a given image. The overall architecture of key point FPN remains the same, the outputs of the SPN can be visualized, as we can see over here. These Gaussian shaped outputs that we see over here indicate that the network is really confident that a point is present at a given location in the image. But as we move away from that point, the confidence level of the network goes down, as we can see from the shape over here. So once we have these feature output maps, we convert these feature maps to a similar size. In this particular case, we have taken the reference size to be the largest feature map that is present with us. We upsample the smaller feature maps to match the size of the largest feature map that we have, as we can see in this column over here. Then we calculate a weighted average of This feature maps and get a final single output as we can see over here, the weights of the weighted average are dependent on the confidence value that the network has in this different feature maps. From this feature map, we can take the point that has the highest confidence value and output that as the point that we wanted to detect this complete post processing that we have done after the simple SPM network is what constitutes the key point FPN network. Alright, so this is the complete network architecture of the SFA 3d techniques. Now let us discuss the different loss functions that are employed to train this network. First off, we are going to discuss the outputs that are generated by this network. The first output is a heat map of the different classes. Heat Map is a data visualization technique that tells us the magnitude of a given phenomena occurring on a given input. Roughly, we can visualize a heat map using a mountain terrain. Usually in a mountainous range, we have a mountain that has the highest peak, and has accompanying hills or mountains that are off smaller peaks, then this highest peak Mountain. As we move away from this highest peak, the height of the ground also starts decreasing in all the directions something like this. As we move away from that highest peak, we approach another neighboring mountain or a hill. After the height decreases, the height again starts increasing until we reach the top or the peak of the neighboring mountain. And once again, as we move away from that peak, the height again starts decreasing. If we view such a mountainous region from the top view, and visualize the height of the ground in that image, we get what is called a heat map. If we denote a larger height by a dark red color, and a lower height by a white color, we are going to observe that the highest peak has a solid red color. As we move away from that peak, that solid red color is going to decay into a white color. Again, as we approach another peak, that white color is going to converge into a solid red color. Again, a heat map can also be visualized in terms of confidence of a network that it has on its predictions. In our particular case, if we want to detect three objects, let's say a car, a truck and a pedestrian, we are going to generate three different heat maps. In this case, the red color denotes that there is a car that is present in that image at this particular location, and the network is confident about this location. Similarly, a pedestrian is present in this location, and the network is confident about its prediction in this particular location. As we move away from both these locations, the confidence of the network goes down. Using this heat map, we detect the class of the object whether it is a car or a pedestrian. In order to encourage the network to learn the right classes, we use what is called a focal loss. The mathematical expression of the focal loss is something like this. Focal loss is usually used when there exists a class imbalance in the data. class imbalance usually occurs when there is a less amount of data for a particular class and a very high amount of data for another particular class. Here P T is the confidence value of the network. This scaling factor over here allows the network to focus its learning more on the predictions that it is less confident about and focus less on the predictions that it is more confident about. Next output we have are the distance from the ego car and the direction in which the other objects are facing. In order to learn this, we use what is called an L one loss. The mathematical expression is something like this. elven loss simply takes the difference of the prediction and the actual value and computes the absolute value of that difference. The next output that we have are the dimensions of the object we are trying to detect and its Z coordinate the distance or the height from the ground. In order to learn these four parameters, we use what is called a balanced Elvan loss. Now given a training data, we can divide the points in that data roughly into two categories, outliers and In liars white training a network that data points that generate a loss value that is less than some threshold, let us say one are called in liars. And the data points that have a loss that is greater than that threshold are called outliers. Due to a larger value of loss, the outliers tend to attract the neural network weights towards itself. Due to a smaller loss value, the inliers are not able to attract the neural network weights towards itself that sufficiently well. In order to efficiently deal with such an imbalance of loss values, we use what is called a balanced elven loss. The balanced elven loss balances the loss values that are generated by both of these inliers and outliers to get a network that performs well on both these types of data. Alright, so, these are all the losses that are used in this network. The next component that we are going to discuss is the learning rate scheduling. While training a neural network the learning rate is an important hyper parameter that we need to take care of a particular technique that we apply to better train a network is called learning rate scheduling. During the training process, we tend to decrease or increase the learning rate values in order to fine tune our optimization process. The learning rate decay technique that we use in this network is the cosine annealing, the graph of the learning rate in cosine annealing looks something like this towards the start the learning rate decreases slowly towards the middle the learning rate starts decreasing in a linear fashion and towards the end, the learning rate decrease again starts to slow down. The combination of all these three components is what constitutes the SFA 3d technique, alright. So, these are all the technical details there are to discuss for the SFA 3d technique. Now, let us see this algorithm in action. Okay, so let us have a brief look at the code. The link to this code and the notebook will be provided in the description box below as well. The first step that we have is the configuration step. Here we set up the different parameters and variables that we are going to use for our network. Next up, we have the different data processing functions that we use. One such data processing technique is to convert the LIDAR data into a bird's eye view of the given data. Next up, we have different utility functions like logging that we use to log the training process learning rate scheduling that we use while training the network and different visualization utility functions that we use to visualize and generate the final output. Next up in this section, we define the different loss functions that we are using to train the network. Finally, we define the FPN ResNet network over here in this section and in this section, we finally train the neural network. One particular problem that 3d object detection network face is training. The training of these networks is really time consuming and require very expensive and good quality hardware. Since we have a time limit and some constraints on hardware on Kaggle I was not able to train the network by myself. Hence I have used the pre trained weights that are provided along with this network architecture. So now let us have a look at the final output. So this is the final output that we have on the raw data set for the purpose of visualization I have slowed down this video. Okay, so some key observations that we can note over here, red boxes describe the cars that are in the same. Blue bounding boxes are for cyclists that are present in the scene and yellow bounding boxes. for pedestrians on the given scene, one interesting observation is that the network is not able to detect that tram that is present in this image, since it was not trained on that data. Also, these blue lines describe the orientation of the various surrounding cars. Alright, so, this is all that is to discuss of the SFA 3d technique. So, let us start with the problem definition. In this problem given a set of images taken from the different cameras present on a self driving car, or a robot, we are required to process these images and output a bird's eye view of the given environment. Bird's Eye View or top view are viewing the 3d scenes from the top as an input, we can either have a single image or we can have multiple different images taken from the different cameras. In this case, we can see this is our self driving car. And in this case, for our particular problem, we have four cameras in the front, left, right and the back. Given these four images, we are required to process them and output the bird's eye view of the environment around the car. As you can see over here, the red rectangle is the ego car or our self driving car and the other blue rectangles are the other cars. Now why do we require to calculate this bird's eye view of the environment? There are mainly two reasons we would like to calculate the bird's eye view. The first application is object detection. Calculating a bird's eye view of the given environment provides us with a different way of doing object detection compared to other different computer vision methods. The second or the more important application is path planning. This bird's eye view provides us with a concise representation of the surrounding environment, the different cars and objects that are present in the scene and the road that we are required to go on. Using this concise representation we can easily implement path planning algorithms on our self driving car. So, this is the complete introduction to the problem statement. Now, let us have a look at the data set that we are going to work with in this project. Alright, so the link to this data and this data visualization notebook would be provided in the description box below. Just like in the problem description, we are given four images, the front, the rear, left and right. Along with that, we are given certain bird's eye view images. Now the images that we have been given are not exactly real time images, but semantic segmented images. We already have the different classes in these images, semantically segmented and we have to derive a bird's eye view image for the same. The cool thing about this dataset is that the data set is derived from a simulation, the self driving car drives around the simulation environment. And we captured the different images from the four different cameras that are present on the self driving car. A drone flies just above a self driving car in order to capture the ground truth bird's eye view images. As we can see, we have these four images taken from the self driving car cameras, the front image, the rear image, the left image and the right image. And this is the ground truth bird's eye view image of the 3d environment. This next image the bird's eye view occlusion image contains occlusion as a separate class. This means that the scene that is hidden or occluded by different objects in the environment are represented using a separate class. As we can see in this gray color over here this car present behind our self driving car is blocking the view of the camera in this gray space. Additionally, some buildings are also occluding the space as we can see over here, and due to this occlusion, we are not able to see this car that is present behind the building and is not visible through our camera. This homography image is similar to the bird's eye view image that we have over here, but contains a lot of noise. So what is the homography operation? In simple terms using a homography operation, we can convert from one image plane to another image plane. In this particular case, we are converting from these four images, the front rear left and right and using these to generate a bird's eye view image. This Bird's Eye View conversion operation is also called inverse perspective mapping. More on this homography and inverse perspective mapping will be discussed in the later part of the video and we will understand why we do not use this technique directly to derive the bird's eye view image. Now, let us have a look at few instances of this data set. Alright, so, the technique that we are going to apply to solve this problem is unit access T. As the name suggests, this technique is an extension of the unit semantic segmentation architecture. Therefore, the unit access T has two components the unit architecture itself and its extension let us discuss the two components one by one. So, the first component is the unit network unit is a network that is used for semantic segmentation. In the problem of semantic segmentation, we are required to segment a region of interest from a given image. For instance, in an image captured by a self driving car, we are required to segment the cars that are present in the image and the road that is present in the image. We can do so, by highlighting them with different colors. The unit architecture is similar to other semantic segmentation architectures where we have an encoder part of the network and we have the decoder part of the network. The encoder part down samples the image deriving different visual features in the image and then we have the decoder part which up samples the image and returns us the output apart from the information derived from the decoder part itself, the decoder uses information from the encoder part using skip connections as we can see over here. Initially, unit architecture was devised for biomedical image segmentation. However, its use cases applies to other domains as well for instance in our case self driving cars unit forms the basic architecture for this technique. Now, let us discuss the Xs D are the extension part of the unit network. The extension part of the technique is that we make use of what are called spatial transformers. So, before discussing what are spatial transformers, let us discuss the concept of homography. For instance, we have an object and we take its image from a camera that is placed over here, we change the camera position as well as its orientation to something over here. And we take the image again, the transformation between these two images is what is called homography. In mathematical terms, homography is a simple matrix multiplication. A cool application of mammography is that if we know the homography matrix that converts from one image plane to other image plane, then from a single image, we can directly compute how that object would look like in a separate image plane without even taking a different picture. However, in application, we would require those image planes to be close to each other. In general, the transformation need not be that complex, they can be as simple as translating the camera plane or even rotating the camera plane at a given fixed point. For instance, given this image, we are rotating the image plane by approximately 15 degrees. Not theoretically, it would seem that we can simply take an image x, multiply it by homography matrix H and get the final resulting image at x. However, it is not that simple. In simple terms, homography is a simple transformation that map's pixels from one image to another image. Hence, what we would have is this image pixel mapped to some image pixel in this image over here. On doing this matrix multiplication, it is easy to observe that the output x x is not going to output integer values. What this means is that a given pixel over here is going to map to a number of different pixels over here. Hence, in order to apply this transformation, we are going to have a lot of different average calculations made Making the final image look not really good. Hence, a solution to this is to do an inverse homography calculation what that means, instead of computing a transformation that goes from input to output, we instead calculate a transformation that goes from the output pixel to the input pixels. For instance, this output pixel over here is going to map to the input image in a rotated fashion something like this over here, we calculate an average of the surrounding pixels in the input image. For instance, this pixel is going to map to surrounding these four pixels over here. Hence, we are going to calculate their weighted average pixel values to derive the value of this pixel. This weighted average is calculated using a technique called by linear interpolation. The weighted average is taken with respect to the distance from the surrounding pixels. In place of by linear interpolation, we can apply other different techniques like linear interpolation or nearest neighbors as well. However, for spatial transformers by linear interpolation is the choice due to its differentiated by nature. So, having understood this complete process of homography, let us discuss the use of this homography. In a spatial transformer. Spatial transformer is an extension module that can be applied to any convolution network. This enables the convolution network to also learn the different spatial properties that are present in the dataset. The spatial transformer learns the different parameters of the homography metrics in order to aid the convolution network. In the same example, if we have an object over here, and we take its image from this point, and let us assume for some reason, the CNN architecture is not able to classify this object accurately, how spatial transformers are going to help the spatial transformers are going to learn the homography matrix parameters in such a way that they present an image to the CNN from a different point, so that the CNN is able to classify that object more accurately. So having understood what is a spatial transformer, and how it helps, let us see how we apply this to a unit xs D technique. For the unit accessory technique, we are not going to learn the parameters of the homography matrix. Instead, we are going to provide the spatial transformers with a fixed homography matrix. So where do we get these homography parameters. Since homography, is a general application, we can also apply this to our self driving cars scenario as well. For instance, given this image from a right facing camera, we can easily derive a homography that maps from this right camera to a virtual camera that is present at the top. We can do this for all the four cameras and derive a top view of the environment. In this way, we will easily get the homography parameters. This technique of deriving the top view from these given images is also called inverse perspective mapping. But there's a natural question to ask why we apply complex network architectures like unit access T, whereas we can solve our problem using simple homography ease. The answer to this is the top view homography or inverse perspective mapping makes an assumption that the world around these cameras is flat. We can easily observe that this assumption does not hold true in any case whatsoever. The cars the trucks, pedestrians and the different monuments, they have considerable height in order for the inverse perspective mapping to work. Although we can get a top view of the environment by applying and inverse perspective mapping, however, the results that we get from that technique are not really accurate and useful for our task. Hence, we need such complex architectures like unit SST Alright, so the two components are unit access D are not discussed. Now let us see how these two components combine to give the final architecture. For this network, we are going to have four inputs for the four different images that we have as you can see over here, we are going to have a similar encoder network for each of these four images as can be seen from the red lines. For the decoder, we again have similar architecture, however, there is a single pipeline that gives us the final output. These skip connections over here are what makes use of spatial transformers. These four feature maps at different levels are combined using spatial transformers. Each feature is first pass through a spatial transformer. And then these different features are combined. This combination is then fed through the skip connection. And this is done similarly for each level of the architecture. Alright, so this is it for the technical discussion of unit SSD. Now, let us have a brief look at the implementation and the results that we get from training this network. Alright, so the link to this implementation would be provided in the description box below. In the first section, we have implemented the different utility functions like load image, and one hot conversion of the input image. In simple terms, what one hot encoding means, given a semantic segmented input that has various different colors, we convert that RGB image into a 3d matrix that contains a sparse representation of the different classes. This representation is helpful for the machine learning model to learn and understand the data. In the next section, we have the different configuration parameters. One thing to note is that there are certain changes that have been applied to these parameters in order to obey the constraints that are provided by us on Kaggle the image shape has been reduced in order to get a faster learning curve. The batch size has also been increased in order to get the same results. Also, the algorithm is only trained for a total of 40 epochs, whereas you would have required to train 400 epochs. In this next section we have the different data loader functions that load the data for our machine learning model. In this next section, we define the different network architecture. First we define the spatial transformer. Next we implement unit along with the spatial transformer extension. And finally, we train the network. Now let us have a look at the final output predictions. Just like before, we have images from the four cameras on the self driving car, the front rear left and right this next image is the output prediction generated by our model. And the next image is the ground truth bird's eye view image. As we can see the model has learned to some extent to represent the bird's eye view image. However, there is some slight noise that is present in the output let us have a look at other images as well. Overall, we can say the model has learned a representation of bird's eye view image, but it would require more training to converge to more better results. Alright, so this is all that we had to discuss for this unit access the technique. Congratulations on completing the course we together did seven different projects and got to learn more about the different concepts applied to perception in self driving cars. We did three projects related to 2d and then did four projects that were related to 3d. In order to follow up with this course. I recommend that you go over different datasets apart from the ones discussed in this course, read and implement new research papers related to perception for self driving cars. Or you can jump due and learn more about different modules for a self driving car like localization or motion planning. As far as computer vision is concerned, you can also try out different other projects like similarity learning, image captioning and generative modeling. This is all that we had to discuss for this course. And I will see you in another video or another course till then bye