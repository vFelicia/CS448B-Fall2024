apache cassandra is an open source nosql distributed database this beginner's course has four modules the first three modules cover the basics of apache cassandra data modeling the final module covers practical applications of cassandra if you want a broad overview of nosql databases you can check out the previous nosql database course on our channel this module will cover the basics of apache cassandra data modeling upon completion of this module you will be able to differentiate between the relational databases and an apache standard database recognize the basic terminologies used in cassandra tables explore cql fundamentals identify how partitions primary keys and clustering columns are used in cassandra tables observe the concept of replication and consistency upon completing this lesson you will be able to understand how relational data modeling concepts compare to those used when data modeling for apache cassandra now you will learn about some differences between relational databases and apache cassandra the cassandra data model follows cassandra modeling methodology while the relational database follows the sample relational model methodology in cassandra modeling we think about the application needs when building the data model since queries are the driving force for this model those queries must be conceptualized before data entry whereas in a relational model entities are the driving factor in relational modeling primary keys are used to ensure uniqueness of the entities in cassandra modeling primary keys take on an additional significance particularly in determining performance on a large scale in order for us to provide high performance and availability on a large scale cassandra uses a distributed architecture cassandra by default does not support asset transactions or joins but uses denormalization instead cassandra does not enforce referential integrity across tables here's a sample of a relational data model this is one of many types of relational data models this model produces conceptual logical and physical data models including steps for normalization and optimizing the data model note that queries aren't considered until the very late stages of the process let's now look at the apache cassandra data modeling approach here we still have conceptual logical and physical data models but the application workflow and inquiries are considered earlier in the process this analysis is incorporated early into the logical data modeling phase rather than waiting until the end of the process let's look at the difference between the modeling approaches in the relational approach we first think about the type of data create a data model based on the data and then think about our application last the developer can then make queries to retrieve this information in the approach followed by cassandra the process is reversed here the application of the data must be decided up front in other words what queries are going to be performed that will dictate how your logical model will take shape once your queries and data model are complete you can load the data into your tables relational databases are asset compliant by default acid is used to describe the transactional behavior in a relational database and stands for atomicity consistency isolation and durability atomicity refers to the integrity of the database transaction either all the statements in a transaction will succeed or none of them will consistency means that the data within a database should comply with certain rules transactions cannot leave the database in an inconsistent state the new database state must satisfy integrity constraints isolation refers to the ability to process multiple transactions at the same time without interference durability refers to the ability of a completed transaction persisting even after a subsequent failure apache cassandra does not support asset properties like most relational databases in database management both commit and rollback can easily be achieved using transaction logging when a single server is involved but when the data is spread across servers in distributed databases like cassandra the situation becomes tricky the locking and coordination required to ensure these twophase semantics of acid can cause severe performance penalties however cassandra does provide atomicity isolation and durability when performing read and write commands on a single row additionally it also provides something called tunable consistency which can be adjusted depending upon the number of nodes which have to agree to execute a particular read or write command to understand tunable consistency let's look at the cap theorem first given by eric brewer this theorem talks about the tradeoffs in implementing a distributed database all distributed databases are primarily tolerant to partitions hence the choice is really between consistency and availability cassandra by default is an ap system which means it chooses availability and partition tolerance while sacrificing consistency however it does provide the option to make the system more consistent by sacrificing availability this is referred to as tunable consistency wherein the consistency of the database can be adjusted or tuned by adjusting the number of nodes which have to return a read or write request cassandra primarily operates under the ap model because all data eventually becomes consistent one more point of difference between cassandra and relational databases is the support of joins to recall joins are a statement often used by relational databases to combine data a join statement facilitates the query through the joining of data from different tables take a look at the example below the join statement combines results from two tables namely videos which stores the details of videos and comments which stores the comments made by different users on the videos this is done by the matching of video id as specified in both the tables the database matches the video id for each video and generates a result this is the output of the join command the select statement in the given transaction selects the column specified from the output of the join command and the where statement is the filtering criteria therefore the output of this transaction only provides the comments of the video titled interstellar the basic requirement for a join command to work is that the results from two combined tables should be stored at one location however in distributed databases the different partitions of data are stored on different nodes hence joins would have a negative impact on the latency as the scale of the databases increases and the random distributions become more prevalent unlike relational databases cassandra does not support joints but uses denormalization instead denormalization ensures that all the required information is confined to the appropriate table without having to perform joins organizing data like this can duplicate data in multiple tables which is cost efficient and allows for faster retrieval of said data while using cassandra to organize the data the developer can decide upfront what tables they want their data to be in in this example we have two scenarios in which we want to access comments first a video view where all the comments for a particular video can be viewed and a second a user view that displays comments made by a particular user to support this two tables must be created the first table comments by video supports the ability to access comments for a specific video here video title is used as a partition key the comment id helps in defining a unique row in the database for each comment the second table comments by user supports the same data but has a different partition key here the user id is used as the partition key note that these two tables reflect the common naming convention we have for cassandra tables we append by something to the table name to convey some idea of the query that the table is designed to support joins rely on referential integrity constraints to combine data let's look at this point of difference between relational data modeling and cassandra data modeling ri refers to the relationship between tables a value in one table requires the same value to exist in another table if there is a user in the user by email table then the user must also exist in the users table and vice versa this referential integrity is required between tables such that a user is either represented in both tables or none of them and also requires for a join to work properly cassandra does not enforce referential integrity because this capability requires a read before a write it is not considered an issue that has to be fixed on the cassandra side although it is a feature that improves performance and accessibility it is considered a developer prerogative referential integrity can be enforced in an application design but that means more work for the developers upon completing this topic you will be able to differentiate between the terminologies used in apache cassandra tables understand the different data types available in cassandra cassandra uses database terms that have specific connotations in the context of data modeling the key terms are data model key space table and partition a data model is an abstract model for organizing elements of the data data models vary depending on the type capability and purpose of the database the type of the data model depends on the capability of the database for cassandra the data model is based on the queries that you may want to perform a key space is the outermost logical container of tables it stores tables and replication data in other words it's a container for replication a table is a combination of rows and columns the tables are contained within the k space as a columnar database cassandra stores data based on partitions partition is a raw rows of data that are stored on a node in the data table based on the partitioning strategy each row in the partition consists of key and value pairs cassandra stores and retrieves data based on partitions a partition is a terminology where you can make or break the data model a primary key is the most important part of the data model it does two things first it guarantees the uniqueness of the data and second it defines the placement of the record in the cluster this allows for easy access to the data in the model a partition key is the first part of the primary key it determines where in the cluster your data will be stored on which node it will be stored a partition key is a left hand column in the graphic indicating one two or three respectively inside each partition there are rows and columns columns are stored locally and referred to as cells in the table a row is contained inside a partition there can be more rows and they are stored together on a partition clustering columns define the order of data within a given partition the default sorting is in the ascending order upon completing this lesson you will be able to explore the basic cql concepts within an apache cassandra database cassandra query language or cql is a nosql language also known as not only sql let's learn about some basic cql concepts including building blocks such as key spaces tables and the primary keys as well as basic cql commands such as select truncate alter table and source the key space is a top level container in apache cassandra to organize a related set of tables it is very similar to a relational databases schema cassandra key space defines replication settings that describe how many copies of a given piece of data are being stored within a cluster once you've created a key space in cqlsh you may want to select the key space that will be used for your subsequent command you can enter a use command followed by the name of the key space if you do not select a key space with the use command you'll have to include the name of the key space whenever you reference the name of a table this helps cassandra interpret your request let's take a quick look at the syntax for creating a table the code here shows the statements for creating two different tables remember the key spaces contain tables and tables contain our data note that every cassandra table has a primary key inside each key space is the table the primary key clause is a unique identification for each row within the table cassandra's primary keys uniquely identify rows of data in this example we see a uuid used as a partition key to uniquely identify each user in our users table however a partition key alone is not enough to ensure uniqueness of a row it is the primary key clause as a whole that determines uniqueness of a row within a partition the cql select command is used to read data from cassandra tables selecting all the rows of the table using an asterisk notation causes cassandra to perform a full table scan this could be very expensive on a table with a large number of rows instead use the limit keyword to get a subset of the rows cassandra also supports a paging mechanism which indicates specific columns we desire to read by name or uses the asterisks to indicate all columns cassandra also supports a couple of builtin aggregation functions such as the count function for counts on larger tables you may want to consider using the spark part of disk analytics another command you'll find useful is the truncate command this command causes all rows to be deleted from a table while leaving the schema in place this command should obviously be used with care as the data is removed from persistent storage and you would have to restore it from a backup to retrieve it truncate sends a jmx command to all nodes to delete ss tables that hold the data if any of these nodes are down the command will fail while working with cassandra you'll most likely encounter situations when you'll need to make changes to your table this is where the altered table command comes into play you can change the data type of a column add columns drop columns rename columns and change table properties but you cannot change the columns in a primary key clause here's an example code for adding and dropping columns from a table the last command is the source command which allows you to execute a set of cql statements from a file to execute a source command you'll want to put the file name in single quotes cqlsh will output the results of each command sequentially as it executes upon completing this topic you will be able to learn about the basic concepts of partitions table partitions partition ordering partition distribution and partition storage to truly understand the data modeling you must master partitioning concepts partitions give you an indication of where your data is in your data model as well as in the cluster we will go through some of these concepts in depth partitions partition keys composite partition keys and clustering columns here are two commands that can be executed on a relational database as well as on apache cassandra in a relational world these queries should work on a video table because of joins here common queries are title equals and added date is less than if you run this in a cassandra database the output appears as an invalid request this is because the partition keys absent from the equality clause of the search expression these commands fail because joins are not supported you've already seen the structure of the storage engine in lesson two this diagram demonstrates how the data is translated from the storage engine into a cql table when used programmatically that is when we select data from the table we see it in rows and columns just like we use in our application determination of the partition key is a critical step if you look at this diagram closely we have the primary key as the id there is just one column in the primary key the partition key is always the first value in the primary key this module will cover the basics of a cassandra data model upon completion of this module you will be able to explore the concept of denormalization and investigate collection counters and user defined data types upon completing this topic you will be able to explain how joints are used in relational data modeling relay the concept of denormalization used by cassandra you have already learned that the relational data model supports joins and apache cassandra supports denormalized tables here you see three tables named videos users and comments each table has unique information in it this is a typical relational example we're going to make all these work together by running a join query to see how they can be used in relation to each other join query joins two tables to work as one let's assume that you want to find some comments for a particular movie the first table titled videos store details of videos and the second table comment stores comments made by users on all the videos we need to find a particular entry which has all the data collected from both the tables let's say we are searching for a comment from the videos table how do we access the comment from the comment table if you just have the video with us this is possible using the join query they are often used by relational databases to organize data the two tables shown previously are joined and this is how the output looks like the tables are merged with the help of the video id specified in both the tables that is how relational databases work in the code the title is interstellar the select statement in the given transaction selects the columns specified from the output of the join command and the bear statement specifies which values to select from the specified column now this can be used to do a join on two tables and get comments for that one title this is how rdbms or relational databases are designed in the output of this transaction we will get the comments of the video titled interstellar that is how the relational databases were designed to be used having multiple tables and then using queries to merge and bring outputs you've just learned how the relational data model works with joins now it's time to see how cassandra works without joins let's look at these two tables closely they look similar but the primary key is different for both in the first table comments by video the video title is the partition key and the comment id is the clustering column and in the second table comments by user the user id is the partition key and the comment id is used as a clustering column these are the denormalized tables used in cassandra of the same joins that were used in the relational data models upon completing this topic you will be able to learn how collection columns and collection types are considered understand how to create and use udts learn about counters you will now learn about some additional data types supported by apache cassandra for efficiency these data types are collections counters and userdefined data types or udts these data types simplify table design optimize table functionality store data more efficiently and might even change the design of the table completely now you're going to learn about collections cassandra provides collection types to group and store data together in a single column take for example a relational database or a cassandra database to understand this concept consider that in a relational database you have to group multiple email addresses of a user meaning there will be a many to one joint relationship between a user table and an email table but cassandra awards joins between two tables by storing the user's email addresses in a collection column in the user table each collection specifies the data type of the data held the collection data structure allows you to store multiple values in each cell while retaining the value all collections are multivalued columns the maximum number of elements in a collection is 64 000 which in practice can be dozens or even hundreds collections are designed to store a small amount of data the maximum size of an element is 64 kb which in practice is much smaller than the number of elements in collection they are also retrieved in their entirety a collection column cannot be part of a primary key a partition key or clustering columns you cannot nest a collection inside of another collection if you want to nest a collection inside another then you have to use the keyword frozen more on this later there are three collection types in total set list and map respectively let's begin with the first collection type set it is a list that stores the typed collection of unique values it is stored on order but it gives back the data in a sorted order let's look at an example of set here a user's table is created with an id first name last name and a set of email addresses the data type is represented in angle brackets so in this example text data type is used after that the values are inserted into the table here there are two email addresses because it is common for a single user to have two email addresses list is like a set it groups and stores values in a single cell the difference between list and set is that in a list the values does not have to be unique and there can be duplicates lists are stored in a particular order and are accessed by an index here's an example of how the users table from the previous slide is altered to insert a list of frequent destinations first the table is added to list of text data types then the user cas one two three is updated and berlin london and paris are added to the list list uses the set command to enter the values for the respective collection type map is the last of our collection types it allows us to enter values which are in relation ship with one another in a key value format map is ordered by unique keys both the key and the value have a designated data type map uses the set command to enter the values for the map collection type let's look at an example here here the table users have again been altered with a map called to do with the data types of timestamp and text then the user cast 123 is updated and a map todo list was added the todo list is a set of dates and todo items in this case the user cas123 is going to create a database on january 1st load and test the database the next day and then a month after move the database to production here's another collection type frozen if you want to nest collections inside other collections you will have to use the frozen keyword without using frozen you cannot nest a collection inside another collection frozen allows us to serialize multiple components into a single value essentially frozen will turn that value into a blob if you want to update a single value in the collection you can't use the frozen keyword now it's time to explore user defined types popularly called udt udt allows you to group together similar fields of data that are both named and typed udts can include any supported data types including other collections and other udts multiple data fields can be attached each named and typed to a single column udts allow embedding of more complex data within a single column adding flexibility to your table and to your data model let's look at some examples here a udt is created which has street city zip code and a set of phone numbers now all these fields can make up a single address but in a relational world it is possible for several users to have the same address so a new udt or full name is created which has the user's first name and last name now let's look at how the udt is used in a table first a users table is created which includes name set of direct reports and a map of addresses in the name column the full name udt is used after that a set collection type with full name udt is used to create a direct reports column and finally address udt is used in a map collection in relational databases to find a query you will have to first join these two tables together but in cassandra's denormalization technique the address is embedded in a single cell of the user's table a counter is a data type where you can have a column that stores a 64bit signed integer they could be incremented or decremented and the values are changed using the update command counters need to have a specially dedicated table that can only have primary key and counter columns counters can lead to the duplicate data in multiple tables you can have more than one counter column in your table let's say we own a range that specializes in cows and we want to count the moo each cow makes we're going to create a table called moo count it has the column name cow name which is also the primary key of the table and the moo count is the counter column now to add to a moo count we will use the update command to update the mu counts table each time we hear a move in this example we updated the mu count to increment the value by 8 where the cow name is betsy upon completion of this module you will be able to learn about the different models in the data stacks modeling methodology upon completing this topic you will be able to understand the purpose of conceptual data model and learn about the cardinality attribute keys er model and relationship keys let's begin with the conceptual data model it is responsible for determining the attributes of all the objects in your domain and analyzing how they are related a conceptual model defines what a model should contain for example users comments videos ratings etc the purpose of a conceptual data model is to identify what type of data is contained in the model itself from here you can extrapolate the essential objects and constraints that the data types would additionally require it is important that you capture all different views of your model while conceptualizing your data model collaboration with other departments and experts is the key as a database architect it is critical that your solutions serve your business needs you will not always be an expert on the business domain and therefore it is important to interact with nontechnical people to get hold of the details both the technical and nontechnical members can share review and agree upon the conceptual data model business experts can then verify and ensure that the conceptual data model is correct and everyone is on the same page collaboration between different technical roles is also immensely critical apache cassandra requires database administrators and developers to work closely with each other the conceptual model is a way of bringing together the technical and nontechnical people to the same table the conceptual data model offers an abstract view of your domain it is generally independent of technology meaning that it does not specify what technology to use for example sql or nosql at this point it is not specific to any database either the term cardinality often refers to the relationship between two entities in a given data model or in other words how many instances of an entity are related to an instances of another entity cardinality tells how many times an entity can or must participate in a relationship other possibilities are one to n and one is to one here the relationship between the video entity and the actor entity is the character name in the video and the actors who played those characters here m and n are the videos and actors respectively the videos feature actors each video will have several actors and each actor can participate in several videos attribute types are the fields that store properties about an entity or relationship attributes or fields where we wish to define our entity for example we may want to store the title and description of each video actors of names which we can further break down into other attributes like first name and last name respectively key attributes uniquely identify an entity two videos can have the same name or description but the id is unique to a single video and the names are the unique key for actors let's look at the entity relationship model here we have an entity type the relationship types and attribute types and first we need to identify the entity types there are different entity types and videos these are things you're looking for to find information attribute types will look at actors and define their first and last names respectively the relationship type looks at the relationship between the video and the actor attributes can have multiple values and we call these not surprisingly multivalued attributes multivalued attributes store multiple values per attribute one can refer to these attributes with a double oval diagram around the attribute types for example one video can be an example for more than one genre now you will learn about the three relationship keys one to one one to many and many to many respectively in the onetoone relationship we use either of the participating entity keys in this example a movie can have one defined date for its first showing so this is an example of an onetoone key relationship one to many relationships uses the entity key on the many relationship side here we have one user but we can upload multiple videos manytomany relationship combines both the entity keys here many actors can be part of multiple videos weak entity types are entities that cannot exist without a corresponding strong entity on the other side for example if the video vanishes or disappears then the encoding will disappear too upon completing this topic you will be able to learn the concept of workflow and access patterns you will learn about application workflow which describes how users will navigate through the application it also helps in determining which queries will perform against an apache cassandra database every application has a particular workflow access patterns help us to determine how the data is accessed we also get to know what queries to run first for example when a user logs into a website what are they going to click what data are they going to use what data are they going to read how will a user go through a website application the workflow and access patterns must be thought through before building the application think about how the application should work and how users should use it for example you may want one query to run when a user logs into the site this query will perhaps find the user with a specified email address once the user is in the site you may want to run the second query to show the latest videos uploaded to the site you may want to run a third query to verify the user's login for them to access their account details is indeed working or not a fourth query can be used to find videos uploaded by users with a known id a fifth query can help you find a video with a specified video id these are just examples of popular queries one can build into an application upon completing this topic you will be able to understand the mapping rules explore a modeling approach to see how mapping rules are applied learn the concept of chaboko diagrams understand the logical and physical udt diagrams and learn about the four data modeling principles at this point we have covered conceptual data model and the application workflow next we will look at how to make a logical data model to do this we have to map our conceptual model to a logical model now you will go through some basic rules for query driven methodology the mapping rules ensure that a logical data model is correct each query has a corresponding table the tables are designed to allow queries to execute properly the tables will return the data in the correct order here are the five mapping rules to guide a query driven transition mapping rule 1 identify the entities and their relationships mapping rule 2 identify the equality search attributes mapping rule 3 identify the inequality search attributes mapping rule 4 identify the ordering attributes and mapping rule 5 identify the key attributes create a table schema from the conceptual data model for each query apply the mapping rules in order for this schema we're looking at videos by user upload therefore mapping rule 1 identify your entities and relationships in this case we have a table named videos uploaded by users there is a relationship between two uploads with users being one entity and the videos being the other mapping rule 2 identify your equality search attributes in this case clearly equality is the user id mapping rule 3 identify the inequality search attributes in this case it is the uploaded timestamp mapping rule 4 identify the ordering attribute mapping rule 5 identify the key attributes needed to guarantee uniqueness and minimality here video id guarantees uniqueness we have the conceptual data model on the left and the access patterns on the right it is important to remember that our application workflow had access patterns based on the queries which we wish to execute on our apache cassandra database this is an important part of the application workflow to get a logical data model we combine the results of the conceptual data model and the access patterns after combining them some mapping rules and patterns are applied this logical model is described using a chiboko diagram a diagram named after artem chaboco who devised his method of mapping to a logical data model chaboco diagrams graphical representations of cassandra database schema design they document the logical and physical data models respectively you already know your entities that is the video and the key attributes at the bottom query one is to find a video with a specified video id now if you look at the chiboko diagram we can see that q1 leads into a table which can satisfy this query we will get into that in more detail but first we have to understand some of the terminologies used here first and foremost this diagram looks very similar to an application workflow we have a queries which lead us into the tables we have the table diagrams which are similar to the tables we have in our cassandra database we have the query list at the bottom right as you know queries are the king and the queen while designing the cassandra data model and lastly we have our udt's which stand for userdefined data types these are a lot like cassandra tables but without a primary key and these can be nested into other cassandra tables on a logical level the cheboko diagram shows the column names and properties on a physical level cheboko shows the column data type in order to move from the logical tables to the physical tables we just need to add column data type followed by a bit of optimization if needed here is an example of how a table is broken down by column name data type and type of column as you can see in our table we have some abstract column names on the left and the corresponding descriptions on the right note that the notation on the left corresponds to the description on the right that is column 7 which is a list is in square brackets similarly column 8 a set is enclosed in curly brackets also a map like column 9 is in angular brackets and so on however the key thing to notice here is that all these columns are followed by their cql types which we have previously studied it is the physical types that makes the table physical we can now write our create table commands using these column names and their respective types into our database also note that column one is a partition key which is represented in the table as k similarly column 2 and 3 are clustering columns which are represented by c followed by an upward arrow or downward arrow representing the ascending order or descending order respectively a static column is specified by s secondary index column as idx and the counter column as a plus plus here you will learn about logical udt diagrams in detail udt is a data type and udts can be nested within one another for example there is an encoding column in the videos table this column represents our data is stored in the database there is more information about the data than just simple text hence it is encapsulated and it's stored in the encoding udt it has attributes of encoding height width and bit rates the same is true for actor name which encapsulates the first name and last name of the actor and stores it into the udt column if we want to have a single column to store both first and last name or have them stored differently it is important to remember that udt can be very helpful in such cases here is the same diagram but with the cql types explicitly specified is a representation of the physical layer udt diagrams please notice how real cql types have been specified for each column this is a classic example of a logical data model in the form of a detailed chiboko diagram for the killer video domain take a moment and carefully observe this diagram for all the concepts that we have covered so far we have queries which access our access patterns are tables with different kinds of data an application workflow that links these queries to the tables and are encoding udt defined here are the four main principles of cassandra data modeling know your data know your queries nest data and duplicate data we will look at each of these principles in more detail understanding the data is key to a successful implementation of a database a proper understanding of data provides the understanding to organize the data as well as we have seen space and time are often are primary concerns while data modeling in cassandra we sacrifice space in order to reduce time hence organization of data is of utmost importance the conceptual data model provides us with the data capturing information once we define what is to be stored we move on to create a database this not only helps in successful organization but also helps in preserving the properties of the data that is required part of understanding the data requires understanding a primary key as well to recall primary keys take care of two essential things uniqueness and satisfying queries entity and relationship keys affect the table primary keys primary key uniquely identifies a row entity and a relationship let's look at the following tables for example the video tables as video id as its primary key here the primary key makes no attempt at organizing the data so as to satisfy queries at later stages on the other hand the table videos by user as a primary key along with clustering columns which specify ordering in order to help with queries that result in returning the results in a specified order the relationship cardinalities as well as the data cardinality are very important when it comes to data modeling data cardinality helps in selecting an efficient partition key whereas the relationship cardinality helps in satisfying the queries queries are the center of data modeling in cassandra and captured by the application workflow models each entity is created with queries in mind in cassandra it's always better to reduce the number of partitions that are to be read for a query result table schema also changes if the queries are changed to achieve this it is extremely important to clearly understand the kind of data required out of the read operations single partition per query is the most efficient access pattern entities in cassandra are designed in such a way that queries can generate the results with the minimum number of reads ideally the data should be organized in such a way that each query can retrieve the results from a single partitioned search partition can be single row or multirow respectively in this example if we want to query for a particular partition we can give the partition key then the node having the partition is determined and the data is retrieved by the node and returned to us no additional filtering or searching the entire cluster is required however there are cases when multiple partitions have to be accessed to retrieve the results and although this is a less efficient way of doing things in cassandra it is acceptable to some extent as long as these kind of queries are limited in number however the strict antipattern that has to be avoided at all costs is asking the database to perform a linear search through all the partitions of all the tables to fetch the result for a particular query since most of the data is stored in distributed nodes it is highly inefficient to go through all the data stored at multiple locations and it defeats the whole purpose of having a horizontally scaled database after completing this topic you will be able to learn the concept of data nesting and explore the three ways of nesting data until now we've talked about creating a logical data model by combining the results of a conceptual data model and accessing the pattern of application workflow by applying mapping rules and patterns let's now create a logical data model containing tables for each query capturing entities and relationships from the conceptual model the working rule with apache cassandra is to group as much data as possible on disk this grouping of data is referred to as nesting of data and is the main data modeling technique for a cassandra data model nesting helps in organizing multiple data entities into a single partition and it also helps support partition per query during data access in cassandra queries are the king and the queen schemas are often driven through these application queries there are three main ways of nesting data in cassandra these are clustering columns collection columns and user defined type columns clustering columns are the primary data nesting mechanisms look at the actors by video table on the right we apply a query to filter by video id but we nest the actors by their actor name and the character name using clustering columns the partition key identifies the entity that other entities nest into while the clustering columns identify the nested entities when a table has multiple clustering column the data is stored in a nested sorted order multiple clustering columns implement multilevel nesting in the given example actors are identified by videos using clustering columns udts are a secondary mechanism for nesting data usually this occurs when there is a onetoone relationship you also have lists of maps or your choice of userdefined types within a single column working with udts is much easier than working with multiple collection columns for example we turn videos into a video type udt with id title description and other parameters then we put all those videos in a collection column in the videos by user table table videos by user tests all videos as a collection in the videos column with the video underscore type type partition per query and data nesting may result in data duplication in relational data models the tables are joined as we read the data in cassandra however instead of wasting time on reads the data is taken and returned to the client application for example we have three tables that represent the video's object videos by actor videos by genre and videos by tag this duplicates data across tables partitions and rows that means every time we want to store a new video we have to write the record into each one of these tables to keep them in sync upon completing this topic you will be able to learn how to use data types in a table understand the ways to load data into the table and explore some of the features of cql copy here's our data modeling methodology again we've created our conceptual model and our application workflow we've also applied the mapping rules created a logical data model and now it's time for us to create our physical data model to make a physical model add data types to each column of a logical model physical models also optimize efficiency and performance ensuring that the partition sizes won't grow too large and will return the queries quickly here we have a logical version of commons by user and a physical version of comments by user as you can see the physical version of cql data types like user id is uuid posted timestamp is of time stamp data type video id is of time uuid data type comment title and type is of text respectively tags is a set of text and lastly preview thumbnails is defined as a map now we will use these data types in creating a table we already saw how to use the data types in the table in theory to better understand the physical data model it's time to demonstrate it with an example creating a table could cause a lot of problems if the tables aren't configured correctly the first challenge is creating a partition that is way too large there are many other challenges that are related to optimization now we have data types which we have added in the previous topic now let's create a table using the cql create table statement in this example we've created a table of comments by user the field in the table are everything that we saw on the previous slide in the physical version of the table the user id is uuid and the posted timestamp is a timestamp and all the other data types that we saw earlier once the table is created the user may want to insert data into it there can be two types of users a new user and an existing user if it's a new application then users will use your site and insert the data for you but if you're migrating from an existing database then it may take a few ways to get the data into the table first is the copy command which can leverage cql copy in cql and has a value of inserting approximately 2 million values into the database second is the ss table loader which is used when migrating from one cluster to another and lastly there is spark for data loading which has a lot of options but does not require a working knowledge of spark here are some basic sql copy features copy 2 exports data from a table to a csv file copy from imports data to a table from a csv file in sql copy the process verifies the primary key and updates the existing records accordingly if the header is false it specifies that the fields are imported in a deterministic order when column names are specified in the table then the fields are imported in that order if any field is missing or empty it is set to null source cannot have more fields than the target table but surely can have less number of fields than the target table so let's look at an example of basic data loading with cql copy this is a good method to use and to get started we will import the data in column 1 column 2 and column 3 from the csv file table 1 data.csv when header is equal to true welcome to module 4 cassandra practical application we're going to help you start your own cassandra as a service db on a cloud provider of your choice cassandra as a service is named astrodb and is provided by datastax upon completion of this module you will be able to create your own astrodb instance and perform the workshops upon completing this lesson you will be able to perform the workshop related to this course earlier we learnt about different apis to connect to our astrodb like the document api graphql api and the rest api all of these apis are part of a project called stargate stargate is built into astrodb for your convenience if you would like to learn more about what stargate is how it works how you can get involved in the open source project please go to stargate.io for more information for an indepth understanding of how it all works and how you can utilize target your advantage view the workshop video on youtube created by the data stacks developer advocates scan the qr code to go to the video or copy the link and paste it in your preferred browser another option for your astra db is to build a to do app using the rest api python and node.js respectively this is an example react to do application built using a datastax astra fee free tier database for an indepth understanding of how it all works view the workshop video on youtube created by the datastax developer advocates scan the qr code to go to the video or copy the link and paste it in your preferred browser in this example we discover a fully reactive version of the spring pet clinic application using spring web flux this example is considered an intermediate level application for developers to build on astra db for an indepth understanding of how it all works view the workshop video on youtube created by the data stacks developer advocates scan the qr code to go to the video or copy the link and paste it in your preferred browser earlier in this course we carried out some basic data modeling to understand how cassandra can best be built and deployed if you wanted to do a deeper dive into a more advanced data modeling for your app we also have a workshop available for you to view on youtube from the data stacks developer advocates once again scan the qr code to go to the video or copy the link and paste it in your preferred browser throughout this course we had mentioned using spark for specific functions on astra db including data loading and connecting we also wanted to provide you with a workshop for you to view if you would like to get a more indepth understanding of how it all works and how you can utilize spark to your advantage on youtube again created by the developer advocates of data stacks scan the qr code to go to the video or feel free to copy the link and paste it in your preferred browser