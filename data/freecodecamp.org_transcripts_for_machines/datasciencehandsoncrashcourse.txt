hello everyone and welcome to this data science crash course my name is marco and i just recently started a youtube channel where i do videos on data science so if you want to check it out the link is in the description below a bit of an overview of this crash course we will first answer the question what is data science then we will walk through a setup so that you are ready on your computers to code along with me and then we will move on to the algorithms which is in my opinion the fun part so we'll talk about linear regression then we will move on to classification with logistic regression lda and qda we will talk about resampling and regularization methods which are very important in any workflow for a data scientist we will talk about decision trees uh most of the stateoftheart algorithms are actually treebased methods so a very exciting subject and then we will move on to support vector machines and conclude this crash course with unsupervised learning now let's answer this question what is data science well data science is a field that uses scientific methods to extract knowledge and insights from data now this definition may look very broad and that's because data science is a very broad field in fact it encompasses three professions so you can be a machine learning scientist or engineer which means that you are the person who develops the algorithms you could also be a data analyst which is the person who answers business questions for example what is the product that we sold the most in the last month that would be the job of the data analyst or finally you could be a data engineer which is the person that builds the software to gather data from different sources because usually the data needed to solve a problem is not in the same place so those data engineers they gather the data from everywhere and put it in a format that can be used after by the data analyst or the machine learning engineer now this crash course will mostly focus on the machine learning part because we will mostly discuss the algorithms now what can you expect from this course well first of all theory yes as in math equations because theory is important in data science you need to know how a model behaves why it behaves in a certain way and how it works because understanding that is actually way harder than coding the algorithm itself as you will see later on also if your goal is to land a job as a data scientist in a company well you will be asked theoretical questions during the interview so a very important part please don't skip it and of course each algorithm section will be accompanied by handson examples in python so for each algorithm we will download a data set and we will apply this algorithm on that data set so without further ado let's get started with the setup all right let's get you set up to do some data science head over to google and search for anaconda then click on the first result which should be anaconda the world's most popular data science platform once you click on it head over to products and then individual edition you can read more about what anaconda is if you want otherwise simply click on download and it brings you here where you can select the installer which is appropriate to your operating system now in my case i have windows 64 so it would be the graphical installer if you're on mac or on linux choose the one that suits you best once you click on it you can download and save the file now i will not do this since i already installed it on my machine but once it is downloaded you can simply follow the instructions on the graphical installer and once everything is done you should be able at least on windows if you hit the windows key and start typing jupiter with a y simply type enter and after a moment you should see this page showing up and that means that jupiter is installed correctly and you're ready to do some data science if you're on mac however simply open your terminal and type in jupiter notebook with a space and you should be fine all right now that we are set up let's kick off this crash course with our very first algorithm which is linear regression we start with a simple linear regression where our target y only depends on one variable x and a constant beta 1 is then a parameter which can be positive or negative that characterizes the slope and beta naught here is the constant to find the parameters we need to minimize a certain error function so here the error is simply the difference between the real target y i and the prediction y hat for linear regression we minimize the sum of squared errors so we raise this equation to the power of 2 and add all errors across all data points visually it looks like this the red dots represent our data and the blue line is our fitted straight line each vertical line is the magnitude of the error so we want the to position the blue line such as the sum of the squared length of each vertical line is as small as possible now you might wonder why do we square the errors well as you saw the points can lie above or below the fiddle line so the error can be positive or negative if we did not square the error we could be adding a bunch of negative errors and reduce the sum of errors it would trick us in thinking that we are fitting a good straight line where in fact we're not it also has the added advantage of penalizing large errors so we really get the best fit possible for simple linear regression you can find the parameters analytically with these formulas where x bar is the mean of the independent variable and y bar is the mean of the target now of course in practice we will use python to estimate those parameters for us here you can see we first initialized the model and then we fit on x and y and then we can retrieve both the intercept and the coefficient as you can see here once you have your coefficients we need a way to assess their relevancy to do so we use the pvalue this allows us to quantify the statistical significance and determine if we can reject the null hypothesis or not in python we can analyze the p value for each coefficient like this here we use a statistical package from python that allows us to print out a summary of the model here you can see an example of that summary so for each coefficient you get here the pvalue which is the p greater than absolute value of t here you see the value is zero but it is really not zero however it is so small that it appears to be zero once we know our parameters are relevant we must assess the model itself we usually use the residual standard error of course the smaller the value the better it is since the difference between predicted and actual is small as you can see with the equation on the screen also we use the r squared value which measures the proportion of variability explained by a feature x as it approaches 1 it means that we are explaining a lot of the variability in our target in python we can use the same process as we did before to find the pvalue using the same package so as you can see when we print out the summary this is the full printout that you get from the python package you see highlighted in yellow the rsquared value in this example now from here multiple linear regression is easy to understand as we simply extend the model to accommodate more features now each feature has its own parameter and p is the number of predictors so in python we saw how to access the constant n1 parameter and if we have multiple parameters then we simply increase the index to get the other parameters in multiple linear regression to assist the model of a multiple linear regression we use the f statistic here p is the number of predictors and n is the number of data points now again in python we'll use the same package that outputs for us the f statistic as you can see here in yellow the f statistic for a multiple linear regression model usually if f is much greater than one we say that there is a strong relationship between our predictors and the target for a small data set of a couple hundred data points then we the f statistic has to be way larger than one so that's it for the theory of linear regression let's see how we can apply it with python so let's fire up our jupyter notebooks here i have a folder called data which contains a data set for this exercise so as with any project we start off by importing the libraries we will use so of course we're going to use pandas as pd let's import numpy as np now it's time to import matplotlib and because we will use that of course to do some plots of our data a lot of straight lines and scatter plots then we're going to use cycle learn to actually fit a linear model to our data set your regression and finally we also import a stats library so stats models dot api as sm and that will give us some very cool um statistical tests to uh test for our predictors and model in general and of course we use some python magic to plot so let's start off by importing our data set so data is equal to pde.read.csv i put in the path of my data so data slash advertising.csv and then i will specify the index call equal to zero and that means that the first column will be used as the index column in our data set doing data.head you should see the following first five entries of our data so as you can see we have ad spend on tv radio and newspaper and the impact on sales so we start off with some simple linear regression so we will only consider the effect of tv on sales in this case feel free to take radio or newspaper so i always like to do a quick plot of my data whenever possible in this case because we only have one feature one target it is indeed possible so i first set the size of my figure and then we do a scatter plot so specify the x which is going to be data tv the y is data sales and then i specify the color i want it to be black afterwards i'm just doing making my plot uh a bit nicer so i just specified an x label um so there's gonna be money spent on tv ads and the units is money such as dollar sign and then plt.y label is going to be equal actually no i do not have to put an equal sign here i need to remove that so remove this from this all right perfect and now i do this is going to be sales and it's going to be in thousands of dollars perfect finally i do plt show and you should see the following plot there you go as you can see so this is our data so the sales with respect to the money spent on tp ads now maybe a linear line a linear regression is not the best model here but let's try it anyways so we're going to specify our feature which in this case is only the data tv so the values are reshaped minus 1 1 as required by the scikit learn library and then we specify the target which is our sales values dot reshape minus 1 1 perfect now we simply call the regression model so rag is going to be equal to linear regression and then we will fit the model to our data we pass in x and y awesome and with that we can print our parameters so we can print our constant and coefficient for tv so the linear model is y is going to be equal to so first we're gonna access uh actually we're gonna access the intercept first uh so we intersect underscore zero and then we add the coefficient for x so in this case tv and the coefficient is equivalent to beta1 if you looked at the theory portion of this video so as you can see we get a constant of 7 and a slope of 0.0475 approximately so that's great we have a positive slope positive constant it seems to make sense so let's get some predictions and actually plot our straight line so you can get the prediction simply by calling the predict method on x and then we do another figure so i'm always again just setting the fixed size here not curly braces sorry about that it's actually just normal parentheses so put it the same size as before 16 8 then plt dot scatter we pass in x y and of course the color is going to be black so this is going to be our data set as shown uh above a bit earlier and then we add in uh the plot of our predations so same x the y in this case is going to be predictions and i want it to be in blue and i'm going to specify the line width to be equal to 2 just to make sure that we can see it now let's just copy the labels because it's going to be exactly the same the same sorry uh the plt does show as well awesome let's run it and boom as you can see we have ours traded our straight line plotted on our graph uh that's awesome perfect so let's move on to the next portion where we will assess the quality of our uh model so to do so we're actually gonna use the stats library so i'm going to respecify again my x and my y and we're going to fit another linear model but using the stats library so specify the exogenous variable as sm add constant x and then the estimator is simply sm.ols that stands for ordinary least squares that's the method we're using pass in y pass in x and we fit finally you can print a summary of the estimator and you should see the following result so as you can see we have an r squared value of 0.6 so that is not very good only 60 of the variability is explained the s statistic is 312 which is much larger than one so it seems that our model is kind of good and as you can see here for tv we get the same coefficients as before and the pvalue although probably not zero it seems to be less than 0.05 so it means that our um that our feature is indeed relevant in this model so that was simple linear regression let's move on to multiple linear regression so in this case we will consider all the features so tv radio and newspaper and see how that affects the sales so all my x's to define them i'm just going to drop the sales column and make sure i dropped it on axis equals one so i mean i'm dropping it only the column and on the rows and the y is going to be the same so that is sales the values dot reshape minus one and one again i'm going to fit using scikitlearn so the regressor is linear regression and you call fit so x's and y perfect now as before we're going to print our coefficients so the linear model is skip a line and then y is going to be equal to so let's start off by printing uh the constant right so reg.intercept underscore square bracket is zero and then the coefficients will be in the same order as in the data set so the first one if i remember well it's going to be for tv so reg dot co f 0 0 and then um multiply that by uh tv afterwards we're gonna add the coefficient as the second one oh this is not in the brackets sorry about that radio and let's put all of this inside the brackets start the squiggly brackets right and now we're gonna add the last coefficient so reg dot co f zero one two sorry and let's bring this a bit to the right so you can see the code uh we multiply that by news paper awesome let's run this cell hopefully everything is going to work no the regression object has no attribute code f indeed it does not uh we need to add an underscore on the coefficient of tv so coeff underscore awesome there you go there we have it so uh we have two as a constant .04 so the same slope for tv point 18 for radio and negative for newspaper that is very interesting we have a negative effect from newspaper so again let's fi let's use the stats library to assess the quality of our model so i'm just going to specify the x as np not column stack and then we take everything so data tv we're gonna take uh data radio right yes data radio and data newspaper awesome so this is our x our features and the target again is data sales sorry shape minus one one awesome so again the exogenous variable we're going to do sm.add constant we use x we define our estimator which is sm.ols and then passing y passing x in this case exalt right.fit and we print the summary of our estimator and you should get the following awesome now as you can see the r squared value is much larger than before we have 0.897 so we're explaining almost 90 percent of the variability of the variability of the sales here the f statistic 570 again larger than before so it means that our model is pretty good i actually to predict the sales from that and as you can see here all the constants and all the coefficients now as you see for the last one we have a pvalue equal to 0.860 so that is larger than 0.05 and recall that this coefficient here is the one for newspaper so that means the newspaper is actually not relevant in our model and we could and actually we should take it out let's move on now to the next topic which is classification first a bit of terminology binary classification is also termed simple classification and this is the case when we only have two classes for example spam or not spam a fraudulent transaction or not of course you can also have more than two classes for example eye color which can be blue green or brown now you see in the context of classification we have a qualitative or categorical response unlike regression problems where we have a quantitative response or numbers so now let's see how we can perform classification with one algorithm which is logistic regression ideally when doing classification we want to determine the probability of an observation to be part of a class or not therefore we ideally want the output to be between 0 and 1 where 1 means very likely well it just turns out that there is a function to do that and it's the sigmoid function that you see on the screen as you can see as x approaches infinity you approach one and if you go towards negative infinity you approach zero the sigmoid function is expressed like this and here we are assuming only one predictor x for now we stick to one predictor to make the explanation simpler with some manipulation you can get to this formula here so we are trying to get a linear equation for x take the log on both sides and you get the log it as you can see now it is linear with respect to x and most importantly the probability is bound between 0 and 1. now of course we can extend the log it formula to accommodate multiple predictors this always gives better results since you are considering more predictors in python we can perform classification using logistic regression like this so again we initialize the model first and then we fit on our training data passing the features x and target y then we can get the probability that an observation is part of a class once we have that probability if it is greater than 0.5 we will say that it is part of that class so we will output one otherwise it will be equal to zero which is what the last line of this script is doing now that's it for logistic regression let's move on to linear discriminant analysis or lda now we want to learn about lda because logistic regression has some caveats when classes are well separated the parameters estimate tend to be unstable they are also unstable when the data set is small and finally logistic regression can only be used for binary classification with lda you can overcome those issues because it models the distribution of predictors for each class so you can have more than two target classes and it does so using bae's theorem now bae's theorem is explained like this suppose that we want to classify an observation into one of capital k classes where capital k is greater than or equal to two then we let pi k be the overall probability that an observation is associated to the kth class then let f k of x denote the density function of x for an observation that comes from the kth class this means that k of x is large if the probability that an observation from the kth class has capital x is equal to small x then the jesus theorem states the equation that you see the probability of the class being capital k given capital x equals to small x is the ratio of pi k fk of x over the sum of pi l f l of x from l to k so for all classes this was a bit challenging so make sure to probably rerun this section of the lesson because it is very important to understand now the challenge here is really approximating the density function so we will assume only one predictor and normal distribution this is expressed by the function that you see here this is the normal distribution function so if we plug this function in the formula that we saw before and take the log we find out that we must maximize this equation now this is called the discriminant and as you can see it is a linear function with respect to x hence the name linear discriminant analysis when applying lda we need to be aware of the assumptions it makes and make sure that it applies to our situation here lda assumes that each class has a normal distribution and each class has its own mean but the variance is common for all classes if you add more than one predictor which should be the case because usually more predictors gives you better results then each class is drawn from a multivariate gaussian distribution and each class has its own mean vector and there is a common covariance matrix so basically we must use vectors and matrices instead of single numbers compared to lda with only one predictor in practice this is how we apply lda once we initialize the logistic the linear discriminant analysis algorithm and then we fit it on our training data passing the features x and the target y then again we get the probability with the method predict prabha and finally if the probability is greater than 0.5 we say it is part of a class so we output 1 otherwise we output 0. now that we understand lda qda is fairly straightforward the main difference is in the assumptions just like lda we assume each class is from a multivariate normal distribution and that each class has its own mean vector but this time each class also has its own covariance matrix of course because we are talking about quadratic discriminant analysis well the discriminant here is expressed like this and you see that the equation is now quadratic with respect to x since we have two terms of x being multiplied together now whereas lda was better than logistic regression in some situations qda is also better than lda mainly when you have a large data set because it has a lower bias and higher variance if your data set is small then lda should be enough again in python you should see some kind of a pattern here because as you see it is very similar to lda and even logistic regression so again simply initialize your model fit it on x and y predict the probability and if it's greater than 0.5 output 1 otherwise the output 0. now before we move on to the coding portion we must understand how to validate our models in the context of classification to do so we use sensitivity and specificity sensitivity is the true positive rate so the proportion of actual positives identified so for example it would be the proportion of fraudulent transactions that are actually fraudulent on the other hand specificity is the true negative rate so the proportion of nonfraudulent transactions that are actually nonfraudulent we can also use the rok curve where rock stands for receiver operating characteristic we usually take the area under the curve or auc that's why you probably hear about the rock auc we want the rock auc to be close to one why well as you can see we plot the false positive rate against the true positive rate ideally we have a false positive rate of 0 and a true positive rate of 1 which would give an area under the curve of 1 and the curve would hug the upper left corner of the graph as you will see soon in the coding portion we can write a function in python that will plot for us the roc curve and report its area under the curve we will actually walk through the plotting and the writing of this function in the coding portion now in the case of a perfect classifier as i said we get an auc of one and the rock curve should look like this so that's it for the theory about classification logistic regression lda and qda let's move on to a handson example alright so let's start off with this project fire up your jupiter notebooks are i have already mine open uh as always i have this folder called data in which i put my data set called mushrooms.csv the link for the dataset is in the description of the video so in this project we are going to classify mushrooms as either being edible or poisonous depending on different features so you have cap shape cap surface cap collar etc and i put all the possible values here at the beginning of the notebook so let's start off by importing the libraries we're going to use open spd known by snp of course matplotlib.pipeplot as plt and also seaborn as sns now we are going to import sklearn the preprocessing and specifically label encoder you will see how that will be used later on also from sklearn.model selection import train test split with underscores and also cross val score awesome from sklearn.metrics we are going to import the raw curve and the auc as well as the confusion matrix shift enter oh sorry first we're gonna put the matplotlib in line so we can see our plots all right so shift enter and now i am just going to um to define a path for my data set so data slash mushrooms with an s dot csv and now i'm just going to display the first five rows of the data set all right so as you can see these are our first five rows we see the class poisonous are edible and then we see the values for each feature so now i'm just going to do a plot to see how many poisonous and edible mushrooms we have in our data set so this will help us see if the data set is balanced or not so for that i'm going to use seaborn and as you can see our data set is fairly balanced we almost have the same amount of poisonous and edible mushrooms so that is very good we're not going to have to do a lot of preprocessing for our analysis now i'm going to define a function that will allow us to see depending on what feature how many uh mushrooms are poisonous or edible so for example if i if i plot for the cap surface so for all possibilities of values for the capped surface i want to know how many of those mushrooms are edible and how many of those are poisonous so that's going to give us a bit of intuition as to which feature helps you to actually classify your mushrooms and that's it so that's the function now i'm going to show you how you can use this function but i am not going to run it because this will actually run for all the features of the data set so it's going to generate a lot of plots so i'm just going to show you how to use it and you can run it on your notebook if you want so you set the hue equal to data class so that means that you're going to have two colors right so one for poisonous one for edible and then you simply want to drop the class column and then plot the rest of the data so to do that you just do plot data and then you pass in the hue and data to plot so fairly simple very straightforward like i said you can run this function on your own notebook if you want to see uh the examples but i will not run it for now so let's move on to preprocessing so you can do this by doing escape 2 by the way and shift enter so now i'm going to check how many null values do we have in our data sets because we do not want any antivalues so if we're calling data.columns i want to print the name of the column and then the sum of null values if they are null so you do that by data call is dot sum if you run it that is amazing as you can see we have zero everywhere so that means that we have no null values in our data set that's perfect now we are going to use the label encoder so what label encoder will do is that it will transform our class column into one and zeros because we cannot work with letters we have to work with numbers right so you do le.fee fit transform data class and now i will show you the result data head and there you go now the you you as you can see the class is now one and zero so either it is poisonous or not poisonous so one being true zero being false then you want to one hot encode the rest of the data set so to do that we do pd.get dummies and then you pass in the data so let's see what the result of that will be and as you can see now we have added a lot of columns so we went from 23 columns to 118 columns because now for every feature we have either true or false and now that is perfect this data is ready to be worked with because we have only numbers everywhere so let's move on to modeling first i'm gonna determine uh what the target variable is so in this case it is the class uh the values.reshape minus one one perfect and then the features is going to be the encoded data the head dot sorry dot drop the class column axis equals to 1 to make sure that we drop the column and then we're going to define a train and test set so we do x strain x test y train y test is going to be equal to the train test split that we imported earlier so you pass in your features you pass in your target variable and then you define the size in this case i'm going to do 0.2 so 20 of the data set will be randomly uh removed to to use as a test set and we're going to use the rest to train you can set a random state by the way to keep the results constant so let's apply logistic regression our first algorithm for classification so from sklearn dot linear model import logistic regression now we're going to initialize the model so logistic regress reg sorry is going to be equal to logistic regression now i will fit the model to our train set so pass in xstrain and ytrain dot ravel now we want to get the probabilities so y prob is going to be equal to logistic reg dot predict underscore probably and we're going to use the test set in this in this case because we fit before and now we are doing probabilities on the test set right and now we set our threshold to 0.5 so the actual prediction is going to be np dot where so in this case if the prediction is greater than 0.5 we're going to say it's equal to 1 and otherwise it's going to be equal to zero so this is really where we are classifying our mushrooms so we have run that and everything is okay you can safely ignore the warning on the screen and now we're going to see at a confusion matrix so the confusion matrix is actually going to show you uh how many mushrooms were correctly classified so confusion matrix you pass in white test and you pass in the white prediction and hopefully if those are equal you will see that we're going to have a diagonal matrix and that is actually amazing we have a diagonal matrix so all poisonous and all edible mushrooms were correctly identified so let's check that with actually another metric we're going to use here the false positive rate and the true positive rate as well as the thresholds and we're going to set that equal to the rock curve pass in y test and y prob and then the rocket you see simply going to be equal to the auc and then you pass in the false positive rate and the true positive rate so here we are actually uh going to use the rock curve and as you can see when we calculate the rocket you see we get one which is again perfect classification now i'm just going to define a function to plot the rock curve so we can visualize it see how it looks so this function is going to take in the rock auc and here i'm just basically building the plot itself so i'm just setting the fix the figure size to seven by seven i'm setting the title to uh receiver operating characteristic then i am actually going to plot the false positive rate and the true positive rate and i'm going to give it a different color here i'm going to use red we're going to give it a label called auc and i'm only also going to approximate the the auc basically so this is going to give us approximated to the two decimal places and then i'm going to give it a legend i'm going to put it on the lower right side of the plot now i am also going to plot a straight line going through 0 and 1. this just serves us as a general guide to evaluate the rock curve and i'm going to make this line dashed now i am going to define the axis or plt axis tight give some labels so the y label is the true positive rate and the x label will be the false positive rate and that's it for our function so let's actually plot the rock curve that we obtained above with logistic regression and you should get the following so this is actually a perf perfect rock curve so it's hugging the upper left corner and we have an auc of one so that means perfect classification so now let's move on to our second algorithm which is linear discriminant analysis and let's see if the results are going to be different of course it cannot be better right so from sklearn dot discriminant analysis you are going to import linear discriminant analysis now feel free to pause the video and try it on your own because we are going to basically repeat the same steps as above only this time we are using a different algorithm so you can always pause the video and try it on your own as an exercise so lda is going to be equal to linear discriminant analysis so here i'm initializing the model then i'm going to fit the model with our train set so pass in stream and ytrain.gravel then i am going to get the probabilities from the lda model so lea dot predict prabha and you pass in x test and then you get the predictions and we use the same threshold as before 0.5 so it's actually going to be the y pro np dot where um y probe there we go if it's greater than 0.5 we're just going to classify it as 1 and otherwise it will be 0. run this cell and awesome everything went well as again again you can ignore the warning on the screen so we're going to build a confusion matrix here so white test and y print lda and let's display the confusion matrix and as you can see perfect classification again so as an exercise we will still build the rock curve and show it uh simply to make sure that we get a rock of rock auc of one so get the false positive rate true positive rate and thresholds to be equal to rock curve passing y tests and bison the y uh probabilities here in this case y prob lda now display the rock auc of lda so first you assign it false positive very true positive rate and now we are ready to display it so rock a you see lda and we should get one and as as expected we indeed get one now we are going to plot the rock curve using the function we defined earlier and as you can see you get the exact same function which is again expected right because our confusion matrix was the same the rocket you see was the same the plot should be the same so again lda is a perfect classifier in this case and finally we are going to implement quadratic discriminant analysis again i strongly suggest that you pause the video at this point and really try to repeat the steps that we have done before using qda so as always we're going to import the model from sklearn so from sklearn.discriminant analysis import quadratic discriminant analysis now you set the model so you initialize it sorry so qda is quadratic discriminate analysis you can always press tab by the way for autocomplete you fit the model on your train set and then you get the probabilities like i said the steps are exactly the same it's just that we are using a different model so white prob qda is qda.predict prabha and you use the test set of course and then we get our classification so np.where use the same threshold so if it is greater than 0.5 classify as one otherwise it is zero run the cell and ignore the warning then we're going to take a look at the confusion matrix so let's see if we also get a perfect classifier here with qda so confusion matrix you pass in a y test and the predictions displaying the confusion matrix as you can see we get the exact same as before so again qda is a perfect classifier for our data set now we're gonna plot the rock uh curve and get the a rock auc as well so just like before false positive rate true positive rate thresholds is going to be equal to rock curve pass in y test and the y probabilities y probe qda and then you use the false positive rate and true positive rate to get your rock auc so here i'm calling it rocket you see qda it's going to be equal to the auc pass in false positive rate and the true positive rate and now you can display the rock auc of qda and you get one perfect as expected now we are simply going to plot it to make sure that it looks like the other plots and you know that it will and it should and as expected uh perfect rock curve auc of one now let's talk about resampling and regularization resampling and regularization are two important steps that can significantly improve our model's performance and our confidence in the model specifically resampling helps us validate our model and we usually do so with cross validation regularization is used to prevent overfitting and we will cover both rich regression and lasso so first let's cover resampling methods resampling involves repeatedly drawing samples from a training set and refitting the model on each sample this allows us to gain more information than if we were fitting the model only once we can also test how the model would perform on unseen data without collecting new data that is important because a model in production will have to predict on data it has not been trained on crossvalidation is a widely used method for resampling we use it to evaluate a model's performance and to find the best parameters for the model there are three ways we can do validation we can do have a validation set we can do a leave one out cross validation or we can use the method of kfold crossvalidation let's explore each one of them the validation set is the most basic approach we have a data set of endpoints and we randomly split the data set into a training set that you see in blue and a test set that you see in orange we fit the model on the blue set and make predictions using the orange set this has some drawbacks because the test error rate is variable depending on which observations were in each set since we are splitting randomly also only a small subset of data is used for training when ideally we want as much data as possible for training so instead we could use leave one out cross validation or loocv in this method only one data point in orange is used for validation and the rest is used for training you repeat the process for as many times as you have data points so in this case n times the error rate is approximated as the mean of errors for each run now this method has the benefit of having no randomness but it's not a viable option for very large data sets so now we introduce the kfold cross validation this is by far the most common approach here we randomly split the data set into k groups or folds we use the blue set for training and the orange set for validation and we repeat the process k times now realize that loocv is a special case of kfold where k is simply equal to n the number of data points usually we set k to 5 or 10 and like i said this method is probably the best and most widely used in python we can perform cross validation like this here we see an example with linear regression we first initialize the model and then we can get the mean squared error for each fold notice that we are performing a fivefold crossvalidation because the cv parameter is set to five then we can get the mean of all errors now notice that we report the negative of the mean because the algorithm uses the negative mean square error as a scoring system therefore we need to get the negative so that we bring it back to a positive value now let's move on to regularization models can sometimes overfit meaning that they will not generalize well and perform poorly on unseen data this brings me to the subject of bias variance tradeoff on the left the model has a high bias and low variance and you see that it's not a very good fit on the right you see a model with high variance and low bias the model is overfitting and varying a lot and we will not give good predictions so we want to find a middle ground and prevent the models from overfitting and that's why we use regularization it will help us decorate our model to prevent overfitting here we will discuss ridge regression and lasso note that these methods are also called shrinkage methods we know that traditional linear fitting minimizes the rss the residual sum of squares with ridge regression we add another parameter to the optimization function here we add the sum of parameters squared with a coefficient lambda lambda is called a tuning parameter to find the best value of lambda we will use cross validation with a range of values for lambda the best value will be the one minimizing the test error with this method all predictors are kept and note that this is also called l2 regularization in python to use it we first initialize the ridge model then the lambda parameter is actually alpha with the library that we use in python in this code snippet we provide an array of different values for alpha going from 10 to the minus 15 to 20. then we will use fivefold crossvalidation to use the best value of alpha for our model we'll get to apply this later on during the project example now with lasso we also add a new term to the optimization function and lasso is also called l1 regularization here we add the sum of absolute values of all coefficients and we still have our tuning parameter lambda now if lambda is large enough some betas will go to zero meaning that some features will disappear and so feature selection can be done with lasso in python as you can see it is very similar to what we did with ridge only this time we initialize the lasso algorithm so that's it for the theory now let's apply these methods in a project all right let's open our jupiter notebooks in this exercise we will revisit a previous data set we used for linear regression so i always have my data folder and we'll use the advertising.csv dataset so we start off by importing pandas numpy and matplotlib so pandas as pd numpy as np and then matplotlib.pyplot as plt and of course do not forget your jupiter magic so that we can display our plots in the notebook then i will set the path to my data set and read in the data so data slash advertising dot csv and then you read the data set with pandas and we will display the first five rows of the data set using the dataframe.head method index call is equal to zero data ahead and there you go the first five rows of our data set now let's define a function that will allow us to plot the target against each feature so as you see the target will be sales and then we have three features tv radio and newspaper so i'm going to define scatterplot and pass in feature as a parameter now i will specify the size of the plot to make it slightly bigger so we can see it clearly on the screen plt.scatter data feature so this is the xaxis and then on the yaxis it will always be sales because that is our target and i will specify the color of the points i want them to be black i'm going to give it an x label so on the x axis this will be the money spent on whatever feature we are plotting so it will be money spent on tv ads or radio ads or newspaper ads and then i specify a label for the yaxis and in this case it will be our sales in thousands of dollars finally we simply display the plot so now we can run this function and pass in each feature so we can get a sense of how each feature is correlated with the target so tv radio and newspaper and once you run this cell you should see three different plots and as you can see newspaper does not seem to be very well correlated with sales so now let's define our baseline model and see how regularization will improve it so first i will import cross val score so model selection import cross underscore val underscore score and linear regression and these will be used for our baseline so our baseline model will be a very simple multiple linear regression so as a first step we will define our feature vector so in this case we are only going to drop the sales column and then we will define our target which will be sales so data sales dot values dot reshape minus one one perfect now we will fit the model so first initialize it linear regression and then we will calculate the mscs so we are going to use cross validation uh here calculate the mean squared error and then we are going to average those errors so you pass in the model x y the scoring we need to use negative mean squared error as required by uh this library and we do fivefold crossvalidation so this will give us five different mean squared errors so then to get the mean we simply do np dot mean of those mses and now we will report the average mean squared error uh bring it back to being positive because remember they were negative and so we get 3.073 approximately so let's see now how regularization can help us improve on our baseline so let's try ridge regression first regularization and then ridge regression perfect we will need to import a grid search to find the optimal value for our tuning parameter and of course we will need to import ridge so esky learn the model selection import grid search cv and from sklearn dot linear model we are going to import ridge simply like that awesome so we start off by initializing the model as always so ridge is going to be equal to ridge and now we will define a list of possible values for our tuning parameter so in this case uh the the parameter is called alpha in scikitlearn and now let's pass in a bunch of values to test and we will use crossvalidation to find out which one is the best so we pass in one e to the minus 15 minus 10 minus 8 minus 4 minus 3 minus 2 and then we go to 1 5 10 and 20. feel free to use as many values as you want but as a starting point we'll use these ones so now to run our five fold cross validation we run grid search cv we pass in the model and we pass in the list of parameters the scoring will also be negative mean squared error so we keep this consistent with our baseline model and again fivefold cross validation now we can fit the model pass in your feature and your target that is awesome and it's done so now we can actually print out uh the best value for the parameters and we can print out as well the mean squared error so you can do that by finding the best params and then it's going to be the best um score exactly so when we print this out you see that the best value for alpha is 20. and let's not forget that our scores were negative so let's bring it back to positive it's 3.0722 so it is a slightly lower msc so now let's try out lasso and see if we get an even better result so at this point you can try and pause the video to work it out on your own as the method will be very similar to what we have done with range so moving on uh we're gonna initialize lasso and set the tolerance to 0.05 so that it can converge in a reasonable amount of steps let's grab our list of parameters for alpha because this i'm simply going to reuse the exact same values to test during our grid and then we exactly reproduce what we did with ridge regression so lasso regressor is going to be equal to grid search cv you pass in your model lasso you pass in your parameters the scoring method will be neg mean squared error and again we do a cross validation of fivefold finally we can print out the best parameters and the best score so this will give us the value of alpha that gave the lowest mean squared error and of course you do that after you fit the model best params print lasso underscore regressor dot best underscore score underscore once we run this cell don't forget your negative once we run the cell there you have it the best value for alpha is one and we get a mean squared error of 3.036 approximately and this is indeed the best score all right let's kick off this portion about decision trees with a bit of theory treebased methods can be used for both classification and regression they involve dividing the prediction space into a number of regions the set of splitting rules can be summarized in a tree hence the name decision trees now a single decision tree is often not better than a linear regression logistic regression or lea that's why we introduce bagging random forests and boosting to dramatically improve our trees now before we move on we need to get familiar with a bit of terminology trees are drawn upside down the final regions are called leaves the point where split occurs is called a node and finally the segments that connect the nodes are called branches here is an example of basic decision trees with the leaves at the bottom and then the branches and you see the nodes where it splits now let's see how a regression tree works to create a regression tree we divide the predicted space into j distinct and nonoverlapping regions then for each observation that falls in a region we predict the mean response value of that region each region is split to minimize the rss it uses a topdown greedy approach also called recursive binary splitting now why top down because all observations are in a single region before the split and why do we call this greedy that is because the best split occurs at a particular step to make the best prediction at that step instead of looking ahead and making a split that will give a better prediction later on now mathematically we define the pair of half planes like this and we seek j and s to minimize the rss of both planes however this may lead to overfitting and as you can see on the right that's why we need to sometimes prune the trees and use cross validation to prevent overfitting in python a regression tree is very simple to implement as you can see we import the model from the sklearn library we initialize it and then we fit on x train and y train now let's see how a classification tree works it is very similar to a regression tree but instead we predict the most commonly occurring class in a region also we cannot use the rss since we are dealing with categories so we must minimize the classification error rate the classification error rate is simply the fraction of training observations in a region that do not belong to the most common class however this is not sensitive enough for tree growing so instead we use the genie index which is a measure of total variance across all classes the gd index will be close to 0 if the proportion is close to 0 or 1 which makes it a good measure of node purity a similar rationale is applied to cross entropy which can also be used for tree growing now in practice we can simply import the decision tree classifier model initialize it and fit it on our data x and y now that the basics are covered let's move on to more advanced topics on decision trees bagging stands for bootstrap aggregation we know that bootstrap can compute the standard deviation of any quantity we also know that variance is a high in decision trees since they are prone to overfitting so bagging is a method to reduce variance and improve the performance of our algorithm bagging involves repeatedly drawing samples from the data set generating b different bootstrap training sets once all sets are trained we get a prediction for each set and we average those predictions to get a final prediction so mathematically we expect the final prediction like this and so you recognize that this is simply the mean of all b predictions this means that we can construct a high number of trees that overfit but by averaging their predictions we effectively reduce the variance and improve the performance in python as you can see it is very simple to apply bagging we can import the bagging classifier initialize it and then we fit it on our data now let's see how a random forest can also improve the quality of our predictions random forests provide an improvement over bagging by making a small tweak that decorates the trees again multiple trees are grown but at each split only a random sample of m predictor is a lot is chosen from all p predictors and the split is only allowed to use one of the m predictors now typically m is the square root of p now how is that a good thing well in bagging if there is a strong predictor it will likely be the top split and all trees will therefore be similar so variance will not be reduced once we average all predictions with random forest because we force a random sample of predictors for each split we avoid the situation also realize that if m is equal to p then it is just like bagging again you see that applying the random forest algorithm is very simple in python once imported we initialize the model and pass in the number of trees we would like to use in this case we use 100 then we can fit on x and y finally let's cover boosting boosting works in a similar way to bagging but trees are grown sequentially they use the information from previously grown trees this means that the algorithm learns slowly also the trees fit the residuals instead of the target so the trees will be small and will slowly improve the predictions there are three tuning parameters for boosting the number of trees b where if it is too large then it will overfit so use cross validation to find the right number of trees we have the shrinkage parameter alpha which is a small positive number that controls the learning rate typically we set it to 0.01 or 0.001 and finally we have the number of splits in each tree which controls the complexity of the boosted ensemble typically a single split works best we also call that the interaction depth now of course i said that those there are three tuning parameters in boosting usually there are more than that but those are the three main ones that we can focus on when using the algorithm to improve its performance in python we apply boosting like this now in this module the learning rate is the shrinkage parameter and estimators is the number of trees in this case set to 100 and the max depth is the interaction depth that we looked at earlier again once the model is initialized we fit it on our data so that's it for the theory now let's see how we can apply this in a project all right so let's get some code done i have a notebook open here also feel free to grab the data set in the description and put it in a folder called data and the data set here is about breast cancer so we are trying to identify patients with breast cancer from a simple blood test so we start off by importing our usual libraries so numpy as np we are going to import pandas as pd then matplotlib dot type plot as plt we will also need seaborn and today we are going to use the function plot confusion matrix as well this is going to be useful to evaluate our decision trees later on finally you use some jupiter magic to display your plots in the notebook so now let's read our data set uh i will simply define my data path in this case so it is in the folder data and the data set is called breast cancer dot csv then i will use pandas to read the data so data is going to be equal to pd.read.csv and i pass in my datapath feel free to use tab at any time to autocomplete and we will display the first five rows of the data set and there you go as you can see the first five rows of our data set perfect now we are going to check if our data set is balanced because we have we are in a classification problem so i want to make sure that we don't have too much of healthy patients or patients with breast cancer in the dataset that would make it imbalance so we use the count plot and as you can see the classes are fairly balanced here so we do not need to know to do some crazy manipulations in this case now it will be interesting to uh define a function to make violin plots that will allow us to see the distribution of each feature for both classes so it can give us some intuition about the data so for example maybe we will see that most of the healthy controls are younger so defining the function we will need x y and data as parameters and then i will enumerate each y so we will define a figure then i will set some parameters in this case i will i am simply setting the uh figure size i want it to be fairly large for you guys to see so i will set it to 11.7 and uh 8.27 i know i am very precise and then you will simply do a violin plot for each one so sns.violin plot x is equal to x y is the colon and data is equal to data that is perfect so now in this case uh the y is actually going to be data dot columns uh everything but the last column so i want so in this case the features are actually going to be y and x is going to be the target variable because i want to get the distribution of each feature for each class now we can run the function actually passing in our x y and data and you get the following plots so feel free to study those plots a little bit longer and get an intuition about the data set we are working with now we are going to check for null values to make sure that nothing is missing so for column in data dot columns i will print the name of the column so curly brackets call and then we will print the sum of null values so that is data call dot is null dot sum running this cell and you see that we have no null values in this data set that is amazing now we will start some preprocessing first i would like to do some label encoding on the target variable so that we bring it to one or zero so from scaling up preprocessing import label encoder we will initialize the label encoder and then we will fit transform that on the row classification so data classification is le.fit transform and you pass in data classification now to make sure that everything is right we'll display the first five rows and everything is right now the healthy control is zero and someone with breast cancer will have a label of one now we will split our data set into a training and test set so from scalar dot model selection we are going to import train test split so our target is of course the classification the values.reshape minus one one and our features is gonna be everything but classification so i'm simply going to drop the classification column and i'm going to specify the axis as well axis is equal to 1. awesome now actually splitting our data set uh note that our data set is fairly small in this case so i will use a smaller test size than we are used to in this case i will use only 10 of it as a test uh size so you pass in x y test size is equal to 0.1 and a random state of 42 so that we make sure that we get the same results so now let's build our baseline model so it's been our baseline model it will be a simple decision tree uh classifier so uh from sklearn dot tree we are going to import um decision tree classifier we will initialize the classifier so then the brackets then we will fit the model so call fit and then you pass in xtrain and ytrain and finally we will plot the confusion matrix so the confusion matrix will show us how many instances were misclassified so you pass in your classifier x test white test and i will specify that i want uh blue colors in this case so it's going to be a gradient of blue um i do not want the grid and i want to show the plot so as you can see we get this confusion matrix and you see that only three instances were misclassified in this case now i would like to show you a cool trick because you can visualize your decision tree with the function plot tree so from excalibur not tree you can import plot tree and then let let's see what it looks like so you're passing the classifier and i'm going to specify the max depth to five so we'll only see five different splits and there you see it so you can see the top split you can see which feature was used what what's the value of the split and you can also check for the genie index of each region so that is pretty cool feel free to you know not even pass in max that so you can visualize the entire decision tree if you want to so now let's try and improve on our baseline model and we will use bagging first so from sklearn dot ensemble we are going to import um bagging classifier we initialize the model as always so bagging clf is bagging classifier then we fit the model so that fit pass in your x train and y train in this case you need to do dot ravel and now we will plot the confusion matrix you pass in your classifier pass in xtest ytest and again i will specify gradient of blues to keep the plots consistent in the entire notebook i will remove the grid and show the plot and as you can see we only have one misclassified instance in this case so bagging is an improvement over our baseline now let's see how we can implement random forest at this point feel free to pause the video and try it on your own as the process will be very similar uh to bagging so to what we've done uh above so from sklearn.ensemble we're gonna import random forest classifier we initialize uh the model so random clf is going to be equal to random first classifier and in this case i will specify the number of trees i want a hundred trees then we fit the model we pass in our train test our train set sorry extreme and white train and then we will plot the confusion matrix so i'll just grab this code right here copy paste it down and then all i have to do is replace bagging clf with uh random forest cliff and i forgot to to ravel the right train sorry about that so i trained unravel and there you have it we have actually a perfect classifier with no instances that were misclassified that is pretty great however keep in mind this is a small data set it doesn't mean that our model is necessarily very good at this point and finally we're going to implement boosting so uh from sklearn dot ensembl we're going to import gradient boosting classifier gradient boosting classifier so as we have done before we initialize the model then we fit it so boost clf dot fit x train and y train and then we will plot the confusion matrix so grabbing the code again copy paste it below and remove a random clf and paste boost clf instead and again i forgot the ravel sorry about that guys my train dot ravel and then you get this following confusion matrix where only one instance is misclassified which is not better than random forest but better than our baseline alright let's cover some theory about support vector machine for classification we have seen quite a few algorithms such as logistic regression lda qda and decision trees support vector machine is another algorithm used for classification its main advantage is that it can accommodate nonlinear boundaries between classes to understand svm we must first understand the maximum margin classifier like i said the maximum margin classifier is the basic algorithm from which svm extends it relies on separating different classes using a hyperplane in a pdimensional space a hyperplane is defined as a flat athene subspace of dimension p minus 1. therefore in a 2d space the hyperplane will be a line and in a 3d space the hyperplane will be a flat plane the equation for a hyperplane is defined like this where p is the number of dimensions now here is an example of a hyperplane in 2d which is represented by the line if an observation satisfies the equation we just saw earlier then the point is on the line otherwise it is above or below the hyperplane now realize that if data can be separated perfectly then there is an infinite number of hyperplanes but we only just want one so that's why we use the maximum margin hyperplane or the optimal separating hyperplane to find it to do so we must calculate the perpendicular distance between each training point and the hyperplane that distance is called the margin then the optimal separating hyperplane will be the one with the largest margin here you see an example of a maximum margin hyperplane and you see the margin illustrated by the arrows notice that the plane depends only on the closest points known as support vectors and if those points move then the hyperplane will move but what if there is no clear separation between the classes as shown here well that's when svm is required as i mentioned svm is simply an extension of the maximal margin classifier this time it uses kernels to enlarge the feature space and accommodate for nonlinear boundaries between classes a kernel is simply a function that quantifies the similarity of two observations the kernel can be a function of any degree but of course if the degree is greater than one then we add more flexibility to the boundary here is an example that we will implement later on during the coding portion of this section here the classes can be linearly separated so it's easy enough however notice the outlier on the left and we can use regularization to account for it or not and we'll see how that impacts the model here is another example that we will code and as you can see here the boundary is definitely not linear but svm does a pretty good job at finding a boundary and separating each class so that's it for the theory now let's move on to the coding project and generate those plots ourselves all right so prepare your notebooks and grab the data from the link in the description in this tutorial we're actually going to import five different data sets so you can see me i'm checking them right now so x6 data one two three and then spam test and spamtrain.mat so as always we start off by importing all the libraries that we will need throughout this project by the way these exercises are taken from the machine learning course by andrew angie i am simply solving them using python here by the way it's an amazing course and i will leave the link in the description if you want to check it out i definitely recommend it so we import numpy pandas matplotlib.pipeplot also matplotlib.cm and from scipy.i will import loadmat and finally mypluslibinline to show our beautiful graphs now i will simply define the path to all my datasets so path 1 is going to be for x 6 data 1 dot mat and then we'll simply do the same data path 2 for x6 data 2 dot matt moving on to data path 3 for ex 6 data 3. and i will define the path for the spam train and test as well and we will use those data sets at the very end of this of the tutorial when we will build a classifier for spam using support vector machines training is done now data spam test is data slash spam train test sorry dot matt perfect uh now we will need to write a function to plot our data i decided to write a helper function here because we will be plotting quite often so we will need x y x label y label we'll use pause label and neg label because this is mostly classification right and we'll also pass in x min max y min and y max and set x is equal to none so first we're going to set the parameters of the plot we're actually going to set the figure size to make it nice and big for you guys so you can see it clearly on the screen i'm going to set it to 20 and 14. now i will specify what will be considered as positive so it's when the the label is going to be equal to one and negative is of course when it will be equal to zero then if axis is equal to none which by default it is then we're going to set axis equal to plt.gca then we're going to draw a scatter plot so axis dot scatter pass in the positives then you pass in the y for the positives i will specify the marker so i want dots so you can specify that by putting uh the letter o in strings then i will also specify a color at this point feel free to use any color you want s equal to 50 and the line width will be equal to two and finally the label will be the pause label because those are the positive data points now i will draw another scatter plot but for the negative samples so it will be fairly similar to the line that we wrote above only this time it's going to be for negative so nag column comma zero and then again so you pass in the y now so x snag colon comma 1 the marker is still going to be a dot this time we're going to specify another color so that we can differentiate them easily on the plot so ffa 600 s will be equal to 50 the line width will also be equal to 2 and then you set the label equal to neg underscore label awesome now i will set the limits on the xaxis and then on the yaxis so of course we'll just pass in the xmin and xmax and then you do the same for the yaxis so while and then you pass in the array from ymin to ymax now let's set the label for x so set x label will be x label simply and we'll specify the font size i'm going to put it to 12 and you do the same thing for y label so you set y label pass in my label and specify the font size as 12 as well then we will specify the position for the legend so acts as a legend is a box sorry is it b box to anchor equal to one comma one and fancy box we'll put equal to true so let's start the coding portion by exploring the effect of regularization on svm and we'll start off this time with a small regularization parameter so first let's see what our data set looks like so data one is load matte the data path number one x will be data x and the y will simply be data y and now we will plot the data set so we use our function plot data pass in x pass in y the label will be x and y then the positive way label we'll simply call it positive the negative will call it negative we want the plot to be from 0 to 4.2 and then from 0 to five and i made a mistake because it should be data one not data all right and you see this following plot so as you can see the positives in dark blue the negative is in yellow and you see that this data set is clearly linearly separable and you also notice this outlier on the left so now let's see um with a small regularization parameter what will happen to that outlier if it will be classified correctly or not so we start off by importing svm from sklearn and we'll set the regularization parameter to 1 for now so clf will be equal to svm.svc the kernel will be linear because as you can see we can use a straight line to separate the classes c will be equal to one and then you specify the decision function shape to obr then you fit the model on your data and now we will plot the data as well as the boundary so here actually we can just grab this line from the previous cell because it will be exactly the same we're just plotting the same data again and now we'll plot the boundary on top of this data sorry i accidentally ran this cell but now let's actually plot uh the boundary or the hyperplane so we start off by specifying x1 and x2 to be a numpy dot mesh grid and then we pass in np arrange so we want from 0 to five and from with the steps of 0.01 and np range oh sorry this should not be here this should be inside the big bracket of mesh grid so np range from 0 to 5 as well with steps of 0.01 so i'm taking small steps here to make sure that we plot um a smooth hyperplane as smooth as possible then zed will be the prediction pass in np dot c underscore x underscore one dot ravel and do the same with x2 dot ravel then zed is going to be equal to z dot reshape x1 dot shape and now we are ready to plot the hyperplane so plt.contour you pass in x underscore one x underscore two pass in zed pass in uh zero comma five and the colors will be uh black so b and there is a mistake we have a warning here no contour levels were found within the data uh range and that is because okay here's the mistake shouldn't be a comma it should be a point so 0.5 perfect and you see this spot here with our hyperplane which is a straight line in blue and we have and you can see here that the outlier was not taken into account and is in this case misclassified so now let's see what happens if the parameter is very high so feel free to pause the video and try it on your own as an exercise because the code will be very similar to the previous cell so here we are going to use a large regularization parameter we'll set it to c equal to 100 and actually i simply gonna grab everything from this cell here and just copy paste it below because the code is exactly the same we are simply changing the value of the hyper parameter so we don't need to import svm again and we'll simply use c equal to 100 and see what happens next so as you can see now the hyperplane shifted to account for the outlier but in this case we are likely overfitting which means that the model will not generalize well so ideally we will go with the previous model that we built so now let's try an example with a nonlinear boundary so svm with nonlinear boundary so for that we'll need our second data set so data2 is load matte data path underscore 2 and like before we'll specify the x and y and plot our data set so x2 will be data 2 x y2 will be data 2 y and now we will use our helper function to plot the data so pass in x2 and y2 again the label will be simply x and y for the class labels we'll simply use positive and negative and now we will set the limits on the xaxis so zero to one and on the yaxis it's going to go from 0.38 to 1. and there you go so as you can see now clearly a nonlinear boundary but the classes seem to be separable so let's see how svm will be able to do that for this example we will use a radial basis function for the kernel here we must define gamma which is a parameter that specifies how far the influence of a data point reaches a low value means very far and a high value means close and you can express gamma in function of sigma so let's define sigma as being equal to 0.1 and then gamma will simply be 1 over 2 sigma squared so one over open brackets two times sigma star star squared perfect then our classifier will be equal to svm.svc we pass in the kernel and this time is going to be equal to rbf so for radial basis function gamma will be equal to gamma we set the regularization parameter to 1 and the decision function shape will again be equal to ovr next we fit the classifier to our data so x underscore 2 and y underscore 2 not revel then we will plot the data set so again just grab this line from the previous cell and paste it under because we are simply plotting the same scatter plot as before and now we will plot the uh hyperplane so the actual boundary from uh the svm algorithm so again as before x underscore one x underscore two will be a mesh grid you range from 0 to 1 with bounds of width steps sorry of 0.03 so here the steps are going to be slightly smaller because we are drawing a nonlinear boundary and again it's just to make it as smooth as possible and from 0.38 to 1 for y then z will be equal to the prediction pass in np dot c underscore this is actually just to stack uh the predictions by the way so x underscore one dot ravel x underscore 2. ravel then we will reshape zed so that don't reshape we'll take the shape of x one so x underscore one dot shape and now we are ready to plot the hyperplane so plt.contour pass in x1 pass in x2 0.5 and then colors will be equal to blue and now there is a big mistake input z must be 2d and easy enough i forgot to pass in z so x1 x2 z and then the array 0.5 color is equal to blue and there you have it we can see in blue the uh boundary that svm uh predicted for us and you see he's doing a pretty decent job at separating everything you see you have a few points misclassified here and there but otherwise a good nonlinear boundary but now let's explore a situation where the data is not easily separable so for that we'll explore the third data set so load matte data path underscore three as before x uh sorry about that x underscore three will be equal to data three x y underscore three will be equal to data three y and now we will plot the data using our helper function so plot data we pass in x3 y3 the labels will be the same so x and y positive and negative for the labels and we want to plot from minus 0.55 to 0.35 and from minus 0.8 to 0.6 so that makes the data points nice and centered and as you can see now there is a clear overlap between both classes so there is no clear boundary between each class so this is when we need to use cross validation in order to find the best parameters for the best boundary so i will specify a list of possible values for sigma so we'll go from zero point zero one point zero three point one point three one three 10 and 30 and we'll do the same for the regularization parameter c so the same values 0.01.03 0.1.3 and then 1 3 10 and 30. perfect now i will initialize an empty list of errors and a limited list for sigma and c so for each value in sigma and then for each value for c we will define a classifier and we will fit it so clf is going to be equal to svm.svc the kernel will be rbf again because the boundary is likely nonlinear gamma will be equal to uh 1 over 2 times each squared and then after two brackets we pass in the value for c which is going to be each underscore c and the decision function shape will be equal to ovr perfect then we will fit it to our data so x underscore three y underscore three unravel and we will append uh the errors to our list we defined earlier so we're going to span clf.score on data 3 x val and data three y vowel dot ravel and then we're going to append to sigma c the value for sigma and for c as a tuple now running this i have a mistake that's because singa should actually be sigma and i still wrote simga at the very end yeah right here simga c is actually sigma c perfect so the loop red it is finished so now we can see uh what value for uh c and for sigma is the best so index will be np.r max pass in errors sigma max and c max will be equal to sigma underscore c at that index and now we can print out the values so the optimal value of sigma is colon squiggly brackets sigma underscore max and we'll print the optimal value for c as well and as you can see we get an optimal value of 0.1 for sigma and 1 for c so now we can fit an svm algorithm so we set sigma equal to 0.1 gamma is going to be equal to 1 over 2 times sigma squared then we will pass in those parameters to our classifier so the optimal classifier is svn.svc the kernel will be rbf gamma is equal to gamma c is equal to 1 and the decision function shape is ovr now we will fit our classifier to our data so x underscore three minus score three unravel then we will plot the data so again just go back up let's grab this line here and paste it back in our current cell and as before we will now find the points for the boundary and we will plot it so x1 and x2 is np.mesh grid pass in np range from negative 0.6 to 0.4 and we'll take steps of 0.004 to make it smooth and np range from negative 0.8 to 0.6 in steps of 0.004 that is then going to be the optimal clf dot predict and we will stack the prediction so np dot c underscore you pass in x underscore one dot ravel and x underscore two the unravel reshape z z dot reshape is x underscore one dot shape and now we will plot the boundary so plt on contour pass in x1 pass in x2 pass in z 0.5 in brackets and the color will be blue and the mistake again here is i wrote symga instead of sigma very sorry about that guys hopefully we're not making the same mistakes as i am and as you can see here we have our best boundary found from cross validation with support vector machine so it's actually not that bad and finally let's use svm for spam classification for our emails right so spam train we're simply going to load the data set called data path and i called it spam train then spam test will be equal to load mat and you pass in data spam test we will set the regularization parameter to 0.1 and then x train will be spam train and we pass in x y train will be spam train and we want the column y then x test will be spam test pass in x y test will be spam test passing y our classifier so clf underscore spam will be again svm.svc we will start with a linear kernel see how that performs so kernel is going to be linear the c parameter will be equal to c we set it above and the decision function shape will be equal to ovr then we fit our classifier to the train set so we pass in xstrain and ytrain unravel and then we will calculate the accuracy so clf underscore spam dot score and we will score against spam train x and spam train y that ravel and then we will test for the test accuracy so again cliff underscore spam dot score and you pass in uh x test and y test dot revel finally we will print out those scores so the training accuracy squiggly brackets pass in train underscore acc times 100 so we have it as a percentage already and we do the same for the test accuracy running everything uh we have a key error for x yes that's because here it should be x test and y test sorry about this little mistake so if we run this cell now our model is training and fitting and you get a training accuracy of 99.8 percent and a test accuracy of 98.9 which is very good moving on now to unsupervised learning let's cover some theory unsupervised learning is a set of statistical tools for scenarios in which we have features but no targets this means that we cannot make predictions instead we are interested in finding a way to visualize data or discovering a subgroup of similar observations unsupervised tends to be a bit more challenging because the analysis is subjective also it's hard to assess if the results are good or bad since there is no true answer in this section we will mainly focus on two techniques which are principal component analysis or pca and we will take a look at clustering algorithms let's cover pca first pca is a process by which principal components are computed and used to better understand data they can also be used for visualizations now what is a principal component well suppose you want to visualize n observations on a set of p features you could do a 2d plot for each two features at a time but that's not very efficient and unrealistic if p is very large with pca you can find a low dimensional representation of the data set that contains as much of the variance as possible that means that you will only consider the most interesting features since they account for the majority of the variants and therefore a principal component is simply the normanized linear combination of a feature that has the largest variance you see the equation here and that should remind you a bit of linear regression also this equation is for the first component the next one will be in a direction perpendicular to the first one and the third component would be perpendicular to the first two principal components in this equation here phi is referred to as the loadings here's an example of how you can apply pca in python in this case actually we are trying to visualize the iris data set in 2d this is something that we will apply later on during the coding portion so this data set contains more than two features for each species of iris so using this snippet we can initialize pca and then specify that we only want the first two principal components and then you can see here that we can uh find the explained variance ratio and then use that to plot a 2d figure of the features of this data set which is what you see here so as you can see we can plot the transformed data and see how each species of iris are different or separable from one another so that's it for pca now let's take a look at clustering methods clustering is a set of techniques for finding subgroups or clusters in a data set this helps us to partition the data into observations that are similar to one another an application of that is for example for market segmentation in the context of marketing we will first explore key means clustering which partitions data in a specified number of k clusters and we will also look at hierarchical clustering which does not need a specific number of clusters instead we can generate a dendrogram and see the clusters for all possible number of clusters but first let's focus on kmeans this method simply separates the observations into k clusters and we must provide that number it assumes that each observation belongs to at least one of the k clusters and that the clusters do not overlap it is important to note that the variation within each cluster is minimized here you can see an example of how the number of clusters will affect how the data is partitioned feel free to pause the video if you want to study this a little bit longer now clustering is achieved by minimizing the sum of the squared euclidean distance between each observation in a cluster you can see the equation here of the euclidean distance and we wish to minimize it to do so the algorithm first starts by randomly assigning each observation to a cluster then for each cluster a centroid is computed which is a vector representing the mean of the features in the cluster then each observation is assigned to the cluster whose centroid is the closest the two steps above are repeated until the cluster assignment stops changing now note that kmeans will find a local minimum therefore it highly depends on the initial cluster assignment so make sure to run the algorithm multiple times to see if you always get the same results in the coding section of the tutorial we will use kmeans clustering to perform color quantization more on that later on this process allows us to take a picture and reduce the number of colors so in this code snippet here we specify that we want only 64 colors in the picture then we can apply the algorithm on the image and output a modified image with only 64 colors the output will look like this of course because we use kmeans we can specify any number of clusters to group the most similar colors together so in this case we use 64 but later on in the coding portion we can use 10 28 whatever number we want now let's take a look at hierarchical clustering as i mentioned the potential disadvantage of kmeans is that you must specify the number of clusters and sometimes you simply don't know how many clusters you need this is when hierarchical clustering comes in because you do not need to specify the number of clusters the most common type of hierarchical clustering is called agglomerative clustering it generates a dendrogram from the leaves and the clusters are combined into larger clusters up to the trunk here is an example of gender grams we see the individual observations at the bottom and they are combined into larger clusters as you move up in the yaxis the algorithm is fairly easy to understand it starts by defining a dissimilarity measure between each pair of observations and it assumes that each observation pertains to its own cluster note that in this case the dissimilarity measure is usually the euclidean distance then the two most similar clusters are combined so that there are n minus one clusters the next two are combined resulting in n minus two clusters and that is repeated until all observations fall in one big cluster now although simple how do we define the similarity measure well that depends on the type of linkage and there are four types complete single average and centroid complete is also called maximal intercluster dissimilarity so it computes the pairwise the similarities in clusters a and b and it records the largest one with single it's the opposite and we talk about the minimal intercluster dissimilarity so here the smallest of the dissimilarities is recorded and this can mean that single observations are fused one at a time then we have average as the name suggests the average of the pairwise dissimilarities is recorded and finally we have centroid which computes the dissimilarity between the centroids of cluster a and b this is sometimes a problem as smaller clusters can be more similar to a larger one than to their individual clusters which can lead to inversions in your dendrogram complete average and centroid are definitely the most popular types of linkage note that the final dendrogram highly depends on the type of linkage you select as you can see here average and complete are quite similar to one another but with single leakage the dendrogram is quite unbalanced and that's why this method is not used often so that's it for the theory now let's get coding let's apply what we learned in python now these exercises are available as examples on the sklearn website i am simply reworking them a bit or explaining them here the links are in the description and the complete notebook on github is also in the description down below so we'll start off by importing some libraries we will need numpy we will also need matplotlib.pyplot as plt and finally from sklearn.utils we will import shuffle so let's kick off this tutorial with a clustering we will do color quantization with kmeans which is a technique to reduce the number of colors of an image while keeping the integrity of the image so to do that we will learn we will need from sklearn.datasets import load underscore sample underscore image and from sklearn dot cluster we will import kmeans now after importing our libraries we will load the image of a flower so the flower will be equal to load sample image and we will pass in the name of the image in this case it is flower dot jpeg now we need to convert to floats and divide by 255 because colors are expressed as rgb right red green and blue with values from 0 to 255 so we need to normalize that so that the image displays correctly with matplotlib so this is what we are doing here so we convert two floats and we divide by 255 to normalize everything finally we can show the image with plt.mshow and we pass in flower now i have made a mistake here uh the name np is not defined that's because i did not import numpy as np sorry about that so after rerunning this cell and rerunning this cell here we finally get the picture of our flower and this is what you should get now we will change the image to a 2d matrix so width height and depth will be equal to original shape which is tuple of flower dot shape here uh d is the depth will be three uh because as i as i explained the earlier each layer will correspond to either red green or blue so three values in this case and now we reshape it so image array is equal to np dot reshape flower and then we'll reshape with the width times the height and the other dimension will be the depth awesome now we will reduce the number of colors to 64 by running the kmeans algorithm where k will be set to 64. so our image sample will be equal to shuffle the image array we'll give it a random state equal to 42 so that the uh results are constant whenever we rerun the cell and we'll take the first 1000 samples now we will fit the kmeans algorithm and we set here the number of colors as i said this will be equal to 64. then k means will be equal to k means we initialize the model we pass in the number of clusters which is the same as the number of colors in this case and again the random state equal to 42 because as you know from the theory part um kmeans starts by randomly assigning uh each observation to a cluster so we keep the random state equal to 42 to give the same results every time and then we simply fit the algorithm then we get the indices for each color for the full image that will be useful when we need to reconstruct the image right so each pixel in the 2d array will be assigned to a certain cluster and that will help us to bring back the color and rebuild the image so it's simply the labels which is the prediction from the kmeans now we need to write a function to rebuild the image so like i said each pixel is assigned to a cluster which corresponds to a specific color so we define reconstruct underscore image and we will need as parameters the cluster centers we'll need the labels and we pass in the width and the height of the picture so d will be equal to the cluster centers dot shape and we take d at index one then the image will be simply an array of zeros in this case and the shape will of course be the width the height and the depth the label index will start at zero and then for i in the range of the width and for j in the range the height we write that image at index i j so this is the coordinates in the 2d matrix will be equal to the cluster centers at labels and that itself will be at the label index and then we increment the label index so plus equal one and finally we return the reconstructed image so that's it for this function now we are ready to display both the original image and the reconstructed one with only 64 colors so the first plot will be the original image so we'll turn off the axes and then plt.title will be the original image with 96 615 colors and then we will show the original image which in this case is simply flower and now our second plot so plt.figure 2. here we will display the reconstructed image so again turning off the axes the title will be here we will write a string actually while passing a parameter in this string so reconstructed image with n colors because you can change the number of colors we will do that after and then we show the reconstructed image so in here in there we will pass in our function we construct image and you pass in kmeans dot cluster underscore centers underscore pass in also the labels and you pass in the width and the height that we defined earlier and you get the following result so as you can see the integrity of the image is kept actually the flower itself is very similar i would say that only the background is very different so let's go above and change the number of colors just for fun so let's say we want only four colors so we're running these cells um as you can see now with four colors the image is very different but you can see it's almost like a an artistic effect that you can play around with so feel free to play around with this number of colors with yourself now let's work with pca for dimensionality reduction here we will work with the iris data set this data set has four features about three different kinds of iris flowers and our goal is to visualize the data set in two dimensions so from sqlearn.datasets we'll import load iris and from masculine decomposition import pca now let's load the iris dataset so iris will be equal to load iris the features is iris.data the target is iris dot target and then the labels or target names here is iris dot target underscore names awesome now let's initialize the pca algorithm and we will specify that we want only the first two principal components since we want a 2d plot then x underscore r is pca dot fit x dot transform x now let's actually print out the amount of variance that is explained by each principal component so the explained variance ratio from pca and you can extract this information from the pca object itself so it's pca dot explained underscore variance underscore ratio underscore running this cell as you can see the first principal component explains 92 percent of the variance and the second one 5 so that means that a total of 97 of the variance is explained with only two components so now we are ready to plot our data set in 2d and that data that newly transformed data contains about 97 of the variance of the original data set so here we'll just specify three different colors to distinguish between the three different kind of iris flowers so the final one will be ffa 600 and we'll set the line width equal to 2. then plt.figure and then for color in oh sorry so for color i target name in zip and we pass in colors we will pass in uh zero one and two and we pass in the target names so 0 1 and 2 here are simply the the classes right so we'll draw a scatter plot so plt.scatter xr when y is equal to i and zero and then x r when y is equal to i and one so this is basically the xaxis and then the yaxis and the color will be equal to uh the color at this point in the loop alpha will be equal to 0.8 and then lw we set it equal to lw that we specified above finally the label will be equal to the target name at that specific step in the loop now we will simply put a legend on our plot the location sorry the location equal will be equal to best and we don't want any shadow finally let's set a title to our plot so pca of iris dataset running this cell as you can see now we get this plot right here and so you can visualize in two dimension a dataset that contained four features and three classes so now you could follow up with some classifier maybe decision trees on this transform data set to classify each kind of flower alright so that's it for this data science crash course i hope that you enjoyed it and that you learned something interesting if you want more videos on this topic or videos on endtoend data science projects please check out my youtube channel until next time take care