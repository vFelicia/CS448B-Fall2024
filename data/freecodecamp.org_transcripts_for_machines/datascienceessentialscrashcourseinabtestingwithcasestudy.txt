in this applied data science crash course you learn all about AB testing from the concepts to the Practical details they can apply in business AB testing is commonly used in data science it's an experiment on two variants to see which performs better based on a given metric this course merges indepth statistical analysis with the kind of data science theories big Tech firms rely on T from L Tech developed this course she is a very experienced a scientist and teacher welcome to the handsone ab Testing crash course where we will do some refreshment when it comes to AB testing if you're looking for that one course where you can learn and quickly refresh your memory for AB testing and how to actually do an AB testing case study hands on in Python then you are in the right place in this crash course we are going to refresh our memory for the a test design including the power analysis and defining those different PR such as minimum detectable effect statistical significance level and also the uh type two probability so the power of the test and then we are going to do HandsOn case study project where we will be conducting an AB testing results analysis in Python at the end of this course you can expect to know everything about designing an AB test what it means as to design a proper AB test and how to do a Ab test results analysis in Python in a proper way I'm dat Vas and cofounder at Lun Tech and I have been in data science for the last 5 years I have learned AB testing end to end after following numerous blogs and numerous research papers and courses and I've noticed that there is not a one place one course that will cover all the fundamentals and necessary stuff both the theory and implementation in Python in one place and that's about to change as we have this crash course that will help you to do exactly that to learn how to design an AB test in a proper way as a good and solidated scientist and to Showcase your skills by doing python AB testing results and asset don't forget to subscribe like and comment to help the algorithm to make this content more accessible to everyone across the world and if you want to get free resources make sure to check the free resources section at lunch. and if you want to become a job ready data scientist and you are looking for this accessible boot camp that will help you to make your job ready data scientist consider enrolling to the data science boot camp so whether you are a product scientist whether you are a data analyst data scientist or a product manager who wants to learn about AB testing at high level and how it can be done in Python then you are in the right place because in this crash course we're going to refresh our memory what it means to properly design an a test test which means doing power analysis and also calculating the sample size by hand by following the statistical guidelines and ensuring that everything is done properly and then as the second part of this Crush course we are also going to do an handson case study in Python when it comes to performing AB testing results analysis so we are going to cover all these important Concepts such as P values sample size and also uh interpreting the ab test results using standard error calculating those uh estimates pulled variance and then evaluating the ab test results including confidence interal generalizability of the results reproducibility of the results so without further Ado let's get started AB testing is an important topic for data scientists to know because it's a powerful method for evaluating changes or improvements to the products or services it allows us to make data driven Decisions by comparing the performance of two different versions of a product or a service usually referred as treatment or control for example a testing allows data scientists to measure the effectiveness of changes to your product or a service which is important as it enables data scientists to make data driven decisions rather they're relying on Intuition or assumptions secondly AB testing helps data Sciences to identify the most effective change changes to a product or a service which is really important because it allows us to optimize the performance of a product or a service which can then lead to increased customer satisfaction and sales AB testing helps us also to validate certain hypothesis about what changes will improve a product or service this is important because it helps us to build a deeper understanding of the customers and the factors that influence customers Behavior finally AB testing is a common practice in many Industries such as ecommerce digital marketing website optimization and many others so data scientists who have knowledge and experience in a testing will be more valuable to these companies no matter in which industry you want to enter as a data scientist and what kind of job you will be interviewed for and even if you believe more technical data scien is your cup of tea be prepared to know at least higher level understanding and the details behind this method will definitely help you to know about this topic when you are speaking with product owners stakeholders product scientists and other people involved in the business let's briefly discuss the perfect audience for the section of the course and prerequisites there are no prerequisites of the section in terms of AB testing Concepts that you should know already but knowing the basics and statistics which you can find in the fundamentals to statistics section is highly recommended this section will be great if you have no priority AB testing knowledge and you want to identify and learn the essential AB testing Concepts from scratch so this will help you to prepare for your job interviews it will also be a good refresher for anyone who does have AB testing knowledge but who wants to refresh their memory or want to fill in the gaps in their knowledge in this lecture we will start off the topic about AB testing where we will formally Define what AB testing is and we will look at the high level overview of AB testing process step by step by definition AB testing or split testing is originated from the statistical randomized control trials and is one of the most popular ways for businesses to test new ux features new versions of a product or an algorithm to decide whether your business should launch that new ux feature or should productional IE that new recommender system create that new product that new button or that new algorithm the idea behind a testing is that you should show the variated or the new version of the product to sample of customers often referred as experimental group and the existing version of the product to another sample of customers referred as control group then the difference in the product performance in experimental versus control group is tracked to identify the effect of these new versions of the product on the performance of the product so the goal is then to track the metric during the test period and find out whe there is a difference in the performance of the product and and what type of difference is it the motivation behind this test is to test new product variants that will improve the performance of the existing product and will make this product more successful and optimal showing a positive treatment effect what makes this testing great is that businesses are getting direct feedback from their actual users by presenting them the existing versus the variated product version and in this way they can quickly Test new ideas in case of ab Test shows that the variated version is not effective at least businesses can learn from this and can decide whether they need to improve it or need to look for other ideas let us go through the steps included in the AB testing process which will give you a higher level overview into the process the first step in conducting AB testing is stating the hypothesis of the ab test this is a process that includes coming up with business and statistical hypothesis that you would like to test with this test including how you measured the success which will primary metric next step in AB testing is to perform what we call power analysis and design the entire test which includes making assumptions about the most important parameters of the test and calculate the minimum sample size required to claim statistical significance the third step in AB testing is to run the actual AB test which in practical sense for the data scientist means making sure that the test runs smoothly and correctly collaborate with engineers and product managers to ensure that all the requirements are satisfied this also includes collecting the data of control and experimental groups which will be used in The Next Step next step in AB testing is choosing the right statistical test whether it is z test T Test Ki Square test Etc to test the hypothesis from the step one by using the data collected from the previous step and to determine whether there is a statistically significant difference between the control versus experimental group The Fifth and the final step in AB testing is continuing to analyze the results and find out whether besides statistical significance there is also practical significance in this step we use the second step's power analysis so the assumptions that we made about model parameters and the simple siiz and the four steps results to determine whether there is a practical significance beside of the statistical significance this summarizes the AB testing process at a high level in next couple of lectures we'll go through the steps one at a time so buckle up and let's learn about AB testing in this lecture lecture number two we will discuss the first step in a testing process so let's bring our diagram back as you can recall from the previous lecture when we were discussing the entire process of AB testing at a high level we saw that in the first step in conducting AB testing is stating the hypothesis of ab test this process includes coming up with a business and statistical hypothesis that you would like to test with this test including how you measured the success which we call a primary metric so what is the metric that we can use to say that that the product that we are testing performs well first we need to State the business hypothesis for our AB test from a business perspective so formally business hypothesis describes what the two products are that being compared and what is the desired impact or the difference for the businesses so how to fix a potential issue in the product where a solution of these two problems will influence what we call a key performance indicator or the kpi of the interest business hypothesis is usually set as a result of brainstorming and collaboration of relevant people on the product team and data science team the idea behind this hypothesis is to decide how to fix a potential issue in the product where a solution of these problems will improve the target kpi one example of business hypothesis is that changing the color of learn more button for instance to Green will increase the engagement of the web page next we need to select what we call primary metric for our av testing there should be only one primary metric in your ab test choosing this metric is one of the most important parts of ab test since this metric will be used to measure the performance of the product or feature for the experiment Al and control groups and they will be used to identify whether there is a difference or what we call statistically significant difference between these two groups by definition primary metric is a way to measure the performance of the product being tested in the ab test for the experimental and control groups it will be used to identify whether there is a statistically significant difference between these two groups the choice of the success metric depends on the underlying hypothesis that is being tested with this AB test this is if not the most one of the most important parts of the ab test because it determines how the test will be designed and also how will the proposed ideas perform choosing poor metrics might disqualify a large amount of work or might result in wrong conclusions for instance the revenue is not always the end goal therefore in AB testing we need to tie up the primary metric to the direct and the higher level goals of the product the expectation is that if the product makes more money then this suggests the content is great but in achieving that goal instead of improving the overall content of the material and writing one can just optimize the conversion funless one way to test the accuracy of the metric you have chosen as your primary metric for your ab test could be to go back to the exact problem you want to solve you can ask yourself the following question what I tend to call the metric validity question so if the Chen metric were to increase significantly while everything else T constant would we achieve our goal and would we address our business problem is it higher revenue is it higher customer engagement or is it high views that we are chasing in the business so the choice of the metric will then answer this question though you need to have a single primary metric for your ab test you still need to keep an eye on the remaining metrics to make sure that all the metrics are showing a change and not only the target one having multiple metrics in your ab test will lead to false positives since you will identify many significant differences well there is no effect which is something you want to avoid so it's always a good idea to pick just a single primary metric but to keep an eye and monitor all the remaining metrics so if the answer to the metric validity question is higher Revenue which means that you are saying that the higher revenue is what you are chasing and better performance means higher revenue for your product then you can use your primary metric what we call a conversion rate conversion rate is a metric that is used to measure the effectiveness of a website a product or a marketing campaign it is typically used to determine the percentage of visitors or customers who take a desired action such as making a purchase filling out a form or signing up for a service the formula for conversion rate is conversion rate is equal to number of conversions divided to number of total visitors multiplied by 100% for example if a website has thousand visitors and 50 of them make a purchase the conversion rate would be equal to 50 divide 2,000 multiply by 100% which gives us 5% this means that our conversion rate in this case is equal to 5% conversion rate is an important metric because it allows us and businesses to measure the effectiveness of their website a product or a marketing campaign it can help businesses to identify areas for improvement such as increasing the number of conversions or improving the user experience conversion rate can be used for different purposes for example if a company wants to measure the effectiveness of an online store the conversion rate would be the percentage of visitors who make a purchase and on the other hand if a company wants to measure the effectiveness of landing page the conversion rate would be the percentage of visitors who fill out a form or sign up for a service so if the answer to the metric validity question is higher engagement then you can use the clickr rate or CTR as your primary metric this is by the way a common metric used in a testing whenever we are dealing with ecommerce product search engine recommander system clickr rate or CTR is a metric that measures the effectiveness of a digital marketing campaign or the user engagement or some feature on your web page or your website and it's typically used to determine the percentage of users who click on a specific link or button or call to action CTA out of the total to number of users who view it the formula for the clickr rate can be represented as follows so the CTR is equal to number of clicks divided to number of Impressions multiply by 100% not to be confused with click through probability because there is a difference between the click through rate and click through probability for example if an online advertisement receives thousand of Impressions which means that we are showing it to the customers for a thousand times and there were 25 clicks which means 25 out of all this impression resulted in clicks this means that the clickr rate for this specific example would be equal to 25 divide 2,000 multiply by 100% which gives us 2.5% this means that for this particular example our clickr rate is equal to 2.5% cure rate is an important metric because it allows businesses to measure the effectiveness of their digital marketing campaigns and the user engagement with their website or web pages High click through rate indicates that a campaign or the web page or feature is relevant and appealing to the target audience because they are clicking on it while low clickthrough rate indicates that a campaign or the web page needs an improvement click through rate can be used to measure the performance of different digital marketing channels such as PID search display advertising email marketing and social media it can also be used to measure the performance of different ad formats such as text advertisements Banner advertisement video advertisements Etc next and the final task in this first step in the process of AP testing is to State the statistical hypothesis based on business hypothesis and the chosen primary metric next and in the final task in this first step of the AB testing process we need to State the statistical hypothesis based on the business hypothesis we stated and the chosen primary metric in the section of fundamentals through statistics of this course in lecture number seven we went into details about statistical hypothesis testing included what n hypothesis is and what alternative hypothesis is so do have a look to get all the insight about this topic AB testing should always be based on a hypothesis that needs to be tested this hypothesis is usually set as a result of brainstorming and collaboration of relevant people on the product team and data science team the idea behind this hypothesis is to decide how to fix a potential issue in a product where a solution of these problems will influence the key performance indicators or the kpi of interest it's also highly important to make prioritization out of a range of product problems and ideas to test while you want to P that fixing this problem would result in the biggest impact for the product we can put the hypothesis that is subject to rejection so that we want to reject in the ideal World Under The N hypothesis what we Define by AG zero well we can put the hypothesis subject to acceptance so the desire hypothesis that we would like to have as a result of AB testing under the alternative hypothesis defined by H1 for example if the kpi of the product is to increase the customer engagement by changing the color of the read more button from blue to green then under the N hypothesis we can state that clickr rate of learn more button with blue color is equal to the click through rate of green button under the alternative we can then state that the click true rate of the learn more button with green color is Lar larger than the click through of the blue button so ideally want to reject this no hypothesis and we want to accept the alternative hypothesis which will mean that we can improve the clickr rate so the engagement of our product by simply changing the color of the button from blue to green once we have set up the business hypothesis selected the primary metrics and stated the statistical hypothesis we are ready to proceed to the next stage in the ab testing process in this lecture we will discuss the next Second Step In AB testing process which is designing the ab test including the power analysis and calculating the minimum sample sizes for the control and experimental groups stay tuned as this is a very important part of AB testing process commonly appearing during the data science interviews some argue that AB testing is an art and others say that it's a business adjusted common statistical test but the borderline is that to properly Design This experiment you need to be disciplined and intentional while keeping in mind that it's not really about testing but it's about learning following AR steps you need to take to have a solid design for your ab test so let's bring the diagram back so in this step we need to perform the power analysis for our AB test and calculate the minimum sample size in order to design our AB test AB test design includes three steps the first step is power analysis which includes making assumptions about model parameters including the power of the test the significance level Etc the second step is to use these parameters from Power analysis to calculate the minimum sample size for the control and experimental groups and then the final third step is to decide on the test duration depending on several factors so let's discuss each of these topics one by one power analysis for AB testing includes this tree specific specific steps the first one is determining the power of the test this is our first parameter the power of the statistical test is a probability of correctly rejecting the N hypothesis power is the probability of making a correct decision so to reject the N hypothesis when the N hypothesis is false if you're wondering what is the power of the test what is this different concepts that we just talk about what is this null hypothesis and what does it mean to reject the null hypothesis then head towards the fundamental statistic section of this course as we discuss this topic in detail as part of that section the power is often defined by 1 minus beta which is equal to the probability of not making a type two error where type two error is a probability of not rejecting the null hypothesis while the null is actually false it's common practice to pick 80% as the power of the ab test which means that we allow 20% of type to error and this means that we are fine with not detecting so failing to reject n hypothesis 20% of the time which means that we are fine with not detecting a true treatment effect while there is an effect which means that we are failing to reject the N however the choice of value of this parameter depends on nature of the test and the business constraints secondly we need to determine a significance level for our AB test the significance level which is also the probability of type one error is the likelihood of rejecting the no hence detecting a treatment effect while the know is actually true and there is no statistically significant impact this value often defined by a Greek letter Alpha is a probability of making a false Discovery often referred to as a false positive rate generally we use the significance level of 5% which indicates that we have 5% risk of concluding that there exists a statistically significant difference between the experimental and control variant performances when there is no actual difference so we are fine by having five out of 100 cas Cas is detecting a treatment effect well there is no effect it also means that you have a significant result difference between the control and the experimental groups within 95% confidence like in the case of the power of the test the choice of the alpha is dependent on the nature of the test and the business constraints that you have for instance if running this a test is related to high engineering course then the business might decide to pick a high offer such that it would be easier to detect a treatment effect on the other hand the implementation costs of the proposed version in production are high you can then pick a lower significance level since this proposed feature should really have a big impact to justify the high implementation cost so it should be harder to reject n hypothesis finally as the last tyep of power analysis we need to determine a minimum detectable effect for the test last parameter as part of the power analysis we need to make assumptions about is what we call minimum detectable effect or Delta from the business point of view so what is the substantive to the statistical significance that the business wants to see as a minimum impact of the new version to find this variant investment worthy the answer to this question is what is the amount of change we aim to observe in a new versions metric compared to the existing one to make recommendations to the business that this feature should be launched in the production that it's investment worthy an estimate of this parameter is what is known as as a minimum detectable effect often defined by a Greek letter Delta which is also related to the Practical significance of the test so this mde or the minimum detectable effect is a proxy that relates to the smallest effect that would matter in practice for the business and it's usually set by stakeholders as this parameter is highly dependent on the business there is no common level of it instead so this minimum detectable effect is basically the translation from statistical significance to the Practical significance and here we want to see and we want to answer the question what is this percentage increase in the performance of the product that we want to experiment with that will tell to the business that this is good enough to invest in this new feature or in this new product and this can be for instance 1% for one product it can be 5% for another one and it really depends on the business and what is the underlying kpi a popular reference to the parameters involved in the power analysis for AB testing is like this so 1 minus beta for the power of the test Alpha for the significance level Delta for the minimum detectable effect to make sure that our results are repeatable robust and can be generalized to the entire population we need to avoid P hacking to ensure real statistical significance and to avoid biased results so we want to make sure that we collect enough amount of observations and we run the test for a minimum predetermined amount of time therefore before running the test we need to determine the samp size of the control and experimental groups as well as later on in this lecture we will see also how long we need to run the test so this is another important part of AB testing which needs to be done using the defined power of the test which was the one minus beta the significance level and a minimum detectable effect so all the parameters that we decided upon when conducting the power analysis calculation of the sample size depends on the underlying primary metric as well that you have chosen for tracking the progress of the control and experimental versions of the product so we need to distinguish here two cases so when discussing the primary metric we saw that there are different ways that we can measure the performance of different type of products if we are interested in engagement then we are looking at a metric such as click through rate which is in the form of averages so the case one will be where the primary metric of AB testing is in the form of a binary variable it can be for instance conversion or no conversion click or no click and in case two where the primary metric of the test is in the form of proportions or averages which means mean order amount or mean click through rate for today we will be covering only one of these cases but you can find more details on the second case in my blog which I posted also as part of the resources section this blog post contains all the details that you need to know about AB testing including the statistical test and their corresponding hypothesis the descriptions of different primary metrics that go beyond what we have covered as part of this section as well as many more details that you need to know about a testing so let's look at a case two where the primary metric of the test is in the form of proportions or averages so let's say we want to test whether the average click to rate of control is equal to the average click to rate of experimental group and under HD we have that the m control is equal to M experimental and under H1 we have that the m control is not to Mu experimental so here the MU control and mu experimental are simply the average of the primary metric for control group and for the experimental group respectively so this the formal hypothesis we want to test with our AB test and we can assume that this new control is for instance the clickr rate of the control group and the MU experimental is the clickr rate of the experimental group so this is the formal statistical hypothesis we want to test with our AB test if you haven't done so I would highly suggest you to head towards the fundamental statistic section of this course where in lecture number seven and eight of the statistical part of this course I go in detail about statistical hypothesis testing the means averages significance level Etc this also holds for the theorem that the some prise calculation is based upon called Central limit theorem so check out the last lecture about inferential statistics where I covered the central limit theorem which we will also use in this section and finally also check the lecture number five in that section where we cover the normal distribution another thing that we will use as part of this section so the central limit theorem states that given a sufficiently large sample size from an arbitrary distribution the sample mean will be approximately normally distributed regardless of the shape of the original population distribution this means that the distribution of the sample means will be approximately normal if we take a large enough sample even if the distribution of the orig sample is not normal so when we are dealing with a primary performance tracking metric that is in the form of average such as this one that we are covering today which is a clickr rate we intend to compare the means of the control and experimental groups then we can use the central limit theorem as state that the mean sampling distribution of both controlling experimental groups follow normal distribution consequently the sampling distribution of the difference of the means of these two groups also will be normally distributed so this can be expressed like this where we see that the mean of the control group and mean of the experimental group follows normal distribution with mean mu control and mu experimental respectively and then with the variance of Sigma control squared and sigma experimental squared respectively though derivation of this Pro is out of the scope of this course we can state that the difference between the means of the true group so xar control minus xar experimental also follows normal distribution with a mean new control minus new experimental and with a variance of Sigma control squ / to n Control Plus Sigma experimental Square / to n experimental so the sample size of the experimental group and the sample size of the control group hence the sample size needed to compare the me of the two normally distributed samples using a twosided test which prespecify significance of alpha power level and minimum detectable effect can be calculated as follows so here you can see the mathematical representation of the minimum sample size so the N which stands for the minimum sample size is equal to and in denominator we have Sig control S Plus Sigma experimental squar multip by z 1us alpha / to 2 + z 1us beta squ / to the Delta squ and here the Alpha and the beta and the Delta we have made assumptions about as part of the power analysis and the sigma control squar and a sigma experimental squared are the uh estimates of the variance that we can come up with using the SoCal A8 testing I would say you do not necessarily need to know this derivation as there are many online calculators that will ask you for the alpha the beta and the Delta values as well as the sample estimates for the sigma squ control and experimental and then these calculators will automatically calculate the minimum S size for you if you're wondering what this AA testing is and how we can come up with the sigma control squared and sigma experimenting squared as well as all the other values then make sure to to check out the blog that I posted before and that I mentioned before as I explained in detail all these values as well as check out the resource section where I've included many resources regarding this but for now just keep in mind that the Z1 minus Alpha / to two and Z1 minus beta are just two constants and come from the normal distributed and standard normal distributed tables I would say you do not necessarily need to know this derivation as there are many online calculators that will ask you for this Alpha Beta And Delta values as well as the sule estimates for the sigma squ controling Sigma experimental control and then we'll calculate automatically the sample size for you for the control and experimental group effectively one example of such calculator is this AB testy online calculator but if you Google it you will find many others that will ask you for the minimum detectable effect for the statistical significance or the statistical power and then it will automatically calculate for you the minimum sample size that you should have in order to have a statistical significance and in order to have a valid AB test one thing to keep in mind is that you will notice that the statistical significance level is set to 95% in here which is not what we have seen when we were discussing the alpha significance level so sometimes these online calculators will confuse or will interchangeably use the significance level versus the confidence level which are the opposite so the significance level is usually at the level of 5% or 1% confidence level is around 95% so which is basically 100% minus the alpha therefore whenever whenever you see this 95% know that this means that your Alpha should be 5% so it's really important to understand how to use this calculator not to end up with the wrong minimum sample size conduct an entire AB test and then at the end realize that you have used the wrong uh significance level the final step is to calculate the test duration this question needs to be answered before you run your experiment and not during the experiment sometimes people stop the test when they detect statistical significance which is what we call P hacking and that's absolutely not what you want to do to to determine the Baseline of the duration time a common approach is to use this formula as you can see duration is equal to n ided to the number of visitors per day where n is your minimum sample size that we just calculated in the previous step and the number of visitors per day is the average number of visitors that you expect to see as part of your experiment for instance if this formula results in 14 days or 14 this suggest that running the test for two weeks is a good idea however it's highly important to take many business specific aspect into account when choosing the time to run the test and for how long you need to run it and simply using this formula is not enough for example if you want to run an experiment at the end of the month December with Christmas breaks when higher than expected or lower than expected number of people are usually checking your web page then this external and uncertain event had an impact on the page page to search for some businesses this means for example if you want to run an experiment at the end of the month of December with Christmas breaks when higher than expected or in some cases lower than expected number of people are usually checking the web page so depending on the nature of your business or the product then this external and uncertain event can have an impact on the page usage for some businesses which means that for some businesses a high increasing the page usage can be the result and for some a huge decrease in usability in this case running AB test without taking into account this external Factor would result in inaccurate results since the activity period would not be true representation of a common page usage and we no longer have this Randomness which is a crucial part of AB testing beside this When selecting a specific test duration there are few other things to be aware of firstly two small test duration might result in what we call novelty effects users tend to react quickly and positively to all types of changes independent of their nature so it's referred as a novelty effect and it vares of in time and it is considered illusionary so it would be wrong to describe this effect to the experimental version itself and to expect that it will continue to persist after the noble T effect wears off hence when picking a test duration we need to make sure that we do not run the test for too short amount of time period otherwise we can have a novelty effect novelty effect can be a major threat to the external validity of an AV test so it's important to avoid it as much as possible secondly if the test duration is too large then we can have what we call maturation effects when planning an AB test it's usually useful to consider a longer test duration for allowing users to get used to a new feature or product in this way one will be able to absorve the real treatment effect by giving more time to returning users to cool down from an initial positive reaction or a spike of Interest due to a change that was introduced as part of a treatment this should help to avoid novelty effect and is better predictive value for the test outcome however the longer the test period the larger is the likelihood of external effect impacting the reaction of the users and possibly contaminating the test results this is what we call maturation effect and therefore running the AP test for too short amount of time or too long amount of time is not recommended as it's a very involved topic we can talk for hours about this part of the ab test and also a topic that is asked a lot during the data science and product scientist interviews therefore I highly suggest you to check out this book about AB testing which is a HandsOn tutorial about everything you need to know about AB testing as well as check out the interview preparation guide in this section that contains 30 most popular AB testing related questions you can expect during your data science interviews so stay tuned and in the next couple of lectures we will cover the next stages of AB testing process if you are looking for one place to learn everything about AB testing without unnecessary difficulties but also with a good statistical and da Science Background then make sure to check out the AB testing course at lunch. a so if you want to learn all this background information including what is statistical significance what is AB testing how can AB testing be done and you want to have this endtoend AB testing course then make sure to check the AB testing for data science course at l. that's the only course that is available at the moment on the internet that covers the most fundamental concept of a testing including the theory and the implementation in Python without know the extra details and right going straight to the point in order to help you to Kickstart your Journey with AB Tes the resource that I would suggest you to keep by the hand is the blog called complete guide toab testing design implementation and pitfalls which is part of the HandsOn tutorials of the towards data science so in here and specifically this part where we are discussing the two sample that test I would suggest you to go through it as we are going to conduct this two sample Z test as part of our Python and we are going to learn how to implement this in Python in this book you can learn everything out there that you need to know about AV testing including different uh pits include of Av testing the process behind it how you can conduct the ab test end to end how you can calculate a sample size how you can choose a test the primary metric definitions different statistical test that you can use including the Ki Square test the two sample Z test and two sample T Test so given that as part of the lectures of the um AB testing and specifically lecture number five we have already discussed the two sample T Test and how to implement it I thought it would be more useful for you to know how to implement the two sample Z test such that you know both of them and you know their theory behind it and also how to implement them and finally if you are wondering how you can Implement them in Python then head towards my uh blog uh in the medium as well as my GitHub repository that I will post in the resource section where you can find all the different statistical tests you can use for analyzing your ab test results including the two sample T Test two sample Z test K Square test and much more so without further Ado let's get started with our demo so uh as you can see here I'm generating the data myself assuming that uh the uh primary metric follows bomal distribution so the output is in the form of zeros and ones because we are looking into the click event and click can be either zero or one and then I'm using here the binomial distribution to randomly sample from it and in case of the experimental version I'm using a probability of success equal to 0.4 and in case of the control version I'm using a probability of success equal to 0.2 because I want to have a quiet difference between the two groups and then later on we can also adjust this and we can change the difference to see how our it has behaves so um I'll assume that um the uh at the end of the uh data generation process we have a data that is similar to the form that you will get from the uh engine engers once they uh finish up collecting all the data from your customers and I will also assume that the Integrity of theab test is held which means that the observations who were in the control group they only saw the control version of the product and observations who were in the experimental group they only saw the experimental version of the product and let's actually go ahead and see how the data looks like so so as you can see here we are generating our data so the data is in this format so you can see that we have an observation in total we have 20K observations because we have two different groups each with 10K observations and then the first col describes the click event so we will either have a click or we will have no click and the primary metric is in the form of a click so we are measuring the performance of the product both control and the experimental with the same metric which is whether there is a click event or no click event and the primary metric is in the form of a binary variable so we have either zeros or we have ones Whenever there is a click then the corresponding value is one whenever there is no click then the corresponding value is zero and then we have the corresponding Group which helps us to understand whether the observation belongs to the experimental group so X or the control group which is a uh Co so uh this is how the data looks like and this also what you can uh expect from uh data Engineers uh once the uh AB test is conducted so you have run your ab test and Engineers have collected data assuming that the data Integrity has been kept and also that there was no systematic error when collecting and measuring the performance of the uh control and the experimental versions of the product first thing that we are going to do is to estimate the P hat control and a p hat experimental and for that what we need to do first is to count the number of clicks per group so we saw earlier that we have this data that we generate ourselves consisting of 20 K rows where 10 belongs to the uh control group and the 10K belongs to the experimental group and each consists of this click variable and the group The Click variable is an indicator uh that says that the observation clicked on the uh page versus uh not clicked on the page so whenever there was a click we have here one whenever there was no click we have here zero and then we have the corresponding uh group such that we can use to group this data based on the control versus experimental group and that's exactly what we are going to do as the first step in our process so we are going to calculate the number of total clicks for control group and for the experimental group so here we are making use of the function Group by in order to group this data frame so this data frame based on the group and then we want to click uh the we want to get the uh click variable and we want to sum this variable because the variable is of a binary nature so we have ones and zeros if we do the sum we are basically counting the number of times we have the uh observation click equal to one so by summing a binary variable we are simply getting the number of ones in that variable and that's exactly what we are doing in this part and then what is remaining is to get the uh number of clicks from control group and number of clicks from the experimental group by using this function code look so we saw earlier when we were discussing the um accessing of observations in a pendis data frame that there is a difference between iog and loog and the reason why we are using here the loog is because the uh group uh data that we are getting in here it will provide us an output where the index is in the format of a string so let's actually go ahead and print that part because I think it's an important part to see how the data looks like and it also will make sense why I'm using here the look function to access the uh control groups number of clicks and the experimental groups number of clicks so this is the uh group data frame that we are getting as you can see we are getting here the group and here we are getting for the control uh index the number of clicks is equal 2,924 and for the experimental group it's equal to 5,7 so then the next thing what we need to do is actually access this value and for that we need to specify that we want to access the value corresponding to the index equal to control and this can be done by using this log function so you cannot use ilog or any other way of accessing this because the index is of string type and therefore we are using the log so let's actually also add some print statements to make our code more readable so this will then print the number of clicks per control group and per experimental group here we go so as you can see we are nicely accessing the correct values then the next step is to calculate the P had control and the P experimental so basically the estimate of the click probabilities of the control group and the experimental group respectively and for that we just need to take the uh number of clicks and we need to divide it to the number of observations for that group so it is this part let's go ahead and calculate those values so as you can see I'm taking the number of clicks that we just obtained and I'm dividing it to the number of observations that we have defined in the very beginning here we go so as you can see for the control group the uh click probability is equal to 020 and in case of the experimental group is equal to 0.5 so we see that there is a large difference between the quick probability for these two groups which is um a reflection of what we saw here because we generated the data such that the uh success probability for the experimental group is equal to 0.5 and the um for the control group is equal to 0.2 so we see these numbers reflecting also in here and the reason for that is because we have sampled our data large enough and we see that the um probability so the the mean of our sample um converges in a probability to the mean that we use and this is also the idea behind the low of large num something that we have also discussed as part of the fundamental to statistic section of this course so the next thing what we need to do is to compute the P poed hat or the uh estimate of the pulled success probability and we saw uh when we were discussing the theory behind it that it's equal to the sum of the clicks for both control and experimental group divided to the total number of observations in both control and the experimental group so basically the P pulled head is equal to xcore control plus xcore experimental ided to the ncore Control Plus ncore experimental then the next thing we need to do is to compute a pulled variance and we just so that the pulled variance can be calculated by taking the pulled uh estimate for the click probability so this p p and then multiply by one minus p p head and then multiply by the inverses of the uh observ number of observations in each of the groups and there sum so 1 / 2 N Control Plus 1/ to an experimental so it can be calculated as follows so pulled variance then is equal to P ped head multip by 1 minus P ped head multip by 1 / to n control + 1 / to n experimental let's also add some print statements here we go and then the next step is to calculate the standard error so the standard error is the square root of the pulled variance so quite straightforward and here we are going to make use of the npy function so the SE is equal to npy Dot and a square roof is simply uh calculated by using the function sqrt which stands for square roof and then here we need to mention the pulled variance let's also add the print statement explaining the uh the code and this really can help your reviewer the code reviewer to understand what you are doing okay so now we have also the standard eror and now we are ready to calculate our test statistics so we saw that the test statistics is equal to the P control head minus P experimental head divided to the standard error and that's exactly what we are going to implement in here so as you can see the test statistics is equal to P control head minus P experimental head divided 2D SC so standard error and then finally what we need to do is to compute the Z critical value the P value and the confidence interval but for doing that we need to assume the significance level so usually this is done before conducting the test but here I'm assuming that before conducting the test there was a power analysis and as part of that we have decided that the statistical significance level is equal to 5% so let's add that here so Alpha is equal to 0.05 therefore we are going to use this specific Alpha so 5% in order to calculate our critical value coming from the normal table and to do this there are uh various options so one way of doing that is to hard code the value which I would not recommend but it is definitely uh an easy way to go if you um haven't used uh the python libraries to automize this process but here I will provide you the code and I will also tell you how you can use the scipi norm um function in order to calculate the critical volume and I think keeping the code as general as possible will help you in the term to because it can be that this time you're calculating the critical value corresponding to Alpha is equal to 0.05 but maybe next time you want to calculate the critical value when your Alpha is equal to 1% so you're interested in the uh case when your type one probability is equal to 1% so for those cases uh you want to keep your code as general as possible such that by changing your uh variable let's say Alpha you don't need to go each time and then in the chat GPT look for the a corresponding uh value coming from the standard normal table so for this what we are going to use is the norm function so the norm function come from the CPI stats library for that we need to import from ci. stats the norm function which stand for the normal distribution so in here what we need to use is the function called ppf which is the uh percentage Point function so the norm done ppf function stands for the present Point function and it's usually known as the inverse cumulative distribution function or the CDF of the standard normal distribution and it takes as an input the probability value and it Returns the corresponding value on the xaxis of the CDF once you provide a p so here we are providing the P which is equal to 1 minus Alpha / to 2 then this function calculates the X so the xaxis such that the probability of observing a volue less than or equal to two or 2 x in a standard normal distribution is equal to P so we have this inverse CDF and we have the xaxis and we have the yaxis on the y axis we have the probabilities and on the xaxis we have the X values so here what we are basically doing is that we are providing the probability that we have which is equal to 1 minus Alpha / to 2 and we want to know the corresponding X valume therefore it's also called inverse humity distribution function and in this way we can calculate Z critical value which can help us to identify the place where we need to have our rejection region and so here is the uh rejection region of this test and as you can see we have twosided test therefore we have also a two regions and whenever the um test statistics is larger than the critical value in the right hand side and it is smaller than the critical value from the left hand side then we are saying that we can reject the N hypothesis therefore it's also called the rejection region so uh once we calculate this set critical value we are ready to go to the next step but before that let's also add some statement print statement for readability here so the next step is to calculate a p volume and a p volue can be calculated by using the norm. SF function so the norm function comes once again from the scipi library and the SF stands for survival function the norm. SF function stands for a survival function and it stands for the complement of the CDF function so the cumulative distribution fun function of the standard normal distribution it calculates the probability of observing a value greater than a given threshold so in this case we want to calculate the uh probability that our test statistics will be smaller than equal the critical volume and as we saw that the standard normal distribution was symmetric here we are multiplying just one side of that probability by two in order to obtain our final value so here once we run this test we will finally get our P value and as you can see here the P value of the two sample that test that we got is equal to zero well now once we have the P value and also we know what is our Alpha we are ready to test for the statistical significance of our results so given that our P value is equal to zero and it's smaller than 0.05 so our Alpha we can state that the null hypothesis can be rejected and we can state that there is a statistically significant difference between our experimental version of the product and the control version of the product so this will help us to test for the statistical significance of our AB test however if you were for instance to have a different samples so let's say we would compute uh we would randomly sample from the binomial distribution so as you can see once we are getting the uh probability of the success the same for the two groups then the P value becomes large at least much larger than the alpha which means that we can no longer reject the ne hypothesis and we can no longer State there is a statistical evidence at the 5% statistical level that the control version is statistically significantly different from the experimental version and this uh verifies that everything that we have done here is correct so the ab test results analysis is accurate now the question is whether we um also have a practical significance once we pass the statistical significance test so let's move this back to what we had before so this is your .5 and once again the P value is just zero and let's go ahead and calculate our confidence interval such that we can test for the Practical significance and we can comment on the accuracy of the test and the general ability of our AB test so we saw that the confidence interval can be calculated as follows so we have the difference between the P had experimental and the P had control and then for the lower bound we need to uh subtract from this standard or multiply by Z critical value and then for the upper bound we need to do the same only with summing the standard multiply by Z critical volume so the difference here you might notice is this round function and the reason why I'm adding this is because I want to have nice numbers that will be rounded uh just three numbers after to decimal instead of having long uh floating numbers so once we go ahead and print this confidence interval we can also see the lower bound and upper bound in numbers here we go so as you can see we are getting a confidence info which is quite narrow so this is a suggestion that our AB test results are most likely accurate and that the Precision of our AB test is high and this is a good sign because then we can say that the ab test we have conducted in here is most likely generalizable to the entire population then the next question is okay do we have a practical significance or not and for that we do need the final assumption regarding the minimum detectable effect so let's say during the power analysis before conducting our AB test we got an mde which or let's actually call it Delta let's keep the Greek letters uh and the Delta let's say is equal to 3% so 0.03 well in this case we can notice that the Delta 0 03 so 3% is much lower than the lower bound of our confidence interval which is equal to 30% so 29.7% this means that in that case we would have said that there is a practical significance also but if the uh Delta would have been for instance the uh 0.31 so we have a 31% Delta then in that case the Delta is no longer smaller than the lower bound of our confidence interval and in that case we cannot say that our results are also practically significant so depending on the business and depending on the Assumption regarding the Del or the minum detectable effect we can then compare this to the lower bound of the confidence interval and we can State whether there is a practical significance or not in case there is a practical significance then we are good to go so we can say that we have a statistical significance we have a practical significance and we also have a narrow confidence interval which is a suggestion that our results are also generalizable and accurate so uh this completes our uh AB test results analysis and this is all that you need to do in order to have a valid and uh good quality AB test looking to elevate your data science or data analytics portfolio then you are in the right place with this AB testing and Trend case study you can showcase your AB testing and coding skills in one place I'm T Vasan data scientist and AI professional and I'm the cofounder of lunar Tech where we are making data science and AI accessible to everyone individuals businesses and institutions in this case study we are going to complete an endtoend case study with AB testing where we are going to test in a datadriven way whether it's worth to change one of our features in our ux design in the lunar text landing page this is a real life data science case study that you can conduct and you can put it on your resume in order to Showcase your experience in datadriven decision making where you will showcase your statistic skills experimentation skills with AB testing and your coding skills in Python using Library such as T models but also the pendas npy also metp lip and caburn we are going to start with the business objective of this case study then we are going to translate the business objective into a data science problem then we are going to start with the actual coding we are going to load libraries we are going to look into Data visual the data The Click data we are going to look into the motivation behind choosing that specific primary metric which is the clickr rate then we are going to talk about the statistical hypothesis for our AB testing I will also teach you step by step all the calculation starting from the calculation of the pulled estimate from the clickr rate and then a computation of the uh pulled variance the standard error but also the motivation behind choosing the searches statistical test that I will be using such as the two sample Z test and then how you can calculate the test statistics how you can calculate the P value of the test statistics and then use that with the statistical significance to test the uh statistical significance of your ab test after this we will also then compute the confidence interval comment on the general ability of the ab test and then at the end we will also test for the Practical significance of the ab test then we will conclude and we will wrap up and we will make a decision based on our data driven approach using the ab test to check whether it's worth it to change a feature in our ux design in the lunar text landing page so without further Ado let's get started so let's now start our case study in here I have in the left hand side this uh version of our landing page so which is our control vers version so to say the existing version where you can see that here we have start freet trial and here we got us our button secure free trial in the right hand side we got this new experimental version that we would like to have which is the Andro Now button so as we saw in the introduction what we are trying to understand is that whether our customers click more on the new version the experimental version versus the existing version the control version so uh um as of the day of uh loading this and uh conducting this case study our landing page uh has a secure free trial but what we wanted to test with our data is whether the uh enroll now is more engaging such that we can go from the secure free trial version to the enroll now version and uh here um for this specific case not only but also in general as we know from a testing is that when ever we got an existing algorithm or existing feature existing button then we are referring this group that we will um where we will expose this existing version of the product we are referring this as a control group so all the users to whom we will show the existing version of our landing page we will refer them as the uh control group participants and then we have the the right hand side our experimental version and our experimental users so the users our existing customers that are selected to be taken part um in our experimental group and in our experiment they will be then uh exposed to this new version of our landing page which contains this androll now button so our end goal in terms of the business as we saw in the introduction is to understand whether we should release the new button which will end up being high High engaging which means that we will have higher CTR or higher uh more uh clicks that will come from our user site which uh automatically means better business because we want to have highly engaging users if they are clicking on this button it means that it interests them more compared to the control version and uh if something on our landing page in this case our call to action is more interesting and highly engaging it means means that we are doing something right and our users might uh either make use of our free products or uh purchase our products or um just stay engag with us to keep real Tech in mind and whenever there is someone who uh is interested in data science or AI um Solutions or products then they can at least refer their friends if they are just clicking to understand and to learn more about our products that's also possibility so from a business perspective we therefore are using here as our primary metric uh our click through rate the CTR of this specific button which in our control version is the secure free trial and in our experimental version is the enroll now and what we want to understand is that whether this new button will end up having higher CTR or not because higher CTR from the technical perspective will translate to higher engagement from the business perspective so here we are making this translation from business versus technical um when it comes to AB testing we can have different sorts of primary metrics we can have a clickr rate as a primary metric we can have a conversion rate as a primary metric or any other primary metric what we want to have as our metric that will work as the single measure that will will compare our dra an experimental group to understand which version performs better is first to understand what this definition of better is and how that translates back to the business because if the engagement is what we are referring as Better Business for some reason and I will explain you in a bit why we think the engagement in this case is what we what matter for us at ler Tech then it means that clickr rate can be used as a primary metric this is just a universal metric that has been used across um different web applications search engines recommender systems and many other digital products to understand whether the engagement of that specific algorithm feature web design whether that is better or not and in this case in this specific case study we are also going to use the CTR because we are interested in the engagement so at luner Tech we really care about the engagement um with our users and we want our users to make use of our products but uh ultimately to engage with us because if they engage with us it means that our products are being seen our uh landing page is being visited and the user is actually interested to click on that button and then the action point and then to start either free trial or to enroll to see what is going on because all these are signs of Interest coming from the user side and in the control version as our click to action is to secure a free trial which directly uh lends the user to our free trial to our ultimate data science boot camp but given that we are expanding which means that we are now offering more courses we are offering free products and also we have uh Enterprise clients uh we have businesses as clients who want data science and AI Solutions and who want corporate training therefore we want to go from this Niche uh version of a landing page so secure free trial to enroll now because we already have a lot of Engagement in terms of the free trial we want to make it more General so that's the business perspective and on the other hand we also want to change beside of changing this um main um call Action we want to make it generalized and at the same time we want to see whether this generalized version will end up leading us um a higher engagement not only in terms of of the other products but also for the tree trial free trial itself because we always are looking for educating people and providing this free trial such that they can make use of our Flagship product which is the the ultimate data science boot camp so now when we understand why we care about the engagement here at ler Tech and we understand why we want to check whether this new button in our ux design will end up increasing the engagement or not we can now make this translation back to the data science terms because we know now from the business perspective All We Care is to understand whether this experimental version of the product is performing better or not but then this means that we need to conduct an AV test and we need to understand whether the ideas that we got and the speculation that the enroll now more General Button as so call you action will be better than the secure free trial version whether this is actually true or not from the uh customer perspective because if we want to call us a datadriven company we cannot just base our conclusions and our decisions for our products or for just in general for our product road map based on Intuition or logic we want this to be data driven which means that the customers are at the first place we are customer driven and our customers need to tell us whether the new um button is better or not and here we have conducted conducted an Navy test and um here I won't be using the real data I will be using the uh proxy data or simulated data that I generated myself and um this one contains the similar structure and this uh the same um idea of the data that we got when we were conducting our IB test and collecting this data and what is our business uh hypothesis in our business hypothesis we can say that we have at least temp % increase in our click through rate so 10% higher engagement when we have our enroll Now versus the secure free trial version of the product so this is our business hypothesis which means that our enroll now CTR so click through rate of the enroll Now button will result in at least 10% higher CTR than the secure free trial so there exists uh 10% at least 10% difference in terms of the engagement when we compare this new version of the product versus the old version of this new uh button and when we translate this back to statistical hypothesis we can say that under the N hypothesis we are saying that there is no statistically significant difference between the um control p and then P experimental which means the um um probability clickr probability clickr rate for control group versus experimental group so under AG n the null hypothesis we are stating what we ideally want to reject we are saying there is no difference between the experimental and control group CTR and under the alternative hypothesis so the H1 we are saying no uh we do have a difference which means that the uh control groups CTR is different from the control experimentals group CTR and one key part here is to mention that they are not just different but they are statistically significantly different so uh when it comes to starting the case study first things first is to load the libraries in this case study we are going to use a numpy we are going to use a pendas as usual for any sort of data analytics data size um case studies you always need those two usually pendis will be needed for our data wrangling to load data process the data visualize it nonp will be used to uh work with different arrays and part of the data then we are going to use the ci. stat uh model and from that we will import the norm function later on um we will see that we are using this in order to visualize this um uh rejection region that we get from for our test to understand whether we need to reject our null hypothesis or not then in this case study we also want to visualize our results and visualize our data for which we are going to need our visualization Library from python which are the curn and the med plot L let's look into our data so what we have in our data we have four different columns and of course this is a filter data that contains the information that we need but in general you can have a larger database you can have more sorts of um um matric matrices and uh different other Matrix but for conducting your ab test the pure AB test you actually need only the following information so you need your user ID to understand uh what are the user you are dealing with so it's the user one user two user 10 it can be that you have other way of referring to your users and uh those can be for instance this long strings that we use to refer to our user but given that our case is a simple one our case study we have just a user ID and this user ID is just a integers that go from one till uh until the end of our uh data and here we got in total 20,000 users therefore this number user ID goes to um 20,000 and those 20,000 um are all part of the user group which means that they are all users and they contain both the experimental and control users then we have our uh click variable and this click variable it's a binary variable which can be uh either one or zero where one refers that the user has clicked on the button and zero means the user didn't click on the button this is our primary metric for our AB test then we have the group reference which is this um string variable and this string variable helps us to understand whether the user comes from the experimental group or from the control group so this can contain only two different values two strings and it is X referring to the experimental and control referring to the uh control group if you can see here we got just this three letters X referring to the experimental group and then if we go in here because we have first the experimental and then the control ones you can see that here we got the uh control group now we have also some time stamp which is not something relevant so we'll be skipping that for now um given that this uh data that we have here it's not the actual data our data but it's a synthetic one but similar in terms of its structures in terms of the uh nature of variables and you can Implement exactly the same steps when you have your data and you are getting it from your ab test and then you are conducting your ab test uh case study so in here what we are going to make use of the most is our click click variable and the group variable because we want to find out per group what are the users that have clicked on the uh button and to be more specific we are looking for these averages so we are not so much interested that that specific user from that specific group has clicked on the product or not that's something that we can explore later but for now we are interested on the more high level so what is this uh percentages what is the click probability or click the true rate perir group and here we got groups of experimental and control as it should be in any source of ab test so once we have conducted our AB test then I will also provide you more insights on what you can do with your data especially with this user ID to learn more about uh the idea behind these different decisions or whether your ab test is different per group but the idea is that this AB test that we are conducting by following all the steps and by ensuring that the uh pitfalls are avoided that we are making a decision that um represents the entire population so we are using a sample that is large enough for us to make a decision for our product and for our business that will be generalized and will be a representation and representative when we apply this decision on our population so let me close this part because we no longer need this and let's go ahead and load this data so here I'm using the pendus library and the common uh approvation of PD and I'm saying pd. read CSV and then I'm here referring to the name of the data that contains my click data and here you can see that dat that that data is here so abore testore click uncore data that's here PV and I will be providing you this data because you won't have this in your own Google collab you will have the link to this Google clab and I'll provide you also the data such that you can put that data you can download it first from my source and then load it in here by using this specific button in here and by doing that you can then go to that specific folder where you downloaded the data and then you will have also this uh corresponding CSV file in your folders so once you have that then you will uh smoothly run this code and uh here I'm loading that data and putting under the name of DF abore test so basically the data frame containing my ab test click data what I want you to do is to Showcase you how the data looks like so here you will see the header given that here I haven't provided any argument it just looks at the top five elements so the top five rows and here I got only the first five users from the experimental group I see that some of them have clicked some of them didn't click and the corresponding user ID and the Tim stamp uh that they um done the click action then um when we look at the describe function you can see here that this gives us more general idea uh of uh what the contains not so much what the top five rows just look like which is great in terms of to understand what kind of data you are dealing with with what kind of variable you have now you can see more the uh total uh picture so high level picture what kind of um data what amount of data you got so the descriptive statistics so here we can see that in total we got 20,000 of users included in this data so 20,000 observations 20K rows and we have the mean for the user ID of course it it's not relevant the mean is 10,000 and um this is an interesting number so we see that um average click uh when we look at both user and control the experimental and control groups it is 40% so 0.40 uh 52 so 40. 52% however this is not what we are too much interested so this is not to be confused with the click through rate perir group what we are interested is the click through rate or the mean clickr um when it comes to the experimental group and the control group so then we have our standard deviation we see a high standard deviation which is understandable given that we have this uh large variation in our data we got a control group and experimental group and this Vari shows that we have a huge difference in the these different values uh when it comes to the click event and then we have the mean and the maxim which doesn't give us too much information because the click event so the click variable is a binary variable it contains the zeros and one so naturally the minimum will be the value zero because the click can take value zero and one and the largest one is of course one which means the maximum will be one and then for the rest the 25% so the first quantile the second quantile the 15% which SS the median or the third quantile the 75th percentile is not that much relevant so when it comes to the descriptive statistics for this kind of data especially if it's filtered is not super relevant but if you would have a larger data more matrices beside of Click which is your primary matric but you all have also measured some other Matrix which is recommendable then you would see more um values which would be interesting to look at so not only to look at the click rate but also to look at for instance the mean or maybe the median of conversion rate or the uh mean uh amount of time the average amount of time the user has spent on your landing page or how much time did that user end up spending before making that decision of a click those can be all very interesting Matrix to look into from the product uh data science perspective to understand the decision process and the channel and The Funnel of these clicks but for now for our study what we are purely interested is in our primary metric which is the click event so what we can also see in here is that we got um uh in our group um when it comes to the control group we got uh 1,989 users out of all uh control users that end up clicking versus the experimental group where we have 6,6 users who did click so do not confuse this with the total amount of users per group this amount is the um grouping of the uh data so using the group by and then group so we are grouping that data per group and we want to see per group what is the sum of this variable sum of the clicks and given that the click is a biner variable we know from basics of python that we are basically accounting the number of click events because if you got a binary variable containing zeros and ones if you do the sum of the clicks adding the zeros do not doesn't have any impact which means that um you end up just summing up all the ones to each other and then you end up getting the number of or the total amount of uh cases when this click variable is equal to one so in this case when there is a click event therefore we can see that per experimental group we got 6,16 uh users out of all the experimental users that end up clicking and then out of control group this amount is much lower so we end up having uh only 989 users clicking so let's now go ahead and visualize this data I want to showcase in a bar chart using this clicks what is the total number of clicks so I want to show the distribution of the clicks when it comes to um the uh click event pair group and here I want to uh see next to each other the experimental group and control group and as you can see here here we are getting our bar charts and the yellow corresponds to the no which means that there was no click versus the uh black corresponds to the yes which means there was a click so whenever you see this amount it means that that amount corresponds to no click no engagement from the user side and this is per group so this is what we are referring as a click distribution in our uh data in our experimental uh and control groups and the way that I generated this bar chart is by first creating this um uh list that will contain the colors that I want to assign to each of my groups and I'm saying zero corresponds to the yellow and one corresponds to Black which means that if my variable contains amount of zero in this case my click is equal to zero it means that I don't have a click so it's a no and this I want to visualize by yellow otherwise I have a black which means that um the um one corresponds to the case when we have um click and in this case we will get a black as you can see here the uh yes which means a click is um visualized by this black color and then what I'm doing is that I'm initializing this uh figure size by saying that I I want to have a figure size of 10 and six you you can also skip it but I I think it's always great to put the size of a figure to ensure that you are getting the size like you want it to be such lat you can also download or take a screenshot then we have this uh here I'm using as you can see a combination of the Met plot leap. pip plot Library as well as the uh cabbo because CBO has much nicer colors and here here I'm saying uh we are going to uh make use of the curn to um create um count plot because we are going to count and we are going to showcase the counts per group uh what is the number or the count of the clicks versus no clicks for a group called experimental and what is the number of um or the percentage of clicks versus no clicks when it comes to the group control and then here I'm specifying that the Hue should be on the click which means that we are looking at the click variable and we are going to use the data dfab test which means that we are going to look in this data from here we are going to select this specific variable called click and we are going to use this in order to group our data based on this group so you can see that we are doing the grouping on the variable called group so the argument is called x x is equal to group we're grouping our dfab has data on this group and we are going to do the count in our count plot based on this variable click basically what I'm saying here is that go and group our data dfap test based on Group which means that we will group based on experimental versus control and then I'm saying go and count the click events count pair group so pair experimental perir control group what is the number of times when we have a no so we have a zero and and what is the number of times when we have a yes or we have a one as a value for click variable and then as a pet I'm using my custom pet that I just created which should be in the form of a list as you can see in here if I would have here also my third group or fourth group then I of course need to extend this color palette because I need to have the same amount of colors as the number of groups pair my target variable in this case The Click has only two possible values 0 and one which means that I'm only or only specifying the two colors in my list so then we have the title of our plot always nice to add by and then we have our labels which means that I want to emphasize uh as my X label so here I want to have my group you can see here is my group because I will either have group experiment or control that's my variable on my xaxis and on my y AIS of course I have the the count so I'm counting the number of times I got uh the uh no click versus click event so here note that the um y AIS is in terms of this count so here you can see it's uh 8,000 here sa 7,000 or 6,000 5,000 which means that we are talking about the numbers and the counts rather than percentages and this is important because um another thing that I'm also doing is that I'm going the extra Mile and I'm also adding beside of this counts on the top of each bar I I want to visualize and clarify what are the corresponding percentages it's always great to enhance your data visualization with some percentages percentages is easier for the uh person who follows your presentation to understand for instance if you got an experimental group and the the users is here 6,000 and um 4,000 they might not quickly understand that you got for instance in total 10,000 of users and then 6,000 has then uh clicked and then 4,000 didn't click so um then the idea is that by adding these percentages we can then see that 61.2% has clicked in this experimental group and 38.8% has not clicked of course this a simulated data I specifically pick the extreme in such way that we can clearly see this difference in the clickr rates but um in the reality you can have a clickthrough rate of 10% up to 14% which is usually a good number if you have a clickthrough rate of 40% is great but it's really depend on underlying user base what kind of product you got how large is your user base because if you have very large user base then 10% can be a good clickr rate versus if you have a very small user base Maybe 61% is considered uh good or average so uh in here we have just a simulated data of course and I've have added these percentages uh by using the following code so I won't go too much into detail in here um uh feel free to check and see uh and if something doesn't make sense go back to our python for data science course that contains lot of information on the basics in Python but here just quickly what I'm doing is that I am uh calculating the percentages and I'm annotating the bars so I want to know what are these percentages which means that per group I want to take the total amount of clicks I want to understand what is number of Click event when the click variable is equal to one and what are the number of cases when there was no click from the user s which is what are the number of cases when the click variable is equal to zero and then I'm counting those amounts and then using the total amount to calculate the percentage for instance in this specific case I'm filtering the data for experimental group I'm looking at the total number of users for this group which is 10K and then I'm counting the number of times when out of this 10,000 users the amount of users that end up clicking on that button which is the click is equal to one case and then I'm taking that number dividing it to the total number of users for this experimental group multiplying by 100 in order to get that in percentage and this is the calculation that you can see in here one thing that is important here is that here I'm using this um uh percentage um so for the current bar I'm saying U as a way to identify whether we are dealing with experimental or control group is by getting by looking into this uh p and uh this p in here is the basically the patches so in this case I'm basically saying if if I'm dealing with experimental group then go ahead and calculate what is this uh total amount of observations and then take what is the uh number of clicks and then divide the two numbers uh C multiply this with 100 and this will then give us the percentage and then I'm doing this for each of those groups so I'm doing it for this group I'm doing for this group and for this one and for this one so I got two groups but then within each group I got clicks and no clicks and I'm calculating these four different percentages and then I'm adding these percentages on the top of those bars so I not only want to have numbers repr presented in my visualizations but I also want to add this corresponding percentages at the top just for visualization purposes I wanted to put this out there because this can help your uh data visualization toolkit and it also will um make your audience from your presentations be more thankful to you when you are telling the story of your data so uh this is about the data that we have we see that uh 38.8% of our experimental group users have not clicked on the button versus the 61.2% have clicked on the button based on the simulated data and then uh in the control group we have a quite the opposite situation we got the majority of the users 80.1% not clicking on the button versus the remaining 19 .9% have actually clicked on that button so we got a huge difference a dis anounce when it comes to the experimental group and uh control group this kind of gives us an indication hey something is going on here we kind of uh have already um higher level intuition what the remaining analysis will look like um which is that there most likely will be a difference in their ctrs when it comes to the uh the um uh control versus experimental group and the corresponding buttons but uh hey let's continue that's the entire goal behind a testing is to ensure that our intuition our conclusions are all based on the data rather than on our intuition so what are the parameters that I'm using here for conducting our AB test when I was designing this AB test uh the first step step was to of course do all these different translations that we learn as part of our AB test course um conducting it properly which means coming up with this three different parameters when doing our power analysis and usually this should be done when you are collaborating also with your colleagues and uh with your product managers or your product people domain experts because they have um a lot of information on what it means to have um threshold that you need to pass in order to say that for instance this new version of your feature is different and is uh considerably uh different from the existing one and here um in order to for us to understand this uh and make these conclusions we need to come up with the three different parameters that can help us to properly conduct an AB test as we learned when we were looking into designing a proper AB test so so first we have our significance level the significance level or the alpha the Greek letter that we are using to refer to the significance level which is also the probability of the type one error and that amount we have chosen following the industry standard which is 5% given that we didn't have any uh previous information or specific reason to choose a different significance level so lower or higher we decided to go with the industry standard which is the 5% this means that we want to have um we want to compare our P value of our uh statistical test to this 5% and then say whether we have a statistically significant difference between the control and experimental group based on this 5% significance level and let's refresh our memory on this Alpha this Alpha uh or significance level is also the probability of type one error so this is the amount of error that we are comfortable making when we um reject the N hypothesis well the null hypothesis is actually uh true which means that we are detecting a difference between the experimental and control version while there is no difference and we are making that mistake and here we are saying that we are fine and we are comfortable with making this mistake at a maximum of 5% but higher than that it's not allowed we are not comfortable making uh error um higher than 5% then the next variable uh in this case the B uh or beta the probability of type two error which is the opposite of the type 1 error which is a false negative rate or the amount of time the um um proportion of time when we end up failing to reject the null hypothesis while null hypothesis is false and it should have been rejected then the 1 minus beta is actually power of the test so what is the amount of time we are correctly rejecting our null hypothesis and correctly stating that there is indeed a statistically significant difference between our experimental group and our control group so we have chosen for this the uh industry standard as well which is the 80% but given that for your results analysis in this case for conducting this case study that part of the power analysis is not relevant we use that when calculating our minimum sample size but we don't need that when conduct our results analysis therefore I'm not initializing that as part of this code so here I'm only providing to my program the values for my significance level which is 0.05 or this is the same as 5% and then the Delta which is the third parameter and this Delta is our minimum detectable effect so this a Greek letter Delta which is the minimum detectable effect helps us to understand whether beside of having this statistically significant difference whether this difference is large enough for us to say that we are comfortable making that business decision to launch this new button so it can be that when we are conducting an AB test we are finding out that the experimental group has indeed higher engagement than the uh control group and we are uh getting a small P or at least is smaller than the alpha and we are seeing that P is small than the alpha level which means that we can reject the null hypothesis and we can say that the CTR or the clickr rate of the experimental group is statistically significantly different from the control group at 5% significance level but we know from the theory of ab test that only that is not enough only statistical significance is not enough for the business to make that important decision to launch an algorithm or to launch a feature in this case to change change our landing page the button from the start free trial to the enroll now which means that we want to have enough users and we want to have enough difference large difference in our click through rate or enough users saying that we are more happy with this uh new version of the landing page for us to go and change our feature and what is the definition of enough what is the difference in the click through rate that we need to detect after we have detected the statistical significance in order for us to say that we also have a practical significant so practically we are also comfortable making that business decision and then launching this new feature and changing our landing page button and that is exactly what we have under our Delta this minimum detectable effect in this case we have chosen for Delta of 10% so you can see here 0.5 this is 10% this means that our Delta or the MD the Min detectable effect is 10% this means that we are saying not only we should have a statistically significant difference between the experimental group and control group but also we need to have this difference to be at least 10% which means that we need to have detected that the experimental version of the landing page results in a at least 10% higher quick rate compared to the control version for us to go ahead and to launch this new version and deploy this new uh ux uh feature so this is really important because many people go and check for statistical significance so they do their Alpha and then check uh whether the P value is more the alpha and then say hey we have a statistically significant difference and then they are done with that but that's not correct after you have conducted your uh statistical significant analysis and you have detected that your uh experimental version has a statistically significant different um CTR than the control version at your Alpha significance level the next thing you need to do is to ensure that you also have a practical significance beside of the statistical significance and this practical significance you can detect and you can check when you use your mde or your Delta and you compare it to your confidence interval that you have calculated something that we have also learned as part of the theory of conducting a proper AB test but once we come to that point so after we check for our statistical significance I will also explain how exactly uh we will need to do this check and at the same time we will also be refreshing our theory on the Practical significance so let's now goad head and calculate the total number of clicks per group by summing up these clicks and I also want to calculate and group by this amounts just to Showcase how you can do that on your own so here what I'm doing is that I'm taking my ab test data I'm grouping by by group group is the uh variable that contains the reference where we are dealing with experimental group or control group and as you know from our uh python series and demos python for data science course that uh whenever we want to group that data a pend this data frame first we need to say pendas data frame name. Group by Within parenthesis the variable that we are using to do the grouping which is in this case group and then within Square braces I want to emphasize and put the name of a variable that I want to um apply operations on so I want to group my data on the group variable and I want to count the number of times I have a click in my control group and in my experimental group this will be my X control and X experimental variables so X control will then compute contain information about number of Clicks in my control group and then each experimental will contain the number of Clicks in my experimental group and given that um I want to refer to the name of that uh Group after I did my grouping so I am getting this kind of this shape of data frame of course I then need to uh use my do log function in order to properly call that amount so to understand what is this amount corresponding to this index and what is this amount corresponding to this index and given that my index is in strings I'm then using here mylog function something that we also learned as part of our python for data science course so here is basically the printing just writing nicely what are the results which means that we are counting that the let me count again that the uh number of uh clicks for my control group is 1,989 so you can see that it is want to double check and see what we got yes so we got same number so we are dealing with the same data set just to to make sure and here the number of clicks for experimental group is equal to 6K and 116 so 6,116 clicks so then we are calculating the uh pulled estimates for the clicks per group let me quickly fix this typo so calculating the uh pulled estimate for the clicks perir Group which means the P estimate for the experimental group and for the uh control group so let me quickly at here how I can calculate the uh total cases when we got uh experimental group users so what is the number of users in the experimental group and what is the number of users in the uh control group so here what I want to do is that I want to say that the group The DF test group should be equal to expermental and this of course should be my filter and I want to count this and let me quickly copy this I saw that is already under the control so here I'm changing to the control and this will need to give me the number of users in each of these groups too so number of users in control and number of users in Click and here I will simply check this so I will print then the number of user users per group and at the same time I will also click the number of clicks per group there we go so now when we have done this what we are ready to do is to go ahead and calculate the F estimate for clicks perir Group which means pair control group and perir experimental group for that what we need to do is to take the number of clicks of the control group divide to the number of all users for control group as you can see in here x control / to n control and we are referring to this variable as P control head because we know that the estimate of this click probability um is always with a hat it's just the way that we reference it in um statistics and in ay testing so this is the estimate something that we are estimating therefore we are saying hat and then we have the same for experimental group which means that the estimate of the experimental groups uh click probability is equal to X exp and then divide it to n x then um in order to calculate the uh pulled estimate or uh pulled click probability which means the value that will describe of the uh control group and experimental group we need to follow this formula which means that we are taking the ex control we are adding to that X control the X experimental this is our nominator of our uh value and then we are dividing this to the uh sum of the sizes of each of those groups which is n control and N experimental so this is the common formula of the pulled estimate uh when it comes to this type of experimentation when you are dealing with um primary uh metric that is in the form of zeros and ones and if you want to refresh your memory on this type of formulas then make sure to also check our AB testing course because in there we go in detail in this uh specific lesson of the uh AB test result results analysis we are looking into this uh all this formulas on how we can calculate the pulled estimate of this uh click probability so click probability but then we are calling it P click for probability and then what we got is this volum so that amount is then 040 this number should look familiar because this is then the mean that we saw when we were looking at the um uh descriptive statistics table if you can recall this table let me see this number so now basically we are then calculating this manually because we need a variable that will hold this uh value so it is simply summing up all the clicks for control group and experimental group to get the total number of clicks and we're dividing it to the total number of users so n Control Plus n experiment so now when we have this we are ready to also calculate what we are referring as a pulled variance also something that we have learned as part of this theory for AB testing so the pulled variance is equal to the pulled estimate of the clicks so P had something that we just calculated multiplied by one minus P head so the uh click event the estimate of the click probability multiplied by the estimate of no click and we know already this IDE of berola distribution that the variable that uh describes this process of clicks and no clicks follows kind of this idea of B distribution when we have a click and no click so we have probability of click and then we have probability of no click which is the one minus that click probability so that's the idea or the part of the formula that we are following as kind of an intuition and then this multiplied by 1 / to n control+ 1 / to n experimental so here I'm purely following the formula for the pulled variance if you want more details and explanations and sure to check the corresponding Theory lecture because we are going into details of each of those formulas and understanding why we calculate this um P variance and P estimates uh in this specific way and using these specific formulas so here by just follow following the uh formula I'm getting that the uh pull uh variance is this amount so this is in nutshell how I calculated my uh pull click probability and a pulled variance of that click event and we are going to need that in the next very important step which is calculating the standard error and calculating the test statistics because in this case what we are doing is that we are dealing with a case when the primary metric is in the form of zeros and one so we let's Now quickly talk about the uh choice of a statistical test be uh before conducting the actual calculation of standard error in the test statistics so here I went for the two samples at test and let me explain you why and what is the motivation because as we learned as part of the theory um whenever we have a primary metric that is in the form of an averages like we have now because we are using the P control head and P experimental head head so we have a primary metric that is the uh click through rate which is the average clicks per group so we have calculated the average click per experimental group and control group then the primary metric the form of it already dictates given that it's in averages that we need to look at uh either parametric test corresponding to this averages or nonparametric test corresponding to the um averages in this case I went for the parametri case because uh it has Better Properties if I have this information about the distribution of my data and why do I have this information and then this also dictates the uh choice of my um statistical test well I have a size of my sample which is over 100 and actually over 30 that's the threshold that we tend to use in statistics and in a testing in order to say whether we have a large size or large data or not if our sample is not large so it contains less than 30 users per group which happens as well then we say that we need to go for um statistical test uh that will be specific for this kind of cases because we can no longer make use of the uh statistical theorems like the central limit theorem which helps us to um uh to take the uh to the inference so to make use of the inferential statistics and make conclusions regarding the distribution of our population just having the sample and what do I mean by that so if my sample is larger than 30 like in this specific case I got 10,000 users per group so it is definitely larger than 30 uh users then in that case I can say that by making use of the central limit theorem I can say that my sampling distribution is normally distributed and this is simply making use of the central limit theorem something that we have also learned when we were looking into this concept of inferential statistics as part of the fundamental statistics course uh course um in lunar Tech so this is a powerful theorem that we use in AB testing in order to make our life easier because when we have a sample that is larger than 30 for each of these groups then we can say that even if we don't know the actual distribution or the name of the distribution that our uh sample follows where it comes to the click um event so the random variable that describes this number of clicks or the average click true rate what is that um distribution exactly but given that we have that this uh size is large enough it's large than 30 users we can say that by making use of the central limit theorem we can say that the uh the uh sample distribution follows a normal distribution if given that the sample size is large enough and this helps us to say that well in that case it doesn't matter whether we make use of the two sample Z test or two sample T Test we can make use of either of these test in order to conduct our analysis and we had this specific template to make this choice easier uh in our AB test course at the loer tech where we were making all this decisions and saying if the SLE size is this we need to do this if the SLE size is this we need to do this and in this specific case following that exact structured and organized approach I ended up seeing that my sample size is large so it's larger than 30 so I can then make use of the central limit theorem I then know what is the random um what my random variable describing this click through rate um follows the kind of distribution in this case a normal distribution and then this means that whether I use a t test or Z test doesn't really matter I'm going to end up with the same conclusions therefore I will just go with the two sample set test simply because um it is just easier for me to do for example you can also go with the two sample T Test and you can even change this case study and tweak it and then make make it your own and put it on your resume in that way by making it more unique and that will be totally fine because you will see that you are going to end up with exactly the same conclusions as we do in this specific case study because if you have a large enough sample it won't matter whether you have a two sample Z test as your parametric test or the two sample T Test and um if you want to know why why this matters and all the different details statistical insights make sure to check the actual uh course dedicated to AB testing because there will be cover this all and you will then become a master in the field of AB testing now we know this uh decisions and the motivation behind choosing the uh two samples that test let's now go ahead and do the actual calculations so here we have a standard error which we calculate by taking the pulled variance and taking the square root of it and this is again using the idea of this formulas that we learned as part of the ab test so we are using this P variance taking the square root of this which gives us the standard error and the standard error as you can see in here is then equal to 0.69 29499 this amount there we calculate our test statistic for our two sample at test so the test statistic is equal to P control head minus P experimental heads divided to standard error so here uh you can now see the motivation behind not only Computing the P pulled head but really also the p uh control head and P experimental head and then I take the P control head and subtract the P experimental head and I divide it to the standard error to compute my test statistics once I did this as you can see this is this amount so test statistics for our two sample that test is this amount minus 5956 around it then um we can also compute the critical value of our Z test which is uh by using this Norm function that we uh loaded in here from the S and this will help us to understand what is this value from our normal distribution table the standard normal distribution table uh where by making use of this table we identify what is this critical value that we need to have to uh create our rejection regions and to say whether we can uh reject our n hypothesis or not so to conduct our test we need to have a critical value for uh to which we will compare our test statistics and this critical value will be based simply on the standard normal distribution so this is this norm. ppf and then uh probability um uh function base basically uh the the probability function that comes from the normal distribution standard normal distribution and as you can see it is correspond specifically to this percent Point function which is the inverse of the cumulative distribution function so this based on the alpha / 2 so 1us Alpha / 2 is the argument that we need to put for our percent Point uh probability function and why divide it to two because we have a two sample test so because we have a twosided two sample test sorry so if you want to understand this difference between uh two sample um test twosided Test please check out the uh fundamentals to statistics course at ler Tech because we cover this uh Topic in detail and it's a very involved topic it contains many complex stst U from statistical point of view so I won't be spending in this case study too much time on that here I'm assuming that you know this formula already but if you don't and if you quickly need to do your case study May testing feel free just just to copy this line which basically is a value that we need based on the corresponding chosen statistical significance level that we need to compute to compare our test statistics so our test statistics is this value and the value that we need to compare it to is the Z critical volue so we can see that this critical value is then equal to 1.96 this is actually a very common value that we know even without looking at a standard normal table when you make use of this test enough often then you know that the uh critical value corresponding to a twosided test when it comes to normal table is equal to uh 1.96 this is just the value that we know and in here by even without calculating the next step which is a P value we can even say already what is the decision we need to make in terms of statistical significance because we know that one way we can test our hypothesis statistical hypothesis is by Computing the test statistics and checking with the test statistics the absolute value of it is larger than the critical value and we see that the test statistics is equal to minus 5956 the absolute value of that is 59 .56 and that value is much larger than our critical value which is equal to 1.96 this already gives us an idea that we can reject our null hypothesis at 5% statistical significance level but I want to uh go on to the next step actually because that's um more structured more organized way to doing and conducting experimentations as in the industry we tend to make use of the p values instead of making use of this econometrical approach and statistical approach of um testing the statistical test so once we have calculated our test statistics the next thing we need to do is to calculate our P value and then use that P value compare to the significance level Alpha and then make a decision whether we need to reject our n hypothesis and say that we have a statistical significance or we cannot reject our n hyp hypthesis and then we need to say that we don't have a statistical significance so we don't have enough evidence to reject anal hypothesis so the idea here is that we need to make use of our uh normal function and specifically the norm. SF so making use of exactly the same Library the norm from CI do DOS and then this time we're using the survival function which is the one minus the cumulative distribution function of normal distribtion this comes again from statistics and then using the absolute value of our test statistics multiplying it by two given that we have a twosided test I'm calculating my P value this is simply by making use of the same formula that we saw when we were uh studying theab test from a technical point of view because we learned that the P value is then the probability that Z will be smaller than equal the minus test statistics or that the test statistic is smaller than equal to Z so uh we basically want to calculate what is this probability the P value which is equal to the probability that our test statistics will be smaller than the critical value or our negative of the test statistics will be larger than equal of the critical value and we want to know this probability because what this probability represents is that what is the chance that we will get a large test statistics well this is due to a random chance and not because we have a uh actual statistical difference between the clickr rate of the experimental group versus control group so this is the idea behind P value so what is this chance that we are uh mistaking this random mistake this random observation that we got a large test statistic and saying that there is a statis significance well there is no such thing and we are purely getting this large test statistics um because of the random chance if the probability of getting a large test statistics by random chance is small so if this P value is small then we can say that we have a statistical significance that's the idea behind it and this P value when we calculate uh we are storing it in this variable called pcore volume and then the next thing what I'm doing is that I'm writing this function quote is statistically significant which takes argument as P value in Alpha so I just need the P value that I just calculated for my test Set uh test uh statistics and then I want the statistical significance level that I want to use for my test and then this is the value that comes from my power analysis as I mentioned before that's the 5% this P value I'm calculating for my test statistics so in here and then I'm taking the two and I want to compare them so I want to assess whether I have a statistical significance by comparing my P value to my statistical significance level Alpha and what is this comparison well we know uh from the theory that um if we have a low P value and specifically in the P that we are getting the P value is small than equal the 5% or 0.05 which is the significance level then this indicates that we have a strong statistical uh evidence that uh the N hypothesis is false and we need to reject it so we have a strong evidence against the null hypothesis and otherwise if the P value is larger than 0.05 so it's larger than 5% that we have chosen as the maximum threshold of that mistake so the significance level is uh uh no longer the largest element but the P value is larger than your significance level then this indicates that you don't have enough evidence against the N hypothesis so your evidence is weak this means that you fail to reject the N hypothesis so this is what I'm doing in here with this code so I'm saying print the P value first and we are rounding it up with this round function I'm rounding it to the three decimal and then I want to check and Det determine whether I have a statistically significant or not and the way that I'm doing that is I'm saying if my P value is more than my alpha or actually let at smaller than equal than Alpha then we can print that there is a statistical significance which indicates that the observed differences between the experimental and control groups are un unlikely to occur due to random chance which means that this is not random chance and uh we have a strong evidence that there is a statistical significance and this suggests that this new feature that we got this new version of our landing page with this um uh call to action um as the and now is better and result in higher statistically significantly higher clickr rate than the existing version of the control uh group so there is a real effect then otherwise if this is not the case which means that my P value is larger than my Alpha then I'm saying print that there is no seral significance and that the observed difference that we see in the clickr rate is not because uh of the real difference in the performance but TR truly this is just the random chance so here we can see that once we run our we call the function in here which is simply the function name and the argument so P value and alpha alpha comes from the initialized value that we had from our power analis so from here we initialize this value 0.05 and then here we got the P value that we just calculated then what we are getting in here is that our P value is actually so small that it's um rounded to the zero so what this means is that that there is evidence that suggest that at 5% statistical level significance level that the uh clickr rate of the experimental group is different from the clickr rate of the control group note that I'm not saying higher or lower because our statistical test was twosided so under new hypothesis we had that the uh P control so in here as you can see our P control was equal to P experimental and under the alternative we had that the P control is not equal to P experimental this means that we um have now how rejected the null hypothesis we have found evidence that suggests that the null hypothesis can be rejected since our P value is zero and it's smaller than the statistical significance level 5% and this means that we can reject the hle and we can say that uh there is enough evidence to say that P control is not equal to P experiment and given that that we saw from the uh visualization and from our calculations that the um clickr rate for our experimental group is much higher then the click rate of the uh control group we can also say that we have found evidence that at 5% significance level we have found out that there is a statistically significant difference between the experimental and control groups clickr rate and that the experimental groups clickr rate is actually higher so statistically significantly higher than the control versions clickr rate so this is really important because this suggests that this difference in their click to rate is not due to random chance Alan but truly that there is evidence statistical evidence that can support this hypothesis that there is a true difference between the performance of the experimental version of the product so in this case in our case the landing page that has enrolled now button versus the control version of the product which had the uh start free trial version of the landing page the existing version so beside of calculating this P value it's always a great practice to also visualize your results and this is great for your audience who are technically sound and who know uh these different concepts and you want to visualize uh the results that you got not only by showing some number that is the P value and say hey I have a statistical significance but you also want to showcase the actual picture of what you got what is your test statistic what is the significance level that you use to kind of tell a story around your numbers and that's the uh art behind the data science I would say so let's go ahead and do some art so what I'm doing here is that I am making use of my standard normal distribution or the gaan distribution the way that we are referring to the standard normal distribution in statistics I'm saying that my mean or the MU is equal to zero my Sigma is equal to one which is my standard deviation and I'm saying that my uh I want to now plot my uh standard normal distribution by getting my uh X values which are the uh number of uh X elements that I want to have my xaxis and then taking the PDF or the probability distribution function for the normal distribution by using the CP Library I'm then providing my my X values for which I want to get my uh corresponding uh values of Y so basically here are all the values between let's say minus something minus three and then so between 3 and three and I want to find all the Y's corresponding to this which basically plus the probability distribution function of the goian distribution or the standard normal distribution and then I want to add to this graph all also the uh corresponding rejection region and as you can see it is here so then what I'm adding here by using this part of the plot is that I want to fill in the rejection regions so I'm saying for all the values in this figure whenever the uh value is lower than that threshold in this case the threshold is z critical 1.96 so whenever my threshold is smaller than minus this uh 1.96 and larger than this 1.96 then we are in the rejection region we are saying then if my test statistics is falling in the rejection region in this case you can see that we are in the far left so the test statistic is minus 59.4 and it's much lower than this threshold as you can see in here this is this left Blue Line in here then in this case it falls in this rejection region so actually this entire thing is the rejection region it starts from here and it goes all the way to here anything anything in this region means that we need to we have a test statistic fully in the rejection region which means that we can reject to n hypothesis if we were to get a test stais that is very large and very positive it means we would be in this part of the figure and again in the rejection region anything above this line is then uh going under this category of rejection region and also anything in here so for anything in here we are in the rejection region being in the rejection region it means that we can reject the hypothesis and we can say that we have a statistically significant results so now when we have our statistical significance it's always a great idea to go on to the next step and it's actually mandatory to do this because not only a statistical significance is important but also the Practical significance as I mentioned in the beginning of this case study so for that what we are going to do is first we are going to calculate a confidence interval of the test and this confidence interval will help us to first of all make um comments regarding the quality of our test and its generalizability uh at our entire population and the accuracy of our results and then we will use this confidence interval to make a comments and to test for the Practical significance in our AB test so let's go ahead and calculate the confidence interval so as we learn as part of our lectures the confidence interval can be calculated by first taking the uh P experimental head and P control head and the standard error and the Z critical so here we need the two different estimat of the experimental groups click through rate and the control groups click through rate we also need the standard error of our two sample Z test as well as the critical value and then we need to First calculate the lower bound of our confidence interval and then we need to calculate the upper bound of our confidence interval and in this case uh given that the um statistical significance level we are using is Alpha uh the uh Z critical is based on that therefore for we are also saying that we are calculating the 95% confidence interval so in here the way we will calculate the lower bound is by taking the P experimental head subtracting from that the P control head and then once we have done that we then substract from that the standard error multiply by Z critical volume and we are just rounding this up up to the three decimal behind the zero then we are doing the same thing only with a plus sign in here for the opp upper bound calculation of the confidence interval so this is just pure following the formula of the confidence interval that I will set you here and let's go ahead and print this value which is this interval so what we are seeing here is that we have a confidence interval that is from 0 399 so 0.4 to 0 uh 43 so quite a narrow confidence interval I would say which is actually a good sign because this confidence interal that provides this range of values within which the true difference between this control and experimental groups proportions or the clickr rate is likely to lie within a certain level of confidence in this case 95% confidence this is very narrow and if it's a narrow confidence interval it means that the uh accuracy of our results is higher and it means that the results we are getting BAS B on our smaller sample it will most likely generalize well when we apply these changes and deploy these changes and we put this new product in front of the entire population of users because now we are doing all this experiment for a small group for the sample and this confidence info that is narrow it's not wide it's narrow it means that the results that we are getting is are accurate more or less accurate and this means that we the results that we are getting based on a sample are most likely a true representation of the entire population that we got this is the idea behind the width of the confidence interval the narrower it is the higher uh is the quality of your results which means that the uh more generalizable are your results so let's now go on to the final stage of our case study which is to test the Practical significance of our results so now when we know that the statistical significance is there the experimental version of our feature is statistically significantly different from the control version in terms of the clickr rate and we have seen that the competence interval is narrow which means that our results are accurate quite uh with quite high accuracy then we can now comment on the Practical significance of our results this means we want to see whether the significant difference that we obtained whether this difference is actually large enough from the business perspective to say that it's worth to put our engineering resources and our money and our uh uh product into uh to put through through this change and to uh say that it's wor from the business perspective to change this button and to put this into um the production and in front of our users and of course here we are not only talking about the engineer ing resources that it will take from us to change this and the deployment and the monitoring but also in terms of the quality of the product we are providing to our users because whenever we are making a change to our product it is a risk because we are changing what our user is used to see and this can always be scary uh when it comes uh to the business because we don't want to uh make our customers scared so therefore we need to also check for this practical significance so for that what I'm doing is that I'm creating this python function that will take two arguments so two values that is the minimum detectable effect and then the 95% confidence interval that I just calculated those will be the two arguments for my function and I'm calling this function is practically significant and this function will go and check whether the uh practical significance is there or not and it will then return through true or false and then it will also print whether we have a practical significance or not and we learned from the theory and we know from this AB testing concept that whenever the uh MD or the Delta that we got the minimum detectable effect is larger than the lower bound of our confidence interval it means that the lowest possible value that we can get based on the results that we obtain in our sample that that amount is smaller than and the minimum detectable effect that we assumed before even conducting our AB test this suggests that we have a practical significance and the difference the minimum difference that we will obtain is large enough for us to have a motivation to make this change in our product for that what I'm doing is that first I'm taking my 95% confidence interval and I'm taking the first element because we know that a confidence interval is actually ranged so two pull of two numbers the lower bound and upper bound I need the lower bound because all I care for this practical significance is to compare the lower bound of the 95% confidence interval to this minimum detectable effect which is my Delta so therefore I'm taking this lower bound of confidence interval putting that into a variable and then I'm using this variable this lower uncore bound uncore CI confidence interval and I'm comparing this to my Delta I'm saying if my lower Bond of the confidence interval actually I'm noticing that here I got a mistake it should be the other way around we need to say that if our Delta is larger or equal the uh lower bound of the confidence interval which is the same as if our lower bound of the confidence interval is smaller than equal our Delta so if our we can also write this the other way around so if our Delta is larger than equal than our lower underscore bounde uncore CI then we can say that we have a practical significance so we the MDA of in this case so I want to use my initial Delta therefore I won't be initializing this so you might recall here a Delta of 10% I want to still make use of that Delta so therefore I will just go ahead and then in here what I want to do is to call this function by using that specific Delta so I want to have a 10% as my MD and whenever this Delta will be larger than the lower bound of my confidence interval that I just obtained I will then say that we have a practical significance and with an MDA of 10% the differ between control and experimental group is also practically significant so you can see that the lower bound is 0.04 something that we obtain in here and that amount is then compared to this Delta and here you can see that we have concluded that we also have a practical significance so amazing we have come to the end of this case study and in this involved case study we have conducted an entire app AB has results in ases so this case study a has and to end going from the point of loading the data and then understanding this business concept or business objective of ab test where we were testing whether the um enroll Now button which is the new version the experimental version should replace the existing button which is the secure vraal and based on this case study what we found out is that we have a statistical significance at 5% significance level suggesting that we can reject the N hypothesis and we can say that indeed there exists a statistical significant difference between the click through rate in the experimental group versus control group uh and specifically that the enroll now experimental button results in statistically significantly higher click through rate than the uh secure free trial button and beside this we also checked the um accuracy of our results by looking at a confidence interval and saw that the confidence interval was quite narrow suggesting that the results we obtained were quite uh accurate and this means that the results that we got for the sample will generalize to our population of users and finally we have also checked the Practical significance of our results by using the 95% confidence interval and comparing the lower bound of that interval with our minimum detectable effect Delta and we saw that we will have at least 10% significant difference between the control groups CTR and the control uh the experimental group CTR and the experimental group CTR will be at least 10% higher than the uh control groups and this suggests that uh from the business perspective we also have a motivation uh beside of this statistical significance we also have practical significance suggesting that we also have enough motivation and reason from the business perspective to put this new button into production so we can conclude that uh based on this datadriven approach and conducting an AB testing we uh can see a clear motivation of deploying this new button and draw now and replace the existing one secure free trial version and we will then expect to see more users clicking on this and engaging with our product and for now this will be all for this case study if you want to learn more about AB testing make sure to check our AB testing course as well as the ultimate data science boot camp don't forget to try our free trial this time using our enroll Now button and if you want to see more case studies like this make sure to check our tic case studies we have many case studies also included as part of our ultimate data science boot camp where we go in detail of these different steps and we conduct different sorts of case studies to put our data science theory in to practice including from the field of NLP machine learning recommended systems Advanced analytics and also AB testing and soon also from AI so for now thank you for staying with me and conducting this case study happy learning thank you for watching this video If you like this content make sure to check all the other videos available on this channel and don't forget to subscribe like and comment to help the algorithm to make this content more accessible to everyone across the world and if you want to get free resources make sure to check the free resources section at lunch. and if you want to become a job rate data scientist and you are looking for this accessible boot camp that will help you to make a job ready data scientist consider enrolling to the data science boot camp the ultimate data science boot camp at l. you will learn all the things the fundamentals to become a jbre data scientist he will also implement the learn theory into a real world multiple data science projects beside this after learning the theory and practicing it with the real world case studies you will also prepare for your data science interviews and if you want to stay up to date with the recent developments in Tech what are the headlines that you have missed in the last week what are the open positions currently in the market across the globe and what are the tech startups that are making waves in the tech and sure to subscribe to the data science Nai newsletter from lunar Tech