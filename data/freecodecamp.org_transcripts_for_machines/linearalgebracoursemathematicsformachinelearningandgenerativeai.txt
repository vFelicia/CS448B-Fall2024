ready to learn mathematics behind machine learning generative AI or Cutting Edge teag then you are in the right place today we are going to learn linear algebra like never before in this Crush course in 6 hours we are going to learn the basics and the fundamental concepts in linear algebra a fundamental part in mathematics when it comes to many applied sciences our Crush course in fundamentals to linear algebra will be a great starting point for anyone who wants to refresh their your algebra and knowledge or who wants to start with the fundamentals of mathematics for AI data science machine learning or generative AI so if you are looking for that one course to refresh your memory in those fundamental concepts and be able to create your own algorithms and create your own AI dream products then you are in the right place and if you're really serious about your education and career and backup with your mathematical foundations then you can also check our 20 5 plus our comprehensive fundamental to linear alup course which is also part of our mathematics boot camp so check at lunch. and you can get this comprehensive 25 plus hour course along with the certification in linear algebra so here's what we are going to cover as part of our fundamentals to linear algebra crash course so we are going to start with the introduction to linear algebra we're going to talk about the prerequisites what is exactly that you need in order to get into linear algebra whether as a student as part of your undergrad or Graduate Studies or as a working professional who wants to learn the mathematics in a good way in order to become a wellrounded professional whether it is for your future in quantum physics in a future of data science machine learning or artificial intelligence so we are going to learn the basics of the vectors this is the first section that we are going to cover here we are going to learn the vectors their basic definitions the theory but also the examples so in this course we are not just going to scratch the surface we are going to dive deep and the idea is that for even for those basic concepts we are going to learn not just the theory the basic definitions but we are also going to implement them in different examples we are going to manually solve different examples by hand in order to truly master and practice this topic and beside this we're also going to talk about the applications of them across different domains including in data science and AI the idea is that those basic concepts are your starting point in the future of linear algebra because to truly understand this topic and to truly understand this entire linear algebra field at least the course the academic level course that you are used to seeing as part of the different Advanced mathematical studies you will need to dedicate more than just 6 hours so if you are serious about your education and career and you want to learn mathematics from ground up then check also our 25 plush hour complete fundamental to linear AR records to truly Master the entire linear algebra in a very economic and efficient way which contains an entire semester equivalent material Theory practice and the applications so here's what we are going to cover as part of our crash course so we will start with the introduction to linear algebra we will talk about this concept of linear algebra we will see what are the prerequisites in order to even understand and start with this course we're going to refresh our memory with the different fundamental concepts here in this introduction to linear algebra section we are going to quickly cover the prerequisite that you must know in order to get started with linear algebra here I'm also going to refresh our memory on the fundamental concepts like real numbers and Vector spaces the norm of a vector the cian coordinate system and also the angles triogen ometry the norm versus equid in distance pedian theorem Etc so uh after this we are going to finally Kickstart dealing your algebra Concepts so first up we are going to talk about the basic Vector spaces here we are going to cover the definition of the vectors what are vectors the basic representation of it but also the indexing of it and how we can apply different simple operations like vector addition Vector subtraction and also scalar multiplication after this in the next section we are going to talk about the matrices and the basic operations we are going to truly understand the matrices the basic definition of them we are going to see many examples of matrices and also in practice what are the basic operations that we can apply to matrices including the additions the scale and multiplication and subtractions after this we are going to look into the dot product and the vector length in this section we are going to talk about the idea of dot product what is the difference between dot product and inner product understand the fundamentals of that product and how they are calculated we're going to complete many examples as this a very important topic especially for data science machine learning in Ai and once we are done with this we will also look into this concept of the length of a vector and how this is related to for instance cosine rule and these different inequalities when it comes to the vector spaces after to this we are going to look into the basic linear systems how we can solve a linear system by using the infamous gaussian elimination here we are going to talk about this basic concept of solving linear equations with a many unknowns using the concept of matrices we're are going to see detailed examples where manually step by step by hand we are going to derve the um entire process using gin elimination how we can solve the system of linear equations first getting the arent Matrix and then this concept of reduce row form so RF and then R RF so row H form and reduce Ro form and uh we are going to look into two different scenarios to ensure that we understand this intuition behind solving this linear systems in various um type of cases now we are going to uh finish off uh the crash course with the preview of the advanced concepts we are going to see the special matrices we are going to see symmetric matrices diagonal matrices and then uh the idea of uh n matrices W matrices how they are used uh and why they are used commonly in programming and then we are going to see a glimpse of how linear algebra can be applied across real world scenarios so in this crash course we are not just going to cover the uh quick definitions but we are going to dive deep we are going to truly try to understand those concept those uh geometric intuition and interpretation behind those Concepts and how they are related uh towards many applied sciences so to truly understand how each of those concepts are actually implemented in the real world like in the fields of data science statistics and AI this course is perfectly tailored for working professionals who need to sharpen their math skills who want to uh quickly and efficiently refresh their memory whether is before their interviews or who want to learn machine learning and statistics and AI but they don't have the mathematical background and instead of just going and studying at University like I did and spending many years to do an undergraduate uh studies in mathematics or statistics instead you can just follow this course to quickly refresh your memory but if you truly want to understand all the concepts which is equivalent to one semester or linear algebra course then definitely check out our 25 plus hour fundamentals to linear algebra course as part of our mathematics boot camp at l. because you can't master linear algebra in just 6 hours so if you are a student this course will also be a good way for you to refresh your memory if you you are at the end of your deadline and you are about to complete your linear track exam and if you want to learn everything from scratch and you have the time then you can also check our 25 plus hour course to learn the linear algebra fully and to Ace your exams this crash course is just the beginning for you in this way you are laying a solid foundation to become a wellrounded professional and it truly understand this topic of linear algebra so if you're ready I'm really excited so let's let's get started welcome to the mathematics for data science and AI road map in 2024 today we are going to discuss the road map behind a topic that powers many algorithms the linear algebra we are going to talk about all the concepts and the topics that you must know in order to master the linear algebra which is the backbone of data science machine learning generative Ai and many other topics and Cutting Edge Tech whether you are aspiring to become a data scientist or AI professional this is your starting point to learn the mathematics of data the fundamentals or linear algebra if you want to become a good professional a wellrounded data scientist and master the art of machine learning generative Ai and many other Cutting Edge Tech domains then your starting point should be to learn the mathematics to truly understand this different mathematical Concepts that power the algorithms behind the scenes including many Infamous algorithms like the GPT series or the bird or Transformers linear algebra is the mathematics of data it's about the vectors matrices projections and how to solve a system of linear equations and how to decompose your Matrix into many parts to simplify the equations and solving of the linear systems think about metric factorization that is used as part of the recommended systems including the ones in the Netflix that uses the streaming company to recommend you movies when you go to the net so linear albra is one of the most fundamental and must to know topics when it comes to mathematics for your data science AI Journey so let's now dive into the road map behind linear algebra and how you can learn linear algebra in 2024 I'm going to make use of the curriculum and the road map that we have carefully drafted as part of our 25 plus hour fundamental SAR algebra course which is also part of our mathematics boot camp at lunch. so I'm going to make use of that road map and I'm going to also make use of the applications of lunch. a in order to Showcase you all these different topics as part of linear algebra road map of 2024 so let's di first we need to start with the linear algebra introduction you will need to understand this Concepts behind linear algebra where it is positioned in this entire field of mathematics and why you exactly needing your algebra to study so um first you need to learn the prerequisites and all this must you know topics that you need in order to be able to understand linear algebra think about the real numbers the vector space this concept of the norm of a vector the length of a vector the cation coordinate system which is usually covered as part of the prealgebra or algebra courses and also the Angles and tri gometry think about the cosine the sign cosine rule the sinus rule the Tang function how it looks like when you visualize it the geometric interpretation behind it and also this concept of Al and distance and how Elan distance is different from the norm and finally you also need to understand this concept of Pythagorean theorem and the orthogonality so when it comes to the linear algebra one thing to keep in mind is that linear algebra is usually a course that is part of the undergrad studies like computer science econometrics advanced mathematics advanced statistics and usually the students of the studies they in the second or third year of their bachelor they learn linear algebra and this road map that I'm providing to you is based on academic level plus the industry Insight curriculum which means that uh the um uh concept is quite technical so if you want to truly understand the linear algebra and you are serious about your career and your education then this road map on one hand might seem bit Technical and intimidating but on the other hand you need to understand that students spend about semester on this entire road map but if you understand these Concepts this will open many doors for you and instead of just using the libraries of people who have created for example large language models like the gpts or the bird or the uh mixtur uh AI models instead of just using those libraries and pretrained model you can create your own models and I think that's a good enough motivation for us to truly understand and invest the time and motivation to master the art of linear algebra so once you have these prerequisites that usually come from prealgebra or courses like the Calculus 1 and calculus 2 to uh learn these different concepts you are ready to go onto the first journey and first section behind the linear algebra which is to learn the vectors and operations so be prepared to learn the foundations of the vector s the special vectors and operations and let me actually go ahead and get more uh in detail curriculum for you so um start to understand uh the vectors and operations what are the foundations of vectors the fundamentals of linear algebra the scalers and the vectors this representation of the vectors in terms of the magnitude and the direction and also understand this uh Vector notation the indexing of the vectors first upop you can uh start with learning vectors and operations so here you need to understand this uh scalers the vectors the representation of the vectors like the magnitude and Direction what is a common notation of the vectors in the industry as well as this indexing in vectors so understand what the indexing behind vectors are because then you need to implement this in programming languages like python then uh once you are clear on those topics then you can get into the special vectors and operations so understand what are the zero vectors and the unit vectors the sparsity vectors the vectors in higher Dimensions the vector addition and subtraction as well as the uh Vector uh multiplication so scalar multiplication how you can calculate the multiplication behind the scalar and a vector also learn to um the different properties of the vectors and the operations uh of these vectors so once you are done with that get into the advanced Vector Concepts so learn the linear combination concept and try to understand this concept of unit vectors the span of a vector what is this definition and the application of it uh the concept of linear Independence is really important understand this concept and also uh be able to uh analyze and see from uh multiple vectors whether they are linearly independent or not look into this concept of scalar multiplication as well as the application of scalar Vector multiplication and look into this concept of a length of a vector and a DOT product as well as what is the difference between the dot product and the inner product after this get into the topic of dot product the coui squares and its application so understand the concept of dot product the inner product what are the differences between the two what is the properties of dot product calculate dot product by hand on many examples and also learn this concept of COI squs CI squs is an Infamous inequality theorem comes from linear algebra and understanding its uh Theory but also intuition maybe you can even derive the proof of it using the cosine law and how that relates to this concept of the norm of a vector and when it is that the norm of a vector is equal to zero so this completes then the vector operation section uh in here this is the high level overview of the topics that you must know in order to learn the vectors and the operations and once you are done with this then you need to get into the next section and the uh domain in linear algebra which is the matrices and solving linear systems here I'm talking about foundations of linear systems and matrices the introduction to matrices the theory and the intuition behind it core Matrix operations gaan reduction n space Comm space Rank and the full rank so here think about the uh General linear systems like uh the co efficient labeling homogeneous versus nonhomogeneous systems and what is this uh definition of matrices the uh notation of it as well as this this concept of the rows the columns the dimensions of matrices the identity Matrix diagonal matrix other other special type of matrices like uh one's Matrix the zero Matrix but also learn the core Matrix operations and practice with them like Matrix operations of addition subtraction scalar multiplication and also the multiplication behind multiple matrices once you are done with this um in terms of the solving of linear systems think about Matrix representation of linear systems the solving of linear systems using the infamous gausian elimination like understanding this argumented coefficient Matrix the row H form the reduced row H form the identity Matrix and the reduced row H form look into the detailed examples implement this in uh practice with multiple examp step by step by um manually deriving the solution to the system as well as how you can calculate a new space a com space and the basis of these different uh spaces on an actual example once you are done with this learn this concept of the rank and in what case you can say that the Matrix has a full rank next up is the section of linear transformation and matrices so learn the algebraic laws of matrices they approves the determinant the concept of the transpose and the inverses of matrices and here this is exactly what I mean in more detail so a way comes to the algebraic Row for matrices learn about the cumulative low for matrices associative low distribution low the scaler multiplication low for matrices and look into the proof of this lws as well as the examples after this get into the determinants and their properties look into the uh theory behind determinants why we use determinant how we can calculate related determinant on an example as as what are the properties of determinants and the geometric intuition behind it so when it comes to the Matrix inverses and identity Matrix look into the definition the theory behind it also this non Singularity of matrices and what is this definition of and the formula when it comes to the inverse of a 2x2 matrix and how you can go from there to a larger matrices let's say 3x3 matrices looking to the detailed examples calculate them inverses of the 2x2 matrices detailed uh in detailed examples and also do the same for the 3X3 matrices so how you can use this idea of conjugates and by changing these different signs to calculate the inverse of a 3X3 Matrix after this get into the uh applications of uh inverses of a matrixes why we need them what is the intuition behind it and where do we use it even when we are dealing with the very basic uh machine learning algorithm like um linear regression we use actually the transpose of matrices the inverse of matrices the multiplication behind matrices to find the uh solution to the uh linear system behind the linear regression so uh this is a very important concept that I recommend you to learn before getting into machine learning next up is the transpose of matrices learn the um properties of the transpose of matrices the U theory behind it but also the implementation and the relationship behind the dot product so once you are done with this the next St is to uh get into more advanced section which is the final section way it comes to mastering fundamentals of linear algebra which is this um different Advanced linear algebra topics think about Vector spaces the projections of vectors the grumme process the metrix factorization and look into couple of examples of Matrix factorization techniques that are widely used across the industry including the QR de composition the igen de composition which includes the calculation of igon values and igon vectors very popular topic as part of data science and statistical modeling also uh Ai and get into the singular value that composition which is the SVD so let's dive into the detailed uh details of this uh section when it comes to the vector spaces projections and the gred process look into the definition of it the stepbystep process behind it the uh projection formula and the stepbystep example behind the uh uh projection in order to truly understand how you can calculate the projection of a vector and also look into the geometric interpretation and the intuition behind vector projections uh learn the theory but also the implementation of the dotproduct of vectors and the dotproduct of a vector with itself look into the uh relationship between the dot product the vector of ve the projection of vectors and then look into the concept of orthonormal basis understand the orthogonality normalization and look into the infamous grme process understand how you can use the IDE of that products but also the projections in order to conduct this grme process step by step once you are done with this then uh look into the application of grme process and the auton normal bases so uh as part of these special matrices and their properties I would recommend you to learn the special matrices like the diagonal matrix the symmetric Matrix The orthogonal Matrix and the different uh applications and examples of them step by step once you are done with this learn this concept of Matrix factorization Matrix factorization is a very important topic when it comes to data science machine learning but also AI uh the metrix factorization different examples and techniques that um are falling under this umbrella of metrix factorization techniques that are used in both in machine learning but also in deep learning and in generative AI so if you want to become a true uh wellrounded professional in this Fields then definitely be prepared to learn metric authorization techniques dive into its applications understand the different examples of what kind of metric alization techniques are used in what uh algorithms and this will help you to get more excited and warm up for the next sections which are detailed examples and you truly understand at least couple of metrix factorization techniques so first up uh learn for example the QR de composition which is a matrix factorization technique learn the theory behind it the stepbystep process behind QR de composition how you can calculate the um q's and then the vectors of r so construct The Matrix of Q and then Matrix R and then uh decompose your Matrix a into uh q and then R matrices in order to decompose your entire Matrix look into detailed example the calculation of it uh with a stepbystep manual process and look into its practical consideration and application in solving linear systems after you are done with this look into the concept of IG value the composition how you can calculate the IG values and I vectors what is the theory but also the Practical implications of them where are ion values and igon vectors used for example in the infamous principal component analysis or PCA then look into this concept of ion de composition look into detailed examples and manually calculate the ion values and ion vectors as well as the igon value that composition so once you are done with this and you have looked into the applications of igon bi and ion vectors the calculation of it as well as the Theory getting into the final topic which is another Matrix factorization technique an Infamous one which is the singular value de composition the SVD look into the theory behind the SVD the left singular matrices the right singular matrices this uh theory behind them but also the Practical example behind them uh what is this idea behind um singular value de composition how you can construct The Matrix of s The Matrix of v and the Matrix of D and how you can decompose a matrix into the three different matrices by constructing them separately once you are done with this look into detailed example this might take a bit let's say an half hour or 1 hour in the first go to calculate this different matrices by hand but trust me it is worth it because you will then truly understand those Concepts and you will understand how and why those uh Matrix factorization techniques are used and applied in the dat science and artificial intelligence welcome to the course on the fundamentals of linear algebra my name is D Vasan and today we are going to start with some basic concepts that are important for understanding linear algebra linear algebra is one of the most applicable areas of mathematics it is used by pure mathematicians that you will see in a universities doing research publishing research papers but also by the mathematically trained scientists of all disciplines this is really one of those areas in mathematics that you will see time and time again appearing in your professional life if you want to become a job ready uh data scientist or you want to do some handson machine learning deep learning Andi stuff but also linear algebra is used in cryptology it is used in cyber security and in many other areas of computer science and artificial intelligence so if you want to become this wellrounded Prof professional you want to go beyond using libraries and you want to truly understand the uh mathematics and the technical side of these different machine learning algorithms from very basic ones like linear regression to most complex ones coming from Deep learning like architectures in neural network how the optimization algorithms work how the gradient descent works and all these other uh different methods and models then you are in the right place because you must know linear algebra such that you will understand these different concepts from very basic ones to most advanced ones in data science machine learning deep learning artificial intelligence data analytics but also in many other applied science disciplines so before starting this comprehensive course that will give you everything that you need to know about linear algebra first I'm going to tell you what we assume that you already know because linear algebra it comes from about third uh year of Bachelors um of different uh highly technical studies and um here um we are assuming that you already know certain Concepts so uh to ensure that this course stays really on the topic of linear algebra and that you uh understand all these Concepts really well for that we need to uh be able to know different topics so before we dive into these Concepts uh let's familiarize ourselves with the basic prerequisites and notations used throughout this course and you will really need to know this in order to understand this Concepts really well such that instead of memorizing you will actually just hear me once or maybe twice and then every time you hear later on or you see it in the papers or in some algorithms you will recognize um this is something that we already learned so uh some key prerequisites overview is here um first of all to fully grasp the upcoming material you should be familiar with some basic concept like real numbers Vector spaces so you don't need to know this idea of vectors though you uh already most likely are familiar with this given that you know how to plot different uh lines you know the idea of x's and y's and how to plot these different graphs but um here we are going to touch base on this every time when we come close to this Concepts I will refresh you uh your memory and we will go through this numbers the idea of norms and distance measures because when it comes to the vectors when it comes to the magnitude and all these different uh topics that we are going to discussed as part of linear algebra knowing the what Norm is and um what is the definition of distance what is the length between uh two points when we plot it in the two dimensional space or three dimension space those are all very basic concept that usually you see as part of a basic prealgebra or just a common algebra courses and um lessons in order to truly understand what the new algebra is about to understand this direction of vectors the angle and then um the uh dimensionality reduction how linear algebra is applied for instance in different algorithms in machine learning deep learning data science statistics you really need to understand this Cartesian coordinate system so uh this is not only important for linear algebra but I assume you already know it given that you have passed those um uh other courses like uh calculus or usually they are covered as part of prealgebra or algebra so the cartisian coordinate system I mean here understanding uh what is for instance the the common um a description of them for instance when you when we write like X and then y on the vertic axis and then we can uh we have here zero and um then uh we can always plot this different plots you know we we have a clear understanding what is this um Y is equal to X line we understand how by knowing certain points we can plot different plots for instance that this is the Y is equal to X line that here it means that if we have here one then this is just one two this is two so we understand when we have the function of the line and we have a certain value that is our y coordinate or x coordinate then the corresponding uh coordinates can be found then um you also need to know um some basic things that I just didn't mention uh right now so for instance that the numbers here can be like 1 2 three up to Infinity so you understand this concepts of infinity and then here the same uh story then here we have minus one you know Min 2 uh and then this is then used later on and we will be uh touch basing this one we will be describing our vectors and how uh we can visualize our vectors in a two dimensional space like we have here because this is two dimensional so we have X and Y but we can also of course visualize it in threedimensional Etc so this idea of basic coordinate system is really important um usually covered as part of algebra if not pre algebra then we have basic triog gometry which means that you need to have a clear understanding what sinus is what cosine is what tangent is and their reciprocals and here I mean uh that you know for instance um what is cosine function what is sign function um you know that you have an understanding for instance that um uh what is this line you know um whether it's a sinus line or cosine line you have also an understanding what this Pi is um one thing that I didn't mention but it it just goes um around all these topics some basic things that you understand what is X what is why why we uh use them and this idea of uh variables uh and also uh you need to understand this idea of square uh or you know a 90° uh angle and then uh pagas theorem here we have the same so what is this relationship between different sides of a triangle that is a very unique triangle and that has one of the uh angles as 90° um and uh this idea of um you know the sides how this relates to the sinus cosinus tangent cotangent um and also um how the Pythagorean um Pythagorean theorem applies when we have uh uh triangular but it is no longer with a angle that is 90° what is the sum of all the angles of triangle so those are basic stuff that are com commonly covered as part of uh trigonometric uh lessons or part of General geometry so when it comes to this R so as part of real numbers and Vector spaces R represents the set of all real numbers so you can be dealing with for instance an integers like 1 2 3 this can Al this will also cover all the negative numbers like minus one 2 3 but also the floting numbers like 1. 223 and all the other numbers that you can think of those are the set of all real numbers so this is in onedimensional space right so you can see that I'm writing just one number you know two three and other numeric numbers then we have the idea of R2 R3 up to RN where all these numers they represent in this case the N it represents the N dimensional aidian space so when it comes to this idea of n dimensional numbers so for instance R2 here we just mean 2D plane so I'm pretty sure you are familiar with this idea of for instance xaxis and y axis here we are dealing with two dimensional plane so for every point that we can find here we can describe them by assigning them a value X so coordinate X and a coordinate y that's exactly what we mean by saying that the number can be represented in a 2d plane so here we are dealing with this two dimensional space this is our twodimensional Elan space and every number in here that is part of this R2 can can be pictured here can be represented in this visualization so for instance if I have this number and let's assume that the value on the x axis is two and we can see here that the corresponding Y is zero I can describe this number which I will call a I can describe this by writing down first the x coordinate which is two and then the ycoordinate which is zero so I'm then saying that a which is a point with x coordinate 2 and y coordinate zero it is part of my R2 and it's part of my two dimensional alian space when it comes to R3 similar thing we can do with that only in that case we need not just x axis and y AIS but we need to add our third dimension so here for instance when it comes to the r Tre then we need to do y AIS we need to have X axis but also we need to have some Z axis so such that every time every point in the space we can then describe by x y and Zed coordinates so if we write it in terms of the vector something that we will see very soon as part of our first unit of this course we will then need to represent every number in this threedimensional Alan Space by writing down first the x coordinate let's say one and then y coordinate let's say another one and then Zed coordinate which is one or even better even easier let's use 0 0 0 which means that we are dealing with this initial number which is the center of this threedimensional Elan space when it comes to the nend dimensional or higher dimensional spaces it's much harder to visualize therefore usually when it comes to visualizations we do usually we us usually only visualize the onedimensional twodimensional and threedimensional spaces above then it just no longer does make sense to visualize it but we definitely deal with them and they are part of Applied linear algebra so understanding the spaces is very important for analyzing vectors for their interactions and this holds not just for this twodimensional and threedimensional but really for multidimensional space let's Now quickly Define this idea of Norm so the norm of a vector denoted by this uh uh V which you can see kind of like similar to the absolute value from prealgebra you can see here that we have this double straight lines like from absolute value then we have the name of the vector or the variable name that we are assigning to our vector and then you might notice is here on the top of this this Arrow this basically says that we are dealing not with just a variable but really we are dealing with a vector this is really important because you can see that there makes a huge difference if we have for instance just V or V1 I have to say or just V those are really important and things that you need to keep in mind when it comes to linear algebra and trying to differentiate vectors from a point you will notice that when it comes to Norm we can uh represented it either by this notation or this usually it's a common um notation uh in machine learning or in data science um with this uh two bars and um when we do this we automatically also know L2 norm and this is something very common and uh usually used as part of U retrogression which is an application of um linear algebra uh and it's used in uh regularization so we are regularizing our machine learning algorithms so when you get into machine learning you will see time and time again this um notation so uh next time when you see this then you know automatically that you are dealing with L2 norm and L2 Norm which is also used a lot in machine learning it is referring to the usage of L2 Norm to uh in the uh read regression and regression or L2 regularization is a very popular regularization techniques as part of machine learning so right now even you can see this uh intersection of linear algebra or um this uh idea of norms in machine learning the norm of this vector v is equal to square roof and then V1 squ plus v2^ S Plus and all this in between numbers plus VN s so here basically it means take square root of V1 2 then V2 S Plus V3 squ blah blah blah plus VN squ so basically take all the units that form this vector and then so are on this vector and use them Square them and then add them and then take the square root of that that's the distance or I have to say the norm of this Vector we saw already the norm here is just a another example what Norm is and um on a specific two dimensional Vector when we have for instance that a vector is equal to three and four which means for the First Dimension let's say on xaxis we have three and then on Y axis is equal to four then the norm so this is equal to we take the x value so three and then we Square it so V you can see here this is the case when n is equal to 2 this is simply equal to square root for v1^2 + v2^ 2 and as V1 is equal to 3 so this is our maybe I can make this just V1 and this is my V2 then the norm or the equ in distance for this Vector so this thing is equal to V1 2 + V2 2 which is equal to 3^ 2 + 4 squ and this value is square root of 25 and it's equal to 5 let's now see the difference between aladine distance and the norm so you could see here the norm here we have just one vector like here and this Norm it has just two corresponding values into two dimensional space you see here we have just three and then four so this is V1 and V2 when it comes to the Alan distance this is kind of the generalization of this idea of Norm so the aladine distance between two points A and B in r n so in the N dimensional space is the norm of the vector connecting a to B so we see that the norm and the Alan distance are highly related to each other only we are talking about the norm when it comes to one vector but when we have this Vector a and the vector B this is simply the Alan distance so for the Aline distance we know already this idea of distance how we can measure it and you can see that this comes very similar to what we see here notation and here we are saying well we have this vector and then it has this two coordinates in N is equal to 2 in two dimensional space when it comes to the Aline distance Alan distance helps you understand what is this distance between two points in an N dimensional space so the aladan distance between two points let's say A and B in N dimensional space is the norm of the vector connecting a to B so for instance if we have a point a and we have a point B we are connecting this and this is the vector connecting these two points then the aladan distance is simply the norm of this Vector so this is the aladine distance so we can see that norm and the distance they are highly related to each other in the alian distance we are using this idea of norm and specifically the norm two as I mentioned before so here you can see that the definition of Aline distance so the distance between A and B the two point is equal to square root of A1 B1 2 + a and then here we have basically A2 b 2^ 2 and then plus A3 B3 squ those are things that we cover as part of this dot dot dot and then plus after the last point when we have a n minus BN 2 so here what we mean basically is that if we have two points here is a and here's B and this our vector and we know all these different points so A1 B1 A2 B2 A3 B3 blah blah blah and then here a n BN we know all this points ly in here in this distance then we are taking them and using them to calculate the L in distance so here for instance if we have um point A and B so in this example let's do a quick one specific example when we have a point a which has coordinates one and two so this is basically A1 A2 and then point B with uh points in it like B1 B2 you you can notice that the da AB so the distance or the Eid distance of these two points which is equal to the norm of this um Vector but here this is a and this is B and this is this Vector this is equal to square root of 4 min1 so it takes the B1 so this is B1 and this is A1 takes the square and then says Plus B2 A 2^ 2 takes the square root of that and says this equal to 5 now you might be wondering but hey why do we do then instead of 1 B1 2 we do B1 A1 2 and the answer to this question lies in the um uh properties that we learn as part of prealgebra because it doesn't matter when we take uh A1 B1 squ or B1 A1 squ because this squared ensures that it doesn't matter which one we take first and subtract the other now the proof of that is outside of the scope of this um course is this is part of prealgebra but I just wanted to put this out there to ensure that uh you are uh seeing what we are seeing here because here it says A1 minus B1 but in this example we are taking instead depth uh B1 and we are subtracting A1 this is a common thing that we do in um prealgebra and just in general uh in different um Alin dist or distance related cases so I just wanted to put this here to ensure that uh later on this is something that can be clear um from the first view why this is important this idea of norms and Equity IND distance beside of being used in machine learning and why is it used so Norms they provide a way to measure the size or the length of a vector in Vector spaces which means that when we want to measure a distance a similarity a relationship between for instance vectors then it becomes much easier to use this idea and El IND distance is not only used in regularization techniques like L2 regularization or retrogression but it's also used in other machine learning or deep learning algorithms as a way to measure the distance or the relationship or the similarity between two different entities those can be variables those can be two people that we want to compare in our algorithm or two entities uh um for instance the um Norms or the distance they are also used as part of K me algorithm something that you might have heard and if you follow later on the machine learning and the clustering section of machine learning you will see that aladan distance is used as part of K's algorithm that aims to Cluster observations into different groups so this also yet another highly applicable uh topic that you must know in order to understand different linear ALB top topics but also machine learning topics why this prerequisites matter and why I mentioned those understanding this concept is very crucial they underpin this geometric interpretation linear algebra they will help you to better understand these Concepts and not just to memorize them but really understand and later on when you go into your machine learning and AI journey and your data science Journey seeing these Concepts will help you to better understand those different all algorithms these optimization techniques what we mean when we say we want our optimization algorithm to move towards local minimum Global minimum this idea of movement this idea of vectors later on WE you will also understand this different concepts in deep learning how these models work how the neural networks work and those are essential Concepts that you need for solving different systems of linear equation a core part of this course they also help you in visualizing vectors spaces which are critical to understand this concept of linear algebra the applications of linear algebra when it comes to the real world applications so those are things that you can definitely Master by following some of our other courses but for this course I assume that you are already familiar with these Concepts right so now we are ready to actually begin and with this prerequisites in mind you are prepared to part your linear algebra Journey we are going to learn everything in the most efficient way in such a way that you will learn the theory you are going to see many examples we are going to learn everything in detail but at the same time you're going to learn the must know Concepts and I'm not going to overwhelm you with this most difficult concept that you will not be seeing in your career I'm going to give you this bare minimum when it comes to really knowing and the must know for linear algebra such that that you will be ready to apply linear algebra in your professional Journey whether you want to get into machine learning deep learning artificial intelligence data science knowing these different concepts in linear algebra you will be a pro in your field going to give you everything that you need the theory examples implementations everything in detail but at the same time you will be doing that in the most efficient and timesaving way so without further ado let's get started hi there so let's get started with our first module which is foundations of vectors in this module we are going to talk about fundamentals of linear algebra vectors we are going to make a differentiation with between scalars and vectors we are going to Define them so first we will learn the theory then we will Implement them into practice by plotting them by looking into different examples then we will look into this representation of vectors by looking into the magnitude and the direction of it and the representation of them just in general we are going to plot them in our coordinate system then we are going to see the common notation of vectors and indexing of them vectors are super important when it comes to linear algebra and application of it and uh they matter not only in mathematics but beyond so uh vectors help us in many ways from figuring out how objects move to solving math problems in science and just in general in technology including in data science machine learning artificial intelligence Etc they are super useful tool so uh let's start our journey with looking into scalers so scalers they are displ numbers and by definition a scaler is a single numeric volume often representing magnitude or quantity for example uh scalers can be describing um the temperature out side for instance the temperature of um a 22° uh can be represented by a scaler or a height of a person can be represented it's a scaler so let's assume we have a scaler that we will Define by a letter s it's just a variable this scaler is then equal to 22 for instance and we are measuring it in degrees so it means that uh if this s measures a room temperature then and the scaler s which is equal to 22° it represents the room temperature it can be for instance 18° or 9° if it's very cold uh it just measures a single volume it represents just a single number or it can be for instance 17 100 2.22 so all this they are just scalers they represent a single numeric volume they often represent the magnet itude or a quantity very soon we will see that scalers they are a value that represent the magnitude of a vector so uh now when we are clear on this very basic concept of scalers let's actually move to this idea of vectors so by definition a vector is an ordered array of numbers which can represent both magnitude and direction in space so uh vectors they are bit more they represent bit more than tailers there are numbers that also show direction like a car spitting down the highway or a bow uh being TR for instance uh when it comes to our previous example where we were using this uh uh room temperature as a way to uh think about the scaler a scaler for instance scaler that we just saw was this room temperature room temperature which was 22° when it comes to the vector vector is different for Vector for instance we can have an example when a bird for instance bird it flies flies at 10 kilomet per hour and I also add here another information which will make this as a vector which is that it flies s sou so here as you can see what I'm doing is that I'm not just oh let me actually remove this part to make it easier to understand okay so uh in this example let me write it down that the example bird flies South at 10 kilometer per hour so you can see that I'm not just adding the scaler which is in this case the magnitude we will see very soon the formal definition of it so I'm writing down the speed I'm defining the speed but also the direction so I'm saying I know that the bird is flying south that's the direction and I know also the speed of it which is the magnitude so thank you hour so here in the vector I have much more information than in the scaler because in the scaler I just got temperature room temperature single value but in case of a vector I not only have a magnitude or speed like 10 kilm per hour but I have extra information which is the direction of it for instance flying to the South so let's now look into some real examples and plotting them to make more sense out of this Ideo vector and what is this magnitude what is the direction so let's assume we have a 2d plane so we have xaxis we have y AIS here like usual we have our z0 Center and we want to plot a simple Vector so uh usually the way we represent Vector in tutorials or just writing down is by writing the name of the vector this can be just a a random name let's assume that it's a v letter V and then on the top we are always adding this Arrow so this Arrow it says and it tells the person who is reading that we are dealing with the vector arrow on the top is that reference so let's assume this uh vector v it starts from the center of our coordinate system and it goes to this point so let's say in here this is our vector V so let's assume that this point in here is equal to 4 which means that the x coordinate is four and the y coordinate is zero as the um uh Arrow it just as the point in here it has a a y value of zero so you can see that it goes straight from zero to this one to this point okay so what tells this Vector uh to us is that we have a value that describes the length of the vector so it goes from 0 to four which means that the length is equal to unit 4 so it's equal to four um and we have just learned and we were just talking about that the magnitude is the length in this case so the length describes the magnitude in this case so this means that the magnitude of this Vector is equal to four and then um what else we can see here we can see the direction of the vector which means that the direction is also something that we can see here this is the direction of the vector so this going straight from this point to this point in a horizontal way so independent whether I plot this Vector from 0 to for in here or in here or in here or in here or in here in all cases as long as the length is this I'm dealing with the same Vector because I am basically in this entire R2 space I have exactly the same Vector all I care is about the magnitude and the Direction Where will this Vector start and where will it end I am not interested I'm interested that the that the magnitude in this case the length is equal to the direction of the vector so let's now look into another example where we go a bit more difficult on our coordinates and on our Vector we already saw that we had this Vector where we went let me change the color so this was our vector v and it went from 0er till 4 so this point to be more specific is so this Vector it goes the vector B it goes from 0 0 to 40 so the coordinate X was four and the Y was Zero now let's plot another one um where the direction is no longer horizontal for this Vector let's call it Vector W and for this Vector w we will again start with z0 so we will start again in here but this time we will go like this so let's say we go all the way to this point so this point has a value for an X axis of three and for y axis it has a value of four which means it goes from this point to this point and this is the direction of our vector v so it goes 2 3 4 because this point is 3 0 and this point is 04 so xaxis is 0 x coordinate and y coordinate is four so now you can see that the direction of this Vector is like this while the direction of the vector B was like this and like in case of vector v i again no longer care about where exactly my vector dou use TS and ends but all I care is about its magnitude so the length and the direction so for instance I can have the same Vector in here the same Vector in here as long as the length the magnitude is the same and the direction I am dealing with the same Vector that's all I care so the magnitude and the direction is all that you care about all right so now about the length um that's uh something that you can see very easily from this specific example because by using the Pythagoras Theorem or P Pythagorean theorem we can see very quickly that as the length of this side of our right angle 30° so right triangle we can see that this side is three this side is four which means that this side is five because 4^ 2 + 3^ 2 then we take the square root of that a square root of 25 and it's equal to 5 so the length or the magnitude of this vector v is simply equal to five all right this was about this uh specific vectors let's now look into the uh common representation of the vectors so we always use the magnitude as well as the direction you know to represent the vectors and they commonly are represented by two different uh ways let's now look into the first way that the vectors can be represented and then we will move on to the next one so when it comes to the vector v so we saw that vector v was moving from 0 0 till uh to the point of 40 so we can represent the vector B by 4 and Zer when it comes to the vector w we can represent that uh Vector so Vector w we can again do the parenthesis and we can say that is equal to 34 so by using the coordinates from the coordinate system we can then represent our uh vectors so this is just one way of representing a vector another way of representing these vectors is by using this Square braces given that we are in a two dimensional space first we will mention here the four then we will mention the zero in here two so we can say three and four this is yet another way of representing the vectors in a two dimensional space so if we were to have a threedimensional space so let me actually show it on a new page so if we were to um if we were um to have vectors in three dimensional space so we are dealing with R3 so we have points that can be described by X Y and Z so coordinates space like this so X and then Y and then Z then every point so let's say we have this Vector then we had to represent it by a value let's say x uh X1 y1 and Z1 or um better let me actually use a different letters a b and c and this would be my vector v and I could also represent this vector v is the same so vector v can be represented as a b and c so what thing that you can notice is that unlike the R2 now I have three different entries what we are also referring as rows and we just got one column so um we can uh often represent and usually that's a common way of representing vectors by using this um columns so columns help us to to represent our vectors and you can see very clearly then when it comes to the twodimensional space so when we have R2 so then our vectors have just two rows so three and four four Z like in here when it comes to a three dimensional space we have three entries and so on so the same holds of course also for for instance R5 then for R5 U our vectors so coordinate space can be for instance X Y Zed and then gamma and then let's say Delta and then the coordinates uh of a vector in that space can be V and then arrow is equals sh and then we would have uh let's say A B C D E you get the idea so depending on the space the coordinate space and the dimension of that space then the corresponding vectors can be represented accordingly so the vectors are quantities that have both magnitude and direction as we just so distinguishing them from scalers which only have magnitude so we saw that the scalers got only magnitude while in case of vectors we saw both for the vector v and for the vector w we didn't we didn't only have the magnitude so the length of the vector but also the corresponding Direction so uh when it comes to the um vectors so this is exactly what we just saw in our example a vector in a twodimensional space so in r two uh can be represented by using this Square braces and the corresponding entries for exam Y where X is basically the x coordinate in our coordinate system so in our X and Y system whenever you have this x and y coordinate then uh this X coordinate Will then describe your magnitude and the y coordinate Will then describe your second entry that you need to put when representing your vectors so here the X and Y indicate the movement in the horizontal and in the vertical Dimensions respectively so for X's it's always the x coordinate so how far you move towards the horizontal Direction in here in here or independent in here so always take the x coordinate that is the value that you need to put first and then the Y need to be put it in here so indexing in vectors when it comes to the um indexing the standard mathematical notation uh indices in the N vectors goes from I is equal to 1 2 I is equal to n so the um notation here can be a bit ambiguous so AI uh could mean the E element of AI uh the a vector or the E Vector in a collection so let's start with a simple one and then move on to this next part so what this means and what this means we will look into now so um usually uh when we have a um n dimensional space we are having hard time visualizing it therefore we use this two dimensional space or maximum threedimensional space in order to get an understanding of what these vectors are so we just saw examples of them uh when uh creating our vectors in um V and V uh and W in uh R2 and also in R3 but we can have similar vectors also in R4 in R5 or all the way down to RN where n can be 100 200 500 any number as large as you want the thing is is that visualizing R4 R5 RN is very hard but we can still benefit from great properties of the vectors matrices and in general linear algebra in order to describe different things that have more than three dimensions therefore we have this a bit more ambiguous notation where we use r n and this n can be any real number and it can be all the way to Infinity so very large number and uh let's say we have a vector in this RN then this Vector is usually described Red by using similar Square uh brackets like before only with uh more entries so like before we got just one column so that's something that we didn't uh change but here we have instead of just two entries or three entries like in the two dimensional or threedimensional spaces now we have A1 A2 A3 all the way down 2 a n minus one and a n so we got in total n elements in our column and this describes our single Vector so this Vector in an N dimensional space this we can call also a so one thing that we just so is that it was saying in our definition and notation that uh we might also be dealing with the E Vector in a collection which means that sometimes you will see this while here the A1 A2 they are vector themselves so here we saw that these are just entries so A1 is a number A2 is a number A3 is a number a n is just a number but it's also possible uh when you have a much more difficult and complicated case that you got an A let's write it down with a capital letter A which is equal to A1 or let's actually remove this so we got let's say A1 A2 A3 all the way down to a n minus one and a n where you can already see what is going on so instead of having just a number as an entries instead we have vectors in here so our first element is actually Vector our second element is actually Vector so A2 Arrow A3 Arrow all the way down to a n arrow so while here this can be for instance some numbers let's say one one one all the way down to one one here we have a vector vector another vector and all the way down here yet another Vector where for instance let me remove this part where for instance A1 error is actually equal to A1 1 A1 2 a13 all the way down to A1 n one thing that you will notice here is that unlike in here here I got double indices so I got here a one1 and then A1 2 and then a13 all the way to a 1 n so the first index it doesn't change as I have here a one so I'm writing down the index corresponding to this Vector but the second index it changes per entry indicating which element specifically in the vector I'm talking about so from the first index you can identify the vector that I'm referring to which is A1 and from the second index you can see the corresponding um entry or the value that that um element is positioned in this Vector so you can see that this Valu is for instance in the um Vector one so A1 to be more specific but then it is in the first position this is in the second position in the third position all the way down to the end position so this is something that is really important to understand well because this notation it's going to appear time and time again across various applications of matrices and vectors so it is really important to understand well therefore I want to go one more time through this to make sure that we are clear on what this indices represent so whenever we have an index uh an a vector that we want to uh represent and it's um it has just um it is just a vector which means that it's not a nested vector vector in vector then um we can Define it by let's say a and then on the top an array and it's equal to and here we can have A1 A2 all the way down to a n so you can see what we are also referring as dimension of this Vector is equal to n by 1 so I got n entries and just one column so n by one which means that this already gives me indication that most likely this A1 is a number this A2 is a number this a n is a number so let's say this equal to one two uh three blah blah blah and then here I have let's say 100 but if I'm dealing with a nested Vector later we will see that this can be represented by a matrix then um I can also Define this by capital letter A which is a common way to refer to either matrices or nested vectors and then this is equal to A1 Arrow A2 Arrow A3 Arrow this already sends a message to the reader that we are dealing with no longer um constants within uh Vector but rather vectors in a vector and uh what can we see here is that the dimension of this nested vector or which we can also refer to as a matrix here the number of rows so the number of entries this elements we can see it's equal to n but then this time the number of values that form these vectors is no longer one because we're are not dealing with just a constant this is not some constant but rather this is yet not Vector so let's assume this Vector has a length or M so let's say this has a length of M then the dimension of this Matrix a is equal to M so something that we will see also when talking about matrices so let me actually clarify this bit more for better understanding let's say we look into one of those um one uh one other example of an entry so let's say we look into this specific Vector which is in the U the Third uh Vector within this Vector capital A so this A3 Vector so one thing to see here already is that I assumed that these vectors they got M elements and keep in mind that all these vectors they should be of the same size so it means that I already know that this specific Vector A3 has M elements elements so M elements so I'm representing this uh H Vector from here I'm taking this out from this entire uh nested a vector and I just want to represent this and now unlike this elements that got an arrow on the top this time I will have uh constants forming the A3 Vector so I no longer have vectors but I have elements in it so in here I will have a a let me actually write down all the A's but to refer and to make sure that I recognize that I'm dealing with the thir a vector so A3 Arrow here I will put three Tre all the way here three so they all come from the same third a Tre Vector but then their positions is different because this is let's say uh one two and then all the way down to Ed position so this indices help us to keep track what are the um position that this values are taking part in the vector A3 Arrow this might seem bit complicated at the moment but once we move on onto bit more complex material like uh matrices it will make much more sense this is bit of an extra I just wanted to Showcase this but this is what uh is at its core and what you need to uh understand at the moment to understand this concept of vectors so you need to know that vectors can be represented by this arrow on the top so let's say Vector a and it has let's say n elements then you can write the square brackets and then you will need to mention A1 A2 all the way to a n which means that you have n different entries describing your vector so you have A1 which is the first element in your vector A2 the second element all the way to a n which is the end element but here you can see for instance so if I had here a Tre that uh A1 is simply equal to 1 A2 is equal to 2 A3 is equal to 3 all the way to a n is equal to 100 so these numbers I'm basically taking and I'm representing them I'm putting them in here within Square braces in order to get a representation of my Vector so my Vector a has all these different entries and different entries and it starts with one and it ends with 100 this is a vector and then when it comes to the vectors within vectors here we need to be bit more careful cuz here we not just have uh constant values forming a vector but we have vectors that form yet not vectors so our Vector a our nested Vector a which we uh later will refer as Matrix a has actually enter that also are vectors so we have A1 Vector A2 Vector A3 Vector they are not just constants but on their own they are vectors so here for instance we have defined also an example of it we have said let's look into this third specific Vector that is part of a which is a Tre uh vector and uh that one has M different elements here we have then the index referring to the which Vector from the ne Vector a it is which is the third one because we have taken it from here but then on its own this Vector has different members and different members to be more specific therefore we have also an index to keep track of the position of this value one to up to M and this can be yet another uh this time it can contain some elements an example of which is for instance zero one 2 all the way to let's say 500 and this can be different numbers it doesn't need to be ordered it doesn't need to have a specific pattern they can be just random numbers describing this A3 Vector so hopefully this makes sense if it doesn't don't worry because we are going to see this time and time again I just wanted to give you brief of an intro such that you can uh remember this when we come uh back to bit more uh complex topics like uh indexing in matrices so now let's talk about about special vectors and operations here we are going to talk about zero vectors unit vectors the concept of sparcity in vectors as well as vectors in higher Dimensions like we just saw about this n dimensional space we will also talk about different operations we can apply when it comes to vectors like uh addition subtraction and then later on in the next module we will also talk about multiplication we will also be looking into the properties of vector addition after we looked into some detailed examples when it comes to operations on vectors all right so let's start with the zero vectors and unit vectors when it comes to the zero vectors you can see here already that um the zero and arrow on the top it basically refers to the vector like we saw before only with the difference that all its members are zero so you can see here that we have zero and then an arrow and then underneath here we have some number tree and then this is described by this common representation with the square braces and then three different members Z 0 0 so all zero and then it says in R Tre okay so why are we doing this well uh when it comes to uh different linear Lal operation sometimes we just need to add zero uh vectors or we just want to create zero vectors it's just easier here to work with we want to uh just create an empty uh Vector we want we know the length but we want to keep it empty such later on we can add something on the top or knowing that when we add a zero on a number the number stays the same we can make use of this property to uh do different um uh tricks when it comes to programming in Python in color or in C++ Etc so therefore this idea of zero vector can become very handy now one thing that you need to notice here is that we are not just writing down this zero to emphasize we are dealing with a vector but like uh before we have this eror on the top emphasizing that we are dealing with a vector then what we are doing is that we are also adding the dimension of this Vector so in what dimension in what space are we um uh creating this zero Vector that this vector is located is it in R2 in RN in R3 in this specific case you can see that in this example the uh index that we got here is three which basically indicates we are dealing with a zero Vector in threedimensional space so in the R3 uh in general we would just note this by n keeping the uh notation general which means that we are dealing with 0 0 all the way down to 0 so it has n by one dimension in r n all right so this is about zero vectors it is just a way to uh make our programming life easier also to use it in different uh algorithms when it comes to bit more advanced algebra uh the next type of special vectors that we will look into is this unit vectors so vectors with a single element equal to one and all the other zero denoted as EI for the E unit Vector in N dimensions are referred by unit vectors so uh what we mean here when it comes to the unit vectors uh if we have for instance E1 it means that we have a vector where the is in this case the first element is equal to 1 so you can see that E1 is equal to 1 0 0 so in the first element we got one and the remaining is zero and this is really important that we are dealing with vectors that contain only elements of zeros and ones and the only member that is equal to the only element in that Vector that is equal to one is the E element in the entire Vector all the remaining ones are zero and you can see here that the dimension is no longer specified but just the um index of the entry where the um uh the uh one is located so let's look at another example in here for instance when it comes to the um uh unit Vector yet another unit Vector is E2 which basically means that in the second element so in the second place uh the uh Vector contains one and all the other members are zero so here you can see Zero here you can see Zero only in the second element we have one and then in the E3 what we have here is that the third element is one and all the other ones are zero so let's actually look into uh one um bigger Vector uh in higher Dimension to make it even more sense so first I will Define and assume that we are dealing with a vector in RN so in an N dimensional space this gives me an idea that we are dealing with um so we are not dealing with nested Vector we are dealing with a simple n dimensional Vector so it has n rows and one column so using the square braces I'm going to represent my Vector so I have all these different members n members C so e let's say it is E5 so what does this mean it means that I is equal to 5 and this e element so the fifth element is equal to one and all the other entries the elements in this Vector are zeros so let's look into this is 0 0 0 0 I'm approaching the fifth element in my Vector so it's this one this is one and the remaining all zeros so this is a unit Vector in an N dimensional space and I'm defining it by E5 because my fifth element is equal to one now those are very handy when it comes to some other uh techniques in linear algebra and just in general think about techniques like um uh row ulum form solving linear equation something that we will see as part of the next unit so many things um we can do by using unit vectors unit vectors are super important so you need to understand this concept uh very well such that later on you will understand uh more advanced concept CS in linear algebra now look into the topic of sparsity in vectors so by definition sparse Vector is characterized by having many of its entries as zero so its parcity pattern indicates the position of a non zero entries so uh what we are basically saying is that if we are dealing with a vector that contains too many zeros we are dealing with a sparse Vector so uh this sparsity pattern indicates uh also the positions of a nonzero elements so um if we have um unit Vector it means that we are already dealing with a spse uh Vector this is a concept that is super important when it comes to linear algebra but also in general data science machine learning and AI because having the spity your vector it means that you don't have much of an information usually a value is zero it means you don't know much about that specific value and if you got just too many of zeros and too few numbers which do um provide information it means that you are dealing with a vector that doesn't provide you much information and there's always a problem when it comes to data science machine learning and AI so sparcity is something that you need to be aware of you need to know how to recognize it and you also need to know whe there's a problem in your specific case or not so let's look into an example let's say we are dealing with this Vector X that has five different elements so X is a vector coming from um five dimensional space so we have for instance an element of three in the first entry then we have z0 in the second and third uh entries then we have an entry um four which coincidentally also contains value four and then the last element in our five dimensional that Vector X is equal to zero now what do we see here we see that the majority of elements of a vector X is equal to zero because we got in total five elements and then we got three of it actually uh being equal to zero and only two of them containing information like equal to three and four so only two elements that are not zero so nonzero elements it means that three / to 4 which is is basically 60% 60% of all the entries in the vector X are equal to zero so the 60% it means that is above half so above 50% 60% of all the information in this Vector um the majority is simply equal to zero this type of vectors we are uh calling sparse vectors and sparsity is really important concept uh that we need to keep in mind later on so uh while we can visualize vectors in two and three dimensions in linear algebra like we just saw in case of this n dimensional vectors visualizing uh the this type of higher dimensional vectors becomes very difficult so uh this mathematical flexibility uh to work with uh this type of uh information so when we can represent uh information many with many entries we can represent it by vectors which we can actually not visualize becomes very handy for complex data structures for different simulations in physics and much more so uh we just saw in couple of examples uh how we can represent vectors in a high dimensional space using this Square braces and this common Vector notation representation we saw that in an N dimensional space we could uh very easily represent this uh very large Matrix or vectors uh by just um using this Vector representation for instance if we got a vector that had n different entries where n is for instance thousand so let's say we have thousand then uh we can represent uh this uh vector or this information by using common Vector notation so A1 A2 all the way to a th000 so of course we cannot visual ize this it just doesn't make sense we can visualize two dimensional vectors we can visualize three dimensional vectors but we cannot uh visualize thousand dimensional vectors so Vector that comes from uh r Thousand but what we can do is still make use of this very useful information in order to uh do different operations when and later on we will see that uh this property as specifically this part of linear algebra it helps us to work with vectors in any number of Dimensions whether thousands million billions this mathematical flexibility is super important for more complex data structures uh for metrix multiplications when for instance we are doing different uh algorithms including how we can represent a very large matrices very large feature spaces all this different information we can represent just by making use of vectors coming from this specific uh part of linear algebra let's now finish off this module by looking into some applications of vectors so one common application of making use of vectors is uh when we are performing different operations while having words and we want to count those words so this is a super common application of vectors and we can account this words and can even plot a histogram of how often each of these words appear in a document so a vector of a length n for instance can represent the number of times each of these words in a dictionary of n words appears in a document so uh just for the sake of Simplicity let's assume that we got um dictionary that contains only three words of course in the reality the um dictionary what we also refer often as Cor purpose it contains much many uh much more many words but for the Simplicity we will assume that we just got three different words in our dictionary so that's a total now let's assume that we got a document uh with these different words and we want to count how many times each of those words that we got in dictionary actually appear in our document so um if our document is described by this Vector so it contains an entry of 25 2 and zero it means that in our document we got 25 word one in our from our dictionary so in the position one two * word two and zero times word three so basically we have a predetermined set of words in our dictionary in this case three words word one word two and word three and they have a specific index specific position in our vector and when we are putting these values in here then the machine or the uh computer the program will understand that if we have 25 in the first position then the word one in the dictionary appeared 25 times in our document whereas the second word appeared only two times and the last word word three didn't appear at all so zero times times in the entire document so let's look into a practical example actually to make even more sense so um this is by the way a common practice to count different variations of a word they are common application in engrams large language models Transformers they are just a Cornerstone of many language models when we want to count the words in the document to understand how often the word appears because this gives us a idea what this document is about knowing how many time the same word appears in that uh document it gives us an indication of the topic of uh the do document also we can make use of it to do sentiment now to understand what this document is about not only in terms of the topic but also is it a positive is it a natural or a negative uh document so to say so uh for instance if we got uh the following words uh that correspond to our dictionary and in our dictionary we got just um let's say six different words then what we can do is that we can say 3 2 one let's say 0 4 2 and the corresponding words are word row number horse e and then document what this means is that we have a text what we refer as a document that contains three times the word word that contains two time the word row contains one time the word number zero times the word horse and four times the word eel and two times the word document so uh this is basically a common way representing the uh frequency of the words in the document let me actually give you uh another example and in here I want to emphasize another thing the concept of stop words so uh let's say I make this 10 and then here I say there is a three times the word I two times the word uh reading two times the word library four times the word book zero times the word shower and 10 times the word uh so uh you can see a that in here we are dealing with a document that contains 10 times the word uh which is what something that we refer as a stop word so those are things that actually don't give us too much information about what the document is about because uh it's just used everywhere but it is appearing too often so you can see 10 times the most frequently appearing word this is what we refer as a toop word and then another thing that we can observe the second thing we can observe is that we are dealing most likely with a document that describes library reading uh because you see the words like reading you see the word like book library but another word shower that is totally unrelated to reading book or library is appearing zero times so even by looking at discounts we can already get an idea what a topic of this document is about so uh you can see already now from this very basic example where I made too many assumptions regarding how small the the uh dictionary should be uh you can even see now how we can use this counts in our dictionary from our text in order to get idea about the topic of the document or topic of the conversation it can be a topic of the uh tweets if you have a tweet data it can be topic uh regarding book if you have many book um uh book text it can be for instance the topic of of the review if you got uh product reviews from uh Amazon for instance using this count can help you to get a topic regarding topic from that text then you can also use it to remove the stop words because usually the stop words are the most frequently appearing words it can also give you an idea about the sentiment for instance here we are dealing with natural sentiment it's not positive it's not negative it's just reading a book in library that kind of topic so all this can be super helpful when it comes to natural language processing that's a field where this uh text processing text cing and then using that for modeling purposes is what uh what plays a central role it also plays a super important Ro in the large language models in the Transformer models and uh in the simple matters like uh back of words or uh in the uh tfidf all this they are based on this idea of counting words and how we can use this information and you can see how vectors come into play in this different applications of linear algebra in data science natural language processing in artificial intelligence in machine learning so they are super important another application of vectors can be representing customer purchases for example an N Vector p so let's say p can record a customer purchases over time with pi being the quantity or dollar value of an item I now what does this mean so let's say we have Vector P that represents the customer purchases and we are dealing with a single customer and we are just saving over time that information how many time this customer has made purchases over time so the quantity is in the um dollars so the dollar value of item I purchase so we are basically keeping track of what is the value of the item I that the customer has purchased so what we can do is we can assume that in here actually it already Mak that assumption it says n Vector which means that the number of rows or number of um items that the C Custer purchases is n now what the um the problem says that it represents is that in each entry and here we have in total n entries we got a dollar value of item I which means that here if I have P1 P2 all the way to PN and here somewhere in the middle I got Pi in the east position it means Pi represents the value of item I so for example if I'm dealing with a CER that buys um let's say uh courses and uh the first item that the customer is buying is a mathematics course so I'm writing mathematics course and this is the first course that it buys e is by the way just a um way to refer to the E purchase so let's say um here somewhere in the middle the um customer decides to buy a deep learning course deep learning learning course and then it continues by buying uh the customer continues buying courses and the last course that the customer buys is let's say um career coaching course now let's say the mathematics course cost uh around $1,000 let's say the uh deep learning course costs $3,000 and then let's say the career coaching service which is usually one of the most applied and personalized one can cost all the way to $5,000 now we see that in the East position this is the East position let me change the color by the way so let's say this is the East position this is the first position and this is the last position so those are just indices we can see that in the East position we got the 3,000 which means that the p e is equal to $3,000 so this indicates that in the East purchase the customer purchased deep learning course and the value of that item was equal to $3,000 all right so now we are ready to go on to next major topic which is about vector addition and subtraction so we are going to do some operations and apply this operations to vectors so let's first formally Define this ideal of vector addition so uh two vectors of the same size are added by adding their corresponding elements the result is a vector of the same size so uh let's unpack this it says two vectors of the same size are added by their corresponding elements so here it refers to two different vectors let's say vector v and Vector W and it says let's add them what we refer as vector addition and says for that what we need to do is to take all the elements of v and then all the elements of w and using their corresponding elements so indices that helps us to understand where those elements are located we are using in order to add each element in the vector v to the element of the vector W in the same position and do note that in the second part it says the result is a vector of the same size because we are adding two different vectors of the same size mentioning here it means if we add two different vectors to the same uh that have the same size we are going to end up with a vector that has the same size now once I go into to the examples it will make much more sense let's quickly also look into this concept of subtraction so on its own a substraction is very similar to this idea of addition so if we have a substraction let's say we have vector v We substract Vector W then we are doing basically uh what we just did to the addition only instead of uh doing add we are doing substract so again we are just uh we are just substracting from vector v v Vector W they have the same size so we end up having the result which is a vector of the same size only one thing that you can see is that this can be also re written as V Vector plus and then minus W so we basically can represent substraction um on its own as a way of adding only we take the negative so the um opposite directed back Vector so this will make even much more sense uh once we go on to the examples so let's look into our first operation example where we are adding two different vectors this a basic example we got just two dimensional two vectors we got Vector a that has entries 2 three and Vector B that has entries 1 4 and what we are doing is that we are adding Vector a to Vector B we just learned that a we need to have the same size of vectors so you can see that Vector a has a dimension 2x one vector B has a dimension of 2x one so their sizes is the same both they got two entries only two elements and at the same time we just learned that what we need to do is to take their corresponding elements and add them to each other now what does this mean it means that we take from a the first element to and then we take the first element of the second Vector which is the B so we take the two from here and one from here the first element of a and the first element of B and then we are adding them to each other 2 + one is equal to three and then the same holds for the second and Tre so three which is the second element of vector a and then 4 which is the second element of vector B we are saying 3 + 4 is = to 7 so let me write it down even in a simpler manner such that it will make much more sense so Vector a has elements to three in the first element we got two in the second element we got three so a then we want to add B which has in the first element element equal to 1 and the second element is equal to four this means that if we want to add this vectors 2 3 + 1 4 this is equal to we need to take two we need to add one so this element and this element and then we need to take three we need to add to four so this one and this one which is equal to 2 + 1 is equal to 3 3 + 4 is equal to 7 so we got a vector 37 do note that this Vector the result Vector contains again two elements and just one column so 2 by 1 so you notice that the sign that the size is the same of this result Vector now let's actually generalize this concept before moving on to the next example so if we got let's say Vector a that contains n elements A1 A2 all the way down to a n and it is from n dimensional space and we got Vector B that also has an element so remember that they both need to have the same size so B1 B2 all the way to b n so they come also so B comes also from n dimensional space so then when we add a to B this is equal to A1 A2 all the way to a n plus B1 B2 all the way to BN so n by 1 n by one the sizes this is equal to let me actually use this color to make it even more visible so I for the first entry for my result Vector I will get A1 + B1 then A2 + B2 so all the way down onto the end element which is a n plus let me use a different color A1 B1 B2 b n so you can notice now in general terms what we are doing here so we are taking the A1 coming from the vector a we are adding in the same uh position the value that comes from Vector B which is B1 we are saying take the a1+ B1 this is the uh first element so the position stays the same and then in the result Vector so we take all the corresponding values that are have the same position in the corresponding Vector first from Vector a and then Vector B we are adding them and this forms our new vector and this new Vector will again have a size n by one so you can see that a the sizes of the two vectors are the same both have n elements and then we are using their corresponding elements to add them to each other element wise and then we are getting the result that has the same size so n by 1 so this is a more General description of how you can add two vectors let's now look into this specific example so we have a vector with the entry 073 so this comes from R3 you can see so three dimensional vectors the second Vector is 1 2 0 and then the final result is 1 193 so how we got this we took 0 we added one 7 we added two and then three we added zero so you can see all these elements element Y and then this is equal to 0 + 1 is 1 7 + 2 is 9 and then 3 + 0 is 3 exactly what we got here so again the same sizes and the result is from the same size so quite straightforward now when it comes to the vector substruction what are we doing that um so what are we doing here so we are doing kind of very similar thing we are taking this element one we are subtracting the other one in this first element then we are taking the nine in the second position and subtracting this again from the second position of the second vector and we are putting in here 1 and then 1 1 is equal to 0 9 y 1 is equal to 8 so we get result Factor 08 like in here and you can you can see that the sizes stay the same so also in this case let's write more General um this ideal of subtraction if we got a vector a from RN so n dimensional space and it can be represented by A1 A2 all the way down to a n so it has n elements n by one and then we got B also for from RN so coming from the uh n dimensional space which means that it got n elements so B1 B2 all the way down to BN again with the same size n by one then a minus B is simply equal to a A1 let me actually use the same colors to make it easier to follow so let me first draw my Square races and then here I will use blue for a and then red for the uh color for second Vector which is B here I will use black minus then given that the same size should be for the result Vector I already know that I expect n different elements for this and then here I'm taking this first element that comes from Vector a i subtracting from this the first element that comes from Vector B so element y subtraction B1 and I'm already getting the result for the first element in my result Vector so you can see A1 minus B1 I'm taking this element and this element and subtracting them from each other to get A1 minus B1 and then the same holds for all the other values only coming from different elements from Vector a subtracting from this the corresponding values element Wise from the vector B so B2 B3 all the way to a n so you can see that in my result Vector a vector minus B Vector in the first element I get A1 B1 then A2 B2 then A3 B3 in the third element all the way down to the end element which is equal to oh this should be b a n minus BN so um this already should makes uh much more sense so every time we take the element in the same position from one vector than the other we subtract from each other in order to get the corresponding element in the final Vector all right so let's now uh before moving on to the properties um I wanted to show you um this only in a coordinate space so what this means in terms of visualization in a coordinate space so uh let's say we have a coordinate space this is my Y axis this is my x axis so this is X and the Y and this is my Center so 0 0 and what I'm doing here is simply I want to have Vector a let's say this is just um Vector a simple one with the coordinates um let's say four and minus 2 and I got Vector B let me use a different color Vector B that has coordinates let's say minus four and four so let's actually visualize them let's first start with the vector a uh which has a x value of four three and 4 1 2 3 and 4 and the Y value 2 so this is my Vector a and let's now visualize the vector B so minus 4 and 4 which means that let me actually extend this this is min 4 so the x coordinate is min 4 so it should be here and then the y coordinate is four so 1 2 3 and 4 it's this one which means that my Vector B is this one all right so you can can see now that the vector a is in here and the vector B is in here now what I want to do is to add these two vectors to each other so what I want to do is to take this Vector a and add to this the vector B which is is equal to 4 4 was 0 and then 2 + 4 is equal to 2 so 0er and then two it is zero and two two so this is my result Vector so now when we are clear on how we can add vectors how we can perform these different operations and what it means in practice when it comes to looking at the vectors in a cordant space and adding them or subtracting them we are ready to look into the properties of vector additions so this is something that will definitely seem familiar to you uh from prealgebra where we are basically using all these properties that we already know that holds for uh numeric values for the scalers that being transferred to this Vector space so we are going to talk about this four different properties that the vectors have the first one is the cumulative property which says that if we add a vector a to Vector B then this is the same as adding a vector B to Vector a so basically the order of the vectors doesn't really matter when it comes down to adding them so formally a plus b is equal to B + a for any vectors A and B of the same size then we have associative property which says A + B + C is equal to a + b + C we can write both us A + B + C now what does this mean we know from prealgebra that this parenthesis means first do this addition and then do the rest of operations in here it basically says if you add add a to the B first and then you add the C is the same as first you add B to the C and then on the top of that you add the vector a so then the third property is addition of zero vectors which says if we add a zero Vector to Vector a then this is equal to adding a vector zero to a and this is equal to Vector a so adding the zero Vector has basically no impact act on the vector whatsoever then the final property is subtracting a vector from itself which means if we take the vector we subtract the same Vector from itself so a minus a and we get a zero Vector so a minus a is equal to zero vector and this heals the zero Vector now let's look into each of those properties one by one and let's uh look into specific examples uh in some cases we will prove this on the example that we have to make this Concepts much more clear so let's start with this cumulative property of vector additions so we want to see whether a plus b is equal to B + a so let's say we have a vector a that has coordinates or magnitude and direction that is equal to one and two then we have a vector uh let's say B that has a magnitude and direction of Min 2 and three so the first thing that we want to check is indeed whether the A + B is equal to B + a so therefore let's first calculate this part and then we will calculate this part that I will Define by one and two and we will see whether we are indeed having the same value the same vector or not so let's see so we have here a so A + B which is the first value that we want to calculate a + b is equal to 1 2 + 23 and we learned before that this is simply equal to take this value and then add this one so 1 + 2 and then 2 + 3 so this gives us a Vector 1 2 is = 1 and 2 + 3 is = 5 so we get that A + B is equal to 5 this Vector now let's look at the second quantity so B Vector B plus Vector a this is equal to 2 3 + 1 2 and this is equal 2 2 + 1 and then 3 + 2 this gives us 2 + 1 is = to 1 and 3 + 2 is = to 5 so we can already see from here that the quantity one is indeed equal to quantity 2 which proves that indeed the A + B is equal to B + a what this basically means is that adding two different vectors the direction or the order is not important whether you add a on the top of the b or B to a it doesn't M at the end is the same and actually you can also see it if you uh combine this or if you do this in a more general terms so let's say if we have a vector a which is equal to in an N dimensional space A1 A2 up to a n so it has n by one dimension and you have a vector B with the same size from the same RN dimension and it has elements B1 B2 up to BN and the dimension is equal to n by one then if we calculate first A+ B and this is equal to Simply A1 + B1 A2 + B2 up to a n + BN and if you calculate the second uh amount which is B plus a and this is equal to B1 + A1 B2 + A2 up to bn+ a n you can see that A1 + B1 is equal to B1 + A1 simply from prealgebra you know that if those are all constants for inance 2 + 3 is = to 3 + 2 in the same way a 2 + B2 is equal to B2 + A2 and then here up to a n + BN is equal to BN + a n what this means is that all these elements they are basically the same which means that we already have a proof so we get this proof and we can see that even for the general term independent what this Vector a is what this Vector B is that a + b is equal to B plus a this is exactly what we saw before in the first property which is called commutative property of the vectors that A+ B is equal to B + a now let's move on to the other property which is called associative property of the vectors now what this property does and says is that a plus b so first we do this plus C is equal to a + b + C and this is then equal to a + b plus C now let's then see um this specific property on an actual example so what is basically says is that if we have this example where a is equal to actually I had this before let me simply just remove this part let's then add our third Vector which is C and let's call it let's say it has a representation of four and five then the idea behind this property is that what we need to prove here that a plus b within the parentheses plus C is equal to a plus B+ C and then this is equal to a + b plus C so let's see actually whether this is indeed true for this specific case now this should come very intuitive so I'm going to do it very quickly so first we have this quantity this one then we have this one and the third one let's do it very quickly so A + B plus C is equal to one2 plus and then we at C so it is simply 4 five and then this is equal to we saw before when doing this that we were getting 1 2 2 + three and then we add this four five this is simply equal 2 1 2 is 1 and 2 + 3 is 5 + 4 5 now given that it doesn't really matter no longer that do we have uh here parenthesis or not this basically means that this volue is simply equal to 1 + 4 so here 1 + 4 here 5 + 5 so this is then equal to 3 and then 10 all right let's then now quickly do the second amount which says first add the vector B to Vector C and only then add the vector a on the top what this means is that we need to take one two this is Vector a and and we will only add this once we have added the minus 23 the vector B plus to the vector 4 5 okay so we can see that we are just leaving this in here let's first add this two 2 + 4 3 + 5 so this gives us one two plus 2 + 4 is uh 2 and then 3 + 5 is 8 so this gives us let me remove this calculations so this gives us 1 + 2 is = to three and then 2 + 8 is = to 10 okay great so now we got already the quantity one being equal to quantity two let's check whether this is all equal to this one it should already be um something that you see now given that um we know just from mathematics that parenthesis doesn't really matter when it comes to the scalers and adding two vectors is basically very close to this idea of edited property um of the edited property of the scalers but just let's quickly do it to be 100% sure so when we take this Vector a to the B and to the C we had all this this is equal to one two added to minus 2 three and then added this to 4 and five now what this is equal to let me actually write this in bit shorter way such that it can be all fit in in the small place so 1 two + 2 3 plus 45 this is equal to basically 1 2 + 4 and then 2 + 3 + 5 now what is this number 1 2 + 4 is simply equal to 1 2 is = to 1 and then + 4 is = to 3 so first element is 3 2 + 3 + 5 is = to 5 + 5 which is equal to 10 perfect so now we get the confirmation thatting the A + B + C is = to A + B + C is = to A + B + C so let's quickly also look into this addition of zero vector and the substracting a vector from itself properties and uh the detailed explanation of this or example of this I will leave it to you so when it comes to this a plus um 0 is equal to 0 + a is equal to a so this property let's say if a is equal to this 23 and then we are adding on this a plus some zero Vector which basically means take two tree and then added the same size of zero Vector you can see that this is the same as adding this zeros on these values now what do we get we get that this is equal to 2 + 0 is 2 and then 3 + 0 is 3 there we go so we already see very quickly that it doesn't really matter whether we add a zero Vector to this original a vector or not we in all cases it just adding a zero Vector has no effect and seeing from the commutative property that a plus b is equal to B+ a we already know that if um a + 0 is equal to uh a and is equal to this then also 0 + a will be the same and we can see indeed that we just saw that a + 0 is simply equal to a so we basically have quickly proven all this now when it comes to the subtracting Vector from itself I think this is a very nice one just to see how we um uh take the same vector and subtract from that value and we get zero and this is very similar to working with just real numbers in the same way as 3 3 is equal to Z also when we have a vector consisting of the scalers like a is equal 2 23 in the same manner if we take this a and we subtract it from itself so a minus a then what we will get is 23 minus 23 and this will give us 2 2 is z and then 3 3 is z so we get a vector zero so zero Vector so now when we are clear on how we can perform different operations on our vectors and also we know uh what are the properties of uh adding and subtracting uh different vectors we are ready to move on to a bit more advanced topics so uh in this module we are going to discuss this idea of scalar multiplication we're going to look into the example how what happens and how we can do the uh Vector multiplication with the scaler then we are going to uh look into the Spen of vectors what it means to have a Spen of vectors uh what is the linear combination and the relationship between the span and linear combination and the unit vectors then we are going to look into the application of scalar Vector multiplication in audio scaling uh example and then finally we are going to finish off this module by looking into the length of a vector and a DOT product and we are going to uh go back to this idea of distance understanding vector magnitude and understanding Vector length so let's get started it now before we look into this idea of span and linear combination I quickly wanted to look into this idea of scalar multiplication and the um specific definition of it so formally the scalar multiplication involves multiplying each component of a vector by scalar value effectively scaling the vector's magnitude so what do I mean here let's say we have a vector and I will write it in the general terms to keep everything General so let's say we have a vector a let me pick up my pen a and this Vector a is from n dimensional space so it is from RN and it can be represented by A1 A2 up to a n and I have this magnitude um of a vector and now I want to scale this uh Vector for which I know the magnitude and the dire diretion I want to scale it with a scaler and we learned before that the scaler is just a number so um scaler in this case I will be uh referring it to uh by C so c will be my scaler and uh this comes from R which means that it's a real number let me actually use a different color to make it easier to follow okay so my scaler will be with the color uh red so C and C comes from R so what do I mean by scalar multiplication I mean that I want to find what is this c times a this is what we mean by scalar multiplying with Vector now what does this definition say it says when we are multiplying in scalar with Vector so the scalar multiplication meaning multiplying Vector with the scalar it involves multiplying each component of a vector by a scalar value so if we translate it to this specific example it means that this amount so this amount is equal to taking C and multiplying it with each element of this Vector so each component of a vector and what are the components of my Vector the A1 A2 a up to the point of a n so all these components so that means that the first element of this new Vector the scalar multiplication result will be C * A1 C * h two dot dot dot so all this middle elements and at the end again c times and then a n and then in both cases of course the number of elements doesn't change so the so the number of rows of my Vector doesn't change it's n so here also n and then number of columns is the same so it's just a column Vector so one column so what we see here is the that we go from A1 to C * A1 we go from A2 to C C * A2 up to the N transforms into C * a n so we see very easily that I keep all the elements from this Vector I take them in here and instead what I'm doing is that I'm multiplying every element from this vector by the scaler C so this is exactly what this definition says and let's actually go ahead and do a HandsOn example with some real numbers to have this um method and to have this uh definition very clear in our mind because we are going to make use of this fundamental operation scalar multiplication on and on in the upcoming lectures and just in general in your journey in any applied sciences so this is an example of scalar multiplication uh here here what we are doing is that we want to multiply this Vector C so in this case the vector is defined by a letter C and then on the top we can see the arrow indicating that this is the vector now and here we refer the scalar by a letter K we are saying we want to perform scalar multiplication which means that we want to multiply the uh Vector C by the scaler K so how we can do that so what we want is to multiply K by C and we just learned that for that what we need to do let me write this over so this equal to minus 2 multiplied by 4 3 this is my Vector so this is the K and this is the C this is equal 2 so I take my scalar and I multiply it with the each of the element of the C so 2 * 4 and then 2 * 3 so 2 * 4 is = to 8 and then 2 * 3 so it goes away it becomes a plus and the 2 * 3 is 6 so my end result the K * C is equal to minus 8 6 this is my final result so let's quickly also do yet another example and this one is a unique one because it's relating to this idea of U multiplying something with a zero uh which is something that we also uh know from our high school that when we multiply number let's say seven by zero they're getting zero and here in this example the uh problem is describe the effect of a scalar multiplication by zero on any vector VOR which means what we are doing is that in this example is we want to know what is this result of any Vector let's say Vector uh C so we will use the same example C only this time instead of multiplying it with the scalar k equal to minus 2 our scalar will be zero which means that c is equal to 4 3 and then K is now equal to zero and we want to find out what is this K * C let me actually write down the K with a different color k is equal to zero so what we want to find out is K * and then C and this is that equal to zero so I'm taking the K 0 times then I'm taking each of the elements of C which is 4 and then minus 3 and I know that when multiplying the number with a zero it gives me zero which means that I end up with 0 here 0 * 4 is 0 0 * 3 is also zero so I end up with a zero Vector now this gives me an idea already that I can make a general conclusion that independent of the type of Vector that I have independent what are these values in my C uh if I have any Vector C and I'm multiplying it with zero then this will always give me a vector of zero because all the members of this final Vector will be just zeros so if for instance the C comes from uh let's say r n so it has n different elements it comes from uh n dimensional space then my final result of0 * C so this zero Vector this one so zero that this one will come also from our end so you will be having a vector so 0 * c will then be equal to 0 0 blah blah blah blah zero so n time zeros so this is then the idea of multiplying so scaling a vector with zero and this is our example to all right so let's now move on on to our application of scalar vual multiplication and then after this we will go back to this idea of linear combinations and dispense so in this specific application we have a scalar Vector multiplication and we are looking into application of audio scaling so the scalar Vector multiplication audio processing this can change the volume for instance of an audio signal without altering its content so um you might have noticed that um when uh when you are listening to video you can simply increase the volume of that video or decrease it but you will notice that the content doesn't change you are just increasing the volume or decreasing it even on the TV when you are watching a show you are increasing ining The Voice or decreasing now what you're basically doing behind and this is super interesting is that behind the scenes what is happening is that there is simply um audio that um contains that show and audio of that show is being multiplied with a scaler and that scale is simply the volume scale if you scale it in such way that you want to decrease the volume so the audio will then have uh lower volume then you are simply multiplying it uh your vector containing the audio information in such way that those newer volume indications they will be they will be containing lower numbers hope this makes sense let's look into the example this make uh this will definitely clear this out so um let's assume we have an a vector a that represents the audio signal and we want to m multiply Vector a by scalar B to adjust the volume so B is some sort of number it can be so B comes from our so is a real number while a is simply a vector given that it doesn't mention it here I'm assuming that a comes from RN so it comes from r n dimensional space so imagine of a as this Vector A1 HQ blah blah blah blah up to a n and each of these values it basically describes uh an the audio signal so it represents um uh an amount so it contains an amount that represents the audio signal of your uh video or uh your uh show and then the b in this case for instance in this example you can see that the B is then uh equal to for instance 1.2 1 2 or B is equal to 1 / 2 so you can see that b is = to 1 / 2 which basically is offensive saying that b is equal to 0.5 or B can be equal to min1 / 2 which is 0.5 now then it says then the B * a which basically means multiplying our um scalar beta by the vector containing the audio signal a so this B * a is perceived as the same audio signal but at the lower volume now why lower because you can see that b is equal to 0.5 or Min 0.5 it means that once you take all these elements of your a and you multiply it with the number that is smaller than one in this case 0.5 then all these numbers will decrease which means that also your audio volume will decrease so let me actually uh show you an example so let's say our talk show is very short and you know the audio variation is very low you have a vector a that is quite small it comes from a three diim menure space so R Tre and it has numbers like three uh six and then five so 3x1 vector and then we have our audio adjustment scalar beta which is equal to 0.5 now when we take the beta we're multiply it by our audio signal then what we do times this clear so times what we are doing is that we are simply taking all the elements of our a so three 6 and 5 and what we are doing is that we are multiplying it by 0.5 0.5 and 0.5 or you can also say 1 / 2 so what this is equal is that 3 * 0.5 is 1.5 6 * 0.5 is 3 and then 5 * 0.5 is 2.5 and you can see that all these numbers 1.5 3 and 2.5 they are smaller and specifically two times less than all the original values in the um original audio so original audio is a which was 3 6 and 5 and the new audio the the scaled one is so audio scaled so B * a is equal to 1.5 3 and 2.5 so you can clearly see this transform where uh this element three is larger than 1.5 6 is larger than three and then the last element five is larger than 2.5 which means that this audio audio is much at a higher volume so the volume two times higher than this audio so this is basically the idea of uh applying scalar multiplication to our audio preprocessing I will leave the other example to you that will show that when your scaler is equal to minus 0.5 you again will end up with a lower volume only that time the volume will be much much lower than the original one so now that we know how we can perform scal multiplication in theory as well as we have looked into an example how we can do it in terms of the numbers and multiplying them and we have also seen uh applying still multiplication in practice uh so we have seen in this audio processing stage the uh multiplication process we are ready to look into the visualization of it this will help us to get a better understanding on uh what exactly happens when we are scaling different vectors let's look actually in the following example so let's assume we have a vector oh let me remove that so let's assum we have a vector and that Vector is let me get a color this one for instance a vector a and this Vector a consists of elements one and two so where does this Vector lie the vector is with um one so here in our coordinate system this is our xaxis this is our y AIS and here we got uh let me actually pick another color let's say black one and then we got one and then two right this is two this is one so it is this one so the line that we get here it is this one so this is our Vector a now let's assume I want to multiply my Vector a so I want to scale my Vector a by a constant Tree by scaler tree so I have a scaler let's say I call K and this k a different number let's say k is equal to three so what I wanted to do is to perform a scal multiplication so I want to obtain K multiplied by a and we learned that this is simply = 2 three times and then one two and then this is equal to 3 * 1 3 * 2 which is equal to 3 and then six so let's also visualize the scaled uh Vector so let me pick this yellow color this will be our scaled Vector so we have that scale multiplication and we are going to visualize that so we have three and six so this is three 1 2 three and this is six so we have this point so you should already see what is going on here so we got 3 a here so you can see that this part is our Vector a and this longer one is 3 a and even visually you can see that this longer Vector is simply the three times of the shorter Vector so we got this and then if you add on the top of this the same three times you will then end up with the original so sced version of that so basically this is a this is a this is a we combine three different so we scale a three times and we simply get a three times longer version with the same direction so you can see that when we are scaling even visually it makes sense so we are scaling our Vector a three times and we are just getting that Vector so we are transforming let me remove this so basically we are taking this vector and we are scaling it up to this point if I would do it only two times then it would be something like this or one and half times it would be something like this so only half of it so now this should make much more sense let us actually do yet another example to uh make sure that we are clear on these visualizations because we are going to make use of it when uh looking into this idea of linear combination in a Spen so let's say we have a vector B and this Vector B has elements Z and three so let's visualize and uh plot this Vector so it contains elements Z and 0er and three so0 and three so this is the X element and the Y element on the Y AIS we can see this is three which means that our Vector B is this Vector all right perfect so this is our B let's now multiply so scale our Vector B by scalar 2 so let's say we want to get 2 times B so what is this amount this is equal to 2 * 2 * and I'm simply taking each of those elements 0o and then three so this is then equal to 2 * 0 is 0 and then 2 * 3 is equal to 6 so this is my new SK fed Vector 2 * B Vector this one so let's visualize this the xaxis value is zero so we are still here and then the Y AIS value is sixth so what is sixth this thing all right so you already should see that this is very similar what we had before so this is 2 B all right so this all uh should make sense uh also we learned as part of the um High School when visualizing different plots so this is quite similar to this idea of having y isal to X and then scaling it getting like Y is equal to 2x so in this case only we know exactly where the vector starts and ends uh so we have a much more specific definition instead of having all this infinite number of points on the line but the idea stays the same so we are taking this vector and we are then scaling it two times so we get 2 B vector and I could do the same only instead what I could also do is I could do like uh 0.5 or 1 / 2 * B so I take the half of it which means I would get this vector or I could multiply it with minus one so minus 1 * B so I was skill with minus one and then I will simply get the negative version of my original Vector so this thing this would be minus b or Min 1 * B so this is basically the idea of uh scal multiplication when visualizing it in our coordinate system cian cordinate system and now when we know all this we are ready to move on on this idea of linear combination so let's now talk about another super important topic which is the dot product and it's applications so we are going to talk about the dot product and we are going to uh make this relation between the dot product and the distance something that we have learned as part of the high school so the dot product we are going to understand it we are going to plotted the vectors we are going to perform this dot product in examples as well as we are going to to look into the properties of the dot product and the inner product concept we are going to see the relationship between dot product and inner product we're going to talk about the cou squares inequality and the geometric interpretation of it so those are really important Concepts um the CI SARS inequality um the intuition of it as well as the implication of it um specifically in machine learning and then we are going to talk about the vector triangle inequality and uh in what cases also the norm is equal to zero and finally we're going to define the angle between vectors so when it comes to the dot product uh the dot product is also known as the scalar product it's a fundamental operation in linear algebra it combines the two vectors to produce a scalar it's a way to measure how much one vector extends in the direction of the one so providing insights into the geometric and the algebraic properties of the vectors so when we are using this operation of the dot product you can see it as an operation that we apply to vectors and in this way once we look into the geometric uh side of it so we will visualize the vectors and we will see the geometric insights behind it it will all make much more sense because this will relate back to our geometric interpretation this idea of Pythagorean theorem because they are highly related this idea of distance and how by using the dot product we can relate all these different concepts so let's first formally um Define the idea of uh dot product so the uh dot product of a vector v with itself gives the square of the length of V so earlier when we were looking into the prerequisites of the course I did briefly mention that we need to know this ideal length of a vector or length of a line we said that we denoted this by this concept or by this notation of two straight lines and then the name of the vector and then another two straight lines and this is what we refer as the length of vector so the distance between two points or the starting point and the end point point where our Vector is described so if we have for instance this Vector then if we take this point and we take this point then we can use that in order to calculate the distance of this vector and the distance of this Vector is defined by this notation so that's exactly what we have here too so you can see that we are saying that the dot product is actually this V so we take the vector we multiply with it itself we are seeing this is the dotproduct of a vector and it is equal to the square of the distance or the length of the vector so the uh in the left hand side we have this notation of the dot product we just use this dot as usual with any multiplication or any product and we are saying this is equal to and we take the length of a vector v and we Square it and this gives us our DOT product very soon we will see when uh applying it on a uh more general terms what this IDE of dot product is when we apply it to vectors just in general so in a two dimensional space when we have each Vector represented by two numbers let's say vector v which is equal to X and Y so we have on the first element the uh x coordinate of our Vector in our c coordinate system and the second element so this one is the Y so from y AIS on our Cartesian coordinate system then the dot product and the length are highly related as usual so we have here V the vector and then again so dotproduct of vector v is equal to x² + y s so this is the length of this vector and it is equal to then the square of this length so uh let's actually plot this to see it in our coordinate system such that we can relate this back to the Pythagorean theorem and we can see how it is possible that the dot product the V * V which is equal to the square of the distance is actually equal to x² + Y2 so let's assume we have this vector v and this vector v is represented by this x and the Y so we are in the R2 so this is our xaxis this is our y AIS and in here our vector v can be represented and I'm just taking random some random point and I'm saying this is my X and this is my y then this is simply the vector v so this X and Y this can be random numbers I just want to keep it General and to make it very similar to the definition what we just saw so we are going to relate this back to the Pythagorean theorem and we are going to prove how Pythagorean theorem is related to this vector and that the dotproduct of this Vector so V * V is actually equal 2 to the square of the distance of this vector v and then on its own this is then equal 2 x² + Y 2 all right so the first thing that we can do is to check and use a Pythagorean theorem in order to prove that the length of this vector and uh so the distance from this point to this point is actually equal to x² + Y 2 and to be more specific square root of x² + y s so when we look in here one thing that we can see is that this side which is the side that we can have here in order to form our triangle so I want to take the beginning uh where my Vector starts and I want to see what is that that we have in here so this side given that this is zero and this is X this means that this entire distance is just X and of course the other way around holdes as well when we look at the horizontal side this length so this is the y coordinate of this point which is y so we see here that it is equal to this part so we see that we are dealing with um triangle where so this is our vector v and we can see that this this side is X and then this side is y now we also know that this angle is 90° using the Pythagorean theorem which states that in this type of triangles when it is a right triangle and we have X and Y as the sides we can compute the opposite side so the distance or the length of this side which is let's call it so in this case we have already the vector so we can say that the length of this Vector this is how we were defining the length of the vector this is then the square of it according to the Pythagorean theorem is equal to x² + y s because the pagoria theorem says if this is a 90° angle here we have a here we have B then this side which is defined by C so c² is equal to a² + b² and here the C is basically our vector and we are seeing the length of this Vector so the length of this Vector is basically equal to C from here to make it easier to uh to compare the two so this means that using the Pythagorean theorem we can already find that the length of this Vector is equal to the square roof of the x² + y² knowing that the vector has those coordinates X and Y okay so now when we are clear on uh what the norm is let's actually look into what we have here we are seeing that the dotproduct v v is equal to d square of the length of it now we just show that the length of the V Vector is equal to the square root of x square + y sare this means that this amount if we remove this what we were wanted what we wanted to prove so this is equal to square root x² + y s and the square of it so we just want to prove by making use of all the geometric interpretation and the phenomenon that we already know including the Pagan theorem so this is then equal to square root with the square here it cancels out so this two and we simply end up with x² + y s so we already proved that the dot product that we defined like this and we are saying according to the definition of the dot product V Dot product of the V so the vector v is equal to the square of the distance or the length of the vector that this in this specific case if we are in the two dimensional space and our Vector is defined by this x and the Y then this dot product is simply equal to the x² + y^ 2 that's something that we just proved all right so let's now apply to an actual example so here we are seeing that the length of a vector is deeply related to dot products that's something that we just saw also when plotting it in our cartan coordinate system so let's now look into this specific example so V is equal to 34 so this is the xaxis this is the Y AIS and here the proof is that the dotproduct VV is equal to 3^ 2 + 4 2 is equal to 9 + 16 is = to 25 which means that the length is equal to square root of this dotproduct and it's equal to five now let's actually prove this this just to ensure that we are at the same page so we can actually go ahead and plot that in here so we have that the vector v is equal to three and four so a few things just to get straight before plotting this so we know that the dot product can be defined like this and we said that this is equal to the square of the length of this vector and we also prove that this is also equal to the x² + y s if we are in the twodimensional cartisian coordinate system so here this is our X and this our y so our X is equal to 3 and our Y is equal to 4 let's actually solve this problem and find it also geometrically to prove what is the dotproduct and what is the length of this Vector so the vector v is 34 which means that it is 1 2 three so this is three and this is four so we are here and this is our vector v so this also means that one of the sides of our triangle with 90° angle so our X so this our X and then X is equal to Tri so X is equal to Tri and our Y is equal to 4 so it is this one this is our y so now we want to find what is this length so we want to find what is the length of this vector v so the length of vector v is then equal square root of so it is equal to x² + y s according to Pythagorean theorem and then this is equal to just filling in the numbers 4 S is 16 and then this is 9 + 16 is simply equal to 25 so this equal to 5 so in this way we can see that a we are proving that the the length of this vector v is equal to 5 if the sides are three and four which means that the vector has this coordinates 34 and at the same time so in this way we already see a few things so we see that the length of the vector v is equal to 5 that's something that we already proved and we also saw that the dot product is equal to the square of the length which means that it's equal to 5^ S which means that it's equal to 25 so and that's exactly what we see here it says that the do product of V is equal to 25 and the length of that Vector is equal to 5 so the dot product of the two n vectors A and B is defined as ATB which is equal to A1 B1 plus A2 B2 up to a n BN so this is basically the generalization of the definition that we just saw before so we go from the um from the description of working with the vector v and the distance to the more General one when we are now dealing with n dimensional vectors so now we are no longer in a basic two dimensional space but we are in an IND dimensional space and we have these two different vectors A and B think of a as this Vector A1 A2 up to a n and then think of this Vector b as B is equal to B1 B2 up to BN and the dotproduct of this two we are writing as a and then this small letter T which means transpose and we will see something very soon and we will also be discussing the topic of transpose later in the next module two multiplied by B so this ATB or a transpose B is um bit more advanced way of writing down this idea of dot product and the dot product of these two n vectors A and B is defined as this formula so we are basically cross multiplying the two we are taking this element from a and we are multiplying it with this one then we are taking this element of a and multiplying with this element of B so we are basically taking all the corresponding elements from these two different vectors and we are multiplying them together and creating this uh combination and adding them up to create this single value to represent these two vectors with the single value which we call Dot product so this notation ATB um which is a common way of noting and denoting this dot product this is just one way of describing this dot product there are also people who will I write down this so you see that this uh triangle type of braces and then the first vector and then the second one and then this one so you will notice one thing that will be uh quite different in here versus in this definition because in here we were discussing the uh dotproduct of a vector so we were talking about exactly the same Vector so here we have V and V and we were just saying let's take the dot product of a vector with itself and this was related to this idea of length when we go from one vector to this idea of multiple vectors so now we are creating no longer the do product of single Vector but we are defining the dot product of two vectors A and B the definition changes as you will notice but that's important to differentiate the two and to understand that one is actually U not the other one and to um to be able to see the difference between the two and not to confuse them so uh in the upcoming presentations we will use this notation of ATB much more often than this uh this second notation but if you see this this uh you can just know that we are talking about the same thing which is that the dot product so when the N is equal to one then the then this inner product or the dot product simplifies to the multiplication of just two numbers because we no longer have vectors but we just got a single value for a and a single value of B so let's assume that the N is equal to 1 and then a is equal to 2 and then B is equal to 3 and then the dot product is simply the a TB uh which is equal to 2 * 3 which is just 6 so uh when it comes to this uh idea of transpose I will explain this in a bit and we are going to uh describe this in detail as part of the next unit so I won't go too much into it but it's always a great idea to to know um also for this module um at high level this idea of transpose this T is basically referring to this so transpose so um to understand this idea of dot product uh the dot product is an operation that takes these two vectors vectors A and vectors B and returns a single number a scaler and the single number is uh computed by calculating the sum of all these individual values so it takes all these elements from one vector then the other one so A1 and B1 A2 B2 a and BN takes the value from one vector with the corresponding one from the other Vector multiplies them to each other and then add them all up and then this single value is created which will refer as the dot product so geometrically it represents this product of the vectors magnitudes and this cosine of the angle between them so um the we already know from high school and also from Tri gometry and geometry this idea of angles and what is the cosine of an angle so I won't go too much into into the detail of it I will assume that you know what a cosine is so the dot product is used to determine the angle between the two vectors and also to check whether they are orthogonal of not because uh we have a property that relates to dot product directly to the orthogonality of the two vectors and knowing how to calculate the dotproduct of the two vectors will help us to check whether the two vectors are orthogonal or not and just in general understanding this concept of dot product is super important because we use dot product in machine learning we use do product in uh generally in AI you will see that product appearing also when you when we are talking about attention mechanisms or calculating this uh attention scores as part of multihead attention in Transformers We also use this idea of dot product scale do product so it's really fundamental and essential to understand this concept of dot product for metrix multiplication but also in general to apply linear algebra in different science related domains so let's now look into a specific example to um understand this concept the dotproduct of these two uh vectors in this case we have three dimensional vectors A and B the dot product can be calculated as follows so you can see that we are taking the one this is the A1 and we are multiplying it with B1 so it is the this one so B1 B1 is equal to 4 A1 is equal to 1 so it is this one we multiply the two and then we go to the next sum so here I will write it down actually underneath to make it easier so this is A1 this is A2 this is A3 as you can see and this is B1 B2 and B3 so taking A1 so taking A1 B1 adding A2 B2 two and adding A3 B3 we are getting the dot product which is equal to 4 6 + 5 which is equal to 3 so if you do the calculation or you use a calculator you will find out that the dot product that can be described by this formula which is very similar to what we have seen in here only n is equal to 3 we are getting that the dot product of the vectors A and B in R3 is equal to 3 so let's now look into another example where we will do all the calculations manually step by step so we have these two different vectors we have Vector a and Vector B let me also hear the arrows and I want to calculate the um dot product a b so the ATB the dotproduct ATB is equal to and we know that if we are in the N three dimensional space so we are in the R3 we expect that we will need to just use the cross product three times and we need to add them up so we know that it's equal to A1 let me use the black color so A1 B1 + A2 B2 + A3 B B3 now what is A1 what is A2 and what is A3 and what is B1 B2 B3 so A1 is equal to 1 B1 is equal to 0 A2 is = to 2 B2 is equal to 1 and A3 is equal to 2 this is three and then B3 is equal to 3 now what is this amount 1 * is 0 2 * 1 is 2 and then 2 * 3 is 6 and what is 0 + 2 6 is 4 so let's Now understand what is this application of without product and how it can be used so the dot product between these two vectors gives us an idea uh a way to measure their similarity so it combines the magnitude the length of the vectors and the cosine of the angle between between them in the simple terms it tells us how much one vector extends in the direction of the other one now we know how we can calculate it once we have the vectors we we understand the notation we also know this relationship between the um vector and the uh distance what we mean when we have um Norm when we have a distance we also know the the length of a vector I want to combine all this and uh combine this with this idea of cosine which is something that we learn in our high school as part of our CH genetry and geometry I want to relate this all to make a sense how geometrically we can interpret deety of dot product and it will all uh make much more sense so the dot product between the two vectors gives us a measure of their similarity this is something that we are also using a lot in the field of artificial intelligence when we want to compare compare two different uh for instance users so we have one user and we want to compare the uh this one user to the other user then we are using this cosine Ru we are using what we call cosign similarity as way to measure their similarity so how similar those two customers are how similar those two users are how similar those two companies are and we are doing that by using this simple cosine idea which also is related to this idea of norms this idea of distance so or the length of a vector and the relationship between two vectors so let's actually wrap this up together and um clearly uh indicate what is this relationship between the dot product the length of vector cosine rule and this cosine similarity so in a simpler terms this uh dotproduct um it tells us how much one vector it extends in the direction of the other one so uh when vectors point in the same direction then we know that the dot product is positive and largest it means that we are dealing with two vectors that are very similar but when the vectors are perpendicular then the dot product is zero and we say that those two vectors do not share any Direction they are not similar so when the vectors point in the opposite direction then the dot product is negative we are saying those two users are really um negatively correlated so one is the opposite of the other one so to say so let's actually plot this and see it um and bring them all together so uh let's say we have a vector s and we have a vector R and we want to see how similar the two are so we have here our Vector s and here we have our Vector R and those two vectors come together and they um create this angle and we are referring this angle by angle Theta this is just a way to refer to our angle those are all things that we learned as part of high school through a gometry so they should all seem very familiar so uh from the geometry we know that if we have this triangle and here we got this um angle which is Theta and here we got uh let's say side C and then here a and then the B we know that the c^ 2 is equal to a 2 + b 2 2 * a * B * cine of the opposite angle of the C which is Theta now let's actually go ahead and apply is to our case when we instead of having just a triangle we got a vectors so we got a set of three vectors that form this triangle and we already have seen in many examples previously one way of doing that and that's actually go ahead and transform this Vector uh this uh into vectors and we can do that if we plot in here this side then this so we basically recreate the same triangle only this time the side are just vectors so this is let's say um our Vector R this is our Vector s and then this is obviously the r minus s and the opposite side is Alpha so the vector R and Vector s they form the angle Alpha so when we apply to the cosine rule to this of course we instead of using Vector notation we need to use their distances because the cosine rule is based on the sides so the length of the sides so the length of the uh of this side this one we denote by our common notation of the norm which is simply this thing and then the same holds for this side which is simply the norm or vector s and then for the third side for this one we then have Norm of R so if we apply this cosine low to our specific example we can see that in our case our C square is simply our the length of our Vector r s only squared which means that so let let me actually write it down for Simplicity that part so a is then in this case my Vector R and then the distance of it obviously so the length of the uh Vector R and then B is the length of vector s and then the C is simply equal to the length of r s then here I will just change my angle and I will call it t done and let's now apply the cosine law to our actual example which means that we got r s and then squ is equal to and then we got R squ and then plus s squ so the length of the vector S and S S minus 2 * R * X s time cosine of theta so one thing that we can recognize here very quickly is that here we are dealing with the S squ of a length of a vector Rus s and we have learned from the previous definition of a dotproduct of a vector with itself so the dotproduct of vector v is equal to the square of a distance so this means that here we already got the squar of a distance of the vector v so this is something that we have because we have R minus s and then squ this is exactly the same as the V vector v and a squ with its length so the length of vector v squ is the same as the length of the vector R minus S 2 because in this case the V is equal to so the vector v is equal to Vector r s and uh from this we can see that this means that here we are dealing with the dotproduct of R minus s so let me actually write this down in here in our example so this is basically the right hand side of the formula that we saw before where it was saying that vector v do product of vector v so this is equal 2 and then vector v the distance of or the length of it and then squared so in this case the vector v is = to r s this means knowing this we can actually make use all that formula and say that here we are dealing with the dotproduct of Rus s which means that we can write that this can be re Rewritten as R minus s and then times r s so in the left hand side we will be stting working with the dot product with the vectors while in the right hand side we will still keep our lengths so here we have the length of the r Vector so R 2 + S 2 2 * R and then s and then times cosine of theta now let's actually open those parentheses so this gives us R and then R basically the same do product of vector R and then minus Sr so dotproduct of vector S and R and then minus s actually plus s and then s and then minus s * R so I'm basically opening the parenthesis of the uh above part this left hand side to simp ified and then in the right hand side I will just copy paste this so r n 2 + S and then s and then Min 2 * R forgot here arrows and then s and then cosine of theta let me actually go ahead and remove all these parts to open up some space for us and there we go right so let's actually go ahead and simplify what we got here now what do we see here we see here that here we got one dotproduct and we know that the dotproduct of a vector r r is simply the distance of that vector and then squared minus here we got so we will just keep it like that we are taking the dotproduct of vector s with Vector R we are adding here here we again got a DOT product of a vector with itself which is equal to the the the length of that Vector s only the square of it using exactly the same lad that we just saw the dot product of a vector and then here minus and I will keep the dot product of these two vectors S and R together so this is then equal to and I we just take over this part exactly as it was so R and then squ plus s and then squ and then minus 2 * R and then s and then cosine of theta now uh we already see couple of things that we can cancel out which is great so we see here that we are dealing with um the length of a vector R and A squar something that we see here too so we can safely cancel those out another thing that we can see that we can cancel is this so again the length of the vector SN squ so this also go away and we end up with this much more simpler for formula so we got minus s and then R and then minus s n r and then this is equal to minus 2 and then R and then s then cine of theta now we already see that this we can combine we can say this equal to minus 2 times and then the order doesn't matter of course in the dotproduct so R and then s and then this is equal to minus 2 * and then the distance or the length of the vector R and then the length of a vector s and then times cosine Theta okay perfect now we can get rid of minus 2 to so let me clear some space up to see what we end up with so we are left with the for following formula so we see that R and then s so the dot product of these two vectors is equal to R so the length of the vector R multiplied with the length of vector s multiplied by cosine and then Theta so this is basically what we end up with and exactly what we wanted to prove we wanted to see that the dot product so this is the DOT product dot product of vectors R and S is equal to the length of the vector R multiplied with the length of a vector s multiplied by the cosine of this Vector that they are forming so cosine of theta now why is this important this means that we can also take this cosine bring it to the other side and we can use that to describe the cosine with this uh dot product as well as the length of these two um uh vectors so from here we can say that the cosine of theta is equal to the dotproduct of the vector R and S divided to the length of vector R multiplied with the length of vector s so using this we can then calculate the cosine similarity between two different vectors and this then helps us to mathematically compute what is this measure of similarity between two different entities whether do we want to compute similarity between two customers whether do we want to compute the similarity between two users similarity between two uh profiles of uh clients so so this can be applied really to do very large range of applications but the idea is that in this way we have proven that we can use this algebraic concept the linear algebra concept of the dot product in order to compute a similarity measure that will help you to compute the similarity between two different entities and this is a very common application in machine learning in artificial intelligence uh the coite similarities one of the most popular uh similarity measures that is used in data science in machine learning in artificial intelligence whether it's in the clustering algorithms like K means whether it's in different sorts of machine learning algorithms or just in general to compute the similarity between uh two uh reviews that customer leave similarities between two profiles of customer similarity of two different um uh users for instance two users that are watching movie and you are building a movie Commander system you can use COI similarity to compare the two users behavior in order to improve the quality of the movie recommender and the applications are so R so versatile that um my takeway uh would be to just uh keep this cosign similarity in mind the formula of it and how it is related to the dot product because uh this is something that will be very useful uh for for your uh journey in applied sciences let's now talk about the idea of in product and how it relates to the idea of dot product so by definition the inner product generalizes the dot product to more abstract Vector spaces which can include spaces with complex numbers so it retains these Key Properties like linearity you know linear Independence and notion of angle and the length that we saw before the norm and how we related back to cosign similarity for much more complex vectors the inner product involves uh basically conjugating one of the vectors and in real Vector spaces the inner product is simply the dot product so if we have vectors that are from uh real spaces so they are from R2 R3 then in this case the inner product is exactly the same as the dot product so let's now move move on towards uh one of our final uh topics as part of this unit and this is a very important topic um very often referred as couchi SS inequality so this inequality and the law states that for all vectors X and Y in an inner product space or dot product space the absolute value of their inner product is less than or equal to the product of their Norms mathematic Al we can express this as the following expression so where this part it simply represents this dot product of X and Y and here as we already saw before the X and with this specific notation it refers to the length or the norm of our Vector X and then this one the norm of a vector y so this definition should and this um and this way of referencing the norm and the length as well as the dot product should seem very familiar to you so what we're saying here is basically if we have a vector X and the Y and we have a DOT product of X and Y if we take the uh magnitude then this will be smaller than equal the length of the vector X and the length of vector y so the intuition behind this low is that it tells us that the absolute value of this dot product between two vectors cannot exceed the product of their lengths so basically if we have a vector a and Vector B and we are Computing the dot product between them so ATB then this amount will always be smaller than equal than the length of vector a multiplied by the length of vector B this is all what this law is about oh this inequality it tells us that the absolute value of the dot product between two vectors cannot exceed the product of their lengths so let's actually look into and understand why does this even make sense so this idea this inequality C squ inequality it is about the limits of similarity so what is this level the maximum level that we can have when it comes to the similarity of two entities for instance if we got two um users and we want to understand how similar those users are this inequality helps us to understand what is this maximum level that we can have when it comes to the similarity of these two people what is this maximum similarity measure that we can have based on the characteristics of these users so no matter how similar the two vectors are there is an upper bound to this measure based on what kind of vectors we are talking about and this uh maximum amount this maximum measure is defined by their lengths so it confirms that this uh cosine of an angle that we just saw because we saw that the cosine Theta is equal 2 is exactly what we got in here so we saw that cosine of theta is equal to the dotproduct of vector R and S divided to the length of them and that's exactly what this cou squares is basically saying it's saying that it confirms that this cosine of the angle between them between these two vectors part of this dot product calculation it can not exceed one which feeds to our understanding of the cosine because we know that cosine is a volume that should be between minus one and one to be more specific minus one and one included and knowing this this helps us to get a limit on this idea so how can we prove the cou sares INE equality this comes down to this formula that we have already proven so let us actually go ahead and prove this couch squares inequality quite straightforward so we have just learned and we have actually proven that cosine Theta is equal to R and then s if those are our two vectors and then here we got the length of the vector R and then the length of a vector s now we know that a cosine is a measure that will always be between Min 1 and 1 so this means that we can use in order to prove this so knowing that this volume will always be smaller than equal to one It means that we can just take this and we can bring it here we know that the lengths are both positive amounts so it means that I can just multiply both sides by this amount that means that I will be left with R and then s so the dot product between the two vectors that will be smaller than equal and then R the length of this Vector multiply by the length of the vector s now as you can see this is exactly the C squares inequality that it says because it says that the absolute value of the inner product is less than or equal to the product of their Norms so the cou SES inequality is a fundamental principle in mathematics but also just in general it's used in so many applied sciences and it's applied across various domains including linear algebra physics but also machine learning and in finance so this helps us to establish a limit on the correlation or this idea of similarity that can exist between two vectors in any inner product space so if we got two different companies and we want to understand how similar they are performing we by knowing their performances and their measures we can uh then have a limit on how similar the two companies can be or how similar the two users can be based on their previous history and this all is being done by using this idea of C squares inequality now why does this matter so understanding this inequality gives us deeper insights into the vector spaces especially when working with this high dimension data which is quite uh often the case when we are dealing with Transformers with large language models with uh deep neural networks or um we are working in finance so this is a super uh important concept when it comes to applied sciences so it ensures that this um validity of operations when it comes to mathematical side and the Transformations we perform on our data on the Practical side that those two are coherent and we ensure that we are safe in making different assumptions and we provide in this way the safety net for um our design of the algorithm we ensure that our algorithm is performing well so every time when we are uh creating a program or an algorithm for a specific case then using this inequality we can basically add a use cases we can create a test cases and we can add these constraints to our algorithm and we can design our algorithm in more intelligent way and we can ensure that we won't have cases when the uh limit goes above uh this threshold that we already got based on this inequality and this is a very important application in a practical sense when it comes to applying layer algebra to applied sciences now uh the final question that I will leave you with is the case when the norm is equal to zero so we briefly tached upon this point and we spoke about this idea of perpendicularity and in a one specific case when the norm of a vector v equals zero so the theorem says that the norm of a vector b equals z if and only if B is the zero Vector so mathematically this means that the distance of the vector v is equal to zero so basically we are dealing with a zero Vector so V is equal to Zer this means that the only Vector with no magnitude pointing in no direction is a zero Vector only in that case we will get a norm equal to Z this concept is super important to understand this uniqueness of a vector and this different foundational principles and properties that will also see appearing in the upcoming lectures and in the upcoming lessons this is all for this module and for this unit uh we have learned ton of uh important stuff when it comes to the vectors so we have looked into the uh from very scratch the idea of vectors the Norms the distances the inner product the dot product we have looked into the geometric interpretation of it we have visualized it we have looked into different properties of vectors the spaces we have also looked into the properties of the dot product we have proven them we have looked into ton of examples we have understood how we can plot vectors how we can understand and relate the IDE of vectors to this dot product and the dot product to the cosine of two vectors and the cosine of an angle and the relationship between two vectors and the similarity between these vectors how we can use and apply that in machine learning in artificial intelligence we have kind of made those relationships there and um we have also looked into fundamental concepts including the um uh scalar Vector multiplications operations with vectors adding them subtracting them and um we have also looked into this idea of linear combination dispan of vectors as well as linear Independence we have defined linear Independence and we have also spoken about uh and provided examples for vectors that can be uh marked and defined as linearly independent and also examples when our vectors were linearly dependent thanks for uh staying with me uh so far and I will see you in the next unit welcome to the module one of this new unit when we are going to talk about about matrices as well as linear systems so those are all fundamental concepts that you will see time and time again when applying linear algebra not only in mathematics but also in applies Sciences like data science artificial intelligence when training different machine learning models and trying to see what is this mathematics behind machine learning models different optimization techniques when you want to solve different problems using linear algebra so in this first module as part of foundations of linear systems and matrices we're going to introduce this concept of linear systems and then we are going to talk about the general linear systems we are going to uh see this common labeling all the coefficients this idea of indices that refer to the rows and the columns we're are going to see what is this differentiation between homogeneous and nonhomogeneous systems so without further Ado let's get started so U the linear systems form the uh bedro of linear algebra modeling this areay of problems thanks to this advancements in these linear systems and solving it in Computing we can now solve a large amount of problems in a very efficient and a fast way so uh the general linear systems can be represented by this uh set of M equations with n unknowns in the previous unit when we were looking into this uh linear combination of vectors we saw this notation which was A1 and then we had C1 multiplied or rather let me keep me uh let me keep the same notation so we had this linear combination of vectors so we had beta 1 and then we had A1 Plus beta 2 and then A2 and those are all vectors plus A3 so bet 3 * A3 dot dot dot and then beta M time a M this is the notation that we saw before and we said we want to come up we wanted to come up with the linear combination of these different vectors A1 A2 A3 up to a and then we use that in order to get a sense of whether we are dealing with linearly independent variables vectors or linearly dependent vectors and then we also commented on this pen that this vectors take now when it comes to um the uh vectors and just in general linear systems we can represent what we had before now in terms of with uh bigger system so in terms of M equations and with n unknowns so here what you can see here is that we have M different equations so we have beta B1 B2 up to BM so you can see it in here and then each of this equation it contains n unknowns so you can see that the unknowns it stays the same so the unknowns are those X1 X2 up to xn so X1 X2 up to xn are the set of all n unknowns and then M equations that you can see in here are all these equations so a11 X1 plus a12 X2 dot dot dot and then a1n and then xn is equal to B1 and here one thing that is really important to keep in mind is that the indexing is what we need to focus on so we need to keep this one in mind this a i j and this XI so this is something that we also spoke about when uh discussing the lar combination of vectors we slightly uh touched upon on this topic so let's now dive into this this indexing and how do we indexes a i j what are these A's what are these JS and here you can see that we have a11 and then a12 and then up to the a1n and this is in our equation one and then we have in our equation two A1 two I may actually write this with a different color so in our equation two we got a 21 a23 up to a2n and this a that you see here those are just real numbers so a11 can be 1 A1 2 can be three A1 n can be 100 and then the same also holds for this B1 for this B2 and for this BN and all these values A's and B's they are just real numbers the only unknowns that we got here are those so the X1 X2 up to xn all right so what about the indexing now so we got a i j and as you can see in this case the first thing that we can see here it stays everywhere the same which is the one so we got here one we got here one and up to the point we got here one whereas the second Index this one it does change it grows gradually with one and it becomes it goes from one to two and up to n so you can see here that the first index first index or index I it goes from one it doesn't change it's just one so it is one one and one so here in all cases for this equation I is equal to 1 but another thing that you can notice here is is that the index 2 unlike index I so the second index which is the J so you see here that the second index is referred as J this is a general way of defining the indexes so here J is equal to one 2 dot dot dot and then n so basically the I doesn't change in the same row but the G changes and then of course we have slightly different in terms of I but then the same for J for our second equation so here I is equal to 2 and then J is again equal to 1 and then two dot dot dot and then n and then here up to for the last equation our I is equal to M and then our J is again equal to one till 2 dot dot dot so you might notice that I was looking at this from the row perspective so I was saying pair equation or pair Row the I doesn't change but then the J stays the same and then it is I one two up to n but the set is the same so it is it contains all these different elements here so one one two and then n but it contains all these different real numbers going from one till n because we are combining and we are creating this combination the sum of all these values a11 and then X1 a12 X2 A1 n xn and another thing that you can also notice here is that here with the second index so with this J J is equal to one then here the X's corresponding index is also one when the J is equal to 2 then the ex's corresponding index is also two and then here the same story and you will notice that while the coefficient contains two indices 1 one one two or 1 n which are the two indices for the coefficients for the unknowns we got just single index which goes from one till n so basically for A's for the coefficients so I let me write with the right color so I can be one 2 all the way to M whereas in case of J it can be one two all the way to n and the indices are basically used to help us to keep track of in which row we are and what is the um variable that the coefficient belongs to because knowing this second Index this helps us to understand that we are dealing with a coefficient that corresponds to this first unknown the first variable X1 and then the same holds in here as you can see in here and in here we are dealing with the same variable X1 therefore the second index the index J is then the same both in the first equation and in the second one in both cases it's equal to one okay so now when we are clear on that let's also understand this high level concept because you will see this system of linear systems this m equations and N unknowns appearing a lot not only in terms of calculating and finding the solution to this linear system but this actually has a very common application when it comes to um running regression linear regression specifically and one thing that you can notice here is that here we got also this B1 B2 up to BM and you will notice that here the index also uh goes from one but then this time to M so when it comes to the rows we have M rows or M equations therefore we also expect when it comes to Counting from the top that at the bottom we will see an M whereas if we count from this side so kind of like imagine it like a column then we see that it goes from one till n so those are common obervations and reference to um number of observations and number of uh features that you will see in your data when dealing with data analysis or modeling data so just this uh just keep those things in mind this uh abbrevation of M and then n m equation and unknowns because this will become very handy and the same also holds for this indexing just to keep in mind that this I and this J what those indices are and how for instance the first you know the I the first index changes when we go from up to the bottom and how the second index J goes and it changes when we go from left to the right when we go through the columns but we are going to see this also in the upcoming slides so uh we can we will have time to practice it so um this is what we are calling a coefficient labeling the coefficient uh a i j so this thing in a linear system they are labeled where the first index represents the row and the second index denotes the column so when we see a i j we know that this refers to the row and the J refers to the column so this is something that we use in order to understand where exactly in our Matrix something that we can we will see very soon where exactly our unit or our uh member that is part of our Matrix where exactly is that located in which row and in which column the systematic labeling is super important because this helps us to keep the structure and this helps us to understand uh what does this uh coefficients are present what what is this row that it belongs and what is the column it belongs so for which equation and for which unknown we have already solved the problem such that we can know what this uh coefficient represents so before moving on onto the actual linear systems and this definition of metrices let's quickly understand this distinction between homogeneous and nonhomogeneous because this will help us to also get an understanding how we can solve a system of linear systems so a system is homogeneous if all the constant terms b i are zero otherwise it's non homogeneous so identifying this helps us to really understand nature of the solution set that we need to get and to understand what kind of strategy we need to to use in order to solve this problem now what do I mean by bi we just saw that we had this system of M equations with n unknowns and we saw that that we have in the right hand side this B1 B2 up to BM which means that we had this m different equations with n different unknowns and to find a solution to the system it means finding this values corresponding to X1 X1 here X2 X2 xn so basically finding the set of X1 X2 up to xn that solves this problem and for us to know how to solve this problem we need to know whether this B1 is equal to zero or not this B2 is equal to zero or not and then this BM is equal to zero or not this is very similar to this idea of solving any sorts of um problems that contain unknowns for instance if we have three x is equal to let's say five solving this is entirely different than if we know that the Tre X is equal to zero so this is a super simplified version of course but the idea is the same knowing that this B1 B2 up to BM this R zero this gives us an idea how we can solve this problem and later on we will see this distinction between nonhomogeneous and homogeneous system and whenever this BS so whenever this B1 B2 up to BM whenever these BS are zero then we are saying that the system is homogeneous and we need to solve a homogeneous system otherwise we are dealing with non homog system so this means that the bis are not all zero let's now move on to the second module which is about the matrices so we are going to define the Matrix we are going to see the definition of it as well as the notation the IDE of rows columns Dimensions uh some of which we have already touched upon but we are going to uh go into the depth of it we're going to learn properly as well as we are going to see many examples then we are going to talk about Matrix types so here we will talk about identity Matrix diagonal matrices and also special type of matrices like matrices containing only zeros and only ones so by definition and a matrix is a rectangular array of real numbers that are arranged in rows and in columns for example an M by n Matrix a can be represented as follows so so let's look into this definition and this reference to Matrix we call this Matrix or Matrix a and every Matrix it can be described by this rows and columns where we always have this uh way of describing this Matrix always should be defined by the number of rows and number of columns so this is super important and let's look into this specific Matrix so we have a matrix a and all these values they are members of this Matrix they form the Matrix and we already saw this labeling of a i j where we said that I is referred to the row so you might recall that those all these equations that we got so this horizontal lines where I was equal to one I it was equal to two I was equal to three up to the point of I was equal to M and then we had this J so this thing and then J was referred to the columns and we had J was here here one and then two and then three up to the point of n so 1 2 3 and N this is exactly what you can see here so in this Matrix we got all these elements a11 is a number a12 is a number up to the a1n is a number those are all real numbers and one thing that you can notice here is that here we got a11 so this is our first row and First Column here we got A1 two this is our first row and second column and then we got up to the point of a1n actually let me just write this down even at a bigger scale such that I can make more nose so let's assume we have this Matrix a and this Matrix a if I'm bigger and we got all these different elements so we start with our first row and here we have a 1 one so here the row that I will write with let's say we blue the row is equal to one and then the column is one so this is Row one this is Row one row one and this is column one let me write it wait right this is column one this is column two this is column three dot dot dot and this is column n and this this is row two this is Row three dot dot dot and this is row M so in total I got M rows and N columns I will come to this notation that I'm putting here later for now let's keep track of the rows and the columns to get a good understanding what this indices were about that we just learned so every time I will also mention this reference to a i j to keep track of this and also let me write it with the right colors so a i this is the row and J which is the column so all the elements I'm just defining by this a because it's just a way to reference a part that comes from a matrix it's a just common way to write the entire matrix by capital letter A whereas its members we will write with the um with the lower case a so this is Matrix Matrix a all right so here in the second row But First Column we got a two and then one because it is still in the first column and then when it comes to this element we have here a the row is the first one because we are in the first row but then we are in the second column so this one should be two then we go on to the next element in our first row so a one and then three and then dot dot dot the last element is an a as we are still in the first row it will be one the I but then given we are in the last column the column index of the J will be equal to n because we got in total in columns so we are now ready to go into the second row so here given that we already have our first element a21 this is in our second row and the First Column so the I is equal to here two and G is equal to 1 let's now write down the element in the second draw a second column as you might have already guessed I is equal to here one I equal to here two sorry and then uh the J is equal to 2 and then we go on to the next element which is in the second row and the third column so it's a the row index is two so I is equal to 2 and then the column index is three dot dot dot and then we got a as we are in the second row it is the I is equal to 2 and as we are in the last column the J is equal to n now you might have already guessed when I was writing this down that whenever you are in row and you move on to all the elements in the same Row the I so the row index it stays the same only you need to uh update the column index so here for instance you got one one one here also one so all the way down in the same row or one which logically makes sense because we are in the same row so the row index should not change but instead you should change the column index like here column one column two column Tre all the way to column n so those are our columns dot dotp so let me make this distinction and those are our row as you can see so this kind of mentally helps us to understand why we are writing all the indices over time once you practice more with this this will become more natural very quickly remove this so now I will write the rest very quickly so as you might have already guessed we are in the third row so we have a tree so everywhere I would just write down the ace so first I'll write down the ace and then the rows the row index will stay the same as I'm in the same row but then I will increase the columns gradually so we are in the column one and the column two column three up to the column n so now the remaining stuff you can actually write down yourself to just practice let's now move on onto the last row and last column so in the last row we got a a a up to here here and in the last row the uh row index is M which means that here I need to have M M M everywhere I need to have M and then the column index is 1 2 three all the way to n so this last column is very interesting too you can see here that we have the opposite of what we have here because in the last column we see that the uh column index is the same so it is everywhere n Only the first index the index of the row it changes it goes from 1 2 3 up to M which is of course logical because we said that in the last column if we are looking it from the perspective of column so all this values this A's so the all the ends they are logical because they we are in the last column we are in the same column but then the row changes here we are in the row one here we are in the row two Row three after to row M therefore we have also at the end a MN now let's talk about this idea of MN we said that our Matrix a has M as a number of rows and n as a number of columns which you can see by the way also here so we always refer the dimension of a matrix so the dimension dimension of Matrix a by these two numbers so first we always write down the number of row in this case M then as the second element we are writing the number of columns in this case n we are always putting this small X in between to kind of emphasize M by n Matrix and we most of the time use the square braces to Showcase that we are dealing with Dimension and in this case we are saying the dimension of Matrix a is equal to M byn so we are dealing with M byn Matrix this is a common convention used in linear algebra in mathematics General but also used in data science uh in machine learning artificial intelligence so whenever you are dealing with matrices a it is a common convention to talk about this idea of dimensions and the idea of Dimensions is super important when it comes to the idea of multiplication multiplying Vector with Matrix Matrix with Matrix so this dot product Dimensions play a central role in here so keep this one in mind once we uh get to the point of that products this one will become very handy so let's now look into a specific example where we see simple Matrix a so in this case you can see that we are dealing with a matrix that has a 2x3 Dimensions so like we just learned 2x3 means that we got two rows and three columns that's something that you can also see here very quickly so you have a small Matrix on the small matrix it's really easy to actually count so you can see that we got Row one and row two and we got column one column two and column three so this basically confirms these Dimensions therefore we are also saying that we have a 2 by three Matrix and like usual we first write down the number of rows and then the number of columns you can see here that here we have this elements for our Matrix so a is equal to 1 2 3 for the first row and then our 4 5 6 for the second row so from this actually I think it's a good exercise to just uh verify our understanding of IND and from this um we can write down that for instance all these different elements uh like a 1 one is equal to 1 A1 2 which means that we are in the first row and in the second column so we have this element is equal to two and then we got a and then one three so we are in the third column so this one is equal 2 3 and then a 21 is equal to 4 a 22 is equal to 5 and then a 23 is equal to 6 so this is actually a good way to practice our understanding of indices our understanding of this Matrix structure and the understanding of dimension of the Matrix which in this case is 2x3 so this is yet another different definition of a matrix structure when it comes to the rows columns and dimensions so this is exactly what we just spoke about on our example and let's just quickly look at the formal definition so the rows of a matrix are the horizontal lines of the of the entries while the comms are the vertical lines so basically it's saying those are let me remove this so the rows are the horizontal line and the columns are those vertical lines those are the columns this helps us to form these columns so col one colum two and colum three whereas this horizontal lines it helps us to create the rows so Row one and row two which is a core Matrix operations so when it comes to matrices we often perform Matrix additions Matrix substraction but also Matrix um a scalar multiplication of this Matrix so multiplying Matrix MX with a scalar and then Matrix um multiplication just in general so taking two matrices and multiplying them we are going to look into this concept in detail we are going to see many examples like before we are going to dive deeper into this such that we lay the ground on uh to the next module which is solving a system of M equations with an unknown so solving this General U linear system so for the beginning uh we will be looking into this Matrix operations where we are adding or subtracting matrices so by definition the sum of two matrices A and B of the same dimensions is obtained by adding their corresponding elements so by taking the element i j from both matrices and adding them to each other so in this case you can see that Matrix A and B are here and uh the uh definition says we just simply need to take the corresponding elements corresponding elements from the row I and the column J take them add them and this will become an element in our final um Matrix because when we are adding two matrices of the same size the result is yet another Matrix so we will use the Matrix a to add to Matrix B and this will give us a matrix a plus b and this i j simply refers to the indices corresponding to the row and the column we will look into an example in a bit and this will make much more sense and the same holds also for the difference so by definition the difference of the two matrices A and B of the same dimensions is obtained by subtracting their corresponding Elements which means that in order to obtain this Matrix a minus B this is a new Matrix we simply need to look for each element so we are going to index them for a row I and J we are going to do this pairwise element wise subtractions we are going to see what is that element corresponding to the row I and column G in The Matrix a which we say is a i j we are going to subtract from this the element in the row I and column G that comes from Matrix B and this will give us our new Matrix which is a minus B so let's now look into an example in this Matrix Matrix um uh a and Matrix B are used and Matrix a is of the size 3x3 Matrix 3 Matrix B is of the size 3 by 3 in order to obtain a plus b what we are doing is that we are performing element wise additions now let's verify this so what we are doing here is that we are saying a plus b let me actually get a larger area here so let's say we have the two matrixes I want to add the two in such way that we do everything one by one such that the idea of a plus b and addition of the matrices will make sense so we want to find out a plus b for that what we are going to do is that we are going to make use of this definition that A + B and then I J is equal to a i j+ b i j which is a fancy way or mathematical way or describing that for each element we need to go a look for the row I and column J and take that element from the um column from that uh Matrix a and from The Matrix B so this means that for a + b this is going to be a matrix that will have the same number of rows and the same number of columns as two matrices because both A and B are 3x3 which means also their sum is going to be 3x3 so this going to be 3x3 and here we are going to do so we are going to take for the first row and the First Column so for a + b 1 one so first row and First Column we need to go to the first row and First Column of Matrix a and the first row and First Column of Matrix B and we need to add this two elements so we need to do 1 + 1 and then we need to go on to the second column so the first row and the second column which means that we need to be here in both matrices so here we have 0 + 2 and then we got 2 + 3 and then we got 0 + 0 so you can see it in here and then we have 1 + 0 and then we have 3 + 1 0 + 1 and then 0 + 2 and then 1 + 3 which gives us so so 1 + 1 is = 2 0 + 2 is = 2 and then 2 + 3 is = 5 0 + 0 is = to 0 0 + 1 is = 1 1 + 0 is = to 1 0 + 2 is = 2 and then 3 + 1 is equal to 4 1 + 3 is equal to 4 which means that our A + B is equal to this Matrix that we got in here so you can see that we are getting exactly what we uh what we have here only we have done it manually one by one so the same idea holds exactly when we have a minus B only instead of adding you will have to do here minuses so minus minus so everywhere minus so 1 1 0 2 2 3 Etc so let's look into another addition so in this case by definition it is defined as this element wise uh of the adding of these two matrices here the only difference in this definition is that it's saying it's calling this a plus b as C so this new Matrix that we are getting as a result of adding a to B it's calling C so basically it's the same as calling this Matrix as C you will see also this type of definitions so in this case The Matrix C is equal to a plus b which basically means that for each row row with index I and with each column with index J go and look for row I and index J take the corresponding elements from Matrix a and Matrix B add them in order to get that corresponding element in our new Matrix C and you can see that in this example that's exactly what we are doing we have a we have B we are taking this element and this one so 1 + 1 we are getting here two and then 0 + 2 we are getting two here 2 + 3 is 5 and then 0 + 0 is = to 0 1 + 0 is = to 1 and then 3 + 1 is = to 4 so now we already go to the next topic which is about scalar multiplication of a matrix so by definition scalar multiplication of a matrix a by scalar Alpha results in new Matrix where each entry of a is multiply by Alpha the idea of scalar multiplication matrices is actually quite similar to this idea of scaled multiplication in vectors so uh we have already seen in the lecture of the vector multiplication that when we were having this scaler C and we had this Vector a then uh when we are multiplying C which is a real number with Vector a then we simply need to take all the elements of vector a so A1 A2 or all the way down to a n and we need to multiply them by this same scaler so C this is what we were doing with vectors and that's exactly the idea behind matrices and when uh during the scalar multiplication of matrices only instead of multiplying only just one vector with this scaler C now we need to apply this to all the rows and all the columns so here we got this one column and Matrix is simply a combination of multiple vectors which means that we need to multiply all these elements of all the vectors of all the columns in this Matrix so let's actually look into a specific example so in this case we have a matrix a and this Matrix a is this thing and we have a scaler which is three so in here our Alpha is equal to three or you can call it C or anything so you can see that when we are scaling The Matrix with a scaler in this case Tre with this Matrix what we are doing is that we are simply taking each of these elements and multiplying it with the scaler so 1 by 3 is 3 2x 3 is 6 3x 3 is 9 and 4x 3 is 12 this is the idea behind this entire scalar multiplication of a matrix in more general terms if we for instance have a matrix a so let's actually look into a high level General example when we have the a matrix M by n so we got M rows and N columns and we want to get a scal multiplication of this Matrix and um scaler that we have here as in our definition it is defined by this alpha alpha is just a number you can go C you can quote B anything so in this case our scalar alpha alpha is coming from R so it's a real number so Alpha time a is then simply equal to to this new Matrix where all of these elements are simply multiplied by this scale so I would just take over all these values H1 up to a M1 and then A1 2 a22 all the way down to a M2 and then let me also add the last column just for fun here a 2 N and then here a MN so here this new scaled mul multiplies so scaled uh Matrix a so Alpha * a is simply equal to Alpha time all these elements are simply multiplied by this scale it is as simple as that so that's the simple idea behind uh metrix uh scaling so when you are doing scalar multiplication of this Matrix you simply take all the values and you multiply them element by element per row and per column by that single scaler Alpha do note that you are multiplying them all without exclusion with exactly the same number which is that Alpha so let's now look into the definition of matrix multiplication so here we are no longer multiplying a matrix with a scaler but we are multiplying Matrix with a matrix so the product of an M by n Matrix a and an N by P Matrix B results in an M by P Matrix C where each entry cig is computed as the dotproduct of the each Road of a and the J column of B now what does this mean firstly let's look and unpack this part of the definition so we got Matrix a that is M by by n and then we got Matrix B which is n by P what this means is that in this case Matrix a has M rows and N columns and Matrix B has n rows and P cups so this is then simply the dimension dimensions of the two matrices so then it's saying that by definition the product of these two matrices so the product of A and B the product of the two me is equal to to Matrix C and each entry cig J so C J is computed as the dotproduct of the E row of a and the J column of B now this part might seem bit difficult but once we look into the actual example and we illustrate this on our common high level General expressions of Matrix A and B and multiplic ation this will make much more sense for now before coming to this one I just wanted to refresh our memory on one thing I said before when discussing also this idea of improving uh this uh different properties of vectors that when we want to multiply a vector with a matrix or Matrix with Matrix or vector with a vector we need to ensure that from the first element the number of columns is equal to the number number of rows of the second element this is also very important for this specific case and just in general for matrix multiplication so you can notice here that the number of comms here is equal to the number of rows in here and the order is very important so in case of matrix multiplication the order is really important which means that if you have a matrix a and you want to multiply it with a matrix B then the number of columns of a should be equal to the number of rows of B otherwise you cannot multiply those two matrices with each other so in case you got a matrix a that doesn't have the same number of columns as the rows of number of the Matrix B then there are some alternative things that you can do including this idea of the transpose that we saw also doing when Computing the dot product between this Vector a and Vector B that's something that we also do in programming when we are dealing with this Matrix and we want to compute this relationship between two matrices but the number of columns of one of the first one is not equal to the number of rows of the second one we are simply uh manipulating these matrices or moving some data if that's not hurting our problem maybe uh flipping so transposing our Matrix or applying any other source of operation to it to ensure that the two matrices that we are multiplying with each other the first one's number of columns is equal to the second one's number of rows so that's just the low and that's something that you should follow if you want to multiply these two matrices all right so now let's move on onto the idea of multiplying and Dot product let's look into a specific example and this will um help us to understand this process better so before doing that I just want to quickly show you this general idea so if we have a matrix a that is M by n which means that it looks something like this like A1 1 a 2 1 up to the point of a m one and then here we got let's say a one 2 a 22 up to the point of a M2 and then at the end we got a MN and here we got a one n so let me also add this one 2 N and we got a matrix B this Matrix B is n by P so it has n rows and P columns so we are fine in terms of Dimension here and we got here b11 B21 up to the point of b m sorry BN in this case let's not confuse the letters so b n 1 B one 2 B 22 up to the point of B n two because n now is the number of rows for Matrix B unlike for the Matrix a up to b 1 p and here b 2 p and here up to the point of B and then n p this is the last element in order to perform um multiplication between these two matrices so to obtain a matrix C which is equal to a * B what we need to do is we simply need to take pair case so pair Row for the row I for instance we need to take this element so this row and we need to multiply it with this so we need to find a DOT product between this row and this column then we need to move on onto the next one and then for the second element we will then take this row and we will multiply it with this one so this is then something that we need to do in order to obtain these elements and you might have already noticed that we got this m by n and n by P so you might have already guessed what will be the dimension of the C if we got that the dimension of a is equal to M by n and the D dimension of B is equal 2 N by P then the results Matrix after M multiplying the two of Matrix c will be will be having a number of rows equal to this and the number of coms equal to this so This middle part basically disappears and the number of rows of the first Matrix will be then the number of rows of this result Matrix C and the number of columns or the second Matrix so Matrix B will then be our final number of columns so we will then have a matrix C that will have a dimension so Dimension so dimension of C will then be equal to M by P so we will have M rows and P columns so how we are going to compute this so for c i j which means row I and column J let's look into the definition of it it's saying CI J is computed as a dotproduct of the E row and the J column so each roow from a and Jade column of B what is the each Road of a the each Road of a is somewhere here so each Road of a it is uh d a and then I one then a and then I2 and then a and then I3 dot dot dot and then a i and then we got in total n color columns and and we always do the transpose right when Computing this um dot product so we then take the transpose so we take this row row I and we multiply it so we do the dot product between this one this is the a i and the B J this is column J it is somewhere here so it is B and then we got the first element which is one and then J and then b 2 J B 3j dot dot dot up to B and then in total we got n rows in B so n and then the J is the column so it's Tak is the same so this is then the dot product between e row that comes from Matrix a and the J column that comes from Matrix B so it's always like that actually so we always take row by row so we take this different so every time we take just a row and we multiply with the corresponding column and then we get the dot product between this row that comes from the first Matrix and then the column that comes from the second Matrix in that specific order in order to get our DOT product and that specific volume and what is this amount actually so when we calculate this dot product you can quickly see that we have a i1 multiplied by B1 J plus a I2 multiplied by b 2 J and then dot dot dot a i n multiplied by b n g and this new Matrix c will then have all these elements so C11 c21 and then c31 dot dot dot and then see the last row as the number of rows of C is m c m see here M so C and then here it will be 1 2 C 22 C3 and then two up to the point of cm and then two and then here the last col will be c one and then p is the number of coms in C so C1 p and then C2 p and then C3 P that do dot dot and then c m and then P okay so this is what we get this is our final Matrix C when multiplying Matrix a and Matrix B so let me clean this up C is to now if you want to find out what is C11 you can easily feel in this general formula that uh that we just calculated the I is equal to 1 and then J is equal to 1 and this will give you C11 by using this formula if you want to get the CM P then just fill in the I is equal to M and then G is equal to P in order to get this value CN P so you can already see the amount of calculations you need to do in order to get all these elements from these large matrices A and B let's actually look into a simple example to clarify this so we have a matrix a here and Matrix B here and we want to do a multiplication of the two and we have just learned how to do it let's actually do it one by one so we got a matrix a which is equal to 1 2 3 4 with Dimensions 2 by 2 then we got a matrix B which has values 2 Z and then then 1 2 so it is 2x two and I want to find what is c that is equal to a * B and I know already by looking at these Dimensions that c is going to be equal to 2 by 2 so you might recall that I said that when looking at this final result the number of rows or the final um Matrix will be this so the number of rows of the initial Matrix a and then the number of columns of this final Vector c will be the number of vectors number of columns of this second Matrix B so two therefore I know already before even doing calculations that the uh product Matrix c equal to a * B is going to have a dimension 2x two let's actually do a calculation to check this so C is then equal to a * B and it's equal to 1 2 3 4 multiply by 21 2 okay so I expect to have four different elements here here here and here so to obtain the C11 so it is C11 in here what I need to do is that I need to look at the first row and the first column in here here so first row from a and the First Column of B and I'm doing the dotproduct which means 1 * 2 + 2 * 1 1 * 2 is 2 2 * 1 is 1 so here I'm getting 1 * 2 + 2 * 1 which basically gives me 2 + 2 and that's equal to 4 so here I'm just writing down 1 * 2 + 2 * 1 now when I want to get this value which is C1 two this means that I want to get the first row and the second column and that's exactly what I'm doing so I'm going back and I'm saying let's look at the first row but this time we will look at the second col from the from the Matrix B so 1 * 0 * 0 plus 2 * 2 and then I do the same only this time for the second row which means I'm picking this row and then this column so it is 3 * 2 + 4 * 1 and for the final element c22 I'm taking the second row and the second column which gives me 3 * 0+ 4 * 2 now what does this giv me this gives me this 4x4 Matrix where 1 * 2 + 2 * 1 is 4 1 * 0 + 2 * 2 is 4 3 * 2 + 4 is = to 6 + 4 which is 10 and then 3 * 0 + 4 * 2 is = 8 so let's check 4 4108 that's is exactly what we have here so as you could see here the idea is that every time to follow what element I'm looking for for the c i j and then I just go to the each rows from the first Matrix and the J column from the second Matrix and I do the dot product of the A and then I and then K let's say so I'm going to the E Row from the first Matrix and I'm taking all the elements which means I don't even need to mention this index it just means the entire each row coming from the Matrix a and then I'm doing the DOT product between this row and the column that comes from the Matrix B which means B and then J which then will give me the c i so I'm looking at this and taking this multiplying this dot product and this gives me the first element then the first row and then the second column which gives me the uh second element in the first row in my Matrix so this one and so on so hope this makes sense uh if it doesn't make sure to reach out because it's a very important concept and uh let's also look into another example to make sure that we got this right so in this case as you can see we have another matrices so set of A and B matrices again 2x two a simple one and we want to know what is a so let's say we call the C we already know C should be 2 by two and what we are doing is basically for C11 we are saying let's look at the first row so first row and the First Column coming from the second Matrix B and let's do the dot product so 2 * 1 2 * 1 2 * 1 4 * 5 4 * 5 we get this and then when we want to find what is C oh what is C and then one two so in the first row but in the second element in our final Matrix so I is equal to 1 and J is equal to 2 it means we need to look at the first row from The Matrix a but this time the second column from The Matrix B so it is 2 by 3 2 by 3 4 * 7 4 * 7 and this gives us a number 34 even if you calculate you can see that 2 * 1 is equal to 2 4 * 5 is 5 so 2 4 * 5 is 20 so 2 + 20 is 22 in here and then you can do the rest of calculations and this will be a good practice to see how we can do a basic matrix multiplication the idea is actually quite straightforward when it comes to multiplying it it just it comes with a practice when we see all this uh much bigger matrices so um this is another example I will leave this one to you to complete it just uh to keep in mind we always do uh so we always look at the dimension first and here 2 * 2 and 2 * 2 which gives me an impression already what I can expect the result will be 2x two and when it comes to the uh cross elements just ensure to always look to the E row and the J column this comes Matrix a and this control Matrix B take them compute the dot product and then you will find your C uh your final result let's call it um k i j because in this case we have a matrix C already welcome to the module 4 of this course when we are talking about matrices and linear systems so in this module we are going to dive deeper into this uh idea of linear systems with matrices and solving linear systems using different techniques and specifically we are going to learn the uh concept behind solving linear systems using matrices named gaussian elimination and gausian reduction welcome to the module 4 of this course when we are talking about matrices and linear systems so in this module we are going to dive deeper into this uh idea of linear systems with matrices and solving linear systems using different techniques and specifically we are going to learn the uh concept behind solving linear systems using matrices named gausian elimination and gausian reduction so uh we are going to solve linear systems um with M equations and N unknowns using this idea of our commented coefficient Matrix and then reduce row hon forms and we are going to see many examples we are going to solve these problems and we are going to do it step by step such that this very important concept from linear algebra will be uh entirely demystified for us so we are going to learn each of these Concepts one by one and we are going to uh solve different problems like before with examples such that at the end of this module we will be clear on how we can use this idea of gaussian elimination and gaan reduction in order to solve a linear system with matrices so uh solving linear systems is crucial in your algebra for many reasons it helps to um provide this fundamental method for understanding and working with linear equations which are the uh fundamental and backbone of various mathematical but also um the applied sciences like uh statistics data science machine learning deep learning and engineer artificial intelligence so uh here is a high level overview of its importance and applications so uh it's a basic but essential part of linear algebra it introduces key Concepts such as matrices vectors determinant so we need to know um and we need to incorporate all these different concepts that we already learned as part of previous modules in here because we need to know um the idea of matrices the dimension of the matrices uh this idea of coefficients versus unknown so variables in our moral homogeneous versus non homog continues how we do the indexing so coefficient labeling in our matrices that we learned before we also need to know this uh multiplication between a matrix and a vector multiplication between Matrix and Matrix the idea of transpose and all these different topics that we saw before uh coming to this specific module so the uh solving linear systems has a wide range of applications when it comes to the um usage of it Beyond linear algebra because it's being used also as part of many other linear algebra Concepts including the um dimensionality reduction techniques uh the composition techniques like um igon values ion vectors uh all these different mathematical and linear algebra concept itself they rely on this idea of solving linear systems with matrices but we also go beyond the application mathematics when it comes to solving linear systems it's used in physics it's used in engineering um and just in general also even in finance economics uh in different uh investment strategies when we are talking about quantitative finance and using data s linear algebra in order to understand what kind of investment decisions we can make so in order to set some uh constraints or objectives and in the essence solving linear systems is kind of this getaway to both understanding deeper mathematical theories but also applying these Concepts to solve real world problem across various disciplines and domains including data analytics data science machine learning deep learning artificial intelligence and much more so without further Ado let's get started so we will start with the refreshment of this General linear system that we saw when we were beginning this uh unit so we saw that we could represent a linear system with m equations and N unknowns with this format where we said that we had this um M equations so you can see we got in total M rows and we got n unknowns which were referring to the columns so then we also saw this idea of indexing a i j where I was the index corresponding to the row and then J was the index corresponding to the column and we said that these coefficient a11 a12 up to A1 n and then the same holds for all the other equations or the rows this were ways for us to understand where exactly we are in our system what is that the variable corresponding to which we are dealing that Co coefficient and what that coefficient will basically tell us in which equation we are and corresponding to which variable or unknown we um have the coefficient because this this a11 holds entirely different information that is a23 and then this amn this amn tells us in the M equation what kind of information we know and the relationship between this when it comes to the end variable or end unknown so this a11 a12 all these AIG Js those are known because we are saying we can estimate them those are constants they come from R whereas the X1 X2 up to xn those are variables those are things that we assume that either we don't know or in the future we will see that those are basically the features that we got in our data and then this B1 and B2 up to BM so far we have been kind of ignoring them we already touched upon this when we were differentiating this idea of nonhomogeneity and homogeneity of our system but in this specific module we are going to finally make this B in use we are going to make use of them and we are going to see how we can transform this General linear system into uh Concepts like matrices and vectors that we already have seen at the end of the previous module so we are going to bring all these different topics together to solve this General linear system and very uh soon it will also become much more clear why we want to do that because solving linear system it means that we are um getting an understanding uh what what is the exact relationship between these coefficients and the unknowns now keep this notation in mind where we have all these different equations you notice here the pluses so we have the common notation of equations that we know from high school and prealgebra because we are going to transform these linear systems into matrices and vectors and the products of them so we are going to go from this uh simpler notations to bit more fancy notation and we are going to bring our linear algebra into use in order to solve a system of M equations with n unknowns so let's go to that bit more advanced notation so here we go so this is the Matrix representation of linear systems and a linear system of equations can be represented compactly using these matrices so we already have seen this Matrix this is very familiar to us we have been using this a lot and this is the m by n Matrix a so this is the Matrix a you can note here we have a11 a12 up to a1n and then we uh we have all these different rows and the columns so another thing that you can quickly notice is that we are dealing with a similar dimension for Matrix a so we have M by n you can see we have M rows and we have n columns something that you can note when you are looking at the last column and the last row and then uh keeping in mind that we have always AI J where the a is the row index and then J is the column index so that gives us understanding what is the dimension of a which is M by n and then we have now a new component here which is something new here which is this Vector so we got a column Vector here X1 X2 up to xn so we already see here from the last element that most likely we are dealing with an N by one vector and how I can know this and how I can be assured by this is because first of all the index is n and secondly we already have learned for us to be able to multiply a matrix with another Matrix or Matrix with another Vector the middle elements of the two so those things they need to be the same so we have learned that the number of columns of the first multiplier in this case The Matrix a needs to be equal to the number of rows of our second multiplier in this case the number of rows of a vector X so you can see it in here so here we got n columns as part of Matrix a here we got n rows as part of vector X which does make sense which means that if we are writing like this and we are multiplying the two then we are indeed dealing with the case when those two should be the same so we are definitely safe to assume that our Vector X it needs to have n rows and only one column so of course the one could have been different but we have already seen another thing before so why do we know that this is right we are indeed dealing with a single column Vector well if we go back in here we are simply taking this system and we are trying to rewrite it in this form and what have we seen here that we only got n different unknowns something that was also given which were X1 X2 up to xn which we can also rewrite in this format X1 X2 up to xn so basically a column Vector that's exactly what we have done here so we have taken all the x's and we have put it in the vector and we are saying let's multiply this Matrix with this com vector and we will get this and how we can know this well let's first look into this first part and then we will understand why we have here this um Vector of P let me clean this up well what we have done simply here is that we have Rewritten this part this part as a multiplication of a matrix and a vector so we have said let's take this we have already learned how we can perform the multiplication between a vector and a matrix we have seen that and how we can do that now we are basically doing the opposite so we have already the final version we have all these equations you can see that we have taken a11 we have multiplied with X1 A1 2 we have multiplied with X2 and A1 n we have multiplied with xn so basically what we have done here is that we have taken the a11 a12 and then a13 dot dot dot and then A1 n we said let's take this and then multiply with X1 X2 dot dot dot xn which is basically the dot product of this row Vector with this column Vector so only the first equation can be Rewritten as a DOT product between this Vector that we see in here and this Vector all right perfect but you can say well we just got uh we not only got one equation but we got M equation this already gives us an idea what we have done in the left hand side what I just did for one equation we can also of course do for multiple equations so basically what we have done was that we have set for equation one or I can also say for I is equal to one we have taken a 1 one a12 dot dot dot a one n because we got n columns or n unknowns and we multiply this with X1 X2 dot dot dot xn and this dot product this is basically A1 vector and then T and then X if we call this X and then this if you open this up and if you calculate this you will see very quickly that you get a11 X1 plus a12 X2 do do do A1 n xn this is simply the the proof how we can go from here to here now if we do if we show the same in the same way you can actually go ahead and prove for yourself that I is equal to 1 we simply have this Vector a11 a12 dot dot dot A1 n multiplied by X1 X2 do do xn but we also have the same for I is equal 22 only with a different row Vector which is of course a21 a22 dot dot dot a 2 N and then dotproduct with X1 X2 do xent so basically the column Vector X doesn't change but we need to change the coefficient that comes because uh in each equation we have different set of coefficients corresponding to this X and not you can see that here coefficients are different their I or the row index is changing because here we have I is equal to 1 then I is equal to two dot dot do I is equal to 1 I is equal to m in the same way here we are also doing the same so here you can see we have exactly the same story only now a is equal to M for our last row which is a M1 and then a M2 dot dot dot and then a MN and then multiply so the do product with X Vector X2 dot dot dot xn given that for all these cases we got the same column Vector you can easily see see how we instead of writing down this equations as separate row vectors we have seen also the same doing previously and instead we will just create one coefficient Matrix so we can say instead of just doing this separately we're also safe to do it um in a combined way so therefore we are rewriting this all and we are saying we can rewrite all these equations now as a11 a12 dot dot dot a1n so as I had before so I'm taking just this part and I'm adding here my Comm mat com Vector that I had to multiply this um A1 T X2 dot dot dot xn so basically I'm just taking this that we had and I'm writing over in here so this is my first row but then of course I can also add there is nothing that I'm changing if I'm adding similar story only this time I'm using this second row Vector a21 a22 up to a2n because at the end of the day I'm doing similar multiplication with x in here too so a21 and then a22 dot dot dot a 2 N and then of course the same holds for a M1 a M2 for my last row and then up to a MN so if you open this up you will quickly see that what you will get is that for the first equation I will get entirely separate amount so a11 X1 plus a12 X2 dot dot dot plus A1 n xn so I'm Simply Computing the dot product between this row vector and this com Vector we have already seen how we can do a calculation like this this should seem very familiar now I'm only doing it at a scale so I'm doing I'm adding here also the second row so a21 a22 dot dot dot and then a 2 N and then here of course I forgot to add the exists so a21 * X1 plus a22 * X2 plus dot dot dot and then A2 n and then xn and then I'm doing this all the way down to the last row a M1 X1 plus a M2 X2 plus dot dot dot and then a m n xn and then what I'm doing is that I'm saying this is equal to this you are free to check it yourself um you can see that once you write down even all the smaller elements in here you will see that you are getting exactly the same this is simply uh another way of writing that you will take all these different um vectors A1 t a 2 t so transpose dot dot dot and then you also represent your last row by a vector and then you take this which is basically M by n and then you do dot product with a vector X1 and X2 dot dot dot xn and why am I allowed to do so because I have M different equations so equation one equation two dot dot dot equation three those are the coefficients of those corresponding equations and those are the unknowns and given that the unknowns are exactly the same across all the equations so as you can see here the unknowns they don't change so this means that every time I need to multiply comput the dot product between a different coefficient so a11 a12 and then it changes to a21 a23 dot dot a2n but the unknowns they do not change so the vector the unknown Vector that I need to to multiply them it doesn't change every time it's this so in here I have the same in here I have the same and in here I have the same con Vector this means that I can simply rewrite the set of dot products into a larger uh system where I have this coefficient Matrix which is a so and we are calling it coefficient Matrix we saw already before y because those are the coefficients corresponding to the unknowns we are distorting them in one place and then we are storing the unknowns X1 to xn in a con Vector because we got n unknowns and then the final piece of this puzzle is what we see in here in the right hand side after the equation sign of course I'm talking about the B so we got here B1 B2 up to BN you might have already guessed that we are uh dealing with um Vector not a matrix because we got M equations and M B which means that we can represent this as B1 B2 dot dot dot BM as a column Vector with M rows because we got here one row two row dot dot dot here M row and then by given that it's just a one column here one so basically we are saying for equation one a11 A2 a12 dot dot dot A1 and multiply it so dotproduct with X so X1 X2 up 2 xn this should be equal to B1 this is exactly what we have let me clean this up this is exactly what we have here we say a11 X1 plus a12 X2 dot dot dot A1 n xn is equal to B1 that's what we have here because we are saying the dot product of this row Vector that we have just used to rewrite this thing that product of this uh coefficient with the corresponding unnown is equal to B1 and we have already seen that it is simply this thing that we are rewriting this first equation we have is equal to B1 as you can see in here therefore when we do exactly the same it's simply equal to here B2 for the second equation dot dot dot and then here is equal to b m so given that from the left hand side here we already have seen that this is simply equal to coefficient Matrix a and then dot product with X where X is this Vector it means that we can also take over this equal sign from here and the only thing that is left from us is to represent this column Vector in here which we can call a vector B and that's exactly what is in here so we soon we'll also see the clear definition of it but for now let's see how we uh we have used this linear system in order to transform it in the notation with matrices so we have taken this C eents and we have seen that in here we can represent these coefficients as a vector a row Vector so a11 a12 a1n multiplied with a common Vector X1 X2 up to xn so this is exactly the first equation the left hand side so we say that this is the first dot product this is the second dot product after to the last row the last dot product so we have already represented this dot products as um multiplication so product of a coefficient Matrix a by column Vector containing all these unknowns X1 X2 up to xn X and we said let's take over this equal sign and given that the same logic rowwise holds also in here so we say this is Row one this is row two dot dot dot row M and everything that holds here for the rows holds here as well we need to take over everything in a consistent way therefore we are saying let's also represent this in terms of the vector if we are doing that in here therefore we are calling this a b and this PS are just real numbers B1 B2 up to BM and this is how we can transform and we can rewrite this system with M equations and N unnown into a product in terms of matrices or coefficient Matrix a a column Vector containing the n unowns and then a vector a column Vector B that contains all these different values B1 B2 up to BM so this basically rewriting what we have in here hope this makes sense but if it doesn't we are going to see this again in the definition I just wanted to warm up with this before we move on onto the uh official definition so a linear system of equations can be represented compactly using matrices and we we just saw that an M byn Matrix a multiplied by an N Vector X results in an M Vector B so you can see it in here so this is an M byn Matrix a this is the n by 1 column Vector X and this is the m by1 con Vector B and one thing that we can see very quickly is that what we are doing here is that we are transforming that uh equations into a matrix representation but nothing changes with the equations we can still write them down like that it's just by using matrices and vectors we will soon see how we can use this in order to solve this problem so why are we doing this uh when we have just two equations with two unknowns we know from high school that we can quickly solve that we can just write down for instance X1 + 2 X2 is equal to 3 and then 3 X1 + 4 X2 is 6 we can quickly solve this problem by uh you know writing down the X1 in terms of 3 and 2x2 filling that in in here in the X1 and then solving the problem because we got two equations with two unknowns the problem become much more complicated when we got three equations with three unknowns five equations with five unknowns 100 equations with 100 unknowns therefore to do this uh solving of the system of equations at the scale we need this Matrix notation and that's exactly what we are doing in here so uh let's also quickly check the dimensions such that we confirm that we are uh on the right path before moving on to the uh practical material so here we got Matrix a containing the coefficients uh from our linear system and it is let me remove this it is M by n then we got our X so this thing is a convector containing the uh unknowns in our system and it contains n rows so the X has a dimension of n by 1 is a single col vector and we are saying this is equal to and that's just given that's how we transform our linear system into this notation this is equal to a Comm Vector B and the dimension of it is M by 1 now let's check whether this makes sense we know that already and we just said that this two needs to be equal it's fine so we have the uh this uh criteria satisfied so the number of columns of Matrix a coefficient Matrix a is equal to n and then the number of rows of con Vector x with unnown is equal to n so we can indeed do the multiplication so we are correct in that notation but uh is it true to claim that indeed here we got M different rows well we have learned that if we take M byn Matrix and multiply it by n by one vector or Matrix then what we are getting is that we first take over over this so the number of rows this will become the new number of rows of our final Matrix and then M by and then this last element so the number of columns of our last item so in this case DX so it's equal to 1 this is our final number of columns so the new dimension then becomes M by 1 and what do we see here that inde the the result that we say we claim that this amount is equal to is M by 1 because if we are saying that the two amounts are equal to each other then uh naturally they should have the same Dimension so a * X will have a dimension M by 1 which is coherent to what we are claiming that is equal to which is M by one uh column Vector B all right so now when we have um refreshed our memory on that Dimensions we are ready to move on onto the next slide so now we need to solve this linear system so we need to find this set of unknowns that will help us to identify and clarify all the unknowns in our system so one way and the INF famous way of solving this linear system is what we are referring as gausian elimination gausian elimination is the systematic method for solving linear systems it basically transforms this system into what are referring us row Ulin form from which the solutions can be more easily identified so in the next couple of slides and uh lessons we are going to learn this idea of elimination we will understand why is it actually called elimination which is quite intuitive on once we see the examples and also we are going to learn this idea of row H form reduce row form because those are very important Concepts to ENT this entire idea of solving linear systems using linear algebra so before moving on onto that we need to know the basics one of which is the idea of argumented coefficient Matrix so by definition the argumented coefficient Matrix of linear systems a XB uh combines the coefficient Matrix a and the vector B into one Matrix so you can see here the Matrix a and then we have here the straight line and then B this is basically this entire argumented coefficient matrix it's a fancy way of saying let's take all the unknowns from our system which are the coefficient in our system and then the constant uh in the B so I have to say not nouns but rather uh the uh values so the scalers coefficients and let's take the coefficients um in our system which are represented in our coefficient Matrix and let's combine this with the information that we got in our system which is the B1 B2 up to BM and let's combine the two and we call it argument coefficient Matrix so we are simply taking the coefficient Matrix a you can see a11 a12 up to a1n and then also in the last row am1 am2 amn this should look very familiar because it's just simply this Matrix Matrix a we are taking it over in here and then we are saying let's draw a straight line and then add here the uh column Vector our result Vector which is B so we saw in here that is this thing so let's take this part in this part put a straight line in between and this will become our new Matrix matx so as you might have already guessed in the a in The Matrix a in the coefficient we got M rows and N columns which means that our new Matrix will have at least M rows and at least n columns now given that we are adding yet another column at the end it means that the number of columns will increase but one thing that is important to note is that the number of rows will not increase because both the coefficient Matrix and the coln vector B they have the same amount of rows they both got uh just m rows so let me actually rewrite this so we know that a is M by n so it has M rows and N columns and we know that b is M by 1 which means that it got M rows and M uh N1 call this means that if we combine horizontally so if we put them together next to each other the coefficient Matrix A and B this means that we will end up with the same rows so we still have M rows but now we will have n + one so here are the N columns from n from uh our coefficient Matrix a and here yet another extra one column that comes from the uh Vector B therefore we get the argument coefficient Matrix that has a dimension M by n + 1 so this Matrix is key when we are applying these different raw operations to solve our system of M equations and N unknowns so we are basically bringing all these values in order to solve our problem and find the unknowns now before moving on onto actual uh definition of row H form RF I wanted to provide here an example so how we go from uh from this system of equations into this uh new form a uh * X is equal to B and then how we can represent our Matrix into this idea of argumented coefficient Matrix so let's assume we have the following system of equations so we got X1 + 2 x 2 + X3 + X4 is equal to 7 and then we got X1 + 2 X2 + 2 x3 X4 is = to 12 and then 2 X1 + 4 X2 and then + 6 x 4 is = to 4 let's now first just write this equations with three equations and then four unknowns into this system where we transform this into this matrices and the vectors so let's write it in this format of a xal to B that we saw before and then we will also write this in the um argument coefficient Matrix so given that I know that we have three equations and four unknowns the first thing that I will do is that I will get my coefficient Matrix a so for a what I need to do is that first I need to understand what is the dimension of it and how can I know the dimension I know that it should be M byn from the Theory where m is the number of equations the number of rows and N is the number of unknowns I got three equations for unknowns so this is 1 2 3 and four X1 X2 X3 X4 those are my unknowns this means that I already know exactly what the dimension of my coefficient Matrix a should be which should be three by 4 let's now go in and search those values and filling in here to get our coefficient Matrix so here I see that X1 has a coefficient of one so whenever you don't see here a value it means we have one times that variable because one * a number is simply that number so here I got one and then uh for 2 X2 I got two for X3 I got one and then for x four I also got one and do keep in mind that in the columns I have to have the coefficients corresponding to this unknowned X1 X to ext4 this will be kind of a guide mentally to check every time whether we are dealing with the right coefficients so now we go on to our second equation or our second draw which means I got one in here see here one and then two two and then minus one and then here for my third dra third equation I got two for X1 I got four for my X2 I got nothing for my X3 which means that it is 0er because 0 * a variable is equal to zero and then finally for my X4 I got six so now I have my coefficient Matrix we can rewrite very quickly the common Vector containing the unknowns we already know what the dimension of it should be because we have four uh unknowns so it should be 4 by one by one and it is X1 X2 X3 and X4 and this is my X and finally I see what my B is here so we got three equations so we remember that the dimension should be M by 1 which is corresponding to what we also see here it's coherent we got three Bs for three equations so therefore I'm saying my B is equal to it is 3x one con vector and is equal to 7 12 and 4 now the final result is simply let me rewrite this in the form that we just saw so I can write this equations the system of three equations with four announce as 1 2 1 1 1 2 2 1 2 4 0 6 multiplied by X1 X2 X3 and X4 equal to 7 12 and 4 this is in the a x is equal to B Matrix notation so this is the first part now we know how we can rewrite the system of linear equations with uh M equations and N unknowns into this form of a x is equal to B let's now try the next step which is writing this um system uh rewriting it into this argumented coefficient Matrix so I want to get this a b this one is actually quite straightforward now when we have our a and the B because what this argumented coefficient Matrix represents it's simply taking the Matrix a and then adding the vector B next to it in order to get our final uh a plus b Matrix so not a plus b but basically a and then next to it the B so what is my a it is this thing so it is 1 one 2 and then 2 2 4 I'm just taking over this 1 2 0 1 one and then 6 so now this is the a with three rows and then four columns and then here I'm simply putting this straight line like in here and then putting the b in there and the B was this so 7 12 and 4 so 7 12 and 4 this is the B which was 3x one 3 by 1 so one column and then three rows which means that this final Matrix The argumented Matrix a b it should have three rows and five columns so it's really important to um distinguish the multiplication and this unique form which is called uh the argumented coefficient Matrix so here we are not multiplying or adding we are simply taking the coefficient Matrix that we just found a next to the VOR B and given that they both have the same rows that's very straightforward and easy so this is about the argumented coefficient Matrix let's now move on to the next concept which is the row etum form or RF in short so Matrix a is in a row etum form if all nonzero rows are above any rows of all zeros so all nonzero rows are above any rows of all zeros each leading and entry of a row is to the right of the leading entry of the row above it and all entries in a column below a leading entry are zeros the leading entry in each row is known as the pivot or corner corner entry now this is a whole mouthful and um the concept itself is not straightforward when it comes to this definition but once we do the actual examples all this concept will be clear and when we solve these multiple problems and we are going to solve like three till five problems it will become easier to understand this concept of row H form what we mean by pivot or Corner entry or pivot uh uh Vector what we mean by uh having all these entries in a column below a leading entry zeros or what we mean with this concept of all nonzero rows being above any rows of all zeros so in terms of the words uh this might seem bit complicated and it is but when we look into the example this concept will be demystified so um once we have the RF which will be uh will uh come back in a bit and we will look into the example to clarify this definition let's quickly look at the next step that we need to perform which is the reduced row Edon form so here you can see that this reduced is what is basically added in front of the row H form so this was the row H form that we will now uh be discussing and then once the row H form is done we basically do a reduced row H form so that is the next step and by definition a matrix is in reduced row H form if in addition to the row H form so RAF we also have every leading entry in a row that is uh equal to one also we are calling that we have leading one and each leading one is the only nonzero entry in its column so r r EF or the reduced R HL form it simplifies the system for easier solution derivation so in terms of explaining or in terms of formulating the uh RF doesn't really uh sound that convincing because we need to demystify all these different concepts so my suggestion would be let's move actual to the actual practical problem problem and once we solve this step by step with all these details all these different criteria this one 2 and three and then the four and the fifth criteria they will be domestify and will be very clear to you let's now solve another problem a similar one to what we just solved only uh this time we will go a bit faster through all these steps just to ensure that our understanding of solving a linear system with this multiple equations and multiple unknowns using this idea of gaan elimination reduction and all these different steps up to the point of the redu through form we can go through quickly and we will ensure that uh we end up uh with the solution uh to the system using those techniques that we have just learned so here is my system of three equations and three unknowns this time and like we learned before the first step is to transform this into this argumented Matrix but first of course I need to know what is my a and what is my B because uh transforming this and writing this in the um argumented Matrix form a and then B it means I need to know what is my a and what is my B so let's write the first that down so my step one in this case is to write it in the argumented so argumented Matrix and for that I need to know what is my a so my a is equal to 1 1 1 and then 2 3 1 and then 1 one and 2 this is my a which I take from all these coefficients that I see in here and then my B I can see in here from all this values so it is 6 14 and 8 this means that my argumented Matrix is a and then B and it's equal 2 1 1 1 and then 2 3 1 and then 1 1 and then two then straight line and then 6 14 and 8 this is my augumented Matrix this is my first step so let me go ahead and remove this details because then I can take this over and I can then simply continue so my argumented Matrix is then 1 1 1 and then 2 3 and then 1 1 Min one and then 2 and then I have the values for my B which is 64 14 and 8 all right so first step is done now we are ready to go on to the next step step two which is to apply all sorts of operations and ideally by every time normalizing each row so every time ensuring that here the leading entries are once we call it normalization and then by elimination so we saw that every time we were eliminating some of the variables to ensure that we will end up with the zeros in the uh lower diagonal of our argument Matrix and we will have only the leading ones in our um only a single one and a leading pivot value so the pivot entry in our argument Matrix per column it will be the only element that will be uh non zero and the r should be all Z so here I'm talking about performing all these different operations to ensure that we uh create and we come to this point of the row eelin form and then from there we will go to the reduceed row eelin form so the r RF that we saw before so let's do that so the step two will be to uh get the RF of a or rather I have to say R RF maybe we can combine the two uh and do it at the same time so for that what I will do is that like before I would just draw this line because this will help me to keep track of all the operations that I'm performing later on to uh to check one thing that we can notice here that the first dra is already normalized which means that I have a one in here so I'm happy with that I won't do anything to it so uh what I need to do is to move on on t uh on the elimination of this um X1 from the row two and Row three so I need to find a way to ensure that this entries become zero because I want to get here one0 0 so I want to have a single pivot entry in here and the remaining values in this column under this one should become zero so how can I do that the first thing that I can do to eliminate this X1 from row two is subtract from this two times the row one so you can see that if I take this row two so this is my row two this is my Row one and oh this is my row two this is my Row one and this is my Row three if I take row two this is my step 1 row two is equal 2 and then I take the row two and I subtract from this 2 * Row one let me remove this to open up some space so if I take this this row and subtract two times this one then what I will get is 2 2 * 1 which is 0o 3 2 * 1 so 3 2 is 1 and then 1 2 * 1 which is 1 and then when it comes to the value of b 14 2 * 6 so 14 12 is 2 so this is what I'm getting and then given that I have already a normalized first row so I have already one in here I will just take this over for now and then for the last column for the R3 in order to normalize that one and in order to get R of this one from here so first thing I want to do is to eliminate the X1 from here which means I need to get rid of this one and how can I do that I can simply take the Row three so R3 and I can subtract from the R3 the R1 because both have one in here and this one I can simply take this and then remove and then subtract from this the first row I can uh then end up with basically a zero so here if I take this one I got this one and then I subtract from this 1 * 1 so 1 1 is 0 1 1 is 2 2 1 is 1 and then 8 6 is two this is what I get after performing this eliminations hence also the name um gausian elimination so using this elimination s we are then getting rid of certain elements and certain variables so this is what we get after performing these row operations and now we are ready to go on to the next stage so we see now that the um in this specific case the row two is already normalized which means that here we already have a one so we are happy with that we want it to be like that so I already see another coefficient that I want to get rid of that I want to eliminate from uh X2 which is this one so I want to get rid of this minus 2 and I want it to become a zero and one way that I can do that is to take this Row three and then add two * row two to this Row three so why am I not taking Row one because row one has a one in here and if I take that I will then um basically um bring me back to the point where I have I'm having here an element because zero and then uh adding something that uh is related to this first row or subtracting from it a multiple of this first row it will always lead me here this zero becoming a nonzero value that's something that I want to avoid therefore I want to perform some sort of oper to this Row three by using row two because that's my only way to eliminate this minus 2 from here without hurting my zero here and therefore I'm looking at this row that already contains the zero here so that's great and I'm going to use row two to eliminate this minus two and the way that I can do it given that here I have a minus so I have minus 2 I need to add to this 2 * Row 2 so in the step number two I will then say that my R3 is equal to R3 and then plus 2 * R2 so let's go ahead and see what that gives us so 0 0 + 2 * 0 is 0 and then 2 + 2 * 1 is 2 + 2 which is zero so I'm already getting rid of the minus 2 which is great that's exactly what I wanted and then 1 here I have a one so 1 + 2 * 1 so 1 + 2 * 1 that gives me 2 2 or 1 2 which is minus1 so I'm getting here a minus1 and then here when it comes down to the B element so I'm talking about this specific element so 2 and then plus 2 * 2 2 so 2 + 4 is equal to 6 so here I'm getting a six okay perfect so let me take over the row number two it's already normalized so I already have a one in here so 0 1 and then minus 1 and here I got a two so I just simply take over I'm not doing anything at this stage to that row number two so oh now the question is whether there is something that I need to do to my Row one well the question is yes because in our definition we saw that we wanted to have um this element ideally being zero because we need to have here a zero for this column to have a single element of one in the pivot entry which is this one which means that I need to make somehow this element one that we see in the row one being equal to zero at this stage let's see how we can achieve that so we got a one in here and we got a one in here so one thing that we can do is to take Row one and subtract from that row to in that case I won't do anything to my first entry my pivot my Pivot entry for my first row I won't do anything to it because 1 minus 0 is 1 but then I will get rid of this one because 1 1 is equal to Z so let's go ahead and do that so R1 is = to R1 R2 so in that way if I do 1 0 I'm ending up with 1 1 1 is = to 0 so I'm getting what I just wanted and then 1 minus 1 it is 2 because it becomes + one + 1 and then 6 2 becomes 4 so this one becomes four this is the end result after performing these operations in Step number two perfect so now uh let's see what else we got in here that we want to change let me make this line longer all right so there is something that I still want to do in order to normalize the last row because here instead of having one I have minus one so firstly I need to multiply this row by one simply by a scaler in order to ensure that I can have one in here instead of just minus one so in Step number three I will do R3 is equal to 1 * R3 so I will get 0 0 1 given and then 6 * 1 is 6 now what else we need to do uh in terms of the rest of the values in our uh argumented Matrix so we got one let me actually write it over such that we see what is going on so I'm taking over this and then minus one and then here I got two then here we got 1 Z 2 and then 4 all right so this is our argumented Matrix and there are few elements that we want to get rid of it is this element and this one because we want them to be equal to zero and how we can do that so we got two things to eliminate we want to eliminate from the row one this two and we want to eliminate from this row uh to this minus one how can we do that well for this Row one we can already see that we got here this one from row three so it's super helpful in that aspect so we can use that in order to get rid of that R2 so let me actually write it down in the uh step four so step number four what we can do is to take R1 and it will be equal to R1 minus 2 * R3 because here here I got one and I can use this one in order to get rid of this two so 2 2 * 1 is equal to uh 2 2 * 1 is equal to 0 so I can get a zero in here and that's exactly what we want to do we want to eliminate this x3e so we want to eliminate this from here this two from this first row that's something that we can achiev by this and another thing that I can also see let me free up some space in here to write down these equations so remove this given that the steps are already written you can always replicate that so then the new augmented Matrix a will become so here 0 0 and then one I'm living the last R it is already in a uh format that is desired so I will leave that there and then I'm going to update my first row so first row R1 is equal to R1 2 * R3 which means it's equal to 1 2 * 0 is 1 0 2 * 0 is 0 2 2 * 1 is equal to 0 and then 4 2 * 6 so 4 2 2 * 6 is equal to 4 and then + 12 which is 16 so I'm getting a 16 here and then for the second row I also need to do elimination to get R this time me remove this this time I want to get rid of this minus one because then I will have um criteria satisfied that this should also be zero because this should be zero for this pivot to be the only nonzero element in this column tree only then I can say that I'm dealing with r RF so that I have my Matrix in the reduced row Edon form so for that what I'm going to do is I'm going to make use of my third row again so I'm going to take row two and I'm going subtract from row two I'm actually going to add Row three to it because here I have minus one and if I have one to 1 I will just get a zero so for that I'm going to write that uh for the elimination so for eliminating each tree from the second row I'm going to write that R R2 is equal to R2 + R3 so then I'm going to get here 0 + 0 is 0 1 + 0 is 1 and then 1 + 1 is zero now why I have used actually the third drawing not the first one because I want to ensure that I want do anything to my zero in here if I do something with this first row then I cannot do that I cannot reach that it's just counterintuitive because I want to keep my work intact I have worked uh in such a way that I can eliminate this X1 from the second dra and that's something that I don't want to go back to instead I want to use something that is in the column three to get rid of this volum and that's something that we could do by using this element so we always look at the shortest and easiest path to get rid of a coefficient in our Matrix in our argumented Matrix now let's see what is left so R2 is equal to R2 + R3 which means that we need to add this two to this minus 6 and 2 + 6 is equal to 4 4 there we go so after performing all the steps we end up with the following argumented Matrix and if you can notice here we not just achieved the row HED love form but we have actually achieved the reduced row hen form of our a this is that form because we have on the entry so our pivots are the ones in here and we can see that all the criteria are satisfied so here on the lower diagonal so the lower part of our Matrix are zeros and on the top of that also all the elements in this columns they are all zero except of the pivot values and they are all equal to one and that's exactly what we wanted to have in order to say that we are dealing with the reduced row Eon form perfect so now we are ready to move on to the last final step to get this solution and unlike the previous system which was bit more complicated and it had multiple infinite solutions so you might recall this example where we had to describe the final solution using this X2 and X3 because depending on different values of X2 and X3 which could take infinite values we then could get a different um solution for our x uh Vector so we we saw that when for instance the X1 is equal to 1 and X4 is equal to 3 then our solution was this but then of course if the uh x 2 and X4 were different we would have gotten another solution and the result of the linear system had infinite number of solution unlike that one this one this example is much more convenient because even from the RF of this so row reduced row form of the uh Matrix a we can already see what is the solution of this problem of this linear system we had three equations and three unknowns so we actually got to single Solution that's something that I can already see from here and let's uh break it down to see how um I see that solution so we transformed this to a system of equations so we are basically doing it backwards and now after getting this RF of Matrix a we are going going back and we are going to rewrite the system with the unknowns X1 X2 and X3 so here I see one which means that the coefficient of X1 is 1 so 1 X1 which I will just write X1 X1 so I get X1 X1 + 0 * X2 + 0 * X3 is equal to 16 and then 0 * X1 + 1 * X2 + 0 * X3 I'm writing down this zero such that it will make sense what we are doing and in case next time you have a different system it will be easier to follow so 0 * X1 + 0 * X2 plus Z plus 1 * X3 is = to 6 now what is this we get rid of all these zeros so you can already see what is going on we are ending up with X1 is equal to 16 X2 is = to 4 and X3 is equal to 6 there we go so we got our solution for X which is X1 X2 X3 is equal to 16 4 6 this is the unique solution to our problem and if we clean this up and we write down in a nicer term so I'm going to remove this then we can say that the solution for our problem so this problem that we saw in here so the X which is equal to X1 X2 X3 it's equal to 16 4 6 which you can see it also here so one trick is always to look into in here if you have the identity Matrix part of your argument Matrix then you have a unique solution to your problem so you got three rows and you got three column and you have all these identity so these unit vectors then this the indication that you got a unique solution to your problem and that solution is simply this column as you can see in here so next time when you have something like this you don't even need to rewrite the transformation back to the equations you can directly say that the solution to your system with these three equations and three unknowns is this one so this column Vector equal with this entry 16 4 and 6 and that's the end of this problem so in this way we have learned how we can quickly solve this type of problems by just following this uh common procedure and this ended up with a unique solution so you can have cases when you will not have any solution and that will be the case when we saw before two so in case um there is an equation says that um in the left hand side you have zeros but then in the right hand side you have a number so you end up with 0 is equal to two number let's say eight this this cannot happen which means that you are saying your system doesn't have any solutions so let's let me actually write it down so you can have three possible cases case one when you end up with one of your um equations and here you got numbers let's say 3 4 5 and here this is zero so basically you end up saying 0 is equal to 5 which is not something that is possible so you say the solution solution is this so it doesn't exist it is also possible that you have case two when um like in the previous case that we saw before in this example you don't have um the um you don't have all the uh unknowns expressed in terms of the other ones which means that you don't have a unique solution to your system so you can even see that from here therefore you have your X2 and X4 so you describe your final solution as a linear combination of some of the uh variables that can take infinite amount of uh vol so X2 can be anything and X4 can be anything which means that your Solutions will also be infinite so this case two will have infinite amount of solutions and the case three is the case that we saw before right now when we end up with this identity Matrix in here and this corresponding uh amounts in here so this gives us indication that we got just single solution so single solution a solution to your general linear systems