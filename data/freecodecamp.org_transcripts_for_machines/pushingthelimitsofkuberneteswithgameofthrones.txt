let me say it's a huge honor and let me first thank the academy wrong stage all right my name is ilya this is zee and today we'll talk to you about hbo journey to kubernetes the journey which began not that long ago from not having a single service running inside the container to hosting game of thrones season 7 on kubernetes z and i will split this talk i'll tell you about why reasons and z will cover how we get it done okay it's downhill from there hbo has come to the shows that everyone is talking about from the groundbreaking series to the documentaries sports to the biggest blockbuster movies available anywhere for over 40 years people who love entertainment have recognized hbo as the original the first and the best place to find world most innovative programming hbo digital products is represented by hbo go which is part of your tv programming subscription through cable satellite or other providers and hbo now subscription directly with hbo both provide unlimited access to hbo programming just about on any device it broad sense digital products is everything and anything to do with the content streaming and digital products where z and i work and platform team so if we zoom if we look at the hbo streaming services this how they could look like from the mile high view this is not the actual image but resemblance is pretty close and if we zoom in they could be best described as a mesh of api services written mostly in node.js also we added more and more sources written in go and all sources were deployed into ec2 into a single service instance single instance paradigm all wrapped with other scaling group which handled both deployment and scale all fronted with the internal or external load balancers and route route usually handled dns needs overall it was and still is tried and true set up for running services on aws it works for general case however if you will see next that hbo case is anything but general hbo traffic pattern can be best described as the wall and this is just a random example of how fast playbacks are started on sunday night during the game of thrones season premiere a season around 6 pm known as a prime time and i think this point in time can be best represented by this image in terms of what our api source is faced with as well as the emotional state of our engineers and so looking at game of thrones uh traffic pattern um episode after episode season of the season left us with very insulin feelings about a future can we hold on during the next episode what about next season and the answers were less on optimistic because we were running into multiple problems chief of them would be under utilization running node.js implies that you can utilize a single cpu core at most now since we're deploying ec2 instances uh ec2 instances with single core typically almost always do not have good network so to find good balance between network and cpu we had to select instances which run at least two cores of cpu so right there we would not be utilizing 50 percent of icps across all deployments on demand scaling is good however it's slow or slower in comparison and sometimes it is inadequately slow to react to spike traffic thus we have to over provision the deployments doubling triple and sometimes more to accommodate for unpredicted traffic patterns so you take initial 50 percent uh on the utilization buffer up with over scaling and multiply by a number of regions that will be a lot of unused cpus so as for elb goes that every service communication requires lb and again it's tried and true elb plus asg approach however even for internal communication within the same vpcu required to stand up elb4 services again that resulted in a lot of elbs which led us to problems uh limits or otherwise known things that we running out of it's ironic because we were utilizing more than 50 of cpus and yet we're running out of all other resources so to keep up with the resource usage we have dedicated alerts which will fire up every time we cross 80 percent threshold utilization on given resources elbs ehgs security groups and when we get notified we'll contact aws support to increase our quarter limits of course things got more interesting when we began to run out of instances for given instance types or ip addresses for given subnets where even aws could not help us with our problems then of course there are external resources that directly tie to instance account like a telemetry provider who would calculate the license usage based on the instances so this brings us to kubernetes and rather than going through these bullet points they're all true i will tell you my personal story in fact this is my second time being on the stage at the kubecon first time was in san francisco in 2015 when i was summoned to stage just like today by kelsey hightower but to my great surprise and horror because i was not supposed to present what happened that walking through the hallways of cubecon 2015 i dropped my wallet somewhere and someone had found it and returned it to event organizers and he called me to stage to return my wallet so i cannot simply find a better example of kubernetes saving the day maintaining stories up and running and prevented outages otherwise would be very interesting flight home so uh we i was settled on kubernetes from the sargon but we did do a diligence we looked at masses with this years we looked at swarm we looked at ecr and for us kubernetes wasn't still is a clear winner we were just the beginning of a journey and we had a very tight schedule uh given that we had to continualize all the services first and continuization and mass scale is a huge undertaking on its own so uh once we begin conquering that health though we start playing with kubernetes and aws and again this is the end of 2015 a lot of change since then so what we started with we with most basic setup available running cube up to deploy kubernetes into our existing pcs we have to tweak and make some configuration changes and what we needed to do we need to show to our peers to our bosses to ourselves that kubernetes known as vaporware but more importantly that kubernetes can be operated in aws cluster to host production grade services and once we get uh we started cutting a teeth with jenkins infrastructure cluster and once we got in successful that we began to provision a home cluster to our streaming services and that's when we realized that basic setup simply won't cut it and we had more work to do and z will tell you how we got it done okay i'm going to tell you something we learned from our kubernetes journey so we create our own telephone templates for our provisioning our clusters we started before some of the community projects started for example our cube aws kops or cube spray this allowed us to do something really cool for example we can deploy our clusters into our existing aws infrastructure by providing our vpc ids subnet ids and security group ids we also had high availability in mind so from the very beginning our minions and master asgs are multieasy um the purpose of the esgs are different though so for masters we want to maintain a fixed number of nodes so if one fails aws will automatically launch a new master for minions we want to scale up and down very fast so we use asg to launch and terminate nodes master is also running in hr mode meaning that api servers are low balanced and schedulers and controller managers are running as leader and followers despite being home grown we keep incorporating best practice from the community we turn on oidc or open id connect authentication for a coupe api server so as long as our developers github accounts are in hbo organization they will get a token for their creepcattle authentication terraform modules is a great way to promote reusability and modularity we create we created selfcontained terraform modules for both communities masters and minions when we want to launch a cluster what we do is we compose a terraform template like this and we will run terraform apply and bam we have a cluster up and running several weeks later we have had some experience of how to operate a cluster and then we noticed a few problems so first we run prometheus in the cluster to script metrics with a provision iops ebs volume as data storage because our cluster is scaled up and down all the time sometimes the meaning that hosts the premises part get terminated we have to wait a long time for permissive spot to come back because kubernetes has to detach and reattach the ebs volume that process could be very slow and during that time we were losing metrics the second problem is for big events like game of thrones premiere of finale or simply our regular load testing we have to scale prescale our cluster up significantly to overcome the wall effect elia just mentioned and sometimes aws cannot provide sufficient capacity of our desired instance type for the minions so these issues led us to an improved version of our telephone modules first we added instance type variable to our minimum module so that all the minutes launched from this sg will get this particular instance type we also added a taint var to the media module and pass that as a cubelet startup flag so if this var is defined then all the all the minions launch from this this sg will register with that particular taint again we benefit from the modularity of our telephone code so for regular minions we pass our main instance type to the 2d module we added another module to our cluster we call it backup minion these minions are exactly the same as our regular minions except that they run on our backup instance type which is c4.8x another module we added what we call reserve the minions so reserved minions are again exactly the same as regular minions except that they are tainted by this chain to reserve equals true at the same time we update our cluster auto scaler so that when cluster scales down the mean uh reserved minions are excluded so to summarize we added two new mini asgs to our cluster to address the issues we had earlier first if aws runs out of the mainstream type we want we scaled up our backup minion to bring more capacity and second for prometheus we update the premises deployment to have affinity to reserve the minions and also tolerate the reserve taint so in this way primitive's part is not interrupted even when cluster scales down um flannel was the networking layer we chose at the beginning compared to other solutions it was simple to set up especially before the cni came and it's included in every um cores digital the digital that we use however when we were doing our regular load test we discovered that on the heavy load there were some problems first it was increased latency and timeouts both between parts and going out of the cluster and second there was udp package drop which impacted our cube dns lookups and custom metric collection both of which are udp traffic these are just two issues that we saw very frequently during the season the github links are on the slide now let's talk about different types of services that we tried and the different ways we used to get traffic into our cluster first we provisioned elbs for every node poor type of service and associate these elbs with the minion asgs so in this way all the minions launched from the esgs will be registered with the obvious automatically however there's a aws hard limit of 50 erbs per asg and also since we are provisioning elvis ourselves you have to keep track of them manually next is ingress which i think is the most common setup out there so we put a shared elb in front of ingress controllers and uh the elb forward traffic to ingress controllers which then proxies traffic to upstream services however there were some problems with that too first when we looked at the cloud watch metrics for shared elb and we noticed some 500 errors but which backend services or service exactly these 500 comes from it's pretty hard to tell without scrunching the ingress controller logs or elb access locks and second we notice the ingress controller seems to be uh seems to struggle against a very burst of spiky traffic that we saw and this this test this setup produced more connection timeout errors in our load test versus the no port setup on the previous slide and finally the publicity of your shared ingress eob will determine all the publicity of all the services and last but not least we tried load balancer service type which is cloud provider specific uh in this scenario kubernetes actually handles the provisioning of the elbs and registered minions with the elbs this method is not affected by 50 elb limits but first we noticed there was a api database api throttling problem and second there was some elb security group customization issue that didn't get resolved until recent 1.8 release so at the end this is these are our choices for services and ingress for production services we use no port plus individual eobs for nonproduction services we use ingress controllers and shared erbs in both scenarios we use the builtin service discovery for making calls between our micro services cube dns is always an interesting topic have you ever looked at the resolve.com file in your parts basically this is how it looks like first you got a bunch of internal domain names to search for and other and aws domain internal domain names so this code is actually from a part in default namespace so you see the default.service the cluster local there um second you got your um service ip of qdns and finally there's n.5 option so this option basically means that there will be many invalid and not unnecessary uh dns lookups basically this is what happens so for example we if you want to resolve a dns called pgsql.backend.xp.com because it has less than five dollars in it we will append all search domains and try them first before an actual dns query happens so why the indus file was chosen was explained in detail in this ticket by tim hawkins first reasons like same name space lookups cross name space lookups and of course cluster federation in the next following slide we will share some of the tunings we have done to reduce those invalid lookups okay these are some of the tunings that we found very important to us first is the cache size of dns mask container i think the default is somewhere around 100 or 200 but you should set it to max with ten thousand unless memory is really constrained in your system well set it into ten thousand only cost you additional couple megabytes of memory um dash dash address flag it is a really big performance booster so this flag tells dns mask to return an ip address for a specified domain name however we use it slightly differently and we specify a whole bunch of invalid domain names that were created by n.5 and we do not specify ip address so effectively for these invalid domain name lookups dns mask will return not found immediately instead of doing an actual lookup this way we speed up things a lot so if you haven't taken a look at your cube dns deployment i recommend this flag and finally if you have some internal name servers you want to hook up the server flag is for you all your parts will be able to resolve internal domain names without additional changes and with that i'll hand it back to ilia a quick word about telemetry uh it's not surprising that we didn't have any containerized services before we couldn't take almost anything from telemetry start to kubernetes except uh splunk and with splunk telemetry team did some heavy customization and tuning for splunk forward to get reliable logs everything else on this slide is uh was new technology to us and i think it was a great thing zero dimension special case for reserve instances for stable service like prometheus and we also love prometheus however running ebs with the availability zones and no affinity for prometheus juggling of it is not fun at all and speaking of ebs it can have some interesting mountain and mountain times so we evaluated rook with a great success and we just didn't risk to put in production before game of thrones season however we excited to see rook become a cncf project he's been submitted to toc okay so for c advisor is one thing to consume metrics from c advisor run a new infrastructure cluster with few nodes uh two cpu cores each and couple jenkins spot deployed it's totally another to run on 300 node cluster with 40 cores each and more than 20 000 containers deployed consuming metrics at that scale felt like drinking out of the fire hose and we had to do some extensive metric tuning filtering and prometheus memory adjustments to get metrics in reliable state so when you know you are ready ready for game of thrones season premiere for us it boiled down by setting up the bar and in terms of threshold viewership and ramp up speed and beating it with the low tests so for about two or three months leading to game of thrones premier season premiere we ran a weekly mega load test and the first attempts were just beautiful it left us in ruins and that's when the real work began it began on both runs on services side a service engineer did some heroic job investigating issues and fine tuning services to accommodate for new environments and on kubernetes side when we began to look for issues reporting if none were found and fixing what we could slowly we began to emerge in better shape gaining more confidence in kubernetes and services running on it if there is any moral to a story after successful game of thrones season 7 on kubernetes it feels it feels good it felt good to be right perhaps for the first time by making the right choice and if there is any advice we can give is trust yourself trust your team succeed the small things and you'll be well positioned to succeed at big initiatives and it won't always be a smooth ride but you and your systems will emerge in better shape than you went in for us many problems we found in our services were not caused by kubernetes they were there all along kubernetes made them more visible so as mentioned earlier we looked in alternatives however the biggest and undeniably most important reason why we chose kubernetes was vibrant and active kubernetes community without all the github issues and fixes without c groups and slack channels without meetups and coupe counts just like this there's high chance that the journey would not end well and will most likely end up like these two guys but likely didn't happen and here we are at kubecon telling a success story of game of thrones season 7 on kubernetes thank you