this course will guide you through the basics of retrieval augmented generation or rag starting with its fundamental concepts and components you'll learn how to build a rag system for chatting with documents explore Advanced Techniques and understand the pitfalls of naive rag Paulo created this course he is a senior software engineer and experienced teacher in this video I'm going to go through a quick introduction of rag rag stands for retrieval augmented generation now if you have never heard of rag no worries that's what I'm going to be doing in this video the main idea is that when you use a large language model but a large language model essentially is a model that was trained on certain um data so for instance if you go to chat GPT and you type in what is the capital of France and of course it will give you the capital of France because it was trained on information about including in this case the capitals of countries in the world but if you were to ask chaj what is the name of my first dog of course chaj wouldn't know because it's using that large language model the model that was trained on something that is not related to your information information that it is particular to you that is specific to you and that of course is an it a problem and and rag essentially allows us to take our own information our own data databases video textol information RW data or unstructured data as they call it sort of inject to the large language model so now the large language model has more information including your own information and so now when you ask questions related to your specific data you are able to get the answer from the large language model because it's able to connect to your data that you have injected happy day so you get the right answer so that is the idea of rag so that's what we're going to be doing this mini course or in this video and I hope you enjoy it all right let's go ahead and get started in order for you to follow along in this course you need to have your development environment setup particularly I expect you to of course have python setup on your machine also vs code or any other code editor of your preference but I will be using vs code so I would encourage you to also use code but that is not a requirement also make sure that we have an open AI account which means uh you also need to create have an API key that way you're able to follow along if you want to actually do the HandsOn with me which I believe I want to believe that's what you're going to be doing so go ahead and have all those things set up and we should be good now for you to set up the open account again you can just go to open.com and go through the process if you haven't done that already ready and just set you up create an account and create an open API key which then we'll be using in this course and if you are wanting to install python you don't have python installed it's very simple just follow this link and they have all of the things or all of the directions you will need to set up python on your machine so I encourage you to go through that and have everything set up okay so they have python for Windows Mac Linux and everything this is all in case you don't have anything set up but go ahead and do that if you don't have that set up and I'll see you next all right so let's go ahead and start doing the Deep dive on rag so I know that most of you who are here may already know what rag is and that's wonderful but I'm going to do just a quick Deep dive overview so that we are have some sort of a summary overview again of what rack is so we're going to look at what is rag the motivation behind Rag and also advantages um now what is rag rag stands for retrieval augmented generation so the key points here is that we have retrieval augmented and generation these are the key points here retrieval augmented and generation so the idea is that we have a system that retrieves information we have also way of augmented whatever we are passing through as well as then push that information into a machine quote unquote that will generate a result so rag has two main components which is the retriever the retriever what it does it identifies and retrieves relevant documents and then we have the generator well it takes retrieve documents and the input query to generate coherent and contextually relevant responses because that is the whole idea to get coherent and contextually relevant responses these are the main components of rag but we still haven't defined rag really so what is a rag so the definition will go as follow a framework that combines the strengths of retrieval based systems and generation based models to produce more accurate and contextual relevant response and we have the keys again the keywords contextual relevant response but that is the whole uh goal of rag okay that sounds great but translating all of that we would say efficient way to customize an llm a model language model large language model with your own data well what that means is what are we doing really is that as we know a large language model like GPT and many others out there they only know so much okay so what we doing is we we are injecting our own data into this large language model so that it knows more than the things that it knows that was trained on so now the large language model is going to know about specific contextual data in addition to what it was trained on let's look at an overview of rag who have documents these documents are cut into small chunks and then these chunks are put through an embedding large language model so to create embedding essentially and then that is what is created embeddings and those embeddings are set okay so now the question or the query comes in goes through the same process transforms to embedding and then we have have this embedding which then is used to go ahead and find in our retrieval system in our Vector database most similar items which then is pushed into a general large language model which knows how to take that information in this case the most similar results with the question this case The Prompt and get the response that is needed that we're looking for so that is how a rag Works notice here when we say rag retrieval augment generation that means that the generated response is augmented by the data I retrieved from the documents in our case hence the name rag so really if you want to do a deep dive into naive rag this is what happens so we have the documents and these documents are going through the phase of parsing and preprocessing so essentially cut them up into smaller documents uh this is the chunking process and then we pass them around into smaller chunks and those are passed through through the embedding model to create vectors out of these chunks okay so we're vectorizing those chunks and then that is what it's saved into a vector store or a vector database so this is the part of indexing that happens here of course is the indexing part as I have shown you this is the part where we cut the documents and preprocess everything and chunk it up and then create those embeddings or vectorize those chunks and save them into a vector store and then what happened is then we have a user who has a query or question of some sort and that also has to go through the embedding model to vectorize that query and then that is actually what is what is sent to search into the vector database so we have vectors and vectors that are easily uh used in a fact database to do all sort of things mainly to search and then the information is retrieved the relevant documents are retrieved or packed up in this case with prompt as well as the relevant documents as I said and the query but notice here this is the different part phase this is the augmentation phase of the rag so we augmenting we're adding something to what we had before so not only we have a query but we also have prompt which is part of the query and relevant documents and so forth okay so once that is augmented we pass that information through a large language model so it could be any kind of large language model and then that's when the response is generated which is then returned to the user all right so now you have the basics of understanding what rag is how really rag works the idea is that we have our own documents we're going to go through the process of extracting those documents splitting those up and then pass them through the large language model of course we're going to be saving that into a vector database now if you don't know what a vector database is I actually have yet another video where I talk about Vector databases somewhere at the top here okay so go ahead and check that out so we're going to do a HandsOn here while I'm going to show you how to use rag to create a system a rag system that allows us to pass through some documents in this case we're going to be a bunch of articles that we going to be reading in saving those to a vector database and then form rag to start conversing or in this case querying our documents so we can ask questions and get the correct answers along with the large language model in this demonstration here I'm going to be using open AI which means that you need to go and have an openai API key for you to be able to do this with me now if you don't want to use open AI you can use other large language models out there and things will be a little bit different of course but the main idea is going to be the same all right okay let's go ahead and get started and so I have this project called rag intro and have a few things here one of the important things is that you have here the openi API key so you need to get that you need to have that then of course I have the app.py which is empty at this point so this is where we're going to start doing our magic now before we do that I need to make sure that I have a virtual environment if you want to learn more about python I have a full video of 1 hour or so that's you can go ahead and check it out also you'll see somewhere here all right or you can search on my channel you will find that all right so I have my virtual environment created there and let's go ahead and say Source VNV and activate that real quick so we have that set up so now it's active we are going to install a few dependences the first one that I need here uh let's see I have my cheat sheet here I have so the first one that we need is the python. EnV so pip install I'm going to pass that this is going to allow us to retrieve information from our virtual environment file okay and then next I'm going to get the open AI because we're going to be using open AI so I say p install open AI okay and because of the nature of large language models and rag system we need to save this information this data that we're going to split up these documents into a vector database if you don't know what a vector database is I do have a course um that talks about Vector databases and so there are many kinds of vector databases we're going to be using chroma DB which is light and easy to use so I'm going to say pip install chroma DB as such so we have that set up for us I'm going to go ahead and import few things that I need here now just to make sure that this is fast I'm not going to type everything because you should have access to this code the OS because we're going to be needing that to access operating system folders and files and so forth and other functions I have chrom ADB and I have EnV here to load all of environment variables and of course I'm going and importing embedding function we're going to use that to embed create embeddings because those are the representations of our data that need to go that we need to have in order to put that into our database Vector database of course we have open AI here which we're going to be using soon right all right so next what we'll do here I'm going to load all of our environment variables and then I'm going to set up the open key in the my from my environment variable as I said from our environment variable from our environment file there and then what we're going to do is we're going to create the function the embedding function this is what it's going to allow us to create those embeddings again once we chop up all of our data which I'm going to show you in a second here we want to transform that into embeddings these zeros and ones Vector space and then that is what's going to be saved into the vector database the chroma Vector database and when you do that when instantiating this embedding function you need to P pass the API key OPI key because it needs to know what model it's going to be used to do that and we're going to pass the actual model name which going to be text embedding three small so this is just a very small light embedding embedding functional system that allows us to embed create embeddings and next I'm going to go ahead and of course initialize the chroma client persistence so I want to be able to persist or in this case I want to be able to save the actual database now looking at the data here you see that I also have these news articles so this is where I have all of these articles news articles that I found online these is what we're going to be using as the documents and then we are going to chop it all up put it into a database right not a normal database this is going to be be a vector database and then we're going to use other techniques to start conversing talking and getting the documents that we need to answer the questions that we are asking so what are we doing here we are initializing the chromer client you can see it's very simple really you say chroma persistent client and then we pass the path we want this to have now I said chroma persistent storage this is kind of long but you can make it shorter if you want and then collection name add whatever name we want and then now we actually say chroma get or create collection which means this function allows us to create the actual collection collection is just a table or document where we can put all of these documents or tables in this case Okay and then we need to pass the embedding function notice that now we are passing what we initiate or instantiated at the top here the actual open AI embedding function that is going to allow us to create those embeddings right vector of the vector embeddings along with the collection name there we go so now we have this collection that indeed we created with chroma all right so let's go ahead and create our client this is our openingi client we pass the API key and the openingi key of course so now we have our client we can do all sort of things meaning we can for instance say client uh dot I think it's chat. open what is that called completions and I can go and create and then here I can pass a few things such as the model I believe let's say model and I going say gpt3 turbo and I can pass messages and rolls and everything in fact let's just do that real quick here so you can see this client working you can see we have the messages and the system says your helpful assistant what is human life expectancy in United States well that's pretty good let's go ahead and see if this works making sure that you have everything set up of course so I'm going to say print actually let me put this say res or res like this and say I can say res that and I can go to choices and go to message and they get the content all right so if I go ahead and run this I should be able to get something so python like this so should let us know that indeed we have everything set up and we should get some result results in a second okay looks like we have some issues here rest choices message let's just go and get the response the whole payload I think I'm missing the actual object it's okay let's run again okay so we can see that we have the response chat completion and went ahead to went ahead and got as of 2020 the average life expectancy in the United States is around 78.8 years old and so forth okay so we're getting a payload to tell us that that this is actually working okay of course you can get to the actual payload if you want this content here if you go through I can go straight to content I believe like that okay same thing go ahead and run we should get the actual content I think okay there we go so now we get the actual content as of 2020 blah blah blah and so forth okay so at least we know that this is working that's all we really wanted that's not our end goal so I'm going to get rid of that okay so the first thing we need to do is of course to load our documents from our articles as you can see here we need to load all of them and then start doing something so I have all of the code already so that we don't spend too much time and I can and you'll have access to all this code anyway and what I'm going to do it's a function that I created before this so what are we doing here we're loading documents from a certain directory so I have a print statement here just to give us what's happening and I go through in this case I know that all of these articles documents end with txt they are txt files it's kind of hard to see you just have to believe me you can see here there are txt files okay so that's what we're doing we're going through and start loading all of them and return the actual documents right it's going to be a list of documents all right so the next thing is we need to split this documents once we get them we got to have to split them up so that then we can pass them through into our database so I already have a function that does just that you can see we pass the split the text and we say the chunk size 1,000 and the overlap is 20 the overlap essentially says once we split these documents we want to make sure that they overlap right overlap like this that way the contextual meaning of each piece of text is overlapped which means it's kept the context is kept because once we split all this up these documents in small chunks they're going to be very distant so the more overlap we have you can see that the more context we'll have kept the less the less overlap so that is the idea really and so we just go through the splitting process and then we return the chunks okay so we have those functions we're going to be using soon and so now we're going to go ahead and load documents from the directory so the directory as you see here is uh let's see right I should have said news articles okay so I'm just going to remove this like that should have put under data but it's okay so is under news articles which is this guy here and it's going to go ahead and get all of them and then for your documents I'm going to go and then I'm going to call load documents from directory pass the path which is this one here and at this point we should have all the documents I'm going to go ahead and print real quick here so we can hopefully see we should have the length of the documents once we have those documents loaded right so in this case here you know that this should return a list of documents because documents is indeed a list okay we should have something so let's go ah save this and I'm going to quickly run again okay loaded 21 documents very good so it went and got all of those documents that we loaded in essentially all of these documents here so they're about 21 now once we have these documents of course now it's time for us to do something else what we need to do really is to get those documents split up so I'm going to go ahead and do that so now I create the split documents into chunks so I have a list and I go through those documents that we just received and then we call the split text split text is indeed what we have here so it's going to go through and return another chunks which is a list of those all those documents that are split but remember we have this overlap for each document to continue having the context okay very good so we do all that stuff and then I can go ahead and say print again split documents so this should give me all of the documents chunk split documents into the length of chunks okay because I should have something at this point let's run again okay so you can see it went through the process splitting docks splitting docks and this is telling me how many splits I got because that is what indeed I asked length and so forth okay so we know this is working which is essentially what we want okay so the next function I need here is a function that will generate those actual embeddings because remember once we split up all the documents we need to take those splits that we did here and create embeddings this is what's actually saved into our database into our Vector database so I have a function here that is going to be helpful for us to use so essentially what this does is we use the we use open AI to create those embeddings from the text okay that's what we're doing here so you can say client embeddings and create and we pass the text the pieces that we are putting through and then we say the model that we want to use to create those embeddings that's all we're doing here and then we get those embeddings and return them okay this is going to be help full in a second here then I'm going to generate the actual embeddings why because I already have the function so to generate embeddings we go through all the chunked documents yes you remember these guys here and then as we go through we call these get open ey embeddings and we pass the actual information through to then create a document embedding field so each time we go we actually creating those embeddings so we can actually print to see our embeddings so I can say Doc embedding and what will happen is let's go ahead and run real quick so you can see we're going to go through the whole process splitting and look at that it's creating those embeddings we'll take a bit and in a second here we should see actual embeddings so after a little while so be patient but this will take a while you can see now we have the embeddings for Vector spaces right and so there we go of all the documents now we have all the embeddings this is these vectors that actually we're able to then add to uh into the database so this is good so I'm going to go ahead and clear this so I just wanted to show you okay so now that we have our embeddings let's go ahead and comment this out so we don't run that let's go ahead and insert each one of these embeddings into our database okay so of course I have the code for that so for each one of these we're going to because we know this chunked documents which is what we have here has all of the information what we do now is that we going to get those chunks the real chunks before we embed anything add into our Vector their base and at the same time we're going to add the actual documents along with the embeddings so now we're going to have these chunks of the documents these little pieces not embedding these are just the text chunks and then we're going to have the actual embeddings they are going to be sitting on our database ah very cool then and we're going to create a function to query our documents so I have all of that and I'm going to just copy that and put it here so we don't have to do all the things so the idea here is that query documents now this query documents is very simple we pass in the question like tell me about GPT 4 something like that anything pertaining to our data that we've just saved okay and then we say how many results we're expecting to receive how many documents essentially because what will happen is we're going to be able to retrieve the documents corresponding to the query that we're passing in right so in the background what will happen is the database is going to be able to go and search do similarity search until it finds what is congruent with the question that we have inserted that's pretty cool right and so we say in collection. query passing the question and the number of results we want documents that we want and then we put in a variable and then we stract the relevant chunks from that uh from our list of documents because this result here is going to have the list of documents that's why we can go through those documents and get the relevant chunks and then once we have them we just return those relevant chunks I have this other code here you can check it out this is going to just give us the distance between the relevancy so essentially tell us how close to the actual answer this doc doents are okay and you can play with that okay so next what we'll do once we have this done I'm going to then of course have a function that will generate the response because think about it we have taken the documents that we have we Cho them up we put them we created a vector database and then we put inside of that Vector database but before that we were able to create embeddings because we want to save those embeddings CU it's easier for the search store happen right for the right document once we ask the question and so now we want to generate the actual response so now we are going to use the large language model again open AI in this case to do all the work with all these pieces that we have right now so as you can see here we pass in the question and we need the relevant chunks right we are taking the relevant chunks that we created we were able to query the database right and then we are passing that along in this with the question so now we have the question our question asking questions about these documents right whatever and then now we have this only relevant chunks that were passed through the large language model and the large language model now has more information or more knowledge of what we want to get the answer from right and so that is what's happening so here I'm creating the context essentially I'm getting this relevant chunks and joining in with other stuff and then I have a prompt here a prompt for our large language model to say hey this is what you need to be aware of when you are answering these questions you're an assistant for question answering tasks use the following pieces of data blah blah blah retrieve context to answer question if you don't know please say I don't know and things like that now prompt is actually its own thing and you have to be really good at prompting to get the right result from large language model and then of course we pass that context and the question we need to pass those two things right so now the large language model will have the question that we asking we typed in and then we'll have the relevant documents because we've parsed that through already you see okay and then we call the create again we go to the client chat completions and create of course the actual now we are going to the actual model say hey here's the information go ahead and give me the answer that's all we doing here okay we pass in the prompt and as well as the question and then we get the answer this is what this will return so now it's time for us to check out to see how this will work so now I'm going to have here this query that I'm going to start doing here so here example question tell me about AI replacing T TV writers in strike now I know this question that would work because in one of these documents here we talk about AI replacing jobs and so forth and so I'm going to see if this works so here's what's happening I have the question and I need to get relevant chunks right from the document in this case from database well I call the query documents and I pass the question so it's going to go ahead and query the documents that are in dator base finding documents that are relevant to the question that we asking which is this here right and once we get this relevant chunks we're going to need that along again with the question the first question and the relevant chunks we got to get the answer right because this generate response here this is where we just talked about is going to go ahead and pull in the context relevant chunks as well as the question create a prompt and then pass that through that prompt and then call the large language model to then ask answer that question and we get that answer okay let's see if this works and then we're going to print the answer here all right let's run this again and see if this works so you're going to go through the whole process of course it will generate everything and one thing also in the beginning of of course we'll go through the process but once you run once because we will have that data and everything uh we should be able to just comment out the first part of this code essentially so everything is good but in any case everything will still work should work because we have that data already the other thing you will notice is that now we should have you can see now we have this chroma persistent storage which is indeed the chroma SQL I3 which is the database that we created this is the this is the actual chroma database pretty cool pretty cool indeed okay so we have that set up so this will take a little bit of course okay now it says retrieving relevant chunks and voila says here TV writers are currently on strike due to the Writers Guild of America demanding regulations and the use of um let's see on the use of AI in writer's rooms so the writer's Guild blah blah blah so all this information actually pertains to the articles that we have here so AI replace TV writers if I click here you will see that indeed I should have something related to that let's see okay regulate the use of AI and clear work and so forth so it goes ahead and looks at the correct other ones here that relate to exactly that so for instance I can go ahead and ask something else let's see let's say something about data bricks okay so them say tell me about data bricks and let's let's go ahead and go through the process it went through the whole process inserting chunks into DB blah blah blah went through all of that we know that and at some point it went ahead and say returning relevant chunks and then of course we hit the large language model and says dat bricks is a data AI an AI company that recently acquired ocara blah blah blah and so forth so just like that we're able to take the information our own data in this case here this could be anything our own data in this case and we pared all that through we extracted everything we created these little chunks of this data and then we used opening eye API to actually create the embeddings that's very important and then we save that into a vector database this is very important also this is not a normal database is this is a vector database and then we're able to search that Vector database according to the question that we passing through it and then we got the right chunks right and then we pass those chunks of documents and the question and pass that through the large language model and then we're able to get the answer this is the power as you see here because now we're able to take our own data and sort of inject into the large langage model so that we are able to ask questions about our particular data and I hope you can see so many use cases that you can use this particular uh rag system here to help you with analyzing data and so forth all right so now that you have the basics the fundamentals of rag how to create a simple rag system that allows you to converse or chat with your own documents so essentially injecting some information your custom information your data with the large language model so you can converse and start talking chatting and getting response conveniently response that is attached or that is congruent with your own data okay so you know how to do that and of course now you know how to create of course a database a vector database which is very important for us to be able to save those pieces of information of our data data documents in this case and then save all of that information along with the embeddings which is really important because all of this is being saved in a vector space which is easier for the vector database to be able to find things faster things that have meaning and relevancy if you don't think about you think that this is where it ends but obviously this is not where it ends because you will see that rag as it is right now we call it naive rag it has its own pitfalls and so now we are ready to move forward and learn of these pit FS that rag naive rag which what is what we've been doing the pitfall that it has and then we're going to learn and Implement a technique or certain techniques that will take our rag system to the next level okay so we can actually get consistent results because as it is you can get some results that are not very consistent and results that may not be necessarily congruent or related semantically with your query and that is what we're going to be touching on next okay let's go ahead and do that so naive rag is the most simple straightforward way of dealing with large language models so essentially it has indexing retrieval and generation so indexing it is the process of cleaning up and extracting data from the documents just like as we saw earlier and then we have the retrieval so this is the part where we turn questions into a vector a vector space and that's what use for comparison which allows us to retrieve closely related chunks which then are pushed with the query into the generation phase which then the query choose chosen documents are combined into a prompt so we have prompt the query and the uh documents that were chosen push to the model to generate an answer so this is the naive rag so really if you want to do a deep dive into naive rag this is what happens so we have the documents and these documents are going through the phase of parsing and preprocessing so essentially cut them up into smaller documents uh this is the chunking process and then we pass them around into smaller chunks and those are passed through the embedding model to create vectors out of these chunks okay so we're vectorizing those chunks and then that is what it's saved into a vector store or a vector database so this is the part of indexing that happens here of course is the indexing part as I have shown you this is the part where we cut the documents and preprocess everything and chunk it up and then create those uh embeddings or vectorize those chunks and save them into a vector store and then what happened is then we have a user who has a query or question of some sort and that also has to go through the embedding model to vectorize that query and then that is actually what is what is sent to search into the vector database so we have vectors and vectors that are easily uh used in a V database to do all sort of things mainly to search and then the information is retrieved the relevant documents are retrieved or packed up in this case with prompt as well as the relevant documents as I said and the query but notice here this is the different part phase this is the augmentation phase of the rag so we augmenting we're adding something to what we had before so not only we have a query but we also have prompt which is part of the query and relevant documents and so forth okay so once that is augmented we pass that information through a large language model so it could be any kind of large language model and then that's when the response is generated which is then returned to the user so this is how naive uh rag works works as wonderful as this sounds and looks uh there's some issues with this well naive rag has some challenges some pitfalls some drawbacks the first one is that we have limited contextual understanding so for example if a user asks about the impact of climate change on polar bears so a naive rag might retrieve documents broadly discussing climate change on polar bears separately but will f fail to find the most relevant documents discussing both topics in context okay so that is a problem because naive rag models often retrieve documents base solely on keyword matching or basic semantic similarity which can lead to retrieving irrelevant or partially relevant documents and then of course we get inconsistent relevance and quality of retrieved documents so what that means that the quality and relevance of the retrieved documents can vary significantly because navag models uh may not rank the documents effectively which leads to poor quality inputs to the generative model and the third one is that we have poor integration between retrieval and generation what that means is that uh in naive rag systems the Retriever and the generator components often operate independently without optimizing their interactions so you can see two things are work independently they're not optimized to work together that's a problem so this lack of synergy could lead to suboptimal Performance where the generative model doesn't fully Leverage The retrieved information so as an example the generative model might generate a response that actually ignores critical context that was provided by the retrieve documents which in this case will result in generic or offtopic an ERS also we have inefficiency uh handling of large scale data naive raich systems May struggle with scaling to large data sets because of its inefficient retrieval mechanisms which leads to slower response times and decreased performance for example in a large knowledge base a naive retriever might take too long to find rant documents or even miss critical information due to inadequate index and search strategies we also have lack of robustness and adaptability the issue here is that naive rag models often lack mechanisms to handle ambiguous or even complex queries robustly so they're not adaptable uh to changing contexts or uh user needs without significant manual intervention you can imagine if a user query is vague or multifaceted a naive rag might might retrieve documents that partially address different aspects of the query but fail to provide a coherent and comprehensive answer okay so these are some of the main drawbacks or challenges or even pitfalls uh of naive rag okay let's go ahead and break it down uh each one of these pitfalls of naive rag so we can have some better ideas okay so let's look at the limited contextual understanding as we focused on in the last uh lecture how does that even look like again we understand in naive rag limited contextual understanding focus on keyword matching or basic semantic search in this case retrieving irrelevant or partially relevant documents so essentially as I said we have a query for instance that says uh the impact of climate change on polar bears and so the idea is that that question is transferred through the whole process this naive rag which we don't have to go through the whole details what happens there as we know it so this is just an illustration goes through all of that and then it just retrieves nonrelevant docs on both topics okay so that is the limited contextual understanding that is lacking which is one of pitfalls of naive rag okay because naive rag models often retrieve documents based solely on the keyword matching or basic semantic similarity which you can see here can lead to retrieving irrelevant or partially relevant documents nonrelevant docs on both topics now the next one we talked about is the inconsistent relevance and quality of retrieved documents so the problem is the same with naive rag we have this plethora of varying in quality and relevance documents which means we have poor quality inputs for the model again the same example we'll have a query that goes through and that says latest research on AI ethics for instance and so that goes through the naive rag process and then we may end up getting outdated or less credible resources because the quality and relevance of the retrieve documents can also vary significantly so naive rag models may not rank the documents effectively which in this case will lead to poor quality inputs for generative model which in this case you know will get very poor results the next one is that we discuss is the poor integration between retrieval and generation and so we know that because of the nature of naive Rags the retriever I should say and the generator components often operate independently they are operating alone without optimizing the interactions within themselves which as you know if two systems are working not in conjunction the Synergy can lead to suboptimal Performance which means we end up with very unoptimized documents or information that is being passing through so again we have in this case the retrieved documents and these retriev documents are of course used through the naive rag system and what happens that is that we may lose what's actually important what is relevant and we end up with very generic or even offtopic answers through this whole process because these two systems are working the generator in this case generator and the retriever are components that are working independently without optimizing their interactions and the next one is inefficient handling of large scale let's look at the overview the idea is that using naive rag systems uh these tend to struggle with scaling to large data sets because of the inefficient retrieval mechanisms which lead to very slow response times and of course decreased performance because it takes too long in larger scale data of course sets and so forth to find relevant box which of course leads to missing information due to bad indexing essentially so essentially it's the same thing we have the index process which creates of course these index uh data structures which then in a grand scheme of things is going to have trouble trying to find the right information in a very large knowledge base if you have a lot of documents and so forth because it's taking too long and of course it may end up missing critical information because of this inadequate indexing and search strategies okay so going to the lack of robustness and adaptability this is another one models often lack mechanism to handle ambiguous or complex queries so remember that queries are not always very direct so you can have a query that is loaded per se it's ambiguous it has more information more questions that it's a little bit complex so with the naive rag they don't just have the mechanisms to handle that because they're not adaptable so that is the problem here's an example so we have a query tell me more about index funds and anything related to finances so this is a little bit loaded this query isn't it and you can see that naive Rags don't have a way of deciphering through this query to get to the bottom of the actual query or um to get the correct coherent and comprehensive answer so these are the drawbacks uh doing a little bit of a deep dive into each one of the drawbacks or challenges uh or pitfalls of using naive racks so in summary naive rag have of course some pitfalls as we've seen and we can subdivide those pitfalls in two categories we have first the retrieval challenges so in this case they lead to the selection of misaligned or even irrelevant chunks which of course leads to missing of crucial information which as you know it's not a good thing and then we have the other side which is the generative challenges so under this umbrella we have issues that the model might struggle with hallucination and have issues with relevance toxicity or bias in its outputs so these are the two umbrellas I would say of the drawbacks that naive frag brings to the table retrieval challenges as well as generative challenges now let's talk about the solutions here in this case we are going to go from the naive rag that we've looked at and it's pitfalls now we're going to look at Advanced rag techniques that help us make this whole process that uh makes this whole process more efficient okay for our Rag and so Advanced rag techniques and their Solutions now let's look at the first one now first let's look at advanc Rags benefits what is it important the beauty here is that they introduce the specific improvements this plethora of improvements that allow us to overcome the limitations of Na rag as we've seen previously the main goal here is that with Advanced rag benefits we are focusing on enhancing retrieval quality so Advanced rag employs the following strategies the first one is preretrieval so idea is to improve the indexing structure and users query because that is very important the first step if you don't have good indexing structure and users query then everything else falls apart and also we're going to improve the data details in this case organizing index is better adding extra information augmentation part of it now and aligning things correctly and then we have the second part which is the post retrieval at this stage we are combining what we got from the pre retrieval stage okay the data that we got there with the original query so we're combining all all of that to finalize this augmentation part of the rag in this case we're going to be we could be reranking to highlight the most important content or doing all other techniques goes ahead and to enhance our retrieval processes now there are many Advanced rack techniques out there have done a lot of studies and written papers that you can go and take take a look uh one thing I'll will let you know of is that as you go through all of these other different techniques even Beyond this course you will realize that most of them tend to overlap and sometimes the namings can also overlap and that's okay but the main idea is the same which is having techniques that allow for a better workflow of our rack systems so first of all I'm going to look at here is the query expansion in this case going to be with generated answers what does that really really means is that we're going to generate potential answers to the query in this case we're going to actually use a large language model to get relevant context so with the query expansion as an advanced retrieval technique uh it's used to improve the relevance again keep in mind these keywords it's used to improve the relevance of search results by augmenting the original query with additional terms or phrases because these additional terms are selected based on various strategies such as synonyms expansion related terms or contextually similar words so the goal is to really capture more relevant documents that might not match the original query terms in this case will not match them exactly but are semantically related so to show you here here's a simple diagram so you have a query and this query has to go through a large language model to create an answer this is a I would call an hallucinated answer okay and then we take that answer that's goes through the vector base and then we concatenate the answer and the original query and use that as a new query which you pass through the vector database in this case retrieval system and then you return your query results which was passed through the large language model again to get the actual results the answer so that's all we're doing here is that we are using the large language model in this case to hallucinate a little bit on that first query and then we concatenate the original query or in this case the answer that answer that we got right and the original query and use that as a new query which then we pass through the fact database the retrieval system and then return the query results again pass those through large language model again to get the results so what happens here is that with this expansion here we have a few use cases right this query expansion with generated answers we can use that in the information retrieval can imagine with this system now we can enhance the effectiveness of search engines because we're providing more comprehensive search results and also in question answering systems because now we're improving the retrieval of relevant documents or p messages that potentially help in answering user queries in ecommerce search because increases the accuracy right and relevance of product search by expanding user queries with related terms academic research it makes sense because now we are able to find more relevant papers by expanding their search queries with related scientific terms and Concepts okay so let's go ahead do a HandsOn so essentially again this is diagram we have the query pass that through a large language model and we get an answer so a hallucinated answer and that's okay but that's going to be used of course concatenate with the answer and the previous query and we get some results for our from our Defector database which then we pass all of that into a large language model and we get an answer so in the next video we're going to see how all that works in code okay so I have a project here here you should have all access to all of this code and data and everything and at the top here we have this data folder which has this Microsoft annual report okay so if you click on that it's just a large PDF with about 126 or 116 pages so it talks about Microsoft annual report 2023 okay so I just found this and I thought would be a good idea to use that for our demonstration here okay so we'll have access to all of this and also make sure that uh you create a virtual environment on your project so this works as well as having in this case an open AI API key you should be able to go and create an account with open Ai and get a key so if you want to follow along and so forth I've got this helper utils other utility methods and so forth okay all right so the first thing let's go ahead and do some importing here or installing some dependen es first I'm going to say pip install chroma DB because that's what we're going to be using and next let's pip install Pi PDF because we're going to be using that as well to read a PDF file and extract everything and let's go and say pip I'm going to install open AI okay so let's go ahead and create a new file here let's call this XP say expansion insert that py okay all right so I already have the code so I'm going to just get parts of it and we go from there first I'm going to add some imports here get PD Pi let's make sure that everything is set up okay we got the our PI PDF PDF reader we have open the eye all of that and I'm going to set up the environment variables as well well so open your IPI key make sure you have that set up inside of your. EnV file it's very important first let's go ahead and read our Microsoft annual report it's under data can see it's under here okay should have access to all of that so that's what we're doing here and then PDX text I'm extracting everything from the pages filter the empty strings real quick and while I'm here let's go ahead and run this real quick to see if this works I'm going to just just print this PDF text that we get here filter out the mty strings and I'm passing through the word wrapper so we can see so I'm going to print that out let's see what's going to happen let's go ahead and run this okay so I need to import Panda as pandas so that's something okay make sure you import pandas as well let's go ahead and run okay so we can see that we're able to extract our PDF and we get all of that response so this is really good okay so now that we have the information that we need which means we extracted the text I'm just want to comment this out we extracted our PDF we got our that information from our document what we need to do next is we want to be able to split the text chunks and to do that we're going to use um L chain which is a framework that allows us to deal with with large language models and do all sort of things okay so uh first let's go ahead and get that so pip install link chain lank chain splitter as such so I can go ahead and get the recursive character splitter as well as the sentence transformance token text splitter because we're going to need those two before we uh get everything and start embedding everything okay so let's go ahead and take care of splitting our document using the recursive character Tex splitter which we are passing the chunk size th000 and chunk overlap of zero and then here we are uh splitting the text by getting all of the actual PDF text that we have created from here okay so taking that this is where we're splitting it and we're going to get the character splits so I can also come here back here I'm going to print the character split text all right the 10th one and then I'm going to show the total chunks so I'm going to save this and let's do a quick run okay so we can see that we have indeed the information that we wanted to see this word wrap of the character splits right in this case just the the 10th one and we see that's the one that we got and then we have total chunks is 410 chunks that were splitted that's very good next we use the sentence Transformers token text splitter to split the text into chunks of about 256 tokens and then we're going to set the chunk overlap to zero okay now the reason why we need to do this is because we need to be mindful of the token size for limitations that the large longer Mar models could impose then we'll be able to use to generate the embeddings which we need Okay so let's do that so here I'm creating the token splitter by using the sentence Transformers token text splitter I'm passing overlap zero and the tokens per chunk 256 and then we're going to go ahead and actually start doing the splitting so here I create the text token split text and I Loop through all of the pieces and I split them up and now let's go ahead and print some information out so we can see the difference here so I'm going to save this so notice the first one the total chunk when we first did it all right about total chunks is for 10 and now you can see the difference that for the chunk the total chunks is going to be a little bit elevated which makes sense okay so looks like good not import sentence Transformer from python packages need to be order sentence okay uh yes I need to go ahead and install that's right so let's go ahead and say copy that and first of all say pip install send this Transformer so we have that okay we have that let's run this okay so first we have the 410 total chunks but next we should have a little bit more because of what we just did okay so you can see it's a little bit more now 419 as opposed to 410 so very good so that is the total chunks of the splits that we did here okay this is the token split text okay so now it's time that we have since we have all the tokens we can go ahead and start the embedding process so first of all we're going to import chroma DB and then we're going to use chroma DB and get the embedding function we could have used some other embedding functions using open Ai and so forth but I'm going to use the uh send Transformer embedding function for this demonstration here so we going to import that and then I'm going to actually instantiate the sentence Transformer embedding function as you see here and just because I want to see I'm going to go ahead and print the token splits token because come as a list the 10th one here the 10th chunk to show so we can print the actual embeddings of the 10th chunk okay so just to avoid having a lot of things printing out I'm going to just comment out all of these other that we had before okay so you can see now we have the embeddings for the 10th chunk okay not very uh useful but at least we see that things are actually working so the embedding function is indeed working okay so we know that the embedding function is working so we no longer to need to see it so I'm going to go this out going to leave it there and next what we'll do is we're going to instantiate the chroma client and create a collection and give it a name and passing of course the embedding function so that we can embed uh each one of of these items that are passed along so chroma going to create a client and then create a function in this case I'm going to just give this Microsoft collection you can name whatever you want and then embedding function it's going to be passing the embedding function which is going to be attached when we create the chroma database okay and once we have that we can extract embeddings of the token splits by just going through like this okay and then we are going to use that where we get all of these embeddings of the token splits and that's what we're going to be adding to our collection right so to do so I'm going to just copy this to do so we do this so we say chroma that add and we're going to add these IDs as we go through and the actual documents so the documents going to be the token splits right and I'm going to just say chroma uh collection count and for now let's go ahead and print just say count and say count like that so we can see okay so we can see the count is for 19 so essentially what we had before very good so at least we know that it's working okay so I don't need to do any of that and so what we'll do next is we're going to create the actual query we've extracted the document split it all up and then added that putting that through an embedding function so we have the embeddings and then we added all of that inside of our chroma DB okay our Vector store so I'm going to have a query here what was the total revenue for the year for instance right and then what we'll do is we're going to query using the collection so collection the query we pass the query text which could be we could pass more than one query that's why we pass as a list and then the number of results that we want to get okay and then retrieved documents I'm going to say results and get docment as such and so what I will do is for us to be able to see I'm going to Loop through all these documents so we can see what we get let's run again all right so you can see that we got some of the documents okay so very good revenue and this year and of course these are split up and that's the whole idea is that they may not make sense or where they start but you can see we have one two three four and a few documents that we getting here all right so this is indeed working so now we know we're getting all the documents so it's a matter of trying to figure out okay how do we see how this technique Works meaning that the first query and we got the documents that we've just retrieved and as well as creating in this case we're going to generate the actual queries right the augmented query as we see because that's the whole idea of this technique okay first thing is I'm going to uh create a function called augment query generated so so this is where I'm going to use the augment query generated to generate an actual answer so we're going to use of course a large language model in this case open Ai and generate this actual answers or one answer in this case so first of all I'm going to have to create a client for open AI so I'm going to copy that and so this augmented query generator what it does is allows us to pass the query and we're setting up the model to the GPT 3.5 turbo you can change that if you want and then I have the prompt here that you are a helpful expert Financial research assistant I provide an example answer because we want an answer to given question that might be found in a document like an annual report so we're prompting it making sure that it knows what to do and then we pass the message here for the prompt here for the system is where we created here to know this is what you need to do what he need to be knowledge knowledgeable uh on and then passing the query so the query is going to be the question the first query that we pass along okay and then we use of course uh the completion API that create pass it the model and the message and we get that response so essentially here so this is where we are looking at this part here so we're going to generate this one answer which we then we're going to take with the query the answer we got go through our uh DB and we got the query results and then put all of that together into the large language model and and get the answer all right so we have that function which we'll be using soon okay so next what we'll do here is we're going to use create an original query here for instance what was the total profit for the year and how does it compare to previous year so this is the original query and then I have this hypothetical query or answer I should say and so I'm going to use the augmented query generating the function and I'm going to pass the original query which is this one here and then I'm going to join those two queries right the original and the hypothetical answer answer so Regional and hypothetic answer and I'm going to print them out so we can see how those will look so I'm going to go ahead and run this you can see here we have this what was the total profit for the year and how does it compared to the previous year very good but then we have this total profit look at this we got an answer so we hallucinated we used large L model to create that answer the answer is the total profit for the year was 10 billion and so forth and um by increased sales and successful cost cutting initiative implemented throughout the year very good so now we have the query and we have the answer that was hallucinated right that was created which was part is part of this technique so now we are golden because we have those two and so once we have those two it's time for us to then pull all of that into our chroma collection and query using those two disjoint queries right right in this case I call joint query but it's original query and the hypothetical answer so I'm going to say chroma collection I'm going to put in a variable result because we're going to use that to just retrieve the documents right so I can say query I can pack query text what is the query text well in this case going to be joint query because that is what I got so the original query and the hypothetical answer which is what we just received from the large language model now you could have created yourself a list of or one question for that answer but it's always nice to use the large language model because then it can create something that is more useful you don't have to think about it all right so now we have this joint query which is that and then we say we want Five results and most importantly we want to include documents as well as embeddings in our results all right because you will see that the reason why I want embeddings you will see because we're going to use actual those embeddings to create a graph a graph that will show the relationship of those embeddings visually so we can see how this works all right so then we have retrieve documents in this case I'm going to go ahead and show those retrieve documents all right so I'm going to print them out let's run and there we go so so the idea we are getting the documents that were passed through in this case we have the query uh the original query as doents that we receive as well as the generated answer put them together and then we pass them through our collection to say okay this is the query text The Joint query and we want Five results so include documents and embedding so we have all the documents that are related to the query so we are seeing the query results from the answer concatenated with the query the original query go through the factor database and we get the query results so now that we have our retrieved documents which is exactly what we want we want to project this data set um on a nice graph so we can see the relationship we can see the improvements that we have uh in the documents that we get that are related to the original query and for that we are going to import a few things here so I'm going to command V I'm going to first of all create embeddings go to the chroma and we've seen this before right I'm going to get the embeddings because we're going to be using those and then I'm going to use the umap library to actually create the projections and for that we need to install say pip install umap dlearn Okay this Library okay it's all done put import um map so we can use it at the bottom here and I'm going to say from hper TS import project embeddings there we go okay so now we have this projected data set embeddings now not notice that we're getting these embeddings here from our chroma collection and because when we saved we saved also embeddings along with other information okay so that's the beauty okay so I'm going to use project embeddings and this is a function from our UTS as well the um map transform so next what we'll do we're going to retrieve the embeddings okay as you see there so I'm going to retrieve embeddings from our results which is our collection query there and then I'm going to get the original query embedding by calling the embedding function and passing the original query that we saw before and getting the augmented query embedding this is for us to see the differences of the results that we get between original query embedding in this case the original query and the joint query which is the original query uh the query plus the answers that we hallucinated to put together okay that's all we're doing here and then we are going to create some projection variables here so here I'm creating an object to project original query embedding by calling project embedding ings and passing the same thing the query embedding for the original and passing to map Transformer and doing the same for project augmented query embeddings here we're passing the actual augmented query embedding which is what we have here okay and then projecting the retrieved embeddings so I'm just projecting them all right and next we're just going to import Matt plot lib like such we don't have that let's go ahead and pip install so that we're able to see the figures see the graphs and everything okay once that is all set up then I'm going to go ahead and just plot everything so now here I'm plotting the projected query and retrieve documents in the embedding space so this is going to be just a 2d and you know that embeddings uh the vector space can be M it's multi n is multidimensional so but we're going to use 2D to facilitate so we can actually see the differences here that's what I'm doing here scatter and creating different uh points in our graph and then I'm going to go ahead and show graph okay let's go ahead and run this real quick so you can actually see what we're talking about so this is beautiful so you can see here this is a twodimensional Vector space that shows all of the information that we've plugged in so what we have here is we the Red X as you see here is the original query and all of these gray dots are the data set embeddings now the retrieved documents are close goes to the augmented query what is the augmented query well this is the augmented query the orange x is the augmented query which contains as you remember the hypothetical in this case the hallucinated answer and the original query and then these Green Dots or circles here these are the retrieved documents so you can see that the retrieved documents are close to the augmented query in the embedding space which is Improvement whereas here is the original one and we are really far away from all the retrieve documents so that tells you that indeed this technique works so the bottom line here is that the augmented query is closer again to the retrieved documents than the original query in the embedding space it tells us that using hallucinated answer which we hallucinated meaning we asked the large language model to create that answer first beforehand was part of the query to improve the retrieval results because the augmented query is closer to the retrieve documents than the original query in the betting space looking at this you can see that is quite an improvement now again this is not perfect of course but you see that we can use it to refine the results that we get from our previous naive rag using this technique here then we get something that is a little bit B more closer to the actual documents right the most relevant documents that were retrieved so what I need you to do is to ask a different question and see how this graph will show the improvements uh between the original query and the augmented query okay so we just saw how to use the query expansion with generated answers as a advanc as an advanced rack technique to generate potential answers to query using a large language model and to to get relevant context so essentially we have the query and then we pass that through the llm to generate an answer and then we combine all of that so the query original query and the answer that we received to get query results and then of course that helps to pass through the large L model and to get answer to see how in a 2d dimensional space how the documents relevant documents are closer to theed query instead of the original query now what are the query expansion use cases well this can be used in the information retrieval system or in the question and answering systems as well as ecommerce search and also the academic research as we've seen so what I want you to do next is to create different queries and look at the differences that are plotted so you can start seeing how query expansion with generated answers works it's a really good technique obviously it's not perfect but you can see the difference in many cases that are not negligible at all there are really good differences that perhaps you can see the power or the usability of using this query expansion as an advanced rack technique okay so now let's look at the quer expansion again but now with multiple queries so the idea is that we use the large language model or the model to hallucinate or generate add additional queries that might help getting the most relevant answer just like before we have a query and we pass that through the large language model and then we actually get more queries so essentially we use a large language model to suggest additional queries and then we retrieve results for original and new queries through the database or vector space and that's what we actually pass through and then we send all those response to the large language model for a final relevant answer so that is the overall workflow how the core expansion with multiple queries works so essentially we have more queries instead of one answer in this case we're actually creating queries as opposed of creating answers in an ed shell we have the original query analysis part that happens so we're analyzing the original user query to understand its intent and context and then we have the subquery gener ation so this is where we generate multiple subqueries that expand on different aspects or interpretations of the original query so we have breath of different subqueries now this can be done using synonym expansion related terms or contextually similar phrases you could have in your organization or in your business you could have a list of these subqueries but we're going to use the large language model to hallucinate those subqueries and then we have the document retrieval so now we retrieve the documents for each subquery separately that is the beauty here and then we have the combination this is the aggregation side of things where we combine the documents retrieved from all subqueries that way ensuring a diverse and a more comprehensive set of relevant documents and then of course the response generation this is where we use the aggregated documents to generate a more informative and contextually relev an response here are some use cases for quer expansion exploring data analysis so this could be uh a way of helping analysts explore different facets of data by generating varied subqueries uh we have academic research again we've seen this before to provide researchers with different angles on a research question by generating multiple subqueries customer support this helps in covering all aspects of a user's query by breaking it down into small uh specific subqueries Healthcare information system so helping uh with retrieving comprehensive medical information by expanding queries to to include related symptoms or treatments and diagnosis let's go ahead and put all together in code so I put together this expansion queries atpy it should have access to all of this anyway and let's get started so I went ahead and did some imports so most of this actually is going to be exactly the same as we had before but I'm going to go through again just to for completion Okay so we've imported everything we have the open AI client and everything is good and first we are going to do what we did before which is to read everything through uh using the PDF reader to get our data okay so we get our data Microsoft angle report PDF and and extract those texts okay from the page and we're going to filter the empty strings as you see here okay so I'm not going to run this again and then I'm going to split all of those pieces into smaller right chunks as you can see here and I'm using the recursive character text split from Lang chain and we're going to always be using the sentence Transformers token text spater as we saw before so essentially really the same thing that we did before okay use the sentence Transformer to get split into tokens as you see here that's what we're doing we have the text splits and we have all set up in this list here we are looping through and adding that to that list of token splits and next we are going to import chroma DB and all of that again so I know this is kind of Overkill but I'm going to put all of this for completion okay okay I'm going to so now we're creating our embedding function using the sentence transform embedding function and so we instantiated our chroma DB CLI okay and all of this is good next we're going to add all of those embeddings into our chroma DB but first we're going to extract the embeddings of the tokens as we did before and we added all of those okay we have the count there as well and now I'm going to add the query pretty much the same as we saw before so what was the total revenue for the year pass that query through our collection to see what we get so collection query we pass the query until here let's go ahead and make sure that we actually getting the documents so I'm just going to Loop through all the retriev documents and see okay very good the same thing we've seen before we're getting all of our documents at least five 1 2 3 4 and five next let have a multiquery function that will be responsible for generating the multi queries so the difference here is that in our prompt in this generate multiquery we still pass the actual query and the model is defaulted at GPT 3.5 turbo and here the prompt here we saying you're a knowledgeable Financial research assistant and your users are inquiring about an anual report but for the given question propose up to five relevant or related question to assist them in finding the information they need now this is very important because everything is driven through the prompt so your prompt should be something that you put some thought into it and the important thing here is that you need to make sure that we have a prompt that encourages the model to actually generate related questions or queries so make sure that the prompt must include different aspects of the topic and the variety of questions that will help the model understand the topic better provide concise single topic questions without compounding sentences that cover a variety or various aspects of the topic I ensure each question is complete and directly related to the original inquiry very important to have a well refined prompt of course we pass the prompt and the query which is going to be passed through the function let's go ahead and generate the multiquery so at the bottom here I'm going to just paste so I have the original query what details can you provide about the factors that led to revenue growth for instance and so the augmented queries here I call the generate multiquery and pass to original because that is what it needs so at this point I should get result that will give me what I need step through and get augmented queries okay so I'm going to just show here so let's query through all the augmented queries we're going to get and print them out and at the top let's make sure that I have con okay very good this is already commented out just going through the process and soon we should see and just like that you can see we have our augmented queries so here is the first one how do changes in pricing strategy impact Revenue growth what role did new product launches play in driving Revenue growth where were there any specific marketing or advertising campaigns that significantly contributed that significantly contributed to revenue growth how did changes in customer demographics influence renew uh Revenue growth that Partnerships or collaborations with other companies impact Revenue growth this is wonderful so you can see that our large language model able to plug in from the query that we passed along uh the original query we're able to plug in and get at least five new queries five new um augmented queries now that we see that we have our augmented queries because we look through and we saw of them all of those were generated let's go ahead and join or concatenate the original query with the augmented queries okay let's go ahead and do that let's go ahead and print join query so we can see okay so we can see that we have the augmented queries still loading that's very good but then we have this list that will contain each one of our augmented queries with the original query okay so you can see here the what details can you provide about the factors that led to revenue growth and then we have the augmented query what were the key products which is the first one here so for each one of these we are concatenating or attaching to the original queries as well as the augmented query so we are concatenating both of them and you can see they're all in a list now we're going to go ahead and get this joint query which will have of course the original query and the augmented queries through our collection in this case through our Vector database and query so let's do that and there we go so now we have retrieved results and next thing what we need to do because we might get multiple duplicates now because of the nature of things we might end up getting multiple duplic duplicates we can now remove duplicates and Sally sanitize the list that comes from our retrieved documents from our collection. query here okay so to do that let's go ahead and so now we're going to go ahead and just go through and sanitize our list to remove the duplicates and let's go ahead and output results documents okay okay let's go ahead and run this so should go and output result documents for each query going to add add that query and then result in Loop through and get each one of those results okay let's go ahead and run this real quick okay there we go there's a lot here looks like gibberish but exactly uh that's exactly what we're doing so we're outputting the results documents so at this point we have passed everything through the query and now we're looking through and get the query results so all the documents that we were're able to retrieve through the queries concatenated with the query the query all right so next step really is to go through pull all that information pass that through a large language model so we can get the answer all right so now what we can do is just plot everything so we can see the results right in a graph so let's go ahead and project everything okay so we pass the embeddings just like we did before we're getting all the embeddings from our collection and then we instantiating the umap transform so that we can have that to uh pass through the data set embeddings and project embeddings to project everything okay so same thing we've seen before so next we're going to visualize results in edding space so by creating the original cor embedding and then the augmented query embedding so we can see the differences we're going to project the original query and augmented queries in embedding space but because because of the nature of things we're going to flatten the list of the retrieval retrieve documents to a single list because the project embeddings expects indeed a single list of embeddings so that's what we're doing there and what we'll do next is we're going to go ahead and retrieve uh the embeddings themselves and result embeddings just looping through all of them and get them and we're going to go ahead and project those embeddings the result embeddings and you map transform the we passing through as you see there and of course as always we're going to go ahead and plot everything using the the mat plot lib so we're going to have to import that and then we're going to go ahead and pass it through the plot figure the plot object and project everything okay let's go ahead and run and we should see if all goes well something projected again you can see here are our results everything is pretty much squished in Red X which is the original queries and then we have the orange axis which are the augmented new queries that were generated okay by the large language model the green circles are the retrieved documents results by the vector database search okay and the gray dots are the data sets embeddings now the retrieve documents are close to the augmented query as you can see here all these yellow or orange X's are the augmented query and you can see most of them are really aggregated around the augmented query which is really good it tells us again that we're able to see the closeness and not so close to the original one even though there's this closeness between the original and the augmented queries here but the actual documents that are relevant are agglomerated or are around the augmented query as you see okay much better so you can see that we have 1 2 3 4 5 6 gu six augmented queries five or six or some sort something like that and of course we only have one original query so this is probably better for you to visualize so we see that with the query expansion we are able to retrieve more relevant documents that we might have missed with the original query which gives us a better understanding of the topic and helps in answering the original query because the model generated queries the hallucinated queries have helped us in capturing different aspects of the original query and provide a more comprehensive view of the topic so the model generated queries help in capturing different aspects of the original query so now we have a more comprehensive view of the topic and can better answer the original query especially for cases where we have complex topics like Financial reports and so forth now there's a downside to this the downside of the this approach is that the model generated queries might not always be relevant or useful and can sometimes introduce noise in the search results so it's important to carefully evaluate the generated queries and the retrieved documents okay so in the next videos uh we are going to tackle this issue of noise that I'm talking about in the search results by using a more advanced technique that will allow us to rank in this case all of these documents so we can get the relevant feedback so before we move forward here I would like for you to do a challenge so what I want you to do is to play with different prompts and queries uh to see what results you get each time so keep refining The Prompt and see the results so this is very important because as we've talked about the reprompt is what guides everything for you to be able to get those multiple queries that are related to the query which is going to influence of course the documents that you end up getting from the vector database so we just finish going through the query expansion in this case with multiple queries which allows us to use the large language model to alistate or generate additional queries that might help us getting the most relevant answer so this is the overall flow where we create queries from the large language model and then we concatenate the the original query with the augmented queries as we call it and then we extract the relevant information from the factor database the square results and then we can use that to do all sort of things to get the actual answer of course use cases for qu expansion range from exploring data analysis academic research we've seen this before customer support Healthcare information systems and so forth as I said there are some downsides to this technique because there's lots of results that means queries might not always be relevant or useful because we end up having noise hence we need another technique to find relevant results so once we get these results from this technique we should probably go a little bit further and find what we just received as a result from this technique okay very well I hope you enjoyed this mini course I made it for you and that you see the whole picture of rag systems now the idea here is that you take this and you build your own rag systems and understanding now the techniques the Advanced Techniques that you can use to make your rag systems even better so that they don't hallucinate as much and you get the correct or the most the closest um information or uh pieces of information from your documents that then you pass through the large language model so then you have a full Fuller response that you confident that it is exactly what is being retrieved from the system so thank you so much for being here so if you're interested in learning more about rag about AI agents and Python and programming in general I do have a channel called Vinci bits so it's right here and also I'm working on a larger more comprehensive course and or courses related to AI large language models and creating AI based applications from Zero to Hero right and so if you're interested I should have a link somewhere in the descriptions where you can go and drop your email in the mailing list in the waiting list I should say and as soon as I have everything set up you will be the first one to be notified so again thank you so much for your time and till next time be well