all right so uh we're here to talk today about kubernetes for developers um so quick show of hands first off how many people have any experience with kubernetes previously okay couple everybody else how about docker how many people have experience with docker a few more okay good so my name is jeff french i'm the principal consultant at moonswitch we're at devops and cloud migration consultancy and we specialize in kubernetes so if you need any help with that let me know so let's start by talking about what is kubernetes um kubernetes is a cloud comes from a class of product known as a container orchestrator uh or a container scheduler um those terms get used a little bit interchangeably they tend to actually mean uh slightly different things and that a scheduler typically is a little bit less full featured than an orchestrator um but we'll get into what that actually means when it actually orchestrates your application so kubernetes is not the only player in this space um they have there are other schedulers and orchestrators including docker swarm that you may have used uh apache meso slash dc os it's kind of based on mesos and hashicorps nomad are just a few of the other container orchestrators and schedulers that are out there and they all do relatively the same thing right or versions of the same thing so a container orchestrator first off when we say container we're typically talking about a docker container um there are other container technologies out there that's well beyond the scope of this talk and most of these orchestrators have the ability with some modification to schedule things other than docker containers or docker compatible containers but the job of a container orchestrator is to take a set of instructions that you give it that say i have some set of containers that represent my application that i need to run and i want you to run them for me right i don't want to be bothered with uh you know figuring out is there room on this i don't know ec2 node over here and go over there and run a docker run command to get my container running and then if my container crashes having to go and say oh well let me go start that container back up or you know upgrade it to a new one or something like that that's the orchestrator's job so that's what kubernetes really does for you it manages a set of nodes that can run containers and it schedules containers to run on those nodes and it does a lot of things that are you know as far as depending on how your cluster is configured it will move workloads around to you know balance them between nodes a little more or consolidate them more on nodes so that you can kind of downsize your cluster so these are all the kind of functions of a container orchestrator and what it does so there's two sort of sides on how you need to on how you need to interact with a kubernetes cluster one would be the operator side where you are typically a sys admin or devops engineer who is in charge of standing up all the nodes and joining them into the cluster and making sure they stay healthy and all that we're not covering any of that part today today this is about if you are a developer who wants your apps to run on a kubernetes cluster here's this is kind of a quick start on things that you want to actually know and we're going to scratch the surface on some things this is a gigantic topic and there's a ton of documentation out there on the kubernetes website and lots of blog posts i'll try to kind of hint at when there's more things you might want to research on your own but we're going to keep this at a pretty high level intro level talk so the first piece of vocabulary we want to talk about in kubernetes is called a pod okay so a pod is a collection of containers and volumes so if you've worked with docker before you know what a container is right it's basically you've taken your app or some code that you've written and put it into this nice little portable container that can be run somewhere else wherever there's a docker engine you've probably also dealt with volumes if you've dealt with docker which is essentially defining a disk that's going to be attached to that docker container so if you think of a docker container as a virtual machine because it almost is then a volume would be like a disk that you're going to attach to that virtual machine where you can store some data and have access to a disk so in kubernetes we deal with pods and as i said it's a collection of containers and volumes that you define now typically a pod will have ideally one container that's not always true but you have one main container that runs your application and that's kind of the core of a pod is here is my web app running in this docker container now there are you can have multiple containers inside of a pod depending on what you need there one common usage is something you call an init container so let's say that before your actual application needs to can run you need to actually you know do a little bit of prework before that happens maybe there's some uh secrets or configuration that's mounted in on a disk somewhere or coming in from environment variables and you need to arrange that into like a config file that your main container is going to read so you could have an init container that spins up first and the way that kubernetes handles that is it spins up your init container it waits for it to successfully complete and then it spins up the main container for your pod the other interesting use for having multiple containers in a in a pod would be what's called a side car which means that you have another container that is sort of an auxiliary container inside your pod that's doing something out of band from your main app so one example for that might be say a a logging container say your app just logs to standard out and maybe you've got this sidecar container that's kind of sharing a process space with your main container that's scraping those logs and shipping them off to an elk stack somewhere or something like that so that's one common use for side cars or having multiple containers in a pod however you could actually have two you know what you might consider primary containers in a pod one example that i've seen referred to a lot is like let's say you've got your main app in this main container is serving some files from a file system so you've got a volume mounted in it has these files on it but these are also files that need to be updated regularly so you might have a second container that's running inside your pod that is just in the background constantly checking some other source and syncing these files into the local volume while your main container is still serving this web application that's provided that's serving those files so that's another scenario where you might have multiple containers in a single pod but typically when you think about a pod you're generally going to be thinking about it as one main container and that sort of represents your unit of work so let's look at a pod spec so kubernetes inter all interaction with kubernetes basically is handled through a kubernetes api but they've got a command line tool called cubecontrol that you can use to do most tasks against the api and a very common method of doing that is loading what are called manifests that are typically written in yaml but can also be written in json and essentially what ends up happening here is cube control will take these yaml files convert them to json and send them to the api in order to tell your kubernetes cluster to create certain resources so when you're looking at kubernetes you're going to see a lot of yaml and it's a really nice way to be able to express what's going on so let's look at this pod in detail here this is a very simple pod uh it's got some metadata attached to it namely it's got a name so it's my app dash pod and then it's got a label which says oh this label says that the app running in here is my app the labels are free form there's no actual required or standard labels in kubernetes although there are a lot of conventionbased labels that people will apply and you can think of labeling if if you've worked with aws or google cloud or anything like that think of it like the same way that you would as tagging right so if you're creating a ton of aws resources it's a common good practice to tag those with some consistent and convention based tags so that you can easily kind of see what you've got running easily issue bulk commands against those things the same principle applies with labels and kubernetes right they're a great way for you to be able to keep track of the different resources that are being deployed in order to support a given application so then we look at the spec and the containers and we'll see that obviously the the containers here is a yaml array so you can list multiple containers this one just lists one and it's called my app container i define the docker image that it uses which is just the busy box docker image but that can be pulling a docker image from any uh docker registry where you've got your docker hosted and that your cluster has permission to pull from but we're defining a command for it to run which this one is obviously just a real simple kind of echoing out hello now we look below that we see that we've got volume mounts so what i've done here is we've got a volume defined down at the bottom of this pod and it says hey here is this is a very simple volume uh it's a volume type known as a host path which basically just takes exactly what it sounds like a path on the host that is running this container which is one of your kubernetes nodes and makes that available as a volume that you can attach to these other containers um there are a ton of different types of volumes that kubernetes supports right out of the box like probably 30ish everything from uh aws ebs discs google persistent disks um you know ceph or gluster ffs file systems nfs mounts iscsi if there's a way to attach a disk to a vm there's probably a way to support that in kubernetes to get it attached to your pot and make it available as a volume so in this we've got a really simple on the host at slash data we've got this directory that directory has to already exist there's another directory type that is directory or create so it will create it if it doesn't exist and then as we look back up in the container spec we can see on the volume mounts that we've told it hey take that volume that we called test volume which is a host path and make it available inside my container at this mount path of slash test dash pd right so that means that now my docker container this busy box image it could read files or write files to and from that mount path you can also make volumes persistent you can define it within kubernetes persistent volumes which say hey this this volume should outlive my pod right i don't want this data to die when my container goes away right if i shut it down and spin up another container i want to make sure that i've still got the same data there i don't want to lose it and so you can do that through persistent volumes and persistent volume claims which are not included on this spec but something else you might want to look at as you actually start looking at running real workloads on your kubernetes cluster okay great so we've got pods sure so do i have to define that all the time well most of the time you won't actually define a pod directly you'll define what is known as a another class of resource known as a controller and the most common controller that controller type that you will deal with in kubernetes as a developer is going to be a deployment so a deployment describes the desired state of a set of pods and manages updates to those pots so what that means is if i want to define this pod and a set of volumes and everything that's all fine and good but i might need to run more than one of them right maybe my app is popular and i need to run five web servers to handle it well rather than creating five different pod specs with different names on my own and managing those i can create a deployment so with the deployment you can actually define the number of replicas that you want to have running at any given time and when the deployment actually starts running what it does it says okay i should have this pod running and because you've set replicas to three i know i'm going to make three copies of it and and distribute those out and it's probably easier to explain that by looking at a definition of one so here's the definition of a really simple deployment so we've got again our metadata section where we can give names and labels and that's all fine and good but then the interesting part becomes this spec so this is what's known as a pod spec and it follows the exact same schema as the pod that we looked at previously but it's going to be it's going to be managed by the actual deployment so under our spec we say i've got replicas of three and this is a to run a simple nginx pod that's going to serve web traffic for my app so the templates here are interesting because this metadata that's under the template over here this is going to actually be applied to the pods this whole template is a template for the deployment to create its own pod specs as needed based on the number of replicas that you've requested and then we've got our same spec and this is really simplified down to just a single container here but we could have volumes mounted here we could define environment variables that need to be passed in so in the background a deployment actually creates another kubernetes object called a replica set and before deployments were around replica sets were like the recommended way for you to run a set of pods that's no longer the case now you know the the sort of recommended guidance is not to create replica sets yourself in most cases but instead to create a deployment which is again a defined uh you know a desired state that you want it to have and so then the kubernetes api is going to say okay cool because you've expressed this desired state i'm going to make sure that i'm always keeping it in that state and doing whatever i have to do if i see that there's only two of your pods running and you've requested three replicas then i will spin up another set of pods to match that the so when you actually spin these up they get distributed to whatever nodes are available and the deployment handles that for you and it says okay great you want more of these you can have more of them and then the nice thing about a deployment is it becomes a way to manage your scaling as well so if you were running three replicas and all of a sudden you start getting hammered with traffic you can issue a simple cube control command to scale up this deployment to 10 replicas and it's going to go out there and handle that for you say okay great i see i've already got three i'll go spin up seven more for you let's get those going and we'll get into a little more detail about some of the scaling features toward the end of the presentation so great we've got a deployment and this deployment is now managing all these pods that are running my app awesome i need to get some traffic to that application i need to send users there to do things typically i mean not always you may have a a noninteractive workload that's just doing bitcoin mining or something but typically you're going to run webbased applications on a kubernetes cluster and need to send traffic to them well a service routes traffic to a set of pods so this is basically a definition uh sort of it's called a service because it is used for service discovery so that you can say i need to send traffic to this pod but i don't necessarily know the name of that pod because the deployment may have killed and created multiple pods there may be 10 pods running but i need to just reference it i want to just load balance between those so that's where a service comes in uh and when you create a service there are a few different types of services that can be created um the default type of service is called cluster ip and what that means is that this service gets assigned an ip from the cider block that is internal to the cluster a cluster ip service is not exposed outside the cluster at all it's only accessible by other things that are running inside the cluster another type of service is called a node port service so when you declare a service of type node port you will specify a port on each one of the nodes that is going to be listening and proxying traffic for these pods and so what it will do is if you say expose a a service of type node port on on you know port 33602 then every single node in your cluster is going to expose that port and route that traffic to your pods based on the definition of your service and then the other type of the other common type of service that you'll see created is called load balancer now what this does is it actually typically integrates with whatever cloud provider or other system you're running your kubernetes cluster on so let's say you're running in aws and you create a service of type load balancer it's actually going to go out through the aws api and create a an elb that is going to route traffic to the nodes in your cluster so in this case now we're actually exposing this to something on the outside world the node port does this as well where you're saying hey these nodes if they're accessible somewhere in your vpc depending on how you have things configured will actually route this traffic now when you're dealing with routing traffic inside the cluster a service actually gives you a very nice convention if you have which most kubernetes clusters have kubernetes dns enabled internally so what you end up with is a dns name like this whatever the name of your service is whatever the name space that it's running in and then dot cluster dot local so if i have let's say a database server running in my kubernetes cluster and i it's maybe in the data name space and it's called postgres then i could have a name of postgres.data.cluster.local and if i want my other apps within the within the cluster to be able to access that i can just feed them that name and the service is going to take care of always routing that whenever new pods come up and old pods go down the service actually handles routing the traffic to those based on a set of rules that we'll look at here in just a second but my other applications that are consuming that get this nice static predictable dns name that they can reference inside the cluster so here is a simple service definition um so we're defining a service it's name is my service it's got a selector here right so this selector section is interesting because this actually interacts with the labels that we saw on our deployments and our pods so what this selector is saying is that the way that i want this service to route traffic is by looking for all pods that have a label of app with a value of my app so this is how the service is able to kind of dynamically route traffic as your pods get spun up and spun down by the scheduler if we look at this we're defining the ports that we actually want to route traffic to so in this we're just routing simple port 80 and port 443 to a target port so this target port is going to be the port on which my actual container inside my pod is listening for these types of traffic so i'll have my docker container inside my pod listening for http traffic on port 9376 and this service is going to listen on port 80 and direct that traffic over to the other port okay so with the exception of the service type load balancer we just said that the services don't actually expose things to the outside world by default the cluster ip service doesn't a node a node port type does but it's very difficult to manage because it's going to be these random ports that are on three five a hundred a thousand whatever however many nodes are in your kubernetes cluster so how do we get traffic from the outside world in as we said there is the load balancer type which will actually create a load balancer in whatever cloud provider you happen to be running your cluster in that can get expensive real quick if every single thing that you especially if you're running a micro service architecture with you know 30 or 40 you know web apps that all need to be exposed all of a sudden you're going to be paying a lot more for those elbs than the value they're actually providing in most cases well this is where an ingress comes in so in kubernetes and ingress is a resource it's referred to as a resource even though it doesn't actually do anything an ingress simply describes a set of hosts and paths that should be routed to a given service from outside the cluster to the service that's inside the cluster they don't actually do anything by themselves because they actually employ what is known in kubernetes land as the operator pattern where there is a ingress controller or operator that you install in your cluster that actually follows these instructions that are defined in the ingress so really common one would be like nginx ingress which is going to spin up an nginx instance or instances and basically read these rules and say okay great you route all the traffic to me as nginx and i will automatically dynamically configure all these backends to route traffic to within your cluster so let's take a look at an ingress here is an ingress that i've used to expose a gitlab web ui in my cluster to the outside world now there's a few new concepts in here an interesting one here is the annotations annotations are valid in the metadata of pretty much every uh kubernetes object they differ from labels a little bit in that labels have to be very simple key value pairs whereas annotations can be more complex as we see here with you know having actual url style identifiers annotations are typically used to tell some other thing how it should interact with this kubernetes resource so in this case some of the interesting ones here are that we're saying okay well the class for this ingress is nginx that means that i have deployed the nginx ingress controller in my cluster and i'm saying hey i want this ingress to use the nginx ingress controller there are ingress controllers that are there's one that's specific to google compute if you're running in gke there are there's one called traffic there's i believe there's actually an aha proxy ingress now and you could actually run multiple of these in your cluster if you wanted to and this annotation helps them decide which one of them should handle your ingress another interesting one here is this tls acme i'm telling it hey i want you to get a tls certificate an ssl cert for my ingress in this scenario i've deployed another controller called cert manager that knows how to interact with let's encrypt so whenever it sees that i've created an ingress with this set to true it says hey cool i'm going to go in there and i'm going to read the data out of your ingress and i'm going to go interact with let's encrypt to automatically issue you an ssl certificate a lot of really interesting things can happen there based on these annotations um the actual ingress itself is specifying that hey for this given host i want you to route the this path to this back end it's very uh if you've ever configured nginx or ha proxy it's a very very familiar type of concept where you're saying hey when you see this come in route it over here to this back end server transparently to the user pass along some ip data stuff like that in the headers this does the same thing so i can actually have this this is passing all traffic at the main root path but i could have it say just you know slash users if i wanted to route that to some different service for a reason i can also have multiple rules defined in here if i need to um but and this is telling this back end is telling it the name of a service so just as we saw in our previous example of the service i'm telling it which service to route to so you can see we've got this kind of multitiered thing where we're saying okay and ingress helps me define how traffic comes into the cluster and goes to a service then a service helps me define how traffic that's already gotten into the cluster gets to its ultimate destination on an actual back end and then in the tls section this is just defining a little more data about where this ssl cert would be so in this scenario we would end up with a kubernetes secret uh called gitlab gitlab tls that would have a couple of keys in it that hold the private key and the public cert that were obtained from let's encrypt and make that available to whatever ingress controller is trying to run this such as our nginx instance so let's look at what all that looks like right so based on what we've talked about so far we have this is a representation of kind of what we just went through which is that we've got traffic that's out here in you know in the internet that gets arrives at some host name that we've defined via external dns that hits our ingress so in this case let's say our ingress is the nginx ingress controller so it's hitting an nginx instance and that nginx reads its own config and says cool because you came in on this host and path i know that you want to go to a particular service so it routes that traffic down to the service once your traffic hits the service the service says okay great i know how to find the pods that you want to use and i'm going to send you to one of them and just because basically a roundrobin dns at that point where the traffic now hits one of the pods that i have running in order to serve that traffic so this is sort of the basic application setup that you'll see for most things that you deploy into kubernetes especially as a developer an application developer a lot of times we're building web uis or apis and this is going to be a really common paradigm for how you actually get traffic to your application that's running inside of the kubernetes cluster so so now that we've got our actually tired of standing behind this podium if i can move just a little bit without messing up the twitch stream um so now that we've got our application running as a deployment how do we update it right this is one of the reasons why you use a kubernetes deployment instead of directly using a replica or pods yourself is that the deployment has a lot of options that help you manage how you actually roll out updates to this thing so whenever i come in and say okay we've shipped version one of our app to kubernetes as a deployment now we've got version 1.0.1 ready to go because we found a bug and we got to release it so when i go and actually update say the image tag that i'm deploying from my docker container of this deployment kubernetes sees that says okay great well now that you've made an update to this deployment i'm going to look at your update strategy and figure out how it is that you want me to roll this out the one option that it has is recreate in that scenario it just basically goes and nukes all of your pods and recreates all of them with the new definition that's a way to go there's probably a good use case for it i haven't had a good use case for it yet because typically i'm deploying apps that are actually in use so the default is a rolling update strategy which means it's going to based on a set of rules look at all the pods and say okay cool i'm going to go through and upgrade them systematically until i've got them all done and when you're doing that you can define uh one of two types of strategies there is the max unavailable and then there is the max surge and these aren't exactly mutually exclusive except that if one of them is set to zero the other one can't be we'll get into that just a little bit but what we've got here is if we do a max unavailable then what we're saying is that a certain number of my pods can be unavailable at any given time during this update and that's okay and you can express that as either an absolute number of five pods or you can express it as a percentage like 25 of whatever i've got deployed i usually find a percentage to be something that scales a little better when you actually start scaling your deployment up and down you probably care more about the percentage of total pods than the actual exact number when so the other type is a max surge so a max surge defines a number of pods that can be running in excess of what you've defined for your deployment so if i said on my deployment that i want to have five replicas then a max surge says that so many pods can be above that number without it actually without the deployment starting to kill them off as oh hey that's more than what you asked for so let's look at this a little more detail with an example so let's say i've got on our on our web server nginx replica set to 10 saying all right kubernetes i want to always have 10 pods running serving all my web traffic so in the scenario where we have defined a max unavailable of three here i've chosen an exact number despite the fact that i just advised you with percentages just for simplistic for simplicity's sake so in this scenario it means that once i tell kubernetes to update our deployment it will say okay cool well you've got you've asked for ten copies of this to be running and i've got a max unavailable of three so i'm gonna go kill three of your pods and and update them to this new spec and i'm gonna wait until they come online before i do any more right so it starts those updates and you know one of them might finish before the others so as soon as one of them finishes then kubernetes says okay cool well i can go take down another one of your pods that needs to be updated right and at any given time it's going to make sure i still have seven available and and at least seven available and no more than three that are not serving traffic now in the max surge scenario we go the other way it says okay you've asked for there to be 10 of these pods running and i'm not going to take it any lower than that instead because you've got a max surge of three i'll bring up three new pods and when they're ready i'll start swapping those in to your set of 10 and swapping out your other ones so in that scenario we might go as high as 13 pods that are active in serving traffic and which one you choose really kind of depends on your application and your constraints and you know whether it's more important to keep more pods up and serving traffic or if it's more important to stay under a certain number so that your you know cluster doesn't get overloaded you know one thing to consider is that whenever any of these schedulers like kubernetes are trying to schedule their pods they look at a few different things to say hey is there enough cpu and memory on this node to put this pod there and if there's no place to put it then those pods just sit there and never get spun up right so that might be a scenario if you know that you typically run sort of at the capacity of your cluster for whatever reason then you might want to choose a max unavailable update strategy so that it starts pulling pods out and freeing up those resources before it creates new ones if you've got plenty of headroom on your cluster and it's more important to make sure that you've got enough you know pods serving traffic then you might want to go with a max surge where it says okay great i'm going to make sure that i've always got enough you know 10 pods running to serve these customers but i'll scale it up as high as 13 in order to get these updates rolled out yes so the the the only real um exclusivity rules um that apply to these according to the documentation and i haven't done a ton of experimenting on these two in practice to see how they actually play out in really largescale scenarios but um is that if max unavailable is set to zero then max surge cannot be zero and vice versa if max surge is set to zero then max unavailable cannot be set to zero right you have to allow it to do one of the other or it'll just be deadlocked and can't do anything now if you define both of them um again like i said i haven't done a ton of experimentation in this scenario but what i expect would happen is that it's going to basically work within those limits so in this scenario if i had both of these things defined then it's going to start creating pods and killing pods as it needs to and the rules that it's evaluating against is okay am i above seven and you know below 13. and as long as it's in that range it's free to start creating and killing pods all that it needs to okay so there are there are several other ways that you can run pods in kubernetes we covered a deployment which is a way of just essentially keeping things running based on some described state right other other things in the same family as deployment would be stateful sets and daemon sets which are a little bit less common and a little more advanced we're not going to get too deep into them here suffice it to say that if you have a daemon set it's like a deployment except it means that it's going to run rather than a number of replicas it's going to run that pod that you define on every single node so it has a onetoone relationship with the number of nodes in your cluster if you do a daemon set that's typically used more for things like uh log scrapers and aggregators or things like that where you're kind of monitoring health usually or anything else that you need running on each node exactly once a stateful set is similar to a deployment except that it makes some guarantees around identity and ordering of your actual pods and that's more commonly used for things like databases where maybe you've got a a leader and you know two or three replicas and which one is the leader and which ones are the replicas is actually very important in that scenario and you need to make sure that there's a certain amount of availability and that things are rolled out in a certain way and those are two things that you can dig into a little bit more if your workload if you think that you need that for your workload typically you'll use a deployment however another class that you'll actually use quite a bit as a developer is a job so it's similar to a deployment in that it describes a set of pods except with a job it runs it once to completion for each pod that you define and usually it's one pod it could be multiple and you can do some interesting things with parallelism and q queued workloads but a really typical example for using a job would be database migrations right whenever i deploy a new version of my app i want to make sure my database is migrated to match that version so i can create a job and that job says okay great create this pod based on this pod spec and run it until it completes and completes successfully so what that means is if it goes and starts my job and and this pod runs but for whatever reason let's say the database is unavailable or something else happened and it couldn't actually run the migrations and complete them successfully that pod is going to die and it's going to get restarted again and it's going to and kubernetes is going to keep trying to do that until it successfully completes once now once you've actually run that database migration you're like cool i don't want to do that anymore so it stops your pod exited successfully indicating that it was done and it completed whatever it was trying to do and now that's it you're done so that job just sits there um and it's it just sits there in a completed state you can go back and inspect the state of it and then you can delete the job if you want it'll clean up any pods that it had left laying around in a nonrunning state so let's take a look at a job again we see that the spec looks very very familiar um albeit tiny that we've got this essentially the same kind of template it follows the same template as the pod spec for deployment and other things and the only real difference here is that because the kind is set to a job and there's a few other things like a back off limit that can tell it how much it needs to back off um from jobs or that it should stop trying to run them if it fails four times then just don't bother trying anymore but it's very simple very simple definition of a pod with the caveat that it's only going to run once now another sort of extension in the job family is actually the cron job so the cron job is kind of a controller type object which says hey based on some crown expression just like you would expect i want you to create jobs for me so if i create a cron job that has a you know has a chron expression that tells me to run i don't know uh once every night at 2 am then once every night at 2 am kubernetes is going to try to create a job uh it may not succeed in that so it's there's not as a firm of a guarantee on crime job execution as there would be with say just traditional crime on your system but it's going to create a job and try to have that job run and it's going to do that every night at say 2 a.m which is a great way to i don't know back up some files or a database or something like that or perform some sort of maintenance task so those are jobs and cron jobs and then so i promised we'd get back into some of the scaling stuff as that is one of the uh interesting you know sort of promises of of kubernetes and the cloud right is is that you can scale things you know to meet demand and then you can scale them back down when you don't have demand and save some money right so from the developer side one of the ways that this happens you can obviously go in and scale a deployment yourself if you notice a spike in traffic you can go and issue a command that says scale this deployment up because i need more pots but do you want to watch the dashboard all day to find out that there was a suddenly a spike at 3 am and you needed to go issue that command manually probably not instead you can use a horizontal pod auto scaler so this will scale out a deployment based on observed cpu utilization or other metrics and i probably should have put an asterisk after that or other metrics because that part gets well beyond the scope of this talk suffice it to say that you can actually monitor different metrics such as the number of you know hits coming in on an ingress or you know disk space or something like that right but we'll just talk about them in the simplistic form of cpu utilization so let's say i'm running this app and this app is starting to you know that the aggregate cpu usage across all of my pods i've got say i've got three pods running and they're at they're averaging out and they're aggregating up to you know 80 or 90 cpu utilization i say oh well we're busy we better scale up so if i've already defined that in a horizontal pod auto scaler then the kubernetes api is already watching that for me and it says oh hey we've hit the threshold add some more pods right scale it up take it from three to six you know um and you can set the definition on what you're min and what your max is and which metric you're you're tracking uh and the way you do that is with this cube control command it's really hard to see sorry but basically it just says cube control auto scale deployment you give it a deployment name you tell it you're watching for cpu percentage to be above 50 and i want a minimum of one and a maximum of 10. so what the horizontal pod autoscaler is going to do is it will scale your pods up or down based on these observed metrics so if we're running above 50 cpu and we need more pods it's going to add those in and then later when the the wave of traffic has subsided and our our aggregate cpu usage is down it's going to start scaling those pods back in as well right now this becomes really really interesting when you deploy it with something that would be more in the operational side of a kubernetes talk which is a cluster auto scaler so there are cluster auto scalers for let's say you know google or aws or something that will say hey whenever kubernetes is trying to schedule these pods and it says hey i don't have anywhere to put these right like i i'm all my nodes don't have enough cpu or memory to support this pod that i need to schedule so it's just pending then the cluster auto scaler would see that and say oh well let me add some more nodes to your cluster you know through an auto scaling group or whatever else and it goes out and puts more nodes in the cluster and then all of a sudden the you know the scheduler can now find a home for those pods so by and then the cluster autoscaler does the same thing in that periodically it says hey well we've got more capacity than we need let's consolidate some of these pods onto a couple of nodes and take a node out using this you can finally kind of realize the the true elasticity of today's you know modern cloud environments where based on observed metrics that your application cares about you can now dynamically scale up you know to as big as you really feel is necessary or need without human interaction just let the machines watch the traffic take the action based on rules scale it up now you're handling enough traffic and then when everything dies down all of a sudden it scales back in so you're not paying for all that capacity you don't need so once you get into horizontal pod auto scalers and cluster auto scalers you really get the true kind of promise of why people want to use things like the cloud and kubernetes in order to actually run their workloads all right question and answer time if you've got questions i'll try to give them answers and uh yes uh oh yeah so yeah so the question is uh does it when a service goes to route traffic to a pod does it look at the load on that pod by default no i think i saw an option or it may have been some sort of an addon that could do that but by default it just does simple robining although i believe you can configure sticky sessions um so that could that could come into play as if it knows that a particular pod is is not uh is overloaded with concurrent sessions um so the question was does the when you go to execute a rolling update does it also um take traffic away before it does that um so yes and no right in that it it's going to go kill a pod which means that that pod is going to immediately be marked before it even gets killed it kind of i mean this all happens like that but it's going to actually mark the pod as as unavailable which means that the services that try to route traffic to it say i only route traffic to available pods so they're constantly every time they need to route traffic they're saying all right well where's the list of available pods that one's not in it so i won't route any new traffic there now if you have a sticky session that was routed there i don't actually know what happens with that but that's essentially how that works and so while it's not explicitly making decisions like oh i should take away traffic for this it's it's sort of this composition of all these other rules that we talked about that dictate how that's how that traffic is going to flow okay so i mean it pulls it out of the pool so no new stuff goes through right if it's in the middle of trying to service a request it's potentially that it could kill basically if you can kill the pod in the middle of trying to serve this request it's not going to wait for connections right so the so to clarify yes uh he was saying that it's not going to necessarily wait for all connections to be drained off of a pod before it stops sending traffic there yeah so and and again this really uh this comes down to you know uh a term that uh has been getting thrown around a lot lately about cloud native and designing cloud native applications um and that really becomes part of it right like in in a cloud native application you should be prepared to handle that scenario for your customer where uh you know this you know at any given time the thing running my application code could get cut off and i need to make sure that my client on the other end has some kind of retry or that it's okay that that connection be dropped right and that's going to be very very application and even particular endpoint or request dependent to figure out but it's something that you should be thinking about as a developer who's going who's designing applications to run on these cloud native uh systems another question yes i'm sorry availability when you're like say you have to upgrade kubernetes okay so yeah the question was how do you stay highly available with your apps when you have to upgrade kubernetes itself this gets a little more onto the operational side of this but essentially because kubernetes is filled with nodes and node pools what you would do is you would simply add more nodes and then that are running the newer version of kubernetes and the cubelet api and your other nodes that have apps on them there's a few series of commands that you can run you can you can coordinate a node which basically says hey don't schedule any new work on here but leave the work that's already there running and then you can also drain a node which tells kubernetes to start taking work off of that node and finding another place to schedule it so typically your path there would be add some new nodes cordon and drain your old nodes and you know at whatever pace makes sense for your application and so that it kubernetes will then start you know taking those pods off and putting them onto the new nodes and they can't reschedule onto the old nodes because you've already cordoned them all right any more questions no all right well thank you very much again i am jeff french with moonswitch if you need help with devops continuous integration kubernetes cloud migration moonswitch.com get in touch we'd love to help you out and i have a few tshirts and stickers here to give away so if anybody wants a tshirt or sticker um let's see here i've got a size xl tshirt any takers all right oh sorry i've got a size large tshirt yeah go to this side of the room make sure this is the large all right and then i think that leaves me with a medium anybody take a medium there you go very excited for the medium also i'll have some stickers just sitting up here if anybody wants them come on bye grab a sticker decorate your laptop all right and uh that's all we've got for today thanks very much