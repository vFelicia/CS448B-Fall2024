multithreading is an important Concept in computer science in this course you'll learn everything you need to know about multithreading in Java but the concepts apply to other programming languages as well for each concept you'll learn the theory and then see some code examples hello and welcome I hope you're doing super good my name is rendu and I'm working as a senior engineer with Uber I have been programming for more than a decade now I believe that multithreading is one such concept which is way too abstract and difficult to understand if it is not taught in a proper manner however if the concepts are explained with relatable examples it becomes a fun and engaging experience that's what I have done in this course I have broken down the difficult and Abstract Concepts in simple English which is really easy to understand to make the things even more clear I have presented relatable examples I strongly believe that multithreading is one such tool which should be in the toolkit of every good programmer the entire video is split in a smaller sections wherein I teach about a particular topic each topic is explained with some theoretical concept followed by the examples and then I implement the topic of discussion in Java to give you a proper working code example the topics are taught in a bottomup manner where I start from the very Basics and then I build on the concepts layer by layer by the end of this tutorial you would become very confident and comfortable with the concepts of multithreading and that's my guarantee to you the code examples are in Java however most of the concepts should be transferable in other languages as well which support multithreading in some capacity so with that in place let's get started so what is the motivation for multithreading by default programming languages are sequential in nature code execution happens line by line in usual scenario consider the below code so in this method we have init de call then we have download data call then we have process on data and then finally show the results so in the usual scenario all these things will be executing one by one so first this will be called then this will be called then this then this but we have a problem in a single threaded program these instructions will be executed one by one and the time consuming section of the code can freeze the entire application what is the solution well figure out the timec consuming tasks and decide if they can be run separately if yes run such tasks in separate trades let's have a quick Layman explanation of how a timec consuming a step in your code can slow down or freeze your entire process let's say you invited your friend over to your place to watch this super cool movie being a great host you decided to make some popcorn for your friend but here is the catch it will take some 5 to 7 minutes to prepare the popcorn during the time you are involved in preparing the popcorn your friend asks which movie are we going toward wors today since you are super involved in making the popcorn you don't respond your friend even though feeling a bit weird about the situation asks you again if you're okay but thanks to your involvement in the process of making the popcorn you don't respond situation becomes super strange however your friend tries one final time and asks you if there did something wrong and thanks to your deep dedication in the process of popcorn making you don't respond by this time your friend gets freaked out and punches you in the face and you reboot but we all know this does not happen in real life unless you are playing a prank we humans are naturally equipped to multitask in this example since you would be aware of the time it takes to prepare popcorn you would probably prepare the recipe and put the pot on the stove and let the popcorn get prepared while it's getting prepared you are available ble to do anything if there is a need so you figured out the task which is going to be timec consuming started its execution and let it finish in its own line of execution effectively you did not block other tasks on you and did not freeze entirely if you follow line by line execution of tasks in your program this kind of freezing situation may arise in your code if there is a task which takes longer time to execute so what is the Improvement so in this case let's go through the different calls so in it de DB is where you are initializing certain DB related things then you have download data then you process the data then you show the results so to me it looks like downloading of the data is something which could take the major chunk of time what we can do now is put this download data in some sort of other threade and everything else in some other threade and in that sense we can do a parallel processing and it will ensure that by the time we are waiting for downloading the data everything else is not getting Frozen up and as a Sy is not lagging so this is one such Improvement we could do by the virtue of multithreading so to give it a formal definition multithreading is the ability of CPU to perform different tasks concurrently now let's have a quick explanation around concurrency versus parallelism concurrency is like having multiple tasks to do but you only have one set of hands you switch between the tasks doing a little bit of each one at a time if you play a guitar it's similar to that where you play different notes and cords using your nine fingers even though you play each note separately the switch is so fast and smooth that overall it appears as if everything is being played together parallelism on the other hand is again having multiple tasks but now you have many friends to help you out each friend works on a different task at the same time so all the tasks get done faster so in summary concurrency is doing multiple things all at once by quickly switching between the tasks and parallelism is doing multiple things at At Once by having different parts of the task been done simultaneously by different entities now let's learn about concurrency versus parallelism in somewhat more technical terms so concurrency and parallelism are two terms which are used quite a lot and that to interchangeably while discussing multi threading but there is a subtle difference let's talk more about it concurrency refers to the ability of a system to execute multiple tasks at the same time or nearly overlapping times so they seem like being executed at the same time in concurrent systems tasks may start execute and complete independently of each other but they may not necessarily be executing simultaneously at any given moment concurrency is often achieved through techniques like multitasking where a single processor switches between executing multiple tasks rapidly or through the use of multiple threads or processes parallelism on the other hand refers to the simultaneous execution of multiple tasks to achieve faster performance of increased throughput in parel system tasks are truly executed simultaneously either on multiple processors or multiple processor course or through other means of parall processing like distributed computing or GPU Computing parallelism is all about breaking down a task into smaller nonrelated subtasks which can be executed concurrently to speed up the overall execution time thus in the context of a hardware with a single CPU code currency could be understood as a perceived parallelism or fake parallelism even more so in scenarios where tasks appear to be running simultaneously but are actually being executed sequentially or in an interleaved manner this is done by something called as time slicing algorithm so in summary concurrency is about managing multiple tasks or processes potentially interleaving their execution to give an appearance of simultaneous execution whereas parallelism on the other hand is about truly executing multiple tasks or processes simultaneously to achieve a fast performance while the terms are related and often used together they refer to distinct Concepts in the context of computing now let's understand what is a process and thread process is an instance of program execution when you enter an application it's a process the operating system assigns its own stack and Heap area whereas threade is a lightweight process it is a unit of execution within a given program a single process may contain multiple threads each thread in the process shares the memory and the resources of the parent process one single process could contain many other threads now let's learn a bit about the time slicing algorithm let's imagine we have multiple threads associated with the process somehow the CPU has to ensure that all these threads are given a fair chance to execute one such approach is to use the time slicing algorithm so uses time for the CPU is shared among the different threads so here is what happens so you see sharing is time slicing let's say the green boxes represent one thread and the Yellow Boxes represent another thread thread T1 and T2 respectively and consider that this is the timeline and at this particular time thread T1 is assigned to the CPU then after some time thre T1 takes a break and we assign thread T2 to the CPU and after some time T2 is given some rest and thread T1 is assigned again to the CPU so as you see it's going into a back and forth manner where each and every threade is taking turns to run on the CPU one by one so here what we are doing is we are basically slicing the time and we are assigning certain time Quantum to the CPU so here we have a CPU and these are the two different threads which are kind of taking its turn to be executed on the CPU so this is how the time slicing algorithm works now what happens when we have enough CPU at our disposal so let's say we have thread one and we have thread 2 and there are two CPUs so in that case thread 1 will run entirely on CPU 1 and thread 2 will run entirely on CPU 2 so it's effectively a parallel kind of processing wherein we are not sharing anything on a given CPU rather each threade has has a dedicated CPU and it does not need to bother about whether it has to share the CPU with the other thread or not and please note that I have put CPU here but it could be a different core in the CPU itself so it could be either different cores of a given CPU or it could be different CPUs so that depends on the hardware in such kind of setup we can achieve the parallel processing now let's look at some of the pros and cons of multi threading the first one is we can build responsive applications so now you don't have to worry about freezing uh situation and thus you can build your applications to be responsive second is you will have a better resource utilization because now with the use of multithreading you could ensure that your Hardware or your CPU is not sitting idle rather once it's idle it could be taken up by some other thread for execution and the third thing is it helps us into building performant applications so with the help of multiple core CPUs we can build parallel programs and essentially we could get some benefit on the side of performance as well now coming to the cons of the multithreading the first one is synchronization needs to be done and it can get tricky at times so essentially when you are doing multithreading you need to share the memory space and other resources with the process and in that case let's say when there is a process and there are certain number of threads you need to share the resources so so we need to ensure that we are not running into funny situations and those things are handled by something called as synchronization we will have a much more focused discussion around all these things later in the video the second thing is it is difficult to design and test multithreading apps so essentially you don't have a control in which the different threads could execute so in that sense it's difficult to predict the behavior of the threads so it's difficult to design and test multithreaded applications and the third thing is thread context switch is expensive so if there are more than required number of threads then it becomes detrimental to your system performance so multithreading is not a silver bullet which will help you with all the situations rather we should use it judiciously now let's have a look on the thread life cycle any thread will start its lifetime in the new state and every threade is in this state until we call start on it after we have called a start on it it goes to something called as active State and this active state has two substates either it could be runnable or running as we saw in the earlier slides in some cases we may have to do some sort of time slicing and in that case there could be five threads which are ready to run but there is no CPU available on which it could run and we have called a start on such threads so those trades will be runnable State and there could be certain threads which will be in running State and as soon as those running State threads are done then they could allow the threads in the runnable state to run again and this is what we mean when we say that it has two substates which is runnable and running effectively this is the active State and the third state is the blocked state so every threade is in this state when it is waiting for some thread to finish so let's imagine there are two threads T1 and T2 and then they both started running on the CPU and after some time T1 got a chance and it was executing its task after some time it had to be taken out of the CPU and T2 got a chance but now T1 is not completed it's waiting for its execution to complete because T2 is now on the CPU so T1 is in a blocked State and this is what we mean by the blogged state now T1 will get a chance to execute on the CPU and maybe it may be done with its entirity of execution and then it goes to a state called as terminated state so every threade is in this state after it's done doing its required task here we are in the ID I have created a normal Java project and it's called as multithreading So the plan is that for the entire duration of this tutorial I'll be using the same project and I'll be creating different packages inside the project to discuss the concepts of the multithreading so in this particular section we will be discussing about the sequential execution so in order to demonstrate the code let's create a class call it a sequential execution demo and here is the idea behind this particular class I will be creating certain methods and the intent of this particular class is to Showcase that in a normal Java program the execution happens line by line and there is no jumping around from this part of the code to the other one so let's get going so to begin with I'll create the main method and in the main method I will have two methods let's call those as demo 1 and demo 2 and let's create those two methods I won't be doing anything fancy I'll just create a normal for Loop which will be iterating in certain range and then it will print some message and that's it so let's copy this one and let's change the name and here let's change the message as well and now let's run the program on running this this is the outcome that we have so first we were executing demo one method so the entirety of demo one is executed wherein it will be printing from 0 to 4 with this message which is from demo 1 plus I so from demo 1 0 from demo 1 2 till 4 and likewise we have executed demo 2 and in that case we print this message which is from demo 2 and the IAT number which is from demo to 0 to from demo to 4 so what we see is that the execution happens line by line so the main method is the first one to get started and the first thing it sees is that we are invoking a method called as demo one it goes there it executes it it comes back then the next line it says is that it's demo 2 it goes to demo 2 it executes it and it comes back here and then the execution terminates so this is what we mean by the sequential execution so in the context of multithreading what we can understand is that each and every program is single threaded unless otherwise instructed so here we just have a single threade and that is the threade that is created by the jvm for the execution of this main method and this could also be called as the parent thread or maybe the main thread now let's learn about the way in which we can create threads in Java and the first way is to implement a runnable interface so we will create a class let's call it as runable thread example and let's have a main method created the way it works is that we will have to Define some sort of class and the class will Implement our enable interface so let's do that so let's call it as thread one and it will implement the runnable interface and the runable interface has one method which we need to implement so that's run method and the logic is whatever we Implement inside the run method that is executed by this thread so let's do that so let's have a for Loop which runs from i0 to I4 and it prints a message let's call it as thread one and I would be the ith time it has been called now let's create an another thread let's call it as trade two which implements runnable and let's implement the run method and here as well we can run from i0 to I4 and let's print the message call it as thread to and I so this is a way in which we can Define the threads and once the threads are created they need to be somehow started so in order to do that what we can do is we can define a thread let's call it as one then new thread and we can pass the class that we have created so thread one and likewise we can say thread two new thread new thread two and we have the handle for these two threads 1 and two so how do we start these threads well we have a method called as start so let's do that so what happens is once you call the start method jbm will start these two threads and they are in the runnable state so they could be either immediately running or they will have to wait because they don't have any CPU available at their disposal where they could go and run so let's run this and see what is the outcome like so what we see here is first thread one is running then we have thread two running but this may not be the case always so in order to see a clear example let's increment the number of times we are going to print this message so let's increment it to 10 and let's do this to 15 now let's run the program and see the outcome so here here is what we see first we have thread one running and then thread two takes over and then thread one is running and then thread two takes over and so on and so forth so once everything is executed the execution will stop and the program will terminate so what we are observing is we have created two threads we have started the threads but there is no sequence in which they are executing rather the thread has been created and it's available to be scheduled by the thread scheduler and once the thread scheder finds an available spot for a particular thread to be run on the CPU it's assigned to the CPU and that's the time it's running for the time when it does not have the access to the CPU the thread will have to wait and that is the reason we are seeing a back and forth execution pattern wherein first one thread will run for some time then thread two will take over and then thread two will wait for some time and then thread one will take over there is also a different way in which we can create a threade using the runable interface and that is by making use of the anonymous in a classes so let's create trade three then new trade three and what we can do is new runable and let's print a similar kind of message so I less than let's say 15 I ++ the message could be 33 plus I and we can do the same thing which is 3. start because three is the handle that we have given for this particular thread one thing which you can observe is that this could be easily turned to a Lambda so let's do that and here we have a much cleaner way of creating a thread using the runnable interface so all we need to do is inside the Lambda we can provide the logic which needs to be executed by that particular thread now let's run it and see its outcome so we can see thread 2 is running then thread 1 is running then thread three is running and every thread gets some time of execution with the CPU and eventually all the threads are executed and terminated so this is how we can create threads in Java by implementing the runnable interface the other way of creating a thread in Java by extending the thread class with the help of extend scale keyword let's learn about the same so let's create a class and let's call it as extends thread example let's create the main method and now let's create the different threads so let's call it as thread one extends thread class and likewise we had to overwrite the run method in the example of runable approach we need to do something similar here as well so let's do that and we can have a for Loop which runs from I as 0 to 9 print some message let's copy this let's call This Thread two let's change the message as well so we have created two threads which is thread one and thread two now we need to instantiate it so let's call this as thread one and then new thread one then threade two and New threade 2 please note that here we are directly creating the thread as we are not passing this object inside the thread Constructor like we were doing in the case of runnable approach so now once we have the handle for the threads we can call do start on these two let's run this method and see the results so what do we see we have thread one running then thread 2 takes over now again we have threade one and then threade two takes over finally everything is executed and the program gets terminated so the basic idea Remains the Same once we call do start on these trades they are in the runnable state and based on the availability of the CPU they will be submitted to one CPU and they could start with their execution now that we have seen both the approaches of creating a threade one by implementing the renewable interface other by extending the thread class let's see which approach is better so if we extend thread then we cannot extend any other class usually it's a big disadvantage however a class May Implement more than one interface so while using the implements runnable approach there is no restriction to extension of class now or in the future so in most of the cases runnable is a better approach to create a thread now let's learn about do join method in Java let's create this class let's call it as join thread example and first we create create the main method let's create thread one and we can use the anonymous in a class or maybe Lambda to create the thread So This Thread is going to print from 0 to 4 and it will have a message call this as thread 1 followed by I let's copy this one let's call this as thread 2 let's change the message as well now thread 2 and this is going to be printed for let's say 25 times and first we call do start on the one then we call do start on the two and then let's have a message which says done executing the threads so what do you think is going to be the output for this particular program if you're new to multi trading then you could say that first these two threads will be executed and we will see all these messages getting printed on the console and finally we will have this message printed but looks like it's not going to be the case so let's run it and find it out and definitely it's not the case in fact done executing the threads is the first thing to get printed on the screen so why is it happening so in order to understand this we will have to take a step back and understand how does the main method works so in this particular program main method is the first thing which is getting called by the jbm and when this happens this main method is run by your main threade so this main threade is the first one which gets assigned to the CPU with the highest priority we will learn about priority and all those things in some time but for now understand that this main thread has the highest priority so it starts with its execution first thing it does is it creates the definition for threade one second thing it does is it creates the definition for this trade two and it comes to line number 17 then to 18 and in these two lines it moves these two threads in the runable state and finally on line number 19 we have this message since the main thread has the highest priority for now this message is printed first so what happens is all these trades are executing independent of each other so threade one will start with its execution independently so will thread two and the main thread anyway has the access to the CPU for now it's going to print this one as soon as possible and that is the time it's done with its execution and it waits for these two threads to complete their execution and once that is done then the program is going to terminate so what should I do if I have this functionality wherein I want that thread one should be completed and only after that happens the main method or the main thread should proceed with its execution so in order to implement that functionality I can make use of dot join method so let's call do join on thread one and do join method throws an interrupted exception in order to correct this we can either surround this with TR catch or could add the exception being thrown in the method signature itself I'll go with the second option now with this in place let's run the program and see what is the outcome so this is what happens threade one needs to be executed five times and threade two needs to be executed 25 times so first threade one is executed one is getting executed and looks like by this time thread one is executed and at the same time thread 2 got hold of the CPU so it started executing and you notice that for the next time thread one was was supposed to be executed but threade one did not have anything left for execution effectively it was done with its execution and that is where the dot join came in effect and it instructed to the jvm that one is done with its execution now it's time for the main thread to take over and proceed with its execution which is printing this particular line so now we print this line and then thread 2 proceeds with its execution it's going to print all the messages by thread 2 and as soon as this is done both the threads are completed and now the main thread also shuts down so what we learned here is 1. join is kind of hinting to the jvm that as soon as I am done with my execution then you can start with the execution of other threads which are in the Que in this case we had two and the main thread so first two was executed for a while on the CPU then the main thread started with its execution which was printing this line so what happens if I place two do join as well so in that case jbm will not mly wait for the thread 1 to get completed rather it will also Wait For Thread 2 get completed and only after that this message will be printed so let's try that out as well let's rerun the code and see what is the outcome so you see all the threads are executed so basically one and two are finished and only after that we see the outcome that is done executing the threads now let's print a message before executing these threads and what we can say is before executing the threads now let's run it so you see first we see the message before executing the threads now all the threads are executed because we have placed dot join on both of these two and finally done executing the threads is being printed so basically why this is happening is till this point of time we have not put the threads into the runable state so the main thread is the only active thread in this context and this is the reason we printed this as soon as weed at this line so now let's understand about the join operation with some theoretical Concepts so first thing to notice is that main thread is the parent thread so when we start a program usually the execution begins with the main method this method runs on the main thread this can be understood as the parent thread since it responds the other threads as well then the other important point to notice here is the independent execution of threads under normal circumstances so when you create and a start the threads they run concurrently with the main thread unless instructed otherwise so under normal circumstances all threads run independent of each other more explicitly no thread waits for other thread so what is join method well imagine threads to be lines of execution so when we call do chwine on a certain thread it means the parent thread which is the main thread in this case it's saying hey thread once you are done executing your task join my flow of execution it's like the parent thread waits for the completion of the child threade and then continues with its execution and here is my perspective on this concept well personally I find the join keyword is not very intuitive at first for the kind of operation it's doing somewhat better terms could have been wait for completion or complete then continue what's your perception about the joint method in Java let me know in the comment section now let's learn about the concept of ton and user threads on the basis of surface of execution threads can be of two types demon threads or user threads demon threads usually run in the background wherein user threads are the active threads so when a Java program starts the main thread starts running immediately we can start children threads from the main threade the main threade is the last threade to finish its execution under normal circumstances because it has to perform various shutdown operations demon threads are intended to be helper threads which can run in the background and are of low priority for example garbage collection thread demon threads are terminated by the jvm when all other user threads are terminated or they are done with their execution so under the normal circumstance stances user threads are allowed to be terminated once they are done with their execution however the demon threads are shut down by jbm once all the other threads are done executing now let's have a quick code demo for the concept of demon threads and user threads so let's create this class called as demon user crate demo and uh first of all we will create the main method now let's create two threads first is demon helper implements run let's overwrite the run method the second thread is user thread helper lements runable let's overwrite the the run method now let's write the code which needs to be executed by the demon threade let's have a counter variable start with zero and while count is less than 500 do a thread do sleep and then do a account Plus+ finally print this message which is demon helper running the sleep will expect us to pass certain time let's give the time for 1,000 milliseconds and the Sleep Method throws an interrupted exception so let's surround this with try catch for the user thread method let's have a sleep timer for let's say 5,000 milliseconds and let's surround this with dry catch then print this message user thread done with execution now let's create these threads in the men thread so first we start with the demon thread let's call this as a background thread that is bz thread so new thread new demon helper for the user helper maybe we can call this as user thread new thread new user helper by default any threade is not a demon threade the way to make a normal threade as a demon threade is by calling do set demon method so let's do that bz thread. set demon and true finally we will start these two threads so BG thread. start and user thread. start now let's run this program so here is what happens the user thread has a sleep timer of 5 Seconds so as long as the thread was started it got assigned to the CPU and once it was assigned to the CPU it went into a sleep stage and then the demon thread was assigned the CPU and during that time there was no threade which was contesting for the CP CPU so the demon threade kept on running in the background so it ran for 4 seconds so remember we had the sleep timer for, millisecond so after every second this was running and printing the message demon helper running as soon as the 5sec got completed this thread came back into existence and it printed user thread done with execution and once this happened then the user threade helper got concluded it got finished with its execution and now what jbmc is is that there is no other threade which needs to be executed so by this time all the user threads which is basically just this threade is done with its execution and now it's time to shut down the jvm and that is where even though this demon thread was not completed because in the normal case of operation it could have ran till the count was less than 500 which was certainly not the case because it ran just for four times but still it was terminated because the user thread was terminated so this is what we learned from from this particular demo that user threads are given the priority demon threads run in the background and once all the user threads are concluded the jvm shuts down the main thread and along with that the demain thread is also forced to be shut down let's learn about a very important concept of thread priority so let's say there are 10 threads in runnable state however there is only one available CPU so only one threade can execute at a given point point of time others will have to wait so who decides which trade gets to run on the CPU well the component who decides this is called as the thread scheduler so each threade has a certain priority to begin with and under normal circumstances the thread with the higher priority gets to run on the CPU please note the keyword under normal circumstances the priority value varies from 1 to 10 and it can be assigned to any thread one priority is represented as the Min priority and 10 priority is represented as the max priority by default the priority of a thread is five and is represented as Norm priority Norm as normal threads of the same priority are executed in fifo manner so the thread scheduler rest stores the threats in a que now let's just spend some time to understand this part of normal circumstance so well what happens is all the main threads are started at a normal priority of five but still they are privileged to be executed first on the CPU by the thread scheduler why so well it's by Design since it's a main thread it has to be executed first otherwise your program will have to wait unnecessarily so this is the first thing which gets to be executed in any case even though the priority of the main thread is five it gets the first priority to be executed on the CPU for the first time now let's have a code example to have a demo of the thread priority concept so let's create this class called as threade priority example let's have a main method we can get the priority of the current threade by calling threade do current trade. get name so let's print this out now let's get the priority of the current trade so we can call trade. current trade. getet priority let's print this out as well now let's get the current threade so threade do current threade do set priority and we have Max priority let's print the updated priority let's run this so the outcome we will expect is the name of the main threade then the current priority which is the default priority because we did not change it earlier and after we change it what is the priority let's run and find out so the name is main to begin with the default priority is five we updated the priority to maximum and then we printed the same let's print something so thread dot current threade doget name says hi now let's create one trade say thre one says high as well and by default the new threade which is threade one will have default priority of five now let's give this the maximum priority possible which is trade do Max priority and then 1 do start so what do you think will happen now that trade one has the maximum priority and the main threade is still at at the default priority of five I should expect that this one should be executed first then I should see the outcome for this one so let's run it out and see what is going to happen well what we see is the first one to get executed is main says High then the second is threade one says high as well so please note that this property of main threade getting the first priority or the higher priority to get executed on the CPU regardless of this having the lower priority which is five as compared to the max priority is only for the first time this thing gets executed once execution is starts it will go by the fif manage and then the thread scheder will schedule the threads based on their priority and if they happen to be of the same priority fif will be used to schedule the threads so this is how we deal with priority in the case of threads in Java now that we have learned the basic concepts of multi threading regarding how we can create trades and how we can wait for Threads to complete using the join keyword Etc now it's time to learn about the important concept of thread synchronization so let me create a package to write the code so let's call this as trade synchronization and I will Implement a class let's call this as synchronization demo so here is what I want to implement in this class I want to have a counter variable that will be initialized with zero and I will be using two threads to increment this number up to certain times and in the end I will check the final value of the counter variable so let's do this so first thing is let's create the main method and uh let's have a counter variable as well let's let's call this as private static int and counter let's initialize this to zero let's create two threads so as seen in the earlier video that we can create threads in line using the Lambda expression so let's do that and uh in this trade what I want to do is that I want to increment the value of counter let's say 10,000 times and let's call this as counter Plus+ let's do the same using another thread so I have a code template that I could make use of so that will save me some time to type the code again let's have this for Loop run 10,000 times and those many times we will increment the counter variable then in the last what I will do is I will start these two threads so let's name this as two oops and uh two. start and before printing the final value of the counter I want to make sure these two threads are completed so the way to do is I would call join on this so let's surr this with try catch let's duplicate it and let's call the same for trade two as well and finally we could print the counter value now let's run this one and see what is the outcome like so let's run this and see the output so the output that we have is 16583 but what we expect I is that since this trade is executing it 10,000 times and this trade is running this for 10,000 times then effectively it should have been 20,000 right but that is not the case what we are having is 16583 how about let's run it one more time and this time we are getting a totally different number let's run it one more time and we are getting a totally different number so this is not correct right then how can we correct this so even before we learn how can we correct this it makes sense to learn why this is happening so the reason for this behavior is something called as a nonatomic operation so what exactly is a nonatomic operation well what we see here is we are incrementing the counter variable counter is counter plus one even though it appears as if the counter variable will be incremented by one in one shot but that is not the case under the hood under the hood what will happen is first the counter variable will be loaded in the memory then it will be incremented by one then this will be assigned back to the counter variable which is holding this particular value and that is where we set the variable value back to counter plus one so essentially this is consisting of three steps so the first step you could think of it as load second step is the actual increment and then set back the value so let's say at this point of time thread one starts operating on this particular logic which is incrementing the counter variable let's say at that point of time the value is zero and uh it incremented the value the incremented value became as one and now it's time to apply this value incremented value back to counter variable but at the same time threade 2 came into the picture and what it did is it said hey let's load the value for counter and it found that the counter value still is zero and then it went ahead so let's call this as this being uh the operation by thread one and now thread 2 came into the picture it said hey let's load the value for counter and it is zero and what I'll do is I'll increment the value so after incrementing the value it became as one and this is where we have thread 2 doing this operation so effectively this value should have been two by this point of time but unfortunately since the time at which this threade one could have set the value as one and applied that value back to the counter variable and that is where the thread 2 came and it intercepted the uh execution and it found out that the value was Zero it incremented it back to one what should have been uh a two here that is still one right so this is the inconsistency and this is the inconsistency due to which we are seeing this error that we are not getting the final value of 20,000 here rather we are stuck at some lesser number so what exactly we call this phenomenon well so this thing is called as race condition in the terminology of multi threading and concurrent programming so what we have here is a shared resource that is the counter we have two threads working on the same shared resource and these kind of scenarios lead to inconsistencies which are called as the risk conditions so how exactly can we fix this so the way to fix this is if you have different uh counters on which these two threads could work on then probably this could work on one counter this could work on another counter but that is not the case here right the whole point of discussing this example is that we have a shared resource and we want to make use of multithreading concept to have some sort of operation applied to this particular share resource so in order to avoid this kind of situation what we can do is we could somehow ensure that this operation right this operation is being done by one and only one threade at a given point of time so that way what we could achieve is this is not being done simultaneously at the same time by two threads and in that sense we could achieve something called as Mutual exclusion and it will ensure that once thread one has the access to this particular logic which is incrementing the counter then it will do all the three parts which is loading the counter incrementing the value and setting back the value to counter so now counter has one and once thread one is done with its execution then the other thread could make use of this particular shared resource in its own execution so essentially we are restricting the users the access to the shared resource by multiple threads at a given point of time and how can we achieve this so one way to achieve this is with the help of keyword synchronized and uh we could do this at multiple levels like have this synchronization at the method level or the Block Level so now let's see how can can be fixed this using the synchronized keyword create an another method and uh let's call this as increment and what this will do is it will simply increment the counter variable let's replace this counter ++ with this increment let's do the same here as well and what we have here now is a method which is going to increment the variable and as I told earlier that we want to ensure that this particular operation is being called by just one method at a given point of time what we can do is we could restrict its access and the way to do this is put a synchronized keyword here so essentially by uses of the synchronized keyword what we are saying that hey jvm please allow this particular method to be accessed by just one and only one threade at a given instant of time at any cost and that is how we have achieved synchronization in a code so this thing is also called as critical section so one way to understand this is that this section is quite critical to the execution of the program in the context of multithreaded environment so we are limiting its access or the users by one and one one thread at a given point of time so with that in mind let's go ahead and run the code and see what is the outcome like so let's run this and now we can see that we are getting the the exact value of 20,000 let's run it a couple of more times and each and every time we see the value remains to be 20,000 itself and there is no changes as such so it feels like we have fixed the issue of synchronization by using this synchronized keyword but there are a few inherent problems with this particular approach the approach that we are talking is using the synchronized keyword at the method level so in order to understand what are those problems let's first understand how does the synchronized keyword work in Java so first thing is let's understand the monitor locks so each object in Java is associated with a monitor which is a mutual exclusion mechanism used for synchronization so when a thread enters a synchronized block or a method it attempts to acquire the monitor lock associated with the object on which the synchronization is applied so imagine that there is a shared room that room could be used by just one person at a time then you have to enter inside the room using some sort of lock you use the lock you open the door you go inside and then once you come out you release the lock you hand over the key to someone else who wants to make use of the room next likewise in every Java object we have this monitor lock which is also sometimes called as the intrinsic lock which needs to be acquired by the thread which wants to make use of the synchronized block or the synchronized method Or the critical section when a threade enters the synchronized blck or method it attempts to acquire the monitor Lock And if the lock is available the thread acquires the lock and proceeds to execute the synchronized code so what do we mean by the lock is available well what it means is no other thread will be currently holding that lock so if the lock is not available that is another thread is holding that lock then the thread enters a blocked state and it has to wait until the lock becomes available the second step of this process is releasing of the monitor lock so when threade exits let's say the threade is going to exit after this execution is completed it has to release the monitor lock and thus it allows other threads waiting for to acquire the lock and to proceed with their execution so the monitor lock used by the synchronized keyword is sometimes referred to as the intrinsic lock or the monitored lock of the object instance each object in Java has its own intrinsic lock and the synchronized keyword acquires and releases this lock implicitely when used at the method level or in the synchronized block so what is the problem exactly if we use this synchronized keyword at the method level so the first problem is that it's a kind of course grained locking so when we use synchronized at the method level it applies the log to the entire method body even though in this case our method body is just a one line but in actual code base it could span to multiple lines so essentially what you are doing is you are blocking any other threade from entering that particular method when you have applied the synchronized block on that particular method but your critical section could be just two or three lines so in that sense it does not make sense to block an entire method which could probably let's say have 30 or 40 lines right so that is the first problem with using the synchronized keyword at the method level and needless to say this leads to reduced concurrency and performance bottlenecks and uh the second thing is kind of related to the first issue itself so when synchronized is used at the method level we lose the fine grin control needed in the more complex scenarios so for example you might want to synchronize only a specific section of the code within the method or you might need to synchronized multiple methods together as an atomic operation using method level synchronization we you don't have this granularity level and the third issue is let's say uh when a subass is overriding a synchronized method from its super class it must also explicitly declare the method as synchronized if it wants to maintain the synchronization Behavior and the failure to do so can lead to unexpected behavior and potential synchronization issues so you see there are multiple issues with the synchronized keyword when we use this at the method level so let's understand this with some code example in the same code let me do some refactoring let's call this as counter one let's create another variable let's call this as counter 2 and let's call this as increment one and this as increment two and uh what we can do next is we can duplicate this method and uh let's call this as increment one and it's going to increment the counter one let's call this as increment two and it's going to increment in the counter two let's print these values in the last so counter two so from the functionality perspective it's surely going to work fine so counter one will be 10,000 and counter two will be 10,000 let's run it and confirm yeah this is working fine let's rerun it a couple of more times to ensure that it's working as expected so it's working fine but there is a problem so if you look closely what we have done is we have used the synchronize at the method level so let's say thread one starts working and at that particular point of time increment one is getting invoked so thread one has acquired the lock to this particular code this particular method and it's working on counter one but if you notice threade 2 has no business with counter one right because it's going to operate only for incrementing the counter two so in in this case even though threade 2 is not directly working on counter one it will be blocked because this synchronized behavior is going to acquire the class level log so this is the class level log for this particular class there is only one log which is this intrinsic log or the class level log or the Monitor and that is acquired or held by the thread one and in that sense threade 2 even though it was not supposed to be blocked it's blocked now so I I think you see the problem now and that is one major problem with the synchronization at the method level so uh to address these concerns it's often recommended to use explicit locking with synchronized blocks or to use more flexible concurrency utilities provided in the Java util concurrent package such as lock interfaces reant locks read right lock Etc and uh we will be learning all these things going forward in this tutorial but for now let's understand how can we fix this scenario by the uses of custom logs so let me create another class let's call this as loog with custom objects so first of all let's have the counter variables so private static in counter 1 as zero let's duplicate this let's call this as counter 2 and let's have the main method in the main method we will have have two threads so let's call this as thread one let's call this as thread to and this will be incremented for 10,000 times let's call the increment one let's copy this one and let's rename this to increment 2 and uh let's start the threads so two do start and let's wait them to complete their execution so in catch we will have interrupted exception maybe call this as e and then we want to throw a runtime exception let's call 1. joint and then do. join and let's print the value counter to what we want to do now is we want to implement the increment methods so increment one and this time we don't want to have the synchronized keyword at the method level rather where we want to place this is at the Block Level where we want to do the synchronization so this will be counter 1 ++ and if you notice when we place this at the Block Level it's expecting us to pass something what does it expect us to pass well it expects us to pass the lock on which we want to lock this critical section so that two different thread for this particular critical section cannot enter in this code block so the way we will do it is that we will Define two logs so let's call this as private static final object log one new object and let's do for log two as well in the similar Manner and now we can have this on lock one so let's duplicate this let's call this as increment 2 and increment 2 right so basically what we have done is that we have the increment one method and then increment two method and these two methods are supposed to work for two different variables counter 1 and counter 2 and since these two methods are being used by two different threads and moreover these two methods are working on two different different shared resources not the same one which is counter one and counter two we want to lock it on two different locks so basically for this critical section if you want to acquire this particular area you have to access lock one if you want to acquire this particular area you have to access loog two and this is how we have decoupled we have separated out the concerns for these two different threads thread one and thread two so this is how we make use of this synchronized keyword at the Block Level and fix the issues with the users of synchronized keyword at the method level but having said so it's not a rule rather it's just a guidance or a general opinion but if in your case if your method happens to be quite a short one and does not have any other implementation and moreover let's say the entire method block is itself the critical section in that case you can go ahead with the method level synchronized keyword as well but more often than not this is not the recommended approach rather you should explore the option of going with the synchronized block itself now that we have seen the concept of synchronization it's time to understand a very important concept of wait and notify so before we start with the concept of weight and notify let's see what we have on the screen so let's imagine there is a room and the room always contains some sort of parcel and the catch here is that the parcel could belong to anyone so let's say there are four or five people and their parcel is getting delivered so you need to go inside the room and then take out that parcel but the room is locked and there is a security who is there on the room entry and the only way to access the room is to get that lock from the security once you have the lock you can go inside the room F your parcel and come out but here is the catch at a given point of time only one person is allowed to access the room everybody else will have to wait so how this can be implemented so the way it could work is that once the security is giving a lock to anyone they will allow them to go inside and F their parcel until the time whosoever is coming to access the room the security could tell them that somebody's already inside the room so you will have to wait till that person comes out so once the person who is inside the room gets his parcel he comes out and he hands the lock back to the security Now the security will notify that the room has been vacated by the earlier person and anybody else who was waiting for the room to access it could go inside and get their parcel but again they will have to ask the security for the access to the lock and then they will open the lock and then they will go inside the room to get their parcel so you see there is a mechanism which is is kind of allowing just a single person entry inside the room in order for them to do something inside the room which is the activity of collecting their parcel until the time anyone is inside the room the entire room is considered to be frozen or kind of locked and nobody could access it and as soon as that person is coming out then they are handing the key to the security and after that is done the security is notifying to everybody else that now room is available to be accessed and then in anyone from the waiting line could come and collect it so the wait and notify methods work in a very similar way so when a threade calls wait on the lock then that thread is suspended from its uh execution and it goes to something called as a waiting State and when it goes to the waiting State then others who were in The Waiting State earlier they have a chance to get hold of the lock and then they could start with their execution and as soon as they are done with their execution they could call notify and notify is a message to all the waiting threads that now whatever they were waiting for that lock has been released and then they could take that lock and access their critical section to work upon so that is the whole idea behind the wait and notify methods in Java now we will see this with one code example and it will become more clear so in order to demonstrate the working of weit and notify I have created this class it's called as cre and notify demo so first let's start with writing the main method next we will create two threads so let's call this as thread one and this one as threade two let's start the first one let's start the second one and uh let's create a l as well so call it as public static private static final object lock and new object let's have two worker methods which will be invoked by the different threads that we have created so first one is public static void let's call this as one and it will throw an interrupted exception why we will see it later so what we will do is we will synchronize on the lock and the first thing that we do is we print some message like hello from method one and then on the Lock This Thread is going to call wait and then we print some message back again from the method one or maybe back again in the method one makes more sense let's create the other method as well it will be called as two so why did we throw the interrupted exception so when we call wait or notify these are interruptible and they will throw an interrupted except that that is the reason we need to have this in the method signature so let's put synchronization on the lock and uh let's have this message hello from method 2 and now this time let's call notify and let's print some message hello from method 2 even after notifying and uh let's call these methods in the threads so this will call one let's add this in the TR catch block let's call to here and let's add this in the TR catch block as well and now let's try to analyze what's going to happen so what we have done is we have created the thread one and it's going to make use of this particular method so what this method is doing is let's say thread one is started and after that the control comes here so in the synchronized block the lock has been acquired and this print message will be executed so we will see this message on the console now what does this thread do is that it calls weight so this thread will be suspended and it will go in a waiting State since the other thread was just started as well and what this thread is doing is it is calling method two the execution comes here and since weit was called so lock was available so this lock was acquired by this particular thread and this message will be printed that hello from method 2 and then we call do notify so one important thing to note here is that even when we call notify then whatever code execution is left out for that particular synchronized block all those things will be executed only after they are executed then the notify will come in effect other threads which are waiting for the lock to be released they will acquire it and start with their process so basically after notify is called it's not that the control will go directly to this particular part rather it will print out the remaining thing which is Hello from method to even after notifying and then the control comes here to this threade and then it will print this message that back again in the method one so let's execute this and see the outcome so we are here run wait an demo so here is the outcome so first is Hello from method one and then we called wait so it goes to the other thread it says hello from method 2 and after notify we had another message as well so that is getting printed that is Hello from method 2 even after notifying then we have the control back here which is back again in the method one so you may have a question that what is the difference between weight and sleep are in these both same same right so on the surface they do look quite similar but there is a key difference that weight is used for inter thre communication and synchronization while sleep is used for just pausing the execution of the given thread for a specified duration and please note that this is just one implementation of weight method there are other implementations as well so when you call weight with this time out information it causes the current threade to wait until it is awakened typically by being notified or interrupted otherwise if not so then after the time has elapsed it will be automatically awakened and there is another variant of notify which is notify all so in the case of notify it pickes up single thread but in the case of notify all it notifies all the waiting threads that are waiting for a given lock so this should be it for the introduction of wait and notify methods in Java now let's go ahead and make use of it to to implement the producer and consumer problem so what is producer consumer problem the producer consumer problem is a synchronization scenario where one or more producer trades generate data and put it into a shared buffer while one or more consumer trades retrieve and process the data from the buffer concurrently such kind of a pattern which is used to teach the concept of synchronization so consider this image here so let's assume that this thing is some sort of buffer some sort of container or placeholder where we want to put some data but the catch here is that producer is the one which is going to put this data here and the consumer is the one which is going to consume the data from here so what I want to implement now is two threads one is the producer threade which will be putting data in this particular shared container and the other thread is the consumer thread which will be consuming data from from this shared container and at the time when the container is full the producer will stop producer and at the time when the consumer is empty the consumer will stop consuming so let's go ahead and implement the same so to implement the producer consumer problem here's the class that I have created and uh it's called as producer consumer and uh there would be a supportive class called as worker so let's create that as well let's call it as worker and the worker will have two requirements one is to produce the element and put in the shared space so let's call this as public void produce and the other would be public void consume using this the worker can consume from the shared space and this worker will need certain properties so the first one is sequence so essentially what we are going to put in the shared space is number so 0 1 2 3 4 5 something like that and it will start from zero so we are calling the same as sequence as zero the next thing is private final integer let's call this as top so this is the maximum number of elements that can be stored in the shared area shared container let's have another parameter called as bottom so this is the least amount of elements that can be kept in the shared container and the next thing is private final list so this is the container itself here we will be placing these numbers that we are generating so here the producer will be generating the numbers and the consumer will be consuming from the next is that we need some sort of lock let's create the lock as well let's let call this as lock and initialize this as an object in order to initialize all these things we will have to have a Constructor all right looks fine to me now let's go ahead and implement the produce method so first thing is that we need to synchronize on the lock and and what I want is that produce should be able to keep on producing these numbers infinitely until a certain condition is met and likewise and the consumer should keep on consuming infinitely until certain condition is met so let's start with producer so let's call this as while true and if container do size is the maximum one then let's print a message so essentially the container has become full so Container full waiting for items to be removed and since we are waiting we will call do weight and this weight we will have to throw an interrupted exception so let's add that and if it's not full then let's print a message so sequence added to the container and let's add this to the container and let's call notify and in order to see the demo we should add some sort of delay so let's add a delay of of let's say 500 millisecond so what's happening here exactly well this thing for sure is going to run infinitely and that is the reason we are getting certain warnings so may not be the most optimal way to implement this scenario but this is fine for the demo purposes moving on coming to the logic so first thing which is checked is is the container full if the container is full then this message is thrown and the thread goes to a waiting State else and then we start adding to the container what we is we first print the message and then we add that sequence number to the container and after it is added we increment that number so it's a post increment so that the next number could be added to the container in the sequence now we are calling log. notify so if you remember the notify discussion from a few minutes back what we understood is that notify does not come into existence immediately rather whatever is there in the synchronized block everything will be executed and only after that is executed then notify will come into effect and the other waiting threads will be notified so here what happens is that we call notify and then the call comes here so since we are into while. true and we check that container size still not full then we come to the else and then we go about executing this so this happens till the time the size is not full and then log. WID is encountered and then this thread goes to a waiting state so this is how this thing is implemented and executed now let's go ahead and implement the consume method as well now let's implement the consume method so for the consume as well let's throw the interrupted exception because we will need to add the later point of time all right so let's put all of this inside a synchronized block and like we did for the case of produce we want this to run infinitely as well so we will put this inside a continuous file Loop and if container do size so if the container is empty then we don't need to consume anything so let's print a message container empty waiting for items to be added and then let's go to a waiting state in the else section let's start consuming so what we say is container do remove first and then removed from the container so basically we are going to remove from the list that is the shared container and print the message and once this is removed then we will call log dot notify and here as well we will have a sleep for let's say 500 milliseconds to show the simulation in somewhat slower manner so that it's easy to read and understand right so let's understand what is happening here we are going to do this infinitely and uh initially we check if the container is empty if it's empty print a message and go to a waiting state if it's not empty come to the else section and then let's remove the element from container and uh print it on the console and then we call notify and given the fact that notify is not realized immediately it will keep on executing whatever is there in the synchronized block and uh since this entire thing is there inside this file Loop this will keep on executing till the time we are not meeting a scenario of dot weight so this keeps on happening and finally we encounter a situation wherein it becomes empty the container becomes empty wait the lock. weight and we go to the waiting State and then this particular thread which had gone to the waiting State earlier comes into effect and then it starts adding again so this is how this is going to work now let's implement the main method so of course we will have to move all this worker class outside this particular class so let's skip it outside so in the producer consumer class let's create the main method and uh in the main method first of all we will create a worker object which will be initialize with five and zero so the shared container could contain up to five elements maximum and uh minimum would be empty so let's create the threads call this as producer create another thread let's call this as consumer and inside the producer threade we can say worker dot produce and of course it's going to throw interrupt exception so let's handle it accordingly and here we could call consume let's handle it accordingly and finally let's start these threads so producer. start consumer do start so let's run this and see what we are getting so let's run this all right so here is the output so you can see Zero added to the container one added two added three added four added so and so forth now let's stop it and analyze so basically first zero is added then one is added and so and so forth four is added and once fourth is added then the container is full and then it's going for a state where it wants the items to be removed and then the consumer part is activated and it's going to remove the items from the container so zero removed one removed and so on and so forth and finally the container becomes empty and then it's waiting for the items to be added this process will keep on happening because we had implemented in such a way that uh it's going to happen for infinite time period And this is one way in which we could implement the producer consumer problem of course there are many other ways as well but this is one way in which we could implement it till now we have learned a couple of ways in which we can create threads in Java such as creating the threads using extending the thread class and implementing the runnable interface how can we create multiple threads let's say if we want to run five tasks then we can create threads in the for Loop and run these tasks what can we do if we want to execute 500 tasks synchronously we can for sure create 500 trades using the for Loop right but where does it stop what do we do if we have to run 1,000 tasks create th000 threads in a loop well something does not sound right here here is the problem in Java one thread is equal to 1 OS level thread creating a thread is an expensive operation so creating thousand trades in a loop is certainly not a scalable approach what could be a more practical approach is to have a fixed number of threads and let's create them up front imagine a pool of end threads and these trades handle the Thousand tasks among themselves and this is exactly what the executor service helps us in achieving in one line we can Define the executor Service as a tool in Java for managing and running tasks concurrently across multiple threads so executor service helps us in creating a bunch of threads a pool of threads does the name threade pool and the threads are not killed once they are done executing the task rather they are reused to execute the another task thus by making use of the executor service we save time needed for thread creation and making things more efficient and manageable there are four types of executors provided by the execut service and these are single thread executor fixed thread pool executor cach thread pool executor and sheduled executor now let's quickly visualize how internally the executor service looks and functions so imagine a execution of the main threade which is this one this is the execution line and the main threade calls for the executor service and now the executor service the way it functions is that it maintains a thread pool there are couple of threads that are there inside the thread pool this depends on the type of executor we are using but essentially you will have a couple of Trades which are created in advance then each type of the executor service will have a blocking queue and this is where the tasks are placed so when we say executor do execute and the task so what I want is that a task should be picked by either of these threads and it should be executed but for the time being it's not getting executed it will be sitting inside this blocking queue so in nutshell the executor will have two things first is the threade pool and then there is a priority queue and there are just two steps the new task will keep on adding to the blocking queue and the thread which is available to execute the new task can pick the task from the blocking queue and it starts its execution and once the execution is done and the threade is available to pick another another task it will pick another task from the blocking queue so more or less all the executors look similar to this and this is the way in which they function of course there are certain fundamental differences in which the que will be managed and the way in which the threats will be created now what we will be doing is write some code making use of these executors and then understand how do they work so let's begin in order to demonstrate the working of the executor service I have already created this package that is the executor service and now we will learn about the single threade executor so let's create a class let's call this as single executor demo we will have a class let's call this as task and it's going to implementer unable and it will have a task ID so let's call this as task ID let's have the Constructor let's overwrite the run method and in the run method we will print a message task with ID task ID being executed by thread and then trade dot current trade dog name and let's put some sleep in the thread let's have it for half second and in the catch handle the interrupted exception so we will just throw the exception for now let's call this as a runtime exception and pass the exception here here we will create the main method and in the main method what we will do is we will create the single thread executor now so in order to create it we have the executors dot new single thread executor and let's assign this to service now the idea is complaining and showing some warning message let's see what does it say it says that executor service used without private resources statement so the executor service should be closed once we are done using it there is a way in which we could close it and that is by calling the service do shutdown but there is a different approach as well in which we can implement the same and that is is to use the TR with resources so let's do this and put it under the try with resources and now we should be good to go so let's say I want to run this task 5 times so for in I as 0 I less than 5 i++ and I can say service. and then call the execute so what is the runable that I want to execute that is my task and pass I as the task ID so as opposed to what we were doing earlier that after creating the threade we used to call the dot start in order for the thread to get started with its operation here we don't need to do it rather once we submit this to the executor service by calling do execute it is handled by the execut service itself now let's run this code and see what is the output like so let's run this so here is the output so basically it's going to print task ID with zero being executed by this particular thread and task ID with one being executed by this particular thread and so on so forth this is what we have implemented in the code as well right the idea is that there is a runable and what it does is it's simply going to print the task ID along with the threade by which it is getting executed so if we had not used the executor service what we would have done is we could have created Five threads and using those five threads I could have assigned the reable for each of the thread and then should have called The Dot start on that but here we are able to achieve the same kind of result using the executor service and this implementation in particular which is new single thre executor so now the implementation looks much more cleaner and we don't have to deal with the creation and destruction of the threads now everything is being handled by the executor service itself let's learn how the single thread executor Works under the hood so we have the main trade and this way we can create in tasks and submit or execute those tasks one by one so here we have the threade pool and in the threade pool there is a single thread there is a task Q which is containing task zero task one task 2 up to task n and what this single thread executor will do is it will pick the task from the task pool or the task queue and then execute it so Fitch task and execute so in the case of single thread executor the size of the threade pool is just one so we have just one threade which is going to Fitch the tasks from the task Cube and run this and if it so happens that due to some exception the thread is killed the executor will make sure to recreate the thread and the execution of the tasks won't be stopped by using this kind of thread pool we can ensure that the task zero is always ran before task one and task one is always ran before task 2 since we have just one threade here it's guaranteed that the tasks would be run sequentially now we will learn about fixed threade pool so let's create the class let's call this as fixed trade pool demo let's maximize this one and as we had done earlier we will be creating a runable which will be the task which needs to be executed by the executor service so let's do that let's have a class called as work it's going to implement runable and we can start by overwriting the run method public void run and it will have certain parameters so the first field is work ID and then it will have a Constructor which is going to initialize this work ID and in the run method what we will do is we will print a similar kind of message the task ID let's provide the work ID here being executed by thread threade dot current thread do get name let's put some sleep timer here so again let's go with half a second and in the catch let's handle the interrupted exception let's throw runtime exception and pass the exception and now let's go to the main method and very similar to what we did for the single thread executor we will also be creating the executor using the try with resources so try executor service let's call this as service and executors Dot new fixed trade pool let's say that I want to have a thread pool of size two and uh I want to execute this task seven times or what you could say is I want to run seven tasks so let's make that so int I as z i less than 7 I ++ and let's call service. execute new work pass the ID so let's print this and see how is the outcome like so let's run this so here is the output so task ID one is being executed by thread 2 task id0 is being executed by thread 1 task ID 2 is being executed by thread 2 and so and so forth so one important thing to note here is that since we had two threads created these two threads are there in the thread pool and they are taking turns in order to execute my task so first thread 2 is being used to execute the task then we have thread one then I have threade two then we have thread 1 thread 2 thread 1 thre 2 and so and so forth please note that this may not be alternating in the stricted terms this is just instance wherein we got this kind of pattern but this does not mean that in all the cases it will be one by one it could so happen that threade two could execute certain tasks in one sequence and then after that thread one could be involved and then thread one could be assigned with certain tasks for execution so this is what we have achieved by making use of the fixed threade pool so to submit up we have created a new fixed trade pool and there are two trades and when I want to execute seven tasks then those seven tasks are picked one by one by either of the available two threads and they are executed now let's have a quick look around how exactly does the fixed threade pool work under the hood so what you see here is a visual representation of the fixed thread pool executor and here in the case of fixed thread pool there is a fixed number of threads as the name suggests that there will be certain number of threads which is precreated and this executor also has a task que where all the tasks will be placed so consider this as a task que of some sort the threads in the fixed thread pool pick the task from the task que and execute them so when the thread is done with the execution of a task it goes ahead and picks another task and this keeps on happening until all the tasks are completed so the basic idea Remains the Same that there is a thread pool and the threads are fixed in the count and once you have tasks flowing in in the task queue these trades will start taking out the task from the task queue start to work on those and once all the tasks are done and this keeps on happening till the time the task que had certain tasks in it so overall This Thread pool executor is fing the task and executing it over and over again now let's learn about the cach threade pool executor in order to do so we will write a code to demonstrate its functionality and let's name the class as cast threade demo we will have a task so create a class for the same let's call this as task one it will implement the runable interface let's overwrite the run method and this class will have some sort of task ID let's call this as task ID initialize the same inside the Constructor and it can give some message something like task then task ID plus being executed by trade dot current trade. get name then we will put this threade to sleep for let's say 500 milliseconds and Surround this with try catch now let's write the main method and we have execut a service let's call this as service and what we want is new cached threade pool let's keep it inside the trate resources and one key thing to observe here is that there is no value that we are going to pass inside the new cach threade pool un like the fixed trade pool where we were passing the number of Trades that we want to create a front why is it so we will see in the next section for now let's run this thing probably for maybe th000 tasks so I less than 1,000 i++ and service. called execute new task one and give it a task ID of I so let's execute this and say it's output all right so here is our output you see all the tasks are being executed by the threads that are getting created so task 27 is being executed by the thread 28 task 41 is by thread 42 task 4 is by five and then Task 1 by two so that's it for the code demonstration now let's understand how does it work internally now let's visualize and learn the internal working s of cast thread pool executor in the case of casted thread pool we don't have a fixed number of threads so you see in the code implementation we did not provide any number to the method which was creating the new cast threade pool the task you here does not hold a number of tasks which we submit the queue here is of a special type and it's called as a synchronous queue the synchronous queue has space only for a single task so every time you submit a new task the casted threade pool holds the task in the synchronous queue and it searches for the threads which has been already created and they are not working actively on any task if no such thread is available the executor will create a new thread and add it to the thread pool and this newly created thread starts to execute the task which has been submitted let's imagine a scenario where 10 threads are executing 10 tasks at a given point of time and then 11th task comes in and since we don't have a thread 11 to C to this task the thread pool will create the 11th thread to execute this new task so in theory the cast thread pool is capable of creating thousands and thousands of threads you may wonder where does this stop if threads are getting created at such a rapid paase so what is the upper limit well there is a guard rail to keep the threade count in check after some time the threads are done EX executing the tasks and they are available to pick another task and if they don't have any other task they are killed and this time of idleness in 60 seconds so what I mean is let's say there is a trade and it has not received any task to work upon it will be terminated so cash trade pool is pretty much autoscaling in nature on the basis of task load it has when there are new tasks to be executed threads are added and when there are less number of tasks to be executed a few among the created threads could handle the task so the other threads the idle ones are retired and killed so that's how the cash threade pool executor Works behind the scenes so the basic takeaway from here is that the queue will contain only one task at a given point of time that is your synchronous queue and the maximum time for which a thread could be there in the thread pool without any work is 60 seconds so now let's move ahead and learn about the final type of executor which is the scheduled executor now let's write the code for demonstrating the scheduled executor service let's create this class let's call this as scheduled executor demo we will create a task let's call this as probe task it will implement the runnable interface let's overwrite the run method and run method is going to print a message let's say probing endpoint for updates let's create the main method so we have scheduled executor service let's call this ad service and from the executors let's call the new scheduled threade pool new shedu threade pool let's have the number of Trades as one and the scheduled executor service does offer a couple of methods in which we could schedule the thades to be executed we will go ahead with the implementation which is scheduled at fixed rate so the idea is that this particular threade will be executed at a fixed rate after certain point of time so the way to write this is service Dot schedule at fixed rate so we have the actual runable then the initial delay followed by the period and then the time unit so runable is the runable initial delay is after how much time this execution will start so let's say you could pause for 2 seconds and then after 2 seconds every 3 seconds this trade could be executed so let's write it so the runable here is the probe task and let's say the first delay that I want is of 1,000 milliseconds and I want this to run every 2,000 milliseconds and the time unit is going to be milliseconds now let's execute this and see how is the output like so we are seeing that this thing is getting printed probing in point for updates and it will keep on printing this every 2 seconds and it will keep on printing this probing in point for updates till the time we shut down the executor service so how can we do that let's stop this and write some code for the same so the way to do this is we can call the await termination method on the service handle this object and Supply certain time so it will wait for the executor to be terminated in that time range and if it's not terminated in that time time range it will call the shutdown method of the executor service which is going to gracefully terminate the executor service so let's write the code for same so we will put this inside a tri block because of a termination is going to thr an interrupted exception let's have the interrupted exception as well and uh and in the tri block we can check service. AWA termination let's give it a time of 5,000 milliseconds and then time unit is milliseconds so if the time is more than 5,000 milliseconds we can call the shutdown method so let's say shutdown now else if it goes to the interrupted block there as well we could call shut down now let's increment this time to maybe 10 seconds and now let's run the code and see its output so you see the delay that we had was of 2 seconds that every 2 seconds this should be printed and after printing it for five times that is like 10 seconds it got terminated on its own so that is how we could terminate the scheduled executor so of course this is a very crude and basic way of writing things in real world probably we could have this termination Logic on certain condition and when that condition is getting fulfilled we can call the shut shut down method and close the executor service so as I told earlier that this is one of the methods which is provided by the sheduled executor service I will suggest you that as part of exercise you could try to go inside the scheduled executor service and explore the entire interface as in what all different scheduling options are there for example we have a schedule then we have this schedule method signature then we have something called that schedule that fix rate that we implemented then we have have something which is called as schedule with fixed DeLay So I think it will be a good exercise for you to go inside this class and learn about this methods and the other thing is that we have called shutdown now so what it means is that this is a strict guideline to the executor service that whatever you are doing stop doing that and shut down the operations of the executor service there is another method however which is called as shutdown so shutdown is somewhat more graceful in nature and the way it works that it initiates an orderly shutdown in which previously submitted tasks are executed so whatever is there inside your task Q those will be executed and only after that the executor service will be shut down but once you call shut down then the new tasks won't be allowed to come inside the task queue but in the case of shutdown now all the actively executing tasks are also halted and stopped so this is the basic difference now we will have a quick look to understand and visualize the internal workings of the scheduled executor service so as we saw earlier that this trade pool is for the kind of tasks which we want to schedule in some Manner and the way this works is all the tasks are submitted in a task queue but this queue is a delay queue in the delay queue tasks are not kept in sequential manner we can understand this to be some sort of priority queue where the priority is the time of execution so if there are two tasks for example task zero it's supposed to be run after 10 seconds from the current time and task one is supposed to be run after 15 seconds of the current time then in that case task zero should come earlier as compared to the task one in the task Q likewise if task one is supposed to be executed first before task two then it should be coming earlier in the task Q so everything everything else Remains the Same other than the Q which is a delay Q in the case of scheduled threade pool executor how do we find the ideal pool size for the thread pool well like most of the answers in computer science the answer to this question as well is that it depends however it's worth discussing what it depends on so let's get started imagine that you have a CPU intensive task task which needs to be executed please note that in Java one thread is one O Level thread so if your CPU has four cores then at maximum you could execute four tasks in parallel at once so on a hardware which can only support up to four course creating hundreds of threads won't help if they are CPU intensive in fact it leads to a performance degradation let's focus more on why there is a performance degradation when you have 100 trades to run your CPU intensive task then all such trades will try their best to get their P of the time allocation with the CPU when this time slicing happens Beyond a certain threshold the expense of content switching overpowers the benefit we get from making use of the multithreading so lots of Trades contesting for CPU time is counterproductive and in such a scenario having too many threads is not a good idea what we can do rather is have the same number of threads as the number of course in your CPU but please note that having created the same number of threads as the number of course in your CPU does not ensure that all the course will be running your threads only some of the course will be utilized by other tasks and Os level processes as well so how can we do this let's say a quick code example in the executor service package let's create this class let's call this as CPU intensive task so let's create a task class and maybe let's call this as CPU task it implements our unable let's overwrite the run method and all it does is it prints some message like some CPU intensive task being done by the and thread dot current thre do get name let's write the main method in the main method first we will try to get the number of the course and we can do so by making use of the runtime so runtime. get runtime do available processors now let's create the executor service call this ad service executors do new fixed threade pool and it could accept the number of of course as the number of threads it wants to create let's print a message for debug purposes so created trade pool with course next let's execute our task so let's say I want to execute 20 tasks and service. execute and new CPU task let's run this and see the output here is the output so created thread pool with 14 cores so my machine has 14 cores and what it does is it says like some CPU intensive task being done by thread 1 thread 2 thread 10 thread 14 thread 14 so what we see is it has created 14 threads which is the number of cod and then those 14 trates are being used to execute the 20 tasks that we want to execute so now let's learn about the other type of task so there is another type of task which is IO intensive such IO intensive tasks are inherently fire and forget in nature this is because IO operations such as reading from files or making Network calls often involve waiting for external resources to respond during this waiting period the CPU is idle and having more threads allows other tasks to execute while one thread is waiting for the io operation to complete however here as well it's not recommended to go crazy with the countless number of threads rather the more St approach is to experiment with threade pool sizes this will help you to find the optimal balance between concurrency and resource utilization so to summarize there is no single answer for the question of what's the ideal thread pool size what you could do instead is analyze your task patterns and based on their type utilize a combination of the above mentioned approaches till now we have seen how can we execute trades using the executor service but something seems not complete in all the examples that we have seen so far we are not returning anything what if we want to return something from the thread execution how can we do that let's done about the same so here I have created a class this is called as callable demo and we have a class for the task which is implementing the runnable interface and inside the run method that we have overridden I'm trying to return something well of course it's giving some compilation error because at the moment there is no way in which I could return some value from this method why so let's go to the runnable interface and what we see here is that the runnable interface has just one method which is run and it's returning a void that means we cannot return anything so what is the solution so if we want to return anything from a thread the way to do this is implement the callable interface so callable is another interface provided in Java and the way it works is that you implement this interface and it's a generic based class so you'll have to pass the generics of what exactly you want to return so let's say that I I want to return an integer so I'll pass the same and now let's get rid of this one and see what are my implementation options so like we had run in the case of runable in the case of callable we have call and here whatever you provide in the generics the same thing is applied here that is the integer and let's say I want to return 12 so now this 12 should be returned whenever I'm going to run this trade but now this part of the code starts to give some error so what is the problem well the problem is that executor service provides two ways in which we can execute or submit threads to the executor service one is the execute other is the submit so let's replace this with the submit and now this should work fine so what submit expects us is to pass either a runable or a callable so even if you want to pass a unable that is fine with submit and for callable anyway you have to use the sub MIT method itself because you cannot submit a callable by using the execute method so now that we have called the do submit on the execut service what does it return should it return an integer let's find it out so what we see here is that this thing is going to return a future so what is a future future is again a class which is generic based and in this case since we were returning an integer it's going to supply this inte is a value over here and using the result we can do all couple of things and one of those things is calling the do get method on this result so let's print it out and it's giving some error reason is that it is expecting us to either add a catch clause or maybe surround with try catch so for now maybe I'll just add the exception method to the signature so now let's run this code and find out what is the outcome so yes indeed it's is going to return the value that is 12 and this is how we are supposed to make use of callable if we want to return some value from a thread execution but there are a few things we should be mindful of while using the colable interface I'm calling the do get method on the future class let's learn about the same so what we have here is the main threade so assume that this line is the main threade execution and then at some point of time you created a callable and then you did a submission of that callable so once that callable is submitted to the threade pool you will immediately get a future of the type of the generics for the callable that you have implemented and using the future object sometime down the execution you could call future.get so what happens is future is a placeholder and it's empty till the time the processing is not completed by your thread and that time could be anything depending on the execution that you have in your code right so let's say at this point of time you called future.get and the future did not have the final calculated value so what happens please note that future is a blocking operation what it means is that if you call Future and if there is no value inside the future then your entire main threade execution will be blocked so whatever was being executed by the main threade that will be entirely blocked let's say at some point of time the future got the value so you will be able to get the value from here and then the execution will resume or start so this is one thing that you should always keep in mind while making use of the future.get call so now let's demonstrate the same thing let's give another print statement let's call this as main trade execution completed let's run this so we can see that the result is pretty much instantaneous so we are getting the result as soon as possible now let's put some sleep in this threade so let's call thread. sleep maybe let's have a sleep timer of 5 Seconds and with this let's try to execute this one more time so we are waiting there is no result as of now so just after 5 seconds once the execution got completed then we see this line that main thread execution completed so basically what we did was we introduced some sort of latency in the call method and when we called result doget since the future did not have the result populated for this so the main thread was blocked and as soon as this future got its value the value was printed and the execution continued so is there any work around for this Behavior so result. gate has an overloaded method with the signature where you could pass in the timeout so what this will do is it will wait for a certain time before giving a timeout exception and in that time duration if the future does not have the value it will throw an exception and move ahead in the execution otherwise if you get the value in that timeout it will work as expected so let's try the same here it's expected that this call method will be taking 5 Seconds to run so let's try to put a time out of 1 second and let's pass the time unit of second this thing throws an exception let's handle that now let's run it one more time after 1 second it could not find the result and then it threw an exception that is the timeout exception and it ended the execution so if you don't want to end the execution this abruptly and probably continue with your execution you could handle this exception in some other manner so that the execution of your main thread continues now let's increase this value to 6 seconds and find out the result so now since we have provided a timeout of 6 seconds and this thread needs 5 Seconds to complete the execution got completed and then we had this message printed on the console that is main thre execution completed so this is good but is there any other way in which we could handle the f Futures so there are three important methods that we should know so the first one is future. cancel and it expects us to pass a true or false so let's say if I pass true and let's say what exactly does it mean so this value is May interrupt if running so if you pass true then let's say if the threade is running then in that case it may get interrupted if you pass false then it will not be interrupted so this is about the do cancel method the next thing is Future dot is cancelled and this returns a Boolean so using this you can check in your code that the given future is it cancelled or not and based on that you could Implement certain functionality in your class there is another important method which is called as is done so it's result. is done and this again results into a Boolean value and what it signifies is that whatever threade was being executed has that completed please note that this will return true for both the cases where the thread is going to run successfully and return some value and in those cases as well wherein the trade is getting interrupted or may be exiting out due to certain error or exceptions this is the way in which we can make use of the callable interface and based on your use case you can use either runnable or callable to implement threads in Java Java has a rich set of collection classes however most of them are not thread safe we need to take precaution to ensure that the collections are behaving as intended when used in a multithreaded context there are two ways to do so the first approach is to use the collections. synchronized method and the second approach is that we can use concurrent collections which are essentially concurrent versions of the collections in this video we will learn about the first approach in order to demonstrate the uses of collections. synchronized method I have created this class called as synchronized collection so let's start by creating uh array list let's call this as list and I will be creating two threads the first one is called as one what this will do is it will add some value to the list thousand times so list. add I and let's close this one I will be creating an another thread and it will also add some value to the same list, times list. add I so let's move these things to the main method otherwise this will not work let's create a main method let's place the things here and now 1 do start to do start and in order to ensure that these threads are getting completed let's call the do join on these two methods second is to do join and now let's print the size of the list after the operation has been completed now let's run this program and see what is the output so you see the output is 1 146 however we are adding value to the list a, times and this is being done by two threads so what we should have expected is that the value would be 2,000 but here the ism is mat let's run it one more time and this time we are getting a totally different value so the point here being that it's leading to some sort of inconsistency when this is being used by different threads so how can we solve this so one approach is to make this thing as synchronized so as discussed earlier in the video that there are two approaches to make use of the concurrent collections in Java one is by using the collections. synchronized method and the other is to use the concurrent collection itself so here we will be learning about the synchronized method so let's do that and the wi to do this is let's comment it out let's create a new one so list integer list and we can call collections dot synchronized so you see there are lots of options synchronized list synchronized collection map s navigable map so and so forth but in this case we have to synchronize the list so we will be calling synchronized list and we will pass an array list so this is the way in which we can create a synchronized list using the collections. synchronized method and based on your need you could also invoke other things like synchronized map and synchronized set and all those things so let's run this thing one more time and see the output so we are getting the value as 2,000 so basically list. size is 2,000 now let's run it one more time and now we should be getting the same value through and through so this means now there is no income consistency in the list even though we are using it across multiple threads so the list is a synchronized list now so uh this is the way in which we can use collections. synchronized to help us with creating a concurrent collection however there are certain downsides of using the collections. synchronized approach let's learn about those one by one so the first one is score scint locking what it means is that it uses a single lock to synchronize all the operations on the collection this means not only one thread can access the collection at a given time even if multiple threads are performing unrelated operations this can lead to contention and reduced concurrency especially if there are frequent read and write operations on the collection from different threads the second is the limited functionality and what do we mean by the Limited functionality well the synchronized rapper returned by the collections. synchronized list does not expose any additional methods for fine grin locking or custom synchronization strategies this limits your ability to optimize synchronization based on specific uses or patterns of your application so when we discuss about the concurrent collections we'll try to cover how exactly those collections provide certain other custom synchronization strategies as well so the third downside is that there are no fail fast iterators so collections returned by collections. synchronized list do not support fail fast iterators the fail fast iterators through a concurrent modification exception if the collection is structurally modified when an iterator is iterating over it without fail fast iterators it's possible for concurrent modifications to the collection to go unnoticed and this can lead to certain unexpected behaviors the fourth downside is performance overhead so synchronization introduces overhead due to lock acquisition and release this overhead can degrade performance especially in the high throughput or latency sensitive applications so if at all you don't need these four things then probably you could consider making use of the collections do synchronize method in your use case but let's say if you require some of these functionalities or these features uh it's better to go with the concurrent collections which are provided by the Java so now let's start learning about those concurrent collections let's learn about the countdown latch countdown latch is a synchronization utility that allows one or more threads to wait until a set of operations which is being performed in another threads completes it is part of the Java util concurrent package and can be used for controlling the flow of execution in the concurrent programs the key concept of countdown latch is that it maintains a count that it starts from a specific number and decreases each time the countdown method is called threats that need to wait for the countdown to reach zero can call the await me method which will block until the count becomes zero so when to use countdown latch let's understand the scenario in which we can use countdown latch with a very simple and easy to understand example imagine you are a take lead who is leading a complex project being the expert which you are you divide the complex task to some smaller and simple to implement tasks after splitting the task you distribute them with your team and they start working on their respective tasks however now you cannot proceed further on this project until they all complete their task in scenario like this you can use a countdown latch countdown latch is like a checkpoint for each team member's progress you set up the latch with the total number of task or team members each member upon completing their task calls the countdown method of the latch it indicates that they are done as the organizer you wait for all the tasks to be completed by calling the await method this method hals your progress until the count reaches to zero meaning all tasks are done once all the tasks are completed the latch opens up and you can proceed with the next phase of the project so countdown latch helps in coordinating multiple threads or tasks to synchronize their work it ensures that they all reach a certain point before proceeding further so let's have a quick code demonstration on on how we can use the countdown latch for developing certain use case we are in the concurrent collection package and I will create a class let's call this as restaurant and uh I will Implement a task let's call that as chef and let it implement the runable so before we move ahead with the implementation let's discuss a bit about what we are planning to implement so what we are planning to build here is a simulation where a group of Chef need to prepare different dishes in a restaurant kitchen each Chef is responsible for preparing a specific type of dish and the kitchen manager wants to start serving customers only when all the dishes are ready so here is how we can use the countdown latch to coordinate all the chefs so let's proceed with the implementation of the chef class so first thing is let's overwrite the run method so with that in place let's have a few properties of the class so first is uh string which is name the second is again a string which is a a dish and Third Field is going to be the countdown latch itself let's initialize all these fields in a Constructor so now that we have initialized all the fields in the Constructor here is what we are going to have in the run method so uh we try to simulate the preparation of the dish and for that let's have a triy block and then a catch Block it's going to throw an interrupted exception let's call this as e and for now just throw a runtime exception and for the tri block what we will be doing is first we print a message like name is preparing and then the dish next let's have a sleep time of 200 milliseconds and the idea is that this is the simulation for the cooking time so essentially your Chef is going to cook for 2 seconds quite a fast Chef right he is cooking in just 2 seconds anyway so let's have another print statement and what this will be printing is name plus has finished repairing and the dish so till now what we have tried to mimic is that the chef is going to prepare some food and once they are done preparing the food this message will be printed and what it means is that the task has been completed so what we will do is we will call latch do countdown so that should be it for this class that is the chef class now let's implement the main method in the restaurant class so let's have an integer value let's call this as number of chefs let's say we have three chef and what we will do now is we will create the trades for the Three Chefs and then we will start those trades the way we can do this is new thread and uh it could accept a new Chef let's name this as Chef a let's assume the chef is preparing pizza and then we pass the latch so we have not created the latch yet we will create in some time and and then let's call the do start method on this one so let's create the latch so let's have the countdown latch let's call this as latch and new countdown latch number of shfs let's create a same kind of threade for the another Chef as well so new Chef let's call this guy as Chef B and let's assume that this guy is preparing fter so let's call Larch and let's call start let's duplicate this and uh let's say this guy is preparing maybe salad and this guy is Chef C so when we have created the threads and called the start on these threads these threads will start running at some point of time and then we should make use of the latch somehow in order to ensure that this entire thread of execution that is this one is blocked till the time all these three threads are completed right and the way to do this using the latch is call the await method what does a method do it will ensure that the thread is waiting for all the dishes to be completed eventually three threads are there all those three threads are done with their execution and and a wait will expect us to handle some exception and uh either we could add into the method signature or surround with trr for now let's add to the method signature that is an interrupted exception and once this is done then we can print a message that all the dishes are ready let's start soling customers so this is what we have implemented let's try to run this and see its output so when we call do start the message is printed that Chef C is preparing salad Chef a is preparing Pizza Chef B is preparing this fasta and after 2 seconds all these three ships are done and what they do is they print Chef C has finished Chef a has finished Chef B has finished and when these guys are finishing with their preparation they will be eventually calling the last dot countdown and since we have defined a value of three that is the number of shfs which was three here so this countdown latch has a value three to begin with and the weight works is that last. a we will hold true or rather it will hold the execution of the thread on which this has been called till the time it does not reach zero and by the time the third Chef is done that is Chef B has finished preparing the pasta then in that case that threade will be calling the latch dot countdown and once that countdown is invoked it means that the countdown becomes the counter rather becomes zero and at that point of time this will be unlashed and then the threade will be unblocked for the further processing which is system.out and this message will be printed so this is what the countdown latge does and this is the way in which we can make use of this for our use cases now you may have a doubt you may think that in the beginning of the video we learned about the join method so is it not very functionally similar to the join so let's understand that so from the purpose perspective countdown latch is designed to allow one one or more threads to wait until a set of operations in other threads complete it is typically used for coordination among the multiple threads join on the other hand is used to wait for a thread to complete s execution before proceeding with the rest of the code and it's specifically used for thread synchronization within a single threaded context so how exactly are these two things different from the user perspective so the countdown latch is useful when you have multi trades performing independent tasks and you want to coordinate them before moving forward and join is useful when you have a main threade that respawns worker trades and needs to wait for them to finish before continuing its execution so I hope these points should have drawn a clear picture of the differences between the countdown latge and join method however you might have one more doubt left you may think if there are three threads and there is a countdown latch for them so either use the countdown latch or call join methods three times which is on all the three threads is it not the same right so why take a pain of learning A New Concept all together think of the scenario where you have a dynamic number of threads so what do you do how many times and in what Manner are you going to call join method on those threads in situations like this countdown latch shines as compared to the uses of join method overall countdown latch is a much cleaner and elegant approach especially if you are coordinating among multiple thread operations so the final thing about the countdown latches can we reset the count well countdown latch is supposed to be used as a oneshot solution so you cannot reset the count if you need a solution which resets the count you are looking for something called as a cyclic barrier and with this let's learn about the cyclic barrier next now let's learn about a very important concurrent collection in Java which is blocking queue since it's quite commonly used we will try to learn it in somewhat more detailed manner imagine a blocking queue to be like a conveyor belt in a factory where items are placed and taken off in Java a blocking queue is a data structure which allows multiple threads to safely put items onto the queue and take items off the queue and this is done in concurrent manner there are two aspects of blocking queue so the first part is the blocking aspect the term blocking means that if a threade tries to take an item from a queue which is empty it will be paused or blocked until an item becomes available similarly if a thread tries to add an item to a full queue it will be blocked until the space becomes available the second aspect is the uses of the queue so the blocking queue follows the first in first out principle meaning the item that was added first will be the first one to be taken out now let's talk about the blocking queue interface in Java the blocking queue interface is the parent interface to a few other interfaces and the concrete implementational classes so this is the parent interface which is the blocking que and the two such interfaces which extend this blocking Q are blocking DQ and transfer Q so this is also sometimes called called as blocking deck or DQ so whatever you wish to call you could call it now coming to the blocking deck so blocking deck is a doubleended q that blocks threads when it reaches its capacity or becomes empty thereby facilitating flow control between producers and consumers it provides methods to access the queue from both the ends in a thread safe manner due to its doubleended nature the performance characteristics of blocking deck may differ from those of blocking queue especially in the scenarios where there is a contention for access to both ends of the deck by multiple threads simultaneously key takeaway from the implementational perspective here is that it provides access from both the ends in a concurrent manner regarding the transfer queue it's a specialized queue where producers can block until a consumer directly receives an item it extends the functionality of blocking queue by providing a method called as transfer what this method does is it allows one threade to transfer an item directly to another waiting threade potentially avoiding the need for blocking if there are no waiting trades transfer behaves like put and blocks until there is a space available for the item transfer queue ensures a strong hand of coordination what does this mean let's understand this with an example imagine a factory where one worker produces items and another worker packages them for shipping the producer worker can directly transfer the produced item to the packaging worker without waiting if the packaging worker is ready to receive it if the packaging worker is busy the producer worker Waits or blocks until the packaging worker becomes available to receive the item this direct mechanism can improve the efficiency in certain scenarios compared to a traditional blocking queue so now let's learn about the major implementations of The Blocking queue the first one on the list is array blocking queue it implements a bounded blocking queue and it is backed by an array data structure the next one is the linked blocking queue so this one implements a bounded or unbounded blocking queue backed by a link list the priority blocking queue implements a blocking queue that orders elements based on their natural ordering or according to a specified comparator then we have delay Q what it does is it implements a blocking queue of delayed elements where an element can be only taken out when its delay has expired it is particularly useful for scheduling tasks to be executed after a certain delay or a specific time the final one is the synchronous queue this implements a zero capacity blocking queue where each insert operation must wait for a corresponding remove operation by another threade and vice versa so based on your requirement and uses you could use either of these implementations but for most practical purposes the uses of array blocking queue are linked blocking queue or a priority queue should be good enough now let's have a look on the blocking queue operations so these are the certain operations which are supported by the blocking queue put method adds the specified Element e to the que if the space is available if the queue is full the operation blocks until the space becomes available next is the take method it retrieves and removes the head of the que if the queue is empty the operation blocks until an element becomes available the third one on the list is the offer method what this does is it adds the specified element to the Q if the space is available it returns true if the element was successfully added or false if the Q is full as opposed to the put method which blocks until the space is available pole method retrieves elements from the head of the queue it returns null if the queue is empty it does not block the Q if the Q is empty as opposed to the take operation which blocks the Q until an element is available the peak method retrieves but does not remove the head of the queue it returns null if the queue is empty some of the above methods also provide overloaded signatures which allow you to pass timeout values as well so I suggest that you explore the blocking queue interface there you'll find all the variations of the measor methods which we have described here so all the implementations of The Blocking deck interface are functionally similar methods which provide these operations on both ends as an exercise you could try exploring the interface and the implementations to understand them better so now let's have a code demonstration which will showcase the uses of The Blocking queue interface in order to demonstrate the users of The Blocking queue interface I'll be creating this class inside the concurrent collection package let's call this as blocking queue demo here is what I'm planning to implement it will be of some capacity and there would be a couple of threads one thread would be a producer thread then we can have a couple of more threads for example let's say two threads as the consumer threads and once we start the threads then the producer thread should be able to produce or write data to the que and consumer threads should be able to consume from the queue so let's implement this so let's start by having the Q capacity let's call this as Q next we can have the blocking Q itself so let's declare that and we want to put an integer value in the blocking queue let's call this as a task q and we are going to make use of the implementation of array blocking queue let's pass the capacity that is the Q capacity and let's start by implementing the main method first on the list would be the the producer thread here in the producer thread what we will be doing is we will be iterating for certain amount of time and then putting that number to the task CU so let's Implement that so we have a tri block let's have a catch block as well we will have an interupted exception so let's put this for now and now coming to the dry block we will have a loop which is running from 0 to 20 0 to 19 rather or maybe let's change the numbers from 1 to 20 and all it does is it puts this number to the task CU so let's call put and the value would be I let's print a message task produced I and let's have a sleep timer for let's say 100 milliseconds so this is essentially the task generation time that we are having here and now that we have written the producer thread let's write the consumer thread as well so let's have the first consumer thre let's call this as consumer 1 and let's rename this to producer so what this consumer thread does is it consumes from the task CU infinitely so let's Implement that so let's start by having a triy block let's have a catch block let's have an interrupted exception and uh let's throw this one as a runtime exception so while true what we are doing to do is we can retrieve the task which we have in the task Q do te and then we will call a method let's call this as process task so let's pass the task and uh the consumer ID so let's now implement the process method before implementing the second consumer so private static void process task then we have the in task then we have the consumer name and let it through the interrupted exception because I wish to provide some sort of sleep in this particular method so first we start by printing a message that task being processed by consumer name and then print the task then we will try to sleep this for 1,000 milliseconds so the idea is that in real scenario as well when you will be processing a task that will involve certain time right so here since we have a pretty much Bare Bones implementation what we are going to do is we are going to provide some sort of sleep and that will mimic the processing time which is 1,000 milliseconds in this cas case and now once we have done the processing we can print the message the task consumed by consumer name and this is the task and uh now we will be implementing the second consumer thread as well so let's call this as consumer 2 and we will have a similar kind of implementation here as well so let's copy this one really quick and what we are going to do is everything Remains the Same so this is going to run infinitely it's going to extract out the task from the task Q process task Remains the Same it's just that we change the name here so now let's call this as consumer 2 and everything else Remains the Same so now let's start the threads so producer. start cons consumer 1. start consumer 2. start so I think this should be it for the implementation side of things let's run this code and see how is the output like so as you can see that task being produced is being printed by the producer method and then task being processed by consumer one and uh task consumed by consumer one so we had introduced some sort of sleep right so during that time another threade takes over that is the producer threade and it's going to produce so basically task being produced so task ID with two is produced and kept in the queue then uh this task is being consumed by consumer 2 as soon as the task is available in the uh processing queue right so this is the way in which the producer is uh creating tasks and pushing to the queue and the consumer on the other hand which we have a pair consumer one and two they are pulling the queue and they are taking out from the queue and working on that and uh consuming the tasks in the task queue so you could try to implement it yourself so using the block Q we have implemented a simulation wherein we are writing to a threade safe concurrent data structure which is the blocking queue and we are able to operate on the data structure using multiple threads that is the producer thread and two consumer threads so you could go through the code one more time and try to get a hang of it if it's not very clear so as an exercise you could go ahead and explore these classes and try to come with certain use cases and Implement those use cases using these classes so this will really help you with your understanding of the uses of The Blocking queue in the Java now let's learn about the concurrent map in Java so concurrent map is an interface in Java which represents a map that can be safely accessed and modified concurrently by multiple threads it extends the map interface and provides additional automic operations to support concurrent access without the need for explicit synchronization concurrent map is needed in Java to handle the situations where in multiple threads need to access and modify a map concurrently concurrent map has a couple of implementations such as concurrent hash map concurrent escap list map concurrent linked hash map concurrent navigable map Etc out of these concurrent hashmap is probably the most widely used one and this should be sufficient for most of your practical use cases so now let's have a quick code demonstration and uh we will Implement a simple feature using concurrent hashmap which showcases how this concurrent collection can be used in a multi threaded context so here is a brief idea behind what I'm trying to implement for this demonstration so I am planning to implement uh concurrent cache and uh basically there would be a couple of threads that will be writing certain values to the cach and uh those trates will be reading the value from the cash right so how we could could achieve this using the concurrent hashmap is what we are going to build so let's start first we have the data structure that will be holding the cache which is a map of course and it's a string string let's call this as cach this will be a concurrent hash map now let's implement the main method and we are going to iterate from is0 to I 9 so let's assume that this I value is the trade count so maybe call this as threade number and this is I and what we will be doing is we will be creating a thread so let's create this thing anonymously and finally we will also need to call start on this one so we have written start here so first thing is we create the key and the key could be a key string at the rate of value and then the thread num what we will do is we will try to Fitch the same key three times so that we could demonstrate that one time the cache is empty so we are going to put the value in the cache and for the next two times the value should be faced from the cache right so in order to mimic that functionality let's write an another loop so from z0 to J Less Than 3 j++ and uh let's capture the value which is going to be let's say a method called as cast value let's pass the key we will implement this gate cast value in a moment for now let's just have this method name itself now let's print some message here so the message could be threade threade do current threade dog name let me format this one and uh let's say key pass the key value and value pass the value so this should be it for the main method now let's implement the other methods which is the cach value for now so let's call this as private static string gate cast value let's say the parameter is the key and what we do first is that we try to fix the value from the cache so cache doget key if the let's correct this spelling so if value happens to be null it means that the key is not present in the map so we need to calculate the key somehow the value rather somehow so let's have another method for the same let's call this as compute pass the key it's not implemented we will Implement us in a while now let's put the same value in the cache so cach output key then the value and finally return the value all right so let's implement the compute method as well so that we can complete our entire implementation so let's return this as an string so compute it takes a key and first we print a message which is key not present in the cache so I'm going to compute now let's put a try and catch block because we will be needing this so now what we are doing to implement here is that we have a compute method and it's going to compute certain value so we want to mimic the logic for computation and uh so that will need certain time to execute so let's mimic the Same by putting a sleep here maybe for 500 milliseconds and once it's done return the value that is value for key so I think this should be it for the entire implementation now let's run this one so let me clean this up a bit all right let's run this and find out its output so what we see here is that the thread get is started and uh first is that key five not present in the cache so going to compute then key zero not present in the cache so going to compute so essentially uh for all the threads from 0 to 9 the key has been created that is key at the rate of five key at the rate of Zer so and so forth and in the first try they are not present in the cash so uh those will be calculated now and once they are calculated they getting getting faced from the map itself from the cache itself so let's try to find this for key atate 5 so here five is being faced right so the for the first time it's being created by the compute method and for the this two times it's being faced in this manner wherein you have the thread five and the key is this one and the value is value for key 5 it's same for the other Keys as well for example for key 7 for key 6 for key four and so and so forth so what we have implemented essentially is uh concurrent cache and the idea is very simple that there are a couple of threads which are going to put certain value in the cach and uh get the value out of the cache how it's being done is like in the G cast value implementation itself we are trying to fetch the value from the cache if it's not present we are Computing it and putting in the cache if it's present then this will not be executed and directly we will be returning the value so the first call to the cach is a cash Miss and then the cash will be populated with the value which is being generated by this compute method which is doing nothing it just has a sleep timer of 500 millisecs and then the value is returned as value for this particular key and once the value is put in the cache then for the subsequent calls the cach is being uh queried and we're getting the value from it so as you you can see that we have not used any um lcks or synchronization and all those things so it's a very neat implementation because we are using the concurrent hashmap and this particular collection is is a concurrent collection and we don't need to handle any sort of synchronization as such so that should be it for the implementation and the code demo now let's learn a few more things about the concurrent hashmap so let's understand about the internal implementation and working of the concurrent map let's break this into two parts the first is the adding an element to the concurrent hash map and the second is fetching an element from the concurrent hashmap and why we are learning about the concurrent hashmap itself the reason is that it's the most commonly used implementation of the concurrent map interface and for most of the use cases this should be sufficient for you as well so let's learn how the elements are added to a concurrent hashmap so as the first step we start with hashing and determining the segment internally the concurrent hash map is arranged as a smaller segments when a new element is added to a concurrent hash map its key is hashed to determine which segment it belongs to and each segment acts like a small hashmap within the larger concurrent hashmap in the Second Step the lock is acquired so once the segment is determined the thread needs to acquire the lock for that segment this ensures that only one thread at a time can modify that particular segment and in the third step data is inserted in this segment with the lock acquired the new key value pair is added to the segment's internal array if needed the segment May reside itself to accommodate the new element and finally as the fourth step the lock is released after the insertion is completed this allows other threads to concurrently modify the other segments of the map and likewise here are the steps that are needed for fetching an element from the concurrent hashmap so here as well the first step is hashing and determining the segment when a thread wants to fetch an element from the concurrent hash map it first hashes the key to determine the segment which it belongs to and as part of the Second Step the activity is similar to adding an element the thread needs to acquire the lock for the segment that contains the desired key in the third step we search in the segment once the lock is acquired the thread searches for the key in the segment's internal array please note that since only one thread can modify the segment at a time the search operation is safe from concurrent modifications if the key is found the corresponding value is returned if not found null or other designated value is returned to indicate absence of the element being searched for and as the final step after F operation is completed the lock for that segment is released here onwards other threads are allowed to access and modify the map concurrently so so this process continues over and over again for all the data rights and deeds from the concurrent map and this is how the concurrent Map works in Java so as we can see that here the Locking and synchronization of the concurrent map is on a very granular level as compared to the approach of synchronization or creating the synchronized collection wherein the synchronization was being done on the entire map or the entire collection itself self so this is pretty much faster in that sense because we are not holding up other threads from modifying or interacting with the map and this is the reason that concurrent map is a better choice when you are dealing with a multithreaded context and you need to have some sort of map based collection imagine you are playing a game with a group of your friends the rule states that everyone must together at a certain spot before you can all move forward to the next level this spot is like a checkpoint now you cannot move forward until all your friends are there with you in Java this is like using the cyclic barrier each friend represents a thread in your Java program when a thread reaches the checkpoint it calls a weight method on the cyclic barrier the thread Waits there until all the other threads have also reached the checkpoint once everyone is there the barrier is stripped and all threads can move forward now let's have a quick code demonstration using which we will understand how we can make use of the cyclic barrier in our Java program so to demonstrate the cylic pario let's create a class I'll call this as multistage tool the idea here is that I want to Implement a feature wherein there is a group of tourist along with a tour guide and there are different stages of the tour and during a stay the tourists are supposed to arrive at a particular location and once all the tourists arrive at a particular location then they will move along to the next stage of the tour with the tour guide so this is what we are going to implement overall this is a small simulation to demonstrate the cyclic barrier users so let's implement the same so first let's have the member variables first we will have the number of tourists so let's call this as public static final int let's say this is five then the next one we could have as the number of stages let's say this is three and let's create the cyclic barrier public static final let's call this as the cyclic barrier let's call this as barrier and new cyclic barrier so cyclic barrier does accept a number which is going to be the count on the basis of which it's going to maintain that how many threads have returned to the barrier point so let's provide the same and meanwhile let's also check the other options that we have so one option is this Constructor there is another Constructor as well which is giving us the uh integer value that we are supplying the parties basically and along with that we could also Supply the barrier action so what is the barrier action so the barrier action is the command to execute when the barrier is stripped or null if there is no action so I think for demonstration purposes let's include a small action for this so let's provide a unable here and what we are going to provide is a simple message that is tool guide starts speaking so don't worry if it's not making sense as of now as soon as I finish the implementation I will explain everything in as much details as needed to understand this concept so let's quickly create the static class as well which is going to be the tourist class and it's going to implement the runable interface let's have the run method overridden and uh it will have a member variable has tourist ID we will initialize this tourist ID inside the Constructor and inside the run method what we do is we iterate for the number of stases and during each iteration we do some operation so let's do that so for in I as zero I less than number of stases I ++ and the first thing that we try to do is we introduce some sort of sleep in the thread this is to mimic the activity being done by the tourist so let's do it so we will put this inside the try block let's have a sleep of th000 milliseconds and in the catch block let's have the the interrupted exception let's throw it as runtime exception for now and uh once the tourist is done exploring the area whatever activity they are trying to do in this particular time frame then we are going to print a message the message could be tourist then the tourist ID arrives at stays then i+ one so what we are trying to achieve is that in the given stas the tourist comes the tourist performs certain activity that is the sleep for th000 milliseconds essentially in 1,000 MCS they do some activity and then once they are done with their activity we print this message that tourist with so and so ID has sort of completed their activity and that is the reason that they are arriving at a particular stage where from we are supposed to move to the next stage but catch here is that it cannot move to the next stage until and unless all the other tourists have arrived as well and that is where we are going to make use of the cyclic barrier so as I mentioned mentioned in the theory section that we can call barrier. weit so we will do the same so let's put a weit here and it expects us to provide certain error handling let's do that and uh it's a weit not weight so let's correct it and when we do await then we will have to add another catch Clause as well that is broken barrier exception let's collapse this to one catch block and uh this should be it for the run method now let's go to the main and implement it so what we want to do is that for all the tourists that we have that is five tourist number of tourists I want to create threads for each and every tourist and then I want to start their thread so let's do that so for INT I as0 I less than number of tourists then i++ let's create the thread let's call this as tourist thread new thread and new tourist the thread ID could be I itself let's let's call the start on this one so now let's run this and see the output so here is what we are seeing there is a delay of 1 second and uh during that 1 second all the tourists are there doing their activity and uh they're arriving at stage one so once all the tourists have arrived at a stage one that is 0 1 2 3 4 then the tour guide starts speaking and uh assume that this is the instruction about the next stage of the tour right and again the similar thing happens wherein the tourists go to the second stage of the tour and they do their activity and then the tourists arrive at the stage two before progressing forward for the next stage and the same thing happens here as well and since we have just three stages this is where the tourist stops but the key thing to note here is that till the time all the tourists have not arrived the tour is not progressing to the next stage so essentially the tour guide or the tour group is sort of waiting for the time being until all the people in the group have arrived to a particular point to a particular checkpoint so that is what we have implemented here let's go through the code ones so first we start by initializing the number of tourist and number of stases and we have a cyclic barrier it's accepting two things one is the number of parties involved that is the number of tourists so it's going to track five tourists the five tourist threads and then there is a runable which is going to print this message once the barrier is getting tripped or barrier is getting broken right and uh in the main what we have done is we have just created number of tourist uh threads and we are calling do start on them and uh in the tourist class itself which is implementing the runable the key area to look here is this run method wherein for all the number of stages what we do is first we allow the tourist so you can visualize in this way that let's say that you are at the stage one and when you are at a stage one then you will be doing certain activity and like you the other tourists as well in the group they would be also doing certain activity and let's say everybody is doing certain activity in certain time range so this is what is being mimicked by this thread do sleep and once you and other tourists like you in the group they are done with their activity we print a message that tourist with this ID has arrived at these stage and then we call the barrier. AIT so let's say this is your threade which is being executed then your threade will call barrier. await and what this barrier. AIT will do is it will wait for all the other threads which are part of this group so basically we have created this thread group right and uh this threade is having the same cyclic barrier which is this particular cyclic barrier and this cyclic barrier is being used by all the threads so like you there could be other tourists as well and they would also be calling barrier dot AIT and essentially what what will happen is until and unless not everybody in the group that is all the five threads have not arrived at this particular position till that point of time the further execution will be halted what could be the further execution it could be anything so this is the way in which this is going to work and in this way we can make use of the cyclic barrier in a code please note that once the barrier do await is Reed and once the barrier is broken the barrier gets reset on its own you don't need to do it explicitly or manually so the barrier count is again uh initialized or reset rather with the initial value that you have provided which is five in this case so you could use it for the next round so that's about the code demonstration now let's learn about how does cyclic barrier work under the hood under the hood a cyclic barrier uses a combination of of a counter and a condition to manage the waiting threads when you create a cyclic barrier object you specify the number of threads that must call the await method before the barrier is broken each thread that calls the await decrements the internal counter of the cyclic barrier if the counter has not reached zero yet the calling thread inter a waiting state so let's say the calling thread is U and you have called barrier do a we but what the cyclic barrier finds out that this internal counter is not reached zero yet so let's say to begin with it was five and then you called it so it became four so it's not zero of course so what happens is you go into the waiting State and when a specific number of threads have called the await the barrier is stripped so let's say that after you other four threads came in and they also called barrier. AIT and eventually the value of the count internal count became zero so at this point all the waiting trads are released and they can proceed with their execution so that is where all the waiting threads that is youu and the other four threads will be released they will be notified that they could come out of the waiting State and after the barrier is stripped the internal counter is resets to its initial value and the barrier is ready to be used again for sub sequent synchronization points this reusability distinguishes cyclic barrier from the countdown latch which cannot be reused once the count reaches zero so this is pretty much about the cyclic barrier and the way in which we can use it and uh we also learned about how does it work internally under the hood so that's all about the cyclic barrier now let's learn about the exchanger in Java an exchanger refers to a synchronization point at which threads can pair and swap elements within a concurrent environment the Java util concurrent exchanger class facilitates this functionality the way this works is that two threads can call exchange method on the exchanger object passing the object they want to exchange why and me to use exchangers exchangers are useful in scenarios where two threads need to synchronize and exchange data before proceeding with their respective tasks for example imagine you have a pipeline of processing steps and each step is executed by a different trade in such a case you might want to use a exchanger to exchange data between adjacent stepes in the pipeline so how is it exchanger implemented in Java exchanger is part of java util concurrent package exchanger is a class and it does not Implement any interface or extend any class also exchanger is not implemented by any other class in Java it is a standalone class which is specifically designed for synchronizing the exchange of data between two threads the class has two important methods first is exchange which an object and this method is used to perform a blocking exchange operation it waits until another thread arrives at the exchange point and then exchanges the object X provided by the current thread with the object provided by the other thread it Returns the object provided by the other thread if the other thread has not arrived yet the current thread will block until it arrives the other method is the exchange variation it's an over loaded method which is going to accept an object and other than that it will also expect us to pass our timeout it's similar to the above one except it has a timeout parameter if the other thread has not arrived at the exchange Point within the specified timeout duration a timeout exception is thrown now let's have a quick code demonstration which will show us how we can use exchanger in our code so here we are in the concurrent collection package let's create the class let's call this as exchanger demo so what we are going to implement is two threads let's call them as first threade and second threade and then they will have exchanger at their disposal using which these two trades will be exchanging certain information with each other so let's start by implementing the first threade let's call this as class first threade implements runable so let's implement the run method it will have the exchanger and uh it's a generic class so we will have to pass the type let's say we pass the type as integer let's name this as exchanger let's initialize this exchanger inside the Constructor and in the run method let's have a data to send variable with a value 10 and uh we will print this value so first trade is sending data data to send and uh now let's call exchanger do exchange and pass the data to send value this will return a value and uh let's capture that as part of a variable which we could call as received data and exchange will expect us to handle some exceptions let's surround this with TR catch and once we have received the data let's print it out on the console so first creade received data now let's implement the second thread as well and once we are done with the implementation we will see how it's interacting with each other so let's call this as second trade implements runable let's have the exchanger here as well and we will pass the integer as the generic type initialize this particular extension in the Constructor and in the run method let's start by having a sleep of let's say 3 seconds and here as well we have a data to send variable with a value 20 you will print some sort of message second thread is sending data and let's say data to send and then exchanger do exchange let's pass the data to send let's capture this as part of received data and let's print this on the console so second trade received and received data let's handle these exceptions I catch and what we can do is we can also capture this and place inside the tri block so we should be good with the implementation of the second threade now let's implement the demo class and in order to do so let's have the main method in the main method let's have the exchanger new exchanger and uh we will be creating two threads so first one new threade and uh first threade let's pass the exchanger because the thread is going to expect us to pass the exchanger object let's do the same for the other thread as well let's call this as two we will start these two so 1 do start two do start so this should be second threade now we will run this and see the output so we can see that we are seeing first thread is sending data and second thread is sending data and second threade received 10 that the first thread is sending and the first threade received 20 that is the second threade sending but there is a key information that we must notice please note that in the first threade we are sending the data right away wherein we are calling the exchanger do Exchange but in order for this exchanger do exchange to be executed the other thread should be able to receive it that means as long as the other thread is not calling its exchanger do exchange then in that case this threade will be blocked and what we are seeing in the second threade is that before calling exchanger do exchange we are putting a sleep of 3 seconds so that is the reason we are observing a delay because threade one here is blocked for 3 seconds because when the execution comes inside the second trade it's sleeping for 3 seconds before calling the exchanger do exchange and once the execution comes here then it means that thread one will be sending its data that is data to send 10 and uh that will be received by the second trade and captured in the received data and likewise exchanger do exchange calls this data to send and this data to send value that is 20 will be received by the first threade which is captured by this received data variable and the same will be printed with this value that is First Rate received received data so let's run it one more time to observe the blocking nature of the exchange method so so we can see that first thread is sending data and there is a delay before all of these things are executed so we called First Trade is sending data then exchanger do exchange was called and because the second threade went into the sleep for that amount of time that is 3 seconds the first threade got blocked and as soon as the second threade is active The Exchange happens and the first threade receives the data to send value from the second threade which is 20 and likewise the second threade is receiving the data from the first threade this is a data send value of 10 so this is it for the code demo I hope it would have somewhat cleared your doubts with respect to how exactly can we use exchanger now back in the theory section let's do a quick comparison between the queue and the exchanger so you may feel that a queue could achieve similar kind of synchronization between the threads right and yes you are partially correct in thinking so however the choice between using an exchanger and a queue depends on specific requirements of the synchronization pattern which you are trying to implement and the characteristics of your application so here is a quick comparison the exchanger facilitates a direct exchange of data between two threads at a synchronization Point each threade Waits until both the threads has arrived at the exchange point before proceeding if you have exactly two threads that needs to synchronize and exchange data the exchanger provides a simple and efficient mechanism to accomplish this exchanges occur synchronously meaning both the trades wait for each other at the exchange Point both traes exchange data of the same type and quantity making it suitable for symmetric communication patterns the que on the other hand however is different from the exchanger in these parameters so the first is that cues are more versatile and could handle communication between multiple producer and consumer threats and depending on the Queue implementation producers and consumers May operate asynchronously producers can inq data without waiting for consumers and consumers can theq data without waiting for the producers qes can act as buffers allowing producers to continue producing data even if consumers are not immediately available to process it and in some cases The Exchange may not be symmetric however the exchanger is very similar to a type of q that we saw earlier and that type of queue is called as synchronous queue so both synchronous queue and ex change of facilitate blocking synchronization between threads however while a synchronous CU is unidirectional the exchanger happens to be bidirectional so with the use of exchanger you could achieve a bidirectional data exchange between two threads which means when you do call do exchange data at one hand you are sending certain value but you are also receiving certain value from the second thread as well so that is the B directional nature of the exchanger so that should be it for the topic of exchanger now let's learn about the copy on WR array copy on WR array is like having your own copy of book to read when someone wants to write in the book instead of disturbing your reading they make a new copy this way you can keep on reading without any interruptions you can use copy on right array when you have multiple threads accessing and modifying data at the same time it ensures that readers don't get disturbed by writers and writers don't interfere with each other making your program safer and more efficient let's have a code demonstration which will tell us how we can use copy on right array in our Java code we are in the concurrent collection let's create a class and let's call that as copy on WR array demo so what we are going to implement is two threads one is the read task and the other one is the right task and as the name suggests that read task is going to read something and WR task is going to write to something and what it's going to read and write from is the copy on right array so let's start the implementation let's create the read task and this will Implement a runnable interface let's implement the run method and uh let's have a list of integers let's call this as list the type would be integer let's name this object as list and we can initialize this using a Constructor and what we want to do inside a y Loop is we want to put some sort of uh sleep to the thread and then print the list so that is is going to mimic the infinite reading operation so let's do that so while true let's call the thread. Sleep and maybe it could be for one second let's do the exception handling and uh once it's done sleeping we will be printing the list so that's about the read task now let's create the right task and it will implement or unable let's implement the run method and this as well is going to expect us to pass a integer so let's do that and uh let's have a random generator and and uh we could have a Constructor for the random we could just say new. random and in the run method we are going to do two things infinitely first is we will wait for certain time second is we will be writing certain value to the list so let's do that so let's have the while loop while true trade. sleep let's have this for 1200 milliseconds let's handle the exceptions and outside the TR catch block let's call list. set random. next integer and it's going to be in the size of list and then random do next int and the upper limit is going to be 10 so what we are doing essentially is that we have a list and in that list we are picking any random index and on that random index we are setting any random value that is in the range of 0 to 10 so that's about the right task now let's write another class that is going to mimic the simulation that we are trying to do so let's call that as simulation let's have the list let's call this as private final list integer list and uh let's create the Constructor so what we want to pass instead is new copy on right array list and uh let's remove this one because this is not needed now and and uh to this list I want to add certain values so array do aslist and uh let's pass some zeros all right so we have basically instantiated a list made that as a type of copy on right array and in that particular copy on WR array we have provided some values which is all zeros now let's have a method call that as simulate and what this thing is going to do is it's going to create certain threads so first could be new right task let's pass the list let's duplicate this this one could be two this one could be three and this one could be four this as well could be a right task this as well could be a right task and let's have this one as a read task so basically we have three writer task and one reader task finally let's start these 3 do start 4 dot start and in the main class let's have the main method to begin with and uh we will create the object for simulation simulation new simulation and then let's call simulation. simulate so let's go through the code once before we run this so what we have done effectively is we have created four threads three out of which are the wrer threads and one is a reader thread what the writer thread is going to do is is it's going to run infinitely and uh every 1.2 seconds it's going to set certain value in the list that we are going to provide which is a specifically the type of copy on WR array and the way it's going to write is it's going to pick any random index and going to place any random value and uh what the read task thre is going to do is it's going to execute infinitely it's going to run infinitely and in every run it's going to sleep for 1 second and then print the list and finally we are running all these trades and uh this is basically we are creating the object and calling the simulate method which is going to do all of these things so now let's run this and see the output so you could see that to begin with we have this array then the writer thread wrote something and then uh that is being read by the reader thread so it keeps on happening that the writer thread will be writing something to the array that is pretty much random and then the reader thread will be reading from the array the point to note here is that this is being done in a multithreaded context and we are not leading to any exceptions for example any modification exception or any kind of uh incon consistency so situations like this when we should have copy on right array list and uh this particular data structure will help us in ensuring that an array list is being used in a multithreaded environment without any of the synchronization or concurrency concerns now let's learn a few more things about the copy on right array so now we will learn how it works so when a thread wants to read from the array it creates a snapshot of the array and uses the same to read if a thread wants to write it also creates a snapshot of the array and performs the right operation once the right is completed the changed array is considered as the latest version of the array from which snapshots could be created for the read and write operation this way readers don't see the changes until they get a new snapshot it ensures that they always have a consistent view of of the data so we could Loosely draw panels between the git branching approach and the way in which copy on WR array Works each modification to the array creates a new version much like creating a new branch in the git just as git would allow for parallel development without conflict copy and right array allows for multiple threads to modify the data concurrently and it does so without interfering with each other and just just as git consolidates changes during the merging copy on wrer eventually consolidates the modifications into a single reference version this version ensures data consistency both systems provide a structured approach to managing concurrent changes so I think it would make sense to visualize what we just discussed so let's do the same so let's assume that this is our array and it has certain values as in 3512 and the first call on this array is a read call imagine this line to be the reference line and consider that this line has the latest value of the array so whenever a read or a write operation is supposed to be done those operations are going to create certain snapshots so the first call that we are getting on this array is a read call so as soon as we have a read call a snapshot of this thing is created and the read will be performed on this one so what effectively happens is that even though if there is any right operation that is going to happen it won't impact this value because we have taken a snapshot of this particular array at this particular time since this line is the source of Truth during creating a snapshot the same value remains to be seen here and this is the array from which the read operation will be served now let's say going forward at this point of time a write operation is invoked what it says is that write a value four at index 2 so since this latest value or the reference value has this array that is 3512 a snapshot is created which is 3512 and on this snapshot the right operation is applied so as soon as the right operation is applied the array changes and the value becomes 3 5 4 2 and the same value would be written to this source of Truth or this line or this reference right so let's say we have a read operation now and this read operation comes after the update has been written so since this reference has this value of 3542 the read operation is again going to create a snapshot and now the snapshot will contain this value 3542 so when the read operation is served the same value is returned that is 3542 and this process keeps on happening so you see what we have is our reference value to begin with and a line which is serving as a source of Truth and whenever we are going to have any read operation we are creating a snapshot and whenever we are having a WR operation we are creating a snapshot and after that right operation is complete then the same is merged back to the main line so that is the reason I told that it is somewhat similar to the way in which G branching works so you could consider this as the master branch and uh read is anywh readed it's not going to write anything or impact the data but still we want to avoid a situation where in the master Branch could change during the read is being performed so that is the reason the snap is being created to serve the read as well but it's very evident that in the right scenario uh another branch is created and on that Branch whatever rights are supposed to be done those rights are done the four in this case at index two and once this is done once it is successful the same thing is written back to the main line to the main branch and once this thing is merged now the main branch has this particular value that is 35 42 so this is the way in which you could visualize the right on copy array and I hope that it will help you in understanding this concept in a much better way lcks are an important topic in Java so let's learn about the locks what exactly are locks well imagine you have multiple threads running simultaneously in your Java program and all are trying to access the same resource without proper synchronization it could lead to inconsistency in data and other chaotic situations that's where locks come in handy locks provide a way to control access to Shared resources ensuring that only one thread can access the resource at a given point of time thus it helps in preventing data corruption and other concurrency issues you may wonder that it's sounding very similar to what the synchronized blocks do well you are right in thinking so so now let's learn the difference between using synchronized blocks versus the locks in Java when it comes to managing concurrent access to Shared resources two commonly used mechanisms are available these are synchronized blocks and locks let's start with the synchronized blocks synchronized blocks use the synchronized keyword to ensure that only one thread can execute a particular section of code at a given point of time they provide intrinsic locking which means that the lock associated with the object is acquired and released automatically by the jvm synchronized blocks are easy to use and require less boiler plate code compared to the locks however they have limitations such as lack of flexibility and lock acquisition and inability to handle the interrupts on the other hand locks provide more flexibility and control over locking mechanisms Java lock interface and its implementations allow to manually acquire and release the locks moreover you could acquire and release the locks blocks in any sequence and in any scope which is not possible if you use synchronized approach so when you should use synchronized blocks versus locks use synchronized blocks for simple synchronization needs where flexibility and performance are not that critical use logs for complex synchronization scenarios where fine grain control and flexibility are required in conclusion both the synchronized blocks and locks are essential tools for managing concurrent access to a shared resource in Java understanding their differences and choosing the right synchronization mechanisms for your specific requirements is a crucial part of writing a robust stand efficient concurrent code so now it's time to learn how can we Implement and make use of locks in our code while doing so we will also learn about a couple of important concepts related to Locks such as conditions different types of logs Etc till now we have learned that locks are an important part of implementing the concurrency in Java but we also need to have some sort of interaction between the threads and locks so the mechanism which helps us in managing these interactions is called as condition let's understand what are conditions with a very simple and easy to understand explanation here I would request you to make a very weird assumption which could help in avoiding any confusion with the terminology of lock assume that lock here is the key as well what I mean to say is when you have access to the lock the door which is guarding the protected resource is unlocked magically and thus the lock is a key as well in that context hence as soon as you acquire the lock you get access to the resource please note that only one person is allowed to access the resource no one else can tailgate behind you with this in place let's learn further a lock in Java can have its own set of rules these rules are called as conditions effectively this means that a lock could have more than one condition associated with it conditions help us in controlling how threads interact with the lock you could visualize threads as people who are trying to access some protected resource and think of condition as a way waiting room attached to the lock imagine a person comes up and tries to get hold of the lock but lock is being held by some other person thus this guy won't be able to acquire the lock so what he does instead is he goes to wait in this room where other people may also be waiting for the lock to be available inside the waiting room people wait until someone signals that it's their turn to use the lock now this signal could would come from a person that is currently holding the log or from some other source so who gets the chance to acquire the log well that's something which depends on the implementation the key takeaway from here is that after a signal is given any one person from the waiting room is allowed to acquire the lock and do their processing in Java these conditions help in managing the interaction between threads and locks now let's visualize how conditions work imagine there are two threads thread one and thread two to begin with threade one is executing something however it cannot move forward until some particular condition is met this condition could be anything based on your use case for example a que being full could be a condition in this case as soon as the thread cannot move forward it will call condition do a wait if you read this from the right it appears as if a threade is saying that I am waiting for this condition to be fulfilled so I am awaiting for a given condition effectively the thread one goes to a waiting State now let's imagine there is another thread thread 2 and this thread is running and doing some operations due to the operations being performed the condition gets fulfilled and as soon as the condition gets fulfilled this trade to calls condition do signal it signifies that the condition has been fulfilled so convey this message to any of the threads which have been waiting for this condition to get fulfilled as soon as the signaling happens jbm finds all the threads which are in the weight State anticipating this condition to be fulfilled and wake the one which has been waiting the longest in this example there is only one thread which is thread one so the state for thread one goes from bit state to the renable state so the threade one which was blocked at this point of time is now active from this point of time because at this point of time threade 2 signals that the condition for which threade one was waiting it has now been fulfilled so thre one could proceed with its processing now let's analize this scenario where we call condition do signal all like we had condition. signal there is an another method signal all so in this example we have three threads and during its execution threade one calls do of wa and after some time threade two also calls doit there is another threade which is threade three and during its execution the condition for which thread 1 and thread 2 are waiting gets fulfilled so the thread 3 calls condition do signal all as soon as signal all is called all the threads which are waiting for the condition to be fulfilled build are waked up by the jvm jvm pulls them out of the blocked State and moves them to the runnable state so depending on the number of course your CPU has threads could be scheduled for Execution accordingly so if dot signal was called instead of signal all thread one would have been waken up since it's the one which has been waiting for the longest so this is the way in which condition helps us into maintaining or managing the interaction between the threads and the locks there are two methods that is signal and Signal all which are used to notify that a particular condition has been fulfilled and there is one method which is condition. which is used to hint that a condition needs to be fulfilled before a given thread could proceed further in its execution in case if you are wondering that condition methods discussed above are very similar to the weight and notify methods in the object class you are correct indeed these methods are similar to wait and notify the main difference however is that conditions provide a much granular control over the synchronization of the protected resource now with that in place let's have a quick code demonstration which will teach us how can we use the conditions and its related methods let's create a package and we can call this as logs and uh let's create a class call this as condition demo and here is what we are going to build so basically we are going to build a producer consumer implementation using the locks and conditions so let's start so first thing that we will have is a Max size which is the number of elements that could be kept in a buffer which is going to be a q so let's have that so it would be private final integer Max X size let's have the value as five the next thing is we will Define the lock so lock and the type of lock that we are going to use is reint lock so reint lock is one type of lock in Java we will have to create the Q or the buffer where we are going to place the elements so let's create the que let's call this as buffer and uh this could be of type link list so basically from this buffer the consumer will be consuming and the producer will be writing to it next we will be creating certain conditions one condition would be that buffer not full and the other condition is buffer not empty so let's create the same so private final this is condition and uh buffer not full lock the way to create the condition is to call the new condition method and we will do the same for the other condition as well so this condition could be something like uh buffer not empty and we could have log. new condition so we will have two methods the first one is producer so private void produce and in item so let's have this and uh so what we will be implementing is the produce section of the implementation and uh we want this to be synchronized while this particular code block is getting executed we don't to want something else or let's say some other thread to come and work on this simultaneously so the order to do this is first we will have to acquire the loog and the way to do this in Java is called log. talk and once we are done with our uh processing finally we need to call log. unlock so this looks fine but there is a problem let's say when the processing is under the way there is some exception which happened so in that case the code will terminate then and there itself right the lock. unlock will be never executed and the other threads which are waiting for this lock will go into a state of Deadlock lock and that is not good so the way to do this is call this lock. unlock in the finally section or the finally block so in either case this lock. unlock is always executed so this is a common pattern which we use while making use of the locks in Java and this is the way to implement this so let's have the tri block and here we will put the processing right and we will have a finally block and that is where we call the lock do unlock so with this in place let's implement the processing part so here is what we are going to implement now first we will check if the buffer is totally full then for that duration we will have to make the producer thread weight otherwise we will put some item in the queue and then we will notify if there is any consumer thread which which is waiting for the producer to put certain item in the queue so let's implement this so we start by checking if the buffer size is equal to the max size which is five if that is the case then we have buffer not full and on this condition this threade will wait after that we will add certain item to the buffer and next we will have a print message which say something like produced and uh produce this particular item and for the buffer not empty we are going to put the signal so let's also complete the consumer method then we can have a complete look over the entire implementation so let's have private void consume and like the produce method it's also going to throw an interrupted exception so let's have that in the method signature we start by acquiring the lock so lock. lock and we will follow the same pattern that we saw earlier that we will have a try and finally block and in the finally we call the unlock method and in the try we have the implementation so here we will check if the buffer is empty and if it's empty then we call Buffer not empty and await on this condition and if this is not the case then we are going to consume from the buffer and the way we do this is we print a message which says consumed and uh from the buffer we take out the element by calling the do pole and next we call a signal on the buffer not full so this is the consumer method so you may have a doubt here what you may think is that the nomenclature of buffer not full and buffer not empty is kind of misleading and confusing so I totally understand that and I also had this confusion for quite some time but the way to interpret this is that this nomenclature should be understood from the perspective of this signal so so the condition is buffer not empty so what it means is that this buffer not empty condition is being awaited by certain threade and which threade may await this the threade which may await this is this one right that is going to consume so if it's consuming then at that point of time it needs to have something in the buffer otherwise there is nothing to consume so that is the reason if the buffer is empty then on this particular condition this consume method is going to wait and let's say if it's waiting for something to be put in the buffer and then the producer method is executed by some thread the producer thread then at that point of time something was put in the buffer some item was put in the buffer and that is where once we have put something in the buffer we are going to call this signal so what essentially we are saying is that buffer not empty condition is is going to signify that something needs to be there in the queue for the consumer to consume and that is the reason consumer is going to wait on this condition that is buffer not empty likewise we have buffer not full so buffer not full is being signaled by the consumer and the idea Remains the Same right so when producer is producing something then it needs to ensure that the buffer is not full and if it's full then it will have to it and this condition is being signaled by the consumer and what it means is once the consumer has consumed something then there is certain space in the queue in the buffer in which the producer could produce so that is the reason if there is a producer which is waiting for this condition then this particular thread the producer thread will be awakened when this buffer not full is signaled so that is the idea and that is the intention behind this nomenclature and behind this implementation now let's complete this by writing the main method and then we will have a code demonstration by running the code so let's implement the main method and uh let's have the condition demo object created so new condition demo right and uh let's have the producer thread let's call this as producer trade and uh let's have a triy block and have a catch block as well so when we call producer it's going to throw that interrupt exception so that is the reason we need to have it and we are also going to introduce some sort of sleep so that will be handled by the catch block itself so what we are going to do is we are going to run this producer threade for times and uh we call the demo do produce and pass I in this one so I is the item that needs to be put so essentially this producer threade is going to put 10 items in the Que in the buffer next let's have the uh thread dot sleep maybe for one second and in the catch we have uh a runtime exception which is this e so this should be for the producer thread and uh for the consumer thread let's have another one first let me rename this one to Consumer threade and uh again we will have the similar pattern we will have have a try and catch let's have the interruped exception and uh let's through the runtime exception and what we do is we are going to call consume 10 times so I less than 10 and demo do consume and we are going to put a sleep for let's say 2 seconds and once we have this threade created we will have to call the dot run on these two threads so first is producer thread. start then consumer thread. start so that should be it from the entire implementation perspective now let's run this and see the output so you see we have a demonstration ass simulation of the producer cons a problem so produced 0 consumed 0 produced One consumed one and uh produced two produced three so remember we had put a sleep timer of 1 second in the produce method and 2 second in the consume method so in that sense consume method is on the slower side and that is the reason here we have uh the producer being producing something twice and consumer is kind of on the slower side so we are producing twice and consumer is going to consume something once right but this pattern is not something which is very deterministic the entire idea behind having that uh sleep was just to showcase the demonstration uh in somewhat presentable manner right so that should be it for the conditions in Java and uh please note that these conditions could be only derived from the lock interface implementations which are reint lock read right lock and any other implementations which you could explore in the jdk and uh these conditions are not available for the logs that we have on the object level the implicit locks which are available with the object in Java now let's learn about the reant locks reent lock is one of the many implementations of lock interface in Java it allows a threade to acquire the same lock multiple times without causing any Deadlocks what this means is that a threade which holds a lock can acquire it again without blocking itself if the lock is reentrant in contrast with a non reentrant lock an attempt to acquire the lock again within the same thread without reducing it first would typically result in blocking it could also potentially cause a deadlock situation if it's not handled properly how does this this work the way this works is that the locking mechanism keeps track of the lock being held by a thread in the case of nonintent locks attempting to acquire the lock again within the same thread typically results in blocking the thread however in the case of reinen lock the lock mechanism allows the same thread to acquire the lock multiple times without blocking itself effectively the held count for the lock by the given thread is incremented since the lock is already acquired there is no sense of reacquiring it this is achieved by maintaining a count of how many times the lock has been acquired by the same thread the lock is only released when the count reaches zero it indicates that the threade has released the lock the same number of times it has acquired it you may be wondering why and in what situations a threade would need to acquire a lock again without releasing the log here is the answer imagine a scenario where a threade invokes another method or code blog which also requires the same log to maintain consistency in the multithreaded context this pattern is quite common in complex implementation s methods call other methods within the same class these methods require synchronized access to some shared resource and in such situations reant locks are quite useful and in fact very much needed so with that in place let's have a very quick code demonstration of using the reen clogs so to begin with I have created this class called as reint loog demo and here are the two fields that we are going to need so in this line we are initializing the reant lock the way to do this is by calling new reint lock there are other variations of this particular Constructor wherein we could pass the fairness criteria and all those things we will learn about those in a while so for now we have created the log and then we have a variable called as shared data which is initialized to zero next we have a method a what this method does is it's going to increment the value of the shared data and then it's going to call another method which is Method B and uh method a needs to get hold of the lock for this section of the code block that is incrementing the share data and then calling the method B and finally this particular lock is released now let's see what we have in the method B so in the method B again we are going to work on the shared data and the way it's being implemented is the Shar data value is getting decremented but we need some sort of Lock and that lock is going to protect this particular section which is decrementing the share data and printing the value and finally we call the unlock method so what we see is that we have a method a which is getting called by some thread and it's going to acquire a lock but in the same block when it's uh being protected by the lock there is a call to Method B and that method b as well is going to need that lock because that is also handling some sort of critical section so this is the reason that we have made use of the reentrant lock so this particular thread is already holding this lock but at this point of time this will need to acquire the log again and if it was not a reant log we could have encountered a blocking scenario and that could have also led to a dead log so that is the reason we have used the reant log now let's see the further implementation so we have a main method and what we are doing is we are uh creating uh and starting multiple threads which are essentially uh five threads and we are calling the start on those so let's try to run this and see the output so here is the output that meod a had share data as one method B has shareed data as zero method a and Method B so and so forth so the key takeaway is that we are able to deal with the shared data value in a concurrent uh environment in a concurrent fashion in multi threaded context and the same thread is going to need to acquire a log multiple times and that is the reason we have used the reint log so at any given point of time let's say here if you would have called log. G hold count then the value would be two so essentially what is happening is the first time the lock is acquired here and the second time the lock is acquired here so essentially this trade has acquired this lock twice but under the hood of course it has been acquired just once it is just the count which is getting incremented and which is two and finally this lock will be unlocked it will be released so the count will become one and then the Call Comes Here and finally this lock. unlock will decrement the value further and it will become as zero and that is where we will conclude that the given threade has released this particular lock so uh that is for the code demonstration now let's look further into certain other aspects of reant lock which is the lock fairness so Reen lock provides a Constructor in which we can pass a Boolean value for the fairness of the lock when this value is true the lock is called a fair lock if it's false it's called as an unfair lock by default the rant locks are unfair let's understand this fairness with an example imagine there are a couple of threads which are trying to acquire a lock among all the threads which are competing for the lock let's say threade one gets hold of the lock all the remaining threads go to some sort of waiting queue after certain time threade One releases the lock by calling the unlock method at that point of time one of the locks in the weight queue will get a chance to acquire the lock imagine that out of all the threads in the weight queue threade two has been waiting the longest for acquiring the lock then if the rint lock is fair trade 2 will get a chance to acquire the lock and do its processing in the case of unfair inant locks there is no deterministic guarantee as to which threade gets to acquire the lock as a sweet side effect the unfair locks could potentially provide a beta performance as compared to the fair loocks what is the reason well they avoid the overhead associated with maintaining a weight que and enforcing some sort of deterministic ordering now let's learn about a few important methods of of the reentrant lock the first one is get hold count this method returns an integer which is the number of times the current thread has acquired the lock in other words it indicates how many times the current thread has successfully acquired the lock without releasing it the next one is the trylock and as the name suggests it is an approach where we request a thread to try acquiring a lock result is a Boolean which tells if a thread was successful fully acquired or not if the outcome is true we can proceed with processing and if false we can do something else and what we gain from this approach is if the lock has not been acquired then we are not blocked rather we can do some sort of other processing as well in our code there is another variation of trylock method which accepts timeout and time unit so this is an overloaded variation of the trylock method using this me method we can request a thread to acquire the lock and be blocked for the given time duration if the lock is being held by some other thread we hope that the lock becomes available during the waiting time period and that's where the lock is acquired by the thread which has called the trylock with duration and if not then of course the output of this particular trylock is a false and then we can do something else because we have not acquired theog due to X and Y reasons well trylock has a problem even if your reent lock is fair and trylock is called by a thread at the moment the lock is released by some other thread in this scenario The Waiting threads for this lock are not given the priority the new threade which made the trilock call gets to acquire the lock the work around for this problem is to pass a timeout value of zero time units and this will ensure that the fairness criteria of the lock is honored then we have a couple of auditory methods so we have something like is held by current threade and it returns a Boolean which tells if the current thread is holding a given lock or not the next one is gate Q length and it tells the length of the weight Q which is the number of Trades these two in particular are mainly used for debugging and profiling purposes but having said that if there is a need in your code implementation or in your business logic you could surely have have these Methods at your disposal the final one in our list is the most important one so the new condition as we saw earlier this returns a condition on the given lock and what it does is it helps in orchestrating the interaction between the thread and the lock so that should be it for the re interent lock in Java a read right lock is a synchronization mechanism which allows multiple threads to read a shared resource currently but only one threade can write to the resource at a time this mechanism is particularly useful when the resource is predominantly read heavy rather than write heavy let's understand this by visualizing an example imagine that there is a booking system using which you could book seats in a bus there are two main functionalities when it comes comes to the seat selection first is you should be able to view the available seats and the other one is where you can book a particular seat so in the image that we have we have threads from one to in which you want to view the available seats and threads from 1 to n which want to book a seat as for what we have learned already for any thread to be able to do some processing it needs to acquire the lock so let's assume that threade one which wants to just read the seat availability acquires the lock and proceeds with it please note that this is not efficient why so it is because there are other threads which also wanted to just read the data since they are not writing anything there would not be any concurrency issue involved if more than one threads acquire the lock and do its processing and this processing is just to read the data so how about if if we could somehow Club the reader threads together and it would be much efficient right and yes certainly it would be very much efficient and the read right log indeed does the same thing after some time the read threads are done reading the data and at some point of time the read lock is released once released either of the wrer threads could acquire the right lock and do their processing effectively in the case of a read right lock one writer thread could get hold of the writer log at a time or multiple reader threads could get hold of the read log at a time please note that we have two logs one for the reader other for the writer so even though we have two different logs two different objects only one can be acquired by your thread or a group of threads at a given point of time so either read lock is used by n threads or write lock is used by one trade but it can never happen that both the locks could be used at the same time ever so what we have seen essentially that there are two kind of threads the first category is that of the reader threads the second category is of the writer threads since reader threads are not mutating anything in the data they are safe to run together and that is the reason that all these reader trades could acquire the reader log at once and do their processing when it comes to the writer thread this could not be the case because writer thread is going to change something to the data and that is why only one writer thread is allowed to acire the lock so in such scenarios wherein you have a very much Clear condition of reader versus writer this is one such lock which could be used in such cases and please note that you would gain performance if your application is or rather if your use cases read heavy not a write heavy so now we will have a quick code demonstration to see how can we make use of the read write lock so as part of the code demonstration what we are going to build is a class which will have a counter there would be couple of reader threads and a writer thread the writer thread will try to increment the value and the reader threads will try to read the value and we wish to do so using the read write lock so let's implement it we will create this class in the locks package let's call this as shared resource we will have a variable let's call this as counter the value is zero then we will have the read write lock so read write lock let's call this as lock and we will be instantiating the object of new reent read write lock we have the first method which is supposed to increment the value and the way we do this is we acquire the lock so lock dot WR lock do lock and we will have the implementation inside a tri block finally is used to unlock the lock that we have acquired previously and what we will do is we will increment the counter by calling counter Plus+ and we will print a message the message could be something like thread do current thread. getet name and it writes and the value that is counter so this should be it for for the increment method now let's implement the other method as well which is going to get me the value of the counter in a given threade and idea is that this is supposed to be made use of by the reader threade let's minimize this one now back to the implementation of the G value first we try to get hold of the read lock and call lock on this one next we will have the dry block and inside the finally we will close the read loock that we have acquired by calling the unlock method and we will read the value for the counter so let's have the message like trade. current trade doget name and beeds counter so this should be for for the get Value method and for the read WR log demo we will create another class so let's call this class as read WR lock demo and uh we would have a main method inside the main method we create the object for the shared resource and we will create a couple of reader threads so let's do that so for I and inside the loop what I do is I create the reg thread so new thread and uh for INT J as zero J Less Than 3 j++ resource dot get value and now let's assign a name to this trade so let's call this as reader threade and pass the value as I + 1 now we will start this thread so let's call do start on this one for the writer trade let's create one so trade wrer trade new trade and we do this for five elements so share resource do increment and likewise we will provide a name to the writer thread so let's call this as writer thread and let's start this thread so let's run this code and see how is the outcome like all right so what we see is that we have reader trade and there is another reader trade the two that we have created right and they're reading a value of zero initially because the wrer thread has not been executed yet the value remains to be zero the initial value then we have the writer thread and what the writer thread is doing is it's going to increment the value five times and uh as a result what we see is the value gets incremented to five now we have the reader thread and the reader thread reads the value and the value that that it's reading is five again the reader thread is invoked and it's reading the value as five so this is how you could make use of the read write lock if you have such a use case wherein you want to bate the lock acquisition for readers and writers separately so that should be it for the code demonstration let's complete this section of read WR lock by learning about the behavior of weight Q in the read write log so let's visualize how the we cues are arranged and managed in the case of a redr lock in the case of a redite lock there would be two types of threads in the we Cube one type of threads are those which want to read the data and the other type is which want to write the data let's say that initially there is one writer thread and two reader threads in this case the reader threads will be allowed to acquire the readlock this is done because it's sufficient essentially what you are allowing is two threads to be executed as compared to the one once all these reader threads are done with their operation the right of thread baiting in the same que is allowed to acquire the right log what happens in the scenarios when there is a writer thread waiting in the queue and the reader threads keep on coming to the queue should they all be allowed because it's faster to execute well if we do so the writer threads will be staffed and this is not good so the this kind of lowlevel optimization is taken care of depending on the type of implementation you choose for the read write lock for example reant read write lock is one such implementation the key takeaway here is that read write locks are optimized for reading and writing operations in the most efficient manner possible and you as a developer should be good to make use of this lock without much optimization or configuration concerns so whenever you have such requirement consider making use of this lock let's learn about this very important concept of visibility problem before we start learning about the visibility problem let's take a important detour to understand and visualize how cores registers and caches are placed along with the ram the image here is showing a machine which has six cores on the first layer we have the course course are the piece of Hardware which are responsible for all the computer activity Loosely speaking code itself does not have any storage capability it can only compute but in order for the computation to happen we need to have some sort of data storage as well right this is where we have different type of storage units let's know about them in a very brief manner first on the list is a register every core has a dedicated storage component which holds the data temporarily during the execution of the instructions this storage component is known as register it has the least amount of storage capacity each core has a dedicated register the next one in the line is the component of L1 cache L1 cache is a small but a very fast memory it plays a crucial role in improving the performance of the CPU by reducing the data access time to and from the main memory which is also known as the ram each core has a dedicated L1 cache the next component is the L2 cache it is a larger capacity cache as compared to the L1 cache its role is to complement the smaller and faster 111 cache it does so by providing additional storage for frequently accessed data data and instructions L2 cach is integrated in two ways depending on the architecture of the processor in one approach the L2 cach is shared across different cores whereas in the other approach we could have dedicated L2 cache for a core the presentation that we have here we are showing a shared El to Cache where the cash is being shared by two course for example here Core 1 and Core 2 are sharing this L2 cache likewise core 3 and core 4 are sharing this particular L2 cache and so and so forth the third type of the cache is L3 cache it sits between the L2 cache and the ram which is also known as the main memory its role is to provide a larger pool of cast data and instructions which can be shared among multiple CPU cores the final type of a stor layer is RAM and as told earlier that this is also known as the main memory it Serv serves as a temporary storage for data and instructions that are actively being used by the CPU so on the one end of the spectrum registers store the data specific to a core and on the other end of the spectrum we have Ram which stores data and instructions being worked upon by all the cores of the CPU when we go farther from the core towards the ram the latency and memory size increases so Ram would be of larger size as compared to L3 cach L3 cach would be of larger size as compared to L2 cach and likewise register is the one which has the least amount of storage capacity but since register is closer to the core as compared to the L1 cache and L2 cache registor would be faster in fact the fastest type of memory whereas Ram is the most farthest one from the core so it is the slowest of the lot so the ram would be the slowest memory and the registor would be the fast fastest memory let's see this diagram what we have here is a code snippet and basically we have two methods assume this write method is being used by a writer thread and this read method is being used by a reader thread and this particular code snippet is being executed in this kind of a machine wherein we have two cores and core one is executing threade one and core two is executing threade 2 so to begin with there is a variable able whose value is zero the count variable starts with value zero and assume at certain point of time thread one gets to execute on core one and thread two gets to execute On Core 2 so basically both of these two threads are running in parallel so the value of count is updated as one by the writer thread and at the same time the value count is loaded in the value so what would happen if you try to access the value of value by either printing it or debugging it so you may see something like this so to begin with count is zero that is in the shared cache then the count is updated to Value one and this value is there in the register which is local to Core 1 now Core 2 is running threade 2 and it is trying to access the value for count so remember that as of now the value of count is one is only with this register it has has not percolated to the shared cach level so the value that is present for this count variable is still zero so the value that is assigned to Val is still zero in this register and when we try to print this value we will get the value as zero as opposed to what we were expecting which should have been one so this is not something which is expected and correct right so this kind of behavior is called as the visibility problem Java provides a solution for this with a keyword called as volatile using this keyword ensures two things first is whenever there is a change in the value it's flushed to the shared memory second is whenever there is a value which is supposed to be read it's read from the shared memory the shared memory is usually and in most of the cases L3 cache which we saw earlier to be the shared cache and this one is a level up to the ram so basically what I'm trying to say is let's say if I used volatile with this variable so what I'll do is I will write this as volatile keyword then int count right and then when thread one is executed and any value to this variable is changed it won't be only there with the register rather it will be written to the shared cach as well and since this variable has been made volatile the thread won't read this value from the register or any other cache rather it will directly read from the shared cache so on the right we're ensuring that the value for the particular variable which has been made volatile is going to the shared cache and for the read as well we're ensuring that Valu is raid from the shared cach itself so that would avoid Us in Reading wrong value and this is how the volatile keybo helps us with the visibility problem there is a word of caution we should always follow while making use of the volatile keyword however while volatile does help with the visibility problem as a side effect it slows down the code and does the application it's a nobrainer that with the uses of volatile the many layers of caching are removed and so are the latency optimization benefits which we gain when we use the caching so when not needed the uses of volatile keyword should be strictly avoided dat loock is an important concept we should know about when we are writing multithreaded code in Java let's learn about the same what is a deadlock here is an image which tells us about a deadlock situation let's have a closer look here we have two trades which are running threade one at some point of time is going to need lock a which is represented by a green color color then after some point in time threade one is going to need lock b which is represented as blue color now let's look at threade two this threade is going to need lock b at some point of time and then it's going to need lock a after some time please note that in both the threads both the locks are needed to be acquired in order for the processing to get completed only after the processing has been completed both the logs A and B will be released let's imagine the scenario at time t threade one needs lock b while it has already acquired the lock a likewise threade 2 needs lock a while it has already acquired the lock b but both the threads won't be able to acquire the needed lock because their need is being held by the other thread and neither of the threads are in a situation to let go of their acquired lock since it's needed to complete the processing of the threade so in this situation none of the trades could proceed with their execution and this situation is called as a deadlock so here is formal definition of the Dee loock in a multithreaded context a de loock occurs when two or more threads are blocked forever each waiting for the other threade to release a resource they need to proceed this situation creates a cycle of dependencies with no thread able to continue with its execution so how can we spot deadlocks in our code there could be two approaches one is the manual approach other is the programmatic approach so let's learn about the same well detecting or spotting locks at the compile time is not an easy task this is so due to multiple reasons Java comes with multiple types of logs so multithreaded code can be littered with different types of explicit and implicit logs also there could be multiple sources of threads in the code base due to these two factors combined with the fact that threade execution and CPU location is not deterministic it's nearly impossible to figure out when which lock is going to be acquired by which threade moreover the example we saw earlier was a simplified version where we had two threads and two locks however in real world it won't be this simple there is a good amount of possibility to have multiple threads and multiple locks which could lead to a deadlock situation and thanks to all of these complexities it's nearly impossible to detect a date log by just looking at the code so the manual approach is something which is not very efficient neither it's the suggested approach so how do we detect the deadlock in code let's have a look on the programmatic approach of detecting the de log so now that we have learned that detecting a dead log is very difficult in manual way are there any other tools which can help us in this detection good news is yes there are tools and ways in which we can detect deadlocks in our code two of the most common approaches are taking the thread dump and uses of the thread MX Bean to programmatically detect cyclic thread dependencies which can lead to a Deadlock Lo so let's have a brief code demonstration where we will be knowingly introducing deadlocks in our code and then use these methods to detect the deadlock situation so in order to demonstrate the dead log code I'll create this package and let's call this package as other Concepts and uh let me create this class as as dat lock demo we will have two logs so lock lock a and make this as a reint lock let's duplicate it create an another lock lock b and we will have two methods first would be a worker one method other would be worker 2 method and uh let's Implement these so public void worker one it will start by acquiring the log a and then there would be some message which will be printed like worker one acquired log a next we will introduce some sort of sleep time so let's call thread. sleep for 200 millisecs let's handle the interrupted exception let's throw it as a runtime exception and uh then we acquire log B then let's print some message it could something like worker one acquired lock b and then we start by unlocking the locks which is lock a. unlock and loock B do unlock so this should complete the first method which is worker one now we will duplicate this and we will write the worker two method implementation what worker 2 does is it starts by acquiring the lock b and then we will of course change the messaging here and again we have a sleep for 200 milliseconds and now after log B is acquired log a will be acquired and again we will adjust the messaging and then we can have Lo lock a unlock itself and lock b unlock itself now let's write the main method where we will be creating the threats and uh executing these things so we start by creating the object for the class so that we can invoke the methods now what we can do is we can say new threade demo do worker one we will give this thing a name this threade a name and we call do start likewise we'll create a threade which will be making use of the worker 2 and we will adjust the name here for the threade which will be worker 2 so let's run this thing and see what is the output like all right so what we see is worker one has acquired lock a and worker one acquired lock b so essentially the worker one got a chance to run and after that point of time worker 2 was supposed to be run but now we are in a deadlock situation because worker one has acquired the log B and here work worker 2 requires log B in order to do its processing but since the worker one has acquired lock b and not released it yet the worker 2 cannot get hold of it and that is the reason we have entered it into a situation of Deadlock what we can do about it now well in such cases you cannot do anything but your code will be sort of in a DAT log situation and you will have to terminate it so as told earlier that we can either take a threade dump or maybe we could use the other approach of using the thread mxp to understand what part of the code is leading to this dat log situation right so let's understand the first approach which is using the thread dump so what we will do is we will go to the terminal so let me come out of the distraction free mode and here we go to the terminal and in the the terminal we will list all the processes which are running Java so we see that these are the processes and the one which we are concerned about is this deadlock demo and here is the process ID so what we do is we call kill hyphen 3 and we pass the process ID which is this one hit enter and go back to the output console so here we can see that we have lots of messages which are getting printed so this is something that we had earlier right where we had the deadlock situation now we are seeing something called as a thread dump which is the full thread dump and it's going to show certain information regarding the threade classes certain messages and so on so forth of course you could try to do it yourself on your system and uh read through all these things what exact is being told but the most important thing is in the last so scroll to the last and here we have this message and this thing says that found one Java level deadlock what does it say it says waiting for ownable synchronizer a fancy name for the lock which is held by worker 2 so worker one is saying that it's waiting for a lock which is being held by worker 2 and the worker 2 says it's w hting for a lock which is hailed by a worker one right so what you could do is you can scroll further down and you could see that this is one line which is being hinted in your code click it out and you could see that this is a probable reason so in your use case in your real world use case whenever you are faced with this kind of situation this is the way in which you could find out the process ID and kill it with hyphen 3 as the argument and this will help you out with this threade dump and you could figure out at least you have a point of Investigation where you could try to understand what is the DAT log situation and what part of the code is leading to this scenario so this is the approach in which you could analyze the threade dump to understand the date lock there is another approach however so let's go to the code and we will have to modify something in our code in order to understand that particular approach so what we are going to do is we are going to make use of this thing called as thread MX bean and uh we will create an another threade let's say a threade three which is going to execute forever and it will be done after an interval of every 5 seconds so the idea is to probe the code and to understand if there are any trades which are in deadlock so let's write the code it will be much simpler to understand that way so let's start by creating a thread and uh let's Supply the code for the same so the first thing is we have trade MX Bean let's call this as MX bean and it provides a management Factory using which you could create the threade MX Bean and uh while true what we can do is we can say mxb find me all the date log trades it will return a array of long which are basically the trade IDs and if the threade IDS does not happen to be a null so we can print this message that dat log detected and uh we can iterate over the long trade ID for each of the trade IDs in the trade IDs and then print this message the threade with id threade id is in deadlock and uh once we have printed this message what we need to do is break out of the infinite Loop and uh if that is not the case so we need to do this every 5 seconds so let's put a timer of 5,000 milliseconds and catch the interrupted exception and we will throw it as a runtime exception and finally let's call the do start on this thread so here is what is happening we have created a thread which is going to run every 5 seconds and we are making use of the management Factory to get an object of the mxp and the MX Bean has a method which will give me the deadlock threade IDs if there are any trades which are de loock and I am simply going to iterate over the given trade ID right and print the message here you may be wondering that trade ID is fine but what if I need certain extra information how can I do that well there is an another method in the thread MX Bean which is called as get thread info so let's make use of that as well so we can say mxb get thread info and it's going to accept a long array which is the threade IDS and what it returns is an object of thread info array and let's call this as thread info so thread info contains a lots of information as in what is the threade name what is the lock name uh where exactly it is waiting for which lck and all those things so in this example we don't really need to showcase all those things so but the idea here is that if needed you could certainly make use of this particular line and uh iterate over the trade info and get the required information so let's run this and see the output so run and initially we have executed these trades there is a timer of 2 seconds remember 2 milliseconds rather and then after some point of time this thread kicks in and it's going to say thread lock detected and thread with id21 is in deadlock and thread with id22 is in dat loock and as I told earlier that if you need certain more information you could make use of the threade info object or the class and uh these are the information it contains and you could make use of these information in your code as per your need so now that we have seen the code demonstration for the dead log let's understand how can we prevent Deadlocks so are there any approaches or suggestive measures using which you could get rid of the Deadlocks or at least try to write a deadlock free code so the first and foremost is use timeouts so let's say when your trade is trying to acquire a lock giving a timeout will ensure so it's not going to end up into a situation where it's trying to acquire a lock for infinite time right and thus it will help in avoiding the deadlock situation so uses of timeouts is a good approach in order to avoid Deadlocks the second thing is the global ordering of the locks so another thing we can do is to pay attention to the global ordering of the locks what it means is let's say we have two trades and two locks to acquire for example log a and log B both the threads should first acquire log lock a followed by lock b so in that sense it won't be a cyclic dependency scenario and it will avoid the DAT lock situation the third thing is to avoid any sort of nested locking so whenever feasible we should always avoid locking within already logged section and the fourth preventive measure is to make use of the thread safe Alternatives so whenever feasible to do so use thread safe collection and atomic variables and minimize the uses of logs sure the thread collections do make use of the logs under the hood however in general the code implementation of the standard threade safe are battle tested industry Solutions so there is a little to no chance of running into the deadlock situation so this should be it for the concept of Deadlock and uh these are the certain measures using which you could try to avoid dat lock situation and even if you are in one situation of a date loock what are the different approaches using which you could analyze your code to correct it Atomic variables are an important tool which help us in writing concurrent code in Java let's learn about the same so what's read modify right cycle in order to understand what is a read modify right cycle let's have a quick look on this image there are two threads and a variable called as count which has the value as zero there is a processing part which needs to be done on this value which is to Simply increment the value by the count one two threads get to execute this process of incrementing the value imagine if these two threads are going to be executed 1,000 times what we would expect is that the final value of the count should be 2,000 however this is not the case so th times the value will be incremented by threade 1 and another thousand times the value should be incremented by threade 2 effective value expected is 2,000 but when you try to run this code you will find the value would be less than 2,000 moreover it will be very much inconsistent so at sometimes you could get some different value and at the other time the value would be totally different so there is some sort of inconsistency let's learn why so well the reason lies in the fact that how do we actually understand the increment operation and how it is actually performed on the low level so on a very low level there are three steps which are performed when we increment the value of a variable as part of the step one the value of the variable is loaded in the register remember register is the local storage which which is the nearest and almost native to the processor core as the Second Step this value is operated on as part of which it gets incremented by one and finally as the third step the incremented value the new value is assigned back to the variable count so to understand with an example at step one the value is zero which is loaded in the register and at a step two it becomes one and finally at the step three the value one which which has been incremented from 0 to 1 assigned back to the counter this entire flow is called as the read modify right cycle and this is the core reason behind the inconsistency in the value if it's incremented across multiple threads since I have already covered this concept in more depth with examples I won't go into much details however if you could remember from the previous discussion what's exactly happening is that the entire process of increment ing the value count Plus+ is divided into three steps and and let's say if threade one is operating on the count operation which is involving these three steps and at the same time somewhere between these steps if threade 2 is going to interrupt then this problem will occur and this is the sole reason behind the inconsistency so now let's talk how can we solve this without using any kind of explicit synchronization mechanism so what are Atomic variables what we saw earlier is something called as a nonatomic operation what it means is essentially an operation for example incrementing a value can be broken down into three steps thus it's not Atomic something which can be broken down so what Atomic variables in Java do is that there are a type of variable that support Lock Free thread safe operations on single variables they ensure that any kind of read modify right operation such as incrementing or updating a value are performed automically it prevents race conditions and this makes them crucial in concurrent programming as they help in maintaining the data consistency and integrity without the overhead of traditional synchronization mechanisms Atomic variables provide builtin methods that perform Atomic operations it ensures that actions like incrementing and updating are completed as a single and non visible step this eliminates the need for synchronized locks which can be costly due to locking overhead and potential thread contention as a result Atomic variables offer a more efficient way to achieve thread safety in the concurrent programming there are different types of atomic variables the atomic variables in Java are basically classes each of these classes are used for a particular data type for example integer long Etc the Java do U.C concurrent do atomic package contains all such Atomic variables however Atomic integer Atomic Boolean Atomic long Etc are some of the important and the most commonly used ones so if you wish you could go to the concurrent Atomic package and explore the other Atomic variable options that we have now with that in mind let's understand a few of the basic operations that we can perform on these Atomic variables these Atomic variables provide quite a few operations which we can make use of as per our need however there are a few which are very commonly used let's learn about those the first one is get so it fetches the current value next one on the list is set operation and as the name suggest it will set some value the third one is compare and set and this operation expects two values which are expected and update it sets the value to the update value if the current value is equal to the expected value then we have have these two operations which are equivalent to the pre and post increment this operation automically increments the value by one and returns either the new value or the old value the fifth one and the final operation are list is gate and decrement and decrement and get which are equivalent to the pre and post decrement this operation automically decrements the value by one and returns either the new or the old value please note that these are the most commonly used operations and there are much more depending on the kind of atomic variable you're planning to use you could explore the respective classes to find about their supported operations in more details but having said that these are a few of the commonly seen operations that you would come across when you write the code related to Atomic variables now let's conclude the discussion on the atomic variables by going through a code demonstration and we will be using the atomic integer Ser as the one of the atomic variables to showcase the uses of atomic variables so in the other Concepts package let's create a class let's call this as atomic variable and what we are planning to implement it's some code which is going to work on a counter variable and there would be a couple of trades working on this and we will see how we could make use of the atomic integer to help us with the inconsistencies of multithreaded programming so let's start with having the count variable so the initial value is going to be zero and then we will have the main method so we will first create the threade one let's call this as threade one and what this will do is it will run for it will call count Plus+ for thousand times let's have the similar trade let's call this as threade two and it will also do the same that is it is going to increment the value by 10,000 times so let's start these two threads let's remove the one and type it as two we will have to wait for these threads to be completed so let's do that surround with try catch let's put two here and now let's print the count value so count value is count so let's execute this code and see what is the output like all right what we are seeing is the value is 14,113 which is not correct of course because what we wanted is that we have two threads which are going to increment the value 10,000 times so effectively the value should be 20,000 when we print it here but that is not the case so the reason this is happening is because we are using count Plus+ in a multithreaded context and which is of course not an atomic operation and due to the reason that we learned in the theory section this is the problem we are facing so how can we correct this so let's make use of the atomic integer so private static final Atomic integer let's call this as counter and let's create the atomic integer initial value is going to be zero so instead of having this count Plus+ let me comment it out and what we can have is counter do increment and get so what this increment and get does is it is going to increment the value of the counter which is zero to begin with and it will get the value so essentially if you want you could capture it somewhere but even if you don't capture it the counter object is going to hold the updated value so let's say if it's getting executed for the very first time and the initial value is zero then the value for counter will become as one because it's going to increment it and get the value which is self assigned if you want you could capture it as some variable as I showed earlier now for the second thread let's comment it out as well and again here we could call increment and get on the counter so that is about the uses of counter the atomic integer in the thread section now everything else Remains the Same and here we will try to print the value of counter instead of count so let's print it out and let's run this and say the output so here is the output what we see is the value is 20,000 let's run it a couple of times to see if it's working consistently so yes we are getting the same value over and over again what it means is the value is consistent and the problem that we were facing earlier of inconsistency because we using a nonatomic operation that was the count Plus+ that problem has been fixed when we started using the atomic integer so as I told you earlier that Atomic integer is part of the Java util concurrent Atomic package and likewise we have many other options so if you go to this package you will see that we have Atomic and we have Atomic Boolean Atomic integer Atomic array and so on so forth there are lots of options that we have in the atomic package so you could explore all of these things depending on your need but having said that this should conclude the discussion on the uses of atomic variables SEMA Force are an important concept whenever we talk about multithreading in Java so let's learn about the same what are semap fores a semap for is a synchronization mechanism used to control access to a shared resource in concurrent programming it uses counters to manage the number of allowed accesses to the resources preventing rise conditions and ensuring safe concurrent operations SEMA force can be binary or counting it depends on the number of permitted axess while I was learning this concept the terminology of semap 4 used to confuse me a lot the word semaphor is not very intuitive and it does not reflect something which is common in our daytoday uses so why do we name a particular concept at semaphore if you are in the similar situation let's learn what does this word mean the term semap for comes from the signaling devices used in Railway and M time contexts to control traffic in programming it similarly signals and controls access to Shared resources among concurrent processes essentially this term semaphore is not commonly used in everyday language among native English speakers it is primarily known in a specialized context like programming operating systems and historical references to Signal devices in transportation so with having known the definition of the topic let's understand how is this semap for used with some visualization imagine we are building an application which needs to connect to a third party service so this is our application and this is the third party service so this is the application and this is the third party service due to sub constraints the third party service could be accessed only by a limited number of Trades at a given point of a time so for example what it means is only three concurrent calls would be allowed to this third party service from our application so in order to implement such scenario we need to ensure that we have some sort of restriction in place in such situations we can think of making use of the semap 4 to implement such restrictions for example assume that our application has 50 threads in thread full however the third party service is going to allow only three threads so in that case we can think of using the semap for now let's understand the concept of SEMA for with this image let's start with an assumption that SEMA 4 at the core of it is nothing but a permit mechanism the image shown has three boxes these three boxes are nothing but permits so at the moment this semap 4 is going to allow three threads to be run so essentially if there is a threade that wants to execute it will ask the semap for that hey I want to be executed so in that case simaf for will check that does it have any permit or not so if there is a permit that permit will be given to a threade so basically this permit is like a token and once a thread acquires it then it's going to do its processing so this is what we mean when we say that at this moment this semop 4 is going to allow three threads to be run to begin with we have thread one which wants to execute so it calls acquire on the semaphore the acquire method is like it's requesting for a permit now that at the given instant we have three permits available the threade one is going to be allowed and it will execute and access the third party service so essentially thread one comes and it says Hey I want one permit to be executed sea 4 is like all right I have three permits here you take this one and you can proceed so once it's given you could imagine that this permit is no more so essentially you have two permits left and once this permit is assigned to thread one it goes ahead with its execution so thread 1 is essentially allowed to execute so once thread one has been given the permit Sima 4 will have two available permits and while thread one is executing and accessing the service let's say we have another threads which are threade two and trade three which come to SEMA 4 asking for permits and at this moment the SEMA 4 has two permits available so it will give the permits to threade 2 and threade three and essentially after these permits are given to to threade 2 and threade three they also go about with their execution so threade one starts with its execution and so does threade 2 and threade three after acquiring the permit now let's imagine at some point of time in Future threade 4 comes into picture and it's going to the SEMA 4 and say hey SEMA 4 I want to execute and in that to happen I want to acquire the permit from you so at this moment SEMA 4 is is like all right that's fine that you want to get executed but I don't have a permit with me as of now so what you can do is you could wait and due to this threade 4 gets blocked so threade 4 had called the acquire method and it's going to get blocked on the acquire method and sometime in future when thread one is done with its execution it will say all right I am done with my execution now I am going to release the permit that I had acquired so the permit will be given back to the SEMA 4 and as soon as that permit is given back to the SEMA 4 the threade which was waiting for a permit in the SEMA 4 will acquire it and proceed with its execution so let me revert back the image and now we have all the three permits available with the SEMA 4 so at some point of time Trade four got a chance to execute by acquiring the permit so this is the way in which SEMA 4 works so let's understand what exactly is this release method well thre says Hey SEMA 4 I'm done with my execution thus I'm returning back your token your permit that you gave me earlier and you could make use of it as per your wish so you could understand that SEMA for permits are nothing but kind of reusable tokens in that sense so if you notice at any given point of time in this semaphore we can only have three threads executing at maximum and that is what SEMA 4 is used for so essentially what we have done is we have restricted the count of Trades which are going to access the third party service using your Sor now that we have understood how exactly SEMA 4 is working under the hood let's have some quick code demonstration which will showcases how we can make use of the SEMA for in our code so here I am in the IDE I will create a new class for the demonstration but before I create the class let me tell you briefly what exactly I'm planning to build so I'm planning to build a small use case where we will be making use of some third party service to scrape the data don't worry it's not going to be something difficult it's rather just a dummy kind of method the entire idea is to Showcase how we can use the semaphore rather focusing on the scripting part of it so let's get started and I will create a class I'll call this as a scraper and let's have this m method to begin with and I will create the scrape service which is the service which will be called by the scraper class so just as a good practice I am going to make the scrape Service as a Singleton class not that it's needed for this demonstration but just as a good practice let's have it as a singl ton and let's have this instance and uh what we will be needing is a seap for so let's initialize the semaphor it provides a Constructor wherein we can pass some value so this semap 4 is going to have three permits and we we have a method that is called scrape what this scrape is going to do is let's going to run in a TR catch block let's have this interrupted exception and this will throw a new runtime exception and we will also have a finally why we will look at it in some time so to begin with this thing is going to acquire the SEMA 4 so it will call the do acquire method on the semap 4 and then we have a helper method which is something like invoke scrape bot so invoke a scrape bot is the actual third party service invocation wherein we are going to call the third party service and uh do the scrapping task so we will implement this method in some time but for now let's have the signature here and imagine that uh some thread has acquired the same of 4 permit and it got terminated due to some reason or maybe it went into a deadlock situation so what happens is the Sim for permit is acquired forever but we don't want that to happen right what we want essentially is that once a thread is done with its execution it should be releasing it so similar to the way in which we used to acquire and unlock the logs right we will be using the try and finity block and in The Finity block we will say SEMA 4 do release idea is that in either case of success of failure the execution inside the finary block will take place and that is when we are going to release the semaphore so let's implement the invoke script B method so it's private void invoke script part and this is nothing this is just going to print some message so let's have the message as scrapping data and to mimic some sort of uh execution time we will have some sleep introduced in the code let's say this is for 2 seconds and since we have given thre do sleep we need to handle the exception of interruption and the way we will do this is we will throw a runtime exception with the exception not the best way to do it this should do for the demonstration purposes so now let's go to the main method and try to make use of this so what I'll be doing is I'll be creating a new cast threade pool and using that cast threade pool I'll be submitting the tasks for the script service so let's to that so I'll be making use of the try with resources pattern so executor service service and executors do new cached trade pool and uh let's say I want to do this for 15 times and service. execute new runable and this will call the scrape service scrape so what we have done is we are creating uh this service that is the new cast threade pool and we are supplying this runnable we are calling this script service method inside this runnable 15 times so what we would expect is that at any given point of time only three threads are executing so let's run this and see the output so here is the output we have a scraping data then we have a scraping data then we have a scraping data finally we have a scraping data so what is happening essentially is out of all the 15 times we have submitted the runnable we are just allowing three threads to execute simultaneously so scraping data is printed twice scraping data is printed twice and so and so forth so this is kind of demonstrating that using a SEMA 4 we are restricting how many trades can run at a given point of time please note that how many trades will be actually running at a given point of time will also depend on how many course you have and all those things but the whole idea is that if you use a semop for and some sort of permit then in the worst case possible at Max only those many threads will be running at a given point of time and that is the whole intention and idea behind using the semaphor so now that we have seen the code demonstration let's learn a few more things about the semap for we also have a concept of multiple permits please note that we can acquire multiple permits at a time for this we have an overloaded method of acquire which allows us to enter the number of permits we want so when we call S 4. acquire you can pass a value so let's say Sima for acquire and you pass a value of two so two permits out of three will be taken by the threade however we should always ensure that the number of permits we take should be equal to the number of permits we are returning back by calling the release so like we had passed the number of permits in the acire method we also need to pass the number of permits in the release method as well so if do acquire is two do release should also be two and uh there are some other methods in semap 4 as well let's learn about those the first one is try acquire so when this method is used the thread will try to acquire the permit if there is no permit available the thread won't be blocked rather it can do something else then we have try acquire with timer out which is essentially the same as TR acquire except it accepts some sort of timeout the next one is the available permits and as the name suggests the available permits Returns the number of available permits with a given semaphore the final one on our list is this new semop 4 which accepts count and fairness well this is an overloaded Constructor of the semaphor it accepts a fairness criteria which is a Boolean value and when given as true the threade waiting the longest gets the chance to acquire the permit once it's available with the semap 4 so that's all about the semap 4 it's very easy and intuitive to understand essentially it's a mechanism to introduce a controlled bottleneck situation to ensure that there is an upper limit on how many threads can work on a shared resource at a given point of a Time let's learn about the mutex so what is a mutex mutex is a short form of the word mutual exclusion it is a synchronization mechanism used to control access to a shared resource in a multithreaded environment in Java the primary purpose of a mutex is to ensure that only one thread can access a critical section or shared resource at any given time it prevents raise conditions and ensures data consistency wait a second hold on are you thinking that it sounds very similar to some other Concepts we learned earlier related to multithreading well you are very correct if you are thinking so mutex is nothing but a fancy term for locks and synchronized blocks moreover the concept of mutex encompasses any mechanism that ensures any sort of mutual exclusion it means only one thread can access a critical section at a time in Java this concept is realized through synchronized blocks and lock interface you may have another doubt you may be thinking if the mutex is indeed just a fancy term for locks and synchronization why do we have this concept at all well the reason is somewhat historical in nature the term mutex is a fundamental Concept in computer science and concurrent programming which predates this specific implementations found in languages like Java the concept of a mutex is used widely in both theoretical and practical contexts to describe a mechanism which ensures Mutual exclusion and thereby preventing multiple threats from accessing a shared resource simultaneously so effectively mutex is the term for General concept of mutual exclusion in context of multithreaded environment and the approaches like using a blocks and synchronized blocks Etc are tools which help us in achieving this requirement of mutual exclusion so if you understand any of the concepts like lock synchronization Etc which provide Mutual exclusion of a shared resource you very well understand mutex so that's all about mutex there is nothing new to learn here as such because we have already covered the concepts related to Locks and synchronized blocks in Java Fork join framework is another important Concept in Java let's learn about the same the fork join framework is a concurrency framework which was introduced in Java 7 to simplify the process of parallel programming it is designed to take advantage of multicore processors by dividing tasks into smaller subtasks executing them in parallel and then combining their results together the fourth joint is very similar to executor service which we learned about earlier in the interest of refreshing your memory executor service mainly consists of some sort of data structure which stores the task to be executed and there is a pool of stes which pick the tasks from this data structure and work on the task depending on the use case you could just execute a task and not return anything or you could return a value from the execution of the task so in these two aspects the fork joint framework is exactly the same as the executor service however there are certain dissimilarities between the executor service and folk join framework the fork join is different from the executor service in the aspects of subtask creation and the fork join framework can create subtasks which is not the case with the executor service now let let's learn about the differences between the FK join framework and the executor service the FK joint framework differs from the executor service in two aspects the first one is the task producing subtask and the second one is per threade queuing and work Stealing In the case of folk joint framework a task is capable of creating subtasks to simplify the problem being solved consider this as the standard divide and conquer approach which is not the case with the executor service unlike the executor service where there is a shared task Q the threads in the for joint framework have their dedicated task queue there is also this mechanism of load balancing where a threade could pick task from the other queue this approach is called as work stealing now it's time to learn why do we have for join framework you may be wondering why do we have yet another framework on top of all the things that we have learned so far in the multithreading well well Fork joint framework is not there without any reasons below are a few reasons why the fork joint framework was provided as part of the Java the first one is the utilization of multicore processors modern processors have multiple cores and traditional singlethreaded or poorly managed multithreaded applications do not fully utilize the computational power of these processors the FK join mechanism efficiently manages multiple threads to leverage all the available CPU CES the next one is simplified parallelism writing parallel code manually is a complex and error prone activity the fork join mechanism abstracts out much of this complexity and it makes it easier to develop parallel applications finally on the list of why do we need Fork joint framework we have this concept of efficient work stealing algorithm the framework uses a work stealing algorithm where idle work worker threads still task from The Busy threads this ensures that all the threads remain productive and proove the overall performance so as I told earlier that we have qes on per threade basis and let's say a threade is done with its execution of the tasks in its queue so in that case that threade could dis steal work from the other threads or from the other queue where it has some other tasks aligned to be executed it's very equivalent to the mechanism of load balancing so you see the fork joint framework has a certain pattern to it and due to this we have certain use cases which fit very well with the users of this framework for example sorting a large data set performing operations on large matrices processing large collections of data in a parallel fashion and so on so forth now it's time to learn about the key Concepts in the fork joint framework forking in the fork joint framework is the process of break breaking down large task into smaller independent subtasks which can be executed concurrently this is achieved using the fork method provided by the fork join task class the next concept is that of joining what it refers to is the process of waiting for the completion of a folk task and combining its results this is done using the join method then we have the recursive task the recursive task is an abstract class and it is used for the task which return a result it is parameterized which means that the type is the type of the result which is produced and returned by the task so whenever there is a need to compute a result and return it after completing the task the recursive task class should be used Loosely speaking you could imagine this to be parallel to the callable interface then on the list we have recursive action it is used for tasks which don't return any result it does not accept any types when the task performs an operation which does not need to return a value or a result it should use recursive action and Loosely speaking you could imagine this to be parallel to a runable interface so what is a fog join pool let's learn about the FK joint pool and I promise that this is going to be the last theoretical Topic in this section before we move to the more juicy implementational side of fog joint framework well fog joint pool in Java is a specialized implementation of the executor service interface and it is designed so to support the folk joint framework it is optimized for parallel processing and efficient management of tasks which can be broken down into smaller subtasks below are some key Concepts and aspects which are related to the F joint pool we have this concept of work steing algorithm so F joint pool uses a work stealing algorithm to efficiently manage and balance the workload among threads when a thread finishes its task it can steal task from the work cues of the other threads ensuring that all threats remain productive and it reduces the ideal time as well next we have parallelism so the pool can automatically determine the level of parallelism based on the number of available processors you can also specify the level of parallelism when creating the fork joint pooling stance so we have two operations like fork and Joint so the tasks submitted to the fork join pool can be split into smaller subtasks and their results can be combined together this mechanism supports the divide and conquer approach for solving the complex problems finally The fol Joint pool is designed to manage the instances of the FK Joint Task and its subclasses which are recursive task and recursive action so these tasks incapsulate the logic for splitting the work and combining the results so in nutshell to summarize the theoretical section what we have is a fol join framework and fol join pool is the implementation of it so essentially FK joint pool is a special implementational case of the executor service where the focus is around the parallelism and it does so by utilizing two concepts first is Fork other is join so Fork is basically creating the subtasks creating the sub of a particular task and join is like collecting the results from the fog subtasks so essentially what we are doing using the fog joint framework is dividing the tasks into subtasks and then asking the different threads to work on the subtasks in parallel and finally the result is collected and the final outcome is given back to the caller so in a way it's a parallel implementation it's a parallelized implementation of the divide and conquer strategy so now let's move forward and now that we have learned most of the basic the concepts related to the for joint framework we shall start with some code implementation for these Concepts so I will start by creating this package let's call this package as Fork join and as I mentioned earlier that we have two classes one is the recursive task other is the recursive action so this particular demonstration would be for the recursive task in the next one we will see how we can make use of the recursive action so what I'm planning to implement is a code wherein I would like to search for occurrence of a given number into an integer array so let's start with the implementation so let's create this class and let's call this at search occurrence task this is going to extend the recursive task and this is going to return integer so basically integer is is the type which it accepts so if you see the implementation it's going to extend the fog Joint Task and the fog Joint Task is going to implement future so now going back to the code it's giving certain error so the idea is that it's enforcing to implement certain unimplemented methods so let's do that and the method that we need to implement is called as compute and by the virtue of the type that we provide which is the integer it's going to return integer from this compute Method All right so this particular class will have a few things for example first is it will have an integer array called as ARR then it will have a start index then it will have an end index and it will have a value which is called as search element so let's create the Constructor for this class and we are going to initialize all these values so to begin with we just have this compute method so let's say I am going to provide a private method which is going to search for the occurence in this particular array for a given element so let's call that method as search and uh let's implement it and probably we will return the value so private integer search and uh let's have the value as zero to begin with and uh what we will do is we will start from the start index then it will go till the end index and we increment the index value if the value at index I happens to be the search element then we increment the count and finally when we are done iterating over the array we are going to return the count value so this is the search method so please note that as of now we are not making use of any fog join Concepts so we will introduce the fog join Concepts in a while but for now let's complete the implementation so this is about this class and let's have the runner class wherein we will be invoking this particular compute method somehow that we will see in a moment using a main method so let's create a class and let's call this as F join pool demo and we have a main method so we will have an array let's say the value is up to 100 and we will have a random variable why we are going to need it we will come to see it in a moment and we populate this array with certain random values so for each and every array index we are going to provide certain value and that will be random dot next int so that is 10 + 1 so essentially I am going to populate each index in the array with a value ranging from 1 to 10 and we have a search element which is again a random value So Random dot next int then we have 10 + 1 and what we are going to do is we are going to create a for join pool so new for join pool and remember that I told in the theory section that if you don't Pro provide the number of processors it will by default figure out how much parallelism is required but in this case let's provide the number of processors as well so runtime Dog runtime. available processors and it's complaining that we should close this resource so probably we can go ahead with drive with resources all right now what we need to do is we need to make use of the fne pool and the way to do this is we need to create a instance of this task which has extended this recursive task so let's do that let's create an instance of this task which is search occurring task and let's call this as task new search occurrence task let's pass the values which is the array start would be zero end would be array do length minus one these are the indices of the array then the search element and we are going to call pull do invoke and here we will pass the for Joint Task and let's capture this as the value of occurrence all right so let's print the value that the array is arrays dot two string AR and finally system out print Ln percent D found percent D times then search element and occurrence and it should be print F not print Ln so let's correct this and uh now let's run this code and say its output so please note that we have not made use of any fol joint pool concept as of now we will do so in a moment so let's run this and we can see that the value 9 has been found six times so you could go ahead implement this code or probably copy the code from the GitHub link that will be shared in this video and you could try to execute this and from this particular value you could validate it so it's working fine I have validated it now what we will do is we will try to reduce the folk and join Concepts so in order to do so what we do is we go to the compute method and we first of all compute the size of the array which is end minus start + 1 and if the size happens to be greater than 50 then we are going to do some operations otherwise I am just going to call the vanilla search method which is return this search method method that we were using anyway so when the size happens to be greater than 50 we will divide this task in two halves and the way to do this is find the middle index say start plus End ided by two and in order to create the fork of the task we will have to first create the task objects so what we do is we can create the task object which is search uring task let's call this as task one then new search occurrence task pass the value the start value is the start index the end value becomes the mid and then we have the search element next we create the other task which is Task two and then we call search aen task then we have array and the start value becomes the mid + one and then the end value becomes the end and then then we have the search element so if you are familiar with the concept of binary search or any divide and concur algorithm implementation you would find it very similar so essentially what we are doing is let's say there is an array with some values right something like this so what we are doing is we have to do some processing some operation on this array so essentially what we are saying is that all right let's say if I have these many values which is let's say six values so I don't want to do operations on the entire array rather what I want to do is I want to split it so if I were to search this array for all the six elements it will take some time what if if I wanted to search for some value on this particular array itself it will take less time right because we are doing the search operation on a smaller sized array so that is the whole concept where we are finding the mid value which is let's say somewhere here and from here to here this is my first section of the array in which I need to do the search and this is the second section of the array in which I need to do the search and this needs to be done till the time this threshold is not M so for example let's say this array is somewhat of let's say bigger size maybe 100 or 200 200 then in that case first split would be 100 100 second split would be 50/50 and when the size is less than 50 then in that case it will come to this section and it will say all right the size is less than 50 the size is less than the threshold then in that case we can go ahead and do the normal search but till the time size is greater than the threshold that is 50 in this case it will go and break the task into smaller tasks and that is where we have made use of this task so the next process in this implementation would be to call Fork method on these tasks so task one. Fork then task two do Fork so what happens is that this task. Fork is going to create another subtask so if you click on Fork you can see that it is going to return the fork Joint Task itself which is basically this recursive task whatever implementation you have here since this class is imple minting recursive task then this particular Fork will also return this recursive task which is the search occurrence task but for a lesser search space right so if that is clear let's move ahead and understand how we can face the value from this subtasks that we have created and the way to do that is by calling the join operation so in a way you could try to correlate this with the way in which the recursion works right and it is pretty much evident by the name as well so basically it's nothing but a kind of recursive call in a multithreaded context wherein you have created different subtasks and all those smaller subtasks could be picked up by different threads and could be executed in parallel so it's nothing but some sort of recursion on stereoids due to reason that we are making use of parallelism and multithreading and all those Concepts which are going to give us some boost with respect to the performance so now let's move and see how we can collect the value so the way to collect the value out of a task is by calling dot join method so let's say task one do join if you try to capture it it will be some integer why integer because that is the parameter that we have provided here so you could do this way or probably we could just simplify it by returning task one do join plus task 2. join and that's the whole implementation so now let's run this code and see the output after executing this code I will give you an another work through so things will be much more clearer so see this value has been found this many times so basically 10 has been found these many times so basically what we have done here is the that we have implemented one of the implementations of the task which is this recursive task and one of the implementations of the fork Joint Task and uh we have certain data structure on which this task is going to be performed and the data structure of the choice here is an integer array and our use case is to find how many times a given value is going to be repeated or is going to occur in the given array and that is the search part of it and that is what we had in the beginning then what we did is we started with the implementation for the compute and in the compute we first found out what is the current size of the array on which we are performing this operation and we had a check that if the size happens to be less than 50 then we are simply going to make use of this search method and if that is not the case then we are going to break this larger task into smaller tasks smaller subtasks and the way to break this is find the middle point in the array the middle index in the array and then create two tasks out of it wherein we have the array itself then we have a start then we have a mid value and the element that we need to find and in the second task we are going to have the same array and the search space is provided from the mid + one as the start value and end as the end value and search element is any way the element that we need to find and after we have created this template for these two tasks we are going to call Fork on these two tasks and when a fork is called what essentially happens behind the scene is this thing is called recursively with this particular search space this particular search space so the search space is going to reduce it's going to get smaller and smaller and likewise when this Fork is called similar thing is going to happen and finally this will keep on happening till the time we have the size the search space greater than 50 and after some time the recursion will reach its base case and then we will like to retrieve the value from that recursive task that we have executed and the way to do so is by calling Dot join there are some other methods as well but in this example we are going to stick with DOT join so let's say task one has accumulated certain value and task two has accumulated certain value so both the values will be added because consider this as a binary tree where we are making calls in two directions in the left side and in the right side and once you are done with the traversal at every point of time from every node you are going to get some value so that is what the intent is and that is the way to visualize this code so that is how we are going to make use of the recursive task to implement Fork join framework and do this operation so before we move to the next implementation which is for the recursive action I would like to give you a small homework and what you could try is you could try to implement this for the use case where we wish to find out the summation of all the elements in a given array so it would be something similar to this but I think you should try it out on your own and it would really help you in cementing the concepts related to the recursive tasks so now let's move to the implementation of the other thing which is the recursive action so let's create an another class and uh let's call this as workload splitter and what I'm trying to build here is that there is a class and it has some workload and based on certain workload it's going to either do its operation on its own or else it's going to split the workload or the task rather into two halves so this is what we are going to implement so let's start with the implementation and this workload splitter class is going to extend recursive action and as I told earlier in the theory section that this recursive action is not going to accept or expect any sort of parameter and uh neither does it return anything so let's implement the unimplemented methods and again the unimplemented method is compute the only difference is that since it does not return anything we have a void over here there is nothing to return right so let's have a value and maybe call this as a workload let's have a Constructor initialize this value and in the workload what we are going to do is we are going to inspect the value of workload if the value happens to be greater than 16 then we are going to do certain things otherwise this particular task is going to be implemented without any splitting so let's print some message we are not going to have any fancy implementation as such rather we are just going to print certain messages to explain the concept so work load within limits task being executed for workload and then the workload value and what happens when the workload is greater than 16 well first start by printing some message workload too big thus splitting and then the workload is given in the message so first of all we find the first workload and this is going to be workload divided by two then we find the second workload and this is going to be workload minus first workload right so again very similar to the recursive task that we had in the previous section in the previous example rather we will create two tasks here for the workload splitter so maybe call this as first split then workload splitter pass the first workload value and then workload is splitter second split then call workload splitter provide the second workload and we can call first split do Fork then second split do Fork all right and that's it since we we are not intending to extract some value and that is the reason we have made use of the recursive action so we are not going to call any sort of join or any value that we need to fetch out of this so that is all with respect to the implementation here we are just making certain load balancing kind of mechanism wherein if needed if the workload is too high then in that case we are going to Fork two new tasks and those tasks will be working on that workload so let's say to begin with if the value is 64 then the value is certainly greater than 16 then the workload will be divided as 32 32 so the first load is 32 second load is 32 again and again this value is greater than 16 so again the load will be divided into two so the first load becomes 16 second load becomes 16 and since that is the value we don't need to go inside this thing rather we will come to this thing and we will do the execution on its own so as I told earlier that there is no any execution as such rather it is just going to mimic certain Behavior around the workload by making use of this recursive action so now let's demonstrate this implementation and uh write the code for the same so let's create a class let's call this as workload split demo let's have the main method and uh we will create the fog join pool so let's call this as pool and new for join pool again let's pass the available course and again this will complain regarding an open resource so we can resolve this by putting inside the try with resources block and the way to use this is create an object for the recursive action class that we have created which is the workload splitter so let's call it as a splitter then workload splitter pass the value as 128 then call pool. invoke and let's pass the splitter object and that's it so so this is what we have implemented let's execute this task let's execute this code and see the output so here is what we have to begin with workload to Big this is splitting workload too big this is splitting and so on so forth so 128 got to split into two halves 64 and 64 32 got split into two halves 64 rather got split into two halves 32 and 32 and uh so basically 32 and 32 and this one is 32 and 322 so as I told earlier that these things are executed all these tasks are executed by threads in parallel so based on the thread scheduling you are seeing that we are having some sort of not in sync kind of response but the idea is that the number is getting split into two halves so 64 64 each 64 is getting split into two halves right one half here other half here and further the 32 is getting split into 16 and since 16 happens to be well within the reasonable limits for the workload we have this message that workload within the limits task being executed for the given workload so these are the messages that we are seeing so that's it about the implementation of the recursive action and that is the way in which you could implement it so that should be it for this particular topic of fork join in Java thanks for staying with me till the end of this video as per one statistics less than 10% people finish watching a tutorial video once they start watching well guess what you are the top 10% of the lot you are awesome so where Do We Go From Here I am hopeful that you learned a few important Concepts which lay the foundation's stone in learning the super awesome yet complex topics of multithreading and concurrency I sincerely request you to start implementing these new found Concepts in your daytoday software engineering activities if not at least try reading some code which makes use of the multithreading Concepts doing these things will cement all the new skills that you have learned and acquired with me in the last couple of hours needless to say the concepts in this video are introductory and foundational in nature yet they are very important so what this means is there are even more Exquisite and complex topics which I did not cover in this video but whatever you learned should be good enough to get you started good news is that I am already working on an another video which will cover more advanced Topics in more details I hope you are excited so to summarize first thing is start implementing the concepts learned in this video then read code which makes use of the multithreading and concurrency third step is watch out for the next video on the more advanced topics in multi trading and most importantly did I tell you that I also have an YouTube channel here is the link please do subscribe it would mean a world to me with this I sincerely thank you once again for your time and attention and I hope that you found this video useful hope to connect with you again in my next video tutorial thanks for watching