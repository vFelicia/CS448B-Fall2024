In this course, I will show you how to identify and fix WordPress performance issues for good. And much of what I teach about performance applies to nonWordPress websites as well. So there will be something for everyone. My name is Beau Carnes. I've created popular courses about many topics including WordPress, Python, JavaScript, and more. Now I'm gonna teach you how to improve the performance of your website. InMotion Hosting provided a grant to make this course possible. They provide secure and reliable web hosting, but you don't need to use their hosting to implement the tips I share in this course. InMotion Hosting has also made a tier three VPS with WordPress installed available as a free one month trial for anyone following along with this course at home. Here are the top steps you can take right now to get immediate results. We'll be covering all of these in this course. Setting your performance goals, understanding a website request, DNS configuration and CDNs, server configuration and tuning for WordPress, caching techniques, and optimizing the WordPress application. Monitoring and adjusting website performance is a critical factor in the success of any website and optimizing your website for speed should be a top priority. This course is an invaluable resource for anyone looking to optimize their website's performance and gain a competitive edge in the online marketplace. By following the practical advice and best practices outlined in this course, you can improve your website's performance and provide your users with a fast, efficient, and enjoyable online experience. First, let's talk about setting your performance goals. Determining goals for optimizing website speed and performance is critical to achieving desired outcomes. Here are some steps to help you establish clear goals and objectives. Define your purpose. Start by understanding why you want to optimize your site's speed and performance. Is it to enhance user experience, reduce bounce rates, improve conversion rates, or achieve better search engine rankings? Clarifying your purpose will guide your goal setting process. Analyze current performance. Use tools like Google Lighthouse, Google PageSpeed Insights, Google Search Console, or Web Page Test to evaluate your website's current speed and performance. Identify areas where improvements are needed, such as slow loading times, excessive page weight, or high server response times. These insights will help you prioritize and set specific goals. Prioritize metrics. Determine which performance metrics are most important for your website. Common metrics include page load time, time to first byte, render start time, and interactive time. Line these metrics with your purpose to identify the most critical areas for improvement. Set measurable targets. Establish specific, measurable, achievable, relevant, and timebound goals, or smart goals. For example, set a goal to reduce page load time by 30% within three months, or achieve a TTFB of under 20 milliseconds. Clear targets will provide a benchmark for progress and help you stay focused. Consider user expectations. Understand your target audience's expectations and browsing behavior. Different websites may require different levels of optimization based on their user base. Consider factors like device usage, mobile versus desktop, geographic location, and connection speeds to tailor your goals accordingly. Break it down. Divide your optimization goals into smaller, actionable tasks. For instance, optimize image sizes, minifying CSS and JavaScript, leveraging browser caching, or implementing a content delivery network. Breaking down goals into manageable steps makes them more achievable and allows for incremental improvements. Monitor and iterate. Regularly monitor your website's performance using performance monitoring tools or website analytics. Track progress toward your goals and identify areas that need further optimization. Continuously iterate your strategies to adapt to changing technologies and user expectations. By following these steps, you can establish welldefined goals for optimizing your website's speed and performance. Remember, the goals should be aligned with your website's purpose, be measurable and adaptable to evolving needs. Before I talk more about how to optimize websites, let's look at an unoptimized site. We'll see how the site performs now, and throughout the course, we'll optimize the site. Then we'll see how it performs at the end. So here's a site that's already set up on InMotion Central. This is a WordPress site, and here's actually the preview of the site, the Sprout Invoices. So this is the settings you get when you have your WordPress site hosted on InMotion Central. You have the preview of the site. You get your site stats, the URL. You can also manage the server from here, and you can do stuff for the plugin themes, and we have a whole section of optimization that we'll be talking more about later. And you can also access the WordPress admin page right from here. So the website is going to look like this. And now let's check out the performance. First of all, I've turned off this plugin, so it's not gonna have any just normal optimization, and we'll talk more about this plugin later. Now I'm at the PageSpeed Insights website, the pagespeed.web.dev. This is from Google, and it helps you check the performance of your website. So I'm just gonna get my URL here, and analyze. So if we scroll down, we can see the performance is only in the 47th percentile. Though we do have some better scores in these other areas, 100th percentile accessibility. So if we scroll down even further, we can see what were the bad things. So anything red is something that's a low performance. First Contentful Paint, Largest Contentful Paint, Total Blocking Time, Speed Index, and we'll actually be talking about all of these things a little more later. So if you scroll down, we can get more details, and it's showing a lot of things that could be improved. These are all the different opportunities, and it's showing how we can improve the site. And so this is kind of a good starting point for any website, you just go to this PageSpeed Insights, and it will give you some ideas on how to improve the site. Now throughout the rest of this course, we'll be going through a lot of these things in more detail. The mobile score is a little different than the desktop score, because it's emulating a mobile device and the network speeds. So on desktop, the performance is a little better. We're at 79, still not in the green range, which is what we would want. But that is a little better, but you want to design all of your websites. So they have good performance, both on desktop and mobile. But again, you can see all these things that you can improve on. Now let's talk about performance testing tools. There are many testing tools available online to gauge your website performance. In this section, we want to get a website to rank better on Google, so we will use Google's tools to achieve our goals. There are also some other pretty awesome optimization tools, such as GTmetrics, Pingdom, and many others. Before we explain how to assess your site prior to optimizing, let's go over a quick outline of how the different pieces are weighted in the Lighthouse tests. The Lighthouse score calculator gives us information at a glance and even allows you to plug in your own scores to understand how your overall page speed score is calculated. Inside Chrome's developer tools, or dev tools, you can use the code inspector to get to the open source extension for Google Lighthouse. It's helpful because it runs tests right within your browser, and it generates a report for you on how well the website you are looking at stacks up for both mobile and desktop versions. This test gives you information about performance, accessibility, best practices, and SEO. And here's a tip. When performing a test with dev tools, you want to use an incognito window or a guest account, as browser extensions typically interfere with the test and skew the results. PageSpeed Insights is an online tool that tests your site and then delivers a score for both mobile devices and desktop computers. You'll see a score and a breakdown of the metrics used to determine your site performance. PSI, that's PageSpeed Insights, will also provide a breakdown of the issues, causing any slowdowns to the performance of your site. Recommendations are provided on actions you can take to improve the performance. PSI uses simulations and realworld data when available to provide its scores. The Chrome User Experience Report is also called the Core Web Vitals metric. This is an aggregate of user experience metrics that is compiled from actual users that visit your website over the last 28 days. The data is not comprehensive, however. These users are limited to people that have opted to sync their browser history with Chrome, and the data is made available by PageSpeed Insights, Google's BigQuery project, and Data Studio. While it's helpful information when optimizing your website for speeding performance, it's not representative of everyone's experience with your site. The Page Experience Report in Google's Search Console provides you with a summary of their measurement for your visitor's user experience. Google evaluates these metrics for individual URLs on your website and will use them as a ranking signal in search results. This report covers Core Web Vitals, mobile usability, and HTTPS usage for every page that you have in Google's index. In order for a page to be counted in this report's data, it must have data in the Core Web Vitals report simultaneously. We should mention other tools as well because this is by no means an exhaustive list. In short, there are a ton of professional tools that you can use to measure your website's performance, such as GTmetrics, Pingdom, Web Page Test, and more. Any tool you use will guide you with things that need to be done to improve your website's speed. There are many factors that will impact the score you see on these tests. These variables can range from your location related to the website, activity on your server, network routes, and even the activity on your local computer. Because of these and many other factors, no two tests will show the exact same results. However, with a little persistence, you can minimize the impact that your server and website bring into the equation. Simply follow these suggestions, learn about them, and implement best practices. This will increase the likelihood that your visitors will have a good user experience. When testing your website's performance and putting together your plan of attack, the tool you use will dictate the metrics used in the audit. Here are the top performance indicators to pay attention to. I recommend focusing on achieving passing green scores on Core Web Vitals before you move on to the other performance metrics. So you can just check out this chart that we have on the screen now. The key to success for your website is to focus on the user experience. Google's Core Web Vitals are a set of metrics that measure realworld user experience for loading performance, interactivity, and visual stability of the page. These metrics are designed to help you understand how good the user experience is on your site compared to other sites on the web. Core Web Vitals are broken down into a subset of page experience signals that apply to all websites and should be measured by website owners. The metrics that make up the Core Web Vitals are subject to evolve over time as the signals are not set in stone and are intended to change as time passes to continue improving the overall user experience on the web. Page experience signals and Core Web Vitals are the best signals that developers have to measure and improve the quality of the user experience and they're broken down into largest contentful paint, first input delay, and cumulative layout shift. You can outline what each signal is and share some best practices and steps to take to ensure you improve your website's performance in relation to each metric. Tools that measure Core Web Vitals consider a page to be passing if it meets the recommended target for of 75th percentile for all three metrics. Largest Contentful Paint measures your website's loading performance in order to provide a good user experience for your visitors, or LCP, that's Largest Contentful Paint, should occur within 2.5 seconds of the initial page loading. The types of elements that are considered for Largest Contentful Paint are usually images or videos. It is important to understand that an element can only be considered the LCP once it is rendered and the user can see it. This metric is also tied to the first Contentful Paint which measures how long it takes for the initial DOM content to render. But First Contentful Paint does not account for how long it takes the largest and usually more important content on the page to render. So Largest Contentful Paint represents a user's perception of loading experience. And the Lighthouse performance score weighting is 25%. What it measures? The point in the page load time when the page's largest image or text block is visible within the viewpoint. How it's measured? Lighthouse extracts LCP data from Chrome's tracing tool. And is Largest Contentful Paint a Web Core Vital? Yes. So what elements are part of the Largest Contentful Paint? Well, image elements, image elements inside an SVG element, video elements, an element with a background image loaded via the URL function, block line elements containing text nodes or other inline level text elements children. And here's how to define the LCP using Chrome DevTools. First open the page in Chrome, then navigate to the performance panel of the DevTools. You can use command option I on Mac or a control shift I on Windows and Linux. Then you hover over the LCP marker in the timing section. And finally, the elements that correspond to LCP is detailed in the related node field. So what causes poor LCP? Poor LCP typically comes from four issues, slow server response time, render blocking JavaScript and CSS, resource load times and client side rendering. Now let's talk about how you can improve LCP. The usual suspects of a poor LCP can be caused by many things. Some of the causes can be slow response times from the server, render blocking JavaScript and CSS, slow resource load times and even client rendering. Slow server response times is the first thing you should tackle when optimizing for performance. Faster server response will typically impact and improve every page loading measurement, including LCP. If the cause is slow server response time, you need to optimize your server, route users to a nearby CDN, cache assets, survey HTML page, cache first, and establish third party connections early. If the cause is render blocking JavaScript and CSS, you want to minify the CSS, defer noncritical CSS, inline critical CSS, minify and compress JavaScript files, defer unused JavaScript, and minimize unused polyfills. If the cause is resource load times, you want to optimize and compress images, preload important resources, compress textiles, deliver different assets based on the network connection, and cache assets using a service worker. Finally, if the cause is clientside rendering, you want to minimize critical JavaScript or use another rendering strategy. Okay, first input delay, or FID, measures a website's interactivity and focus on input events from discrete actions like clicks, taps, and key presses. When a website does not respond in a certain amount of time after a user interaction, users experience this as a lag or slowdown. To provide a good user experience, your web pages should have a FID of 100 milliseconds or less. So what this represents is your user's first impression of your site's interactivity and responsiveness. It doesn't apply to the Lighthouse performance score. What it measures is the time when a user first interacts with your site, when they click a link, tap a button, or use a custom JavaScript power control to the time when the browser is actually able to respond to that interaction. How is measured? It requires a real user and thus cannot be measured in the lab. However, the total blocking time metric is lab measurable and can be used as a proxy as it correlates well with FID in the field and also captures issues that affect interactivity. And is this a web core vinyl? Yes. So what causes poor FID? The main cause is heavy JavaScript execution. Optimizing how JavaScript parses, compiles, and executes on your web page will directly reduce FID. How to improve it? Well, you can reduce the impact of thirdparty script execution, reduce JavaScript execution time, like by deferring unused JavaScript or minimizing unused polyfills, can break up long tasks, optimize your page for interaction readiness, use a web worker, minimize main thread work, or finally keep request counts low and transfer sizes small. Also, you should note that FID is being replaced by INP in March of 2024. INP is interaction to next paint. Cumulative layout shift or CLS measures visual stability. A good user experience is measured as having a CLS of 0.1 or less. This allows the experience to avoid any surprises for the user, but isn't particularly helpful for page speed. It's more measurement that your website is following best practices and is a necessary piece for achieving a high performance score for your page. These best practices including following common UX patterns that are optimized for CLS. When your page is loading, you should always be sure to set size attributes or reserve space. Specifically, specify the width or height attributes to create a placeholder for a content. This will help ensure your page does not jitter and move for the customer while they're trying to interact with it. Your website should never insert content above existing content unless a user interacts and expects the behavior. And you should always use transformation animations when transforming the layout during said interactions. So what it represents? The visual stability of a page. Instability is defined as unexpected movement or page content as the user is trying to interact with the page. And it's 15% of the Lighthouse performance score weighting. What it measures? It measures the largest burst of layout shift scores for every unexpected layout shift that occurs during the entire lifespan of a page. A layout shift occurs any time a visual element changes its position from one rendered frame to the next. How it's measured? To calculate the layout shift score, the browser looks at the viewport size and the movement of unstable elements in the viewport between two rendered frames. The layout shift score is a product of two measures of that movement. The impact fraction and the distance fraction. And is this a core, a web core value? Yes. So let's look at how to see CLS using Chrome DevTools. Use DevTools to help debug layout shifts by leveraging the experience pane under the performance panel. The summary view for a layout shift record includes the cumulative layout shift score as well as a rectangle overlay showing the affected regions. What causes poor layout shifts? Changes to the position of a DOM element. This is often the result of style sheets that are loaded late or overwrite previously declared style or delays in animation and transition effects. Also changes to the dimension of a DOM element. This is often the result of images, ads, embeds, and iframes without dimensions. Or insertion of or removal of a DOM element. Another cause is animations that trigger layout. This is often the result of insertion of ads and other third party embeds, insertion of banners, alerts, and modals, infinite scroll and other UX patterns that load additional content above existing content. Or finally, another cause is actions waiting for a network response before updating the DOM. And how you can improve it? Always set attributes to images and video elements or allocate the correct amount of space for that element while it's loading. Also, never insert content above existing content except during an intentional user interaction. And prefer transform animations to animations of properties that trigger layout changes. Finally, preload fonts using the link rel preload on the key web fonts and combine link rel preload with font display optional. Another performance opportunity is first contentful paint. This measures the time to render the first piece of the DOM content. It represents the amount of time it takes the browser to render the first piece of DOM content after a user navigates to your page. Images, nonwhite canvas elements, and SVGs on your page are considered DOM element, DOM content. This is 10% of the Lighthouse performance score. What it measures? This is a user centric metric for measuring perceived load speed because it marks the first point in the page load timeline where the user can see anything on the screen. How it's measured? The score is a comparison of your page's first contentful paint time and the first contentful paint times for real websites based on data from the HTTP archive. It is not a web core vital. Here are some ways to improve the first contentful paint. Eliminate render blocking resources. Minify CSS. Remove unused CSS. Preconnect to required origins. Reduce server response times. Avoid multiple page redirects. Preload key requests. Avoid enormous network payloads. Serve static assets with an efficient cache policy. Avoid an excessive DOM size. Minimize critical request depth. Insert text remains visible during web font load. Keep request counts low and transfer sizes small. Another thing to think about is the speed index, the SI. This represents the average time at which visible parts of the page are displayed. It's 10% of the Lighthouse performance score weighting. What it measures? The speed index measures how quickly content is visually displayed during page load. Lighthouse first captures a video of the page loading in the browser and computes the visual progression between frames. How it's measured? The speed index score is a comparison of your page's speed index and the speed indices of real websites based on data from the HTTP archive. It is not a web core vital. Here's some things you can do to improve speed index. Minimize main thread work. Reduce JavaScript execution time. Ensure text remains visible during web font load. Eliminate render blocking resources. Another thing to think about is the total blocking time, or TBT. This represents the total amount of time that a page is blocked from responding to user input, such as mouse clicks, screen taps, or keyboard presses. It's 30% of the Lighthouse performance score weighting. What it measures? This measures the time between first contentful paint and time to interactive. It is the lab equivalent of first input delay, the field data used in the Chrome user experience report and Google's page experience ranking signal. How it's measured? The total time in which the main thread is occupied by tasks taking more than 50 milliseconds to complete. If a task takes 80 milliseconds to run, 30 milliseconds of that time will be counted toward the TBT, the total blocking time. If a task takes 45 milliseconds to run, 0 milliseconds will be added. Is total blocking time a web core vital? Yes. It's the data, it's the lab data equivalent of first input delay. Total blocking time measures long tasks, those tasks taking longer than 50 milliseconds. When a browser loads your site, there is essentially a single line queue of scripts waiting to be executed. Any input from the user has to go into that same queue. When the browser can't respond to user input because other tasks are executing, the user perceives this as lag. Essentially, long tasks are like that person at your favorite coffee shop who takes far too long to order a drink. Like someone ordering a 2% venti, four pump vanilla, five pump mocha, whole fat froth, long tasks are a major source of bad experiences. Some things that cause a TBT on your page are heavy JavaScript and heavy CSS. And that's basically it. Here's how to see TBT using Chrome DevTools. And how to improve TBT, you can break up long tasks, optimize your page for interaction readiness, use web worker, and reduce JavaScript execution time. Now let's talk about time to first byte or TTFB. This is a foundational metric for measuring the time between the requests for a resource and when the first byte of a response begins to arrive. It helps identify when a web server is too slow to respond to requests. TTFB is the sum of the following request phases. Redirect time, server worker startup time, DNS lookup, connection TLS negotiation, and request up to the point at which the first byte of the response has arrived. So here are some ways to improve TTFB. Hosting web servers with inadequate infrastructure to handle high traffic loads, web servers with insufficient memory that can lead to thrashing, unoptimized database tablets, suboptimal database server configuration, avoid multiple page requests, preconnect to required origins for crossorigin resources, submit your origin to the HSTS preload list to eliminate HTTP to HTTPS redirect latency, use HTTP2 or HTTP slash 3, and use serverside generation for markup instead of SSR where possible and appropriate. Minimizing TTFB starts with choosing a suitable hosting provider with infrastructure to ensure high uptime and responsiveness. This, in combination with a CDN, can help significantly. In this section, we'll discuss how the speed of a request to a website impacts overall website performance. When a user makes a request to load a website, a series of processes happen behind the scenes to deliver the requested content to the user's browser. This process can be broken down into the following steps. Step 1, DNS lookup. The domain name system is a distributed database that maps domain names to IP addresses. When a user types a URL into the browser, the browser first sends a request to a DNS server to look up the IP address of the domain name. Step 2, connection. Once the browser has the IP address of the website, it establishes a connection to the server hosting the website using the Hypertext Transfer Protocol, or HTTP. Or the more secure HTTPS protocol, the S stands for secure. Step 3, request. After establishing the connection, the browser sends an HTTP request to the server requesting the specific webpage or resource that the user wants to access. Step 4, processing. The server receives a request and begins to process it. This may involve executing scripts, querying databases, and retrieving files from the file system. Step 5, response. Once the server has processed the request, it sends an HTTP response back to the browser. This response includes the requested content, such as the HTML, CSS, and JavaScript, and any images or other resources required to display the website. Step 6, rendering. The browser receives the response from the server and begins to render the webpage. This involves parsing the HTML and rendering the layout, styling the page with CSS, executing JavaScript, and loading any images or other resources required to display the page. Step 7, user interaction. Finally, the user can interact with the webpage, clicking on links, filling out forms, and performing other actions. These steps happen very quickly, often in a matter of milliseconds, but any delay or bottleneck in any of these steps can cause the page to load slowly or not at all, leading to a poor user experience. To optimize website performance, it's important to identify and address any performance issues at each stage of the process. Some of the WordPress optimizations have to happen right on the server. So let me show you how to manage your server in Inmost and Central. Now, there will be a similar process, even if you're using another hosting provider. I'll go to Tools, then Manage Server. And then we can see the host name, the IP address, and then we can see that it's currently running. Now, you can see we can use this to restart or stop the server, but what we want to do is add our SSH key. That's going to allow us to connect to our server from our local machine. SSH keys are used to securely authenticate with the server so we can connect via the command line interface. This is usually the preferred method for advanced users to manage their servers, since you can quickly make changes without having to navigate through a graphical user interface. So to get our SSH key, we're going to have to generate one. So you can open a terminal on your local computer, and then we can type in ssh slash keygen or dash. Okay, then press Enter, and it's going to ask us to enter the file, where to save the key, and we'll just press Enter again. And mine already exists, so I'm not going to override it, but if you haven't created it before, that's what it would show you. And now we can get it by a copy where the key is, and then I'll do cat, and then just put the location.pub. And then we can see the whole key here. And then I can just copy this key. So I'll go to add SSH key, and then I will just paste in my public key, and then update. And now we will be able to connect to the server via SSH. So I need my IP address. So I'll just copy this IP address, or actually I'll just copy the host name here. Now we'll go back to a terminal SSH root at, and then the server name, or you could also use the IP address, and then we'll just hit enter here. And here we want to put yes, we are sure we want to continue. Okay, we're logged into the host, we're logged into the server. These tips explain what measures can be taken with Redis, MySQL, Opcache, and other technologies to achieve a high performance server. There are many opportunities to improve performance. I'll focus on fast storage and emphasize tuning the cache. Accessing files from NVMe storage, storing PHP scripts and Opcache, and compressing responses with Brotly are all part of a high performance server. I'm going to show you how to analyze your server performance and make incremental improvements. Services like Redis and Memcache can significantly improve WordPress performance by caching data in memory and reducing the number of database calls required to serve content. They act as key value stores that temporarily hold frequently accessed or computationally expensive data, resulting in faster response times and reduced server load. WordPress can be configured to use Redis as an object store by employing specific plugins, such as Redis Object Cache or W3 Total Cache. So once Redis is installed and configured, you can check the following items to ensure Redis is optimized and operating its full capacity. Right now I'm connected to my server through InMotion, the server that has the WordPress installations installed and already has Redis installed. So I can check the Redis memory usage with this command, Redis CLI info memory. And then we get all this information. So if I go scroll up a little bit, when the used memory peak is close to the max memory that's shown further down, Redis may not have enough resources to operate to its full capacity. In addition, Redis may be actually evicting keys from the cache to make room for new ones. This is not ideal and key evictions should be avoided if possible. So you can increase the max memory variable in your Redis configuration to solve the issue. And here are where the configuration files are. If you make any changes, make sure to restart everything to apply your changes. Key eviction. Key eviction is the process of removing old and unused data from the cache to make room for new data. This can be especially problematic for high traffic websites that rely on Redis caching to improve performance. If your system resources allows for it and the evicted keys count is high, it's again recommended to increase max memory. The following command will provide information about the number of keys and how many have been evicted. By examining these aspects and making the appropriate adjustments, you can optimize Redis to significantly enhance your WordPress site's performance. Optimize the database. If object caching like Redis is not available for the WordPress site, requests for database content go straight to the database. First, the database checks its cache to serve the results directly, if possible, and stores it for future use. If caching is unavailable or not allowed, the query proceeds to the database. The database combines cached and uncached information, requesting data from the operating system. The speed of retrieving the data depends on available RAM and the storage type, with NVMe being the fastest and HDD being the slowest. Simple queries return data immediately to the PHP call, while complex queries requiring temp tables demand more processing time and can slow down the website or cause server issues under high traffic. Several tuning techniques can improve performance. You can set the buffer pool size. The buffer pool is the area of memory that MariaDB uses to cache data and indexes. It's important to configure the buffer pool size appropriately based on the amount of available memory and the size of the database. The buffer pool size can be controlled using the nodb underscore buffer underscore pool underscore size variable. You can start with 128 or 256 megabytes, depending on the total available RAM on your server. Check the hit rate and adjust accordingly to ensure it's working as expected. You can see here where the configuration files are, and always make sure to restart your services to apply any of your changes after you configure this. One way to analyze buffer pool performance is by observing the nodb underscore buffer underscore pool underscore weight underscore free status variable. If it's decreasing, then you don't have enough buffer pool, or your or your flushing isn't occurring frequently enough. In this case, you should set the nodb underscore buffer underscore pool underscore size variable higher if your system resources allows for it. You can also enable query cache. Query cache is a feature that stores the results of frequently executed queries and memory, which can significantly improve query performance. It's recommended to enable query cache, but it's important to note that it may not be effective for all workloads. The query cache can be enabled or disabled using the query underscore cache underscore type variable. The SQL server store stats about the query cache. Query cache is actually deprecated in MySQL, but it still exists in MariaDB. Query cache has taken a less desirable place in most MySQL setups, because of the high adoption of multi core and multi server deployments, where it can be a bottleneck. For Mario DB, it's still useful for websites and fits well in our use case, where the deployment consists of a single site on the server. Q cache underscore inserts contains the number of queries added to the query cache. And Q cache underscore hits contains the number of queries that have made use of the query cache, while Q cache underscore low mem underscore prunes contains the number of queries that were dropped from the cache due to lack of memory. The example could indicate a poorly performing cache. More queries have been added and more queries have been dropped than have actually been used. You can set in memory table size limits. As previously mentioned, complex queries will cause the SQL server to create temporary tables. The SQL server will attempt to create tables in memory up to a certain size for performance. It's important to set the max size to an appropriate value to avoid creating on disk temporary tables. The variables that control this behavior are the tmp underscore table underscore size and max underscore heap underscore table underscore size. You can compare the number of internal on disk temporary tables created to the total number of internal temporary tables created by comparing the created underscore tmp underscore disk underscore tables and created underscore tmp underscore tables values. You can see if it's necessary to increase by comparing the status variables created underscore tmp underscore disk underscore tables and created underscore tmp underscore tables to see how many temporary tables out of the total created needed to be converted to disk. In this example, the SQL server is not having to create temporary tables on disk. This is considered optimal. Another thing you can do is to prune the WordPress database. Without intervention, WordPress will store junk and unused data in the database. Over time, this can add up causing database bloat and serious performance lags. This includes items such as post revisions, which includes autosaved drafts, trashed posts, trashed comments, comment spam, and tables for plugins and themes that are no longer active or installed, and transient records. In your wpconfig.php file, add or edit the following setting to limit the number of post revisions saved to the database. Many resources exist on the internet for dealing with these items. Monitor performance. It's important to regularly monitor the performance of Mario DB and adjust the configurations as necessary. This includes monitoring CPU usage, memory usage, disk IO, and query performance. Use a tool like MySQL Tuner to gain insights on the performance of the SQL installation, or enable slow query logging to identify longrunning queries. Opcache provides significant benefits when used with PHP FPM. When a PHP file has been previously requested and there's enough opcache available, the script is already stored in memory. This allows for faster execution as it stands ready to accept data inputs without having to reload the file from disk. It's important to maximize opcache hits and minimize misses. And here are where the configuration files are. If you make any changes, make sure to restart everything to apply your changes. For opcache, WordPress plugins are available that are restricted by auth. Standalone opcache dashboard pages need additional security, but WordPress plugins are behind authentication already. PHP provides several builtin functions to obtain information about opcache status and configuration, like opcache underscore get underscore status and opcache underscore get underscore configuration. Additionally, there are several webbased monitoring tools to visualize opcache usage and performance. Some popular options include opcache GUI, which is a single file PHP script that provides a simple and clean interface to monitor opcache usage and performance. And then there's OCP, which is a webbased opcache control panel that allows you to monitor and manage opcache settings in real time. Opcache status, which is a onepage opcache status page for the PHP 5.5 opcode cache, and cache tool, which allows you to work with APCU opcache and the file status cache through the CLI. It will connect to a fast CGI server like PHP FPM and operate on its cache. When reviewing opcache statistics, there are a few key items to look for and assess. Opcache hit rate. A hit rate is the percentage of requests that are being served using cached opcode instead of parsing and compiling the scripts anew. This hit rate suggests opcache requires more memory or resources to perform its full capacity. A hit rate close to 100 indicates that the cache is functioning effectively and providing the desired performance benefits. Monitoring the hit rate can help identify if adjustments to opcache settings are necessary to improve its efficiency. The next section will explain how to adjust the memory allocation for opcache. Memory consumption. Analyzing memory usage involves checking the amount of memory allocated to the opcache and how much of it is being used. In the output you can see here, the administrator would want to consider increasing the opcache.memory underscore consumption to a value higher than 128 megabytes as the cache full boolean is true and free memory is at 8 bytes. This information helps determine if there is a need to increase or decrease the memory allocation based on the size and complexity of your PHP scripts. Ensuring an appropriate memory allocation helps prevent cache evictions and promotes a more efficient use of server resources. Both numcache keys and maxcache keys are metrics that provide insights into the number of cache keys compiled PHP scripts currently stored in the opcache, the numcache keys and the maximum number of cache keys allowed, the maxcache keys. By comparing these values you can identify if the cache is nearing its capacity or if adjustments are needed to optimize the cache size. If numcache keys frequently approaches maxcache keys, it can be beneficial to increase the opcache.maxacceleratedfiles value to accommodate more scripts reducing cache evictions and further improving performance. Regularly reviewing these key items can help you optimize the opcache configuration leading to improved website performance and reduced server resource usage. As previously mentioned in the database optimization section, NVMe storage helps database server performance by providing significantly faster data access and reduced latency compared to traditional storage solutions. This results in quicker response times and more efficient query processing leading to improved overall server performance. Disk I.O. helps increase performance of all aspects of your web server and having NVMe speeds usually means that the database caching in W3.12 cache plugin is not needed and it can have adverse effects to performance if enabled. NVMe storage improves server performance for WordPress websites in several ways. Faster page load times. With its high speed data transfer and load latency, NVMe storage allows for quicker access to website files and assets such as images, style sheets, and scripts. This leads to faster page load times and a better user experience. Improved database performance. The benefits of using NVMe storage when processing database queries can be outlined as follows. When processing queries, the database relies on both cached and uncached MySQL data requiring interactions with the operating system to retrieve the necessary information. If the operating system has ample RAM, the files needed for a query might already be stored in RAM rather than only being available on a persistent storage device. Finally, in cases where data must be accessed from a storage device, an NVMe drive provides significantly faster data retrieval compared to a SATA SSD while traditional HHDs are much slower and typically not used in such scenarios anymore. Overall, the NVMe storage greatly enhances database query performance due to its rapid data access capabilities. Compressing data server responses with Brotli or gzip can significantly improve website performance by reducing the amount of data transferred over the network. This provides a final stage of optimization on the response being returned from the origin server. On average, gzip compression can reduce the size of web content by 60 to 70 percent while Brotli tends to provide even better compression rates often reducing content size by 70 to 80 percent. Enabling Brotli compression within Nginx is accomplished through the use of the Brotli Nginx module. The module can be compiled and installed alongside Nginx and loaded dynamically. Analyze a server response for compression. To determine if a request response was compressed with Brotli or gzip compression you can use Chrome or Firefox developer tools to review the content encoding headers. So you can follow these steps. Open the developer tools in your browser. Click on the Network tab in the developer tools. Refresh the webpage or initiate the action you want to analyze. This will populate the network panel with a list of requests made by the browser. Then we'll find the requests we're interested in such as the main document request or a specific resource like a CSS or a JavaScript file. We'll click on the request to open the detailed view. In the detailed view we'll look for the Headers tab. This should be the default tab in both browsers. Scroll down to the Response Headers section. Then we'll check the value of the content encoding header. If this response is compressed with gzip it will display gzip and if it's compressed with Brotli it will display br. If the header is absent or has a different value the response is not compressed using either of these algorithms. By examining the content encoding header in the developer tools you can quickly determine if a request response was compressed with Brotli or gzip. Serving requests from the NGINX CAS can result in significant performance improvements as it eliminates the need for the request to traverse the entire stack and access the backend services. This can lead to reduced processing times and improved response times resulting in a better user experience. According to benchmarks serving requests from the NGINX CAS can result in a 90% reduction in response times compared to requests that traverse the entire stack. This is due to the cache mechanism's ability to serve requests directly from memory or disk reducing the time taken to process the request. Furthermore serving requests from the NGINX CAS can significantly reduce server load and improve scalability. By reducing the number of requests that must be handled by backend services like PHP, FPM or MySQL the server can handle more requests concurrently leading to improved performance and faster response times. This is essential for a hightraffic website. The UltraStack NGINX configuration adds an xproxy cache HTTP header to responses and the upstream cache status to NGINX access log entries. The following three techniques explain how to identify whether the page being received is a cache version served by NGINX. We can use curl. We can make a simple get request to the URL in question and use grep to check the xproxy cache header. Also we can use chrome dev tools. You can use F12 or the other shortcut to bring up the dev tools window then click the network tab. Pick the box that says disable cache which will bypass chrome's internal cache then reload the page to capture all of the network information. Not disabling chrome's cache will allow you to see 304 not modified responses for static assets which shows how the client side cache is working. The following are the possible values for upstream cache status. Miss the response was not found in the cache and so was fetched from an origin server. The response might then have been cached. Bypass the response was fetched from the origin server fetched from the cache because the response matched a proxy cache bypass directive. Expired. The entry in the cache has expired. The response contains fresh content from the origin server. stale. The content is stale because the origin server is not responding correctly and proxy cache use stale was configured. Updating. The content is stale because the entry is currently being updated in response to a previous request and proxy cache use stale updating is configured. Revalidated. The proxy cache revalidate directive was enabled and nginx verified that the current cache content was still valid. Hit. The response contains valid, fresh content direct from the cache. Another technique to explain how to identify whether the page being received is the cache version served by nginx is using the nginx access log. You connect to the server using SSH and read the access log with a tail or a cat. The value assigned to UCS or the upstream cache status will be one of the cache statuses we talked about before. Using central's managed server tools, here's how you would enable nginx proxy cache on your site. With nginx proxy cache configured you may use the ultra stack utilities to determine the cache hit rates for all requests to nginx. While this is a tool specific to inmotion hosting the information can be received using the third party nginx module VTS. So in summary, using nginx proxy cache will help drive performance for high traffic websites. While the exact amount depends on the specific workload of the website, a minimum of 4 GB and 2 vCPU cores is recommended when benchmarking your resource needs on a high traffic website. In a high performance server running MySQL, Redis, PHP, FPM, Apache, and nginx, having enough RAM is critical to allow each services caching techniques to work at their full potential. These services use RAM to cache frequently accessed data and reduce the time taken to retrieve it from disk or other storage devices. If the server does not have enough RAM, the caching will not be as effective, leading to longer processing times and slower website performance. For instance, MySQL uses the NODB buffer pool to cache frequently accessed data from the database, and Redis uses memory to store frequently accessed data in key value pairs. PHP FPM uses opcache to cache PHP scripts, and nginx uses RAM to cache frequently requested webpages and assets. If the server does not have sufficient RAM, these caching techniques will not be as effective, and the server may resort to swapping data in and out of slower storage devices, leading to a slower response times and reduced server capacity. Swap usage. To determine if a Linux server is swapping, you can use the free command to view the system's memory usage. If the swap value is greater than 0, it indicates that the server is currently using swap space to store data in memory. Additionally, you can use the vmstat command to monitor the amount of swapping activity occurring on the server, with the SI and SO values indicating the amount of memory being swapped in and out, respectively. This example does not use swap and is not swapping. OOM OOM kills. To determine if a Linux server is experiencing OOM, OOM or out of memory kills, you can check the system logs for messages related to OOM events. On most Linux distributions, the system log file is located at var slash log slash messages, or var slash log slash syslog. To view OOM related messages, you can search for the keyword OOM using the grep command. If OOM or OOM events are occurring, you'll see entries that indicate which process or application was killed by the OOM killer. Additionally, you can use tools like DMESG and journal control to view kernel messages related to OOM events. By having enough RAM on the server, each service can utilize caching techniques to their fullest extent, resulting in faster processing time, reduced disk usage, and an overall better user experience. There's a plugin that's super helpful when it comes to optimizing WordPress sites. Let me show you how to install it really easily with InMotion, but you can also install it basically with any WordPress installation. So we'll go to optimize and then WordPress. We have the W3 Total Cache. Now you can see that it improves SEO and the user experience of your site by increasing website performance, reducing load times via features like content delivery network integration and the latest best practices. So basically I'm just going to click here and W3 Total Cache is one of the most popular ways to optimize and manage the cache for your WordPress site. So this is going to result in faster load times and less strain on your server, which is definitely going to help with SEO. You can also enable this within the WordPress dashboard if you don't have InMotion Central. And I'll show you how to do that in a second. Right now I'm going to click on setup guide. And this is going to help you set up things, but we're going to configure on our own so I'll skip this setup guide. So before we do that, I'm going to show you how you would install this with just WordPress if you don't have the InMotion settings. So we just go to plugins and then add new and then you just search for W3 Total Cache. So here is where you would install it and activate it, but we already have installed and activated. So I'm going to go back to this performance tab here. That's the W3 Total Cache tab. And before I go through setting it up, let me just give you a little more context. So by default WordPress is a dynamic CMS. That means for every visitor request that WordPress has to process it must first connect to the database to see if the requested page even exists. In a lot of cases this might not be problematic on a site that just doesn't receive much traffic. However, a sudden surge in traffic caused by search engine bots or just a general increase in normal traffic can quickly cause your WordPress site to use up a lot of CPU resources from the server while trying to fulfill needless duplicate requests again and again. You can counter this increase in CPU usage by implementing a caching plugin. What these do is cast the first visitor's request of a new page to a plain HTML file on the server. Then when another visitor comes through and requests the same page, so long as the page wasn't updated in your admin section or updated by comment, the cached HTML page will be served. This can greatly reduce CPU usage of your WordPress site very easily. As an example, let's say you had 100 views of your front page. Without caching that would require the same database query to have to run 100 times and every time it's just getting back the exact same data anyway. With the caching plugin, only the first user would have the database query run to generate the cached HTML file. Then the next 99 visitors would get that cached HTML served to them right away without having to wait for any database activity to complete. This is basically always a winwin because your visitors don't have to wait as long for your pages to load and you're reducing the impact of WordPress's requests on the server's performance. Now I do want to note that database caching is not recommended for shared hosting as the process is dependent on the disk speed of the server. So due to the nature of shared system resources, disk speed may be limited. And so database caching may adversely affect the performance of the site. Really, what you want to do is experiment with different caching techniques to see which one works best for your site. So now let's see how to set up the w3 total caching plugin. I'm going to go to the general settings here. Okay, and then I'm going to make sure that the page cache is enabled. And then for all these settings, we're just going to want to click either save all settings, or save settings and purge caches after you update the settings. One thing to keep in mind is that you never want to use both nginx page caching and total cache, that would slow things down and would cause some conflicts with the caching. So you basically just want to use one or the other. Another good feature to use is the minify. You can see this is going to reduce the load time by decreasing the size and number of CSS and JavaScript files. So it's, it's going to make things load a lot quicker if you enable the minify. If you are enabling minify, you're going to want to make sure to check your WordPress site to make sure the site looks normal. If you see problems then you can disable the minification. And this is where you can use op cache to validate timestamps so it will request the cache with the latest version. And then you see the database cache. Caching, it caches the database objects and this decreases the response time of the site so it doesn't have to load from the database every time. We're not we have it disabled because it's best if object caching is not possible. By our case, we're going to use object caching. And you also have the browser cache and you can enable or disable a CDN. And when you're choosing a CDN, you're going to have to choose basically the CDN provider, which CDN you're going to use. I'm actually going to disable this because for a CDN you're going to have to sign up for a CDN account. And make sure not to click the CDN button if you don't have a CDN. That could actually mess some things up for your site. Okay, I'm going to save this. And then we'll go over to the page cache tab. So we're going to just check the settings on this page. We're going to we want to make sure we cast the front page, cast the feed, cast SSL. We don't want to cast the URIs. We want to cast the 404. Don't cast page for logged in users. Don't cast page for the following user roles, just the administrator role. So we're just going to use all these default settings. And this is just going to make sure we cast the page we want to cache and don't cast the pages we don't want to cache. So I'll just save settings and purge caches. So now I'm going to go to the website, I'll just go to visit site here. And I will go to view page source. And then if I go all the way down to the bottom, I should see something. Yep, performance optimized by w3 total cache. So now I know that it's you it's working correctly and it's installed correctly. So I'm back over on the total cache setting page. I want to talk about cleaning your WordPress cache. This is something you may need to do from time to time when using a caching plugin to speed up WordPress. If you make changes to your WordPress site and don't see the changes when visiting the updated page, you may need to clear your cache. This is because the caching plugin could still be showing the cached version of your page. So to force WordPress to show your most recent changes right away, you can delete the cache. So basically just go to the the main settings page and then we can click empty all caches just like that. All caches successfully emptied. Now let's talk about browser caching and some implementation techniques. e tags. The e tag HTTP response header is one that web servers and browsers use to determine whether a resource in the browser's local cache matches the one in the origin server. When a user revisits the site, one of two things happen. It displays content to the user in the browser if it's within the allowable cache refresh time, the time to live. If content is out of its cache time TTL expired, then if it has an e tag, then it will need to send an e tag to the resolved URL to see if it needs to refresh. Then if it gets the matching e tag, then it will keep the content it has no new downloading of content and set the existing content TTL into the future, which makes it a viable cached content for the TTL. And if it gets a different e tag, then it will trigger a fresh download of the new content. If it does not have an e tag, then it will send a request out to get the potentially new content and it will download that content regardless of whether it really is new or not. It's important to note that the e tag request and response is much, much smaller than a download of content. As most websites do not change between TTLs, the TTLs need to be low just in case the site does change. Therefore, the e tag process is much more efficient than TTLs by themselves. So how do you check if your server response is using e tags? Well, you open the developer tools in your browser using the shortcut key or you can right click on the web page and click inspect. Then click on the network tab in the developer tools. Next, refresh the web page or initiate the action you want to analyze. This will populate the network panel with a list of requests made by the browser. Next, locate a specific resource like a CSS or JavaScript file. Click on the request to open the detailed view. In the detailed view, look for the headers tab. Then scroll down to the response header section for the selected file and check for the presence of an e tag. How to configure e tags in W3 total cache. In your WordPress admin dashboard, navigate to performance browser cache. Scroll down to the general section. Check the box next to the set entity tag e tag setting. On the same page, using the same method, you can also enable or disable e tags for specific files types, including CSS and JS, HTML and XML media and other files. Finally, press the save settings and purge caches button within each respective setting box to save and apply the caching changes immediately. Time to live. The time to live value is the time that an object is stored in a caching system before it's deleted or refreshed. In the context of CDNs, TDL typically refers to the content caching, which is the process of storing a copy of your website resources on CDN proxies to improve paid load speed and reduce origin server bandwidth consumption. TDLs are used for caching purposes, and the value is stored in seconds. If the TDL value is set to a low number like 3600 seconds, the data will be stored in the cache for one hour. After one hour, a new version of the data needs to be retrieved. If the TDL value is set to a high number like 86,400 seconds, the data will be stored in the cache for 24 hours. High TDL values cache your data longer and lessen the frequency for retrieving new data. This gives your website a performance boost by reducing the server's load, which improves the user's experience. Let's look at how to set TTL policies in w3 total cache. You can make some limited changes to the TTL by going to page cache, and then you scroll all the way down to advanced. Here's the advanced section. And then we can update the garbage collection interval, which is how frequently the expired cache data is removed. And then also the comment cookie lifetime. This is going to reduce the default TTL for comment cookies and it's going to reduce the number of authenticated user traffic. Let's talk about the HTTP cache control headers. So here are the different ones. There's cache control max age. The max age directive specifies the maximum number of times that a resource may be cast by a client or a proxy server. After expiring, a browser must refresh its version of the resource by sending another request to a server. Cache control max age 3600 means that the returned resource is valid for one hour after which the browser has to request a new version. Cache control no cache. The no cache directive tells a browser or a caching server not to use the cache version of a resource and to request a new version of the resource from the origin server. Cache control no store. The no store directive tells browsers to never store a copy of the resource and not to keep it in the cache. This setting is usually used for sensitive information like credit card numbers and ensures PCI compliance by not storing this type of data in the cache. Cache control public. The public response directive indicates that a resource can be cached by any cache. This directive is useful for resources that change infrequently and when the resource is intended for public consumption. Cache control private. The private response directive indicates that a resource is userspecific. It can still be cached but only on a client device. For example a web page response marked as private can be cached by a browser but not a content delivery network. Now let's see how to set HTTP cache control headers in W3 total cache. So to set up the cache control headers in W3 total cache you go to the browser cache performance browser cache and then we are going to determine what are the cache control policies will be for some different files. So I can scroll down to the CSS and JavaScript section and then we'll set the cache control header. Now we'll select the cache control policy and I'll just select this one cache with max age. And then you can do the same thing for HTML and XML. We can set the cache control header, choose the policy, and then for the media and other files we can set the cache control header and set the policy. And then you always want to make sure to save changes and purge caches. Cache Busting cache busting is a technique used to force a browser or caching service to retrieve a new version of a resource rather than using the cached version. One way to do this is by using query parameters such as a timestamp or a version number in the URL of the resource. For example, instead of requesting resource at the URL example dot com slash style dot CSS a cache busting URL would look like example dot com slash style dot CSS and then you get the version one two three four where V equals one two three four is the query parameter. This query parameter can be a timestamp or a version number and will be ignored by the server but it will force the browser or caching server to treat the request as a new request rather than using the cached version of the resource. This technique is often used for static resources like like styleship scripts or images that are intended to be cached but they may not be needed to be updated frequently. By appending a query parameter to the URL a new version of the resource will be retrieved each time the query parameter changes. This technique is commonly used to ensure that the client gets the latest version of the resources after they're updated on the server side. To enable cache busting in WCTurtle cache we'll go to the performance and the browser cache section and then we just have to go to prevent caching of objects after settings change. This will add a random string at the end of static asset URLs. Cache storage is a finite resource so every bit of storage performance matters. Caching at proxy servers is one of the ways to reduce the response time perceived by web users. You have to choose between ease of use versus complicated configurations versus your needs. You have to look at the CDN level edge caching the server nginx proxy cache, the WordPress plugins application level caching. What do you have now? What's your budget? CDNs can be expensive. Where's your audience in relation to your server? What do we want to guide the user to do? Cache replacement algorithms play a central role in reducing response times by selecting specific websites assets for caching so that a given performance metric is maximized. They do this by deciding which objects can stay and which objects should be evicted the next time the website is accessed. The different policies are least recently used. This replaces the cache line that has been in the cache longest with no references to it. Then first in first out replaces the cache line that has been in the cache longest. Least frequently used replaces the cache line that has experienced the fewest references or random. Pick a line at random from the candidate lines. Page caching refers to caching the content of a whole page on the server side. Later when the same page is requested again, its content will be served from the cache instead of regenerating it from scratch. A page from a WordPress website contains dynamic content. PHP scripts, JavaScript, and SQL queries. Executing this dynamic content is very resource heavy and takes a lot of time. Page caching allows for forming a part of the website into static HTML. With page cache enabled, website content displays faster for a visitor with less load on the server. It's one of the most efficient ways to improve your website performance. Object caching is the process that involves storing database queries to serve a specific piece of data on the subsequent server request. As a result, there will be fewer queries sent to the database and the result is your website will load much faster. WordPress object cache. The idea of object cache is that WordPress core themes and plugins may store some data that is frequently accessed and rarely changed in the object store. This is so these objects will not have to be retrieved and processed on each request. WP object cache is WordPress's class for caching data. And by default, the object cache is nonpersistent. This means that data stored in the cache resides in memory only and only for the duration of the request. Cache data will not be stored persistently across page loads unless you install a persistent caching plugin. Ultimately, object caching will reduce the total number of database queries required for each page load. When the CPU does not have to rebuild this box of data, your response time will decrease. Once the server and database have been tuned for performance, the next step is to ensure the application is optimized for performance. This can be done by leveraging techniques designed to deliver assets much more efficiently or removing any unnecessary data from them resulting in faster transfers. The following are some of the most effective techniques you can use when it comes to optimizing your WordPress site for speed. But it's not an exhaustive list. When focusing on page website performance, there are some essential tasks you can complete to help you achieve the performance results you're looking for. These tasks include configuring caching, following best practices for elements that can introduce layout shifts, and adjusting your assets to ensure they are configured for the fastest possible delivery to your visitors. One, page caching. WordPress is a dynamic program by default. So the easiest way to speed things up is to create a static version of your website pages and use redirects to serve those cached files instead of calculating the result again with PHP and MySQL. This cache can be stored on the disk in the server's RAM or if you're using a CDN on a point of presence or edge server. If you're storing the page cache on the web server, then it's recommended to use disk enhanced storage. So the web server redirects can serve the cache prior to the request reaching PHP, essentially making the site load faster by using the static files instead of recreating them with queries and calculations. Storing page cache on a CDN at the edge is a practice known as full site delivery or acceleration, where the website is served by sending a cached version of the website from a geographic location near the visitor as opposed to the request reaching out to the origin server where the data lives. This removes location introduced latency from the equation, making websites much faster for all of your visitors to object caching with Redis. By default the object cache in WordPress is not persistent. This means that data stored in the cache resides in memory for the duration of the request and is often removed after the response is delivered. To store these cached objects for reuse, you can use a plugin that connects the WordPress cache to Redis, a RAM based service that is used for persistent object storage. W3 Total Cache will allow you to use object caching with Redis. Redis stores the data as keys in the web server's memory, which allows for retrieval of the data to happen much faster than the WordPress method, which caches those objects to the options table in the database by default. Three, leveraging common UX patterns. When adding elements such as fonts, carousels, banners to your website, you want to ensure that you are following best practices. These types of elements can have a negative impact on Core Web Vitals if not implemented correctly. A great example of this is not explicitly setting proper placeholders or dimensions for images, videos, iframes or other similar embedded content throughout your page. When the content is loaded, this can cause layout shift and Core Web Vitals has very specific metrics, as the moving page can create a bad user experience for your visitor. Learning custom fonts on your website is another example where things can go wrong. Adding them incorrectly can delay the text rendering or even cause layout shifts. Typically your browser would delay displaying text until the font is loaded, which can also impact your FCP or LCP. If the text does load prior to your web font and there are different sizes, layout shifts will occur and directly impact your CLS score. Four, minification. When developing a website, most programmers tend to use spacing and comments to make code readable for both themselves and others. Minification is a technique used to reduce load times on a website by removing unnecessary characters and comments from the code for faster delivery and rendering. This technique can be used on HTML, CSS, and JavaScript files and can dramatically improve your site's performance. To minify your assets with W3 Total Cache, you'll need to enable it in general settings. There's an automatic mode which attempts to handle the heavy lifting for you, but if you have issues with it, you'll need to enable manual mode and add the assets you wish to be minified yourself under the minify page. There is a help wizard for setting up minify, which you can use to specify the template to use when loading the asset. Five, concatenation. The term concatenation refers to the action of linking things together in a series. In the context of a website performance, this means combining multiple CSS and JavaScript files into a few files to deliver the assets the browser needs faster with less requests. In W3 Total Cache, concatenating files is an option in general settings under minify. Checking the box will allow you to combine the asset files into one of three different files in the DOM. One in the head, one in the body, or one at the footer before closing the document. Six, eliminate render blocking resources. Render blocking resources is essentially coding your website files, usually CSS and JavaScript, that prevents a webpage from loading quickly. To ensure your site loads as efficiently as possible, you can specify attributes that tell the browser how to download the code in relation to the other site assets. When concatenating and minifying files using W3 Total Cache, you also have the option of embedding them before the the close of the document's head tag, or after the body tag. To improve performance, you have the option of setting rel attributes for the assets such as async or defer, which will tell the browser when and how to load the file. The minify wizard in Total Cache will allow you to specify which JS and CSS files load by template, allowing you to have to have fine tune control over which assets load when and where. Seven, preconnect and prefetch DNS hints. If your site is using multiple third party scripts, the location those scripts are hosted on will be considered a cross origin domain. That third party domain will need to be resolved to an IP address for the browser to download the file and complete the request. The time it takes for this DNS resolution to take place can introduce latency into the page load. And using preconnect and prefetch will instruct the browser to handle this prior to fetching the asset. This can help remove the latency introduced by resolving a large number of DNS requests for assets in the background. Using DNS prefetch and preconnect links will help your page load faster by performing these operations for later use. The DNS prefetch will do a DNS lookup while preconnect will establish the connection to the server. This connection established will also include a TLS handshake if served over HTTPS. The preconnect hint should be used sparingly and only for the most critical assets being used in the viewport during the initial page load. For noncritical assets, using only the DNS prefetch hint is recommended. Eight, image optimization. Image optimization refers to the practice of a website delivering the highest quality image with the right format with the right format, resolution, and dimensions for the device and viewport accessing them. This is done with the intent of keeping the file size as small as possible. There are several layers when dealing with optimizing your images, including removing metadata, converting to a different format, resizing, and lazy loading. One of the ways to reduce an image's file size is to remove any metadata attached to the file. This static can include information about the camera used to take the picture, GPS coordinates, the file's owners, comments, a thumbnail, and more. There are several types of metadata that can be added to the images including EXIF, IPTC, XMP. Removing this data will help to decrease the file size of the image. An added benefit is that removing metadata keeps these details from being publicly shared. WebP is a compression format for images developed by Google. When optimizing your images for speed, using WebP is the recommended format for websites. Most modern browsers now support the WebP image format. There are many WordPress plugins that offer this type of image conversion and most even assist with redirects to automate the process and a fallback option in case the browser does not support the WebP format. W3 Total Cache has a service for conversion of images and redirects under media library when image service extension is activated. Dynamic image resizing. When different devices are used to access your webpage, the image size needed will vary depending on the visitor's viewport. Dynamic image sizes can be used to make each of these devices load the image with a size tailored specifically to the viewport where the site is being rendered. A great example of this would be the hero image on your website. While a desktop would likely need the full size version of the image, the narrow viewport of a mobile device may only need an image that is 20% of that size. Providing the browser with the smaller size would reduce the time it needs to download the file and display it. There are many WordPress plugins and CDN providers that offer dynamic resizing for images. The solution you choose could be based on your audience. If your visitors are mostly local and your traffic is not high volume, a CDN's cost for this service may outweigh the benefits to you, so using a WordPress plugin may be your best option. 9 Lazy loading images and other embeds. Lazy loading is the practice of identifying noncritical assets and ensuring they only load later as needed to render the page in the browser. In the context of images, lazy loading is usually based on a user interaction such as scrolling, and loads images needed as the user gets near them. Lazy loading allows the first render to happen faster, as it instructs the browser to only load the critical resources needed to display what is being rendered in the viewport. Then, as you scroll, the application can detect when resources are needed and initialize the download. W3TrollCache offers lazy loading for images. Finally, using image facades for interactive elements. One of the biggest issues authors run across is having multiple video embeds from providers such as YouTube or Vimeo. Loading these videos will force your site to do DNS lookups, make connections to these thirdparty servers in the background, and download the elements needed to render the content. Providing an image that is stirred from your site's domain as an interaction point to trigger loading these types of content embeds can save you precious time rendering your webpage. It also saves a user from downloading JavaScript and CSS for an element they are not going to use, as the video does not render unless the user specifically clicks on the image anchor link to trigger it. This is also true for other interactive elements such as chat embeds or other interactive elements that are not hosted on your server and are being served to your visitor from a remote service. Okay, now that we've done some optimization to our WordPress page, let's enter the WordPress URL in the page speed insight and analyze it again. And we can see the numbers are better. Our performance is up and our performance is up on mobile and our performance is up on desktop. This was a basic overview for configuring your performance. And there are additional ways to drill down into your configuration when aiming for higher scores. A great example of this is using custom page templates for your most visited and important landing pages and using the minify help wizard to only load the critical CSS and JavaScript assets on those pages if they're needed. Some plugins like contact forms can load their resources on all pages, even if there's not a contact form embedded. You've reached the end of the course, you've learned about key metrics important to website performance, and you've learned a bunch of things you can do to optimize the performance of your website. Thanks for watching.