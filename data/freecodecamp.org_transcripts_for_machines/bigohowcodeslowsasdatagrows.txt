so as ned said my name is ned batchelder i'm ned batt on most social media and there's a bitly short link there at the bottom of the slide that links to this talk online if you want to follow along with the slides and those two short things will be at the bottom of most of the slides too so i've been writing software for a long time and one of the things that interests me about writing software is that there are two mindsets that inform the process of writing software the first is computer science which is really a branch of mathematics and so it's very theoretical you do proofs you think about very abstract concepts the other mindset that informs writing software is software engineering which is very pragmatic and is basically concerned only with whether you are writing software that works how can we write software that works and there is some crossover lots of computer science underpins software engineering but we don't think about it every day but there's a few topics that do cross over into the everyday of software engineering and in particular i'm interested in people who are working in software engineering who don't have formal computer science backgrounds and maybe feel a little bit insecure about that and one of the things that they seem to feel keeps them from sitting at the grownups table is this thing called big o a big o is really a simple thing and i've made a rhyme here to help you remember what it is it's about how your code slows as your data grows and an english major friend of mine pointed out that the rhyme is on the o sound like big o and i didn't intend that at all but that's cool so the question is how does your code slow down as the data gets larger and larger and this is not the same as the running time of any particular run of your code we're not trying to measure the time in seconds we're not trying to figure out exactly how many of them you can do in one transaction we're talking about the trend over time over many runs of your code how does it slow down as the data gets larger and larger and larger and one very pragmatic way to think about this is let's say you have a chunk of code you give it a certain amount of data it takes a certain amount of time how much longer will it take to work on 10 times as much data if i give it 10 times as much data how much longer does it take and you might think intuitively well it'll take 10 times as long obviously but that turns out not to be true some code will take 10 times as long some code will take twice as long some code will take a hundred times as long and some code won't take any longer at all and big o is all about characterizing that growth of the time of the code as the code as the data grows how the code slows as the data grows and computer science people approach this topic in a very very mathematical way but software engineers approach it in a very very pragmatic way and i'm trying to i'm going to explain the pragmatic approach it doesn't have to be done in a mathy way it can be done in a pragmatic way so let's get some terminology out of the way this is called the big o notation and the way it's written is a capital o and a parenthesis and then a bunch of stuff with an n in it and a closed parenthesis okay i told you it was simple the n is meant to stand in for how much data you have and the o stands for order of and the idea is that we're talking about the running time of your code grows on the same order of some mathematical expression of n the key thing here is although it looks like a function call there's a name and then parentheses with stuff inside it's not a function call it's just a notation and there's probably a mathy reason why it looks like a function call but it doesn't matter just know that it's not a function call it's just a way of labeling a piece of code as having a certain growth pattern so let's take a real world example let's say we have to count the number of beans and jars right we've got this guy on the left he opens up the jars he starts pulling out beans one by one right we can see here n is the number of beans right if we give this guy a jar with 10 times as many beans it's going to take him 10 times as long obviously and he's sweating this is what's known as o of an meaning that the time it takes to complete the task grows in the same way that n grows it's on the order of n if n doubles the time doubles if n is 10 times more the time is 10 times more and you might think well there's no other way to approach this task but there is of course you get beans jars that have labels on them that tell you how many beans are in them right this guy on the right has a much easier job you can see how much happier he is about it because it doesn't matter how large a jar you give this guy on the right it's going to take the same amount of time for him to tell you how many beans are in the jar this is what's known as o of one which is kind of a weird mathematician's way of saying that n isn't involved at all no matter what happens to n the running time remains the same o of n is is slower in the long run than o of one and this is a silly silly real world example um but for instance when you do for x in my list you you have an o of n operation because you have to look at every element in the list every being in the jar when you do len of my list you don't have to look at the elements of the list at all turns out python lists are kind of like those charts on the right the length of the list is written on a label on the outside of the list and so no matter how long the list is getting the length of the list is a constant time operation by the way these drawings were drawn by my son who is in art school one thing you might not have noticed if you look at their eyebrows they're shaped like beans that's that's art school for you all right another real world example let's say i tell you i'm going to give you a book and i want you to find a certain word in the book like horse right if i hand you a novel you're going to start reading until maybe you find the word horse right this is sounds like an o of n operation again right because if i give you a novel that's twice as long it might take you twice as long until you encounter the word horse but let's say i give you a different book i give you an encyclopedia now you open the encyclopedia to the middle if the word you're looking for is earlier than that then you do another divide and conquer step until you find the word horse if i now if i give you an encyclopedia that's twice as large it's not going to take you twice as long there's just one more divide and conquer step to find it that's what's called o log of n which is a fancy mathematician's way of saying that so these are both real world examples of the kinds of tasks that sort of sound similar when you first hear them but how you organize the data and therefore what algorithm you can use on the data really affects how the length of time it takes you to do the task changes as the size of the data changes right that's what we're talking about is how your code slows as your data grows let's get some other terms out of the way because i'm going to be speaking here and i might throw out some words that are a little bit different than earlier when we say o of one we might call it constant time i might say that the labeled bean jars are a constant time algorithm because the time remains the same no matter what o of n is often called a linear operation because if you look at it mathematically there's a linear relationship between the size the data and the running time of n squared is a thing we haven't seen yet but we will that's the case where when you give it 10 times more data it takes a hundred times as long to run and that's called quadratic because now you've got a quadratic equation involved if you don't know what a quadratic equation is it doesn't matter it's just a word that means n squared and some other words for big o it's sometimes called complexity or time complexity or algorithmic complexity or if you want to sound really fancy you can call it asymptotic complexity it's all the same thing right one of the underlying themes of this talk is that this this this topic of big o notation is littered with mathematical detritus that doesn't really matter to the key concept and don't let that stuff throw you that's just chaff being thrown at you by mathematicians you don't have to let it throw you off the path so how do you actually determine the big o of a piece of code the first step is you figure out what code you're talking about and that sounds kind of silly but in a large system there's you might be looking at one function and that might be the important thing but it really might actually be important to consider all the callers of the function or maybe you're looking at too large a chunk of code you need to think about a small piece if you're going to describe a huge piece of code be very clear about what piece of code you're talking about and then when you look at that code you should figure out what n is and i don't mean like whether it's a hundred or a thousand i mean what is it measuring so if you're you have some code that's iterating over all the records in a database then n is how many records in the database and our bean example it was how many beans in the jar if you're doing a string search it might be the length of the string and then here's where the real work comes in you're going to think about that code running and you're going to figure out how many steps there are in the code in a typical run let me tell you what i mean by typical first there's two meanings of typical one is what kind of data is it going to get in the real world there's sort of real world data that kind of is what you kind of can expect and then there's worstcase data right a string of 40 000 spaces is not typical data typical data is you know last names it's mostly ascii it's about most to 15 characters along that kind of thing so you can think about what's your typical data and then another meaning of a typical run is that over many runs of your algorithm there's a certain number of times a loop might run or a certain length of string it might get so you kind of think about the design center of your code and you imagine running that code through that design center and you count the steps and what i mean by steps is very vague and in a way a lot of this topic is very vague there are no units in anything we're talking about and the number of steps it kind of doesn't matter what you count as a step and it kind of doesn't matter that some steps might actually take longer than others because really what you're thinking about is if i'm doing n equals 10 i'll have this many steps now how many for n equals a hundred and so the exactly what steps there are doesn't matter as much as how does that count grow and i'll show you some examples so you'll get a sense of it so you count the steps in a typical run and since we put in n at the top and not 10 exactly the number of steps is going to be an expression in n you might end up with well it's 3n plus 47 steps something like that and then what you do is you keep only the most significant part of that expression so you keep only the highest coefficient piece and then you throw away the coefficients so if you had 47 n squared plus 53n plus 101 that's n squared you throw away all the lower order components and the exp and the coefficients and the reason is that as n gets larger and larger and larger the lower order components matter less and less right three n plus one the one is really important when n is one but when n is a billion who cares about the one right we're trying to get to that long term trend as the data gets very very large and if it's 3n that doubles when n doubles just the way n doubles when n doubles so the 3 is irrelevant too so you get rid of the lower order components and get rid of the coefficients and what's left is your big o notation now let's look at some examples true fact i wrote this code in november and it didn't occur to me until i was lying in bed this morning that this code is about moms and it's mother's day so here's an example of some code what we're going to do is we're going to have a data structure called moms which is a list of tuples and the tuples are people and their mothers right and then we're going to write a function called find mom and find mom is going to take that list of moms in the name of a child and it's going to find the child's mom okay now if you think about searching through this list in a typical run in some runs you'll find it in the first entry and some runs will find it in the last entry so on average in a typical case we'll find it about n over 2 times we're going to look through half the list so if we come down to this line this loop is going to run n over two times and i'm going to say that there are three steps in this loop we have to get the tuple out of the list and then we have to assign the child to child name and we have to assign the mom to mom name so there's three steps which means that this line is going to contribute three times n over two steps to our count this comparison we're going to do n over 2 times and there's only one step so that gives us another n over 2. and then this line is only going to happen once because it's the end of the function so that's going to give us one more step and so what our total is going to be 3n over 2 plus n over 2 plus 1 which simplifies down to 2n plus 1. i said that we get rid of the lower order components which is the 1 we get rid of the coefficient which is 2. this is an o of n function right so we've just determined that the algorithmic the sorry the asymptotic complexity of find mom is o of n and the way people say that in the real world in a cubicle is find mom is o of n or find mom is linear bind mom is of n okay and so you saw when we were going through the steps we didn't really care which steps were expensive which weren't we all we wanted to know is the relationship between the n and the number of steps and when n changes the number of steps is going to change and that's what we're looking for and notice we have no idea whether this is fast or slow right we don't know whether this function is going to take a minute or a millisecond all we know is that if we give it 10 times more data it's probably going to take 10 times as long let's look at another example also about moms the same same data structure the same moms data structure but now what we're going to do is we're going to write a function which tells us in that data structure how many grandmothers are there that is how many people how many people are mentioned both as a mom and as a child in our list right and so now we're going to go all the way through to the end of the list and i mentioned that n over 2 before but remember we're throwing away coefficients so in a way the half never mattered and as you work through this more you'll sort of get a sense of what you can not collect in the first place because you're going to throw it away anyway so for instance this line is going to run n times right and notice i didn't write 3 times n here because like i said we're throwing away coefficients this is an o of n line we're going to run this line n times now this line is going to run n times also but it's calling a function find mom which we just determined was an o of n operation in and of itself when you call it once at so of n and we're going to call it n times that's going to give us n squared and we can continue on and say this is n but remember we're going to throw away the lower order components we already found an n squared it's kind of uninteresting to keep finding the ends right but we're finding a bunch of n's we're going to end up with n squared plus some number of n plus 1 which is o of n squared right so find how many grandmothers is a quadratic function it's o of n squared now the ideal of course is o of one right constant time you can do the same amount you can work on any amount of data and not take it have it take any longer and it seems kind of impossible like how could that be but remember we saw len of my list as of one because no matter how long the list is the length is written on the outside we can just pick it up that's kind of boring really interesting is the looking up a key in a dictionary is of one no matter how many keys are in a dictionary it takes about the same amount of time to look up a key as in a one element dictionary and in the million element dictionary which is why dictionaries are heavily optimized and engineered and underpin every name lookup in python because they're fast and we'll get back to why it is but very quickly it's because there's a thing called a hash function which turns a key into a number and in typical data the numbers are all different and so you can very quickly use that number to find the place in the dictionary where the value is now no discussion of bigo notation be complete without showing you the graph along the bottom we have that flat green line labeled one of the the xaxis is data so data grows to the right and then the time grows going up um so the big flat line at the bottom is o of one that looks great log n was looking through the encyclopedia the linear line going diagonally is o of n and the big red one n squared just zooms literally off the chart right so the n squared is one of those bad things you try to avoid because it really grows really fast when things get big now when we looked at our code we have to understand what functions we're calling and how what kind of complexities they're adding into our total function this is a chart of the typical operational complexities of lists dicts and sets in python by the way when people say dictionaries are o of one that sentence doesn't make any sense and nouns like dictionary can't have an algorithmic complexity operations have an algorithmic complexity so what you're supposed to say is that looking up a key in a dictionary is of one now you'll notice that a lot of these are kind of the same appending to a list is o of one adding a key in the dictionary is of one adding a value and a set is o of one a big difference is looking up a value in a list is o of n so if you're going to search for a value in a list it's gonna have to look at every element in the list it's gonna be that left bean counting guy with the sweat coming off of his forehead but looking up a key in a dictionary or a value in a set is o of one which is why they're really really valued so pro tip right off the bat if you've got a program that's going too slow look to see if you're looking up a value in a list and replace it with lookup in a set but there are tradeoffs so when i say replace the list lookup with a set lookup you've got to keep your eye on the big picture and this is where understanding what piece of code you care about matters so i said replace a list lookup with a set lookup let's say we have some code like this where we're going to make a list we make a list and then we try to find the thing in the list and that line is of n right we just saw that on the table of python complexities so you might think well i know what i'll do i'll make a set instead and then i can look it up in a set that's good if you can do that that's good so you don't make the list in the first place you make a set bad is you go ahead and you make a list anyway and then you convert it to a set and then you do the lookup in a set right so now that last line is great it's so of one but you've added a line before it turning the list into a set which itself is o of n like literally you've actually slowed down your program by a tiny amount you still have the o of n operation of converting the list into a set so it's very easy once you get into this algorithmic complexity stuff to get sort of focused on the little things and lose sight of the big picture right this would be a bad tradeoff a good tradeoff would be even if you're making a list if you can convert into a set once and then do many lookups so if you're going to do many lookups in your list then it makes sense to turn it into a set once and then you have o you have one o of n and then many o of one and your program will go faster so you always have to keep in mind what the real usage is of your code and where the time is being spent about where you're going to sort of work on reducing the algorithmic complexity and whether it's worth it now this is don't read this code the code doesn't matter this is a real example of code that got me started down this path this was from a project last summer the code on the left is shorter and has fewer data structures and fewer functions and in fact fewer loops but it's slower than the code on the right and the reason is that if we label things as of n and o of one the code on the left has an o of n operation there because it's looking up a value in a list the code on the right only has of one operations so the code on the right even though it's longer and has more functions and more data structures and more loops is o of one where the one on the right is o of n and in fact i was using it to draw drawings like this these functions work over an entire list of points and if you go up a level in the code you'll see actually that that function is called once for each point in the list of points so the o of n on our slow side was turning into o of n squared and so the slow code was taking 20 seconds the fast code was taking a half a second on only 2 000 points right i said i've been giving examples like oh when n gets to a billion n got to 2 000 here and made a huge difference in my running time because n squared is really worse than n with an n of 2000 n squared is 4 million operations i know of n is 2 000 operations and that's a big difference so it really does pay off sometimes to reduce the algorithmic complexity of your code to reduce the running time now we've been talking about o of one and o of n and over n squared there's more possibilities so there's more possibilities of kinds of complexities you might encounter in the real world of course there's o of n cubed and then fourth right if we called how many grandmothers once for every child for some reason we'd have an n cubed operation if i used that point algorithm once again for every point i'd have gone back up on an n level i'd add another coefficient to my n you can also have worse things like o of 2 to the n if you have n boolean choices and you try all the combinations of them you've got 2 to the n if you have n things and you try to try all permutations of those n things you'll have n factorial so as bad as n squared is there's sort of no upper limit to how horrible your code can get so think about how your loops are working where how much data you're working on and keep an eye on where those complexities are getting really really big the other kinds of possibilities is there could be more dimensions so we've been talking about doing algorithmic analysis where we have one variable n but you could have others right if i'm telling you that i've got a string search algorithm over some number of strings i should also have to consider the length of those strings typically they're fairly short but if you're doing you know bio python or something you have dna samples that are millions of characters long and then suddenly the lengths of the strings matter too for example when i was doing that point drawing code there was a line intersection algorithm that i found whose stated complexity was n plus k times log of n where n is the number of lines and k is the number of intersections among those lines and i don't know how to figure that out that's like a mathy thing that you can just read about i didn't have to figure out what that complexity was now we saw this graph before algorithm the complexity really matters as numbers get large but another place where you have to be careful not to over apply the idea is when numbers are small so let's zoom into that lower lefthand corner of this of the graph if we zoom in there suddenly the lines don't look so clearcut the green line is actually above the other lines for most of them and the n squared line that red line is actually below everything for a lot of the time and the reason is that when numbers are small all those coefficients and lower order components that we threw away those mattered right three n plus one when n is one that one at the end really matters and also we haven't taken into account what the actual time of the steps is you might have an n squared operation where you are doing n squared times a millisecond and you might be comparing that with a constant time algorithm that always takes a minute well n has to get pretty large before that constant time algorithm is worth it when n is a billion it's worth it but when n is 10 you should stick with the n squared algorithm so as rob pike once said fancy algorithms are small when n is small and n is usually small so don't go overboard with trying to fancy up your algorithmic complexity it doesn't matter when n is small and usually your n is small all right some advanced topics there's a thing called amortization which is really a long term averaging over operations so when i say that appending to a list is o of 1 that doesn't mean that every single time you append to the list it takes a small amount of time in fact it usually takes a small amount of time but every once in a while the whole list has to be copied and moved someplace else which is which gets longer and longer as the list gets longer but it also gets less and less frequent as the list gets longer so that over the long run the average is still of one so amortization is a fancy word meaning averaging and it means that individual operations can take different amount of times algorithmic analysis is really about the longterm trends over many many runs and we haven't talked about the worst case so earlier we talked about the typical case and some people think big o implies typical case or big implies worst case no you have to say whether you're talking about the complexity of the typical case or the worst case here's an example where i make a set of 50 000 numbers which differ by 47 i'm kind of walking up the numbers by 47 adding that number into the set is an of one operation so the whole building the whole set is o of n and it took about 10 milliseconds here i'm building another set of integers exactly the same size 50 000 numbers but i happened to choose a step that i happen to know was going to make all the hashes exactly the same so all the numbers got exactly the same hash which turns it into an of n operation which means making this set took 34 seconds 3 300 times longer dix also have this problem and people were using it to ddos web servers which is why python added hash randomization and it's a fascinating topic but it's an example where although dicks are of one in the typical case occasionally you have to worry about the worst case and there's more math so if you dig into the math basically mathematicians have taken every letter that either looks like an o or sounds like an o and given it a meaning and you don't need it you don't need it at all and there might be mathematicians in the audience right now who are going to say you know you're not really even talking about big o yeah shut up i don't care we all we all this is what we which is what we mean by big o and those experts by the way so i wrote a blog post when i was first starting to think about this with the same title big o how code slows as data grows and a lot of people liked it but one guy wasn't so pleased um he thought that not only had i gotten something wrong but the thing i'd gotten wrong was so important that the entire blog post was something that i should be ashamed of and i actually looked into it i gave him the benefit of the doubt i learned a little bit more about algorithmic analysis i concluded he was actually wrong he remains convinced he is right the good news is i got another blog post out of it so if on your journey to explore these things you find people like this just walk around them and keep going it doesn't matter and i mean if you're into the math go and do the math but if you're just trying to do software engineering it doesn't matter all right so in conclusion big o is useful it can help you understand how your code might perform when the data gets very large it doesn't have to be complicated it doesn't have to be mathy and you can do the thing thanks