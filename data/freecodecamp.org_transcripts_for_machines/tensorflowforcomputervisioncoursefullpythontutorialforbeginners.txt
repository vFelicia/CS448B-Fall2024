tensorflow can do some amazing things when it comes to computer vision in this course machine learning engineer noor islam muktari will teach you how to create multiple computer vision projects using tensorflow 2. hello and welcome to my course introduction to tensorflow for computer vision so just a quick introduction about myself my name is noor islam i work as a machine learning engineer and i'll be your instructor for this course if you have any questions regarding the course material and content or anything related to machine learning and computer vision feel free to connect with me on linkedin and twitter and if you're starting a career in machine learning and computer vision then please check out my free ml job ready checklist that i have put together which contains some information that might help you to set a roadmap for things you should learn and maybe things that the machine learning and computer vision industries might expect from you so all of these information regarding my social media accounts and also this free checklist can be found in the link below it's one link but it's gonna show you all of the necessary information once you click it let's take a look at the course outline so we're going to start the course by doing some software setup then i'm going to show you how to do image classification using mnist dataset which is basically the equivalent of the hello world examples in the machine learning field then we're going to look at the image classification using german traffic science which is a real world data set that reflects more challenges compared to the mnist dataset so for the software setup we're going to start by downloading and installing visual studio code i'm also going to show you how to get miniconda i'm going to tell you why we need it and how to install it we're gonna be using minicanda with vs code so i'm gonna show you how to make those two work together then we're gonna install tensorflow 2 cpu and gpu versions we're also gonna be installing different python packages along the way for the second part which is the amnest example we're gonna basically explore the amnest data set i'm going to show you how to build a function that randomly looks into the data set and it gives you some ideas about how this dataset is constructed i'm also going to show you how to use tensorflow layers how to import them and how to use them to build the neural network we're going to be building the same neural network architecture in three different ways which are the ways supported by tensorflow so the first one is called the sequential way by using the sequential class the second approach is called the functional way which is basically your model is wrapped inside a function and the third approach is the model class way which is by inheriting from the model class we then gonna do the compilation of the model and we're gonna fit the amnes data set into our model we're gonna finish this part of the course by restructuring our code for better readability which is something you're going to find yourself often doing when working on machine learning projects in the industry and for the last part of the course we're going to be working with the german traffic signs data set which is a real world data set that reflects the real challenges when working on machine learning projects compared to prepared data sets like the amnest dataset so for this we're going to start by downloading and exploring the data set we're going to prepare the training validation and the test sets because they're not already prepared we're going to build our neural network we're going to create some data generators which is a different approach than the one we will be using for the first example which is the amnesty example we're going to compile the model and fit the data i'm also going to show you how to add some callbacks and why they could be very useful and i'm going to show you how to evaluate your model using the generators you created before then i'm gonna show you some potential improvements that you can do to get even better results and we're gonna finish this uh part of the course and basically the whole course by a an example of how you would use your model to make prediction on single images so basically how to build a standalone example that can make or that can use your model in order to make uh separate predictions so who is this course for basically this course is for anyone who's interested in learning about tensorflow specifically for computer vision applications ideally you should have good knowledge about python like classes and functions in python and also basic concepts in deep learning such as convolutional layers so why would you learn tensorflow anyway well there are several reasons but these are three of the main reasons in my opinion so tensorflow is the leading framework in deep learning followed by pytorch and you can build powerful deep learning solutions with it for your own use and also for your company the company you're working for and there are lots of companies working on deep learning projects and they are using tensorflow so the job market is very large and it's only growing so these are the main reasons of why you would want to learn tensorflow but at the end of the day maybe you just want to learn it for fun so that depends on you so just a quick note before we continue with the course material we will be using an ide and not notebooks we will be specifically using visual studio code and this is for several reasons in industry projects you usually use ides and not notebooks with ides you easily scale your machine learning code so you can add a lot of scripts you can test them separately you can import them in different and other scripts and also the i am for the opinion that says that ides are for building your projects and notebooks are for presenting your work so for me and i think many other machine learning practitioners agree with me on this is that we usually use notebooks for presenting the work but when we're building large machine learning projects we usually go with ides so the first thing we're going to start doing is downloading visual studio code is an ide that we will be using throughout the course to write the code and also to run that code so if you go to code.visualstudio.com you're going to get this page here and as you can see it automatically detected my system so i can just click here and download visual studio code for my windows machine if you have a different machine it should detect it if not then you can just click on this button here and choose the right software for your machine so if you click this you're just going to get this download here and you can just save it to your machine and run this executable just like you would run on any other installation process there's nothing unique or special here so i'm not going to go through it here now after we install it if you go to your run menu here and you just type visual studio code you're gonna get this uh the icon here where you can run visual studio code usually i like to pin it to my start so as you can see it's already pinned here which means i can just access it from here you can choose to do that as well if you prefer that so let's run visual studio code and the first time you run it you're gonna get this new window here where you have the starting menu so you can create new file open a file or or do all of these other things and also for me as you can see i have this part here that says recent which are my recent projects if it's the first time you're using visual studio code you will not have this so the first thing i'm gonna do is just to open a folder so i'm gonna click on open a folder here you can choose any place on your machine to to open that folder so for me i have chosen this place on my machine so i have a disk called d code for courses and introduction to tensorflow 2. i created this new folder and now i'm just gonna select it so select folder and now it's gonna open this uh new window okay here it says uh if i trust the authors of this files yes no problem these are my files so here you get this new window where you have this empty folder on the left and you can add new files you can also add new folders inside of this folder and here we will get uh the place where we can write code and also we can get the terminal at the bottom here so for now this is it for installing and setting up a visual studio code what we're going to do now is look at the other software that we will use throughout the course and also later we're going to start adding files here so that we can start writing some code so the second software we need to install is called miniconda so if you look online you're gonna find two types of products for condo there's mini conda and there's anaconda anaconda is actually a gui that allows you to do almost everything that mini conda allows you to do the only difference is that uniconda does not have a gui we will just use it in a command line prompt so if you type miniconda in your google search bar you're gonna get the first result here so if just open this here you get all of these links to install miniconda on your machine so if you are on windows you choose this otherwise you choose the other types here and again this there's nothing special here you can just click the link save it save the file to your to your local machine and then run the installation process and also there is nothing special when installing minicanda so i'm not going to go through it here now when you install miniconda if you go to your run menu here and you just type you're going to see anaconda prompt and between that parentheses there is minicom.3 so if you open it you're going to see that it looks just like the command window on windows so they are basically or almost identical the only difference that we can notice here is that here we see this name called base between parentheses here and here we can see it in the default windows command line window now what this means is that in fact in the miniconda prompt prompt we are actually now inside what's called a virtual environment and this virtual environment is called base so i'll go a little bit into more details about what these virtual environments do and why would we need them and why don't we just install things straight uh on our system using just the windows command line prompt i'll go into details about this just next so what are these virtual environments well you can actually think of them as these separate entities inside your system but they do not necessarily affect your system so for example we can think of it in this way so you have your system and inside your system you have python say 3.8 installed and you have some python packages and now with virtual environments what we can do is create as many environments as we want so for example here i can create i show you an example where we created three different virtual environments so the first one is called vm1 the second one vm2 and we have vm3 so in the first one we can install python 3.7 and for example we installed tensorflow 2.6 now when you do this you're not actually installing uh python or the python packages straight onto your system you're actually installing them inside this virtual environment so this virtual environment it actually lives inside your system but what you install in it does not necessarily affect your system so here as you can see on your system you might have python version 3.8 but inside your virtual environment you can change that and you can use python 3.7 and as you can see you can create as many virtual environments as you like and the goal here or the why would you even want to do something like this is really when you're doing machine learning or data science you're going to be doing lots of experimentation so for example there might be a new tensorflow version that has some new functionalities and you want to test it if you have everything installed on your system and then you just upgrade to the newest version of tensorflow then you might affect your existing code so you might something that has already worked in the previous version of tensorflow may not work in the newest version and that could cause a lot of conflicts so now that now you find yourself in a situation where you have to go back to an older version which is not uh very good so when you do this you can do these experimentations separately and as you can see for example if i want to test pytos and i don't want it to be used in the same virtual environment as tensorflow i can create uh completely separate virtual environments install uh whatever version of python i need and then i install pytorch here and if i want to test for example an older version of pytorch i can create another virtual environment and do the same thing and you can imagine all of these benefits that you can have for example many companies are still using tensorflow version 1.x so if you want to do this if you want to install a previous version or older version of tensorflow so 1.5 1.6 then it's better to install it in a virtual environment instead of doing it on your system so this is uh the goal of why we're trying to use virtual environments i really encourage you to use this kind of framework when you're doing your experimentation instead of installing everything on your system i've been working in the machine learning field for some years now and i have always found that it's better to use virtual environments rather than install everything on your directly on your system so i hope that now you can see the benefits of following this kind of paradigm and next let's start creating a virtual environment and installed necessary python and python packages inside of it so now just before we start installing tensorflow and creating our virtual environment let me just clear some some of the doubt that you might have here so as you can see when we installed miniconda we have this command prompt here and also i showed you that we have the default windows command line here and they are similar but how are we going to use our virtual virtual environments inside visual studio code so in fact if you go to visual studio code if you go to terminal new terminal here in fact we are inside our command line default command line window so what you do here is going to be identical to what you would do here and in fact let's just for example type python so here you see on my system there's python 3.6.8 installed and here if i write python i get the same uh version here so i have whatever i do here i'll uh i'll be able to do it here and vice versa and now let me just exit this what we want is to actually be able to use our virtual environments and just use mini conda in general inside our terminal here on visual studio code so in order to do that what you can do is just look up where miniconda is installed on your machine so for me i'm using a tool called everything it's a search tool you don't have to install it i just prefer it than the default search tool on windows so here when i type miniconda3 which is the software that we installed if i open this folder here there is another folder called scripts and then there is activate.bat so if i just take this copy it paste it and then run activate dot that now as you can see i have the base name here written before my before this path here and as you can see we have the same thing here so now we are in fact inside the virtual environment called base and whatever you do inside this command this window here is going to be reflected here and vice versa so now that we have we can access condyle commands on our visual studio code terminal let's try to see what we can do with it so here let me just clear this part here and in fact you can run so many conduct commands so to run the contact command you start by writing canda and then you finish the rest of the command with whatever you need to do so for example we can look at our uh virtual environments that are already installed in order to do that you can run this command calenda info dash dash ms and if you've never had a mini conda before you will have nothing when you run this command but for me as you can see i already have some virtual environments installed here so let me just clear this one here and next what we're going to do is create a new virtual environment and then we will choose the right python version that we want and then we're going to install tensorflow inside that virtual environment to create a virtual environment using canda the command that you should run is kanda create dash and now you can give a name to your virtual environment you can choose any name you like usually for me since i do many experimentations with tensorflow i just like to call it for example tf2 uh or i give it the full for example version of tensorflow so tf 2.5 or 0.6 or whatever and here let's just call it for example tf2 vm so vm for virtual environment and then you can give it the parameter python and here you can choose whatever version of python you like so let's just use 3.8 since i've been working with it for uh for a long time and it's uh it works great with tensorflow so here i'm just gonna run this command so now it's gonna ask us whether we accept to install these unnecessary packages so i'm gonna choose yes and click and this should not take that long because these are not large packages so after we have our packages installed we can run kanda activate tf2 vm and of course you can see that the suggestion is shown here in order to activate your virtual environment so now if you remember we are inside a virtual environment called base so after i run this command now we're inside our new newly created virtual environment called tf2 vm and here whatever you install is going to be separate from your system so it will not affect it so here for example let's just run python and as you can see is version 3.8 if you remember on my system i have 3.6 installed but here inside my virtual environment because i specified that i want python version 3.8 i have 3.8 installed here so now that we are we have created our virtual environment we start installing the necessary packages so now we can install for example tensorflow and if you go to the official website of tensorflow you can see if you just go to tensorflow.org install you can see different ways you can install uh tensorflow for us we're gonna use uh tensorflow or download it and install it as a python package so these are the commands we can run so here uh first command we can run as just for upgrading pip so if you don't know what bib is so bip is just a package manager that helps us install python packages so let me paste it here and see if i don't have the latest version apparently not so it's just gonna download it and install it so when i tried to upgrade pip on my machine it gave me a basically an access error but uh you if you have that error you can just run the same command but you add dash dash user at the end so for me it says requirements already satisfied in fact i didn't really need to run this command i already had the last version of pip already installed but you can run it just for the sake of it and now we're gonna use this command so pip install tensorflow and here i just want to mention something really quickly as you can see it mentions that the current stable release for cpu and gpu so when you run this command it's gonna install the stable versions for both cpu and gpu support before i can't remember before which release from tensorflow but before in order to install the gpu version you would add tensorflow gpu so you specifically mentioned that you want to install tensorflow with gpu support but apparently now when you when you just run this command it's gonna install the stability release for cpu and gpu so i'm gonna go back to my virtual environment here i'm just gonna paste it pip install tensorflow and let's run this command again just to mention this important point whatever we're installing here is being installed inside our virtual environment and not inside our system so this might take a little time depending on your internet speed and on your system so when the download and installation is finished i'll be back to show you what we can do next so now i have tensorflow downloaded and installed inside my virtual environment and i can verify that for example by running python console here and just trying to import tensorflow and it should be imported without any problems and just to show you that we're actually not installing things inside our uh on our system directly here if i run python and i do import tensorflow you see that module name or module not found here so it doesn't find the tensorflow module because it's not installed on my system i'm just showing you this here because from time to time you might face a problem like this and you start asking yourself why why don't i have tensorflow when you have already installed it but just make sure that if you install it in one specific virtual environment you should be using that virtual environment to run your code and this just this small detail here could help you solve this kind of errors and now that we have it what i would like to do is just run a command tensorflow.config.list physical devices so what this command is going to do is to show me on my machine which devices can i use to train deep learning models using tensorflow so as you can see here i have two devices there is the cpu and there is the gpu there's only one gpu as you can see for you it's very likely if it's the first time you're using tensorflow it's very likely that you will not have this second part here you won't have the gpu and this could be for several reasons for example you may not have an nvidia gpu on your machine so that's a common reason why you wouldn't have this the second common reason is that you might have an nvidia gpu but it's not it doesn't have support for training tensorflow or deep learning models in general on that gpu and the if you if you're not in one of these two categories then most likely you're not seeing the gpu because you just haven't set up what's necessary in order for you to run the training on the gpu and for me i have already done the necessary setup and i do have a nvidia gpu that can run the training that's why i'm seeing it here for you if you don't fall in the first two categories and you know that you can run the training on your gpu then i'll show you next what are the necessary uh hardware requirements and software requirements what are the things that you need to install in order to be able to use your gpu if you want to be able to run the training on your gpu and you know that you have nvidia gpu one thing you can do is to follow the right process that's mentioned in the official tensorflow installation page so here if you go to the download package which is what we just used if you go to gpu guide you can open this page here and when we when you use a bib package like we're doing you need to have some hardware requirements and also software requirements so the first thing let's check the hardware requirements so as you can see here it mentions that there are some nvidia cards where you can that are supported for you to train deep learning models on them so if you open this list here of cards that are that have cuda enabled architectures you can see a list of devices here and you can try to find whether your device is inside this list or not so in fact funnily enough for me i actually have a card geforce gtx 1050 ti and it doesn't exist on this list but i still can run the training on it anyway i remember the first time when i checked the list i didn't find it then i just started searching online i found that many people said yes they have the same card and they were able to run the training on it so this is just a small remark but uh if you find your card here then that's you can definitely run the training on the gpu so that's the first thing you need to check uh if you have that then most likely you can run the training and then you have some some remarks mentioned here for example for gpus with unsupported cuda architectures or to avoid compilation use different versions of nvidia so you can check these if you uh if you fall into one of these categories but for me usually just checking whether the architecture works is enough and then for the software requirements this is where in fact you need to have some software installed before your nvidia card can recognize that you can do the training using tensorflow so the first thing is the nvidia gpu drivers so here it says cuda 11.2 requires 450 or higher so you can go to the website here and for me it has automatically detected in fact i think that it's because i have already done this so maybe it's saved in my in the cookies but uh for you if it doesn't if you don't see your machine directly here you can just go search for it manually so you choose the product type the product series and then at the end you just click search and it's gonna give you the download link and again this is when you download this and you start uh installing it okay it does didn't start automatically let me click again okay now when you have this on your machine you just run the installation process there's nothing really special you just go next next next and you install everything so that's the first thing that you need to do and then you have cuda toolkits that you need to install and just i want to mention something here because uh i think a lot of people make a mistake when they start installing all of this software so here specifically it mentions for example cuda 11.2 and also tensorflow supports cuda 11.2 so any version of tensorflow that's higher than 2.5 it requires cuda 11.2 so for us we have installed tensorflow so if i do for example tensorflow and then i just run or check the version of tensorflow insert flow version so here you can see i have version 2.6.0 so i do fall into this category i need to have cuda 11.2 uh installed here when you go to the cuda toolkit archive you just don't take the latest version for example i see many people do this mistake they just go to the latest version but here if it's mentioned that you need to use 11.2 then you need to look for 11.2 and here what i usually do is i take the latest version of 11.2 because here you can see there's 11.2.0.1.2 so i just take this one here and then you just choose your operating system the architecture is only one here version 10 of windows and then you can choose one of two options there is the local installer and the network installer the difference is that the local installer has a large file which doesn't require a require internet access i mean after you download it you don't require uh internet access in order to install cuda toolkit and this one is a smaller file but it does require internet access if you have good internet then i suggest you just download this small file if not then you can download the largest file here and again there is nothing really special about this you just download it and then you go next next next there is and then you have the cuda toolkit installed on your machine so that's the second part uh for the cup tie you don't need to do that it mentions here that it ships with cuda toolkit so if you have the kuda toolkit you already have kubtai and then you need to install crew dnn sdk 8.1.0 so if we click this let me check here so download cooldmn and in fact for you to download that file or that folder you need to be a member on the website and this is you can do this for free you can just log in if you already have an account or you can join now for example and you just uh use your email and there is nothing special here you just join the the website so for me i already have a an account here so once i log into my account i get this web page here so i'm just going to agree to the terms and then here you can download the version for uh or kudianon 8.2.4 and in fact here when you look at this you see that there is for cuda 11.4 and this is for cuda 11.2 let's see if we can find for cuda 11.2 so there is none in that case we're just gonna use the latest version here so let me just go back here and check okay we have the cuda versions yes apparently okay 11.1 here yeah in fact there is so this is again this is a sometimes a problem that many people face so here we need hoo dnn sdk 8.1.0 when i clicked on the qdnn versions i get to this website here and here if you just don't take the latest version you need to look for your own specific version so here i have uh the version 8.1.0 which is what we need so for eight point one point zero we have one for cuda 10.2 and then one for cuda point eleven point zero point one and point two for us we have cuda 11.2 as you remember so we need this this version here so these are small details but just make sure you follow them so that you don't face a problem later many times i find people saying that they installed everything but still their car does not recognize uh cannot do the training with tensorflow but usually the problem just comes from one of these small details so if you click this then you can choose the right software for your machine so for example we are on a windows machine so i can just click goody and then library for windows and this is going to download a zipped folder and in fact i already have it on my machine here now i'm going to show you what what you need to do with this folder so when you installed cuda 11.2 it was installed in this path here so if you go to c program files nvidia gpu computing toolkit then you go to cuda for me i have several versions of cruda because from time to time i would upgrade or use the newest version of tensorflow and then many times it requires new version of cuda so i would download and install the right cuda version for you if you've never did this before then probably you will have only one version which is 11.2 and now what we're gonna do with the uh the zipped folder that you downloaded what you need to do is to unzip it so when you unzip it you're gonna get a folder called cuda inside of it you're gonna have these folders and this file here so what we're gonna do now is basically just copy the contents from these folders to the corresponding folders in cuda 11.2 so for example you would go to the bin folder copy everything then go here and paste everything for me i've already copied and pasted these so i'm not going to do it but this is the only thing you need to do then you go to the include folder copy everything paste it in the include folder and last you go to the lib folder you copy everything inside x64 and then the same thing here lib you go to x64 and then you paste everything there when you do this you can go back to your terminal so what i would suggest is to close and reopen the terminal and then if you run the same command that we that i showed you before so for example let's just do it again import tensorflow so here if we if we run tensorflow.list or config not list physical devices now you should be able to see this gpu part here so now i have shown you exactly the full process of installing a tensorflow so that you can use it with your gpu if you have a cuda enabled nvidia gpu and this is gonna allow you to run the train the training of your deep learning models much much faster which is uh almost always recommended and with this let's now look at what we can start adding from the coding part so that we can start exploring tensorflow library so here we're gonna start with the first classification task using tensorflow and it's actually a classical task so basically what we want to achieve is uh to build a system that can take images like this one here or here or here and at the output it would tell us whether there is a three or there's a seven there's a zero inside that image so our data set looks something like this of course these think of these are as separate small images just like these ones here these are just put together here for visualizations purposes so our data set has so many zeros so many ones twos until the digit nine and we want to build a system that can take an image like this at the at the start or as an input and adds the at the output we want to get the value that's written inside of that uh of that image so it's always good to really think about whatever task you're trying to achieve before you start the coding part so here this is just a quick overview of what we want to achieve and in fact this amnest test is a uh you can say as a tradition in the deep learning community it's just like a hello world program when you're learning a new programming language so now that you have hopefully understood the purpose of this task let's get into the code and see what how we can achieve this this goal here now let's get to the coding part so the first thing i'm going to do is just add a new file i'm going to call for example mnist example.buy and it's automatically opened on this side here so let's start by importing for example tensorflow since we will definitely need it later and one thing we gonna add from the start is just check if name equals main then we're going to run the code so if you're not familiar with this python part here just means that if i run my script then i want this part here to be rand and if i import it for example i can import my file into another script and in that case this part will not be called it's a good habit to always add this one on your scripts when you want to separate the behaviors between importing those scripts and running those scripts so the first thing that you might notice here is that it says tensorflow is not accessed it doesn't find tensorflow so here in fact what's happening is that if if you run your script from the terminal then there will be no problem so if i do python and this example dot y nothing is going to happen but there will be no problem that's what i mean but here if you try to run it using the run button here then you see it says no module named tensorflow and in fact what just happened is that vs code has opened a new terminal window automatically and it tried to run this script using our python that exists on our system and not inside our virtual environment so in fact we still have our virtual environment here and this is the new terminal window that was started by vs code so in order to run your scripts so you can in fact just ignore these these warnings here and you just run your script from the terminal but if this uh is annoying for you as it is for me you can fix it by actually telling vs code that this code here that we're writing is going to be around inside our virtual environment tf2vm so in order to do that you can click ctrl shift p on your keyboard so again ctrl shift p you push them you click them together and then you just search for interpreter and you choose python select interpreter and here is going to give you a list of a different python executables that it found on your system and here we're going to look for the virtual environment that we created so it's called tf2bn so let's just go step by step here try to find it so i have many virtual environments that's why it's not clear okay it's this one for you you may not have all of these virtual environments so it will be quick to spot which one is the environment that you created so i'm gonna click on tf2 vm and now as you can see vs code recognizes tensorflow because it's installed inside our virtual environment and i can click this one here and as you can see the program runs correctly with no problem so now let's go to our code editor here and let's start looking into what we can do for our task that we defined previously so one thing we can start doing is by actually importing the data set so in fact if you're doing a new project with your own data set it may not be this easy but since this is the first example of the first tutorial we're gonna do we're just gonna use the easiest and the quickest things so that we can put something together and see uh see basically the workflow of a tensorflow program so what we're gonna do is use tensorflow.js dot data sets or let me just check yes in fact it should be data sets dot load or dot mnist dot load data so let me just do this and in fact what this is going to do is load our data but we need to tell it where to store it so for us we're just going to use x train for the training data set and y train for the corresponding labels and then we're gonna use x test and y test so what this is going to do is load the mnist data set in fact if it's not if it doesn't exist on your system if it's the first time you'll be running this program here then it will download it first and then it's gonna load it into these variables that we defined here and it's already split into two parts one for training and one for testing so let's run this command or this program here and just i would like to print the shapes of these uh these variables that we defined and that we're going to load our data set in them so xtrain dot shape and let me just run this and i'm just gonna paste this three times and i want to do this for every variable that we have here ctrl c ctrl v so now let me just run my script again from the terminal and let's see what we get so as you can see it has loaded the data set and it has put them into these variables and we can see the shapes or basically the size of the data set so as you can see for the training part there are 60 000 images each image is of 28 by 28 pixels the labels there are sixty thousand labels because each image has its own label and by the label here i mean that if an image contains the number three in it then the label will be the number three and for the test part we have 10 000 images and of course their corresponding 10 000 labels so just just a quickly to go through why do we separate our data set into these two parts so in fact in deep learning what we do is we do some sort of cross validation there are many types of cross validation the simplest one is to split the data set into two parts one you're gonna use to train your model and one you're gonna use to validate your model and the difference is that during the training these are the examples that will be used and at the end of each epoch uh what's going to happen is that the neural network is going to run one forward pass of these test examples and this is going to give you an idea of how your neural network is performing on data that's not used during the training so it's this is going to give you an idea of uh how well the network is learning so this is just you can keep it in mind there's a whole theory behind this but i just want to go quickly through this so that you understand why do we have these two different splits so now what is usually good to do is to explore more your data set so what we have done here is some sort of an exploration we basically checked the size of our data set we now know that each image is 28 by 28 pixels but it's also good to plot some of the examples and their corresponding labels so for that i'm just going to create a help helping function so i'm going to call it for example display some examples and what's going to take as parameters are the examples or data points and the labels so basically what i want to do here is create a figure and add many of the images randomly i'll choose them randomly from the data set and then i'll add them inside uh this figure and then i'm gonna plot this figure so that we can see those images so one thing you can use which is a very famous package in python is a matplotlib so usually we just call it plt for short and in order to do that you need to import matte blot lib dot pi as plt and then you can use this package to do all sorts of things like creating figures and adding different images on those figures so as you can see here visual studio code does not recognize this package here because in fact we don't have it installed inside our virtual machine so before we go anywhere let's just installed it and to install any new package you can just run pip install and the name of the package so matt lard lib and usually i install packages on the go so whenever i need the package i installed it i don't like to install many things in the beginning because i don't know beforehand which ones which of the packages i'll definitely need and which ones i won't need so usually i just write my code and whenever i need a package i install it on the spot so here let me run pip install map.lib so it shouldn't take long okay it's installed and now it should be recognized i guess maybe it's not refreshed automatically but now we can use plt so we're going to start by creating a figure and we're going to give it a fix size of let's say 10 by 10 and then what we're gonna do is basically plot 25 images so it's going to be a grid of five by five so we're gonna do a for loop so for i in range 25 so i'm choosing 25 here you can choose whatever number you like for me i think 25 images is already enough to get an idea about your data set and here the first thing i'm gonna do is randomly choose so each time i'm going to randomly choose an index from my data set so let me just code it and it's going to be much clearer so i'm going to do index and numpy okay i don't have numpy imported so let me just import numpy snp and here i'm just gonna do np dot random dot rant in so i want to randomly choose an integer and i really like this uh kind of a tooltip that comes when you're coding on visual studio code and it basically gives you a small documentation about that function and what you can do with it so here for this function we need to give it a lower end and the higher end and it's going to use an integer between those two numbers so here i'm going to choose something between 0 and the size of so examples dot shape and i'm going to do 0 and here minus 1. so what am i doing here so if you remember here whether you choose the training set or the test set for example the training set there is 60 000 images so i want to choose a number between 0 and basically 59 999 because that would represent the number corresponding to the last image and here this is all i'm doing so here i'm basically choosing between zero and uh and sixty thousand minus one and then what i'm gonna do is use this index to get the corresponding image so now i can do examples index and then i want to get also the corresponding label so i'm gonna do labels and index and now i'm gonna plot that specific image so for that i'm gonna use plt subplot and just make sure i think there are there is subplot and subplots but what you want to use a subplot without the s and here we gonna give it basically the grid size so it's a 5x5 because we want to plot 25 images and here this is the number of the image and here i'm gonna do plot i'm gonna first plot the sorry here the title so the title of each image i want it to be the corresponding label so and it also needs to be into a string sorry so here i'm just gonna transform my label into a string so this is going to be the title of each uh image and then i'm going to add an mshow method to show the corresponding image so here i'm going to do show my image and at the end you need to tell python that show all of the figure now so now that we have this function coded let's try it out here by calling the name of the function then let's run it on the train set so here i'm gonna run it on the xtrain and y train so let me clear the terminal quickly and do python mnistexample.buy so now as you can see we get these images in a 5x5 grid and we have all of these examples now one thing you you might observe is that we have these images that have this kind of weird color and also we have basically the space between the images is too small so we don't clearly see that okay this is this has a label of nine but it's not very clear so let's fix these two problems quickly now so first of all we know that our images are grayscale images they're not rgb images because here we see that the size is 28 by 28 which means there's only one channel and there are no there are three channels so it's not an rgb image so in order to tell matplotlib that this is a grayscale image we can use the cmap argument here and we can tell it that it's actually gray and to fix the problem of the layouts what we can do is use plt dot type layout and this is gonna automatically add more space so that we can see the labels and the images uh in a much more understandable way so let me run the command again python and this example.buy and as you can see now it looks much more clear these are the images that exist inside our data set they are grayscale images and above each one we have the corresponding label so this is a two this is a one a one this is a four a three here a six so we have all of these examples and why do we do this it's just a way to basically quickly check your data set sometimes what happens is that when you're building your own data set and you do this kind of exploration you quickly spot some things for example you might spot that this is a zero but uh but it's not labeled correctly for example you must see that this is labeled as nine when it's actually a zero so that's gonna give you basically just quick hint that there is there are problems with your data set and you can maybe go fix them quickly and the good thing is that whenever you run this script it's going to choose randomly different images so you can see different parts of your data set each time you run the script so here we have the two six one for example in the beginning just so that we can remember and let's run it again okay and now as you can see we have five 572 instead of the 261 i think it was before so this is just about the exploration of your data set as i mentioned it's always a good idea to do these quick exploration steps before we start doing anything and now that we see that okay our data set is more or less okay let's see what we can do next in regards of creating a model and later training it now that we have our data set loaded and ready let's see how we can build our neural network so in fact tensorflow gives you access to so many layers that you can use to build your neural network so if for example you do from tensorflow import here you have many different layers that you can import for our classification task and for many of the computer vision tasks we usually have a set of layers that we basically always use so these layers are conf 2d so this is a 2d convolution we also uh always use of course an input layer so here i'm going to add input so this is a layer that basically gets our input and then passes it to the rest of the layers and there is also the dense layer so it's also called fully connected layer we have the max pool 2d so since we're doing 2d convolutions we're going to combine them with a max pool 2d and it's always good to use batch normalization to normalize our batches and also we might want to use a flattened layer which basically takes the output from some multidimensional layers like a convolutional layer and then it just flattens them and puts them in some sort of a vector and also in fact in many cases you can use just global average pooling or global sorry here global average pool 2d so this layer also can uh can work in a way that's closely similar with flattened but they're different in the way they do it so flatten basically takes all of the uh all of the outputs of a previous layer puts them into a vector but in the global average pooling there is some sort of a compute computation of the mean based on some axis and then the output usually is much smaller than the output given by a flattened layer so here i'm basically mentioning some of the basic layers that we use in a neural network for doing image processing or doing uh deep learning for images type of things so for classification this is one case there's also object detection image segmentation all of these tasks usually you're going to find this type of layers in them and sometimes you find more complex layers and sometimes those complex layers are just basically stacking together different uh basic layers like these ones here so these are the layers that we're gonna use to build our neural network and now let's see how we can build that neural network there are several ways and mainly three ways that you can build a neural network in tensorflow we're gonna explore them now one by one the three approaches that tensorflow gives us or provides us in order to be able to build deep learning models are the following so the first approach is by using tensorflow.keras dot sequential so you can call it the sequential way this is the easiest way to build a deep learning model you just stack different layers together and we're gonna do this in a minute the second approach is by what's called the functional way or the functional approach so basically here you build a function that returns a deep learning model or just let's just write a model and the third approach is to basically inherit from a base class this base class is in tensorflow.keras dot model so you inherit from this class and then you you reimplement or you overwrite some of the methods of this class and then you can have a deep learning model so inherit from this class so these are the three different approaches let's start with the first one so in order to build a deep learning model architecture using this sequential class in fact it's very easy you just call for example let's call our model model and then you call this sequential class and show and then you can start stacking layers here so what we start with always is an input layer so here we're going to use the input layer that we imported here and what this input layer takes as a an argument it takes many arguments but the main one that we need to set is the shape of the input so as you remember our images are 28 by 28 and they only have uh one channel they are grayscale images so there's only one channel this means that our input needs to be 28 by 28 by one one represents only one channel and then the second layer that we're gonna add here is going to take as input this layer and usually when you're doing image classification tasks or other computer vision related tasks in deep learning you take the image and then you pass it through some convolutional layers so here we're going to do the same thing so conf 2d and since this is the the first convolutional layer let's go through some of the parameters that exist in this layer so one thing that i like to do is to just hold the ctrl key and then hover over that class and as you can see visual studio code can give you so many information about that class and here the mandatory parameters that we need to set are filters and the kernel size and then you have the rest of the parameters already set you can change them if you like and here even gives you some examples and they give some explanations of the different different parameters so the number of filters here so let's start with for example 32 filters of size 3 by 3 and here let's also set an activation function and i'm going to use a real u activation function so here there are all of these parameters what do they basically represent for us so this number here the 32 represents how many filters or how many uh basically well each filter has a set of weights and in fact here we're defining how many weights values are in that filter so this means that we're creating 32 filters each filter filter is three by three of size and then we're choosing an activation function here we're choosing rectified linear unit there are several activation functions and all of these parameters you might be thinking why do we choose 32 and not another one why do we choose three by three and this is real you why do we choose these specific parameters where in fact these parameters are hyperparameters you might have heard of this term and these are parameters that you can change and each time you change and you train your new deep learning model and you see whether the accuracy has improved or not so far there's no very solid theory on how many parameters you should choose in each phase so it's really an experimental approach you choose some of these parameters you train your model and then you go change them again and train your model again and see if you get a better accuracy now i am choosing some of these parameters based on much previous work that has been done by researchers if you see a lot of the neural networks used for classification they would start with something very similar to this so i am following on their footsteps because i know that they have done so many experimentations with those neural networks so here i'm going to start with a convolutional layer usually what we do is we create a block of convolution where you have a convolutional layer then you would use max pool 2d and then you would use a batch normalization layer and then you can do the same thing again another convolution max pool normalization and you just change for example the number of filters or the filter size here or the activation function so what this max pool 2d does is that it takes the output of the convolution and it looks in both directions and based on the pool size so again let's maybe go to the documentation that's written inside the code here what you have is let me just go way down you have the pool size and it's two by two what this means is that the output of the convolution we're gonna look a two by two window of the output of this layer and each time we're gonna use we're gonna keep only the max value between those four values because it's a two by two window so there are four values we only keep the max value between all of those four values and here you can in fact change this you can set the pull size to four by four which means it's a four by four window where you keep only the max value in that in that window and again why do we keep two by two why don't we change it as these are also hyper parameters that you can change and each time you change you retrain your model and see whether you get better results so this also goes into that experimental experimentation approach that we do at uh as machine learning engineers or data scientists so this is one way to do things and the in fact you're not you're not let's say you don't strictly need to follow these approaches you can experiment for example you can add a convolution to the here and you for example let's do 64 and the three by three and activation is uh is it activations or activation sorry let me just check again so here let me go into the parameters activation now it's only is without an s okay so you can do this you can remove it you can add another layer feel free to experiment with this uh architecture because this is where deep learning is is different than uh conventional software engineering you can do so much experimentation with all of these things and you will have different results and only based on those results you can say that your model is better than the previous ones or not so here let's for example keep it like this and let's create another stack of layers here so conf 2d and i'm gonna for example use 128 filters keep them three by three and the activation is the same as before so real you and this approach here where i take 32 64 and 128 is also something that i've seen being done in research so many neural network architectures follow this kind of approach where each time they double the number of filters so i'm choosing to do that but in fact nothing is stopping you from using other numbers here you can use for example 110 or just 100 or whatever you like so i'm gonna finish the architecture using the same approach here so now i have these layers that are very similar so we take an input we do some convolution here then the second convolution we max pool we do batch normalization then again we do convolution max polling batch normalization and here just if i didn't mention batch normalization before so batch normalization is basically looks at your batches and does normalization on that batch this is an operation that is that is inspired from the uh normalization of the input which is something that we're going to do a little later so basically researchers have noticed that when you normalize your input this helps the gradient the computational of the gradient and it helps when we're trying to minimize our cost function so some other researchers said okay if we're normalizing our inputs why not normalize the outputs of the layers inside the neural network so this is why they created this new type of layer where it takes the output of this previous layer and then it normalized the the batches so each time we have a new batch we normalize the data inside that batch and finally since we finished the convolutional parts here which are the main layers that's going to affect our neural network because they can learn so much and you can check some of the papers about this maybe the young lacoon paper which was the first one that introduced the convolution or 2d convolution and you can see how much we're saving in memory and how much they actually improved the results now let's get to the part where we basically built what comes later at our neural network architecture so here i'm going to use a global average pooling and then i'm gonna add a dense layer i'm gonna add a dense layer of 64 units and an activation of real you so i'm gonna keep the same activation function as before and then i'm gonna add a final layer here and now i'm gonna use an activation called soft max and with this i have actually finished building my neural network using the sequential approach and let me just go through these layers quickly and what we're doing here so the global average pool 2d it takes the output from this patch normalization layer and it computes the average of those values according to some axes and then we're going to get a set of values here and then those values are going to be fed to a dense layer this dense layer you can think of it as a vector containing values so there are no filters like the convolution like 2d convolution here and finally we're going to add another dense layer which has 10 values and this will be the output layer of our neural network and we're choosing 10 here because in our data set the amnes data set there are 10 different classes that we can have from zero to nine so there are ten classes so again if we go back to the first uh the first time i spoke about our classification system i mentioned that we want to build the system that takes the image as an input and we want it to tell us at the output which of the ten categories so is it zero one or until nine so this model here reflects exactly that we're taking an input image and the output is a set of 10 values which will be in fact 10 probabilities we're getting probabilities because we're using this activation function called soft max so the soft max gives us 10 probabilities and they sum up to one so what we're going to do is look at those probabilities and look at the highest one and that would correspond to the class predicted by our model so this basically explains why we're using 10 here and we're using the softmax activation function here and at this stage here you don't have uh as many options as you would have in these layers for example in this dense layer you can choose a different value here so 128 132 it doesn't matter you can change the activation to something else but at this level here here we are at the output of our layer and we know that we want to predict one of 10 values so it has to be 10 here and also we want those values to be probabilities so that they reflect what the model thinks about our image that's why we're using soft max here apart from this there's also the input layer where you don't have much choice you have to set the shape that corresponds to the shape of your dataset so we know that our images have 28 by 28 pixels so it has to be 28 by 28 input we also know that our images are grayscale so they have only one channel that's why we have to choose one here so apart from the inputs and the output layers these layers here you can change so many parameters in them and each time you train your model and see whether it gives better results or not so with this we're actually done building our first neural network using the sequential approach now what we're going to do is continue creating the rest of our program so that we can make use of this model train it and then test it on some of the examples in our data set so now that our model architecture is ready let's look at how we basically set up the rest of the things so that we can run the training so the first thing i want to do here is just basically omit this part here and one thing that i like to do is just use some kind of switchers here so it falls so that i don't like to comment the code that's uh that's important and also i don't like to delete it so i just like to use this kind of approach here and now what we're going to do is the first thing we need to consider is that our data is in some sort of a a row uh manner this is how we got it from the load data method here but before we start to pass it through our model so that it can start to learn we need to normalize it so in order to normalize the data we're gonna use some basically division approach so here we're going to do x strain and usually what you would do is just divide it by 255 because our values are between 0 and 255 to 255 represents white and 0 represents black and the thing is our data set here is an unsigned integer 8bit data type before we do that before we do the division i mean we need to basically transform it into another type so as type and here i'm gonna do float 32 bits and why are we doing this is because if we don't transform it into this float 32bit type what's going to happen is that the values that are between 0 and 255 so let's say 2 200 for example when you divide it it's gonna become zero instead of zero point something that's because the data is an unsigned integer 8 bits format but when we turn it into 32 float format what's going to happen is that we're going to get that zero point uh something which will not be absolute zero which is which is exactly what we want to have here and we're gonna do the same thing for the test set so test as type i'm gonna do float 32 and then divide it by 255. so again why do we do this normalization approach here uh it just it's been uh by experimentation we've seen that when you normalize your data the gradient moves faster towards the global minimum of your cost function but i have to say that i have done so many experimentations where i didn't normalize the data and it still worked okay there wasn't that much difference between when i normalized and when i didn't normalize the data but here just for the sake of good practices i'm gonna keep the normalization here and uh one thing also that we need to do is to basically make our data set in a in in such a way that our model can accept it in its input so as you can see the input model expects expects 28 by 28 by one and here as you can see our data set uh of course there are these sixty thousand images this does not matter for our model it's gonna take them batch by batch but here we see that our arrays are formatted in such a way where we only have two dimensions so 28 by 28 but we want to have 28 by 28 by one so in order to remedy this we're gonna do x train equals mp dot expand dims so this is to expand the dimensions of an array and we're going to give it the same array so x strain and then we can use the axis number so here we want to expand the last dimension so we know that our array has here three dimensions so of course when they are three that means this is dimension zero this is one this is two so we can use for example three here and that would work but uh you can also do just minus one which means we want to add dimension at the end so here we're going to do x test the same way so let me just copy this and here i'm gonna do test so now in fact our data let me just maybe copy this and paste it again just so that you can see the difference so now our data is normalized because we use this uh these operations here and but also the dimensions have been changed so let me just clear this then run mnist example just quickly to see what we have differently so as you can see we had this shape for our data set before now we have this and this is the right shape to use with our architecture because again our architecture accepts 28 by 28 by one of course the first dimension that represents the batches it doesn't matter so we don't mention it here so uh it's not gonna cause any problems but if we don't have our data set in this format it's gonna actually pose a problem and maybe you will have this question of why don't we just remove this here why don't we just do this and then we don't need to expand the dimensions in fact if you do this what's going to happen is that the combo you're going to have a problem with this layer here because the 2d convolution expects a 4d fourdimensional tensor and what this means is that here we have the this tensor that represents the shape of the data but there will also be a dimension here it's hidden here it's not shown which represents the batches so we can take several images at the same time so with adding that dimension for the batches we're going to have four dimensions here and then the convolution 2d will not complain about dimensions so that's why we're doing this here and that's why we're expanding the dimensions here and okay let me just maybe remove this for now we don't need it and now that we have all of these uh all of our data ready to be uh passed through our model let's compile our model here and by compiling i means i mean we're gonna set some of the important aspects of the training so we're going to do model that compile and then inside this function here you have several parameters that you need to set so there's the optimizer and there is also the loss and finally there are the metrics so these are the three important parameters that you need to set so here the optimizer what it represents is the algorithm that we're gonna use in order to uh to optimize our cost function and by optimizing i mean trying to find the global minimum of our cost function so for this we have several options in tensorflow the most famous one is adam which we will be using but there are or several uh optimizers in tensorflow so let me just maybe try to okay sometimes it's not giving me option to see uh to see all of the parameters but here we can set the parameter atom there's also ada and there are several others that you can use so here if you go to the tensorflow documentation and the tf.keras.optimizers you see that you have all of these options that you can choose from there's the ada delta other grad adam so on so forth you have the stochastic gradient descent here so you have all these options that you can use again which one to use really depends on uh the case that you are uh you are doing and this also goes into the hyper parameters part where you for example you might choose adam train your model and then you might change that and choose for example an adam and you might get better results using this uh other optimizer so it's really an experimental approach and i again i do encourage you to change things and see what you can get with those different parameters so let me uh go back to our example here so here when you want to pass a parameter to this argument here optimizer you can just put it between parentheses or between these quotation marks here and you just write the name and you can of course choose any of the other ones and write it here and this is how you set the optimizer the second thing which is very important that we need to set is the loss function so for the loss for every task in uh in deep learning you have a set of loss functions that you can use for that specific task so in classification one of the widely used loss functions is called cross entropy so these are two different uh two different words cross entropy and this is actually a function that uses uh probabilities and it computes some sort of probabilities that helps penalize your neural network weights when they are when the when it predicts the wrong thing and also helps it know which values are predicted correctly so there is a whole a lot of research about cross entropy that you can look up online but in fact just in tensorflow you have three different loss functions that have the name cross entropy in them so here again i'm looking at the documentation on in tensorflow and if i didn't mention it before i would very much recommend looking at this documentation from time to time it's uh written in a quite understandable manner and you can get a lot of information from it so again for cross entropy if you go to the documentation you'll see that you have binary cross entropy you have categorical cross entropy and also you have sparse categorical cross entropy so all of them they have cross entropy in them so they use this cross entropy approach but each one of these three loss functions uses it differently and for us here we're gonna use categorical cross entropy and we're gonna use spas categorical cross entropy and i'll show you what's the difference between them so here let me start by using catigorical cross entropy and i'll explain a little bit later do we why would you want to use categorical and why would you want to use sparse categorical for now let me just finish this part here for compiling our model so for the metrics which is the last argument here we need to set a metric for which the uh basically we're going to guide our training so that our model knows that it's becoming better or worse so here usually for classification we would use the accuracy so the accuracy represents how many examples are we predicting correctly from all of the examples that exist in the data sets so let's say you have a data set of 100 examples let's say images and when you do the prediction prediction you get let's say 88 images correct and the rest are incorrect in this case the accuracy would be 88 over 100 which is 88 this means the accuracy is 88 so this means that your model makes a good prediction 88 of the time so if now we have a thousand images and around 880 images will be correctly predicted so for classification this is what we would need as a metric and of course here i am using the usual values that are used in classification but there's a lot of advanced things that you can do for example you can create your uh your own loss function your own optimizer your own metrics and then you can pass it here as a parameter but this is a little bit advanced so for now i'm just gonna go through the steps with the usual values that are used in the deep learning community so now that we have compiled our model the only thing that's left is to fit the data into our model so for that we're going to call model fit and the fit function uh takes several uh parameters the first parameter is the x strain so the uh the images and then it's gonna take the y train so the labels corresponding to those images it's also going to take a batch size so this batch size represents how many images is our model going to see each time so here you can for example if you set it to one this means that we're gonna pass one image uh at a time to our model but we usually don't do this we usually choose a larger number of images to pass them through our model so here for example we can use 64. this is also one of the hyper parameters that you can change and see how your model will perform based on different and also we're gonna set the parameter epochs here and let's say i'm gonna choose for example only three epochs so that we can quickly do some experimentation and the epoch represents one epoch in fact represents that the fact that your model has seen all of your data set once so when all of your images are passed through the model if they're if this is done once then this is one epoch if your model has seen them twice then these are two epochs so on and so forth so basically here we're defining the number of times that your model is going to look at all of your data sets this is also a hyper parameter so here in fact we can keep it just like this and we can run the training which means that all of the images that exist in the train set are going to be used for training but one thing that that you can do in fact is use a validation split so a validation split here for example let's say i'm going to use 0.2 this means that i want to use 20 of the train images for validation so again what would what does validation mean what what is it why is it different than the test set for example in fact in the when doing cross usually you split your data into the train the validation and the test sets split them into these three parts the train set will be used to train the model the validation will be used at the end of every epoch we're gonna run the model on the validation not for training but just for prediction so that we can see how well the model is doing on data that it did not see it did not use to train so these in fact these two splits are the ones we use to fine tune our our model so when i mentioned hyper parameters i mentioned that this for example hyperparameter this is a hyper parameter the values here are hyper parameters but when do you say that one or a set of hyper parameters is better than another set of hyper parameters in fact you do this using the validation split so whenever you train you test on the validation split or on the validation part and then you see whether that metric that you're using has improved or not so in our case the metric is the accuracy is the accuracy uh becoming better when we change the hyper parameters so you do one training with a set of training parameters you see the validation set is it good or not and then you change the hyper parameters you run the training again and you look at the validation accuracy did it become better they become worse and you keep doing uh this loop of experimentation until you get to a point where you see that you're not improving anymore on the validation set and then only when you're done with this experimentation phase that you keep the final model that works well on the validation set and then you test it on the test set so with this parameter here validation splits what we're gonna do is split our training set into two parts eighty percent of the training images or the images that exist in this uh extreme variable are going to be used for training and 20 of them are going to be used for validation and then we're going to keep the tests set here until the end when we finish the training and we're going to use it to evaluate our model to see how well it performed on images that it has never seen so the images here whether they are in the train set or the validation set the model is gonna see them at some point during the training but the test set we're not gonna use it in this function here which means that the model will never see it during the training and is we can uh we can pass that set of images to the model only at the end of the training so for example we can pass it here model that evaluate and we can give it the x test apply test and we can also choose a batch size here let's just keep it the same as before so again just to summarize quickly what we're doing here we're taking the train images so 60 000 images we're gonna split them into two parts eighty percent of the images so eighty percent of the sixty thousand images is going to be used for training the rest which is 20 is going to be used for validation and the train set or the test says sorry which is the 10 000 images here are going to be used at the end when the model has finished training so at this point here this is model training once this line here is done then the model is trained and here we can do evaluation on test set this set here the x test is going to help us identify how well our model is going to perform on data that it has never seen because usually what you do is you train a model you put it in production and then you start testing it on new data that's coming from for example your users if you have some sort of a web product and those examples the model has never seen them so in order to get an idea of how well your model will perform on examples like that you keep this part here separate from the training and the validation you keep it until the end and then at the end you test your model on it so this is the goal of this test set and the the goal of splitting the this set here into two parts now we have a complete program that we can run and we can train our model and we can see how well it performs on train validation and test sets so let us do that right now let me just clear this and i'm gonna run our example here so this might take some time depending on your machine but usually it shouldn't take that long well in fact when i ran the code i got into this error here and in fact i wanted you to see this error because it's a very common error and uh i wanted to use it in order to explain to you why you would want to use categorical cross entropy or sparse cross entropy in fact this error is coming from uh from this specific detail here so when you have a data set like we have here if you use categorical cross entropy then in fact this loss function is going to expect your labels which are these ones uh the x the y train and the y test is going to expect them to be one heart encoded and if you don't want to use one heart encoding then you need to use a different loss function which is the sparse categorical cross entropy so this is a minor detail but it's very important so if you go in fact to the documentation so here in the documentation i just opened uh the two pages for categorical cross entropy and for sparse categorical cross entropy so for the categorical cross entropy here although uh it's not specifically mentioned here but in fact if you look at the example you see that this is how you would use categorical cross entropy these this this is how your labels need to be set so in fact this is one label for one image which is in the form of a vector and in that vector the one will correspond to the right class and the zeros will correspond to the wrong classes so if you have three classes then you need to set one to the right class and zeros for the rest in our case if we wanted to do one hot encoding then for example if you have let me just do this quick example here if you have label two then one hat encoding will be like this it will be a vector of 0 0 1 then 0 0 0 and 8 9 10. this will be the corresponding label for that for that image so the image contains the number two but your label needs to be like this and this is called one hot encoding of your labels now uh if in the case uh how we're using our data set in fact we're not using one hot encoding so our labels are just encoded as numbers so if it's the second if it's number two then the corresponding label is two if it's zero then it's zero so on and so forth we're not using onehalf encoding so if you use categorical crossentropy then you have to use onehalf encoding which means that you have to take the y train y test and turn them into one hot encoded labels and if you don't want to do that you need to add the sparse categorical entropy here instead of the categorical cross entropy because if you go to the documentation you look at the sparse categorical cross entropy you see here that we're using labels in the format of the normal format not the one using one hat encoding so when doing this if we save this let me clear this again if i run my example now we should have no problems everything should be running smoothly and as you can see the training has started here and let me just stop it here just to show you uh if you were to use categorical cross entropy what would you have what would you need to do differently because sometimes you might have a data set that already has one hot encoding labels in that case if you just want to use them as they are you don't want to transform your labels into a different format in that case you would just use categorical cross entropy but if you have labels like this that are not one hot encoded and you want to turn them into one hat encoded what you can do is use a utility from uh tensorflow so here we're gonna do y train tensorflow.keras.utils.2 categorical and then you would give it y the same uh array so y train and then you want to tell it how many classes you have so we have 10 classes between 0 and 9 and you would do the same for the y test tensorflow keras details 2 categorical and here i want to give it test and 10 classes so now if i do this then in fact i'm transforming my labels into one hot encoded labels and in that case i can use categorical cross entropy so let's run the training again and see if this works or not so as you can see the training has started correctly with no problems and i hope now you understand differences between categorical cross entropy and sparse cross sparse categorical cross entropy and when to use which so i'm gonna wait for the training to finish i've only set the epoxy 3 because it already gives us some good metrics here and it's done so let's go a little bit over the logs here and i would like to explain to you what these things mean so this one here the first number here 11 seconds represents how much how much time we spent uh doing this one epoch and also this tells us the speed so 12 milliseconds per step and the step here means doing a full badge so we have a badge of 64 images so where the model is processing 64 images at the same time in 12 milliseconds and by doing that the first epoch we got a training loss so this loss here is the training loss of 0.2368 we have an accuracy of around 93 we have a validation loss of 0.0994 and we have a validation accuracy of around 97 and what this means is that as you remember we did this split here validation split so the first loss and accuracy are reported based on the 80 of the data that we use for training and this part here for the validation is reported based on the 20 that we left out for the validation so it's 20 of that extreme and after each epoch we get these values so that we can see the development of the loss and the accuracy and what you want to get here is a lower loss during the training and a higher accuracy and at the end this last line here we've actually ran it over the test set just to see how well our model our final model that we got here how well is it doing on the test set and here we see that we got a 98.36 accuracy which is not bad on this task so as you can see now we have a full program that has a deep learning model and it compiles that model with specific optimizer loss metrics and then it fits the data into that model and when we're doing evaluation we're getting some good results so with this we have finished the first let's say a part of doing training for an image classification task and as you can see it with tensorflow it makes so many things easier you don't have to do a lot of things you just need to set the right parameters and prepare your data set in the right way so now that we have this uh first version of our program working let's see what we can do next this first model that we created and we trained was using the sequential approach here where we just stacked different layers and now let's take a look at the other approach that we can use which is the functional approach so first let me just maybe change this to sequential model just changing the name here and here what we want is to build the same exact architecture but in the functional approach so in fact to do this you just need to create a function and let's call it for example functional model and here you need to use these same same layers that we used in our sequential model but we're gonna use them in a functional way so let us just maybe copy them from here and just paste them here let me fix the indentation and in order to build a model using the functional approach what you need to do is basically define a set of layers and each time you need to pass the output of the previous layer to as an input to the new layer so here for example i would call this my input and then for the convolution 2d i need to actually pass my input as as a parameter to this convolution 2d and then i need to store this into a variable and i need to do this for the rest of the layers so each time i have an output of a layer i need to pass it as an input to the upcoming layer so let me just do this quickly here so now i have done exactly what i explained to you i'm just using each time the variable x to store the output of a certain layer then i'll pass that variable as an input to the next layer and i do this for all the layers oops i've got this one here and okay we have it here here we have it everywhere and now when you finish doing this you need to basically create the model where you tell it that this is the input and this is the output so the input is this and each time we're passing it to our layers and we're getting an output here which is x but in order to construct a deep learning model that you can call the compile function and the fix the fit function on it you have to use the tensorflow.keras.model class and here you need to give it inputs and it's going to be my input and also outputs which is going to be x and then you return your model so this is how you would build a deep learning model architecture using the functional approach so as you can see it's the same exact model it's just that we're building it differently now why would you use this approach rather than this sequential approach well in fact the sequential approach is rarely used when you're building deep learning models for some let's say complex tasks and even for smaller tasks i think it's if it's uh it's better to build your model in this way here because it's much more flexible you can do so many things for example if you have let's say two different inputs you can just set them here this will be a list of inputs where you just add the inputs to it the same thing if you have several outputs so it's a lot more flexible i think and also it's going to allow you to for example you can build one part of the model then create another part and use those two parts into when building a larger deep learning model so this is a much better approach in my opinion i've only seen the sequential approach here used in some tutorials online because it's easy and it's basically allows you to quickly build something that you can manipulate but for real world projects i think that using this functional approach is much more recommended so now that we have this we can just go to our code here sorry let me just close this here here i'm just gonna call my function so model functional model and then i'm just gonna call the same exact methods on it so the compile method the fit method the evaluate method and what's also good about this functional approach is that you can make this uh this architecture much more flexible for example by uh adding parameters to your function that you later add them here for example the number of filters here the uh the filter size here you just add them all as parameters here and then you can generate several models quickly you can just create for example a for loop and you create several models with different hyper parameters and then you compile them and fit them and compare between them so as you can see this would be a good use case for functional model instead of using sequential model so now that we have this let's run our code again and see if this works or not so let me just run this so the training should start and it did so as you can see we have just defined the same architecture in a different approach which is the functional approach and it's working well the loss is going down the accuracy is going up and let's just wait for it to finish this should not take long it's going to run the evaluation on the test set here and as you can see we got the uh the results here from our functional model and there was no problem during the training uh or the evaluation so this is the second approach that you can use to build deep learning models the third and last approach is by using or by inheriting from the tensorflow.kerastat model class and which is what we're gonna do next to build a deep learning model architecture using this third approach you start by creating a class you can give it any name you like so for me i'm just gonna call it my custom model and then you need to inherit from the class tensorflow.keras.model and in python to do that you just add parentheses and you have the class that you want to inherit from so here model let's start with the instructor here and here we're calling it on the uh the mother class which is this class here let me just maybe remove this we don't need to use it and now what you would do and in this third approach is that you need first to define all of the layers and you need to give them a name and then you would use the or you would call the function in fact implement the function call which is just like this and then you can give it your input and then here what you would do is you would pass your input throughout your layers here so you start with the first layer and then you continue so to make this more clear let me just do this here first we're gonna define all the layers and again i'm going to use the exact same architecture and just we're implementing it in three different ways so that you can compare between the uh three approaches so here i'm just gonna start by creating these layers and give it giving them names so here i have just done exactly that as you can see i call my first convolutional layer self.com1 cell.com2 so on and so forth and here when i go to my call function here i can start calling them so my input needs to be passed through this first layer and then the output of it needs to be passed to the second layer so on and so forth so this is what the call method here needs to do so let me just do this quickly now so i have just finished doing exactly that i just i take my input i passed it to my com1 cell.com1 layer which is this convolutional 2d and then i get an output then i pass it to the second one so on and so forth and at the end i need to return x and this might look a little strange this approach compared to the other ones but this is also one of the approaches you can use to build custom models especially if they're much more complex than than this you can do so many things when you have a class specific for your for your tensorflow model and in fact if you've worked with pytorch before then this should be very familiar to you because it's very similar to what pytorch does in pytorch you do the same thing you define the layers first and then there is a method i think it's forward if i'm not mistaken instead of call and then you do the same thing you pass the input to the first layer and then the output to the next layer so on and so forth so we're doing this with in fact we just finished uh defining our model using this third approach and we can use it to in the practically the same manner we used the functional and the sequential model so let's see how we can do this so i'm just gonna comment this one here and now instead of using my functional model i'm just gonna use my custom model so just calling or instantiating an object from my custom model class i get my model and then i can do exactly the same thing i can compile my model i can fit my model i can evaluate my model and let's test this quickly here let me run the example again so okay it says okay now the training is running and as you can see uh we're doing the exact same thing as we did with the first two approaches and now you have seen the three different approaches used when creating an architecture for a deep learning model and tensorflow and you can use whatever you like you can also use them all three depending on the task that you're doing and depending on the specific circumstances but here as you can see we defined the same architecture in three different approaches and we were able to fit the data and evaluate our model and the results are very close so now you have all of these uh these tools and these approaches that you've learned and we use them on the amnest data set let's see now what we can do next so there are some things that i omitted in this first tutorial here i just wanted i wanted to only show you the main parts that you need when training a deep learning model but in fact you can do a lot more things and there are some necessary parts that i didn't mention here so for example one thing is about saving the model so here we're just doing the training and evaluating the model but we're not saving anything so what if i want to use this trained model in production what if i want to use it in some sort of a web app or an embedded application in that case i would need a model that i can take and then i can run in a separate script that just loads the model and makes predictions with it something else that i didn't do is saving the best model during the training so there's one thing you can do here we just saved the final model but if you notice here for example let's see if we have a good example uh yes okay so you see this second step here we have a higher accuracy than the last epoch so it's actually better to save this model that was uh finalized in this epoch rather than the last model with which was uh saved or it wasn't safe but which we got at the third epoch so we're not doing this now we're not saving the best model we're not saving any model in fact i wanted to show you this in this tutorial but then i thought it would be better if we leave this in the next tutorial that i'll show you which will represent a real world uh scenario because here with the amnest examples which are the hello world equivalent in programming you have the amnest data set already prepared for you you see we just used it as part of the tensorflow framework so we're just loading it and then we are using these these images and labels when training our model but if you're working on a real world case then maybe you would maybe you would have collected images by yourself maybe you went outside and captured some images here and there so in that case you need to create your own data set that is compatible with tensorflow and that you can use it to train a deep learning model that you would define just like we did here so i wanted to leave these details of saving the model during the training and saving the final model the format of saving the model i wanted to leave them for the next tutorial that we're gonna do so with this first tutorial you should have all the uh necessary steps that you would use to train a deep learning model for classification uh many of these or most of them in fact most of these uh parameters most of these things that we've done here we're gonna redo that in the next tutorial but just we're gonna go into maybe more details about specific things like the data set uh how to create a data set that you can use just like the keras or or just like the amnes data that sorry how to save the model during the training how to save the final model in the saved model format and things like that so for now this is it for this tutorial and let's take a look next at what we can do in a real case scenario so before we continue along the course let's maybe restructure a little bit our code for better readability this is something that you're gonna find yourself doing often when you're developing projects for uh for a company that you're working for and in by doing this you're actually gonna make your life easier in the future because you're gonna have all of these different modules that contain different parts of the code and many of them will be reusable modules so that you can use them in other projects as well so the first thing we're going to do is maybe move these two models the functional model and the the model that inherited from this class into a different file so for that let me just create a new file and maybe just call it deep learning models dot by and what i'm going to do here is just move these parts of the code so let me just move these ctrl x and control v and here i can just import the same modules or the same functions and classes that we used in our code and also i'm gonna import of course tensorflow because we're using it here and by this we're gonna have our models in a separate file and then we can just come to our main script and do for example from deep learning models import functional model and also my custom model so as you can see we moved large part of the code into another file and now we have this file here that just contains the deep learning models so maybe we can use these models in a different projects and we will find them in a separate file instead of finding them in this script that's specifically made for mnist data set another thing we can do for example is move this function here display some examples to a different file so maybe we can call it for example my utils dot by and also okay let me close this i'm gonna come here and just cut this and paste it here and here of course i'm gonna have to import mad plotlib dot pi plot as plt and also i need to import port numpy as mp so now we have this and of course we can import our functions the same way from my utils import display some examples and now as you can see a large part has been moved from this file and now this file is specifically made for the amnest data set and we're importing the necessary models and functions from our other files that we just created so i am showing to showing you this here just as an example that you can use for your projects because when you're working for a company and you're building ai projects machine learning models you're going to find yourself doing lots of experimentation you're going to find yourself reusing parts of your code so if you get used to this habit of taking uh different parts of the code that are reusable and putting them into separate files this is going to help you a lot uh and your future self is gonna thank you and with this uh that would be it for the coding of this first tutorial let's see what we can do next so let's just recap what we have seen in this first tutorial so basically we took a look at the amnest data set we saw how we can load it from the tensorflow data sets we explored that data set by implementing a function that can go through the data set and randomly choose some images with their corresponding labels we took a look at the tensorflow layers how to import them and then we saw how we can use those layers to build a neural network architecture in three different approaches the sequential way the functional way and by inheriting from the model class at the end we compiled our model and fit the data and we finished the tutorial by restructuring our code for better readability so with this this would be the end of this first tutorial let's see what we can cover next welcome to this second part or second tutorial in this introductory tensorflow course so in this tutorial what we want to achieve is something like the following so we're going to have a data set of traffic signs like the ones you see here and the goal would be to develop a deep learning model that can take an image of a traffic sign like this one and then it gives us at the output that this image actually corresponds to the traffic sign of speed limit 100 kilometers per hour and maybe it will not tell us exactly this expression here but maybe it can tell us that it's one and that one corresponds to this speed limit uh 100 kilometers per hour and if we try if we change the traffic sign we want to get the corresponding sign here so we're going to be developing and building and training this deep learning model and this will be the goal of this second tutorial the data set that we will be using for this tutorial is called german traffic sign recognition benchmark so this data set you can find it on kaggle so if you don't know the website kaggle it's a website that many data scientists machine learning engineers and just anyone who's interested in machine learning uses it because it has so many data sets it also has some competitions that you can join and you can even be part of a team and you can learn a lot by doing that so this data set in order for you to get it you can go to this link here and if it's maybe the link is not that clear since we see so many meow meow here you can just take the german traffic sign recognition benchmark expression here and you can look it up let me just maybe go back to make it clear so if you're on the kaggle website you're gonna go here on the search bar type this and look it up and it should be the first result and then you're gonna get to the same page that i showed you earlier just a quick note here so in order for you to get access to the data set and download it you need to have an account on kaggle it's completely free i already have one and i'm already logged in but if you don't have an account you can just create a an account for free and then you can continue with going to the data set page and download it so in order to download the data set you just click on the button download and then it's gonna download it to your own machine and once it's downloaded it's going to be a zipped folder like this and then you can unzip it in order to get a set of files and folders like this just a quick note here also when i was unzipping this folder here i realized that i think the data set is duplicated so many files it asked me that whether i want to replace them or just keep both versions when i was unzipping this file here i chose to replace them so that i don't have a duplicate of every file and every folder in my data set i think this is maybe a mistake that was done by the person who collected or put the data set on kaggle but it's not a big deal you can just unzip the folder and when it asks you whether there are new files and folders that have the same name just choose replace or choose do not replace for the rest of the data set and that way you won't have any duplicated folders and files and now that we have it here and as you can see i unzipped it we see that we have three files and three folders so the main files and maybe the main folders that we're gonna be looking at are the tests and train folders and also the test.csv file so let's first go to the train folder here so as you can see we have 43 folders of from 0 to 42 so 40 43 folders and in each folder we have images that belong to the same class let me see if this is the large let's see i is the same okay so for example we can see here that this is a sign for the traffic sign that says there's a speed limit of 20 kilometers per hour we also have this data set here of the speed limit 50 and we have all sorts of speed signs as you can see and what i really like about this data set is that the images represent real life cases now not like the amnest example where the data set is more or less already prepared for us but in the case of these traffic signs we see that these signs are taken into in uh different luminosity levels uh different light levels uh it also comes in different sizes so different images have different sizes they're not all the same and also if we go to the next folder which is the test folder here you can see that we have a set of images and we can't know just from the name for example whether this image belongs to class 1 2 or whatever so we can't know from just looking at the image and looking at the name of the image in order for us to know uh each image belongs to which class we need to go and look at the csv file corresponding to that folder so test.csv for the test folder and let me just open it here okay let me close this so here as you can see in the test.csv file we have a different lines corresponding to different images each line has these width and height of the image also has a roy area i guess it's a more tightened area around the traffic sign we also have the name of the image and we have the id of the image and these are the two most important information that we need from this so we need the path and we for each image we need to know the corresponding class idea so here let me just maybe open this close so let's go to the test so for example the first image 000.png it says that this belongs to class number 16. this is the class id so it's 16 and if we want to just quickly verify uh just remember this is uh i think a traffic sign that says the trucks cannot pass through here or only for trucks i can't remember honestly and if i go to the train set and i go to class 16 as you can see it corresponds to the right class so from with what we see here we have two folders the first one for training the second one for the test set the test set is not organized like the train set uh in order for us to know which image belongs to which label we have to look at the csv file and there's also the fact that we only have two sets so there's only the train and test set but in reality what you want to have when you're developing deep learning models for real life cases you want to have three sets you want to have a train set a validation set and a test set so there is a lot of work to be done before we can train a deep learning model on this data set and this tutorial for me represents a much more mature case of uh much of a machine learning project where basically you are using a reallife data set if someone collects it maybe uh he chooses to annotate the data set like this where it just puts the images in one folder and keeps a csv file corresponding to those images and their labels so as you can see it's not very clean not like the case where we had the amnes data set we just basically loaded it so now in order to build a deep learning model on this data set we have to basically do some cleaning and also some preparation of this data set and this is exactly what we're going to be doing next so the first thing we're going to do now to prepare our data set for training is to go to this train folder and we're going to split this data set here that exists in this folder into two different folders one for training and one for validation so basically what we want to do is to create some function that can go through all of these folders and each time it goes through the images in every folder and it takes part of them and puts them in a training folder and the rest it puts them in the validation folder so let's start by uh coding this and before that let me just maybe add a new script we're going to call it street signs example that buy so this will be for all the necessary code to basically load the data set and uh also train the model and evaluate it and here maybe let's first start by defining a function that's gonna split the data so let's call it split data and what this function is going to take is the path to the data so the original folder which is this one here we're going to take a look uh or we're gonna look into this folder here and then we want to split the data into a training folder so that to maybe let's call it say train and also path to save valve for validation and we're also gonna choose a split size and it's going to be between zero and one and i'm just going to give it a default value of 0.1 and what i want is for 90 of the data to be put in the training folder and the rest which is 10 it's gonna be put inside the save for the the validation folder so here let me come here and the first thing we're gonna do is basically get the folders so i'm gonna use os dot list deer and i'm gonna give it the path to data and since we don't have this module let me import it here so what this uh line here is going to do is basically it's going to give us a list of all the folders that exist inside this directory here and then what i want to do is to iterate through the folders so i'm gonna do for folder and folders and here first i want to get the path to or maybe let me call it full path which is going to be the path or the full path to the directory so here we're gonna have this path and then this first command here is going to give us the folders now we want to concatenate the name of the folder with this path here in order to get the full path to this set of images for example or this set of images so here i'm gonna use os.path dot join and we're just gonna use the path to data and also i'm gonna concatenate that with the folder after that what i want to get is a list of all the paths to the images inside that folder so i'm going to call it images paths and for this i'm going to use the module globe so it doesn't exist here so let me just import glob here so this module is going to allow me to look inside a folder and load all of the files that exist in that folder depending on an extension that i choose so here what i want to do is use os dot pat dot join and i want to join the full pad that i just constructed and also i only want to load the images so our images let me just take a look quickly all of our images have a png extension so i only want to load png files so here by doing this what i'm what i'm saying is that i'm telling this glob module to give me a list of all the files that have extension png inside my inside this folder here full path so now we have a list of all the images inside that folder and what we can do now is we can for example to extrane and x val and we want to basically split the data set into these two different parts and for that there's actually a utility function called train test split which we can import from the scikitlearn module so here if i do from scikitlearn dot model selection import train test split so by doing this i'll be able to use this train test split function here and what this function takes is basically the images pads images pads and also it's gonna take the test size so this is going to be the fraction that represents the number of images that we want to put inside this variable here and for this i'm just going to use my split size which i pass here as an argument so what i'm what i'm saying here is that please split all of uh the the please split the list of all of the images here into two parts train and val for validation and i want this second part to be only uh this fraction of the data set which is uh if we keep the value as it is then it's going to be 10 and if we call the function and we change this split size then it's going to take that exact fraction so now we have our train and validation pads and what we can do now is basically just go through them so first we're gonna go through for so for x in x trains so we're going through all of the images that were selected for the train set and the first thing we're going to do is get the name of the image and in order to get that we're just going to use the function os dot pad dot base name and we're gonna give it x as the argument here and was remember this x represents a full path here we want to get just the name of the image so we would like for example to get this only this uh five zeros five zeros five zeros dot png so this is what we can i'm gonna get as the base name and then we're gonna use basically the space name to reconstruct a new path that points to this path here and is we're going to save our image inside this path here so let us first do this folder i'm going to construct a path here os.path dot join and i'm gonna do path to save train and folder so here what i'm doing is uh imagine that for example i want to save my images into this let's create a new folder for example and let's call it training data and inside of this i'm going to create a folder called train and another call another folder called val for validation so what i'm going to do is take the images from folders like this and just go for example to the train set or the train folder and just put that image here and for now i'm just going to copy the images i'm not going to move them just so that i keep the original data set as it is but first here i need to add a folder for example one two three that responds to that specific label for that image and that's why what we're doing here is we're constructing this full path here so this is going to be this path and then we're gonna add a folder with the same name as the folder that we found in the original data set here and after that what we're gonna do is check if this folder does not exist so path to folder sorry if not os dot path dot his directory so this is gonna check whether this folder exists or not if it doesn't exist we're gonna create one what's that make theirs so this is to create that folder and i'm gonna do that to folder so this part here just checks whether that folder exists or not if it doesn't it creates that folder and now what we're gonna do is use the you the utility module sh util and i don't have it imported so i'm gonna import it here sh util and this module here gives me access to several functions one of them is the copy function that i can use and now what i can do is basically copy my image and put it inside a new directory that i just constructed here and if it didn't exist i would have already created it here and now i want to put my image inside this folder so what we can do is actually two things now that i'm looking at it we can either do path to folder and this is going to copy the image with the exact name that it has inside this folder or we can construct a complete directory with the name of that image so basically we don't need this here is in fact just going to take the name of the image and put it inside the new folder so this is going to be for the train set and we're going to do the same thing for the validation set so i have just added the part here for the validation and this is going to be identical to the train set the only thing i'm doing is looking at the original directory going through the folders constructing a new folder inside the train and the validation folders and then copying that image from the original data set or the original folder into the corresponding folder whether it's strain or validation and now that we have this let's test our function and i have also here activated the virtual environment that we used before which is tf2vn and in order to do that you can just point to the folder where you have anaconda 3 installed on your machine uh in order to use the script called activate and then point to the name of the environment that you have and also something that may be a quick way to do this let me just clear this first in fact if you're on visual studio code and if you have already used a specific virtual environment with anaconda before then what i have noticed is that if you click on the plus sign here is going to open a new terminal and also is going to activate that environment automatically so you can use this quick way to activate your virtual environment and now that we have this uh let me maybe first activate or save my script here and one thing you can notice is that this script or this module scalene is not recognized and uh just to reiterate what i showed you in the first tutorial if you click on control shift p you can choose the right interpreter and here i'm making sure that i am using the same virtual environment so it's tf2 vm and this module is not installed inside this virtual environment so for that you can do a quick pip install scaler for scikitlearn and this is going to install that module if it's not already installed and once this is done then here we can do the same thing as with before so if name equals main then i would like to do a set of things here so i'm gonna do uh i'm gonna basically choose the paths that i want in order for me to split my data set so the first path needs to be towards my original set which is this one so i'm gonna do this ctrl c and here i'm gonna paste it i'm just gonna add these uh backwards slashes here just so that i don't get any problem and if you are on linux machine i believe that this is a safe way so that you don't face any problems because between windows and linux they handle paths differently but if you do two backward slashes like i'm doing here then you shouldn't face any problem so this is the path to data and then path to save train so this is going to be the path where i want to save the train part so i want to save it here ctrl c ctrl v and let me fix these as i have done before very quickly okay and then i want to basically paste the same path here so path to save valve and let me paste it except that it's going to point to the valve folder which is this one here so now that we have our three different folders or paths we can call our function called split data so it's going to take path to data path to save train let me just make sure that i'm using to save train and also path to save valve as path to save val and for the split size we can keep it as it is so we don't have to call it here and let me just save my script let me clear my terminal so the goal is to copy the images from the original folder into these two folders and it's going to put 90 of those images in the train folder and 10 in the validation folder so let me run this script quickly and see if it works without any problems so here it's gonna this module should be installed so there should be no problem here and now the script is actually running it's just that we don't see uh we don't see it work because we don't print anything but if we go here you see that it started to construct these folders in both the train and validation folder and in each one we should have a set of images like it's shown here so this might take a little bit of time because we have a lot of images at the end i'll show you what we get and all the images that were copied we're gonna see that they have been correctly copied with the correct split size that we chose so now the script has finished and the program has finished working and if we look at our data here we see now that in my training folder i have 43 folders and the same for the validation folder and in fact if we check quickly the number of images here we see that in the train set we have around 35 000 images and in the validation set we have around 4 000 images so that sounds about right so the split size seems to be correct and if we go to every every folder we have the exact same uh images that we had in the original set which was this one here so now we have our training and validation sets what we need is the test set and as you remember the test set is comprised of images like this and in order for us to know which label which basically which label corresponds to that image we need to look at the corresponding csv file so for that let us create a function that does this so our function is going to take as arguments the path to the folder and the path to the csv file and based on these two we're gonna construct a folder that contains the right images just like this so we want to order our test set so that it starts looking like this folder here or these folders here so let us get into this and maybe since this is also a utility function maybe we can just remove it from this main script and maybe add it to our utilities script and here we need the os module so import os so and also import glob import glob and finally the scikitlearn from scikitlearn dot model selection import train tests later from numbers taken let me check this train test splits this sounds correct and also we need the sh util sage util so this these are the modules necessary import so these are the modules necessary to run these functions and now here we don't have that we can also remove these if we want for now i'm just going to keep them a star problem and here i'm going to do from my utils import split data so this is a much cleaned a much cleaner script and now let's maybe do the same for the test set as i mentioned so we want to build this function that orders the test set so let's call it uh or maybe let's just code this straight in the utilities script here so i'm going to define a function called order test set and what this function is going to take as i mentioned the path to the images and also that to csv file or just csv is clear and here what i want is to construct a dictionary that basically has the name of the image as uh as the key and also the corresponding label as the as the value so let's start by defining our dictionary and by the way this dictionary here is just going to help us keep track of the data set later if we want but it's not really necessary for us when we're just ordering the test set i just prefer to do this so that i have or i could have some sort of a data structure that contains my image paths and also their corresponding labels but for you it's not it's not really necessary for our ordering part because as you remember we just want to go through these images and just order them like we have here so here let me start by creating a try block and what i'm going to do here is to open my csv file so path to csv and the mode is read i don't want to write this inside this file i just want to read and i want to open it as csv file and then i can create a reader and it's going to use the module csv the reader and since we don't have the module csv we can import it here and just maybe a quick note here if you see me using module that's not installed inside your virtual environment then it's very easy to install any module you want you just run pip install the name of that module so here i'm gonna use this function or this method here and i'm gonna give it my csv file and also i'm gonna use i'm gonna choose the delimiter to be a comma here and why comma because inside my csv file you see that these elements are separated using a comma and now what i want to do is basically go through all the rows that exist in this reader because this reader is going to contain each row inside my csv file including including in fact this first row here that only contains information so only the headers which we don't need so we need to make sure that we don't use that information in that first row so that's why we're gonna do for i row and enumerate reader and the first thing i'm going to do is check if i'm looking at the first row and if i am i'm just going to continue which means that i don't want to use any information in that row i just want to go to the next one and for the rest of the rows what we're going to do is first get the image name and for that what we can do is use our row and in fact our row is going to contain all of the elements inside that row so it's going to contain this value then this and this this and you can access them using the correct index so row 0 is going to access this value here and row minus 1 for example is going to access the last element which is the path to the image so here what i want to do is get the image name so for that i'm gonna get the final or the end value inside that row and remember that each image here this is how the data set was constructed so we have we don't just have the name of the image we also have this test and forward slash next to it so we don't want it we just want the name of the image so for example the first one would be zero zero zero zero zero dot png we just want this we don't want this part so what we what we're gonna do here is since this is a string we're gonna use the method replace and i want to replace this string here with an empty string this means that we're gonna take this full information we're gonna remove this and we're just gonna keep the name of the image and the second thing we want to get is the label so for the label this is going to be row minus 2 and that's because this is the second element starting from the end so this class id is the second element from the end or it's zero one two three four five six sixth element so we're gonna use this or you can do just this for me i think this is much cleaner 2 because now i know just from the end i'm just going to look at the second value there and then what we're going to do is construct a path to folder and this is going to be a path that uh that is constructed using the path to the images and also the label so add a join that two images and also label since it's a string and then i'm gonna check if this path does not exist that path that is directory path to folder if it doesn't exist i'm gonna create it and here i'm gonna do os dot make deers path to folder so this is exactly what we did in the previous part so i'm just redoing it here and now what i want is to move my image from the original set so here from the test set here i want to move it into this new folder that corresponds to the right label and you'll see in a minute what i mean by this so here i'm gonna do image full path os.path dot join path to images image name so i'm constructing a full path and then i can do sh util dot move you can either copy or move the images i showed you before in the previous function here we we copied it in this one split data and now we're just gonna use the utility function move so for this we're gonna move the image so because we just got the full path to that image and we want to move it into the right folder which is path to folder and this would be it here i'm gonna use the accept command so that if we can't open the csv file i'm gonna get a message here that lets me know that we couldn't do that so for that i'm just gonna do print and for example i'm gonna use the info class and i'm going to write error reading csv file and if everything goes fine at the end we're gonna return test data well in fact since we're not using the test data here we're not constructing anything let me just maybe remove it because it's not going to add a lot of information for us so i'm just going to save this i'm going to open the csv file read the necessary information move the images to a new folder that responds to their labels and with this we will have our images in the right order and now here what i'm going to do is import my function so this order test set and again i'm going to use that basically trick that i use false so that i deactivate parts of my code and now we're gonna use or we're gonna use our function that we just constructed and remember we're gonna have to give it the path to the images and path to the csv so i'm gonna do path two images and this is going to be this one here ctrl c ctrl v and let's go quickly to our path here and also we're gonna have to give it the path to csv and this is going to be this one here in fact for the test we can just click shift and then copy path and we're going to get the full path like this so now we have our path to the images we also have the path to the csv file and then we can call our order test set function and this function is going to take that to images and also path to csv so if this function works correctly and works as it's intended to do then now what we should have is a set of folders here that contain the right images we're gonna move these images to their corresponding folders so let me everything is saved okay let me run the same script again and again this might take a little time because we have a lot of images but if we take a look at our folder as you can see it has created all of these folders and now it's moving the images you see their numbers going down it's moving them to the right folder and with this now we have our data set in the correct format and in fact when we added just a quick note here when we added the part of replace here it was necessary to add this part because here we have that test part as part of the image path and if we didn't do this it would have given us an error and now we have our data set for the test set correctly uh presented correctly ordered and now we have a set of data or a set of folders that correct that contain our data in a very good format that we can use for training and testing our model and with this we have our data set ready let's see what we can do next for building our deep learning model and doing all the fun stuff for deep learning let's start creating our deep learning model so that we can fit our traffic signs data set that we just ordered and prepared so for that i'm gonna open my deep learning models file as you remember we have created this file where we put all of our deep learning models in it and i'm just gonna add at the end i'm gonna create a new function for our new deep learning model so in the previous tutorial i showed you for the amnes data set how to create a deep learning model using three approaches sequential way functional way and by using a class that inherits from the model class but for this example since i have already shown you the other three approaches i'm just gonna use one approach which is the functional way so here i'm gonna create a new function for my model i'm gonna call it street signs model and what my function is going to take is the number of classes as a parameter and now let's start building our deep learning model so one thing to notice is that between the amnes data set and the science data set so for the science data set we have an rgb we have rgb images so it's uh we see we have blue red we have all colors unlike the amnes data set which was grayscale data set so now we need to take this into consideration because when you have an image that's that has a red blue and green this means it has three channels if it's just different layers of gray scales then in this case it has only one channel so why i'm mentioning this because for our first layer here which is the output layer we're going to use as before the input layer that's imported here and we're gonna have to give it a shape so this shape here is gonna represent the width height and also the number of channels for our images so when looking at these images we see that most of them are almost squared shapes and here i checked a few images just to see the size and as you can see this is 44 by 45 almost a square and even the largest ones are still in squared form and here we can see 80 by 84 almost square as well and the same thing goes for all of the images in our data set so since these are small images relatively small and they're squared i thought that using a size 60 by 60 would be a good choice because i've seen some images that has that had width and height smaller than 60 and some of them a little higher than 60 so 60 by 60 is an acceptable choice although you can do something which sometimes i do which is going through all of the images in the data set computing the mean of the width and the height and maybe even the median and by looking at those values you can have an idea about where the widths and heights of the images in your data set are lying so in that case you can basically choose a good value for your width and height and for your input model like we're doing here so here i'm going to choose 60 by 60 and since we have rgb images i'm going to choose three which is going to be different than the first model where we had one and then since we have this we're just going to do the same thing as we did for the first model for the mnist data set and i'm just going to do this quickly here so i have used the the layers that we used before which is convolution max pooling batch normalization and i use them in these blocks as you can see these are basically similar blocks each time i use a convolution followed by max pool 2d then batch normalization and here i have actually i wanted to show you the difference between a layer called flatten and the global average pool 2d layer so first of all let me import this layer here so it's flattening so there's no problem and also let me get the model the model function from okay let's find them here it should be in the okay i can't see it okay in that case let's just go here and do from tensorflow.keras import model so this is what we're going to use here so here i have commented this part this is the same layer that we used previously with the amnes data set and i told you that you can also use the flatten layer here which is just going to flatten all of the uh the values that are coming from the previous layer so in this case the batch normalization and what i wanted to do is to basically build this model and show it to you and then we're gonna change this layer to global average pool 2d and i'm going to show you the new model and then we're gonna look at the difference between the two so here uh basically we have our model and we can import it from here so from deep learning models import street signs model and we can use it here and just something that i also do from time to time when i'm doing experimentations such as these ones sometimes even in my scripts that are not used in the main script so these are scripts that just contain some utility functions and classes just like this script here sometimes even in these scripts i do the i create basically part of my code where i can test things straight inside this script so for example now what i wanted to show you is this model we're going to build it with this layer flattened layer and then we're going to build it with the global average pool 2d layer and we're going to see the difference there so here i'm gonna do model equals street signs model and for now i'm just gonna give it uh whatever i want since this is not the model that i'll be using with the data set this is just for testing purposes and what i want to do is to call the method summary and this method is going to show us the model architecture the different layers inputs and outputs shapes so this is going to help us get an idea of how our model looks like so here let me run this python deep learning models dot by and as you can see the the goal of these of this part here this is gonna help us for example when we're doing tests like i'm gonna show you in a minute we can just call this script here but if i import this script in another script so for example here i am importing this function from my script here in this case this part of the code will not be called so this is a good way to separate parts where you just want to test something quick and also the parts where you want to reuse them in other scripts so with this let me run this quickly here and let me maybe put this like this so this is the architecture of our deep learning model as you can see the the input is 60 60 by 3 and also we have this first uh value here represents the batch size so none means that it can be any batch size we want and then there is the convolution 2d the max pooling and so on and so forth and when we come at the end here we have the flattened layer as you can see what the flattened layer has done is that it took all the values from the previous layer so here we have uh basically a some sort of a cube where you have five by five and it has a depth of 128 and what it did is that just took all of the values and put them in one vector that's why we get 3200 because it's equal to 5x5 by 128. now if let me let me change this to the global average pooling instead and i'm gonna comment this so let me run the same script again and also enlarge this part here now when we come to the layer here so before you see we had the flattened layer third from the last layer so this is we changed it with the global average pulling 2d and now you see we don't have 3 thousand and two hundred because the global average pooling 2d what it does is that it actually computes the average in these uh in these windows here so imagine you have a cube where it's 5 by 5 100 by 128 so every 5 by 5 will compute the average there so at the end we only we're only left with 128 values so this is the difference between those two layers i wanted to show you how basically reflect in the architecture and whether you should use one or the other it's actually one of the things that you can experiment with just like i mentioned in the first tutorial when you're doing deep learning machine learning in general you're going to be doing lots of experimentation so this could be one factor of your experimentation where you change these two different layers and see which ones uh which one gives you the best results so now we have our architecture ready and we can call it from our main scripts here and we can compile the model and fit the data so we're going to be doing this next in fact before we compile our model and fit the data we have to create what's called data generators so data generators are actually going to take the images from our data sets folders so as you remember we have train and validation folders and we also have the test folder that we organized so now we're going to use some uh basically tensorflow utilities that help us achieve this very easily which is one of the main points of tensorflow i've been using tensorflow for some time and every basically with every release there are lots of functionalities new features that make your building the model uh preparing the data set makes all of this a lot easier so here i'm going to create a utility function again i'm inside my myutils.by file which i use to add all types of utility function that i might use in the future so here i'm gonna define a function i'm gonna call it create generators and basically my function is gonna take uh some arguments so the first argument is going to be the batch size i want to be able to pass the batch size to as an argument to my generators and also i'm going to pass the train train data path the validation data path and also the test data path so as you can guess we're going to create generators for each part of our data set so the first thing we're going to do is create a preprocessor so let me just define it here so this preprocessor is going to basically be used in order to preprocess our data before it's fed to our deep learning model so for this we're going to use a class from tensorflow which is the image data generator class so let me just import it here first so from tensorflow that keras that preprocessing that image import image data generator and here we're gonna be using this image data generator so i'm gonna call my class here and here in fact if you look at this class image data generator you're gonna see that you have many different transformations that you can apply for now let's just not worry too much about them for now i just want to focus on one thing which is the rescale parameter and the rescale parameter is gonna basically rescale our data so it's going to rescale our images and here what i want to do is choose a value that will be multiplied with our data set so that it's rescaled so i want basically to divide the values of my pixels by 255 which is the maximum possible value in rgb images so here i'm just going to choose 1 over 255 and what this means is that i want my preprocessor to be used i'll show you in a minute how we're going to use it but i want it to be used to preprocess my images before they are passed through my deep learning model so here i'm going to create my generators so i have i need three generators so train generator is going to be the first one and for this i'm just gonna use my preprocessor and i'm gonna use a utility function called flow from directory so this is a very helpful function that you can use and in fact what it does is that it takes a path to your folder so for example for the trained data we're gonna do uh that or sorry what did i call it train data path so train data path and then what it's going to do is just going to look at our folder so for example for the train data path folder and you're just going to look at all the folders that exist within that directory and it's gonna suppose that each folder contains images that belong to one class so that specific class so this means that our generator is gonna automatically know that all of the images belong that exists inside this folder belong to the same class same thing for this folder and for the other folders as well so this is going to help us immensely we don't need to specifically build the data set where we say this image has this label in fact with this utility function tensorflow is going to help us do that automatically so other parameters that we need to give to this utility function as the class mode so for the class mode i'm gonna use categorical and for the target size i'm gonna use 60 by 60 so this means that i want all of my images to be resized to 60 by 60 and for the categorical as i mentioned it in the first tutorial if we use categorical here we have to also use categorical cross entropy when we are compiling our model we can also use uh sparse let me just check here if it's maybe uh shown here class mode one of it's either categorical binary or sparse so if we use sparse then we need to use uh sparse labels if we choose categorical then this means that our labels need to be one heart encoded so let's look at the next parameter so here we're gonna choose color mode to be rgb because my images are colored images uh also there is a parameter called shuffle that i want to set to true this means that in each epoch or in each batch my images will be shuffled so that between two epochs basically the order of the images will not be the same this kind of uh randomness helps our model be more generalized and learn features and ignore that order between the images because we don't want our model to learn that and finally the batch size we're gonna give it the batch size which we are passing here as a parameter so this will be for the train generator and we're going to be doing exactly the same thing for the validation set and for the test set the only thing that's going to change is the the path and maybe for the test set we can set this to false because we don't need we don't need to shuffle the data when we're just testing so let me do this quickly now so i have finished creating generators for my different folders so for the test data i'm using this approach the same thing for the validation set and for the test set and as i mentioned the only difference is that for the validation set of course i'm using the validation images for the test set the test images and i'm just returning these generators here i also set shuffle to false for validation and for test because it doesn't matter that much for when we're validating on or when we're testing our model whether we have that basically a randomness or not that randomness matters much more when we're doing the training which is why we're setting shuffle to true for the train generator so now we have our generators ready what we need to do now is to use our function so that we have our data set ready in order to start training our deep learning model so let's instantiate first our generators and here let me just maybe deactivate this part of my code because i don't need it of course you don't have to always follow what i'm doing here you can just remove these parts because we don't need them anymore so we split the data we ordered to test the test set so we don't need this these parts but for now i'm just going to keep them i might delete them next but they will not have any effect on what we're going to add here so first i need to import my function that creates generators and then i have to use i have to set the paths to my train and validation folders so they will be these folders here so let me just put this here in fact let's just delete these they're not they're just going to be taking some space and here since i have the train folder the validation folder i only need to add the path to test and here i'm just gonna maybe change the name so that because we're not gonna be saving anything in these folders and here i'm gonna set it to my test folder so my test folder is this one so i'm gonna copy this path paste it then i'm just gonna add my backward slashes here so now we have all of our three pads and i can use my create generators function and it's going to take a batch size let me maybe set a batch size here so i'm going to choose a batch size of 64. now this is also again another parameter you can play with you can choose different values i recommend you choose something like you know a multiplied number of two so you choose for example 2 4 8 16 32 64 so on and so forth and here i'm going to keep it 64 and you might face a problem sometime when the batch size is too large for example let's say you choose 512 maybe the training will not start and you're gonna get an error where it says oom which means out of memory which means that you don't have enough memory to load all of those images at once so if you face that error i i suggest you decrease this number here and it should work fine so let's go here and pass our parameters so path to train that to well and path to test and of course let's go to our function just to check we're going to get these as outputs so let me maybe just copy them so that we can go quickly here so now with this i can generate my or i can create my generators using the images that exist in these paths here and with this we can now pass these generators to our deep learning model so that we can train the model first we're gonna create our deep learning model architecture so for that we're gonna use our street signs model here street science model and the street science model takes the number of classes as a parameter so here for the number of classes we can actually get them using the generators any of the generators since the train generator is the one that contains the most images let me maybe just use it so number classes equals train generator dot num classes and then i can pass this to my deep learning model and now i have my model ready then we need to do the same as we did in the first tutorial tutorial so here we're gonna first compile our model and to compile it we have uh several options the first one is the optimizer so for now let's just keep adam as the first tutorial second we need to set the loss and here i'm gonna use categorical cross entropy and again i'm using categorical cross entropy because my generators here i have set the class mode to categorical if i set this to sparse and i set this to and i keep this categorical then i'm going to have an error at the end and the final thing we need to define here are the metrics and for that i'm just going to use the accuracy so by this by doing this we're going to have our model compiled and what's gonna be left now is to fit the data so here i'm gonna do model.fit and here we can actually do something uh that's that looks a little different than the first tutorial so in the first tutorial we had that x and y but in fact you can just pass the full generator so this generator and this and this one they all contain the correct image and also its corresponding label we didn't have to explicitly do that by when we used the from flow from directory method it has done this for us so we have all generators ready so we can just pass the train generated generator sorry as a parameter like this and then we can set for example other parameters like the ebox so here we can choose whatever epochs we want maybe i'm just gonna add it here so that it's easily changed later ebox and also we have the batch size we're gonna use the same batch size and also we gonna pass the validation images and the validation generator here so for that we can use the validation data parameter and we can pass the valve generator as the argument so here as you can see we didn't have to specifically mention the images and their labels separately like we did in the mnist example so here in the amnest we had the x strain contains the images the y train contains the labels but in this case here we didn't need to do that because our generator takes care of that there's already the images and their corresponding labels so with this we can fit our data and we can train our deep learning model and this would be almost uh almost identical to what we did before apart from the generators and i mentioned in the previous tutorial that there are some things that we need to do when we're building a project like this so for example one of the important things would be to save your model save the best model during training so i'm going to show you how to do this now so in order to save the best model during training we're going to use something in tensorflow that's called callbacks and these callbacks are actually functions that are called during the training and basically they do whatever you want them to do specifically we want to save the best model so we're going to use a callback that saves the best model so in order to do that we're gonna be using some some modules or some classes from tensorflow so here i'm gonna do from tensorflow.keras.covacs i'm gonna import a callback called model checkpoint so here i'm going to use this one here i'm going to give it a name so for example we can call it checkpoint saver so kpt saver and i'm gonna instantiate that class so model checkpoint and then i'm gonna pass a set of parameters to my class so here the first thing we need to give it is a path to where we want our model to be saved so path to save model and let me maybe define it right here so path to save model and i just want to save it in the same directory i want to create a directory called data and or maybe just models and that will be it so i'm gonna be saving my model inside a directory here that will be called models and then i need to give other parameters to my callback so one thing would be the monitor and here i'm going to give it the valve accuracy so what this parameter is going to do so this parameter here monitor is basically going to well monitor this value here and what we wanted to do is that we wanted to follow this mode so here i'm going to set it to max which means that i want my basically my program to save the model whenever i get a validation accuracy that is higher than the previous saved accuracy and this is why i'm setting here max because it's going to be my model is going to be looking at the validation accuracy and whenever it gets a little higher then it's going to save it if it gets lower then we don't do anything we keep the same model that was saved before and why we're sending here max is specifically for the reason that i just mentioned which is we need to basically only save the models that have higher validation accuracy if for example i had here to monitor validation loss then i would be using the min mode because for the validation loss the less it becomes the better my model would become so i only want to save my model when this validation accuracy becomes lower hence the mode min so for me i want to monitor the validation accuracy the higher it gets the the program will save it so that's why i'm setting to validation accuracy and the mode to max and also there is a parameter called saved best only i'm gonna set it to true so i only want to i only want the best to to be saved which means that i'm only going to have one model to be saved which means that if i had a validation accuracy i save a model for that validation accuracy if a new model has better validation accuracy then i'm gonna replace the previous model uh if i set this to false then i'm gonna keep the previous model and save the new one next to it so for me i just want to save the best one so i'm going to set this parameter to true and also we can choose a frequency of saving so here i'm going to set it to epoch so we're only going to look at the the validation accuracy at the end of the epoch and only then we can choose whether to save the uh parameter the model or not and finally i'm just going to set verbose to 1 which is just helps us for debugging purposes to see when the model has been saved or not so now i have my callback ready and to use the this callback what you need to do is to add a parameter here called callbacks to the fit function and here you can give it a list of callbacks so for now we only have one which is the checkpoint saver so i'm going to pass it here if we had other callbacks then we can also pass them here as part of the list but we're not gonna do this unless as i mentioned we have another callback and speaking of callbacks i also want to show you another interesting callback that you can use which we can also import from the callbacks module here and it's called early stopping so this uh callback here what it does is that we can set some number of epochs that according to these this number of epochs if my model does not improve then i'm just gonna stop the training so this could be useful when you're training for a large number of epochs and maybe for let's say 30 or 50 epochs your model does not improve at all so it's better to just stop the training so here how to use this is almost the same way we use the model checkpoint callback so we're going to have to instantiate the class so i'm going to call it early stop and i'm going to use the class early stopping i'm going to monitor something so here i'm going to choose to monitor the validation accuracy just like i did for the model checkpoint and also i need to set the patients so here for example i can set the patients to 10 so what this means is that i'm telling my my program here that if after 10 epochs my validation accuracy does not go higher then just stop the training so for example i might i might use let's say let's let's say 100 epochs and after 10 epochs we reach certain validation accuracy and at the 20th epoch the validation accuracy does not go higher so that means the model did not improve in that case i'm just going to stop the training before continuing the rest of the 80 epochs so this could be a very useful and different scenarios and we're using it here and in order to tell our model to basically use this callback we need to pass it as part of the list of callbacks here so now we have our program ready and we can fit the data to our model and we can save the the model inside a folder here called models and we have everything ready so with this let us test our model let me clear the terminal i'm gonna run python stream in fact i just noticed that actually my script is called stree instead of street signs so let me just quickly modify this so street signs example and now let me call it or let me call my script from here so street signs example so let's run this and see what we get hopefully we don't have any errors so as you can see from the logs uh when you when you're using generators also gives you an idea about number of images that are using that are being used so since we have trained validation and tests this is for the number of trained images this is for the validation this is for the test and we also have 43 classes and as you can see the training has started and our script is working properly so now this might take some time since it's 15 epochs i'm going to wait for it until it it finishes and then we're gonna look at our uh saved model and maybe i'm gonna come back before it even finishes just to show you that there was a model saved here after the first epoch so here after the end of the first epoch as you can see we reached this validation accuracy but the most important part for us now is that there's this line here that says validation accuracy improved from minus infinity to 0.4 so this means that our validation accuracy has improved this means that we're going to save the model to this folder here and as you can see this folder was automatically created and if we go inside of it we see these files here and these files actually represent our model so we have this file we have these two folders so this folder contains the weights and this all of the elements that exist inside this folder represent our model so now each time it's improved it's gonna be saved in the same place here and for now i'm just gonna wait until the training is done and then i'll show you how how to use that final model to evaluate the test set for example and to do other things so now the training has finished it didn't take that long and at the end you see that in different uh epochs whenever the accuracy the validation accuracy became higher we saved the model so now we're sure that this folder here contains the best model that was saved during the training and now that we have this we can use our model in order to evaluate the data sets and this could be done easily using also some tensorflow utilities so for example since we don't need this part here i can just for example said this and this is also something that sometimes i do which is to uh basically set what i call switchers so train i'm gonna use i'm gonna set it to false because i'm not training and i would do this whenever i'm doing part of the code that i know i might use it again so i set it to this switch switcher here and i can set it to true or false depending on what i want and for example i can set this test to true and then my part of the code that does the testing will be added here so if test so if test then if i am using this switch then i want to use my model that was saved in order to evaluate my data set so now that we have uh our model here we can use some tensorflow utilities in order to load our model so here for example i can call my model and load it using tf dot keras actually i don't think i have okay import tensorflow as tf i did not import tensorflow in this module here so tf.keras.models dot load model and here i can give it a path to my model so my model is saved in this path here so whenever i call my script it's going to find it it's right next to it and then what i can do is for example maybe i might be i can basically print my model's architecture and the most important thing is that we can use the model to evaluate the set the sets that we have so i want to evaluate my model on the validation set and on the test set so we can also do this very easily here by passing the generator directly and maybe i can add a print here evaluating validation set and evaluating test set and here i'm gonna call model.evaluate on my test generator so let's now if we run the code the training will not start we will only have the ma the data basically loaded into these generators and then we're gonna evaluate those generators so here i'm gonna call my script again everything is saved let's see if this runs without any problems okay again since we have the generators it's gonna show us this uh i don't know this is actually a summary okay let me save this i'm gonna run the script again hopefully there are no more errors let's see okay the architecture is here and then the evaluation has started so now it finished evaluating the validation set which is 99 percent accuracy 99.67 to be exact and on the test set we'll see how much it is so on the test set we see that we get an accuracy of 96.81 which is not too bad it's really good so this means that by training our model on these uh images here and by testing them on the test set that we organized these images were never seen by our model and it still achieved a very high accuracy of 96.81 so now you know how to basically train your model and also how to evaluate it let's see what we can do next so what i would like to show you now are maybe some things that you can do to improve the results even more because when you're doing machine learning or deep learning you're going to be doing lots of experimentation and your goal will be to always improve this test accuracy this is the most important metric for you when of course when you're doing a task such as classification if you have a different task for example object detection then maybe your metric will be mean average precision for example but here since we have classification we want to improve this accuracy even more if we can so i want to show you some few techniques that you can do in order to try to improve the results the first ones are just by basically doing lots of experimentation by changing these values here so for example you can train for longer i've only used 15 epochs maybe you can go longer 30 50 even 100 epochs and see if that can improve the model you can change the batch size you can also change your deep learning model architecture you can add few layers remove few layers you can change these numbers of filters here you can for example experiment with global average pooling you can maybe remove this and add flatten layer instead and see how all of this can affect your results so as you can see you have a lot of things that you can experiment with and also this is from a maybe a model perspective but from a data perspective you can also do a few things so for example here you have the generator and we're using this image data generator class and in fact inside this class you have several data augmentation techniques so data augmentation techniques can help a lot in avoiding overfitting and it can also give your accuracy a bit of a notch and so that you can get maybe a few points more and for example if you're going to follow this path then i want to mention something important here so so far we created only one preprocessor that we use for train validation and test but this is because the only thing we're doing now is scaling so if we're scaling the images for the train set we need to do it for the validation set and the test set but if you start adding other data augmentation techniques for example let's say let me just check here shift or maybe let me just read a little bit here what can we use i think there's rotation range for example and rotation range for example you can choose i think it's in degrees so you can choose for example 10 degrees this means that your images will be uh will be rotated between 10 and 10 degrees uh you can add other things like shift or maybe width shift range yeah so this is to shift your image a little bit uh from left to right just a little bit and you can give it a small amount for example let's say 0.1 which means 10 so i want like my image to be shifted 10 to the left and 10 to the right and between these two so it can be like 5 3 this is uh going to be used as a range and you can do all sorts of other uh data augmentation techniques so the first thing that i want to basically emphasize here is that if you're using this as preprocessor you shouldn't use it for validation and for the test set and the reason for this is that the validation and test sets basically represent the sets that your model were in your model where will encounter in real life so in real life uh it's not necessarily you're not necessarily gonna have these shifts because these are synthetic augmentations in real life you want to test only on real images but when you're training you can do all sorts of these augmentation techniques so in order to avoid this you should always create two uh two preprocessors for example you can call this train preprocessor and you can for example add another one you can call it test preprocessor for the test preprocessor you remove these you only leave the scaling part because it's necessary if you scale the train set then you have to apply the same thing for the validation and the test sets but then you can use this preprocessor for the training part and this for the validation and the test generators so this is how you would do it and this could also bring some improvements to your model so just make sure you follow this kind of methodology when you're doing these different data advancation things and apart from this as you can see you have several things you can do you can add more augmentation you can change you can even change the target size maybe use 40 by 40 or 80 by 80 see if that can improve the results or not so i definitely recommend and i urge you to experiment with this and see if you can beat this accuracy which is 96.81 and this is gonna help you basically learn more and it's gonna show you the power of deep learning sometimes just by changing a small parameter you can get a large improvement and then one last thing i want to show you also in terms of things you can improve for example here for the optimizer i have passed it as a string because this is one possibility to do it another one would be to create an optimizer here and you can use tf.keras dot the optimizers let me check so tf optimizers and here you can call for example the optimizer adam and this is something that you can find in the documentation so tf keras optimizers so the documentation can be very very helpful when you're working with tensorflow they have lots of examples they have lots of uh of tutorials that you can use for your projects and here so what i wanted to show you is that if you use the adam optimizer like this then you have the option to pass some parameters to it so here if we go to the documentation you see that you can pass the learning rate for example which is very important parameter in deep learning these parameters here i usually don't touch but for the learning rate i do lots of experimentations with it because i have seen that it can have great effect on your model so here for example you can add learning rate so lr for learning rate let's say 0.02 uh so this 10 to the power of 4 and then i can pass this as my parameter here r let me check just if this is the correct way to call the parameter yeah learning rate and then i can pass it here and then you can this is another hyper parameter for you that you can play with and see if you can improve the results and then the way you would do it is that you can pass this optimizer here like so so this is also something that i wanted to show you just for the adam optimizer you see you can play with the learning rate value here and also since we're talking about atom optimizer just a quick note you see here that there is this last parameter called or this parameter called ams grad and it's by default set to false this is actually a variation of the adam optimizer i've read a little bit the paper that showed this technique and they showed that in some cases adam actually fails to converge and by converging i mean it fails to basically get the loss function to go very low and in those cases in many of those cases ams grad can help so for example you can also if you're using adam for your deep learning task and you see that the loss function is not going down then maybe one thing you can experiment with is to set the ams grad to true and you can test with this variation of adam optimizer so as you can see you have a lot and a lot of things that you can do but just this this set of parameters and this set of techniques that i mentioned now can help you improve the accuracy by a large margin so when you're done training and evaluating your model what you basically need to do at the end is to use that model in your application it could be a web app web application it could be a mobile application or an embedded application and for that what you would usually do is uh use this standalone script that just loads your model and for example it takes a path to an image as a parameter and it gives you a prediction on that image so this is what i'm going to show you now so let's create a new script and i'm going to call it for example predictor dot by let me just maybe choose my predictor dot phi in this example what i want to do is uh to show you how we can just load an image load our model and make a prediction with our model on that image so let me maybe start here by defining this part here and i want to basically have a pad to my image let me call it maybe image path and what i would like to have is some sort of a function called predict with model and this function would take image it would take the image path um maybe it would take my model and image path and it would return to me the correct prediction so it would be something like this so now let me define this function here and maybe let's start defining it here and maybe later move it to the utility script so define predict with model and it's going to take model and image path so here what i want to do is to first load the load that image and then use my model to predict on that image so first thing uh the first approach i'm going to use is by using purely tensorflow functions so let me import tensorflow stf here i'm going to use my tensorflow import here to load my image so i'm going to use here or create a variable called image tf dot io dot read file and i'm going to give it my image path here so this is a utility function that's going to help us load this image and save it inside this variable called image and then i want to basically decode my image tf dot image dot decode png since all of my images are png images so here i'm gonna do this decode as png so i'm gonna decode my image and then i need to choose the number of channels since it's an rgb image i'm gonna choose three then what i'm gonna do is basically rescale my image and in order to do that i'm gonna use a utility function also so i'm gonna do image equals tf dot image dot convert image d type and i'm gonna give it my image as a parameter here i'm gonna choose the type to be tf dot float 32 so in fact just by doing this during this part of the of the code here i'm reading the file reading the image using this read file method i'm decoding it since it's a png image i'm using decode png and then by using this function here here convert image d type it's going to convert my image pixels since now they are unsigned integers in eight bits it's gonna turn them into a float32 uh type and also it's gonna scale them by turning the pixel values between 0 and 1 instead of 0 and 255 so without even mentioning here the scale value by saying for example divide by 255 this function is going to do it for us and you can check that by going to the documentation regarding this function here after that i'm going to have to resize my image so i'm going to use tf dot image dot resize and i'm going to give it my image and then a list of the width and height and here i'm choosing 60 by 60 because as you remember in the generators here or in the flow from directory we are using target size 60 by 60 which means that all of our images are resized to 60 by 60 so we need to keep that same order when reading an image to pass it through our model so here we have resized our image and last we need to basically expand dimensions of my tensor and here i'm going to give it my image and axis 0. so just to explain what this does in fact at this level here we can have a shape of 60 by 60 by 3 but by doing this by expanding the dimensions we're going to get a shape of 1 by 60 by 60 by 3. and this is the format that our model is expecting our images to be in so let me check here as you can see the input layer expects none then 60 by 60 by 3 and this none can be anything but there needs to be this dimension here so if we're using one image to pass it to our model we need to set that first value to one to say that this is one image of size 60 by 60 by 3 and then our input layer will not complain because this this basically follows the same shape that we defined during the training and then once we have our image ready what we can do is call our model predict function and we're gonna pass the image to it so this is gonna give us a list of probabilities of that image belonging to one of the one of the classes so here i'm going to save my predictions to this variable here so you can choose this function to return the list of probabilities maybe you want to to use this for maybe using some threshold to say that this is acceptable or not so at this stage here for example you might get 0.005 for the first class 0.003 for the second class then you might you may you might get 0.99 for the uh class here the third class and then also the rest will be zero zero point something and for the rest of the other classes as well so this means that our model if we get a result like this and our model thinks that this image most likely belongs to class three or class two because our folders as you remember they start from zero so we start from class zero if we go to the third class it's actually two so if we do if we run this command and we get this list here then this means that our model thinks that this image belongs to this class here so what we want is uh basically to get only that class and we don't want to get probabilities at least for me that's what i want to do but for you feel free to maybe return this list here and then do whatever you want with it but for me since i just want to return the exact class that we got i'm gonna do predictions equals numpy dot in fact we can give it my predictions here and let me just import numpy in port numpy s and p so what this is going to give me is the index of the max value which is exactly what i want so in this case for example it would return let me just maybe do it in a comment here it would give me 2 because 2 is the index of the max value 0.99 and then i'm just gonna return my predictions so now we have our function ready and here let's choose for example one of the images in the test set let's choose this for example and copy as path so that we can quickly do that i'm just going to paste it here i'm going to add the backwards slashes here quickly and then of course i need to load my model so just like we did here we can load our model by using this command here so the model exists in this folder called models so we're going to be able to find it from our script mypredictor.pi i'm going to pass it to our function predict with model and then we're gonna get the corresponding class and then here i'm just gonna print something like okay i'm gonna use an f string prediction equals prediction so let's run our script and see what we can get let me clear this first python my predictor.buy and just do one last look here see hopefully we have no errors okay well the first prediction was actually uh wrong so this prediction it says this this is class 12 but it actually is class 2. let's test with other images as well so here for example i added this image here from the test set which which should give us the label 0 so let's run our predictor.buy and here as you can see we get the correct result is zero i know i know that the correct result is zero because i'm just looking at the corresponding folder here so we got the wrong result for this one the correct one for this one but this is basically this is not a way to test the model the best way to test the model is by doing evaluation as we did before but this is just to show you how you would use your deep learning model to predict on images in a standalone application so as you can see here we have an application this could be easily embedded in any sort of application web app or mobile app or whatever and basically we just have a function that reads the image uses the model and makes prediction with it and then you get that prediction and you can do whatever you want with it so now you have seen how to use your model in a standalone application in order to make predictions on specific images so just a quick recap of what we've seen in this last part so basically we used a german traffic signs data set that was collected and we had real life images we explored that data set we prepared the training validation and test sets we built a neural network the functional way we also created data generators we added callbacks to our fit method we trained and evaluated our model and we discussed some potential improvements that we can do to get better results for example data augmentation we also finished that part by by running inference on single images and building standalone examples on how you would use your model so if you have any questions regarding what we have covered so far then please feel free to contact me on my social media accounts so i have linkedin account and also twitter account and if you're interested in a free machine learning job ready checklist which is a checklist that i put together based on my experience working on machine learning projects in the computer vision industry then please check the link below there will be only one link but it's going to point you to all the necessary information regarding my social media accounts and also this free checklist