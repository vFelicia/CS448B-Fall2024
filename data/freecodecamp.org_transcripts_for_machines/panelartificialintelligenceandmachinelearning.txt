all right well it wouldn't you know so we covered uh we covered uh kubernetes cyber security it wouldn't be a good morning if we didn't include artificial intelligence uh in the mix here uh this is uh we're very fortunate to have a set of experts here this morning to discuss the ai landscape how it intersects with open source so i want to welcome all of them to the stage why don't you all come on up and uh sit here and i'll introduce each of you so uh deepak agarwal uh he is the vp of engineering uh and artificial intelligence uh at linkedin how many people here use linkedin a few of you uh so he's responsible for ai efforts across the entire company please have a seat um mazin gilbert is uh vice president of advanced technology at at labs he's heading up ai initiatives uh there as well uh let me make sure i've got my right list here uh terry singh is an author ai machine learning expert uh in uh both deep learning uh and ai at coursera uh and finally rachel thomas who's the cofounder of fast ai please give them a warm welcome so i want to kick off by just having each of you quickly talk about what you're doing in ai what your organization uh is doing um and uh uh what kind of business imperative it is for each of you so deepak i'm gonna start off with you so i'll probably start with the mission and vision of linkedin so linkedin for those of you are on it and for those of you who are not i'll encourage you to sign up uh so our mission is to connect uh talent with opportunity at scale and again opportunity i don't mean in a narrow sense of just finding a job opportunity could be any professional opportunity and so ai and machine learning is something that is embedded in all our product in fact we many times refer to be the oxygen of color product right so it kind of is a horizontal layer that permeates all our product so if you're on linkedin and you're looking for a job all the job recommendations you get is all powered through machine learning in ai if you're a recruiter trying to source a candidate then the search results which you're getting is powered through ai if you're on the news feed consuming content that's all powered through you if you're trying to connect with someone which you should for getting professional opportunities in the future those recommendations are powered here so it's kind of ubiquitous and we have been doing it for a long time now it's very mature and we are no longer in a state where we think about it it is kind of become an integral part of everything we do it's embedded in everything we do i think we are now at a state where we are thinking of what can we do with ai to power the next generation of user experience on the platform mazdan you guys are running some huge networks out there with hundreds of millions of users and 5gs just around the corner yeah here i see you're two in two cities yeah it's um so a tnt which i'm hoping that 95 of you guys are att customers today so thank you very much um so atnt's mission is is to really connect people with their world um where they live where they work and and really do it better than anybody else and when you think about a company who's who's their sole business is communication and entertainment ai is a foundation not just to drive one application but really to drive how we live and how we work as a society and basically globally so if you're thinking about well how would we drive 4g and go into 5g and how do we really scale cloud technologies all the way to the edge or really how do we try to try to drive um our network how do we operationalize our network we talked about security earlier on how do we make sure we get you know uh tens of millions of attacks every day on our network how do we really address those really ai is the foundation of pretty much our business from the getgo so uh i'm gonna start with the mission and the vision no i'm just kidding so my name is terry um i am a founder ceo uh neuroscience researcher uh studying brain and that kind of stuff uh deep kapha.ai i'm also a mentor at coursera uh working with a couple smart people andrew and a bunch of other guys who are developing uh deep learning specialization so uh help you know we're this year we have a plan to get about a couple hundred thousand to about a million people are trained in deep learning and ai and an amazing amazing training uh set up you know from stanford and andrew setting it up and i guess i know you do that a lot of stuff as well with fast ai but so so you know very short what i do is i i work with enterprises and and helping specifically they could be engineers software developers or they could be phd post docs as well training them to to work with uh deep learning techniques right composition neural networks or vision or or text kind of stuff you know for the guys who don't understand but uh so the goal is for for me the goal is to convert a whole lot of people to to adopt artificial intelligence or deep learning and and the other part of the things that i do if i'm not working with with the enterprise i work in the most difficult parts of the world i go to tunisia i even go to syria and and we work with women and these are young girls you know having all kinds of problems in the world and and for instance after this i'm going to tunisia after i'm done with this conference and then we go to turkey but we have you know people who were totally distracted with everything that life can take out of you and put those kids and say let's go and work and let's work with code let's work with tensorflow and this is this is what what i do so i guess i this is a bit of an introduction that's amazing that's uh that is incredible uh thank you rachel i your i checked out your book uh this is the practical deep learning for coders so lots of coders in the audience here yeah so um i'm rachel thomas i'm cofounder of fastai which is a nonprofit research lab and i'm also a professor at the university of san francisco and with fastai we're trying to make deep learning easier to use and so we do this both through kind of building the tools we have an open source library it's kind of very high level and encodes best practices called fastai and we also have a free course practical deep learning for coders over a hundred thousand students have taken it we've had students get jobs at google brain have their work featured on hbo but we're particularly trying to reach coders who don't have to have any advanced math background and we're interested in people that are i think particularly that are kind of working on projects outside the mainstream things they very care about very much care about and don't have access to a lot of resources and we've had students improve agricultural loans in india try to stop illegal deforestation of endangered rainforests help patients with parkinson's diseases um so a lot of interesting applications amazing very very cool well i want you know for for a lot of i get these questions all the time a lot of people in the audience find sort of the ai landscape totally confusing right that there's all these different tools and there's different ways to deploy and how you do it i think you're all kind of the exception in that you're in roles where you're just far down that path but like you know if you're talking to someone who is maybe confused about it what are some of the exciting areas what are some of the things that people should be looking at right now in ai i think i'll just kind of go back around robin you're starting with you deepak yeah so i think first of all uh ai before 2012 was very different than what it is today and in 2012 something great happened so you know a bunch of profess researchers they were able to figure out a new way of computing things at scale by using gpu cards and this is what kind of ushered in the era we are in right so i would definitely want you to pay attention to everything that's happening in deep learning so what what has happened is that you you have with the with the availability of cloud computing data management has become commoditized right and various and with the availability of deep learning tools and you know things like past ai and what andrew is doing slowly that's also becoming commoditized so if you have a problem where you know you want to predict something by using a lot of input signal this is slowly getting commoditized and this alone can be transformative look at what has happened in computer vision look at what has happened in natural language processing and speech i mean these things have become way more accurate than what they were like 10 years ago and the impact of that is pervasive right you know in every area now we are kind of we are able to use ai technology to do things that you're not not able to do before like for instance in the old days if you're a radiologist you will actually send the mri image to india i know and then someone there is going to read the image and so in the morning you have it on your desk you don't have to do that anymore right i mean you know you can have ai software kind of do that for you and again i can go on and on with with example so definitely supervised this is this class of problem where you're predicting some output based on input that's called supervised learning and this is slowly getting commoditized and this alone can actually have a very big impact in many different things that you do right so you have this paradigm where you can have data learn itself learn learn patterns through through algorithms right you don't have to write rules to kind of program a computer you can have a computer learn there are other areas that are not very well that are not very well researched and we have a long way to go like if you look at unsupervised learning or human level intelligence uh that machine can learn that's still an open research area and i think in the next 10 years or so maybe we will be we will be there and it may become as commoditized as as the supervised learning techniques are called how about you mazda like you know you've been involved in some pretty cuttingedge stuff in terms of trying to get this technology out there and deployed you know i'll plug accumulates a little bit uh for you you know in terms of building a marketplace where these things can be reused what's exciting to you so um just for those who who who don't know me that i i did my phd in the 80s in neural nets for speech i was excited about ai because the the concept of getting a computer to use neural nets which at the time there is there was this resemblance of artificial neural nets with the biological neuron that still today there's that sort of association and confusion to get them to have a machine actually articulate sounds and speak just like humans do was was just amazing and completely fascinating and that's how our house started and we were at the time like few hundred people we will go to workshop we will go to conferences um and even in the past 20 years 30 years is there there's been phases of interest of ai it went for a buzz to a hype it went down again if you look at the literature uh deep learning and neural nets and ai never actually stopped in the community in the research communities being worked on for for at least three decades it is different now and i agree with deepak here that the distributed computing the gpus the the the flux of of larger data um is is really made a big change what i actually think that a bigger driver to that uh we started with a few hundred people today thanks to you guys that we have hundreds of thousands going to the millions really the big driver the big revolution is open source if you think before is that you know 20 years ago when i was working on this is there there are a few of us who can write some code we would join very large companies who have deep pocket to have big computers great uh craig computers and others to be able to run these type of jobs that can require a lot of data today if you've gone to ces every company is now an ai company because to really be in that business now it's nothing more than just download a software and you can just get going and you can take a course at coursera or somewhere and you can pretty much get going in a very very short period of time and by doing that that's really trying to have created a huge revolution in the industry so what we did however even with that excitement and revolution from a company like at t and probably most of you are the same is that we are hitting big bottlenecks and our bottlenecks we believe those bottlenecks are tremendously large bottlenecks that ai cannot move to that next step of scale without addressing those the first one is that it's still the case that there are uh lack of understanding and and and training and learning about what ai is and where it can be used there are a lot of tools out there and the question is that which tool do you use and are they interconnected with each other we needed to figure out a way to harmonize those number two when you ask your team i need to build an ai solution they would go and start from pretty much from scratch there is no reusability of ai these are very expensive things to build it takes months it may take more years or so so what accumus is trying to do which we have announced on the linux foundation is that you create a marketplace a marketplace a distributed marketplace for ai so think of the app store with one difference is that with this app store is that the applications are built by many different tools it's agnostic to the tool that's being used that's number one number two is that the applications that you build in this marketplace they interconnect and interoperate think of them as microservices that interconnect so you could be using tensorflow and she could be using scikitlearn and you can actually connect the outputs of those to create new solutions the third thing is when we talk about machine learning a lot of people talk about data scientists and machine learners and so forth and then when you start thinking about what does it take to move that into production from an att it's sometimes months a year two years you have to figure out funding you have to get prioritization you have to put a team together the developers don't think the same way as the data scientists in fact these are completely different organizations okay that maybe they report to the same you know senior vp at some point what we try to do with acumas is really to streamline the process from a data scientist building a model to that model being completely production and do that in a matter of minutes as opposed to what it takes today and really have that as part of a marketplace that you can just download and run on any cloud and be agnostic to the cloud so this is a very big revolution it's a community we're all trying to really get together to change that i actually believe that without doing that it's going to be very hard for us to really move to where we are today where we are deploying ai for some applications to really making ai mainstream where practically a 12 year old kid who can design and build and deploy a website can basically do the same thing with ai well terry it sounds like you to some degree are doing that right you're working with young kids and they're able to take this open source tooling and do real things with it like what are your experiences there so yeah i think uh the the point that you've raised on open source is something we we just kind of assume you know it's like like fresh water you pick it up and you start building stuff so i want to say first thing is i i wouldn't know how to take these technologies uh either to the enterprise customers or into syria uh which i am going to be there on the 19th amazing you know we're going to be getting coverage from cnn and bbc and a couple other guys that's amazing but i almost forget that if it was not for you know you know i contributed tensorflow as well you know you can just go contribute to developer it's all open source it's like free you know it's like free as in super free that's that's great really i mean it's something i can just get on my hard disk and i can go and i can implement and and you can set up for servers you can set up virtual machines also free you can split up on virtualbox you know free up okay i think oracle has bought it but it's still free you can download it so uh you know i think that is something which i realized when i had a question so angela she she sent me a list of questions to all of us and i said hey you know this is the revolution it's like a silent revolution and you know guys like richard stallman all these guys everybody's been working on it it's it's really super we should be like super thankful for everybody all these millions of developers who make this happen so i can just pick up my laptop and and i can go to tunis where i'm going to be in tunis and there are like people pulling me all over the place so the political parties saying why don't you talk about ai because we need to really clear the air said okay fine so i'm going gonna make a businessy kind of presentation as long as you don't talk about that stupid robot called sofia but uh everything else is free you can take it and you can just implement it guys and and the practical example is this so i said okay so i have free stuff so what do i do and then you know so we said okay there is you know i can go into detail so i'll keep a little bit high level for example skin cancer detection is is something which you know you have you know human bias and then you have technology and and i said okay so i you know that stuff is free as well you can download it i have all those data sets in fact you know like i don't know it's like 40 50 gigabytes i can put it on my flash disk and just go anywhere and we can train on those data sets provided by is i see this international skin image classification society here in the u.s there are a whole lot of data sets from skin cancer images you know just identifying your moles or you know those kind of you know if it's some uh malignant or if it's benign or if it's never you know people have been teaching me different things i'm not a surgeon but uh it's it's like okay so we have free software and then we have a problem which we can solve and then what that led to i gave a bunch of lectures on different you know i we're researching capsules just like 10 people in the world who are researching on capsules building stuff and and capsule networks is like the next uh let's say convolution neural networks thing or the evolution of that and and so we take that and then i said okay so we're going to take a step further who wants to develop an app in android free stuff again and okay core ml with apple is also free you can download it and can build an ios app as well and people are building apps right now so i'm just back from finland where uh i know if martin's still in the room uh you know being at espo and they're like really smart researchers and we're building apps so it's all possible i didn't have to go and anybody to to ask money or permission from a manager who would say i need to talk to an account manager because this big corporate company needs to give you software and when you have software you need licenses you need this you need that so it's it's just available it's it's super amazing rachel i want to ask you though a question that i i get a lot you know i i agree i think that we're standing on the shoulders of giants in terms of folks like richard stallman who came up with this concept of sharing and critical open source licenses and all the folks who followed that you know whether it you know folks from apache or other organizations but one question that we keep hearing is all right that's code what about data right how you know is is data the new proprietary right is this where one thing that um so i think there's some misconceptions and that a lot of people think that yeah you need google size data sets and you need you know like millions of dollars worth of gpu power to do deep learning and those aren't the case and so kind of getting to your question about this like what if you don't have the data um a lot of people are releasing pretrained models so you know if someone has a large data set train a model they release that model and then you can do something called transfer learning where you are finetuning that to a much smaller data set so we had a student download i think it was just 20 pictures of people playing baseball 20 pictures of people playing cricket and trying to classify her to tell cricket from baseball just using 40 images and that's something because they were using this pretrained net you know and just finetuning the last layers and so there's really amazing potential there i think in terms of being able to get deep learning to work on smaller data sets and it's part of why it's so important that people do um share their weights and their models openly through open source yeah and you know we worked on a data sharing license agreement that would both a copy left one which would be a share back license then a permissive license which you just didn't didn't need to do that um but we're trying to get ahead of this ability to kind of share yeah and there are issues because it's like i mean data you know is an important part of you know of how models are trained um but there's also so much around privacy and anonymous you know true anonymization is almost impossible and there have been several kind of high profile cases of people thinking they've anonymized data and then it's been deanonymized and so i think there is kind of a bit of a tension sometimes between yeah wanting to protect privacy um and yeah like it is really important though to be sharing models in your training process yeah i wanted to imagine here because you know i mean you're the telecommunications market's very very competitive but you know to her point what what are data sets that you would want to share i don't know sell time with power maintenance or whatever that just isn't uh competitive per se but kind of follows that open source philosophy of like hey this is just data we want models that we just want to share with you do you see patterns there and so i i um i think the idea of of having companies share data is not new it's been people been talking about this for for several decades we've never cracked it we've never created an open shareable infrastructure community where people can easily with security can share data with hipaa requirements and their privacy rules i think we're starting to get almost there i think with you guys and and a lot of the policies you guys putting in place and that's what i'm hoping we're going to do with achievements achievements cannot really succeed without having a a really clear understanding of the data behind it from an att there are obviously data we can't share um there's no doubt about that you know we we we carry data and we track data about cell uh coverage so we those kind of data we cannot share but there are other data that we actually we want to do as part of the acumen to consider whether we can open a community looking at that just like you mentioned with transfer learning one of the we're a very capital intensive company so you can imagine that one of the things we do we send people out we own millions of polls we own thousands of cell sites macro cells small cells etc we send people literally up a poll to go and check a cell to see if there's something wrong with it if a wire is disconnected if it's rusty if there is dirt etc what we're trying to do now with transfer learning and ai is that we send a drone and that drone has visual capabilities that drone detect whether there is what the object it's looking at and we can't do that we don't have enough data to do that so we've just collected few hundreds of those data points and then that drones looks at to see if that object has a rust and if that's the issue that we can perhaps do something about or send somebody there or not 95 of the time we don't need to send somebody so there's a safety aspect we're using this for we could never do that by collecting significant amount of data we can only do it with small set of data but thanks to the open source community by having models it's not just data people think of data as raw data and i think raw data is very important and there are situations we cannot share raw data for for many reasons but there are there are derivatives of data there are probably an area we need to talk more about which is that when you build these models these models now reflect weights and and capabilities that that resemble the data you cannot probably do a reverse engineering they hide the privacy aspect but those models can be shared and those models with some additional new data can do something really remarkable and that's exactly the kind of things you're talking about deepak how about you like at linkedin are there similar patterns where you're finding these sort of commodity components like you know what how do you make those decisions how are you doing yeah i think so i agree with everything so one thing i would like to mention even if you if you're a developer we provide developer apis where you can actually get information about linkedin public profile information right so for instance you can you can you can use the apis to get a person's job title or you know if you want more information about a company like how many employees work there and how many people have changed up so these are things that are available through the developer apis there are some other information that we provide to companies those are not available for free so you have to talk to us and based on the use case we can still provide that to you so that's already happening now i think one example for instance i can give you right away for instance you know one of the things challenges we face on our news feed is there is a lot of uh nonprofessional content that gets like things like hate speech and porn and things like that so we don't build those models from the scratch like for instance there have been a lot of nice models that have been built by using imagenet data using resnet and again we use the same technique like we chop off the last two layers of the neural net and then customize it for our use cases we have a lot of different teams that are working with different problems so we have a notion of what we call internally a feature marketplace right so features or signals that go into your machine learning models and we don't want every team to be building the same signals over and over again so there is this there is there is a framework which where you know if you create a certain user interest vector for instance or signal that kind of captures a user interest you can actually share it with every other team right so it goes into the feature marketplace and anyone can then grab it and start using it in their model so that's how we are scaling it and you know we are in a world where we are we are not just doing machine learning we don't have only experts doing machine learning anymore we have opened up the machine learning to every single software engineer in the company so we have training programs where every software engineer can get trained in machine learning we have a feature marketplace these think of these as cookie cutter prefabricated features if you will right and so if you are a developer you take a course you have prefabricated features available in the marketplace and you have a problem that you want to solve in the product you can go take these prefabricated features build a model and deploy it in your product you don't even have to talk to an expert in many cases okay i had a question this morning that i want to i'll start with you tyrion but i want to go to rachel and as and and come back to you deepak that uh it's not necessarily on our list of questions so i'm going to surprise you on this one there were a list of questions this yeah there was a list of questions um so you have a lot of developers and people who care about coding out here and um uh direct you'll remember this on the 25th anniversary of linux i made a toast to the colonel community and folks saying you know congratulations on 25 years of linux and i want to announce our next big project after linux which is an artificial intelligence uh and uh that is actually self a selfcoding platform so drink up because tonight's your last night of employment but this is i get this question all the time and there there's startup there's a startup in spain called source which sort of cached all the code that's on github and never been written in a lot of other repositories just as experts i'll start with you terry when are we going to get to self co to to aware coding like to the ability to either like azure coding to use ai to improve the quality or to actually have selfcoding systems crazily i get this question all yeah yeah you know um what you probably are never gonna get and i hope you're not gonna ask about these general ai kind of things because i kind of shut down i i get it i told you yeah yeah the the evil robots take it yeah yeah yeah yeah yeah yeah yeah no no this is specific so i think it the most beautiful thing is that there's a whole lot of code out there uh the intuitions behind the way the code has been written is not something you can encapsulate in software and create a kind of an automated software development kind of library that says you know these guys are great in in convolution neural networks image classification kind of technology that these guys are great in in recurrent neural networks it's advanced kind of you know text analysis and making kind of predictions what you will not get is the intuition of what is coming next so as long as you are building things for instance just an example uh we know you know there's been delay and i've studied astronomy as well so i really really follow a whole lot of things and i code also on the side to to understand how we're kind of you know learning about gravitational waves and all that so we have web going into the space next year sometime in june and it keeps getting delayed it's really sad but anyways so we're going to get huge humongous amount of data coming out of the universe at us and and to do that you that's you know that software which we are super excited and selfcongratulating that you know we have great software it's not going to help to a certain extent it will definitely help in best practices and and you know doing unit testing and all those things definitely huge scope in making those things work and automating it i say really i think we should automate that so from that perspective i think shouldn't worry too much about jobs getting out of you know the door because there's a whole lot of beautiful things we need to do if you need to you know colonize mars or you know something i wanted to do as well as a kid so i said like when you know this gentleman from south africa who comes to the u.s and sets up a bunch of companies says says hey okay makes sense but so there's a whole lot of beautiful things we need to solve in the universe so from that perspective there's definite like 60 70 percent of encapsulation of doing things which we don't need to do anymore can be grabbed literally from github and and from even you know best practices from stack overflow whatever and you can put these and kind of provide guidance so people don't lose time we spend a lot of time doing a whole lot of stuff which we should not be doing so from that perspective i totally agree but i don't think my cognition as a human being a single human being i can already envision a universe i don't need like 100 people to do that i can do that already you know that power has been given to me the other thing is my intuitions of trying to solve a problem in it could be any problem here right now physical problem object detection problem is something is has doesn't exist at all um in in in software library what does definitely exist is best practices which are definitely search and seek out to and so yeah i think so i guess you know 60 70 percent of that work definitely can be automated so self encoding well we can call it self coding but basically just grab information understands how you want to follow that logic and eventually you know throw it into your uh into your algorithms and or in your software library and do that and the other 30 percent just keep you know hold on to it you know your intelligence and and your beautiful cognition and your powers to you know uh grab the universe and make it your own or yours they're not going anywhere they're not going away all right quick quick last word from rachel mazen and deepak on you know advice to coders out there and how can they use mlai to improve their projects to yeah i mean i would say with um i mean one just to know that it's possible if you know how to code you can learn to use deep learning um that domain expertise is still incredibly valuable um and so i think um something i hear a lot from companies is you know like oh it's so hard to hire like a stanford phd and it's just that's not what you need at all like the people that are already working with and for you are kind of the right people like they understand your problem and your domain um and so i guess again this is something that i think is like whatever um and i know here we're a lot of coders but um like having specialized knowledge um around a domain is still super crucial um so this showed up recently mit released a deep learning course and i don't know anything about the course but the image that they led with was like you know see why the algorithm predicted pneumothor thorax you know it's a picture of the lungs and a radiologist who's also a machine learning specialist responded and was like oh that doesn't make sense you know like uh that model must have been overfitted you know and so that like that kind of um domain expertise is going to remain very valuable for a long time and like one of our goals at fastai is to kind of be taking the domain experts and teaching them deep learning as opposed to trying to engage a deep learning specialist with your particular domain yeah mazdan deepak your last word on that debug yes so so i think in order to machine learning or ai you need three things one is you need to know what your objectives are right you need to know what you're trying to build the algorithm to do you need data label data and then you need algorithms that can learn from data so if you have a simple objective like identifying a cat in an image that's easy i think that is something that you can put together very quickly like with few lines of code because all the other materials are available right as long as you have label data but let's say if i have to solve a more complex objective like i want more users to come to linkedin every day this is a very complex objective right like users can come to linkedin because they like the news feed because they want to connect to people because they want to search for someone how do you formulate a series of machine learning problems that can actually solve this objective is very difficult like this you cannot in cos encapsulate at least today in a software right so that's where you have to go and understand the domain very well do some data analysis and once you're able to formulate those series of objectives then again the coding is very easy right so i would say i think in a few years everything else will get commoditized pretty much not completely but pretty much and we have to spend a lot more time understanding what you're really trying to solve like if you want to do job recommendations without caring about diversity well you know if you if your algorithm does not take care of that it's not going to optimize for that it's just going to optimize for number of applications but if you tell the algorithm now i care about that as well and put that as part of the object then it's going to do something about that so that that's that's probably what's going to happen in the next 10 years as we start commoditizing and we will be able to solve more complex problem than what we are able to do today um coders you know doesn't matter what your background is absolutely get involved do some training and learn this field and my organization every code every programmer has to doesn't matter what their they have a phd or a master's whatever it is 100 compliance this year they all have to learn it they all have to be able to code machine learning and ai capabilities at t in general we put out six months ago a plan a program that everybody in the company we have 300 000 employees to be able to go through mlai training even if you're a marketing or legal or whatever we all need to know this is not about just coders it's about everybody we the people in the legal team they need to understand and the marketing team needs to understand it uh financial people everyone really need to come to the same page so we go away from every problem ai is the solution that hype too really what are the key problems we need to solve for from an atn t and from even the coders is there look where the problems are where the challenges are as i have always told my team three things look for where spending a lot of money to look where there are opportunities of revenue we can have and we could do with these technologies and three is safety there are places where we can apply these technologies and i mentioned the thing about polls and the 5g for safety and i think that's really if you can start with a real problem just like what dpad just mentioned real problem that really wants a solution that's probably a long way to go so here here if you're not training everybody in the organization you're going to be caught up in a neverending hype cycle uh and that's a pretty interesting practical way that att is handling that problem so thank you everyone i really appreciate you uh coming here today