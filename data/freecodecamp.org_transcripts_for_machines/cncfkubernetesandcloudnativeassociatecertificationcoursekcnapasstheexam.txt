hey this is andrew brown your cloud instructor exam pro and i'm bringing you another complete study course and this time it's the kubernetes and cloud native associate made available to you here on freecodecamp so this course is designed to help you pass and achieve cncf issued certification and the way we're going to do that is by going through lecture content doing follow alongs in our own account we've got a full free practice exam so you can simulate the real exam and we've got cheat sheets on the day exam that you can go and print out and cram that last minute knowledge so that you can pass that exam and prove on your resume and linkedin you have that kubernetes knowledge and get that cloud job or that promotion you've been looking for so i'm your instructor andrew brown i was previously the c2 of multiple tech companies 15 years experience with five years specializing cloud i'm a nativist community hero and i've published many many free cloud courses just like this one and if you ever want to be buy me a drink coconut water is what i like so i just want to take a moment to thank you the viewers for making this free course possible because it's you who buy our additional study materials that allows us to produce these large free courses and if you want to support more courses like this one the best way is to sign up on exampro.comforce kcna to get that additional study notes flash cards quizlets downloadable lecture slides downloadable cheat sheets practice exams ask questions get learning support and more and just by signing up for free you're going to get that free practice exam and cheat sheets with no credit card required and no trial limit so if there are updates of this course the best way is to look on the youtube and to see if there has been an update if you click that that's where you're going to see things like corrections additions modifications to make sure you are using the latest version of this course and to keep up to date for upcoming courses the best way is to follow me on twitter andrewbrown and if you do pass the exam or you'd like to know or would like to suggest what course to be produced next you can go tell me that on twitter and it's time to jump into the course hey this is andrew brown from exam pro and we're asking the most important question first which is what is the kcna's the kca stands for kubernetes and cloud native associate and it's the entry level uh cncf certification so cncf is the organization that is issuing this certification and so this certification is going to teach you the landscape of cloud native technologies a close look at the core kubernetes components a quick look at the vast amount of cncf projects and cloud native tooling a general overview of security deployment monitoring the structure and governance of cncf and the community around cloud native so the course code here is the kcna um and it doesn't have like versioning like other certifications like aws or azure where azure would be something like the az300 and then go 301 302 for the next versions um and a bus would have like co1 co2 co3 on the end and so they just don't do that's the only way you know what version you're on is you'd have to go to an obscure github page and see the curriculum version there but we are on version one uh for this so consider this version one kcna um and kubernetes is one of the hottest technologies being adopted in the world it's in the top four there with aws azure then kubernetes and terraform so definitely worth it to add to your journey so uh who is a certification for well considering taking the kcna if you are new to cloud native in kubernetes and need to learn the fundamentals you are an executive management or sales level and need to acquire strategic information about cloud native for adoption or migration you are a senior cloud engineer or solutions architect looking to quickly add kubernetes and cloud native to your skill set and so notice that we have um three asterisks up here and the reason why is that um unlike other fundamental certifications this one feels like it was made by engineers people that want to use kubernetes as opposed to people that are from the business perspective and so it's missing things like uh adoption frameworks uh cost like total cost of ownership shared responsibility model um uh migration paths so for that i you know i kind of fill in the gaps best i can but it's not on the exam and it's kind of a misstep of a fundamental certification in my sense so if you are in the executive management sales level and expecting to have a lot of that stuff you're going to be a bit disappointed to find out it's a lot more code than you'd like but that doesn't mean you shouldn't take it just means that you might not want to sit the exam because it might not be worth it because you'll fail because you're not really trying to become someone who can code in kubernetes as opposed to just understanding it from a managerial perspective but for anyone that is like wants to be an engineer like a kubernetes engineer this is the right course for you so the kcna is a difficult uh exam for entry level and you're going to need both handson and broad conceptual knowledge of the kubernetes cloud native projects and i really stress handson now this certification is multiple choice so technically you could do without handson but to contextualize a lot of the questions or even to go to the next step of certifications you're going to be in bad shape if you don't do handson so for technical implementation roles like kubernetes engineer or cloud native engineer this might not be enough to get you that role if you already have prior experience like you work a tech job then yes it is a good addition uh and it can get you a role as a kubernetes engineer but if you're just like from starting from zero then you might need a bit more work or go further down the uh road the roadmap there so when you complete the certification you'll be able to deploy a simple application into a kubernetes cluster understand uh various kubernetes components but not enough to deploy a production ready application so really missing those complex production setups like for deployment security uh like working with a lot of different microservices and things like that so let's take a look at our roadmap here so we have suggested prequels uh prerequisites and kubernetes track now the thing is i never ever ever ever at a fundamental have to recommend suggested prerequisites so this should tell you something about the difficulty of this exam you will really just say i have everything inclusive you can go ahead from day zero and and get into the cloud right but this one uh you're gonna need some linux knowledge you're gonna need some uh linux networking knowledge it networking cloud networking whatever or a associate level cloud certification if you can pass one of these then you probably have the knowledge for the ones above there i'm not saying go sit all of these i'm just making suggestions of one of those categories here and then from there i then i would recommend to proceed the casing a you can do the kcna without doing all this prerequisite stuff you just might be a bit more confused than expected okay and so after that i would generally recommend the ckd because once you are done the the fundamental knowledge of being able to work with clusters usually people want to deploy them and so uh you know there is the kca which i recommend next but that one the kca administrator is more about like managing nodes and selfdeployment of clusters where the kc ad feels more practical and it says application developer but it's really not you're not building out apps per se it's more like stuff around them and so since a lot of providers are managed this feels like a better fit for for most people after that you can go with the cks but if you really want to call yourself a kubernetes engineer you're going to have to at least make it to this stage right either one or the other depending on or both depending on what you do but generally like once you're once you go beyond the kcna all these things are pretty much like the kcad seek cks and cka they're all the same kind of difficulty just in different kind of areas and honestly i feel like they should have been like a pro cert you should have just studied them all together um because it's like not that much work to do the extra ones the key difference here is that this here is multiple choice these are all handson and so that's why i say that i have to spend extra time with you doing handson because i'm preparing you for the next level if you don't do those you're going to really struggle the next step so doing those handson is very important how long should you study to pass well um for this beginner i have 50 hours at one end so if you've never done linux networking or cloud you're going to be doing it outside of this course but trying to find to fill that knowledge if you've never written a code or held a technical role it's just going to be a lot of work for the experienced if you have worked with a csp like a bus as your gcp linux and linux networking you're looking at 20 hours um that's still a little bit long normally i would suggest like 14 hours and the course is like the video content is not even um uh you know like 20 hours but the thing is is that you're going to have to spend a lot of time doing handson labs and you have to factor in the time that you're going to be doing practice exams so it is of course longer there the average time i would say is 30 hours um so 50 lectures in labs 50 practice exams and we're recommending a couple hours a day for three weeks out uh normally like i would say two weeks for a fundamental sometimes even a week like in the old my old clf c01 this uh it was certified club petitioner we would recommend like literally like book it the next week um but this one is really hard so you're going to need a bit more uh time to get there okay what does it take to pass the exam we'll watch the video lectures memorize the information do handson lab this one is especially important we call them followalongs but um i strongly strongly strongly strongly recommend doing these within your own account and when i say that like we're going to be doing all in aws on cloud nine and uh there are sandboxes out there and they are an option if you do not have money and you do not have a credit card but um if you can please do them in cloud nine and follow along because if you do sand boxes they're abstracting away a lot of things that you that could go wrong and that's the parts that you you need to actually become a kubernetes engineer dealing with those hard bits so you are cheating yourself if you use sandboxes but i understand why people want to use them because they're just they want to kind of have that okay so do paid online practice exams the same with the real exam the great thing is that um if you sign up in exam pro today no credit card require kcna i got a full free practice exam for you extremely generous uh and if you want you can definitely sign up with a credit card to get access to the rest of the practice exams all the rest of the lyric content cheat sheets and more would greatly appreciate it so we can produce more free video content our content outline is out of five domains so each has its own weighting that's going to determine how many questions so we have fundamentals container orchestration container native architecture cloud native observability cloud native application delivery you'll know some say like between x like 27 20 questions it's because if you do the math you end up with like a decimal point for how many questions there are so that means that they could have one additional or one less on certain ones there and most of it's fundamental so fundamentals is strongly focused on kubernetes components so where do you take this example either in person at a test center or online for the convenience of your own home um for these which is through the linux foundation uh i know it's cncf but like cncf is part of the linux foundation and so you'll go to linux foundation to sign up for this and this uses the psi network of test centers and online proctor exam systems so it'll look like linux foundation has their own thing but then you'll end up on psi and if you can please take it in person because online it's so stressful you know like your dog could bark your kid could cough someone could knock on the door um you know the internet could go out and the checkin process is always painful it's not psi it's any of them and uh you know if you want to just be less stressed i would suggest in a person if that is an option for you if it's not then you have to do online um i think psi they're the more strict ones so pearson vue uh i prefer pearson vue but psi like you can't even have anything on the desk so like for me i like to have a box on the desk in the middle of the room to prop up my laptop they won't even let you have that box to prop up it up so just make sure you're really comfortable and you have a table that's a good eye level for that if you don't know what a proctor is it's a supervisor or person who monitors students during an examination so let's take a look here at grading so the passing score would be about 75 percent i say about or around because it might use scaled scoring usually the exam takers will point that out that just means that um the system isn't exact and so it's possible to fail with the 75 percent um and since it's on psi i just assume that it's following like what all the other providers are doing so always aim for you know more at least one percent above there but you probably can pass at 75 percent always aim 10 above there are 60 questions and as far as i can tell there are no unscored questions unscored questions happen on exams for many reasons uh so like eight of us has like 15. i think it's like 15 or 10 no it's 10 i think it's 10 unscored questions and um you know they're just oddball questions to help detect cheaters to see if they want to introduce new questions things like that they don't do this here with the kcna so every question is scored and there are some eyeball ones in there so that's why i thought there would have been some but there's not you have about 15 questions you can get wrong there is no penalty for wrong questions the format of the questions is multiple choice multiple answer when i take exams i remember the number that i get wrong and actually count in my head like okay i think i got this one wrong i'm okay you know what i mean um the duration you got 1.5 hours so that means 1.5 minutes that's 90 minutes uh your seat time would be 120 minutes see time refers the amount of time you should allocate about the time that you're sitting the exam this includes time to review the instructions show online proctor your workspace read and accept nda complete the exam provide feedback at the end um and really checking online is super super super stressful because every time i've done it something's gone wrong you know what i mean like they don't like my card they make me check the whole room again there's a lighting issue or i'm taking i'm trying to take a photo but it's saying like it's blurry but it's not so you know just make sure you have ample time there and uh even if you do do that like sometimes they start the exam early just because you're ready um so it might not be exactly the time that you take it but um and it's similar to in person as well if you sometimes show up early they'll just say hey you can go um and so this exam is valid for three years or 36 months before recertification i don't think you can sit in an exam uh if you have one active that's usually the the thing there you get two attempts to pass so if you fail you get another try by rescheduling a future date i actually failed my first attempt because when i went in i just i knew i had two attempts and so i i sat it without a single like a study and i got 74 which is pretty good to show like how much cloud knowledge carries over um but i you know i would say that uh you know you're paying for two you should take two so uh you know if you want to uh fail it and go sit it that's totally fine because then you kind of get a sense of like okay i need to brush up on some areas that i don't know um but uh you know it's up to you but there you go all right let's take a look here at the kcna so i just typed in kcna into google and it made our way here and so there's a register for the exam here at the at the current cost at least for me it says 250 and we can see the five domains but that doesn't really tell us a lot so there's a handbook down below if you click that it gives you way too much information on how like taking the exam works so they have a lot of information here to me it's a bit overkill but um if you need to know anything it's it's in this handbook and this is generalized for any of the linux certifications and you're thinking well isn't this the cncf well yeah but the certification is through the linux foundation because cncf is a uh suborganization of the linux foundation so you can roll this way notice they have a bundle for a course for 50 bucks more i can't really speak to the quality of the course um so i'm not sure it kind of looks like it's pieced together with other content so i mean that's going to be your decision if you want to do that um there are always a way to get kind of a discount 250 dollars is pretty steep um and so if you try to reach out to like maybe like a cncf um community ambassador maybe they'll know where a coupon code is but try not to pay full price because i didn't i got a code somewhere but here you can see the breakdown again for uh the different kinds of categories here we'll just expand it here and another place to look is the curriculum and i wouldn't call these curriculums they call them exam guides curriculums are very detailed but this is what i'm talking about where we see the versioning right and so here they say kcna and they might have some resources here and i'm sure that they will add my resources here soon enough because i just have yet to publish my course but i could see mine being on the top there very soon um but yeah so this is the curriculum here so we'll open enough and give it a moment if we scroll on down here are our categories so not very detailed compared to other other ones but let's just take a look here so under kubernetes fundamentals we have resources architecture api containers scheduling and really this is knowing the components of kubernetes okay there's a lot of different components and we'll go through that then we have container orchestration so this is understanding uh run times what is an orchestration system the basics of security like the four c's of security networking and cluster networking uh very little knowledge we need a little bit knowledge on service match how does storage persistent volumes uh go into play the network cloud native architecture we have uh the fundamentals auto scaling serverless community and governance personas now personas is weird because it doesn't really show up on the exam like there's a page about personas um that talks about all the different kinds of customers that could use it but really the only personas you really see are sres and there might be like one or two questions so if you do see something asking like what kind of roles whatever it's probably going to be sre so that was kind of weird one open standards like all the types of interfaces that can be used telemetry and observability so things like open telemetry prometheus grafana cost management that's another squishy one where uh they don't really have a good definition of cost anywhere maybe it's in that paid course by them but it's definitely nowhere else online so i had to kind of piece that together and they only ask you like one pricing question and it's such a bizarre question that uh you know it's just what it is uh then underneath we have application delivery so like uh git ops ci cd things like that you should just know generally the different types of deployment tools uh for get ops but you don't really have to know how to deploy i mean you technically do but not like with a pro like not with like flux or not with argo nothing super complicated but you do need to know the different types of deployment strategies so hopefully that gives you kind of idea of what you're going to be dealing with here but there you go hey this is andrew brown and what i want to show you here is just where you can access that praxis exam on the exam pro platform so once you sign up for your account there if you scroll on down this is where the prax exams will be you generally should not attempt your practice exam until you've gone through all the lecture content and done the follow alongs because you know once you see those questions once you're you might prime your mind in order to uh remember them and so uh you know i always say try to take it like a serious attempt now right now the time of i only have one and two in here on the publication of this course we should have all three in and there will be a free one so if you're on the free tier you'll see that shown up at the top there but really what i want to do is just show you what some questions look like so here you can see i have one uh running uh prior so i'll just go there and resume and so i just kind of want to you know click through a few questions and just talk about how these questions are maybe different from other exams and how they're formatted so one thing i noticed for the official exams is the questions are very short at the top they get right to the point um and you may or may not like that but that just means that usually um the choices are a lot more complex and as you can see in this example it is actually asking uh little code things like cube ctl commands and for the exam they absolutely will ask you technical things um even though this is a fundamental course they made it it's written by engineers so they're really testing your knowledge and so you can expect to see those kind of questions there sometimes questions are as simple as choosing the right service or product or tool so here you know it's just a list of things and so you'd have to know uh which thing to choose there as well and then you will see conceptual questions where there's not a lot of these usually everything is tied to something technical uh practical but you'll see conceptual questions like here where they say what architecture automatically optimizes for cost in kubernetes and so for that one i would choose serverless because serverless can scale to zero but that generally gives you an idea that the questions aren't um hard to parse but they can be tricky based on the choices and um there was ones where you know if you knew too much about kubernetes and you were doing this you might know that there is a more right answer but always go with the answer that seems the most obvious even if you know of those technicalities and you will do very well on the exam so hopefully that gives you an idea of what those questions look like again don't tempt these until you go through the whole uh course material first and and practice your flash cards and things like that but uh hopefully that gives you an idea of what to expect so there you go hey this is andrew brown from exam pro and we are asking the most important questions first which is what is cloud native so cloud native describes an architectural approach that emphasizes application workloads that are portable modular and isolate between different cloud deployment models and cloud service providers so that's my own definition because when i looked up cloud native and as i understood it it was a little bit more complicated than that and the context seemed to change depending on who was using the term so one example is with cloud service providers and so a cloud service provider would be something like aws azure or gcp and when they would use the term cloud native and this is my first introduction to cloud native when i was using aws was it meant everything built on top of the cloud service provider and so what i learned was that this is more uh meaning cloud first so where um you know you could start on premise like in your own data center but if you choose to build all in the cloud provider that is a cloud cloudfirst approach and so that's where that term kind of got a bit muddy and so that's why i redefined it up here to say something that you can take you can build and you can move it to anabus azure gcp in theory there's also kind of these like diagrams online that talk about the components that go into cloud native and so the idea here is that they'll say modern design automation micro services containers and backing services so the idea when they say modern design modern design would be using a modern architecture which below is microservices and containers uh or serverless architecture and then automation which is just a component of it and then backing services meaning like okay most of it is going to be probably portable modular isolate but you might leverage backing services like closer providers services or other cloud native services that run anywhere so some describe uh cloud native being four key principles so you can just see there's variations on this micro services containerization continuous delivery and devops so it really depends in the context and so the context of this course cloud native will mean technologies like kubernetes and the cncf projects which we'll talk about this course that are both distributed and agnostic to any cloud service provider and just to really uh be very specific i found this definition by the cloud native computing foundation it was in one of their documents um somewhere so i don't know who wrote it but it is by them and their definition was cloud native technologies empower organizations to build and run scalable applications in modern dynamic environments such as public private and hybrid clouds containers services meshes microservices immutable infrastructure declarative apis for that approach these techniques enable loosely coupled systems that are resilient manageable observable combined with robust automation they allow engineers to make high impact changes frequently and predictably with minimum toll the cloud native foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source vendor neutral projects we democratize the state of the art patterns to make these innovations accessible for everyone and so what i get from that when i read it is that the cncfs approach is that it's democratized right so if you are using a classical provider like database azure gcp you're really buying into that whole system but the idea is that you buy into cloud native you have more uh portability and uh you know there's a bunch of people that are involved to make sure that you aren't uh you know being sucked into a gravity well of a particular managed provider so you know hopefully that gives some context it is again a squishy term but uh you know we have to kind of define it okay hey this is andrew brown from exam pro and we are looking at cloud native versus cloud service providers because sometimes people get these really mixed up and so i just kind of wanted to spell it out here so a cloud service provider also known as a csp is a collection of cloud services with strong application integration and their synergy between those services they utilize metered billing and they're under a single unified api so examples would be aws gcp azure ibm cloud and there's a few others there and so this is my definition because you know you kind of have to piece it together and just know over the years what it means um but really the important thing to know is a cloud service provider is useful because of those synergies between services so they don't always have the best in class service sometimes they do sometimes they don't but the idea is that by utilizing a collection of them uh and the synergies between them it outweighs anything like a v a single cloud platform with a single cloud service that's trying to have the best one specific service metered billing means like you pay on the hour on the minute on the second single unified api means that it's just very easy to work with all the services pragmatically in the same way okay so on the other side we have cloud native and this is a workload application or system that is designed to run on cloud services and take advantage of cloud offering and when i say cloud services really we're just talking about compute right because that's all you really need is a place to run your virtual machines your containers your kubernetes cluster and then cloud native can integrate with some cloud service provider application or services very common like for databases or their load balancer but you know they're not necessarily taking advantage of the cloud service provider they're taking advantage of cloud offerings like cloud computing and that's a key thing to point out is that cloud native workloads cannot take advantage of all or the full advantages of a cloud service provider because a cloud service provider intentionally has proprietary technology to encourage you to use their managed services that have exclusive integrations with other services and so um you know that's just the nature of it i mean you know if you're a cloud service provider you'd obviously prefer them to use your services over the cloud native solution but both of these exist in the same ecosystem and all cloud service providers um you know offer support for cloud native and so you know hopefully that is clear so no fundamental uh cloud certification would be complete without a shared responsibility model and for the kcna they don't actually ask you any questions about a shared responsibility model which to me is a bit disappointing but for your own benefit i thought it was very important that we go find one and so the cloud native shared responsibility model is a little bit hard to define because uh you don't have like a large organization defining it and the cncf isn't defining it um and so what i had to do is find a community member's one and then look at and say is this good and i thought yeah it's pretty good and i want to show it to you so this is the shared responsibility model that i found it's made by lachlan white and i did ask them and say hey can i use this model to show off in the course and they're very happy to say yes go for it and so the idea is a shared responsibility model is the responsibility of the customer or the team or of the organization for the types of workloads and so the comparison here is we have traditional it hybrid it and then cloud native it and i know it's really small but if you look down the list here we have applications data runtime middleware os virtualization server storage networking pretty common for shared responsibility models but what's interesting here is who is responsible on the team and there's a big word here that you'll see that says get ops and get ops can be a little bit hard to define i have a very practical definition to get it up so we'll get into the course but i just want to emphasize that when you're working with cloud native there is a larger emphasis on git ops and so you are seeing things like network engineers security engineers solutions architect sres platform engineers data engineers and developers whereas in the traditional sphere used to have like your infrastructure team your your middleware team and then some specialized things so there's not really much to say here other than i just wanted to point out that there's git ops and that it is core to cloud native and so that is really all there is to share here okay hey this is andrew brown from exam pro and before we can talk about the cncf we need to talk about the lf which stands for the linux foundation so this is a nonprofit technology consortism founded in 2000 is a merger between the open source development labs and the free standards group to standardize linux support its growth and promote its commercial adoption and so the linux foundation is supported by a variety of technology companies so there's a lot of big names here and this isn't the full list this is just a handful of them like anabes should be there and all sorts of things but the idea behind the lynx foundation is it's just a nonprofit thing to make sure linux uh keeps running right and um that that there is a community behind it that is kind of steering its future growth and so that is the premise of the linux foundation all right so let's take a look at the cncf also known as the cloud native computing foundation and if you ever get the initialism wrong or can't remember the name that's okay i mess it up all the time but i want to tell you right now that this will be a question on the exam for the kcna if you know what the cncf stands for so take a look here very carefully and it is cloud native computing foundation okay uh very very very easy for me to forget i think when i took the exam even though i've said it a hundred times i forgot what it was so the cncf is a linux foundation project that was founded in 2015 to help advance container technology and project is a funny word because it's not always what you think it means uh but it's used a lot in um like technology stuff and a good example would be like oh wasp where it's called the like the open web security project and then it's an organization but they call it a project so the cncf operates as an independent nonprofit organization from its parent organization which is the linux foundation and it has all this structure and stuff underneath of it so it has board members that meet to decide on things like the budget and marketing um the cnf cncf has its own global tech conferences like cloud native con plus kubecon and you'll see this it's always the same conference we'll talk about that when we get to that uh later in the course the cncf has its own cloud native certifications which is the one you are studying for right now the kcna and the cncf has its own collection of projects so it's funny that it's called a project but it's really an organization but then within it it actually has technical projects and so projects here would be kubernetes which is the key project that we are focused on in this uh certification course prometheus at c or etcd contain container d and a lot more and we will go look at them all okay so that's the cloud native computing foundation all right so i just wanted to show you where these websites are so we have the linuxfoundation.org and then the cncfio and if you go here you can go to projects and you can see all sorts of different projects if you scroll on down here you can just drop on down to anything like networking and edge and you might see something that you know so there is a lot like a lot of projects um for the linux foundation but uh they also have their own training so you go here and that's where you get like linux foundation certifications for learning linux then if you go over here to the cncf the cloud native computing foundation uh you can learn more about it scroll on down here and look at the cnscf projects and so these we're going to be spending a lot of time here but not right now we'll get to it later okay hey this is andrew brown from exam pro and we are taking a look at the cloud native landscape so this is an interactive map developed by the cncf to showcase all the available cloud native technologies and to help identify the category to which they serve so we just got to go to landscape.cncfio and you can see the projects here and there's a bunch of rich filters but just to give you kind of a preview of all the categories i extracted them out for you we got app definition and development orchestration and management runtime provisioning there's special ones like uh certified service providers or training partners maybe one day i'll be a training partner who knows um observability and analysis and then there's these sub landscapes so we got members serverless and the cd foundation the landscape so not all of these are tech projects per se they could be like things that um are professional services but we will take a look at it and see you know what the landscape looks like okay all right so we're back on the cloud native computing foundation website and if you go to projects here um it should be projects there will be a thing that says cloud native landscape and that will open up this interactive landscape where we have all these uh fun little boxes and these are all different kinds of cloud native technologies so here you can see kubernetes it's right here i can click to it i can get a rich information about this project we can go to card mode here and so this is a little bit easier to consume go back to landscapes here and we can even filter it based on category these filters should be working i guess we have to choose based on the projects we'll say cncf projects so these are projects by the cncf right and so you kind of get the idea there it's nothing super complicated but there are um some uh landscapes within landscapes like this serverless landscape so you go over here and that's one this apparently is a landscape the members people that are participating or um providing sponsorship so you know we didn't show aws as a logo um in the linux foundation but you can see these are all the people that are putting money into this oh and is utilizing cloud native technology so there's a lot of people here all right so you know hopefully that gives you an idea there's also this guide up here if i just click guide and we'll give it a moment to load and so there is some kind of prescriptive stuff here that talks about like buzzword this is so bizarre they say buzzwords infrastructure is a code declarative configurations those aren't buzzwords so whoever wrote that i don't know but i mean i guess they kind of define things like container registry security compliance key management these are like um barebones stuff around these categories to kind of help you understand based concepts but i'm not really worried about going through that with you i don't think that's necessary but i just wanted to show you the interactive landscape hey this is andrew brown from exam pro and we are taking a look at the cloud native trail map so the trail map is a recommended path to adopting cloud native architecture and so it's pretty much a glorified pdf and so the idea is that they set out a path of saying okay the first thing you're going to do is you are going to take your application and you are going to containerize it and then once it's containerized then you can add ci cd to automate that and then now you have a containerized application um you'll want to be able to orchestrate it and deploy it to something like kubernetes and then you'll want observability analysis analysis and get data and then you'll want to uh put something rich on top like a service mesh uh because you know if you want to build up microservices that will be useful you'll care about uh security making sure things are secure then you'll need databases storage etc so the idea is they just have this big old list here and then they have like these abcs along the side so if you look here you can get my pen out here just says help along the way so training certification which is me by the way consulting help or joining the cncf as an end user community and i just want to point out that the path order will vary based on your use case they're just trying to give you something so that when you go to your team to your stakeholders to the executive level you say look we have a road map we have a plan but um you know the idea here is that you just do what you need to do because a lot of these later ones are just going to be in the order that you need them right it's not necessarily a strict guide okay all right so let's go see where we can find that trail map so i'm on the cncf website i go to projects then you go to cloud native trail map and that will bring up a github repository and you can scroll on down until you see it and there it is so you can pretty much you know click into it probably and expand it here i'm just waiting for it to load it's a big old png and it's way too big but you know the idea here is that you can read it and uh it will have suggestions around that stuff and so if you want to use more than what's just listed here that's where you go back to the landscape and you'd filter based on those categories so nothing super fancy here and by the way if you look in the bottom left corner okay this is where i got my cloud native definition when i was talking about the cncf earlier in the course it was from this document there you go all right so let's take a look at virtual machines also known as vms compared to containers because the idea behind kubernetes and cloud native stuff is that there are virtual machines involved but you're generally working with containers not virtual machines so the idea is that we'll have a virtual machine and the example says ec2 instance that is just amazon's name for virtual machines so just think virtual machine there and so a virtual machine will have a host operating system installed on it could be windows could be linux and then on top of that it's going to have a hypervisor and that hypervisor is what allows you to run a virtual machine on a real physical machine and so the virtual machine has its own guest operating system which is here and that is ubuntu as an example and that's where you can install libraries packages binaries then from there you'd have your workload like let's say you have a django app you have your database you have your queuing uh and so the thing with vms uh is that they're great because they allow you to run multiple workloads on a single machine so you are best utilizing uh that physical machine to the best of its ability but the problem with virtual machines is that they're still going to be wasted space because you have to um you know provision a virtual machine to be a particular size and so that's where you're always going to be over provisioned and it wouldn't be really nice if you know this part if we're only just paying for this much and anything that was not being utilized would not cost us anything or could be used like something to be slotted in there and so that's where um containers come into play so containers are very similar uh initially so you have they run on a virtual machine it has a host operating system like linux or windows and then it has what we call a container runtime now i have docker daemon because docker uh is a very popular uh container runtime um so it could be like container d now i suppose but the idea is you have a container run time and so each container would have its own uh guest operating system that would run on and then in that container would be it the workload so the django app with its own libraries and stuff and then each container is extremely isolated and so the advantage here about having these like isolate workloads is that you know you are best utilizing the space on the physical machine and the other thing is is that you get isolation so when you're looking at this virtual machine over here if the django app does something to consume more resources it could take away from mongodb or rabbitmq where these ones you can just say okay you're only allowed to use x amount of space and so conceptually um instead of thinking of this additional space as wasted it's available because if you were to use something like amazon ecs elastic container service that space would be able to be used by another customer right where when you launch a virtual machine you're always over provisioning so virtual machines do not make the best use of space apps are not isolated which could cause config conflicts security problems resource hogging containers allow you to run multiple apps which are virtually isolated from each other launch new containers configure os dependencies per container there you go oh and by the way just think container run time here so i just kind of patched that graphic there for you all right let's talk about micro service architecture and to understand that we need to compare it against monolithic architecture so monolithic architecture is one app which is responsible for everything and the functionality is tightly coupled so imagine you have a repository that contains everything uh the the database caching load balancing the marketing website the front end stuff the api the orm background jobs everything and generally monoliths are installed on virtual machines they don't utilize containers and the issue here is that when you have so much stuff running on a single machine what happens when uh your load bouncer runs into a problem it's not like you can just replace it uh and so this is where microservices come into play so the idea is you have multiple apps which are each responsible for one thing and the functionality is isolate and stateless and so here the idea is that we are using the cloud service provider load balancer we're using the cloud service provider caching the cloud service provider database the queuing for cloud native you might be spinning these up in your cluster so you might not be using cloud service provider services as these isolate apps but they would be an isolate on your cluster and so the idea is that you can easily remove or add any of these components and manage them and so that's the key part about microservices now the tradeoff here is that when you have microservices you end up with a lot more uh effort between communicating between all these different little apps whereas in a monolith they're all the same place and so that is one of the key advantages but that's just something we have to overcome when we are using microservice architecture so there you go hey this is andrew brown from exam pro and we are taking a look at kubernetes this is the reason why we're here for this technology so it is an open source container orchestration system for automating deployment scaling and management of containers originally developed by google and now maintained by the cloud native computing foundation the cncf as a cncf project and we are going to touch on google cloud i'm going to show you how to deploy uh your cluster there and you're going to find that google cloud is actually the easiest of out of all the managed providers because i use them all in this course kubernetes is commonly called k8s or k8s on the 8 represents the letters between the k and the s generally if you see k8s you can say k8s or kubernetes the advantage of kubernetes over docker because docker is what people always think when they think of containers is ability to run container apps distributed across multiple virtual machines or as we'll call them in the course nodes now i have a double asterisk there and that's just to remind me to tell you that uh this is a gross gross gross growth simplification docker is not as simple as uh like an apples to or it's like an apples to oranges comparison docker is a suite of things it's a run time it can have docker swarm which is an orchestration tool so technically a better comparison between kubernetes and docker swarm but i'm just trying to keep it simple here for you all right uh a unique component of kubernetes art pods and a pod is a group of one or more containers where shared storage network resources and other shared settings so the idea here i'm just getting my pen out here is we have a node which is a virtual machine and another one here and you got pods running across them now and pods contain containers in them that's where the containers are now as we look at pods throughout this course you're going to find out that basically most components that we run kubernetes are pods so that's why pods are so uh front and center because basically everything is a pod kubernetes is ideal for micro service architecture where a company has tens to hundreds of services they need to manage and so a service um would be something like a containerized application and the thing is is that when you have a containerized application you're very likely going to be running more than one of them all right even like redundant ones so here we have pods and a pod would run a specific application so let's say it's a ruby on rails application and i mean you could count this all as one service or you could count each of these as redundant services and so it's very quick uh or easy to get into the tens so kubernetes can be used uh quite early on but it's going to be really dependent on your use case hey this is andrew brown from exam pro and we are looking at the kubernetes component overview so what i'm going to do is give you a top simplification on a lot but not all of them but a lot of the kubernetes components because there's just too many it'd be too much to go through all of them but these are all the ones that we are going to be spending a lot of time with and you definitely will need to know them all and you will know them all because we deep dive on a lot of these and you'll get to see them again and again and again in the course so at the top of our list is the cluster and this is a logical grouping of all the components within a cluster so mostly everything that follows after this is generally inside of a cluster there are some exceptions uh the next is a namespace so this is a named logical grouping of kubernetes components within a cluster and it's used to isolate different workloads on the same cluster i like to think of it as a slice of pie so when you look at the icon and i made all these icons by the way because i do not like the ones that are provided right now in the community so i just made my own but i made it look like a slice coming out of that cluster so you can remember what that is then you have nodes so this is a virtual machine or it could be a serverless container because some providers like google and aws lets you run it on their service container platform but either way it's just the underlying compute the underlying server and there are two types of nodes we have control plane and worker nodes so worker nodes is where your application or workloads run and the control plane node manages worker nodes so it does a lot of stuff like that then you have pods and pods are the smallest unit in k8s it's an abstraction over containers it generally defines an application workload but basically lots of these components are just pods and we'll see that again and again when we list out pods because i list them all the time to show you under cube ctl namespace what is running they're always pods so the next is service so a service is something that we use with a pod to give it a static ip address or a dns name for a set of pods so the idea here is that if a pod dies because pods get dynamic ips but we want them to have static ips and so that's the purpose of a service but the service is also utilized as a load balancer and i'm going to point out that them calling this a service is really confusing because a lot of times a service when we're talking about containers means a workload or application that is continuously running right so if you've ever used aws uh ecs uh you have the option to run a task or a service which are just containers and based on how long they run but for whatever reason that's what they wanted to name it the cncf decided to name it or kubernetes project named it i don't think it's a great name but that's what it is ingress is also not a great name but that's a name they gave it but ingress is used to translate https rules to point to services but what we'll really see is that it's commonly used and this is the hardest component that we are going to learn throughout this course but it's used for getting a load balancer like an external load bouncer on a bus gcp azure to a road traffic to our pods okay then you have the api server the api server allows users to interact with kubernetes components using the cube ctl or by sending http or s requests probably put an s on there because probably everything uh is encrypted in translate transit i really doubt that they would have it so that it's just http and i like to think of the api server as the backbone of communication for kubernetes and you will see that uh in a future diagram where it looks a little bit like uh like um like the backbone okay uh then there's cubelet so cubelet is an agent installed on the nodes cubelet allows users to interact with a node via the api server and cube ctl and again that is a simplification it actually does more than that but we will dig deeper into that we have cubectl it's a command line interface that allows users to interact with the cluster and components via the api server so the ctl stands for i think controller and ctail is very very common to put after a name for some kind of tool that's used for controlling things via cli and we spend so much time tons and tons of time with the cube ctl so you will know cubectl inside and out by the end of this course you have cloud controller manager this allows you to link a csp like aws azure gcp to leverage cloud services i never had to provision one in throughout this course i never even noticed one i think like when you launch a managed service that's already there for you and it replaces i believe the controller manager but i'm just saying it's there but we don't really ever have to think about it so we have the controller manager and this is a control loop that watches the state of the cluster and will change the current state back to the desired state so it's basically state management but it's also you could think of it as the brain of kubernetes because it's doing all the controlling we have a scheduler so it determines where to place pods on nodes and places them in a scheduling queue so that's why we have this little crane here because it's picking them up and putting them where they need to be you have cube proxy so an application on worker nodes that provides routing and filtering rules for ingress or incoming traffic uh to pods you have a network policy these act as a virtual firewall it says as but at sorry at the namespace level or the pod level so it just um puts restrictions around how pods or namespaces stuff in namespaces can communicate with each other because by default everything just can talk to everything within a cluster you have config maps so this allows you to decouple environments a specific configuration from your container images so that your applications are easily portable it's used to store nonconfidential data in a key value pair so this is just application uh configuration uh details okay then you have i'm not sure what's not showing up there we go then you have a secret so this is a small amount of sensitive data such as a password a token or key it's basically config map with the option to encrypt it then you have volumes so volumes are basically well there's some variations here but they're basically mounting storage so locally on a node or remote to cloud storage then you have stateful sets we do not do stateful sets in this course because they are just too hard to do but we definitely talk about them in great detail so these provide guarantees about the ordering and uniqueness of the pods so think of databases where you have to determine reads and writes in order or limit the amount of containers staple sets are hard when you can host use a database externally from the kubernetes cluster so like if you have a relational database put it on rds right or put it on cloud google cloud spanner but say the sample sets just basically give you a guarantee that you're going to send traffic to a very particular pod you have replica sets so maintain a stable set of replica pods running at a given time it can provide a guarantee of availability so this is just saying like take a pod and run copies of it so that we have redundancy there and we generally do do not launch replica sets directly we do them through a deployment and this is also how paws are deployed so deployments deploy a replica set and the replica set deploys pods so a deployment is a blueprint of a pod so think like an ec2 launch template or something that just templates up what should be launched hey this is andrew brown from exam pro and we are talking about manifest files in kubernetes so when i was studying for the kcna and i sat it i was just seeing uh conflicting terms about what they call their configuration files because i couldn't understand if there were variants if they meant different things but then i kind of settled on the knowledge that they're actually all manifest files and that's why this slide is here for you or slides i should say so let's describe what a manifest is using nontechnical terms so a manifest file is a document that is commonly used for customs so like going over the border from canada us to list the contents of cargo or passengers it's an itemized list of things so what is that in the context of kubernetes well a manifest file is a generalized name for any kubernetes configuration file that defines the configuration of various kubernetes components and the name will change sometimes they'll call it a pod spec file sometimes they'll call it a kubernetes configuration file but they're all manifest files right if you're listing these this is yaml stuff in a uh uh with kubernetes components they're all manifest files and these are all masked files with specific purposes like i was just saying deployment file pod spec file network policy file and manifest files can be written either in yaml or json now i've never seen a json manifest file but taking my terraform knowledge i bet the reason why there's json is if you are pragmatically creating them the same thing with aws with cloud formation templates whether yaml and json um you probably would never write a json file by hand but it can take it and probably it's being generated by another app so here's an example of it in yaml and here's an example of it in json so a manifest file can contain multiple kubernetes components which makes sense because we said manifest is an itemized list of things so it can uh definitions configuration so here is an example of one and in yaml they have the syntax which is the three hyphens which allows you to define uh basically an array so like we have two different things here and once we have all of our um oh how what whichever amount of um kubernetes components we want to define in a manifest file we're going to be using cube ctl and writing apply and this is something that we use a lot in this course we're doing cube ctl apply hyphen f etc you will absolutely know this by the end of the course inside and out uh and resource configuration files is sometimes used to describe multiple resources in a manifest file which is confusing because manifest by definition means an itemized list multiple things but that's just the terminology throughout the documents throughout the ecosystem there i just wanted to clear that up for you okay hey this is andrew brown from exam pro and we are taking a look at control plane nodes versus worker nodes so control plane nodes were formerly known as masternode and the only reason i'm mentioning this is because a lot of documentation out there like tutorials and stuff are completely out of date and they're not using the new inclusive term so i'm sorry for mentioning uh this term here but i just have to because you need to know what it is when you are looking up information out there even the kubernetes.i o website still has some outdated stuff mostly in the graphics not so much uh in the text so they seem to fix that so you know a few years out and i think we'll all be moved over to the new terminology there and then you have worker nodes so control plane node manages processes like scheduling restarting nodes and then the worker node does the work as the name implies it runs your apps it runs your jobs and it's running pods and containers they're both running pods and containers but when we think of pods and containers we're thinking about more of the worker nodes because the other ones even though they are pods we're thinking them as distinct uh components okay and so let's take a look at the the components that are involved here so the first thing is the api server and notice how i remember i said it was the backbone of communication well look at it it looks like a big long line here and everything communicates along it so that's what i mean by backbone of communication and the way you are talking to stuff is generally going to be through the cube ctl okay or there's the api i guess they don't have a graphic for the api i guess it is the api server so that would be you sending um like https requests directly to it so i imagine that's like how um managed cloud service providers are talking to it but anyway so that's the api server then we'll have the scheduler so this determines where to start a pod on worker nodes we have the control manager so the detect states change it so like if the pod crashes it it tells it to restart you have xcd or etcd however you want to call this is a key value store that stores the save the cluster so controller manager highly relies on etsy and you have cubelet so this allows users to interact with the node via the cube ctl because cubelet no matter if it's a worker node or control plane node it's on both of them and so you know generally all of these are in the cube system i really should have wrote this in the course but they're in the cube system namespace and uh the components that can be in the control play node slightly vary so maybe instead of the control controller manager you'd have the cloud controller manager if you're using something like k3s they probably have slightly different control plane um components inside of them but you generally need to know for the exam what is in a control plane so know know what these all are and know that they are in the control plane and not in the worker nodes all right so now onto the worker nodes the worker nodes will have cubelet so all all nodes will have a cubelet for communication and that's the way it will talk to container runtime we'll talk about the second and then you'll have a proxy and so we'll have container runtimes pods and containers so just kind of an illustration there so why is there a proxy and why is there a cubelet well the proxy actually you know what i don't think i have it here but what's missing over here is core dns okay so generally we'd also have i can't believe i have it missing but you'd have core dns i have it in another slide so it's not a big deal so we should really have that here as well but i suppose it doesn't talk directly to the api server but proxy is what is used when you have incoming traffic right so i'm a user i use your website and how am i reaching that pod well it's going to be through the dns service coordinates or cube dns and then that's going to go through the proxy the proxy is going to go through ip tables which we'll talk about in this course and it will reach the application around the container if we're trying to interact with our container programmatically through cube ctl it's going to go through cubelet and that's going to go through the container runtime into the container so again if it doesn't click right now it's okay we're going to cover this in different variations here and you will know it but there you go hey this is andrew brown from exam pro and we are taking a look at probably the most important component pods so pods are the smallest unit in kubernetes and pods abstract away the container layer so you can directly interact with kubernetes layer in my mind the smallest unit really is containers but i guess they're talking about like newly defined components that are part of kubernetes okay and so here is that graphic there uh and so a pod is intended to run one application in multiple containers so notice down below that we have one two three four containers they're all running the same app like the same app over and over again now uh so that the idea here is a pod would be a database pod a job pod a front app pod a backend pod pod um and what we could say is is if we use the term service like the way we're supposed to a service would define a continuously running type of application so you could say database service job service frontend service we talked about microservices that's how we describe it but that's the idea there you can run multiple apps in a pod but those containers will be tightly dependent i don't even know how to do that but the idea is if you wanted these all to be slightly different you could but i i again i have no idea how to do that i don't know why you would do that so each pod gets its own private ip address so look up here there's an ip address and containers will run on different ports so nodes here 3000 3001 8080 81 containers can talk to each other via local host so if you have a container i will just clear on out this uh eraser stuff if container one wants to talk to container two all it has to do is talk to it on its port number on its own localhost and that is how simple communication is from container container communication which we'll talk about because we have a whole section on different types of cluster communication each pod can have a shared storage volume attached all containers will share the same volume so the idea here is you see this line here drawn to the pod to a persistent volume apv and we talk about persistent volumes and persistent volume claims we'll get to that later when the last remaining container dies or maybe crashes in a pod so does the pod but when replacement pods is created the pod will have a new ip address that will be assigned so ip addresses are a i never can say that right efferent or temporary for pods so they don't by default persist and that's why we're going to need kubernetes services which we will talk about in this course so to get pods and show their ip addresses it's very common you'll type in get pod get pods and then doing this hyphen o wide okay hey this is andrew brown from exam pro and we're taking a look at the api server now we saw a graphic of it uh when we were looking at the control plane versus worker nodes so we don't need another graphic here and i think a lot of this text i grabbed direct from kubernetes.io because i'm just trying to get you through the key points of it um so this part's a little bit boring but let's get to it so the core of kubernetes control plane is the api server the api server exposes http api that lets the end users different parts of your cluster external components communicate with one another cube api lets you query and manipulate the state of api objects and kubernetes like pause namespaces config maps events the api server is a component of kubernetes control plane that exposes the kubernetes api the api server is the front end of the kubernetes control plane the main implementation of kubernetes api is cube api server cube api server is designed to scale horizontally that is it scales by deploying more instances you can run several instances in cube api server and balance traffic between those instances everything has to go through the api server you can interact in the uh with the api server in three ways so the ui api and a cli like cube ctl now i put an asterisk on ui because i don't know what they're talking about there the only thing you have is the kubernetes dashboard which is something that is optional to launch and um i mean you can't really launch resources in the kubernetes dashboard so i don't know what they're trying to say there but technically if you are interacting with kubernetes uh via the api server it's going to go to the api so i guess in that sense it is in terms of like how it scales and stuff i don't really know i don't know how to observe that directly in there but apparently it just does it and hopefully that gives you an idea a little bit more information about the api server hey this is andrew brown from exam pro and we are looking at deployments in kubernetes so a deployment provides declarative updates for pods and replica sets so deployment controller changes the actual state to the desired state of at a controlled rate and the default deployment controller can be swapped out for other deployment tools like argo cd flux and jenkins so what i'm saying there is that uh deployment is built in to kubernetes so you don't have to install a thirdparty tool but generally you want to because the the tool there is very limited it doesn't do git ops and so that's why you'll want things like argo flux and jenkin x which we will talk about more when we get to our deployment section of this course notice at the top here i say pods and replica sets but in reality a deployment is always deploying a replica set and so even if you're just deploying one pod it's going to be a replica set of one so here is the actual manifest file of doing a deployment you're going to be seeing a lot of these manifest files through the course it's going to be kind and then the type right so deployment and then generally i think usually yeah then no i guess spec is just for deployments but uh and pods and things like that and so here spec is our pod spec file which we'll talk about later here it says replicas so it's going to create a replica set with three pods in it and then down below here you can see uh the container that we are going to be deploying for these three pods so we'll end up with three containers so here is a graphical example or architectural diagram of the deployment controller and so this one in particular is using flux okay and so the idea here is you have deployment it goes to the deployment controller the default one or whichever one you install deploys your replica set and that's how you get your pods so a deployment defines the desired state of replica sets and pods a deployment will create and manage a replica set a replica set will manage replicas of pods so there you go hey this is andrew brown from exam pro and we are taking a look at replica sets so replica set is a way to maintain a desired amount of redundant pods replicas to provide a guarantee of availability so here's a replica set and inside of it we have multiple pods or you could say replica pods one could be the main pod just who knows right because when you distribute traffic to them like with a service it's just random where it goes so they're basically all replica pods here is the manifest file notice that it's called a replica set and it looks just like a deployment you specify the replicas you're specifying the containers okay the pod field metadata owner references determines the link from a pod to a replica set uh so that we would have to observe that on the pod itself so if we went using the cube ctl and we said cube ctl describe pod in the pod name we would see that metadata there and that's how we would know that it's linked to that replica set it is not recommended to directly create replica sets so you can but instead a deployment can create and manage replicate sets for you the idea here is that if you launch a replica set and then you delete it it's gone now if you delete a replica set i believe that a deployment will say hey your replica set's gone it will spin up another one okay same thing like if you have a replica set and you have three pods in it you delete your pods then your pods might reappear okay so horizontal pod autoscaler can be used to auto scale replica sets and we'll talk about that when we get to hpa okay hey this is andrew brown from exam pro we're comparing stateless versus stathal very important concept we need to know in kubernetes but also just when building applications when we're running them in more than a single virtual machine or compute because we might want to have a version of our application in multiple um data centers across multiple regions and so how do you handle that when a user goes to the website but then it goes to a another server how does it knows the same same person or does that matter so we're talking about stateless the idea here is that every request so like a web request to the server does not care it forgets about the previous or current state that's what we mean by stateless so think of a goldfish that you know has poor memory then uh the idea with staples every request relies on state data to remember uh so very uh a good example would be a database a database you have to know who it is every time uh like if you were to have 10 requests in a row accessing data uh it has to go to the same um database the same place so that we know who it is so think of an elephant okay so for web applications it's not that they don't have a state it's just that the state is stored outside the context of the web app running on the virtual machine and so that state will be pushed to something like a database or cookies which are are stateful but the web apps themselves are stateless so those http requests come to the servers uh the server does not uh know if it's from the same person this time around or other it just does not matter now that's not to say that web apps can't be stateful they are if they are on a single machine and so the problem with a lot of people with monoliths is that when they build them they do store the state on a single machine they might store it in memory and that's where we run into problems because then it makes it really hard to scale out to multiple machines so that's a pattern that we generally want to break so that we can move into a microservice architecture or at least an architecture where we can run our monolith on more than one machine so when we're talking about stateful imagine you have a database so you have a primary database and then you have a second version of your database but it's only for read replicas the state here has to remember who can do writes and reads and who can do reads right because in the primary database you can always do writes but the rewrite only do reads and so it has to remember that state data so hopefully that makes sense but anyway the weight ties to kubernetes is stateless is where we will use replica sets as that's where you would launch your applications with and stateful sets is where you would use things for like a database if you wanted to run it in cluster okay hey this is andrew brown from exam pro and we are looking at stateful sets now this is one of the few things in this course that i'm not doing a follow along with i'm not showing you it handson because it's so hard to do and it's out of the scope of the kcna and generally it's recommended not to run your databases uh in cluster but using managed service so for those reasons i'll just provide the information here so you generally have uh the knowhow now that doesn't mean that you will never have to do it there might be some cases but again it's just out of the scope of this course so stateful sets are used when you need traffic to be sent to specific pods and the thing with staple sets is they will always have a unique and predictable name and address ordinal index numbers assigned to each pod like zero one two three four five a persistent volume attached with a persistent link from pod to storage if a pod is rescheduled the original persistent volume will be mounted to insert data integrity consistency because that's how you ensure state stateful set pods will always start in the same order and terminate in reverse order so that's really important now in order to implement stateful sets it will require to use a headless service to manage identities headless services were really really really confusing i can't tell you how much time i spent to try to make sense of them but what i figured out is that headless services are generally used because of stateful sets so there is a headless server service used to maintain the network identity of the pods and another service that provides read access to the pods and i have an architectural diagram we'll walk through it so here's an example of a stateful set i'm going to get my pen out here you can see there's a lot going on here but we have stateful set at the top okay and then there it kind of looks like a deploy so you have a match selectors to match to something the replicas like how many do you want to run um and where it kind of varies is that it has a mounted volume so it's not like you can't mount a volume to a um you know a deployment replica set what have you but staple sets would have a volume because they're usually for databases so you definitely would want uh to have a volume attached there so that here is my big fancy diagram and so i have some accompanying text here that is from the documentation and so what i'll do is read it up to you and then try to figure out how it matches up to the diagram so the first thing is when we're looking about staple sets we have a dns host name for rights so rights will be i'm sorry i got to set up this scenario here if you look here what we have is a stateful set and it's a postgres database being deployed in cluster okay so the first thing is for our staple set of a pro square database is that rights will be directed to the main pod by its dns host name which is identified by the headless service so uh you know you have in your web application someone wants to write to the database so the idea is that in order for us to know where this main pod is we need this headless service so that we get a dns record and so here's it says main.psq because we can't rely on ip address we need to rely on that domain name or that dns record so that's what it will do to help it get there then the idea is we have cluster ip for read so for read traffic it can be redistributed or distributed to all the reading pods using the cluster ip service now this part of the course we haven't covered all the different service types so when you if you find this confusing what you should do is watch the whole services section when you get to it and then come back to this and look at this because it is pretty darn complicated to be honest um but anyway so the idea here is that when we want to do a read we use a um a regular service like we would cluster ip that's the default one and that by default uh does load balancing at random so here it doesn't matter because both the pods uh can read and so you'll either go to the main pod or the read replica then you have the headless server so the headless service is a service with cluster ip set to none and it does not provide load balancing it doesn't it does not provide a static ip address a headless service is used to identify specific pods by signing them dns records that's all it does it gives a nice dns record so that we can identify uh what pod to send to okay i have those services required in order to uh for a stable set to work and i've said that a few times here then we have our pvc and our pv so persistent volume claim and persistent volume again we haven't reached that section yet of the course but we will but the idea is that we do need to have some kind of storage because it is a database and we said earlier in the previous slide that you'd want to retain that information so if this main pod was destroyed and then a one was to take its place this same volume here would get matched up it wouldn't match up to some other random volume it would ensure that this one matches with the main pod so there you go hey this is andrew brown from exam pro and we are looking at kubernetes namespaces so a namespace is a way to logically isolate resources within a kubernetes cluster and so namespaces can be organized based on project department or any user defining group and so if we're using cube ctl it's cube ctl get namespace or ns for short but you generally get the idea with these get commands it's always get whatever you want kubernetes starts with four initial namespaces there's the default one so this is just where your stuff is going to end up if you create anything without specifying a namespace you have cube public these are for resources that are publicly visible and readable you have cube system this is a namespace that stores objects stores objects created by the kubernetes systems or we say objects it's pretty much pods okay and so engineers deploying applications are not supposed to touch this namespace we say not supposed to let's say we're using adabus eks to deploy our managed kubernetes cluster over there they might tell you to if you want to have an ingress controller and use the load balancer with aws you would then for that one case create a controller ingress or ingress controller for aws in that namespace so it's not that it's it never happens but it rarely should it be done and the cube system namespace is one we'll see throughout this course because every opportunity when we use a new managed kubernetes provider or a different lightweight distribution i will always look at all the stuff and show you what's q in cube system so you have an idea what's running on the cluster then you have cube node lease so this holds lease objects associated with each node it's used to detect node failures by sending heartbeats and so you can create your own name sources by doing cubectl create namespace and the namespace here would be we're calling it production so in clusters with a small amount of resources namespaces aren't necessary but it's up to you if you want to use them you can use them but the great thing about namespaces is that you can apply um like network policies and other kind of uh permissions so if you have rolebased access controls you can say only give access to stuff in this namespace so um for me i would probably always just create a namespace but they say you don't have to might complicate things so names of resources you need to be unique with a namespace but not across namespaces they're talking about objects themselves so like if you have a pod it um and you call it my app it can be my app in a bunch of them right namespace based scoping is applicable only for namespace objects so deployment services things like that uh but not for clusterwide objects storage class nodes persistent volumes so what we're talking about is there's um certain objects we're saying objects we're talking about components i don't know sometimes the docs say components sometimes the doxate objects but the idea is that certain kubernetes components can be namespace and some of them can't be so that's something that we need to know so there are objects or components kubernetes components that can only reside in a single name space there's ones that are multiname space so there's ones that are clustered wide so they can never live in a namespace so example of single namespace objects are config maps and secrets they cannot be shared across namespaces they exist in one okay so config maps and secrets then you have uh services and pods which can belong to multiple namespaces um and then for cluster wide we have volumes and nodes they do not exist in namespaces they are just in the cluster now i don't know the exhaustive list of all the objects are there's probably some way to look it up i never did but these are the ones that you need to remember and just realize that there are three different kinds of buckets okay so you can apply a system quota restrictions on namespaces to avoid overusage like memory compute so that's an example of leveraging namespaces to put limitations uh like security restrictions things like that if you don't provide a namespace for a component it will end up in the default namespace as we already said but there you go hey this is andrew brown from exam pro and we're going to talk about entry versus outreach so these were terms that i kept on seeing when i was looking up different um cncf projects and uh the term varied so there was no consistency with the way this term was used and so i'm kind of imposing my own terminology to help us understand what it is uh and i can tell you right away like i was talking to some kubernetes folks and they were dead set that it meant one particular thing but i just couldn't reconcile it because i was just seeing it used in different ways so just understand that this is my own personal opinion it's a way to help you just kind of contextualize it even if it's not perfectly accurate so in cloud native projects you'll hear the term entry in out tree so here is a little uh forest here to i don't know have a visual but entry the way i describe it is plugins components or functionality that are provided by default or or doesn't mean they have to but or they reside in the main repository so think of entry as internal plugins things that come by default with the project then you have out of tree so plugins components or functionality that must be installed manually and extends or replaces the default behavior so think of out of tree as external plugins and again this is my own definition that some people are not going to agree with me on um but this is the best way i understand it um but yeah we'll maybe we'll take a look at something where you know i see this in trio tree stuff okay hey this is andrew brown from exam pro and we're just trying to find the example of entry versus out tree and so i remember seeing this under coron dns it's for a variety of projects but if you go to core dns which we cover in this course it's a gns server that comes by default with kubernetes which replaced uh cube dns i believe and so if you were click to if you were to click through to the website you almost the top it says plugins and then external plugins okay and if we go to github and that's not always the case but sometimes when you are in a github repository and this is where i consider the repository to be a tree because um it's a nest of folders or you could say it's a directory tree a folder tree so within our folder tree here we have plugin and here is a bunch of plugins that would be considered in tree for this project so if we go back to core dns things that are out of tree are external plugins which to me those are just plugins but um you know these are all plugins that are not part of the repository again entry doesn't mean it has to be in the repository it just means that it ships with it by default and these do not ship by default you have to do extra things to get them in uh so hopefully you know that helps you with that definition or where i'm coming with that again we could find tons of examples of this but this is the one i'm just showing you okay hey this is andrew brown from exam pro and we are taking a look at endpoints and endpoint slices so endpoints track the ip addresses of the pods assigned to a kubernetes service if that's confusing just give us a moment we got a graphic okay so when a service selector matches a pod label the pod ip address is added to the pool of endpoints so pods expose themselves to services via endpoints okay so here's the graphic that i'm talking about so the idea here is that we have a service right and we learned that services are used to load balance to multiple pods but the question is how does a service link to a pod and that answer is endpoints all right so remember each pod has a dynamic ip address so the idea is that you are storing the dynamic ip address in endpoints okay and they exist in like this endpoint pool all right so it's the connection between services and pods and rarely rarely do you ever have to manipulate these or do anything with the end points okay but if you want you can do cube ctl and get those endpoints now there's this concept of endpoint slices so endpoint slices break up endpoints into smaller manageable segments and each endpoint slice has a limit of 100 pods so why like why do we care about endpoints and endpoint slices well according to the documentation it it's when you're scaling out uh there is a cap on endpoints like the amount of endpoints you can have in a service and so that's where you run into scaling issues and then that's why you need endpoint slices and also the term endpoint just comes up in the documentation all the time and endpoints is a general term meaning uh like when we talk about apis an endpoint would be like you point your request your http request to an address or an ip address and that could be called an endpoint i just kind of wanted to clear that up so that you knew that endpoint was a very particular thing in kubernetes and we're not just saying the general word endpoint okay hey this is andrew brown from exam pro and we are looking at jobs and cron jobs and before we can describe that we need to describe what is a background job so a background job is a oneoff task that is used to run a piece of code or a function and they're commonly used to perform maintenance or to trigger a computational workload so an example could be to back up the database every x minute so like every 30 minutes delete all users who not have not confirmed their email basically you could trigger any kind of code but the idea is that uh you know they're in jobs right so a job creates one or more pods that will continue to retry execution of the pods until a specified number of them successfully terminate so an example here would do cube ctl create job we're saying the job's named hello and we are using busybox which is something we will talk about in this course to just do an echo hello a cron job is just like a job but it executes based on a repeating schedule and so a cron job uses um this uh kron tab expression or cron expression so this i'm not sure what it says maybe every minute this will run every hour i'm terrible chronic branches quran expressions but a very very common um use case for jobs is that you want it for long running tasks so it doesn't hang up your main application and a very common use case is sending off emails so you wouldn't hang the main thread of your main app you would run it in an isolate job there okay hey this is andrew brown from exam pro and i just want to talk about the kubernetes dashboards the kubernetes dashboard is an open source application you can deploy to your cluster to provide a ui to view kubernetes components and so it looks like this so when i was first using kubernetes i was doing it on aws and i launched up um a cluster on amazon eks or aws cks whatever they want to call it and uh there i couldn't see anything i was just like where's all the stuff where's the ui um and so i was kind of disappointed i thought maybe it was offering wasn't very good or maybe kubernetes just doesn't have an interface but then i realized um that they just have this ui thing and you can launch it on any provider that you want and it's very easy it's like a oneliner but even with um lightweight distributions like um minicube or micro k8 s you can literally just write like enable dashboard you'll get one and so you can click around and you can see um all the kubernetes components that are running in your cluster and i just noticed that there's like this create button here at the top because i was when i'm going through the follow alongs i can't seem to find a way to create resources so that it's possible that you can create stuff there and also when we were talking about earlier um about api server it says interacting with the ui and i and i was saying like renee's bat dashboard doesn't let you create stuff so maybe i just haven't noticed this button here so maybe it is uh able to create resources there but i want you to to know about it and it's part of the kubernetes project so it's not like it's a third party it's totally safe to install and use but there you go hey this is andrew brown from exam pro and we are taking a look at selectors so selectors are a way of selecting one or more kubernetes objects remember objects could be pods objects is also could be generalized as meaning kubernetes components so in kubernetes there are three types of selectors so we have label selectors so selects kubernetes objects based on the applied label and so this is the example or the type of selector we're going to heavily focus on in this course but the idea here is that you have your selector and we're saying match on all pods that have the key environment with the with the value production and so then it selects only those pods you have field selectors so select kubernetes object space on the object data like metadata or status we have node selectors so select nodes for very specific plot placement let's take a look at what we do when we apply labels so labor select label selectors to find labels as key value pairs probably have a slash there under metadata in the manifest file so notice here it says metadata and then there is our label this is the key this is the value as you can see we can have multiple multiple labels okay so on the worker node here we have for example this one says env prod and then it says app engine x web just to kind of match up what that means there so you can use the hyphen show labels to see all labels so here's an example for pods and this will work for a variety of different kubernetes objects you can apply labels with the label command so you can do cube ctl label pods apache web owner equals devops so you can do it here in the manifest file or you can do it here on the fly i always do it in the manifest file i never do it on the fly um but you know i just want to make sure you are aware of all those options okay let's take a look at recommended labels so these are recommended labels that should be applied to every single resource or resource object and so i say should in parentheses because in practicality most people aren't doing this so let's take a look at what is recommended so we have name instance version component part of managed by created by their selfexplanatory um and so just note that the prefix of the the key here has this app.kubernetes.io and then name so this is the key right so remember there's a key and a value for labels and so labels can have forward slashes and they can have dots in them so just be aware that they can get kind of funky there but if you had a label without a prefix like the app.um kubernetes.io then these are considered private to users okay so just understand that one caveat so here is what it would look like if you applied all of those labels and again i've never seen that happen i mean i'm sure it happens that people are like packaging things for like helm and stuff like that because people are going to be ingesting them and people are usually are a bit better about that but when you're building your own stuff you might not be doing that at all okay but one i definitely do said is the name so that's something that i will do quite often so we looked at applying labels but how do we actually select labels so kubernetes objects like services or replica sets can target pods based on label selectors you have to use them otherwise it doesn't know uh like a service will not know what pod to associate with same with replica sets and so here we have an example where we're doing selector and then it's the key and the value on a service and then this one is for a replica set notice that the selector syntax varies for different templates or we could say different kinds of kubernetes objects so this one says match labels and this one doesn't have it just has a selector like that um you can use the selector in the cube ctl as well so you just do hyphen hyphen selector or use its alias hyphen l so here is an example where you are selecting all the pods or things like that so it's not necessarily that it has to be for a service or replica set you can even do it uh that way if you're just trying to grab resources that have labels and and see them okay let's talk about annotations which seem very similar to labels but they're not they're something else so kubernetes annotations allow you to attach arbitrary nonidentifying metadata to objects so clients like tools or libraries can and may require this annotation in order to work a good example here is ingress so this often uses annotation to communicate ingress controllers so here we have um an ingress kubernetes ingress and so it needs to communicate to an ingress controller and this one is specifically for nginx and so it always expects this rewrite target thing in here when you look at different ingress controllers like aws they have some different ones and it just varies so that's the only use case i've seen for annotations i'm sure they show up somewhere else but that's all i've seen okay hey this is andrew brown from exam pro and we are taking a look at a pod spec file so pod spec is a configuration file that describes a pod so that seems pretty clear remember all files are manifest files especially if they look like yaml for kubernetes but here is an example of us defining a pod but notice that it says spec down here below so the spec section is what we care about and that's what makes it a pod spec file and i want you to know that this spec thing can show up in a service or sorry a component that is a kind of pod or deployment or a replica set uh probably also a stateful set so just realize that's what i mean when i say pod spec file we're talking about this section here so the idea is you can define multiple containers and you'll have the name of the container the image here which is nginx the command to run on startup could be an option it's not shown here the port the container will operate on the restart policy which again we don't have it displayed here and a variety of other things so you can directly deploy a pod with cube ctl apply command as we see here so it's showing a pod but in practice you won't directly deploy pause instead you'll use deployments or job as the kind so in some um uh follow alongs you will see people deploy pods just because they just don't want to deal with deployment because they want to be able to delete the pod and the pod not to spin back up again um but yeah you rarely ever would never would ever make sense to deploy pod without deployment okay hey this is andrew brown from exam pro we are looking at grpc uh before we understand what grpc is we need to know what our pc is so that stands for remote procedure call and this is a framework of communication in distributed systems it allows one program on a machine to communicate with a program on a remote machine without knowing that it's remote and the concept of rpc has been around since the 1970s so grpc is much newer it's a modern open source high performance rpc framework that can run in any environment and grpc was initially created by google so you can kind of think of grpc as an alternate method of communication instead of rest or graphql and it can connect services in and across data centers with plugable support for load balancing tracing health checking authentication uh and probably more so let's try to take a look at how this actually works so the idea is you'll be installing a grpc library for your program you'll be defining a protobuf file that describes the data structure and those files usually have a dot proto extension you'll be writing code that works with grpc so distributed systems like kubernetes uses grpc for pod communication and so that's kind of the example and honestly i was a bit lazy here with grpc because i probably could have showed you code examples and stuff like that but with kubernetes you're never really touching grpc it's just there um and my experience with grpc is i used it when i was using something called nats nats not to be confused with nat network address translation that's is a an event bus that requires grpc these protobuf files but you know even though i don't have it here the idea is that you would have a file it's it has its own syntax but it's a file that defines the structure of communication and then you implement in your chosen language like ruby or android how to um uh like you load that structure into a like using your standard library so you can read it so it basically you define the payload in a structure and then you have files that read it um but anyway if you ever use an event bus you come across these things um and there's something kind of similar to it it might be g you might be using protobuffer but uh it is like an aws um the abs event bus with um to remember what's called eventbridge so that's something that's similar there but you know for this i'm not going to get into those examples and show you because you're never going to touch it at least i've never seen anyone have to touch unless you're developing something for a project but there you go hey this is andrew brown from exam pro and we are taking a look at cubelet so cubelet is responsible for pod internal api communication via the api server and cubelet is a node agent that runs on every single node that includes both control plane and worker nodes and i know this because i took extra care to know because i wasn't sure for quite a while if it actually ran on the control plane but it does so what is an agent well an agent is a program installed on a server to obscure what occurs for specific programs and to communicate information externally or trigger actions to be performed sounds kind of uh very interesting um so here's an example of a cube where is it cubelet so here's our cubelet and notice that it's on inside of our worker node so cuba performs the following tasks it watches for pod changes uh configures container runtimes to pull images create namespaces and run containers and cubelet uses podspec files to determine what images to pull and containers to run so notice we have this pod spec file there and then it knows to deploy that container into that container runtime but we are going to go even a bit deeper so we can see this because i have another diagram that i created here uh to try to make it even more clear and interesting and now you'll see why we covered grpc so here's grpc in a few different places to show that that's where that communication is happening so for pods cubelet will continuously monitor pods for any kind of changes okay that's how we know something's happened in the pods is because of cubelet cubelet will send back https requests to the api server containers sorry container logs and execution requests so the logging information from your pods uh cubelet's the one that sends that stuff along um for storage cubelet can interact with storage through the container storage interface csi using grpc uh very few diagrams were actually show off um cloud storage interface cloud runtime interface and cloud networking interface so this is the this is the very important diagram showing you those integrations between all that stuff and notice it's communicating using grpc cubelet can interact with the container runtime interface also using grpc and so remember we said that a pod spec file is deployed to um a container and has to go through the container runtime and the reason i show uh the container networking interface over here is just to show you that it's not cubelet this is just another route into the container runtime okay so there we go hey this is andrew brown from exam pro and we are taking a look at cubectl probably the most important thing when we're learning kubernetes because we use it so much so cubectl is a command line interface tool that lets you control kubernetes clusters so the idea is cubectl is a program where you run you write in commands into your bash bash terminal and it will send http request to the api server onto the kubernetes components or objects if you prefer to call them that and cubectl looks for a file named config in the home directory so notice it says dollar sign home so that's usually the same thing as writing a tilde in linux if you write a like cd tilde it usually goes to home okay and that will be in the dot cube directory and then even though it doesn't show here be like forward slash config and we cover config in a very uh granular detail in this course uh but that's during follow alongs i don't know if i actually make a slide on it so cube ctl has the following syntax the idea is we have cube ctl then we have command type name and flags so let's cover all of these parts okay so the first is command so command is the operation you want to perform and so here you can see example where we're doing copy so the available commands we have is annotation so key value data that can be applied to resources apply so executes manifest files to create and modify kubernetes resources this one we use a lot we have auth so inspect if you are authorized to perform an action and we do show you this one in the course when we're doing rolebased access controls to say am i allowed to perform these actions under a particular user when we're setting up their permissions auto scale so it creates an autoscaler that automatically chooses the set number of pods that run in a cluster we definitely covered that in this course cp copy files directories to and from containers actually we didn't do that in this course in the in the labs but it's not too hard to figure out how to do that create so create specific kubernetes cluster level resources most times we're using apply because it creates and modify but in some cases you just never modify particular objects so i guess in some cases we use create but a lot of times you can you just use the word apply instead delete so delete resource files uh file names stastdins resources and names by resources and label selects we use delete all the time describe show details of specific resources we use that one a lot as well diff the online configuration with local configs i don't think i've never ran that once but uh yeah so there you go for commands oh wait there's more we got edit so edit resource from the default editor execute so execute command with a container we definitely use that one a lot expose expose a resource as a kubernetes service that one we use a lot too get so generally used to get the status of an existing kubernetes resource similar to describe but not as detailed cause to my so print a set of api resources generated from instructions to customization yaml we don't do that in this course but we we do cover what customizes label so update labels on a resource logs so print the logs of a container in a pot or a specific resource patch so update fields of a resource using strategic merge patch a json merge patch or json patch and i mean you can do this i just never patch i would just update a file and then do an apply port forward we definitely use that quite a few times in this course so forward one or more local ports to a pod proxy creates a proxy server between local localhost and the kubernetes api server i'm just going to wipe a bit of this away here run so create and run a container image in a pod scale so setting new size of a deployment replica set replication controller staple set we cover scale when we do auto scale then we have our type so type is the resource type you want to command so this could be deployments resource types can have abbreviations so you might type in deployments or you might just type and deploy same thing with persistent volumes might be pv pods might be p o uh and i think it will also take pod as well so a lot of times i'll just forget to write it with the s on pod and it still seems to work so here is the same example this and this is identical and i'm pointing this out because when i was doing the exam for the kcna i would see code and you do see cube ctl commands we have to pick the right one and i got mixed up because i thought maybe like deploy wasn't an actual command because i just kept seeing deployment when i watched tutorials and i learned stuff so just realize that if you see a word and you know that there's an object like a persistent volume you see pv uh you know you can pretty much guess that it is it exists i don't think in the exam they would just make up a term like try to trick you up by taking a name and like dropping the s and having to guess okay there are over 50 plus resource types so here you can see them all they don't all have a abbreviations but you pretty much learn the ones you need to learn so like ing is a common one sc storage class pv pvc i never write pod to just write the full thing there um so yeah there's a lot of them but you just learn the ones you need to learn okay then you have the name so the name specifies the name of the resource so here we have pod and notice we can actually provide not just one but two so sometimes they take multiples these are case sensitive so just be aware of that if the name is omitted details for all the resources are displayed and very common it's very common like if i only have a single resource so if i create a single pod i know a single pod a lot of times i'll just write cubesat till get pods or describe pods because for describe because i know it's only going to show the one and i do that a lot in this course but often when you have a lot more you'll be specifying the name flags specify optional flags um a flag is a concept for commit like command line interfaces you see them all the time it's this double hyphen followed by something here now you don't have to provide an equals between um the flag and its value um but sometimes i do sometimes i don't when i do and don't it's just random but you'll see me do that throughout the course some command line interfaces are sensitive cube ctl is not you can either have equals or not flags generally start with two hyphens and i know you can't see but there are two hyphens here it's just the font uh group them together sometimes flags have abbreviations with a single hyphen so double double or sorry a hyphen hyphen server is the same as hyphen s and so you know some people just don't want to type as much so they'll use the abbreviated ones available flags will vary based on commands uh sometimes flags can be assigned values uh or do not expect a value at all okay so um yeah there you go oh just one more thing for cube ctl there is really good documentation it isn't uh straightforward how to find it so i just had to point it out that you go to kubernetes i o four slash docs four slash reference four slash generator four slash coupe ctl cube ctl commands uh if you google cube ctl uh kubernetes it's usually the second link and so what i do when i'm looking at google is i just like carefully look for this one underneath because if you get here it has so many examples it's really really useful okay all right just because it's so tricky to find the cube ctl documentation i just wanted to show you so i typed in cube ctl commands and if you were to click the first one you would end up at the cheat sheets and the cheat sheets are kind of okay but what's a lot better are the commands and now if i go this one i don't know if it goes oh it does right there but usually i'll click on this one here to get to it okay but for whatever reason the cheat sheets are above the commands and the commands show you everything so if i want to apply something we could click apply might have some sub ones there you can go and see a bunch of different examples here so that will help you out quite a bit when figuring out how to use cube ctl all right let's take a look here at mini cube so it sets up a local single node kubernetes cluster on mac os linux windows for learning purposes and in particular minicube is actually setting up a virtual machine and then running a control plane and worker processes uh within docker as the container layer so um that's one key thing is that it runs in a virtual machine where we'll see other ones they uh operate a little bit differently but minicube is very popular because it's just super super easy to use it's not my favorite i like micro k8s but let's talk about some of the benefits so supports the latest kubernetes release cross platform deploys as a vm a container or on bare metal multiple container run times a docker api endpoint for blazing fast image pushes advanced features such as load balancer file system mounts feature gates addons for easily installed kubernetes applications supports common ci environments so continuous integration and so that's kind of example of starting up a mini cube and you can see it's kind of pretty so it gives you an idea how easy it is to use all right let's talk about another lightweight distribution uh k3s and k3d so k3s is a lightweight tool designed to run production level kubernetes workloads for low resource and remotely located iot and edge devices and bare metal uh it was originally created by rancher it is a sandbox cncf project um so you know like you can use it for testing if you want there's no reason uh you can't but it is a little bit more work uh to utilize um but it's just considered a lightweight distribution right so a few differences is that k3s does not use cubelet but it runs cubelet on the host machine and uses the host scheduling mechanism to run containers k3s uses cube proxy to proxy the network connection to nodes as opposed to k uh kubernetes which uh uses q proxy to proxy network connections of an individual container uh k3s has a tighter security deployment than uh kubernetes because of their small attack plane surface and also one point the only way you'd have a backing store was with sql lite but now apparently you can use a like a miniaturized version of fcd um so the idea is that um k3s has some advantages but comes with some limitations you'll need to investigate for yourself whether it makes sense to use k3s for your use case we're talking about production use case but for testing and development you can use it but just understand that you are developing on a different distribution okay it's not it's not going to be similar uh to um fullblown kubernetes okay so then there's k3d and so this is a platform agnostic lightweight wrapper that runs uh k3s in a docker container because the one above runs in virtual machines and so if you want to run this in a docker container use k3d it helps run scale run and scale single or multinode k3s clusters quickly without further setup while maintaining a high availability mode right i guess i said virtual machines above but really the above one is actually bare metal i suppose you're like right on a machine not necessarily even with a virtual machine so i really should have said that instead straight onto hardware but that is k3s and k3d hey this is andrew brown from exam pro and we are taking a look at kind and so kind is another lightweight distribution but it's primarily designed to test kubernetes uh and it helps you run kubernetes clusters locally in a cic ci pipeline using docker's container as nodes so it looks very similar to minicube but you know the key difference here is that it's using docker containers so it is an open source cncf certified kubernetes installer that supports highly available multinode clusters and builds kubernetes release build from its source um and so you know some people like mini cubes some people like kind i personally like micro kubernetes but it's just another tool and we will give it a go and you'll see it in action um in the fall lungs okay hey this is andrew brown from exam pro and we are looking at micro kubernetes or micro k8s uh depending on how you like to say it so um before we talk about that let's talk about a few other things like ubuntu so ubuntu is a linux distribution based on debian i'm going to explain why this makes sense here in a moment it seems like a deter but it's not so ubuntu is known for lots of linux programs preinstalled uh one of the easiest links distributions to use more frequent updates more progressive on new linux programs and systems and so here's an example of ubuntu the desktop edition uh but of course you know we would probably be using server or core because we'll be utilizing it for running uh servers right and if you do not know ubuntu is produced by canonical and this is the company behind ubuntu and another project that canonical makes is called snap so snap is a package manager by canonical that can be installed on many different distributions of linux and so here's an example of installing ruby and a lot of times you'll see this classic flag and so this allows access to your system's resources in much the same way a traditional package so without the flag snaps runs in complete isolation it just depends on the state of your uh environment but um sometimes you have to use the flag sometimes you don't so let's talk about micro kubernetes so micro kubernetes or micro k8 as i say mostly throughout this course is created by canonical and is installed using snap that's why we're talking about those two things so you do like a pseudo snap install micro k8s um and i think this is the major reason why we haven't seen as major adoption like minicube as micro kubernetes because of snap and snap is actually really great but just the idea of having to install a package manager manager people go i don't want to do it you know what i mean so and also it's like the only time people do it is that they spin up ubuntu but ubuntu is very popular on all cloud servers providers so it's pretty easy to get a hold of so it is a kubernetes distribution designed to run fast selfhealing and highly available kubernetes clusters it is optimized for quick and easy installation of single node multinode clusters on uh on multiple operating systems including mac os linux and windows as long as you have snap it is ideal for running kubernetes in the cloud local development environments and edge and iot devices you could devices you could use it for production use cases but you probably want to use a managed service for that and if you did use it you'd be using it for um you know a selfhosted right uh so micro kubernetes is a modular design uh you start with nothing and uh you can enable addons to quickly use exactly what you need and nothing more uh so i feel like this is like the best way of learning and so we do end up gravitating towards this lightweight distribution when we are doing our fall longs but there you go okay so we looked at a few different lightweight uh kubernetes distributions and i just want to do a quick comparison here so you can kind of understand the differences of of these things so the first is mini cube which runs in a virtual machine it's intended just for development purposes very easy to use very very popular uh probably the easiest one to utilize okay then there's kind it's designed to run anywhere containers run so anywhere docker is running you can run kind in it's intended just for development purposes it has faster startup times than minicube since it's not spinning up a virtual machine so you have one that runs in a virtual machine you have one that runs in containers then we have a k3s and k3d which are kubernetes distributions by rancher uh they're slightly different in terms of what they start out with like how they work they are still cncf certified all these are cncf certified kubernetes but these ones can be used for production use case the difference between k3s and k3d is this one is basically installed on bare metal or embedded systems and then k3d runs wherever docker runs anywhere that is similar to what kind does it's designed for embedded edge devices or limited resources and we did cover that there are uh some uh design choices that make uh these things limited compared to other things for production use case you have to make a trade on that then we have a micro kubernetes or micro k8s this is created by a canonical the same company that publishes ubuntu you need snap to install it if there is another way it's probably pretty hard to do it's a modular starts with nothing installed restarts everything if there's a crash well suited for selfhosted production use cases so everything here is great for production everything here is great for development but technically you can use all of these for development and even myself i prefer to use micro kubernetes for development and i think it's great because then you know if you do do selfhosted we better reflect that production environment but it's up to you what you want to use and it's going to be depending on what resources you can find so there you go okay all right let's take a look here at manage kubernetes providers and so these could be something like a cloud service provider or a cloud platform that abstracts away the effort of setting up maintaining like such as updating patching and cluster and they can easily uh perform auto scaling as well so um i mean that's going to vary the auto scaling part but um you know mostly this is how people are setting up the kubernetes they're going to be using a managed provider very common to use a public cloud but we'll talk about the ones that i know about i'm sure there's ones outside of this but these are probably the most popular ones especially if you are used to using csp so the first is google kubernetes engine gke and in my experience this was the easiest to use with the richest amount of features built into the ui right so when i wanted to set up an ingress controller and we did this in the course or or set up a service that goes ingress it was so easy to do um and it just had a really great experience out of all the the big csps next i would probably put amazon elastic kubernetes service um it does have a very difficult uh ui so um aws is like you know sometimes you'll be setting something up and it'll be like you need this thing and so you have to go to this other place and create it first and then go back and so there is a bit of running around however they do have a a cli called eks ctl i believe and it really does make it a lot easier so uh you know it's not really recommended to use the ui and a lot of people doing kubernetes you get used to using cli because because of cube ctl but the real reason i think that amazon elastic community service i put it second is just because it's worth it for the integrations with other awes services because uh you know maybe uh eks is a bit clunky but all the stuff around each of us and that's what you want to be integrating with like your managed database and and your persistent storage and things like that it's just such a great ecosystem that that's why i put it second but google's really really good for cloud native you have azure kubernetes service this was fairly easy to use um you know like its interface wasn't as good as gke but you could basically accomplish things it has some unique offerings like i've seen like debugging live containers which is really cool um and they have good tutorials so like the thing is is that you know even though they're clunky um their instructions are a lot better than aws's and so uh you know works a little better but this is like if you are bought into that microsoft ecosystem so you know all three are are pretty decent i put these at the top three nexus ibm uh cloud kubernetes service also known as iks it's easy to use um beautiful beautiful ui out of all the uis that i saw it wasn't as featurerich as i was expecting to be and it was very very expensive and so like ibm likes to say it is cost effective because they actually do allow you to spin up a free kubernetes tier so technically um you know for 30 days or or what have you you can actually have a free cluster and this is actually a great way to test out a managed service but the only problem is that the nodes are so darn expensive and so you don't play for the control plane on uh icast or ibm ibm cloud community service but the cost gets so expensive so fast it's like using heroku where it's like it's free entry um the small sap is um uh you know like very inexpensive but it gets super expensive super fast so i generally probably wouldn't recommend unless you are already using ibm and you like using bare metal and you're used to paying those kind of costs okay you have oracle container engine for kubernetes um oracle is known for being highly cost effective out of all these other providers i don't think they charge you for the control plane but honestly their ui is really really bad and they even default it to bare metal when you start to spin it up and if you do that it'll cost you like thousands of dollars so it's easy to make a mistake when setting it up and costing a lot of money and um you know support on oracle is not very good so if you do have a problem and there's a high chance you will you know just being cost effective is not going to help you one thing i remember about ibm cloud was that when i set it up it was really good at estimating the cost and making it very clear what things cost so kudos to ibm for that but even still like still really expensive you have digitalocean kubernetes so doks very easy to use predictable spend beautiful ui um and i mean the only thing was that it was a bit like everything was beautiful but it was a bit clunky to use where i was like like you know the experience is like the ui so good but then like the technology kind of fails to deliver the same kind of reliability and so it kind of feels like there's this beautiful ui and then things are a bit clunky behind the scenes that's how digitalocean felt to me but but it was very simple to use you're not going to get that whole ecosystem that a cloud service provider is going to provide you but it is very simple and easy to manage if you're a startup this is a very safe bet for you to use use digital ocean kubernetes so it's not terrible but um uh digitalocean is very very very very good with uh tutorials um and they had some stuff there but for kubernetes i'd say it's probably the weakest i've ever seen them for instructional content they even had a dead link that i had to point out be like hey this goes nowhere so i think it's more of a newer service for them and uh you know as they continue to build up the resources around it it'll be good then you have um it's the most cost effective they don't charge for the control plane um i wouldn't say the ui is beautiful but it's extremely simple so if you don't know what you're doing when you enter there you might be a little bit frustrated at the start um but uh like if you've used it lotion then you'll know how to use sevos um and they do have a nice like labs like instructional content within sivo um but it really is just a cloud platform but different from digitalocean it is specialized just for kubernetes right so they technically do have vms and other stuff on there but it's just it's more focused on kubernetes than any other cloud platform um and so you know there's a lot of tradeoffs there you just got to decide what you'd want to use for me i'd be going with eks just because my primary workloads are already there but i really did enjoy a google kubernetes engine for sure but there you go all right let's take a look here at management layers there's a lot of management layers for kubernetes and some of these might kind of be their own distributions with a platform on top of them so it's really hard to kind of put them in a box but i try to group them separate from distributions as best i could that were pure distributions so management layers are for running kubernetes on other platforms or allows you to extend your control plane to multiple platform and when i say other platforms i mean like um you know like adabus has eks and that is their kubernetes as a service offering but i'm saying like there is no as a service you're just like setting it up on maybe either virtual machines or things like that so let's go walk through them the first is weave kubernetes platform wkp now if you go to the website right now you can't find this exactly it's kind of weird they might have rebranded this as like weave core or something like that but um i just stuck with this name because i preferred it so all of weaves open sources tools packaged as a platform so you can build out a git ops enabled cluster and so what it looked like to me was that they had a lot of ci cd stuff and the idea is like you could set up a cluster relatively easy uh most places and have good governance over them good way of being very uh agile and just start to get to work so that was kind of interesting then there is rafae no idea if that's the correct branch nation but similar to openshift and we'll talk about openshift in a moment but with a larger focus on governance and get ops based management for any kubernetes clusters running on anything including openshift so when you go to the rayfay website uh they'll have this picture of like showing that they can manage anything from anywhere and so the idea is if you buy into their system um you know you just need to learn one interface as opposed to a bunch of them and so that could be very attractive then there's vmware tanzu so wherever vsphere runs you can manage you uh you can manage and deploy and monitor kubernetes cluster my knowledge of vmware is not that strong like i kind of know what vsphere is like it's a thing that you install on your virtual machines it has to do with virtualization so i think of it like you go on aws and i would assume that you install this on bare metal probably that's what you do so imagine you have like bare metal like machines that don't have virtualization installed on it so you install vsphere you press a button that sets a vsphere and you install your license but anywhere there's vsphere is anywhere you could install kubernetes cluster um and then you could emanage it from tanzu so like it makes it it's like a management layer uh anywhere so kind of similar to ray faye but rafe is actually leveraging the actual as a service um offerings so like it will actually leverage eks it's not like uh it's a vm you know what i mean like it's a custom install so there's kind of a difference there then you have these things that are multicluster management so first is azure arc and so this allows you to govern compute such as kubernetes like and also virtual machines or sql servers across more than a single cloud service provider on premise or on the edge so it's not kubernetes specific but it is very often used for kubernetes and the idea is we are extending our control plane so we have one interface uh to govern uh compute other places right um and that's really useful i feel like from a compliance standpoint so like let's say you want to run stuff on a bus or somewhere else um but uh you know like it just becomes a lot more complicated if if uh you know from a governance standpoint so it's really nice to have a single tool uh for that okay we have google anthos so this is also multicluster management this one in specific is just kubernetes and as far as i understand it's gke so google kubernetes uh um engine but it's extending the uh the cluster to be managed on other uh cloud service providers so like i believe aws i don't think they have azure for some reason or on premise but basically um it takes over vms as far as i understand and then installs uh whatever it installs on that and that allows you to manage everything from anthos um and so you know from kubernetes productive i think it has an edge over azure arc um but it doesn't talk about like governance and stuff like that so maybe for like a security or like compliance standpoint azure might be better but not as nice to use then you have platform nine so this is similar to rayfay but relies more on third party tooling whatever that means but as far as i understand it doesn't leverage native functionality from cloud service providers so it's not using gke or it's not using eks uh it is using some other thing that might be installed on virtual machines to kind of like simulate maybe what google anthos is doing stuff like that um so you know there's the idea there so this is this stuff and then there's two that i kind of want to give a little bit more attention to because they're bigger deals like rancher and openshift so red hat has openshift and it's a platform as a service for kubernetes and so cr openshift is just kubernetes like it's not it's a distribution but the idea is that it is just a commercial platform with kubernetes installed on it and then they've extended like a platform around it to make it really easy to use and what's interesting like when i saw an ibm cloud you could actually launch openshift in ibm cloud and you can probably do in other cloud service providers so the idea here is that they uh they extend cube ctl so they have their own called oc which stands for open shift you think it'd be os but it's oc cli and so it has some additional functionality that makes conveniences easier like rbac which is kind of a pain to set up and they make that a lot easier and things like that you quickly deploy local code to a remote openshift cluster via odo so that's another cli tool they have that makes things easier they have quality assurance pipeline built into the platform so that saves you some time there fixing critical bugs earlier instead of waiting for next kubernetes release that red hat is known for being very very very very good in the enterprise for um rolling up bugs fixes very quickly right because they're used to managing their red hat distribution um and so you know that's really the reason people pay for it is just to make sure that everything's patched and good uh their platform utilizes red hat core os and this is an opera operating system optimized for running containers so that's kind of interesting operators hub is an automated installation tool for one click marketplace other providers like gke is a oneclick marketplace so i'm assuming that's even more integrated because it's so opinionated about even like the operating system that it must be really easy to set up um it has a graphical ui for developer consoles that's really nice code ready workspace this was really interesting so like developer environments and we used one in the course cloud nine and we look at uh google's for a bit but um this one is specific for kubernetes right the only thing is like i didn't use it because um red had such a pain to sign up for like i tried signing up for openshift and it couldn't take my canadian postal code and it broke and i waited like a week for them to fix it and they were asking for so much information it was i was not happy about it but like they do have like a 30day trial but i don't know i thought it was better use cloud9 in that case but it's just very impressive to see that there's a cloud developer environment for kubernetes because like you can't do it on git pod and on any of us we do it because we're running a virtual machine but i mean this one's more optimized for it we have rancher kubernetes engine rke and uh i don't know the logo is not there probably shows up at the end here but it runs entirely within docker containers um rancher is the one that built k3s i don't think it's k3s in particular it's their own just it's another distribution because i would imagine that you'd want the full power of kubernetes it works on bare metal and virtualized servers rk solves the problem of installation complexity a common issue in the kubernetes community installation operation kubernetes is both simplified easily automated it's entirely independent of the operating system and platform you're running as long as you can run a supported version of docker you can deploy and run kubernetes with rke so as far as understand you can bring our key anywhere you want so it's like oh you want to run on aws great make sure you have something like a vm that runs docker right go to google check box docker and uh use that thing and so that gives you a bit more ability there but you know these are the two big ones i would say and that's why i gave them a lot more detail but there you go so something you're going to keep hearing is a term that is cncf certified kubernetes what does it mean when a distribution has been certified by the cncf well they have this page called software conformance and the idea is that they have this kind of test that you can run that gives a guarantee that your kubernetes does what it says it does that it meets very specific requirements in order to conform uh to what is expected with the open source um kubernetes and so here you can see that there is a lot of distributions like even here it says like 60 something i think they say that there's 80 plus over 90 over 90 certified kubernetes offerings but the idea is that if they all are certified then more or less they will work pretty much the same with the expectation of what they are asking and uh there is like a landscape for it here so like i clicked the link somewhere here and it's just another way of opening up and seeing pretty much the same stuff in the cloud native landscape and the way it works is um they have a bunch of instructions here to run the conformance tests and it looks like um it uses this tool called sano buoy and so this is a diagnostic tool that makes it easier to understand the state of kubernetes cluster so this checks for the state and then you actually have the tests which probably reside uh somewhere in here they probably explain that there so just kind of give you an idea what we mean when we say cncf certified distribution okay all right let's take a look here at container runtime interfaces and to understand that we need to understand uh what's um on top of it and what's below it so the first thing uh when we think about um uh this kind of this kind of layered approach is orchestration so you would have something like either docker or kubernetes and so these orchestration systems will use a container runtime interface now at one point in time docker just meant containers right um but there was work to be had where they started separating out all these components and this is basically where we see container runtime interfaces and container runtimes underneath which are the actual containers and so the idea here is that um you know kubernetes would be utilizing something like either container d um or it would be using something like crio so when we talk about container runtime interface you'll notice that's why i never had an icon uh earlier in the course when we were talking about container time interface because the runtime interface is one of these two okay so container runtime interface allows you to run a variety of different container runtimes and so the cri actually is doing things like pushing and pulling images and supervising examiners it's not the thing that creates and runs the containers what does that is the container run times and the containers that we care about are oci compliance so from the open container initiative and we have two types native runtimes and sandbox slash virtualize runtimes and the major difference between these two types of runtimes between native and virtual is isolation so for virtualized it can provide security benefits through isolization because it's virtual so you can have more controls around it um so you know hopefully that helps make sense that you have an orchestration and that's going to choose a container runtime and then from there it's going to choose a uh interface and then from there it's gonna choose a run time but i think by default when we use kubernetes most times what it's going to be using it's gonna be kubernetes it's gonna be container d and it's gonna be run c and there used to be a lot more container run times but i had to look at each one and i found so many were like no longer maintained and so i think that this is like the new list that you'll mostly see is like run c nabla and uh cata okay all right let's talk about container d and with these container runtime interfaces um they don't really tell you a lot about how they work internally i tried so hard to find some kind of visual and i just couldn't find anything and so you know if i can't find it then you don't have to worry about it so container d is an industry standard uh container runtime with an emphasis on simplicity robustness and portability and the way it started was it was part of docker and they extracted out their container runtime into the project we know now is container d and then gave it to the cncf so this includes docker functionality for executing containers handling low level storage managing image transfers and container d makes it easier for projects like kubernetes to access the lover docker elements they need instead of actually using docker so when you're building images you uh you aren't actually building uh docker images you're building open container initiatives which has been standardized um for the container industry okay let's take a look at crio so ciro is an implementation of the kubernetes cri interface to enable oci compatible runtime so the cri stands for container runtime interface and the o stands for oci okay so that's the logic there and it's just an alternative to container d so it's a lightweight alternative to using docker container d as a runtime for kubernetes it allows kubernetes to use oci compliant runtime as the camera runtime for running pods today it supports run c and cathode containers i didn't look at what container d supports for runtimes underneath but that's where you know if you need particular runtimes you might have to choose a runtime interface that you can use and when you use managed providers they might lock you into very particular container runtime interfaces that you cannot swap out so that is something that might matter to you but it really is more like a security and performance thing uh and not so much like functionality wise because things don't seem to be much different um so that's the idea there but yeah so cro is just an alternative container d and you know i wish there was more to say about these container runtime interfaces but that's all i have to find okay so let's talk about container run times and again there's not a lot to get into detail about container run times but a key thing to understand is difference between virtualized and uh ones that are native runtimes that are utilizing uh the machine directly and so from cata containers they have this nice graphic that makes it very clear visually what the difference is so virtualized runtimes uses lightweight vms for isolation and then native runtimes are on the machine and they're using c groups shared kernels and other things for isolation so you know in that sense when everything is in a lightweight vm you can put a lot more restrictions around it and so that is something that makes it a lot better from a security perspective it confuses me because like i would imagine that um you know like you can run can you run a lightweight vm inside of a vm i don't know um so i don't know if like you just replace the hypervisor with something like lxc which is something we actually do talk about um in the section of this course is like um linux containers um where it's not full virtualization it's lightweight virtualization so i imagine that's probably the difference but i just couldn't find definitively if that was the truth um so let's just talk about run c which is a native container runtime so it's a low level container runtime that creates and runs containers and it would be used alongside container d or crio that's all i could really tell you about it um so imagine you have container d and remember container d or cri basically similar it's a daemon process that manages and run containers pushes and pulls images managing storage and networking supervises the running of containers and then you have your container run time like let's say run c which creates and runs containers so just reiterating that i know we said it like three times but just make sure you know the difference between a container runtime interface and a container time okay hey this is andrew brown from exam pro and we are taking a look at c groups so what is a c group well we have to answer what is a process first so a process is an instance of a running program on linux so then what is a c group well c group stands for control groups and allows you to group processes to apply different kinds of limitations so limitations could be things like resource limiting so groups can be set to not exceed a configured memory limit which also includes file system cache prioritization so some groups may get a larger share of cpu utilization or disk input output throughput accounting so measures a group's resources usage which may be used for example like billing purposes controls so freezing groups of processes so they're checkpointing and restarting so think of c groups as a way to limit programs on linux from overusing cpu membrane storage and the reason we're learning about c groups is because of linux containers so the primary goal uh design goal of c groups was to provide a unified interface to manage processes or whole operating system level virtualizations including linux containers and um you know i came across c groups even recently when i was using git pods because i wanted to run kubernetes on git pods but i found out that uh at the time and probably still that they're using docker with a version that is using c groups version one and so to support uh kubernetes the run within docker which is kind of weird running docker inside kubernetes to run docker um uh it needed version two so c groups is something you definitely will come across and so that's why i mention it to you here hey this is andrew brown from exam pro and we are talking about linux containers so linux containers is an os level virtualization technology that allows creation and running of multiple isolated linux virtual environments on a single control host and so the idea is that linux containers or virtual environments in particular are paired against virtual machines quite often because i mean the idea is virtual environments are more lightweight so why would you use one over another so virtual environments there is no preloaded emulation manager software as a virtual machine in a virtual environment the application or os is spawned in a container and runs with no added overhead except for a usually minuscule virtual environment initialization process there is no hardware emulation which means that aside from the small memory uh small software penalty lxc so linux containers will boost bare metal performance characters because it only packages the needed application and uh ves i didn't capitalize e on that cannot be easily managed via neat gui management consoles and they don't offer some neat features of virtual machines such as infrastructure as a service setup and live migration so i definitely didn't rework this text wherever i got it from that's what it is plain but i just wanted to mention linux containers because you know uh it is a type of virtualization for containers um uh it's quite popular there and i came across it somewhere in the course and so i just had to figure out what it was okay hey this is andrew brown from exam pro and we are taking a look at container storage interface also known as csi and what csi does is it standardizes how container orchestrator system so cos access to various source providers now i don't find that this is a very common initialism but i found it so i just put it in there so a container orchestrator system is on one side on the other side we have our storage providers and in between we have our cloud storage interface so container orchestrator systems could be things like mesos kubernetes and docker swarm so we know what kubernetes is mesos was like um a very early orchestrator system very modular and things like that i think it's still kind of in use but i don't know if they'll stay around for long because a lot of people moved over to kubernetes and then docker swarm is kind of like a competing thing for kubernetes because docker didn't really do orchestration the same way that kubernetes did but now it came with docker swarm so storage providers would be things like azure disk aws elastic block storage netapp trident uh openstack uh cinder google cloud storage there's over a hundred plus storage and plugin drivers and so um you know the csi is uh essentially what we're using when we are using persistent volumes okay because persistent volumes are pretty much dealing with block storage and um you know most of these plugins are block storage i'm sure there's object storage in there as well and the cloud search interface is not something you really have to worry about because it's kind of abstracted away but if you were to develop a uh your own search provider then you would have to know how the container storage interface worked because you have to make it compatible with it in order to provide your um your volume okay hey this is andrew brown from exam pro and we are looking at kubernetes backing store and fcd so components of kubernetes clusters including pods nodes control planes volumes all sorts of things need a level protection in case of disaster we need to store the state of what our stuff is in case we need to recover it so what kubernetes does is it uses or it stores the state data inside something called at cd which is a key value star which we'll talk in a moment but it is possible to uh back or use a different backing storage like mario db i have no idea how but i know that it's possible and so application data is stored in persistent volumes for applications running on clusters but understand that the the resources um are like the state of those are stored in a key value store so fcd is a strongly consistent distributed key value store that provides a reliable way to store data that needs to be accessed by its distributed system or clusters of machines and xcd resides in the control plane node and you need to know that's where it is so lcd is not just used for the backing storage it's used by some other projects so obviously the kubernetes cluster uses it because of the control plane core dns uses it rook uses it but you don't see it used for other things it's a key value store and it serves its purpose like in these cases okay hey this is andrew brown from exam pro and we are looking at min io and rook and the reason i want to look at them is because uh you might see them as options on the exam and if you know what they are it's going to really help you determine what the right answer is rook and midnight are two things that we do not cover in the fall longs because they're a little bit too complex to set up and they're interesting because like they are basically um i would say like special types of storage um but we'll talk about what they are so you kind of understand so rook turns distributed storage systems into selfmanaging selfscaling selfhealing storage services it automates the task of the storage administrator so deployment bootstrapping configuration provisioning scaling upgrading mitigation disaster recovery monitoring and resource management and it can use things like cipher fs i believe and things like that but basically as it says it's distributed storage systems okay so we think of pods pods are generally backed by block storage but this kind of stuff is not attached to a specific pod it's just distributed storage that you'd use maybe this would be for like data pipeline workloads or things like that i don't know i didn't i haven't deployed rook but i just know that i've seen the word on the exam so i wanted to point out kind of what it was min i o is a lot more clear to me its use case so it offers high performance s3 compatible object storage so it's essentially object storage um and it has a um it says s3 compatible basically works just like s3 so it's native kubernetes so mini o is the only object storage suite available in every public cloud uh i i mean anywhere that you can deploy kubernetes basically uh you can use it so min io is software defined 100 open source under uh new apgl version 3. so i really like the idea of min io it's i really like s3 so midio is an easy sell for me but yeah that's all those two things are okay hey this is andrew brown from exam pro and we are looking at volume so kubernetes supports many types of volumes and a pod can use any number of uh volume types simultaneously so let's talk about the different types we've got persistent volume so these this is when you're attaching external storage to a pod and the data will persist even if the pod is terminated we have a fearable i i have looked this up before but i just can't remember how to say ephermal volumes so volume that only exists as long as the pod exists intended for temporary data storage so i mentioned here but we don't actually use it in the kcna as an example projected volume so maps several existing volume sources into the same directory and volume snapshots so archiving a volume configuration and it's data for roblox or backups so uh yeah in the kcna i don't show follow along on doing volume snapshots but they're not that complicated they're not covered in the exam and one thing that's not here is pvc we cover it in the course i'm just not messing it here because it's not a volume it's a way of claiming a volume now these are types of volumes but there's also types of volumes that are supported that you can be backed by right so the idea is persistent volume attaches to a storage class and um you know it has to be something right so here we have elastic block store azure disk azure file cepher i think that's how i pronounce the stuffer cinder config maps um maybe local storage nfs secrets we have the persistent volume claim so you know there's a lot of stuff there and you don't need to remember the whole list but you will in practicality understand when we use um a storage class and we have a persistent volume and we have it attached to something okay hey this is andrew brown from exam pro and we are taking a look at persistent volumes so a persistent volume or pv is a piece of storage in the cluster that has been provisioned by administrator or dynamically provisioned using storage classes so here is my diagram and so a pv is similar to a node in that it's a cluster resource okay so um when i say that a persistent volume does not reside within a node or a pod um and i want to point out that this diagram here is technically not accurate i just had it as a simplification and also while i was learning kubernetes um it wasn't clear to me that you had to have a persistent volume claim and only when i did the followalongs do i realize that you always 100 have to have one um but there are diagrams out there that don't show the pevc so um i thought this was accurate at the time that i made it but i still left the sense just so i could say that this was technically not accurate and that sometimes you'll see um diagrams that do not have a pvc okay and we'll talk about pvc in the next slide so don't worry if it doesn't make sense right now or future slides here so uh persistent volumes are volume plugins like volumes but have a life cycle independent of any individual pod that uses the persistent volume so the idea is if the pod goes down the persistence persistent volume can still rebate it really depends on your reclaim type or uh yeah how you want to reclaim it so the api objects captures the details of the implementation of the storage be that nfs iscsi or a cloud provider specific storage system mounting persistent volumes directly to a pod is not allowed and is against the kubernetes design principles it would cause tight coupling below the pod volume and the underlying storage that's why we need a persistent volume claim to ensure decoupling so there you go all right let's take a look at storage classes so a storage class is a way of defining a class of storage that is backed by a provisioner and so here's an example diagram that we have here and just notice that i don't have pvc in here because again some diagrams online do not show the pvc but they are always always there and we will address that when we get to the pvc which probably is the next slide and so to set up a storage class the idea here is um we need to define a few things so we need to define the provisioner so who is this from uh so this is from aws so it's going to elastic block store parameters what type of storage so to use so in this case we are saying use the gpu type of elastic block store on aws you have the reclaim policy so you know if the pod is deleted it should retain the persistent volume it should not vanish notice the storage class is just like uh defined and then it's um associated with a persistent volume so it can be that way there but notice that a storage class doesn't define one volume but it's just saying like okay persistent volume you're going to be using elastic block and you're going to be using the elastic block but they have their own block storage for each of those okay okay so we kept on mentioning persistent volume claims and here we are pvc so a persistent volume plane is used to decouple persistent volumes from pods and a pvc uh will ask for a type of storage and if a persistent volume meets that criteria it is matched and the persistent volume is then claimed and bound okay so what does that mean well if we go down below here the idea is we have our persistent volume claim and we're saying i want um i want a elastic block store and it has to be at least uh 500 megabytes and then the persistent volume says oh well i have a 30 gigabyte one here so match up to me that's the idea there so pvcs are similar to a pod requesting resources from a node so pods consume node resources so a persistent volume claim consumes persistent volume resources pods can request specific levels of resources like compute and memory as i was describing here and a persistent volume claim can request specific sizes and access modes uh and here's an example of a persistent volume claim uh notice that the storage class is mentioned sometimes i see it there sometimes i don't um sometimes i see it with like a double quotation so you know just be aware there are some variants there not something super important to understand right now um but yeah i mean that should be clear uh yeah there's also the storage type so this one would match on the storage type and the access mode to find it so there you go hey this is andrew brown from exam pro and we are taking a look at config maps so config maps is an api object used to store nonconfidential data in key value pairs for pods pods can consume config maps as environment variables command line arguments configuration files in a volume and so here is an example of one and we do use this one in the follow along uh but the idea here is that you have a section i mean you define the config map you define your data uh and notice that you can have a key and a value and then for some uh there's ones that are multiline that get interpreted a particular way when you parse them so here's an example of config map and just notice that you can use a config map for more than a single pod a config map allows you to decouple environment specific configuration from your container images so that your applications are easily portable so yeah you are passing along basically environment variables or things to provide application configuration hey this is andrew brown from exam pro and we are taking a look at kubernetes services so kubernetes services allow you to attach a static ip address and a dns name for a set of pods and a kubernetes service allows you to persist an address for a pod even if it dies and the service also acts as a load balancer and i kind of regret not having like something that looked a little bit more like a load balancer by like having a second uh lined a drag layer to show load bouncing but i mean that's the way i designed the icon but here you can see we have a service we have endpoints so that's basically um the storing of the public ip address so that we can map the service to the pod and you can see that this will have i mean i didn't write it in here but this would actually have a static ip address that never changed and then it would randomly distribute to the ones there a pod without a service will have a dynamic ip address so when the pod dies so does the ip address so that is the reason we want a service so that doesn't happen kubernetes services have the following service types we have cluster ip this is the default if you don't specify a type it will be a cluster ip it randomly forwards traffic to any pod set with the target port you have headless sends traffic to a very specific pod when you have stateful uh pods like a database and we covered headless in the stateful set section there where it doesn't assign a static ip address it doesn't do load balancing it just gives you like a dns hostname resolution so you can specify the pod that way we have node port so external service that allows you to use a worker node ip address so it's a way of exposing um the node like with like opening a port on the node because generally a cluster ip you know it's internal so you have to access it within the cluster but we'll talk about that when we get to that slide there we have load bouncer similar to no port except leverages cloud service providers load balancers and then we have external names so a special service that does not have selectors and uses dns names instead and this is uh used when let's say you had you have a database um like a managed database like from aws rds or google cloud spam or something like that and you can't point to an ip address and you need to point to a dns name so use an external name to point to external resources all right let's talk about traffic policies for services so a service allows you to set a traffic policy to determine how ingress traffic is routed there are two types of traffic policies external traffic policies so how traffic from external sources is routed and has two valid values cluster so route traffic to all the ready endpoints local so only route to ready node local endpoints we have internal traffic policy so how traffic from internal sources is routed and has the same values above cluster and local so if the traffic policy is local then there are no node local endpoints then then q proxy uh does not forward any traffic for rel relative servants uh relative relevant services we do not cover this in the fall along traffic policies we do not need to create one but i just wanted to point it out because i think i saw the term on the exam and it wasn't an answer but i just saw it and i just wanted to include that so you knew what it was let's take a look at the cluster ip service type so cluster ip is the default service type so if you don't specify one this is what you're going to get it is used for internal traffic external traffic will not reach the service so traffic will be randomly distributed to any targeted pod so here's an example where we have internal traffic it goes to proxy um which goes to our cluster ip to your pods so traffic originating from within the cluster will pass through the nodes kubernetes proxy and then onto the kubernetes service that's where it's using something like iptables or stuff to do the load balancing but we can use that or ipvs which we do cover in this course if you emit the emit the type it will be cluster ip but here i am explicitly specifying it a service can span multiple worker nodes for cross node pods and that is not just specific to cluster ip that's all of them which i should have mentioned in the service section there but just taking a look here over on the right hand side we have our port and our target port so our target port is the port that we want to target on the pod so if we're running a web application on port 8080 that would be the target pod and if we wanted to um send traffic to a particular port like port 80 it would ford over to the 8080 port okay so hopefully uh that is pretty clear so when would you use the cluster ip well for debugging testing internal traffic internal dashboards but honestly like when i was doing the follow alongs there were cases where there were there were ways of um there were there was lots of use cases for a cluster ip but this is what kubernetes tells us that it's used for so that's what i wrote there okay let's take a look at node port service so notepad allows you to expose a port for a virtual machine a node running pods that the service is managing and so the idea is this is what a way we can get external traffic and it's the easiest way to get a traffic in but not necessarily the recommended way for production use cases and over here this looks a lot similar to our cust or cluster ip manifest file but here we're specifying node port and now it has this target port so there is no external load bouncer so node port is intended for single community service and for nonproduction workloads um and since i have the diagram here i'm just going to quickly explain how these three ports work and then we'll go and look at a more verbose text description so that we just are we're very familiar with these three numbers so node port is the port that you're exposing the node on the machine so that the traffic can reach it to port is the service port so the port that you have to hit internally to reach the pods and the target port is the port here that uh is our application is listening on so i write 8080 but this one says 80. okay so just to reiterate because it is kind of confusing and i just want to give it some extra attention port port exposes the kubernetes service on the specified port within the cluster other pods within the cluster can communicate with the service on the specified port target port is the port on which the service will send requests to that the pod will be listening on your application in the container will need to be listing on this port also node port no port exposes a service externally to the cluster by means of the target target node's ip address and node port node port is the default setting if the port field is not specified so there you go all right let's take a look at the load balancer service type this allows you to use an external load bouncer and the external load balancer handles the routing and traffic distribution logic so the idea an external load bouncer could be something like the aws network load bouncer which would be managed by a third party cloud service and so the example here is we are specifying type load balancer and then the status here the load balancer is we're specifying the ingress so like the ip address i suppose of the load balancer and then sometimes we have to specify the cluster ip i'm a bit confused about this because in the follow alongs we generally use ingress instead of load balancer and ingress seems to utilize um the load balancers of a like a cloud service provider so probably people don't use load balancer often or at all and then when i do see them in the documentation they're describing level three or four load balancers so this isn't at the application layer so maybe there is a case for it maybe ingress came later and then this just became kind of mute but it is a type of service so we do learn how to do it uh load balancer type is well suited for production workloads generally it's recommended to use the kubernetes ingress as i just said here let's take a look at the headless service type and we've already had exposure to this a couple times but let's talk about one more time a headless service is a service with no cluster ip address a headless service does not provide load balancing or proxy proxying so the idea is you have a cluster you have a kind which is cluster so even though we don't specify it is still a cluster ip kind but we are specifying cluster ip none okay so that's what makes it headless so head lists are useful when you are dealing with a stateful application so like reads and rights and you need the rights to go to a specific pod so here is a diagram that is different from the one that we saw earlier but makes the point clear where you have the headless service it's pointing to a pod and the way it works is the naming is based off of um the pod and the service if you have pod one it's called that and then uh the server the headless service called h serve that's gonna be the dns record that you use how the services need to manage network identity for uh for or of the stateful pods by signing a dns record teach pod so you can route traffic to a dns hostname so there you go hey this is andrew brown from exam pro and we are looking at the external name service type so external name services is the same as a cluster ip service with the exception of instead of returning a static ip it returns a cname record so what is a c name record well stands for canonical name record and it's a dns record that maps one domain name an alias to another one a canonical name and so an idea here would be a cname would be like mydatabase.example.com which would point to like if you had um a pod that was trying to call out to that um domain name the cname would route it to the external service and that's how that would work though here i'm showing pods so um i mean maybe it would make more sense if the arrows went this way but anyway the point is is that it's used for routing things to external services so i don't show it here but imagine you have rds okay rds you cannot sign an ip address it has like a a fancy dns host name so you would put rds host name here and so when the pod in your application code would hit this address it would go to the external service and that's how it would know where to go all right so really this should be pointing to this i suppose okay but you know hopefully that is clear it is very clear in the follow alongs because we talk about it a few times okay hey this is andrew brown from exam pro and i want to point out a very particular command for cube ctl that has strong relationships to deployments to services and this is the exposed command so expose is used to quickly create kubernetes services for deployment so here it is and uh we do it quite a few times where we'll type cube ctl expose then the deployment then the app sometimes the documentation will show it like this we'll say deployment forward slash my app again that's just a alternative syntax that you can use you can definitely use spaces you could say deploy and then the app probably works with replica sets as well but honestly you should always be using deploys anyway but the idea here is that it just allows you to quickly create a service and attach it so that it points to the pods of a particular deployment so here what you can see is that we are setting up one that is a node port service we're setting the name the port the target port the node port i'm assuming i got that right and i wasn't supposed to put a hyphen in between there or something um and there is another command where it's like cube ctl create service and so those can have kind of a bit of an overlap but the idea there is that if you create a service um it still has to link to something so most cases it doesn't make sense to do that there's probably some edge cases and i feel like cube ctl exposed does a little bit more like i thought maybe like it added a proxy or did something but i just could not find that documentation and i found it somewhere online wasn't official docs so it definitely does a little bit more but i don't fully understand that extra bit to explain it to you but i just know that we will use it and uh if you always can use manifest files to create your services but this is just really good for a quick way to expose kubernetes or like a kubernetes pods okay hey this is andrew brown from exam pro and let's talk about busybox so busybox combines tiny versions of many common unix utilities in a single small executable it's basically the swiss army knife of embedded linux as the single executable replaces basic functions of more than 300 common commands and so busybox can be used to interactively debug services to ensure they are working um i'm pretty sure on the the ck ckd you will use busybox for things and in this course we use busybox uh so that is something that we can do there usually it's going to be something like cubectl run dot i t so interactive t i don't know what the it stands for but i remember doing it many times but this is an example of running busy box that is continuously running and you can sh into it and then use its many commands one command it does not have which is very frustrating is curl because i really prefer curl over wget so even though that's 300 it doesn't have curl but i guess you can't have all the commands you want but it is still very useful hey this is andrew brown from exam pro and we are looking at kubernetes ingress probably the hardest service to deal with not because ingress is super hard to write but you have to deal with ingress controllers and configuring those can be kind of difficult and in the follow alongs we definitely have a bit of a struggle there but we do get through it uh and we do it multiple times but let's talk about ingress so ingress exposes http and https routes from outside the cluster to the service within the cluster and traffic routing is controlled by rules defined on the ingress resource so here is our diagram here the reason we use kubernetes ingress is so we can translate a custom domain on an ssl to a service running within our kubernetes cluster in order for ingress to work you need to have an ingress controller something like abs gces uh load balancer uh or i guess ingress controller they're called english controllers or nginx ingress controller generally those are coupled to um uh particular load balancers right for those providers which you'll see so ingress enables you to consolidate the traffic routing rules into a single resource and run as a part of the kubernetes cluster so getting out my pen tool here just making sure we're in the same place we have our traffic it reaches our ingress controller this one is nginx and from there it goes to our ingress service which defines the routes and then it goes to the routes to the service and the service will reach our pods so there you go hey this is andrew brown from exam pro and we are looking at dns so dns stands for domain name systems and uh i mean the way i'm conceptualizing it is that it's a service that is responsible for translating resolving a service name to its ip address so this is in the context of kubernetes because dns stuff is um complicated uh so to say um and we could spend i don't know like a micro course just talking about dns but i just want to focus on the last part translating or resolving a service so kubernetes service name to an ip address and so for um a domain name system it's not uncommon to have like a table of things that point to things so we would have a service registry and so in the context of kubernetes this would be the service name and this would be uh the static ip address that is assigned to it so um you know the idea is that if you were to use that service name or registry like with an http request or just use a a tool that allows you to look up dns records like if you're trying to resolve where front name would go that it should result to that ip address so now we need to know about core dns so coordinates is the default dns server for kubernetes in the documentation you might see something that's called cube dns and that was the default prior to that uh and it was replaced because it wasn't that modular or had great plugin support and so core dns is the current default you can swap it out this is the one you'll want to use because it's pretty good and it ensures that pods and services have fully qualified domain names fqdns and without core dns the cluster communication would cease to work because it just would not know how to resolve things to things and you might ask what is a fully qualified domain name so it's a domain name that specifies its exact location in the tree hierarchy also known as an absolute domain probably a visual would help there but you know it's not that important what's important to remember is that we have this table of service registry and there's a way to look up dns and of course there is dns like let's say you're using an aws and you had route 53 which is um a dns service that allows you to register domain and then forward that domain making record sets to point to stuff and so you know that is like again another level of dns and it can integrate into core dns and things like that but this dns is just for stuff within the cluster okay so uh let's look at the functionality that is provided by core dns because there are entry plugins internal plugins and then out of tree plugins so things you can add on and so this will give you an idea of the things that core dns can do other than just resolving uh you know a domain name to an ip address so we have acl so enforce access control policies on source ip and prevents unauthorized access to dns servers any so any given minimal response to this anything azure so enable server zoning data from microsoft azure dns that's what i'm talking about those integrations with those cloud service providers cache enables a frontend cache health enables a health check endpoint something that you'd be very common with if you were again roughly three they have health checks built in so now is at the um cluster level log so enable query logging to standard output and many more such as like i think aws would be in there like web 53 and things like that so out of tree plugins get alias uh redisk so enabled cache uh using redis uh cuber not tie serve multiple uh kubernetes within a server and many more so you know hopefully that gives you kind of an idea of how robust this thing is and how modular it is let's just talk about some tooling that we should know especially if we're debugging this stuff not something that we really need to do at in the kcna level but uh it's just practically very good to know so coordinates pods are abstracted by a service object called cube dns remember everything kubernetes is pods so coordinates are pods that run in the cube system and um i don't know if i wrote this anywhere but it's very important to remember that core dns or whatever dns thing that you want to use lives in the control plane node when we were looking at a diagram that was the one thing i forgot to put in there i didn't have to get update the graphics i'm just reiterating on that again each pod and not just any pod uh like sorry when i say each pod we don't just mean coordinates pods we mean any single pod has this resolve.com file to help with dns resolving now this isn't a kubernetes specific thing this is just a linux thing where um you will have this file on your i don't know if it's in virtual machines but it's definitely in containers but if you were to cat it out like print the contents of it so here we're logging into that pod that container sorry and we do that it's going to show the name server so what we are in terms of the ip address and then you know the domain names that would resolve to this um pod okay so notice it says default svs cluster local things like that and another really useful tool is using ns lookup so let's look up name server lookup is a way that we can discover uh or see where things resolve so like remember i said there's a service registry that contains a service name and a um and a uh ip addresses so we could dump an ip address in here and then it would hit the core dns and be like what'd you find and this is what i found right or you could supply the service name or the full service name because this is just like a part of the name right the real name would be like really long like this okay so just notice like how long it is and like how short that is um but again you know a little bit outside the scope of the kcna but good to know um you know in general okay hey this is andrew brown from exam pro and i want to talk about load balancing because we see load balancing in a variety of different ways in kubernetes and it can get kind of confusing so i just wanted to describe the different types of load balancing that we'll come across but let's define what load balancing is first load balancing is a networking component where traffic flows through the load bouncer and the load balancer decides how to distribute traffic to the multiple targets such as compute nodes or other different kinds of targets based on a set of rules so examples of this would be ingress and the service kubernetes service both have load balancing but there's more than that so we have external load bouncers uh we have ingress we have service and then we have internal load bouncing like ip tables and ipvs which technically is what service is using so for external load bouncers um the load balancing is controlled by third party service think of elastic it's weird like uh i know application load balancer elastic load balancer by aws or nginx or things like that then you have um ingress so that is the actual service itself um so the idea is you have load balancing algorithm backend weight schemes you have service which will persist sessions uh will have dynamic weights and then in the internal uh turn load bouncing the load bouncing to the containers with the pods which is randomly distributed so just understand that there's more than one level of load bouncing and don't get too overwhelmed with that okay hey this is andrew brown from exam pro and we are taking a look at probes so probes are used to detect the state of a container and we have three types uh liveness probe readiness probe and startup probe um and in the kca i think you need to know how to set up probes and we do actually use um a type of probe i think uh readiness probe yeah we use a readiness probe um in this course but we you know we don't go super deep into the probes but we do get some exposure to them so let's talk about the liveness probes the liveness probe is used to know when to restart a container okay and so for example liveness probes could be could catch a deadlock where an application is running but unable to make progress so restarting a container in such a state can help to make the applications more available despite bugs okay then we have the readiness probe to know when a container is ready to start accepting traffic so pod is considered ready when all of its containers are ready and once uh one use of this uh one use of this signal is to control which pods are used as back ends for services so when a pod is not ready it is removed from the services load balancer then we have start probe so this is where we know when a container application is started so if such a probe is configured it disables liveness and readiness checks until it succeeds making sure those probes don't interfere with the application startup this can be used to adopt liveness checks on slow starting containers avoiding them getting killed by cubelet before they are up and running so there you go hey this is andrew brown from exam pro we are looking at net filters so net filter is a project that enables packet filtering network address translation and port translation so nat or napt so knapped not to be confused with nats which is an event bus that uses grpc it can be used in microservices uh translation packet logging user space packet queuing other packet mangling and the net net filter hooks are a framework inside the linux kernel that allows kernel modules to register callback functions at different locations of the linux network stack the registry callbacks functions is used when called back from every packet that traverses the respective hook within the linux network stack so projects that build on top of netfl filter that's why we're talking about it is iptables this is a generic firewall software that allows you to define rule sets nf tables so this is the successor to iptables ipvs so this is specifically designed for load balancing and uses hash mapping as its means so um when we are using a service it can be or sorry proxy i suppose it can be backed by either iptables or ipvs and nf tables is just not in the mix for whatever reason um and probably in the future kubernetes will default to ipvs and just not use ip tables but right now at least when i'm making this course and video iptables is still the default but we'll talk about that and we'll talk about ib tables in greater detail all right let's take a look here at ip tables but before we do let's define what a user space is so user space for modern computing operating systems segregates virtual memory into kernel space and user space so kernel space is reserved for running a privileged operating system kernel kernel extensions and device drivers user space is the memory area where applications software and some drivers execute so now what is ip table so iptables is a user space utility program that allows a system administrator to configure the ip packet filter rules for the linux kernel firewall so we have iptables which is for ipv4 and ip6 tables which is for ipv6 but generally it's just iptables here and iptables is simply a virtual firewall on linux and it's very common uh like on linux to have to experience ip tables so it is a common skill and really worth to know um so an example would be like let's say you wanted to restrict access based on ports and protocols you could add or update to the ip tables saying like okay the destination port is 80 the protocol is tcp and we will accept traffic open that up if we list them out hyphen l we can see what is open what is not what is blocked and things like that and again iptables is at least when i'm making this the default for cube proxy and you can change it out to ipvs which we'll talk about next all right let's talk about ipvs which stands for ip virtual server which uses the net filter framework and also incorporates virtual linux server lvs so the reason we use ipvs is because when using ip tables it can struggle to scale to tens of thousands of services as iptables is bottlenecked at 5000 nodes per cluster ipvs is uh specifically designed for load balancing and uses more efficient data structures so hash tables i think i might have called it hash mapping earlier but it was i supposed to say hash tables on the right hand side here allowing for almost unlimited scale under the hood if you've never heard of hash tables um we describe it in crypto uh cryptographical stuff like imisc 900 course we talk about it there in the future q proxy will default to ipvs i really wanted to get like a nice diagram to show how it works but i really just couldn't find anything and the point is is that you're just going to end up using in the future and it's used because of the limitations i'd be table so that's really all you need to know okay hey this is andrew brown from exam pro and i want to talk about the various proxies that we will encounter in kubernetes because there's more than just one and it can get confusing if you just don't know that there's more than one so what is a proxy well is a server application that acts as an intermediary between a client requesting a resource and the server providing that resource so here's an example i got this nice little graphic from a wikipedia but the idea is that once it asks them the current time it goes to the proxy the proxy says hey what's the current time it says the time it sends it back to the person so the idea is that they are in the middle um uh between the communication of two resources many or more so there are many uh kinds of proxies you'll encounter kubernetes and so we'll talk about all the kinds here so the first we have is cube ctl proxy so proxies from a local address to the kubernetes api server then you have an api server proxy so a bastion built into the api server connects a user outside of the cluster to cluster ips which otherwise not might not be reachable cube proxy so runs on each node and used to reach services proxy load bouncer in front of api server so acts as a load bouncer if there are several apis cloud load balancers so for external cluster traffic to reach pods and in particular the ones that we will notice will be cube proxy and cube ctl proxy i know when i was learning uh kubernetes getting mixed up because i thought maybe this proxy and this proxy were the same thing and they absolutely weren't because they weren't prefacing it with cube ctl but just understand there's a lot of different proxies and just to give more general information about proxies there's the concept of a forward and a reverse proxy so ford proxy which is the default proxy when we just say the word proxy is a bunch of servers egressing traffic to uh have to pass through the proxy first so imagine i'm going to just draw a little boxes here you have three servers and in front of it there is a proxy and the idea is that they're going to all go through this proxy all right and that kind of acts as a way of filtering or doing things with their requests then you have a reverse proxy so this is ingress traffic trying to reach a collection of servers and this is what we will see quite often especially with web applications is it is that you have let's say three versions of your web app running and in front of it you have a proxy and the proxy is going to send the traffic to those and in a sense you basically have load balancing so hopefully that makes it very clear uh and there you go hey this is andrew brown from exam pro we are looking at kubernetes proxy so kubernetes proxy also known as just as q proxy is a network proxy that runs on each node in your cluster and it's designed to load balance uh notice i have the word load bounce there traffic to pods so down below this is the diagram from the docs i probably should have made one here but i also kind of wanted to show it to you because often they have permutations of this or it can be really confusing i just want to kind of clarify it up that you have q proxy here and uh you have a cluster ip and you have the ip tables but they'll put it like they always put iptable somewhere else but where it goes is it's a program running on the virtual machine the node so it's really over here okay and the idea is that cluster ip and q proxy can be both using it all right um and so that to me is a lot more clear because that is the thing that is driving the rules to say okay how do we load balance how we how do we filter traffic and stuff like that and these things leverage it and this is a program running on the node so cube proxy maintains network rules so that's going to be in this case iptables these network rules allow network communication to your pods for network sessions inside or outside of your cluster q proxy uses the operating system packet packet filter laying if there is one and it's available otherwise q proxy forwards the traffic itself q proxy runs in three modes we've got iptables which is the default uh suited for most use cases ipvs probably going to be the future default because it's so good and suited for thousands of plus services but it's fine for simple use cases as well and then user space this is legacy and not recommended for use so hopefully that gives you an idea of q proxy it is installed on all nodes not just the worker nodes because you need a proxy to move that traffic around but um you know hopefully that gives you an idea there and there you go hey this is andrew brown from exam pro and we are taking a look at container networking interfaces cni so cni is a specification or open standard for writing plugins to configure networking interfaces for linux containers and so down below here uh imagine this is the uh container networking interface which again is a specification for writing plugins and then there are plugins there are builtin plugins that ship with it and there are thirdparty plugins i would imagine i would imagine this is where we say entry and out of tree uh but maybe not um but you know that's the way i'm thinking about it and so you might recognize some things here like bridge and flannel and then on the side we have calico cilium and weave which are all very popular thirdparty uh things to install uh and definitely you'll need these you'll need one of these in order to do network policies okay so this is just the interface that um is used so that containers can leverage these uh plugins right it goes to the container network interface just a standard way to do it so it will run through the container run time to reach the containers uh and so that's the idea is like if you want to interact with calico right it's going to have to go through this path right same thing with any of these things like if it's a bridge it's going to be going through here so hopefully that helps make sense okay hey this is andrew brown from exam pro and we are talking about service meshes so a service mesh manages service to service communication for microservice architectures a service meaning an application not a kubernetes service component so this is where um you know i said how service is a confusing name this is where we get that confusion so a service mesh is an infrastructure layer that can provide the following reliability traffic management retries load balancing observability some metrics and traces security so tls certifications and identity uh and so the idea is that you'd have a service control uh service mesh control plane so the installation of the proxies into pods as well as the proxies capabilities is managed by the service mesh control plane and a service mesh uses a side card pattern carefully listen to this because this is super important and will probably show up on the exam but a side card pattern basically is a proxy container that is installed on each pod and the apps container must pass the proxy before leaving the egressing pod so the idea is that you when you install a service mesh this proxy or sidecar as it will as it will say will will sit beside it and so the idea is the container will pass through it and the proxy can do all sorts of stuff all these things here above there is a lot of different service mesh implementations you don't have to run a service mesh but in most production use cases there's no reason not to and i forgot the e there maybe it turned into a hyphen for some reason um but available service messages for communities are istio so this is currently the most popular service mesh but some people might debate me here for kubernetes due to its highly configurable nature seo uses envoy as its proxy istio is not a cnn safe project so when we talk about the sidecar um you know like like there's the control plane and then there's the actual proxy the sidecar that you install and so seo says it uses envoy for that istio is not a cncf project but envoy is and so envoy is an open source edge and service proxy multiple service meshes use envoy as it's a proxy envoy is a graduated cncf project you have kuma this is also a cncf project but it's in sandbox mode that uses envoys at proxy i'm not sure if it's competing with istio since it's not a cncf project sometimes that's what you see you see projects that are competing like they're not for whatever reason they're not a cncf project and so then a csv project spins up to kind of compete with it or fill some kind of edge cases and it can get kind of confusing because there's some sometimes a lot of overlap on certain stuff you have linker d which is actually what we use or we attempt to use in the fall longs it's a graduated project known for having strong security and it just works liquor d does not use envoy instead it uses a simple ultra ultra lightweight microproxy called linker d to proxy and then if we're talking about like a thirdparty service like uh hashicorp console so console's an open source service mesh by hashicorp it's not a cncf project console is offered by hashicorp as a managed cloud service mesh so there you go all right let's take a closer look at envoy which is a selfcontained process that is designed to run alongside every application server envoy can be installed on a virtual machine or as a container so you don't have to have a control plane with envoy but that's generally what you'd want so you'd want something like sd or something to communicate or gather the information that passes along through your sidecar which is the envoy proxy and voice supports a wide range of functionality so l3 l4 filter arctic architecture l7 filter architecture so l7 is the application layer 3 and 4 is like packets like tcp udp things like that first class http 2 support and http 3 http 3 support uh it has layer 7 so that's the application layer routing grpc support service discovery dynamic configuration health checking advanced load balancing front edge proxy support best in class observability and in practice you will likely not install and manage to configure envoy you would allow a service mesh control plane to install into your pods a service mesh will or may cont come with a ui configuration files to configure your envoy so there you go all right let's talk about network address translation not to be confused with nats which is a event bus service that is used with microservices and grpc but is not the same thing so what is network address translation well it's a method of mapping an ip address space into another by modifying network address information in the ip header of packets while they're in transit across a traffic writing device okay but what does that mean well it's practical use case is imagine that you have a server it's a public server right publicly accessible via the internet and you need to communicate uh with a private network and so we have two different private networks private network one private network two and uh they have their own address spaces so this is in the 10.0.0.0 and this one also has it right so if these are the same and they were to talk to each other that wouldn't make any sense right because they have overlapping addresses so a nat can translate one in another by mapping or translating uh there's this is like a nat table that will um uh store uh like it will say like okay 100.1 goes over here but we will then give it a new ip address and we'll remember that so that if anybody talks to us we'll know to send things back through that path okay because a uh two private two private networks that have conflicting address spaces are gonna have an issue there and um when you send a packet over here if you send 10.0.0.1 to the server and it sends it back it has no idea what 10.0.01 is so it really needs to be mapped to something else and so that is the idea behind network address translation okay let's talk about ethernet 0 and network namespace so first we need to know what an ethernet device is so an ethernet device is a software or hardware technology that allows a server to communicate on a computer network so a network interface card a nic are commonly used to establish a wired connection to a network it's the thing you plug the internet into a cloud service provider such as aws or azure gcp have virtual nics for your virtual machines uh and you can actually manage them too a lot of times they're created for you to connect to your virtual network so what is a network namespace well ethernet 0 represents the first ethernet interface an ethernet device attached to your virtual machine and a network interface is an abstraction on top of the ethernet interface to provide a logical networking stack with its own routes firewalls rules and network devices linux by default has one network namespace called root and network namespace and this is what programs we'll use by default so the idea is you have your linux virtual machine you have your root network space and then you have uh ethernet 0 so the these the first there's ethernet zero and there's unit one but one is the second device if you have one but either zero is primarily how things are getting outside that virtual machine okay so now the question is how would you observe these things uh so if you were to log into your virtual machine and i actually did this on aws what you can do is you can observe uh the type of devices that are attached to your virtual machine by typing in ifconfig i don't know what ifconfig stands for but i know that it'll list out things like zero so there that's the way that you can see it and there's other things so i guess down below we have lo i can't remember what that stands for but you might see other things like bridge as well so all sorts of networking devices and you can create and modify your own network namespace so we said there's a root name space but you can create your own um here so are we showing yeah so here this goes pseudoip net ns which stands for network namespace add namespace one so i created a custom one there you type in ipnetness and there is that namespace so there you go hey this is andrew brown from exam pro and we're onto cluster networking which is an extremely important section for kubernetes because if you don't understand the networking for it um it can be really hard to use and it will kind of show up on the exam indirectly not directly but you will have questions which should be like hey if you talk this way to that thing what can talk um and this is where i say as a prerequisite why you need to know linux and why you need to know cloud networking or sorry linux networking because this stuff is tricky but kubernetes has the following opinions about cluster networking all pods can communicate with all other pods without using a nap network address translation all nodes can communicate with all pods without using a nat the ip that a pod sees itself as is the same ip as it sees it as and even though i'm saying uh you know without using a nat without using a nap gnats are and can be used in kubernetes even though the bub is a contradiction there are four broad types of network communication for clusters we have container to container pod to pod pod to service external to service and um beyond what i'll show you if you really want to go deep on this stuff there's like a long form medium article but there's also a really good talk and it's a cncf tech talk called a life of a packet by michael rubin for uh so if you want very detailed cluster networking knowledge uh if the stuff is just not clicking or you just want to get more that's on youtube and i strongly recommend watching that as an additional thing if it interests you all right let's take a look here at container to container networking so containers all in the same pod have the same ip address and the port space and containers can communicate with each other via the local host via different ports so to help this make a little bit more sense i have a visualization here so here we have a worker node and pods run in worker nodes or nodes in general and here you can see that we have an ip address and so that is a dynamic ip address meaning that if this pod is destroyed and it spins up a new one then there will be a new ip address and so down below we have multiple containers here and all these containers are communicating on local host so if you log into this container here and you were to local host on port 3000 you would reach this container uh and that's kind of the story there so it's not too complicated in terms of the configuration so here is a deployment file and in the pod spec part of it the spec part uh if we go down below to the template and then into the containers here you can see we are setting the container port so see where it says port 80 here that would pretty much be the same thing where we are setting it here on the container in the graphic obviously not the same port number but that's the idea there so hopefully that is pretty clear pods can all the containers in a pod communicate on a local host and you just access them via the ports all right before we go any further into more cluster networking we need to understand what is a virtual ethernet device also known as vet so these devices they can act as a tunnel between network name spaces to create a bridge to a physical network device in another name space but can also be used as a standload network device uh packets on one device in the pair are immediately received on the other device so here is our visual we have two different network name spaces notice where it says net ns that's just kind of like the short form uh you could write to say network namespace but of course you can name your network namespaces whatever you want and so that devices are always created in interconnected pairs so i know it's a bit gray i don't know why i made it so dark or so light there but it says veth pear and the idea is that these um two parts of it uh include the pear and i'm just going to wipe a little bit away here uh so you can use the iplink command to create beth pairs so here you type in iplink add a net ns uh and in practicality of course you're never doing this but i just wanted to show you uh in case you had never you were just wondering how those links are established in linux so there you go let's take a look here at pod to pod communication because we just saw how simple containers can communicate with other containers within a pod just on the local host pod pod's a bit different and also it can be a little bit more tricky because you have pods that can be on the same note or you can have pods that are across nodes on different nodes and so it's kind of the same but a little bit different um so i have this big fancy diagram that we'll use as a reference as we talk through our information here so for pod depod communication on the same node virtual ethernet devices is used to communicate from the pod network namespace to the root network namespace that's why we were talking about uh virtual ethernet devices to connect two namespaces together notes and named it here like ef0 ef1 it's still virtual it's just that you can name them whatever you want and so some people just do that to simplify it where they'll just call this one uh f0 and that's zero here but this could have been one and two we could name them whatever we want um but i mean this is done for you right so you don't you don't set this up manually it's just happening but i'm just talking about if you were to set up a vet pair yourself you can name whatever you want um so in the root network namespace a bridge is used to allow all pod network namespaces to talk to other pods so that's a networking component that is set up for you here the br0 represents the bridge pods can see all of their pods and communicate using their ip addresses so notice that they both have two distinct ip addresses in the same network namespace or address namespace and so if you were logged into a container here and you were to ping this ip address you would reach that pod and you could then also hit the port number and hit a very specific container if you wanted to i just want to distinguish between uh routing and bridging because a lot of people might be thinking you know isn't a bridge just a router but they're not so routing allows multiple networks to communicate independently and yet remain separate using a router so bridging connects two separate networks as if they were a single network using a bridge so just under standard bridge is a bit simpler and treats everything on the same network which is what it does right because they have the same address space at the top there at the 10.0.0 idea there now let's take a look at across node so here's our diagram looks pretty similar and just so you know there is a thing that's missing here that goes in between and i just don't have it there because we're going to be talking about it uh when we look at services so just be aware that this is an incomplete diagram on purpose but so pods can communicate to other pods running on other worker nodes how pods can communicate uh pods on other nodes is network specific okay okay so it's gonna be specific to the scenario and will vary based on your provider so in the case of aws they have their own implementation of the container networking interface cni and it's called amazon vpc container network interface plugin for kubernetes which allows you uh to have no intimidated communication uh via the aws virtual private cloud so if you've used aws which is a virtual private cloud basically it's gonna communicate on there so your vpc like it's pretty straightforward right so the idea is like i can't really show you that in granular detail because we'd be really digging into aws specifics or any cloud service specifics but the point that you need to know is that every provider has some kind of solution and when you set up on a managed provider it's going to already be installed there and so it's just going to work and you're going to have to do the research uh for each of those but it's always probably going to be happening with uh the container networking interface it's always going to be some kind of plugin for that okay all right let's take a look at pod to service networking so here's our diagram looks very similar but the the key difference here is we have this element called iptables and this is being uh linked via a service now the way we can represent this um as an architectural diagram varies and you're gonna see in the next slide when we're talking about ingress egress that i've done it slightly different and it's because i looked at so many architectural diagrams and there's just no consistency so i figured i'd do some variation here just so that you can conceptually still understand what is going on and the placement of things so when a pod dies its ip address changes and this can make communication hard if you're uh relaying relying on the ip address for communication right so if uh you know this pod dies the ip address is gonna change right so a service creates a virtualized ip a static ip so over here and then uses iptables so iptables is over here which is installed on the node right so it lives on the node not in a pod to do nat so network address translation and load balancing to other pods and that's pretty much all there really is to it um so you know that's that's all there is okay all right let's take a look here at ingress egress from an internet cluster and here uh we are reaching the pinnacle of very complex diagrams and notice i told you that um the uh the representation of service is slightly different here so notice that service is having a dotted line that goes around iptables now i didn't double check but i'm not sure this could be a fault on my diagram but i'm not sure if the service lives in the control plane or the worker node it could live on the worker node i just didn't double check to be honest um but anyway what is clear and the reason i put it over here is because it has to talk to the um the cloud controller manager so to me that made a little bit more sense but i did tell you that it represents service slightly different uh like from the prior diagram here so let's talk about egress so egress is how uh pod traffic exit to the internet so the idea is you have a pod up here a container and it's flowing out this way it's leaving the pod and even though i don't have the diagram what it would do is it would go through the container networking interface um and the container networking interface it uh it can we have a diagram with cubelet and so cubelet shows that um it talks to the container runtime and so the idea is that you would have uh the the plugin so in aws's case it's their amazon vpc container networking interface so that would let you talk to uh vpc and from vpc you could then go talk to aws's internet gateway and that's how you'd get out to the internet and so you know i'm just running out of space on this diagram and that's why i didn't kind of do that variation for egress for ingress and i swear i i remembered it to put a double s on there but i guess i forgot but it's two s's and i digress that i made a mistake smelly ingress it's for traffic to reach a pod so it's going in this way okay and so the idea is it has to travel to a service so from a service it could be using a kubernetes service with a type load balancer this will work with a cloud controller manager to implement a solution that works with a t4 load balancer something that does udp tcp um but the other option is ingress kubernetes ingress and so it will use an ingress controller to work with a cloud service provider load bouncer i believe it's specifically for um t7 because any time i use ingress it's always using um like kubernetes it's using the application load bouncer can use it with a t4 i'm not sure that might be inaccurate so i would probably say it's just c7 but um you know uh the key thing is is that you can either use one or the other okay um but you know hopefully that makes it clear this stuff is probably the hardest stuff to understand in kubernetes they're not going to push you too hard on the exam to know this stuff for the kcna uh there's a lot of videos out there that go into super technical detail but honestly it's very hard to remember it all and you'll be okay if you just generally understand that for ingress we have a type load balancer of the service or kubernetes ingress and then for egress it's going to use the container network interface plugin to uh get to access stuff and they might probably won't even ask that the egress one but there you go hey this is andrew brown from exam pro and we are looking at the four c's of cloud native security which is cloud clusters containers and code and that order does matter because each layer of cloud native security models builds upon the next outmost layer and so we could describe uh this generally just as depth and defense so you know the four c's is just kind of a um a framework or a mental model to think about cloud native security but it just really is the general idea of depth of defense which you might see if you're looking at the security of data centers but it is a series of defensive mechanisms that are layered in order to protect valuable data and information so the idea is that you have a malicious actor and they're trying to make it to your data so what layers are they going to be going through so first we have the cloud layer so that would be something like aws you have your cluster layer like something like kubernetes mesos docker swarm containers so maybe your runtime is container d or you could just say docker in that sense uh and then your your code and and there there could be your code and your data or information i probably should have made a nice little like uh data file there to indicate what they were looking for but i didn't but yeah those are the four layers and you absolutely need to know the order now how much do you need to know about the interior or how they actually like what uh defensive mechanisms right that you need to implement which would be here at the border of uh passing into it um not too sure for kcna i didn't see too much stuff but we are going to walk through each layer and look at what it takes to secure them so that we just round out our security knowledge for kubernetes and cloud native workloads okay all right let's take a look at the first layer which is the cloud layer and this is also known as the base layer because it's the basis for all the other layers you have to pass through the cloud layer before you can go to any of the other layers so security at this layer is going to vary based on managed or selfmanaged infrastructure so the idea here is we have the managed infrastructure so infrastructure as a service as we would know from our cloud service provider knowledge or selfmanaged so for managed infrastructure you can think of any cloud service provider bus azure gcp oracle oracle ibm cloud or a cloud platform so think something like civo and so the main difference between a csp and a cloud platform is a cloud platform it's kind of like a precursor to a cloud service uh provider same thing as like a virtual private server it's just that um they might not have all the elements of it maybe they don't have a lot of services um or they're just a highly focused on just a very specific thing just like just giving you kubernetes and just a load balancer and things like that so security is going to greatly vary based on the provider and you'll have to do independent research to determine what you'll need to do so if you're on aws and you're deploying to eks which is what you'd use elastic kubernetes service you're going to have to research what the best security practices for eks because when you're using managed infrastructure the responsibility infrastructure is the cloud provider so um maybe all of it is taking care of you and but there might be some things you still have to worry about but let's take a look at selfmanaged so selfmanaged would be um code located servers so think of equinix i don't know if that's how you pronounce it properly i've never said it out loud before or corporate data centers so basically you have the machines you have the office space or you're renting out an office space and you're directly working with the machines and you don't you don't have a cloud layer or you're installing your own cloud layer onto um those uh those servers so security is going to be based on the infrastructure security such as network access to api servers the control plane network access to nodes kubernetes access to cloud provider apis access to fcd lcd encryption um and so you know hopefully that gives you kind of an idea of infrastructure security there of course if you can go with managed it's going to be a lot easier but kubernetes is flexible so you can use it anywhere you want it just depends on how much responsibility you want to have all right let's take a look here at the cluster layer and there are two parts of cluster layer security you have components of the cluster and components in the cluster and remember a cluster could be um and i mean we're talking about kubernetes in particular so you think they'd just say this is uh kubernetes uh security but um no matter if you're using kubernetes mesos or a docker swarm which are orchestration tools that set up clusters um this stuff kind of applies but everything in the context here is kubernetes so for components of the cluster we're securing configurable cluster components for components in the cluster we're securing applications running within the cluster so one is focused on the components and one is focused on the applications so on the component side there's a lot of things you can do so let's go through this list and it is huge but we will uh do it anyway so we have controlling access to cube api using tls for api traffic api authentication api authorization controlling access to cubelet controlling capabilities of a workload or user runtime limiting resources usage on a cluster controlling what privileges containers run with preventing containers from loading unwanted kernel modules restricting network access restricting cloud metadata api access controlling which nodes pods may access protecting cluster components from compromise restrict access to fcd enable audit logging restrict access to alpha or beta features rotate infrastructure credentials frequently review thirdparty integrations before enabling them encrypt secrets at rest receiving alerts for uh security updates and reporting vulnerabilities so that is a big old list but it gives you an idea that there's a lot of work on the cluster side and i would think and i don't know but i would think that using something like knative which uh basically takes a strong opinion on how to set everything up for you might reduce some of that burden um but i mean this is what you get when you use kubernetes you have uh full access of all the components underneath so you have more of an obligation to secure those let's talk about in the cluster securing the applications running within the cluster so rbac authorization authentication application secrets management ensuring the pods meet the pod security standards quality of assurance network policies tls for kubernetes ingress so you can see there's a huge difference of responsibilities here between of the cluster and in the cluster all right let's take a look at the container encode layer now these are separate layers but there wasn't a lot for me to say about each one without going super deep on particular ways of implementing them so i just group them together here so we have container layer and the code layers the container layer things that we can do at this layer to protect the container would be container vulnerability scanning or os dependency securities image signing and enforcement disallowing privileged users using the container runtime with stronger isolation okay and that could be choosing like virtualized container runtimes which we talked about in our runtime section right for the code layer the application code is one of the primary tax services over which you have the most control so um i mean you know you have a lot of options here but we don't list a whole lot here so access over tls only so make sure you use https for your application limiting port ranges of communication so like if it's a database maybe don't make it public facing or limit it to only your ip address thirdparty dependency security so like if you're installing like node.js modules make sure that they are up to date and safe or use a tool like sneak that will detect and tell you whether there's a problem with it or not static code analysis which you know is kind of similar to third party it actually looks it's not just saying like hey we know that there's a history of problems with these things but we'll actually look at the code and tell you if we find a problem i think like it was devops guru does that but you have to use particular languages like python or java dynamic probing attacks so you know pen testing basically for your application um you know and then for the code layer you know you follow all the good wasp rules so aus has like a top 10 for application defense it's going to be the same here but there you go all right let's talk about infrastructure security we actually did kind of cover this in a brief earlier when we were talking about managed cloud service providers because they take care of all the stuff for you but these are the suggestions for securing infrastructure in a kubernetes cluster the first is network access to the api server which is your control plane so all access to the kubernetes control plane is not allowed publicly on the internet and is controlled by network access control lists restricted to the set ip addresses needed to administer the cluster so pretty clear don't make it publicly available network access to nodes nodes should be configured to only accept connections from the control plane on the specified ports and accept connections for services and kubernetes to uh of a type node port or load balancer and you know just put a load balancer in front of that and you should be aokay like you know like a cloud one or something like that kubernetes access to a cloud provider api so each cloud provider grants a different set of permissions so now we're talking about cloud providers to create control planning nodes it's best uh to provide the cluster with cloud provider access that falls the principle of lease privilege so polp that's the initialism for that for the resources it needs to administer so this is just like imagine you're on aws and you make a a role just make sure that role is given to somebody that should have it or you know just limit that kind of stuff there access to fcd so access that cd the database store for grace should be limited to control plane only depending on your configuration you should attempt to use xd over tls more information can be found in the std documentation xcd encryption so whenever possible it's a good practice to encrypt all storage at rest and send lcd holds the state of the entire cluster including secrets it disks should especially be encrypted at rest again if you're using cloud service providers they'll usually integrate with the provider so like on aws it would use kms for azure and use um vault store and sometimes it's like just a check box during the setup so you know it makes that things a lot easier for you so there you go all right let's talk about the concept of the three a's authentication authorization accounting framework for identity management systems so it's just a a security concept that is good to know um and so let's talk about the first one authentication this uh deals with your identity so this is how do we know who you are and so this could be utilizing a static password you enter password and therefore must be you a onetime password so we send you um maybe we do the password but we also send you a onetime password to your phone you enter it in to make sure double check that you are who you say you are uh we know this is multifactor authentication ufa digital certificates would be another way so remember the identity is in the certificate um and so if it's selfsigned i don't know if we can trust it but if it is signed by some kind of certificate authority then they say yes we attest to this person is we are and we because we issued it to them right basic auth could be something um like trying to get access to um an application so we apply basic auth there as well we have authorization so to get permission so we know who you are but what do you get to access and in kubernetes this is going to be role based access controls rbac for other things like azure um you know they and it's still rolebased access controls but they might have risk adaptive based policies so they might factor in a bunch of smart information about your location the time of day the device things like that to determine what you get access to but for kubernetes it's just rbac okay then we have accounting or auditing to audit to login audit trails so in kubernetes there's something called audit policy so i suppose that's just like uh i can't remember but they have other policies and they have audit back ends so that's where the logs will be stored um so you know it's not super important for the exam but the concept of the aaa is something that uh you should know uh in terms of security because sometimes they mention it especially in the kubernetes space so i just wanted to make sure you knew the three a's okay hey this is andrew brown from exam pro and we're taking a look at rolebased access controls also known as rbac or some people like to say are back and and rolebased access controls is not a concept just limited to kubernetes but it is something that kubernetes can do and is very popular among cloud service providers it is a way of defining permissions for identities based on a organizational role so rbac authorizations use the rbac authorizations k8 io api group to drive authorization decisions allowing you to dynamically configure policies through the kubernetes api and when i show you things like this it's because i want you to remember them just in case you see them on the exam when i point out these random things but i don't always describe them in detail because i just don't think it matters um but to enable rbac start the api server with the authorization mode flag so if you're starting one up you'd use cube api um server i can remember what the one like in the ckd or ck that use ck they do like cube add adm or whatever to start up clusters but you do have to specify the mode if you are using a lightweight distribution like micro k8 you can just enable it by adding the addon rbac and it basically be doing this underneath if you're using managed services like aws azure tcp it might be turned on by default you might just have to hit checkbox because you're not the one starting up the cluster but you need to know this it might show up on the exam so that's why i'm showing to you not that we would ever have to provision our own server in the kcna but you might want to need to know that flag so with kubernetes rbac there are only allow rules and everything is denied by default so if you were to create a new user they would have access to nothing you'd have to give them a role with permissions to be able to do something okay all right so the rbac api declares four kinds of kubernetes objects we have roles role binding cluster role cluster role binding so here is the diagram uh that we see here and so we have roles which is a set of permissions for a particular namespace and then cluster role which is a set of permission across all namespaces there are some kubernetes components that cannot be named space so i would imagine that if you had to give them permissions they'd have to be in cluster rule or there are um components that can be uh in more than one name system at the same time so again maybe that's where cluster rule might play a part so that's the reason for the two different ones in order for um roles or cluster roles to be bound to a subject that's where you're going to need um a binding and so for a role you have a role binding for a cluster role you have a cluster role binding and so those are going to attach to subjects which are just essentially identities like a user account which would be a single user a service account which represents a machine user to be used by an application service a group which is a group of users or services service accounts and just notice here that for both we do have this this binding in between each one and the thing is is that when you're using cloud service providers or other things with rbac you usually don't have to worry about that component but in the world of kubernetes where everything is decoupled and you can see all the components underneath um you know we have to manage that component so it's not too hard to use and just to kind of tell you a little bit about machine user just because this one that's a service kind of threw me off but then when i realized it was a machine user it made sense but if you don't even know what a machine user is um that's like let's say you are on github and or like you have a project that you want to deploy to a server but you need to have access to github so you go and you can use your main uh github account but that might have access to all your repositories and that's just too much permission so what you do is you create a new user with the intent to be just to be used for deployment so it only has access to the repo and permissions to read and things like that and so that's the idea behind a machine user it's for the usage of service it's not tied to a specific person okay all right let's take a look at the role configuration example and so i mean there is the role and cluster role but they're very similar but the key thing is that uh the role sets a namespace so here it says namespace default where with a cluster role you're not worried about namespaces so here you can change the kind to role or cluster role then you have your rules or permissions of actions this role is allowed to perform so we have api groups and we do this in the cube ctl extras where i list out the api resources which is a command that you definitely will need to know for the exam that shows you all the possible groups so if you just leave it blank there like that then it's going to consider all core api groups um and i imagine that you can do a comma and it's just say the exact groups that you want then the resources of that group and then you have verbs so get watch list things would be other things like patch delete update create things like that but the exam doesn't really test you on that kind of stuff but this is just practical knowledge you should know of course you would have to create a rule binding we do all that stuff in the follow along so even though we're not seeing a lot here we will do a practical example but we're not going to do a cluster rule just because it's a lot of work and outside the scope of the kcna but this gives you kind of an idea of what you'd have to do hey this is andrew brown from exam pro and we are looking at secrets management so a secret is similar to a config map with the exception that they can be encrypted so here's an example um i mean it's very simple but just imagine instead of having config map now we have secret um and the configuration is pretty similar but what you're going to have is a type of secret so this one says basic auth and then the idea is we have string data so based on the type is going to be uh different on what you ingest here um so basic basic auth is a way of gaining access to a website so and it will prompt you with a very ugly box saying enter your username and password so it makes sense that that's what that does there so by default secrets are unencrypted in scd school uh store and anyone with access to the fcd store has access to the secrets so anyone who has access to pods within namespaces uh will have access to the secrets used by that pods so it's very important to understand by default they're unencrypted okay and you have to do a bit of work to make them encrypted or make sure that they are secured and so those steps would be to enable encryption at rest for secrets enable or configure rbac rules that restrict reading data in secrets use mechanisms such as rbac to limit which principles are allowed to create new secrets or replace existing ones and you know for the exam you just need to know what a secret is i just thought it was important that you understand that they're unencrypted um and they're pretty much just like configmap but just slightly different and they have a bunch of different types okay hey this is andrew brown from exam pro and we are looking at network policies so network policies acts as virtual firewalls for pod communication and pod communication can be restricted based on the following scopes pod to pod name spaces or specific ips so just to kind of visualize it if you have a network policy the idea is that you can control the ingress and you can control uh the egress um is that the right thing is that sorry you know what one second here i think the thing is i mean like they both are pointing ingress actually so this these are both going in right but you can also say what's going out and so i guess this graphic could have been a little bit better but you get the idea there's ingress and there are egress but we actually do set up a network policy so you'll see in the follow along now selectors are used to determine to select resources with matching labels for the network policies to be applied to except with the exception of specific ips um and the network plugin you are using must support network policies so if you don't have a network plugin installed like calco weave net psyllium i have no idea if that's how you say it um it's just not going to work right so you can create a network policy but if you test it out without having one uh a plugin installed it just won't do anything it'll be like the policy doesn't exist and we do actually cover that edge case in the fall long so you can see uh but calico and weave net are pretty popular uh but there you go hey this is andrew brown from exam pro and we are taking a look at calico so calico is an open source network and network security solution for containers vms native host space workloads and something we absolutely need if we're going to be working with network policies so calco supports a broad range of platforms including kubernetes openshift mirantis kubernetes engine no idea what that is but sounds cool open stack bare metal services and calico gives you a choice of data planes so you can use berkeley packets that's what ebpfs are standard linux hns on windows so there's a few different data planes for that and data planes is just like the mo the means of communication okay uh calico network policies extend the base functionality of network policies so i say in the network policy section that if we do not install something uh our network policies will either work or do very little okay so um calculate network policies um do a few things it will make policies that could be applied to any object uh the rules it's not he it's just the the t is missing there so the rules can contain um uh the specific action you can use ports port ranges protocols ip subnets selectors on the rules you can control traffic flows via dnat um i think that's destination network address translation i think that's the ds4 settings and policies for traffic forwarding so really without it you're not gonna like without it you're just not gonna have good network policies uh calico can perform better than alternatives like flannel uh there should be comma here psyllium and weave nets so because i was looking up like what what does calico do you know what i mean like why would you use it over other ones and basically it's just performance uh like in terms of resources it's using and and its flavor of network policies that's the major reason why and uh calico is what we use uh in the course okay all right let's take a look at the autonomy of a network policy file so these are the files that we would create um because once you install calico and we're using them um and i think that when you install different ones it might change the syntax slightly of this but i'm not 100 sure on that um but anyway so the idea is i'm just trying to get my pen tool out here um is that you know in this network policy you're either going to specify a pod selector or a namespace selector because network policies are either scoped on the namespace or the pod or you're selecting very specific ip addresses and there's two policy types ingress and egress and you can have them both in the same file so ingress defines traffic that's permitted to enter the pod and then you have uh traffic that egress that can exit the pod and you even say like ingress is allowed specifically from this namespace specifically from these pods and block these ip addresses right and the traffic has to be on this port okay so that kind of gives you an idea of it but we are going to write a network policy so uh we'll get hands on there and that's where we will really learn what we're doing okay all right let's talk about in transit uh versus at rest encryption so in transit is data that is secure when moving between locations like algorithms tls and slssl and then encryption at rest is data that is secure when residing on storage within a database or even something like block store like a volume so you see algorithms like aes ras and so uh two protocols or algorithms we should really talk about is tls and ssl because they are very popular and they can be easy to mix up so tls is an encryption protocol for data integrity between two or more communication computer applications so tls 1.0 and 1.1 are deprecated and i use this wrong a lot of times but i'm pretty sure it's deprecated and so now the current practices uh 1.2 or 1.3 is the current best practice but at one point it was ssl you know so um you know an encryption protocol for data integrity between two or more communications uh computer application like 1.0 two and three so it goes back and forth um i think we're on tls now but you'll still see us talk about ssl as if we're using it but probably we're using tls um you know in the context of kubernetes when you are communicating uh traffic inbounds you want to be using https which would underneath be using that encryption transit like tls and then your volumes which are going to be on uh most likely managed providers like elastic block store or azure disk and you're going to be using whatever encryption method they use for that but that generally gives you the idea okay hey this is andrew brown from exam pro and we're looking at certificates in tls so kubernetes provides an api called certificate kubernetes io which lets you provision tls certificates signed by certificate authorities cas that you control the ucas and certificates can be used by your workloads to establish trust uh and even though like in my 15year career i just never can remember what these things are like i know what they are but i just mean like the specification and all the settings underneath so if you find them like a little bit uh daunting or confusing just understand that i've been doing for a long time and even i don't remember so what is public key infrastructure so pki so pka is a set of rules policies hardware software procedures needed to create manage distribute use store and revoke digital certificates and manage public key encryption so it sounds like a very fancy system for managing public keys what is x 509 certificate because we hear this a lot in the web it's a standard defined by the international telecommission uh union itu for public key certifications if you've ever had to open up like an ssl certificate i believe it follows the 509 standard here so these certificates are used in many internet protocols so as i said ssl tls and https that's where you most commonly see it or the pro probably the first time you ever saw it if you've been in the uh in the cloud or web development for quite a few years signed and encrypted emails code signing or document signing so like when you submit code to your repo you can sign it to be like yes this code came for me um a certificate contains an identity like a hostname organizational individual so like who is the certificate we're signing on the behalf of a public key so these are algorithms like rsa dsa ecda and you can use a variety of different ones so kubernetes requires pkp pki for the following operations client or client certificates for the cubelet to authenticate to the api server server certificate for the api server endpoint client certificates for administrators for the cluster to authenticate to the api server client certificates for the api server to talk to cubelets client certificates for the api server to talk to xcd client certificates cubeconfig for the controller manager to talk to the api server it's always talking to somebody here client certificate cube config for the schedule is to talk to the api server client and server certificates for front proxies most of these are going to be stored in etc for close communities pki it's going to vary for lightweight distributions like micro kubernetes we'll place them somewhere else minicube will place them somewhere else and when you create a cluster generally it creates one for you like a key but there's actually a bunch of keys for a bunch of different things we will come across this when we need to um do our back roll uh roll base access controls because we will want to have a user and we'll have to generate a selfsigning key like we are the authority for it and we'll have to sign it against keys from the cluster and so that's going to be our exposure to this stuff and i always find it kind of hard but you know what we do have to learn it and it's not too bad okay hey this is andrew brown from exam pro and we are looking at kubernetes security best practices and i got these from aqua security so it's just a a summary of what aqua has on one of their websites and if you don't know aqua security they are a really wellknown security company that has products and services um and so the recommendations here is enable kubernetes rolebased access controls uh because as soon as they're enabled that means that all users by default do not have access and then you have to give them permission and that's a great idea use third party authentication for api server so um authentication could be octa maybe cognito any kind of decentralized authentication system azure active directory again i don't know how those would integrate but using a third party authentication is just generally a good idea protect xcd with tls firewall and encryption so that sounds like a good idea i thought tls was just on by default so i'm not exactly sure um about that but uh i mean it sounds like a good suggestion it did come up when we were talking about infrastructure security in the or uh cloud cloud layer of security and infrastructure security so isolate kubernetes nodes so maybe isolation or sorry nodes so yeah uh i mean if you want to isolate workloads that's using namespaces but i suppose isolating nodes is a different story monitor network traffic to limit communication so maybe that's installing a service mesh uh just ha like holding on to kubernetes logs i'm not sure uh there but i mean monitoring is always a good idea use process white listing so you know if you have a machine like a linux machine and it's running particular processes only allow certain processes and and everything else should be denied turn on audit logging sounds like a good idea logging the idea is that if you have logs make sure that no one is tampering with your logs keep kubernetes versions up to date with managed kubernetes providers a lot of them will do that for you so that's one advantage of using something like google cloud's google kubernetes engine and we have a lockdown cubelet so you know some of this advice we saw in the layers section but it's generally good advice here i just wanted to share it again hey this is andrew brown from exam pro and we are looking at auto scaling so what is auto scaling well in computing auto scaling is when systems without manual intervention adjust capacity the amount of cpu and ram to meet the demand uh traffic from users by adding or removing resources commonly triggered by events uh so for pod based scaling uh we have two things we have the horizontal pod scaling so that's the horizontal pod auto scaler which is what we'll create when we create a manifest file which adds more pods to meet the demands and then we have vertical pod scaling so the vertical pod auto scaler which right size pods for optimal cpu and memory resources then there is node base or cluster based scaling depending on the way you want to call it and so this is where we have the cluster auto scaling and so its implementation because this is more reliant on um uh managed providers or things like that the solution is uh uh kind of like thirdparty or integrated so we have cluster auto scaler and then we have carpenter and this will add or remove nodes uh based on the demand and then there's the cluster api so this is a declarative api and tooling to simplify provisioning upgrading and operating multiple kubernetes clusters the cluster api can be extended to support any infrastructure and bootstrap or control plane providers so i don't know for sure if cluster autoscaler is using cluster api i think it is i just can't remember um but you know my guess would be you know maybe carpenter and auto scaler leverages cluster api but these are all kind of components that are involved here for the case dna uh we're not going to even uh notice cholesterol is going this is really complicated and vertical pod scaling no but horizontal plot autoscaler is easy to learn and use and so we will do that one okay hey this is andrew brown from exam pro and we are looking at scale versus auto scale so the scale command is used to update the amount of replicas in the state of a deployment object and perform a deploy so here we have a scale we specify the amount of replicas and what that will do is just update the amount of replicas in the state of the deployment object and perform a deploy it's almost like opening up the manifest file of your deployment file just changing the number and redeploying the only difference is that that obviously won't persist in the manifest file so you're doing it on the fly to those components there then there's auto scale so this command is used to create a horizontal pod auto scaler of course you can create a manifest file and that's what i recommend but if you are making one really quickly there that's all you got to do so this one's doing a replication controller which is really old we don't do those anymore but i guess i just didn't swap that out in time but here this could be i suppose a replica set a deployment it would be a deployment and you'd specify the min the max the cpu percent that's not the only fields you can provide but just understand this one is actually just changing the current capacity this one will actually set up rules to auto scale okay so there you go hey this is andrew brown from exam pro let's talk about keda so keda stands for kubernetes eventdriven autoscaling and it's a project that allows you to scale based on event data uh it's not gonna show up on the exam but it's just important to know because it is another component to scaling it allows you to access a wide range of scalars that you normally would not have access to so you might recognize some of these things if you're using cloud service providers so or particular types of open source um tools or projects so like we have kafka active mq aws services like kinesis sqs tons for azure tons blob storage event hub log analytics monitor pipelines application insights service bus storage queue cassandra which is um a it's kind of like it's a it's a columnwide database all sorts of things datadog elasticsearch uh google clouds pub sub ibm mq mongodb mysql new relic post sql prometheus um redis and a whole bunch more so uh you know it is a very powerful tool to have if you need event driven auto scaling uh and i just didn't know what it was and i just felt like we should know in general uh that this thing exists okay hey this is andrew brown from exam pro and we are looking at open standards for cloud native or in particular kubernetes so what is open standard or open standards this is when multiple organizations or technologies adopt a specific technical standard and a standard is considered open when a public facing community can participate on the development maintenance and future changes to that standard and so we have a lot in our cloud native kubernetes sphere so let's take a look at it the first one is the open container initiative so this defines industry standards around container image formats and run times to make sure that all container run times could run images produced by any build tool and so the container open initiative commonly initialized as oci or oci compliant images was created by docker because um you know we needed a way to just kind of standardize container image formats then there's cni container networking interface so this is a specification and libraries for writing plugins to configure network interface in linux containers so we're going to see a lot of these like container x interface like container network interface runtime interface and kubernetes has a lot of these because um and it's not just kubernetes it's just these are designed for any kind of orchestration system that wants to adopt it adopt it because if you buy into these uh container interfaces um then uh you know anyone that's building out these products that are compatible with these plugins can be utilized for any orchestration system okay so it's kind of like a middle layer and it really just makes our ecosystem a lot more rich you have container runtime interface so this plugin interface which enables cubelet to use a wide variety of container runtimes without the need to recompile so crio is an implementation of kubernetes cri to enable using oci compliant runtimes as an example and i just list that one there just because this is a cri and this is crio and there's no graphic beside it you notice there's no icon it's because it's going to really be dependent on the runtime that you install so they just don't have a general graphic for that you have the container storage interface so a standard for exposing arbitrary block files and storage systems to containerize workloads on container orchestration systems cos sometimes the i guess i should have capitalized the s there but that's fine like kubernetes and then there's open telemetry a collection of tools apis and sdks to instrument generate collect and export telemetry data like metrics logs and traces so these are all the open standards there could be more that i'm not aware of but these are the ones that we care about and hopefully that gives you kind of an idea there okay hey this is andrew brown from exam pro and we are looking at the cncf governance structure so uh it's important to know this because it does kind of show up on the exam and also just to understand uh uh the whole body of the cncf and how they operate so that you can really trust um the cloud native uh projects and the ecosystem in general so the cncf is composed of three main bodies we have the governing board uh initialized as gb and they're responsible for the marketing and other business oversights and budget decisions and so the gb has the marketing committee as uh one component in the governing board then there's the technical oversight committee known as toc and they're responsible for defining and maintaining technical vision for the variety of projects under the cncf and in that they have special interest groups sigs or sigs then we have end users so euc which i don't really see people ever using um the initialism very often but end user community and there to provide feedback from companies and startups to improve cloud native ecosystems they actually are the companies and startups and so they have their own sigs and then they have user groups a user group would be like a a meetup you know like maybe in toronto there's a cloud native meetup and so they're considered end users and memberships are composed of different tiers platinum gold silver end user academic nonprofit members and we will talk about how membership plays a role in terms of what positions you can have and uh what money is involved uh in this structure for the cncf all right let's take a look here at memberships so the top tier for the cncf memberships is platinum members and this gives you the ability to appoint one representative to the cncf governing board you can appoint one representative as a voting member in any subcommittees or activities of the governing board i enjoy most prominent placement in displays of memberships including on the website most these tiers it's all about the governing board and again um the real power is really being held by the toc because they're making technical decisions on the projects at least that's the way i feel about it because as you uh we read about there the governing board is doing things like marketing and things like that um but you know we gotta get people to pay so that's just how it works we have the gold members so a point one representative to the cncf governing board per every five gold members up to three maximum gold representatives for the silver members the point one representative to the cncf governing board per ten silver members up to three maximum silver representatives we have end user members so participate in end user advisory community as described so nominate one representative to the end user technical advisory board um and i should state that all the tiers above i just didn't want to cram this slide together but if you are uh end user and you're in one of the other tiers you get both benefits for academic and nonprofit members participate in limited to academic and nonprofit institutions respectively and requires approval by the governing board entitled to identify their organization as members supporting the mission of the cncf to any other rights or benefits as determined by the governing board so we got a bunch of tiers here but where's the money what are people paying to get these memberships and it's expensive you'll see here in a moment so for the platinum members 370 000 usd at three year minimum contracts so companies like aws grafana new relic apple so and there might be more i don't know if i grabbed all the graphics here because we're just limited on space but you get the idea of like what's the spend here for gold members 120 000 annually so we got salesforce american express equinix hcl for silver members it's kind of like on this sliding scale depending on how many employees you have uh the more you have the more you pay so here we just see a bunch of recognizable names hashicorp mastercard accenture digital ocean i thought hashicarp would have been a little bit more up there just because they focus on multicloud and cloud native is uh pretty keen on that but that's just where they are uh then for academic and nonprofit members a thousand dollars so mitre wikimedia cloud foundry internet2 and then end user members there is no cost so um really the question is what kind of powers does the governing board have and so we'll look at that next all right let's take a look here at the governing board and see what kind of powers they do have so governing board is responsible for marketing and other business oversights and budget decisions for the cncf and so here's the graphic i pulled from the website so you can see some of the members there and the governing board does not make technical decisions for the cncf other than working with the toc to set the overall scope of the cncf so what can they do well they can approve a budget and that's a lot of power so they're not making technical decisions but uh you know if you want if you need money for it you're gonna have to go through them in order for it to get approved so directing the use of funds raised from all sources of revenue to be used for technical marketing or community investment that advance the mission of the cncf they can elect a chair of the governing board to preside over meetings authorize expenditures approved by the budget and manage any daytoday operations and there's about 30 governing board members and they meet about three to five times a year uh so just to kind of expand it's we covered the most important ones but let's just go through them all so that we can see what they do so approve a budget which we just said elect a chair of the governing board to preside over meetings authorized expenditure approved by the budget so when they say like preside over meetings i'm assuming like any kind of meeting not just ones on the governing board vote on decisions or matters before the governing board define and enforce policies regarding intellectual property direct marketing evangelism efforts through events press analyst outreach web social and other marketing efforts oversee operations and qualification efforts establish and oversee any communities created to drive the mission of cncf establish and execute a brand compliance program if any based on the cncf requirements which may include a certification test to use the brand marks established by the toc adopt guidelines or policy for use of trademark provide financial governance overall so you know they approve a budget right like that's what i'm going to remember from it um and they they can preside over meetings but of course there's a lot of stuff here but you just have to generally remember that they're they're not making technical decisions and they're approving budgets okay all right let's take a look here at the technical oversight committee also known as the toc and they provide technical leadership to the cloud native community and they're not a really big team this is the representatives here even though they're cut off it's really a small team and there's lots of contributions but this is the main team and their responsibilities include things like um defining and maintaining the technical vision of the cloud native computing foundation approving new projects and creating a conceptual architecture for the projects aligning projects and removing or archiving projects accepting feedback from end user committee and mapping to projects aligning interfaces uh yeah interfaces to components under management so code references implementations before standardizing and defining common practices to be implemented across the cncf projects so there you go all right let's take a look here at special interest groups sigs because we did see them in the overall uh organizational structure so special interest groups are specialized committees that work under or report to the toc and there can be like end user sigs but we're talking about the ones that are in the scope of the tfc right now so sigs include things around traffic observability governance app delivery core applied architecture security and when i say around i mean like literally there's a sig for each one right it's not like they cover all these things they're specialized and even at the time this is being drafted so this list might change but this is generally what they're thinking right and sigs are long lived groups so they're not just temporary groups that spin up for a particular use case and then spin down they are sticking around uh and they're led by uh recognized and relevant experts so you know if it's like observability it's gonna be something like people that specialize in service mesh or something like that and so things that are going on here that uh you know they're supposed to be doing is strengthening the project ecosystem to meet the needs of end users and project contributors identify gaps in the cncf project portfolio find and attract projects to fill these gaps educate and inform users with unbiased effective practical use useful information focus attention to resources on helping foster project maturity systematically across cncf projects clarify relationships between projects cncf project staff and community volunteers engage more communities and create an onramp to effective toc contribution recognition reduce some project workload on toc while retaining executive control and tonal integrity with elected body avoiding creating a platform for politics between vendors so hopefully that gives you an idea of what sigs can do and yes there are end user sigs we're not really covering them here but this is the sig that i want you to know is the the technical ones okay hey let's take a look here at end user community also known as euc or if you want to attempt to say uke but i'm just going to say end users so end users in the cncf are individuals or organizations that use cloud native technologies but they do not sell cloud native services so examples of not end users would be vendors consultancies training partners telecos but i kind of feel like the line's a bit uh blurred there because when you read the charter and how some end users can be promoted to roles outside the end user stuff i just get really mixed up okay so um i think that uh it's it's kind of a soft line okay um the uc acts as a feedback loop between those using cloud native and those maintaining and building cloud native solutions um but not just being a feedback loop you know you're not just there to to give them answers as a customer but the idea is that the end user community is an ecosystem of finding cloud data of talent finding local user groups uh meeting project maintainers contribute to the cncf technology radar um there is a class to be like there it's free to be an end user but then there's membership tiers but it's confusing because like if eight of us is a uh like a platinum member are they an end user because they have a service but um i think this is just kind of a mistake in terms of the marketing pages where um you have end users and then things beyond that and it gets confusing but anyway end users if you are paying a particular membership you don't have to then you get access to things like educational resources and um either cheaper or free tickets uh to the cloud native conference so that will vary there but uh there you go all right let's take a look at the end user technology radar so technology radar is an opinionated guide to set a uh of a set of emerging technologies and basically looks like this um and so the cncf end user technology radar is intended for a technical audience who want to understand what solutions end users using cloud native and which they recommend so basically they do a survey and they ask what are you using what do you trust and then they kind of have this little radar diagram and this is an original idea from uh cncf and they admit that too that this is something that a lot of companies like to do like a technology radar or a radarlike thing and they do also have uh subradars uh like technology radars um for like maybe security serverless things like that but this is just the general one there but let's take a look at these three categories and what they should mean to you when you are adopting uh technology so the first is assess so the cncf end user community has tried it out they find it's promising and recommend having a look at these items when you face a specific need for technology in your project okay then there's trial so the end user community has used it with success and we recommend you have a closer look at technology adopt so clearly recommend this technology um that have been used for long periods of time in many teams and has proven to be stable and useful so it's interesting because you have projects and those are like if they're graduated you think that's like a guarantee that you should use them but this is more um less of like what status a project is of like maturity in terms of adoption because that's just a signal to say like enterprises you can use graduated projects you should trust them but this is more practicality saying what works as a combination of technologies um yeah so it's nice to have that kind of balance and have that driven by uh the end users as opposed to just taking the word of the cncf further projects right because like yeah things like that okay let's take a look here at the cncf charter so the cncf charter is a public document that contains the organization structure of the cncf the mission and values of the cncf the description of membership tiers the definition of cns projects governing policies references to code of conduct general definitions in relation so like what does a organization related to the charter like well how would we describe that general rules so it's basically a big document to help define and guide the cncf and um in this section i used quite a bit of information from it so you didn't have to read the charter and then i used the marketing pages to fill it out so you know hopefully that gives you an idea of the contents of the charter okay so no organization is complete without some values like things or a list of things that they believe in um and that we're supposed to go along with so um i'm being a bit tongueincheek there because to me it's just all right you know what i mean like uh it's like no different than the it was leadership principles or whatever microsoft or google does but let's read it and see what we have here so the cncf strives to adhere to the following principles fast is better than slow so enable projects to progress at high velocity to support aggressive adoption by users fast is better than slow open open accessible and operates independently of specific partisan intersects that seems fair accepts all contributors based on the merit other contributions i like that that sounds good too technology must be available to all according to the open source values fair uh sorry the technical community and its decisions shall be transparent so fair avoid undue influence bad behavior or pay to play decision making we have strong technical identity so achieve and maintain a high degree of its own technical identity shared across the projects clear boundaries so establish clear goals and what the nongoals to allow projects to effectively coexist help the ecosystem understand where to focus for new innovation scalables support all scales of deployment from startups to service providers and enterprises platform agnostic the specifications developed will not be platform specific such that they can be implemented on a variety of architectures operating system so i mean it's pretty much aligned with um uh you know kubernetes and stuff because it's platforgnostic uh from from small to large but i mean really kubernetes is not easy for small startups but i mean you know it's cloud native so we can't just look at just kubernetes there's a whole scope of projects there but yeah i guess that generally aligns with the cncf all right so we've been talking about um you know governance stuff and i just kind of want to show you where this stuff is coming from so if you ever want to look it up yourself so i typed in cncf charter and it brought me here to the charter page and so basically a lot of stuff we covered is from here where i've summarized it or just kind of made it a little bit more digestible if you're kind of curious uh what the stuff looks like and if we go up to foundation i wonder if there's more stuff in here so there's all sorts of stuff not a whole lot um that we were looking here this was pretty much uh the main thing there but uh if we go over here and we go to uh here we can see the governing board here we can see uh oversight committee i don't think i checked what staff was but it wouldn't hurt to take a look if we go to who we are that's probably where i got the values from so yeah they linked to the charter on the github here on the about us page they talk about their cloud native definition which again i i'm not a big fan of it we did it in the beginning of the course it's there okay if we go ahead and close that on off we can see the governing board so here you can see all the members you can view their profiles um so nothing super exciting what's interesting is like uh you can read their past minutes because everything's in the open right so you can go here see they meet three to five times a year click and do it and it shows you who showed up uh very formal um and also sometimes what they'll do is show like on github maybe more for um the toc but if we go to the tlc the technical oversight committee right um they show their activity here they have a mailing list so i guess you could start uh talking them there you know this is such an old school thing but hey they still work um if you go to the toc they have their own page here the technical oversight committee explains who and when and why they got there the projects their meeting times where the zoom call is the passcode to get in i don't know if you can just like kind of like show up and uh uh bomb the meeting or something like or people can listen in i guess you can come in they can just mute your kick you uh then there's uh technical advisory groups which we never talked about um the tags and these look like sigs so i'm a little bit confused because sigs and tags um basically have the same thing so maybe they renamed it maybe tags are the same thing i'm not really sure oh look at that yeah look at that same thing so maybe this is cigs or tags i don't know now so that's cool okay so tags and things i guess are the same thing all right um and so you know that's pretty much it i just oh maybe the radar thing we should take a look at the radar so if we go to um and use a radar okay um here this says devsecops that's the one i had in the actual um presentation the slides if you go down below you can see like who end users are they kind of have a presentation it doesn't tell you much but this is what's interesting where you see uh the data so like this is how you understand how it's happening where they're going okay let's vote on terraform as soon as just links to the terraform website oh no it pops up right and so they gather metrics to see adoption so that doesn't necessarily mean that that means that you should use it just understand that's what the data is so we have 21 companies that's not a lot of companies but maybe they're larger companies i'm not sure if it shows us um uh the information there like how do we know what kind of company they are and here i guess we can see the size of them so you know it would be nice if you could like filter based on your company size because that could really help you understand or even industry help you understand that but down below we have different radars so we have multicluster management secret management database observability i said serverless i guess there isn't one um but that's totally fine there as well and so here you can read all sorts of stuff these things are again you know take them with a grain of salt they're uh they are what they are uh only other thing i'd like to show you is like end user stuff to show you where i was talking about confusion so we go end user cncf they actually have a page right here and if you scroll on down this is where we start seeing memberships and the language just keeps changing or varies in other places so here they call it a supporter which is 4500 um but you can join the end users for free how you do it i have no idea right i guess you just said join now and you sign up and that's you signing up for free i don't know like i've already signed up before because i when you set the certification you need to sign up to do that so i suppose as soon as you sign up you technically are an end user but here you can see kind of the benefits that you get for doing that but this matches to the memberships we saw earlier so everyone's an end user or stuff like that so you know if you feel confused just know that i'm confused a bit too but um you know it's not really going to show up in the exam but there you go okay all right let's take a look here at kubecon plus cloud native con so kubecon is a technology conference for kubernetes and cloud nativecon is a technology conference for cloud native uh why is there two and why are they combined is another question because uh you always see presented as kubecon plus cloud native con so um as far as i understand talking to people that have been in the industry longer than me for cloud native stuff i believe it was kubecon existed first then they wanted to do cloud native stuff as well so they tacked that on i don't think cloud native was ever its own conference i think it was just like an adjacent conference with kubecon so it's essentially one conference um and so that's the idea there and it was only north america but then they decided to add a european one uh and so the attendance is actually very significant so if we just take a look here on the right hand side in 2018 we had 8 000 in person then 12 000 in person uh 22 816 attendees virtual because of covet uh then we saw kind of a split where i had 17 000 attendees and only 4k in person and so who knows what it'll be for 2022 um i'm not sure and even at these conferences you might see adjacent conferences in north america like istiocon con get up get ops con i don't know how you draw the boundaries like i don't know if they have like a separate building or stuff but uh that's a thing but you know i do hear really good things about these conferences i hear they're very inclusive and welcoming uh it's much more refreshing over something like abyss reinvent so hopefully one day i'll get to go in person but uh i just wanted you to understand that this is a thing all right so i wanted to show you a bit more about kubecon and cloudnativecon just because i felt my slides are a little bit lacking but just to kind of show you how big of a deal it is so i just typed in kubecon plus cloudnativecon made it to the maine's website here it's on linux foundation if you scroll on down you get some really fun screenshots to get a good idea um you know what it's like to be at cubecon so there's probably even more photos if i click here uh it's not loading oh there we go so yeah we can see uh it's it's a big deal it's just like going to um like the aws re invent conference or ignite uh their big or like azure ignite or microsoft ignite it's a big deal right so you know just consider that uh you know you can see the schedule here look how big that stage is right uh you can see all the sponsors so there's a lot of money being put into it if you cannot attend um i don't know if all the videos make it up on here but a good chunk of them do and so you know you can go back and watch some videos that were at previous ones probably go to the playlist to see um what they are so if you just scroll on down here you can see there are some featured playlists right so um you know like that i don't know if all really i don't think all the videos get on there i think it just kind of happened because of covid and they just put some of the videos up because there's a lot a lot of events and i'm not seeing uh the volume of that i would think that if i if i was at kubecon so you know there you go and uh you know hopefully it convinces you that you might want to go okay all right let's take a look here at cncf projects so projects are technologies and they are managed by the cncf so many cfcf projects are developed by external tech companies and then are handed over to the cncf for longterm support and you can access all these projects at cncf.org slash projects and projects are categorized based on their maturity level and i'm pretty sure you need to know that for the exam so here they have this diagram here that kind of shows some information and this is actually based off a diagram from a book but let's go through the different categories so we have graduated meaning it's production ready for enterprises incubating so the this is where the apis might be rapidly changing and you may have incomplete uh they may be incomplete to be adopted by large enterprises but they are totally fine for smaller companies uh then you have sandbox so experimental prototypes uh and i would not bet that these would pass the security review so um you know there are so many sandbox projects they're just not listed on the website and the maturity level again is based off of this book called crossing the chasm uh which is a diagram in the book from 1991. i don't particularly care for it i'm not really sure why they use um this model like i don't know it's like to try to sell more of those books or somebody in particular liked it and thought yeah that's a good framework to use um but it's fine i mean i would just prefer it being like stable or you know release candidate or something like that but that's how they name them so there's that but let's talk a little bit more about sandboxes um so if let's say you wanted to find sandbox projects because i just said that they're not on the main website but they are in the the landscape so what you can do is go to cncf projects and then check box there and that's the way you can see them now all these projects um go through different kinds of eco or life cycles and so it might be worth to take a look at the different life cycles so here i have project stages and this is from the documentation for cfc projects right from their website so the idea is we have a sandbox so these are experimental neutral collaborations and here they're suggesting like they're low barrier low reward uh and so you know you might not want to adopt them then they have this significant barrier so this is where the majority of development happens i think that's what they're saying there and then from incubation they say the obvious pass so incubation to graduation not a lot of descriptions around the stuff but you kind of get an idea where things are flowing and that some things just completely skip over the sandbox section and go straight to incubation and all these have even more detailed information so when you look at the sandbox process the idea is that there is a project proposal that is in the case of a github issue um the technical committee will triage it and give it a brief request a sig the sigs will make a presentation that is recorded the presentation slides a recording complete recommendation template um and then the toc will review recommendation and presentation and then it will be in sandbox so three toc sponsors to the step forward so that's how we get to that then there's the incubation process so you have a project proposal and it goes to special interest groups again makes a presentation whether it's recommendation they do their due diligence with tech and governance user interviews um and then here we have another review with another presentation the toc vote on it and it makes it into incubation so you can see there's a lot there um i don't think there was one for graduated there probably is a process but i i didn't see one because if i if there was i'd have it here but that's that there so there is at least one graduated cncf project generally for each cloud native category and that's just something that i observed and that kind of makes sense because like how do these categories exist um you know if they don't have it so we're looking at things that are graduated so like container run times k or d core dns envoy at cd uh fluentd harbor helm uh jaeger i think it's pronounced jagger like the j's the y uh kubernetes linker d open policy agent prometheus um the update framework so tough rook and uh tikvi and vitus or vitus lots of names i've never had to say out loud before so hopefully i got them all right but there there are a real a lot of really good uh incubating projects that uh i'd strongly recommend using so i don't think that should be a discouragement but this is really for like you know if you're a large organization like a huge enterprise then you're probably going to gravitate towards these projects okay hey this is andrew brown from exam pro we're looking at what is serverless so serverless architecture generally describes fully managed cloud services and the classification of cloud services being serverless is not a billion answer as a yes or no but an answer on a scale where a cloud service has a degree of serverless so a serverless service could have all or most of the following characteristics and you basically just have to take an estimated guess to say whether it is serverless there's a lot a lot of debate about serverless but you know i've talked to engineers like at aws and stuff like that where we saw serverless very early on and the definition was really simple there's a degree of operations that you just do not have to perform and you can just focus on providing business value but let's look at the elements that i've included here that i've generally been able to find out about serverless so serverless is generally highly elastic and scalable highly available highly durable secure by default abstracts away the underlying infrastructure and are build based on the execution of business tasks so for business value serverless can scale to zero meaning when not in use the serverless resources cost nothing and so the idea here in these two sections here is we're paying for value you don't pay for idle servers my friend that runs the uh service toronto user group likes to describe it as energy rating so the idea is that um you know consumers uh you know what they're gonna do like think of like if you have like a washer or a dryer and you have this energy rating saying like it's out of three stars out of five of being energy efficient right so the idea is like some things are more serverless than others um you know and that's just the idea there okay all right let's take a look here at cloud native and kubernetes serverless so the cncf has a landscape for just serverless um services or or cloud native serverless things and so what does that mean well so the cncf classifies as serverless in this landscape to be things like functions as a service so aws lambda azure serverless functions google cloud functions serverless frameworks such as dapper i think that's how it's pronounced it was sam chalice uh installable platforms kubernetesbased eventdriven autoscaling keda which we did talk about in this course apache open whisk open foss knative fission cubeless and tools things like uh lumigo and dashbird so it's interesting that they put um like managed providers because i guess technically you know they're cloud native but they're not they can't be moved to anywhere else so um i think it's kind of like those things where like adabus paid a bunch of money to be like a toptier uh provider and she's like hey can you throw a little slam down there because to me if it's cloud native it has to be an installable thing it can't be a um a service that is uh managed do you what i mean that can't be moved to any other platform so to me this is where uh you know like maybe these frameworks and this is where um actually i'm going to just zero this out i would just say this is what really feels to me to be um what i call cloud day of service and these things are just kind of like adjacent to them right um so just bear that in mind okay hey this is andrew brown from exam pro and let's take a look at k native so k native is kubernetes based platform to deploy and manage modern serverless workloads and k native is a project to create a standard set of building blocks that i think that's kind of an important statement right there standard set of building blocks for kubernetes to enable serverless development patterns k native generally is composed of two parts uh and i say generally because there is a third part called building but k native serving so this takes containerized code and deploy it to uh with relative ease it scales to zero costs you have k native eventing triggers serverless functions based on kubernetes api events loop in other managed sources to trigger serverless functions without this part the eventing we might not consider it serverless because if it just had serving to make it easier to have an opinion on setting up a bunch of infrastructure for you to deploy your apps it would just be like a managed layer um so over here you kind of get an idea of what else is going on here so k native has building inventing and serving it's on top of kubernetes and there can be layers like the app on top like that's just istio for service mesh like discovery to reach those functions some considerations is that it's not a complete serverless framework and it does not offer function as a service offering now that doesn't mean you can't run functions on it but function as a service is more of a fully managed uh thing and so there's just some things that are missing this is not my words this is k native like you watch the proper presentation uh like uh for those uh who like do it like use it and they even say that this is the case so uh that doesn't mean it's not great it's just a degree of service remember back to the energy ratings where something can be more servless than others but it's still pretty darn serverless now k native has its own kubernetes objects that it defines with kubernetes custom resource definitions crds that is something that's out of the scope of this course like talking about crds but just understand that they're used to make your own components and so candidate of components is server service so manage life cycle of a workload route mapping network endpoints configuration maintains desired state revisions so point in time snapshots of code and so basically they're using the word service more like how we would use the word service to run a workload except it doesn't necessarily mean that it's continuously running so you know like when you use something like aws ecs they say service and task service something that that continuously runs uh and doesn't spin down and the task does it runs and it spins down but just understand they call their service and so here's kind of a diagram how they all work together so of course the service would have a configuration a service is basically the function revisions are uh snapshots so like backups of ones that you made and routes that point to this particular one and you're not going to really be doing all the rest you're just going to be working with service and the way you do that with is with the k native cli called kn and use this alongside cube ctl because it's not like you're not going to touch any cube ctl but when you're working with your apps and stuff and deploying your functions which are called services and uh can it's going to be like this kind of different syntax here so think of k native as abstracting awake deployment services autoscaling many more k components and you just create and deploy k n or k native services so there you go let's take a look here at k native versus open fast and really all i did was grab a excerpt from an article for on the cncf blog from 2020 called serverless open source frameworks open fast k native and more strongly recommend to read it even though we are a few years in the future here uh i still think it rings true based on what i was looking at and just maybe will help to understand the difference between these two a little bit more so unlike openfast k native is not a fullfledged serverless platform but is better positioned as a platform creating deploying and managing serverless workloads however from the point of the view of configuration maintenance open fast is simpler with openfast there is no need to install all components separately as k native and you don't have to clear previous settings and resources for new developments if the required components have already been stalled still as mentioned above a significant drawback of openfast is the container launch time depends on the provider while knative is not tied to any single cloud solution provider based on the pros and cons of both organizations may also choose to use knative open fos together to effectively achieve different goals so one thing that was interesting is the container launch time depends on the provider i didn't fully understand why didn't i mean so that's one thing i wasn't sure about that i really tried to research another interesting thing is just when you see the stacks like the tools involved or knative feels a lot more towards um the cncf's projects like like standard projects and openfast is a little bit different um so like canada feels more again a better migration path back to full kubernetes if you need it in open fastest really feels like serverless like you don't have to do a lot of work there so there you go hey this is andrew brown from exam pro and we are looking at the pillars of observability so before we talk about observability in kubernetes we should just understand uh this general concept of pillars of observability um and i don't really show them as pillars i show them more like a triforce because i think that that makes it for a better visualization but let's talk about what observability is so it's the ability to measure and understand how internal systems work in order to answer questions regarding performance tolerance securities and faults with a system and application to obtain observability you need to use metrics logs and traces and you have to use them together using them in isolate does not gain you observability so just understand they all work together for metrics we have a number that is measured over a period of time so if we measured the cpu usage in aggregate and aggregate over a period of time we would have our average cpu metrics for logs we have a text file where each line contains event data about what happened at a certain time so it's a log and then there's traces a history of requests that it travels through multiple apps and services so we can pinpoint failures or uh performance or failures and traces go along with spans we'll talk about that and i say it looks like the triforce of observability than the pillars of observability but that's the idea there okay hey this is andrew brown from exam pro and we are looking at open telemetry uh initializes otel and is a collection of open source tools apis sdks to instrument generate collect and export telemetry data it standardizes the way telemetry data is collected like metrics logs and traces um and it uses the wire protocol which refers to a way of getting data from point to point like soap m q p so the idea here is we have applications that are instrumented with the otel library that go to collector agents so an agent that is on the host and it's going to send it to the oetl collector service uh which essentially are back ends and i mean that's what it is open telemetry is a big deal because it could uh really change the way we collect information and we're starting to see cloud service providers um adopt it so you know you have aws cloud met or sorry cloud watch but like why would you use that when you use open telemetry which gives you a lot more richer data so it kind of competes with um services in some sense but uh definitely is a really great tool to have so let's talk about instrumentation so instrumentation is the act of embedding a monitoring library into your existing application in order to capture monitoring data such as metrics traces or logs so i really like ruby so i grab the ruby example and the idea is that you'd have some kind of library and you'd import that library set some defaults you'd run the configure you'd create a tracer because instrumentations the idea behind it is you're creating traces right uh and traces and spans per se so there uh you have your tracer so you're gonna have one trace that goes along a bunch of projects and then in this application it's going to create a span because the span starts within an application you pass it a variety of attributes like data that you want um and that's all there is to it and the telemetry otel or open telemetry supports a variety of languages so basically all your favorite languages and for certain frameworks uh there are plug and play libraries to quickly instrument your app so if you're using spring asp.net express quarkus uh i assume that there's kind of like built into the background so you're not really doing instrumentation you just say turn it on and it just happens i'd love it for ruby on rails maybe there's a third party for it i don't know um and let's just talk about the collector for a moment so open telemetry collector is an agent installed on the target machine or is a dedicated server and is vendor agnostic way to receive process and export telemetry data it removes the need to run operate maintain multiple agent collectors and this works with improved scalability and support with open source observability data formats like jaeger again i think it's jagger not like jagger but jagger sounds cool too prometheus flew in bit sending to one or more open source or commercial back ends uh the local collector agent is the default location to which intermittent instrumentation libraries export their telemetry data and so here they're just kind of representing what we said so you have oltp uh jagger prometheus and that's going to receivers um and you can see that it's adjusting on both sides here just as an example right to that but this is all what's inside of the ol the hotel collector okay so that's all i want you to know for open telemetry they don't really say much on the exam or if any but it is a really important project to know okay hey this is andrew brown from exam pro and we are taking a look here at prometheus and open source system monitoring and alerting toolkit originally built at soundcloud yes the company that makes um or allows you to host sound clips prometheus collects and stores its metrics as a time series data and that should give you an indication that it is a time series database and its main features are a multidimensional data model with time series data identified by metric name key value pairs prom ql a flexible query language to leverage the the dimensionalities no reliance on distributed storage single server nodes are autonomous time series collection happens via a pull request on http publishing time series is supported via the intermediate uh intermediary gateway targets are discovered via server discovery or static configuration multiple modes of graphing and dashboarding support prometheus values reliability so you can always view what statistics are available about your system even under failure conditions if you need 100 accuracy such as per request billing promises prometheus is not a good choice as the collected data will likely not be detailed uh to be or complete enough for that use case in such a uh case you'd be best off using some other system to collect and analyze the data for billing and produces prometheus for the rest of your monitoring so let's take a look here at um the actual system itself that it works and so the idea is that prometheus scrapes metrics from instrumental jobs either directly or via an uh intermediate pushed gateway for short lived jobs uh it stores all scrape samples locally and runs rules over this data set to either aggregate and record new time series from existing data or generate alerts and grafana or other api consumers can be used to uh to visualize the times siri data you can see that it also has alerting and other cool stuff built in there um and there's not really much to show for prometheus every time i see grafana i always think it is prometheus but graphon is only the visualization tool on top of it but it's the next thing that we will talk about okay hey this is andrew brown from exam pro and we are looking at grafana which is an open source analytics and interactive visualization web application grafana is commonly used along with a time series database like influx db prometheus or graphite and so it looks like this and there's really not much else to talk about it for the kcna because you don't need to know that much other than it's a visualization library that works with promised to this but there you go all right let's take a look here at traces and spans because these are things that we'll come across especially in serverless especially in micro service architecture like using anything with kubernetes because you have to track things between containers or contain between workloads and that can get kind of confusing so here is our graphic let's talk about what a trace is a trace is data or execution path through the system and can be thought of as a directed acrylic graph of spans and so we're talking about a dag this is that thing over here so the idea is that you have a unique id that's going to be tracked through uh all these um spans and then this makes up the trace this is the trace right uh and then the spans are the individual services the individual containers or or whatever that's tracking saying this is what happened uh in my little app this is what happened in my little app and then there's the context in between them like the latency and stuff in between them or did it fail how many times did he try so a span represents a logical unit of work in jager so i actually pulled this from the jaeger documentation this graphic is from jaeger or jagger i'm not sure how to pronounce it because it's used for um tracking the stuff that has an operation name the start time of the operation the duration spans may be nested and ordered to model casual relationships so uh hopefully that gives you an idea what traces and spence are but there you go all right let's talk about cost management now on the kcna exam i only had one question about cost management it was absurdly hard and obscure um and it had to do with like if you like it was actually scenario based like oh if you had these many things running if you scaled it down would it be better to scale up or scale down or whatever it just didn't make any sense for a casino question so my brain i go that must have been an unscored question or just a bizarre one and there aren't a lot of good resources out there for cost management even though it's part of the exam i had to like scrounge for information and try to make it kind of relevant so let's just go through what i could find and i mean it's just gonna be general knowledge but you know just understand that uh you know it's just the best i can do here okay so first thing is like labeling resources if you label your resources um with like metadata then you can use a visualization tool like prometheus or graphana it will end grafana to then kind of figure out costs all right how you get dollar amounts in that i do not know but i do know that that is a suggestion there you can find idle and unallocated resources because if they're idle or unallocated you're paying for them the idea here would be to visualize idle cpu memory storage and producing graphana and say like hey where do we see zero cpus being used but something's running horizontal sorry workload right sizing would help a lot so with that we can use a vertical pod autoscaler to adjust the cpu memory of pods a horizontal plot autoscaler to add or remove to meet the demand so one is right sizing of the the pods usage itself another one could be right sizing the cluster so to speak but at a pod level then there's cluster downsizing opportunities which is again i guess right sizing where you're using cluster auto scaler there's a double s there there should only be one so i apologize for that uh but it will add or remove nodes to meet the demand um you can leverage uh free trials of kubernetes cost tools like cubecost you know whose recommend that was cubecost so i think they're just trying to use the tool but i mean it's good advice it's not going to be an answer on the exam but it's true uh you can estimate future costs and so here this is where you use like load testing using something like speed scale k6 j meter gatling and so you'd say well what would it cost if i had this kind of traffic or these kind of nodes and stuff like that um so not wasting uh space this is pretty similar to finding idle unallocated stuff but to kind of add to that it's like you could adopt serverless architecture that scales to zero so when no traffic is received for a period of time it just shuts it off okay um and technical debt so maybe you can rearchitect to reduce the amount of pods or maybe you should be using more pods and breaking up your app into smaller workloads so you can then isolate what things are cost effect like like costly and what aren't right and also just continually evaluate cloud native technologies and this is the best i can do for cost management but uh yeah for the exam probably just it's going to be around right sizing like vertical pod auto scale or horizontal plot autoscaler and then making the best guess you can okay hey this is andrew brown from exam pro and we are looking at kubernetes system logs and k logs so system logs um is system component logs that record events happening in a cluster which can be very useful for debugging and you can configure log verbosity to see more or less detail so logs can be as coarsegrained as showing errors with the component or as fine grain as showing stepbystep traces of events such as http access logs pods state changes controller actions scheduled decisions and as easy as they make it sound and detailed information i couldn't find a whole lot here i think i would have to probably jump into follow along and make a practical example to really extract that information out but really they just it was like cost management it's like i could not find good examples readily available to digest in a slide here but there is the cube ctl command logs and so here we're providing the pod name or deployment name whatever it is probably pod name and so that would get you log information there's a library called k log it's the kubernetes logging library it's based off of g log which is golang's logging library and it generates log messages for the kubernetes system components and i think we did talk about cubelet being able to in the cubelet section uh you know taking log data and sending it back uh where wherever uh there so you know hopefully that gives you kind of an idea there about logs okay all right let's take a look here at chaos testing and testing so what is testing well this is the assertion uh that we have an expectation of inputs and outputs of functions that's the most basic thing in programming so the idea is we have a variable called hello right here and we assign it a string called world and then we assert using our testing framework to say does the variable hello equal world and if it does uh pass if it doesn't fail okay so that's the basic concept now it's not going to be the same for um uh kubernetes but we don't really have to dig that deep because it's not on the exam okay so let's talk about chaos testing so this is building a system to withstand and tolerate any kind of failure by purposely introducing random failures in a production system so it's just like imagine if you uh had a junior developer that was just doing whatever they wanted you know and you're just making sure that your application is resilient to any kind of problems so there's a few different frameworks out there so for chaos testing we have chaos cube for a just a testing framework we have test cube and then there's the generalized chaos monkey which is another kiosk testing framework so i just wanted you to know that there was a chaos cube and test cube frameworks and the exam's not really going to ask you about this stuff but you should know it so that's why i'm showing it to you okay hey this is andrew brown from exam pro and we are taking a look at helm so before we answer that we asked to ask ourselves what is a package manager so a package manager is a collection of software tools that automate the process of installing upgrading configuring and removing computer programs for a computer in a consistent manner so helm is a package manager for kubernetes and helm is broadly composed of three components we have charts so these contain all the resources definitions necessary to run applications tools services inside kubernetes clusters repositories so this is the place where charts can be controlled and shared and releases an instance of a chart running in a kubernetes cluster so i just want to show you the helm chart directory structure so if you were to create a package this is an example i found online where it's a wordpress helm chart and so here you can see there's the chart yaml file license readme things like that one thing is that you always have this chart yaml file but there's also this values yaml which defines the variables and the default values and the values you can pass into this to configure the helm package okay so helen reserves the use of charts direct directory crds so i assume that's custom resource definitions template directories and uh of the listed file names other files will be left as they are okay i'm just pointing out that chart yaml is important and values yamls is important or at least i thought it was because when i opened up the chart um chart.yaml file i was expecting to kind of see something interesting but it's just kind of very boring it's like package.json's like okay these are the dependencies the api versions the description the home page the maintainers nothing super exciting but basically if you want to um package this and put this on their the place where they host them which some are listed in this section here um you know that's something you're going to need so let's talk about packaging installing so to package a chart directory in a version chart archive the helm package command is used so that we do helm package hyphen hyphen sign i guess you have to sign it provide its directory there so version charts archives are used by helm package repositories and i was saying that there's a place where you can find all these and they're on artifact hub so you go to artifacthub.io and there's all sorts of helm packages you can access so i just typed in something like postgrass or something like this and i found this one and often what you're doing is you add the repo so you know where um where to go get this package then you do an update then you do an install and that's pretty much the procedure that you're going to run of course if you have configuration you'll provide values uh via flags and things like that but that's helm in a nutshell but we do cover it in this course so you will at least know how to install something from helm not necessarily make a helm package because it's a lot of work but there you go let's talk about customize so customize provides more flexibility when writing kubernetes configuration files by allowing you to overlay or override to patch configurations and customize is built in the cube ctl at least a version of it and so customize kind of is put up against helm and being like an easier simpler version of helm we do not do a follow along on customize just because helm i think it's just i mean like if we were to do showing you how to package a helm which we don't we show you an install helm package i'd rather do helm than customize because i think that it's just um has better utility for uh organizations but uh the way customize works is that you define your base architecture in a folder called base so you have deployment service anything you want in here and then you have customization yaml which contains how things should get patched and so then you have down below patches overlays whatever you want to call it because it doesn't matter what the folder name is you can call it whatever you want because you're just going to be using an apply and so it will have a customization yaml and that file will say in that thing it's like a yaml file that says um patch with these other files and these files look very similar to the other ones it just literally replaces components like almost overlaying them um wasn't easy to find anything that would show me how to do this really quickly i probably would have to spend like a couple hours just to get a simple example for you not that it's hard but just hard to find available examples for customize that easily translated over for you but it's not going to really be heavy on the exam i just want you to know that's an alternative to helm and it's uh supposedly easier to use okay hey this is andrew brown from exam pro and we are taking a look at infrastructures of code and so first we have to kind of outline the problems with manual configuration so there is an advantage to manual configuration it allows your cloud infrastructure your cloud native infrastructure uh to easily start using new service offerings new apis versions to quickly prototype architect architectures however it comes with a lot of downsides so it's easy to misconfigure a service through human error or kubernetes components so this is just a general slide that's why i'm keep on adapting it to kubernetes terms here it's hard to manage the expected state of a configuration for compliance it's hard to transfer configuration knowledge to other team members so infrastructure as a code allows you to write a configuration script to automate creating updating or destroying cloud infrastructure and the idea is an iac is a blueprint for your infrastructure iec allows you to easily share version or inventory or cloud infrastructure and so the idea is that you'd have your script or collections of scripts or whatever tool you use and then by running it once you can do it and so kind of in a sense when you write manifest files in kubernetes you technically are doing infrastructure as a code but you generally get some extra benefits there like um state management so uh things like that so we're doing infrastructures of code but like there's just other benefits around there and so we could just kind of have to understand um the difference there because it's a bit fuzzy with kubernetes okay all right so what i want to do here is look at popular infrastructures of code tools and also describe declarative versus imperative so declarative we'll start with that so declarative means what you see is what you get it's very explicit it's also more verbose but there's zero chance of misconfiguration so these would use scripting languages like json yaml xml or a kind of hcl and so every single cloud service provider generally has their own tool or they use a generic one like terraform but for azure we've got arm templates and blueprints for aws we have cloudformation for google we have cloud deployment manager for everybody else we have terraform uh does ibm cloud have their own i don't know i think you would have to use terraform for that and oracle cloud you definitely use that um but terraform is this one where uh it's it's designed to be um multicloud right on the imperative side we have uh you say what you want and the rest is filled in so it's implicit less verbose you could end up with misconfiguration does more than declarative because you're able to utilize full programming languages that does come with additional complexity but it can be worth it so things like uh the aws cloud development kit cdk or pollumi are two options there um but it's interesting because when we're talking about infrastructure code there is the managed infrastructure or managing infrastructure of the cluster but then there's managing infrastructure in the cluster or app or application infrastructure and that's something we'll talk about here next okay all right so for kubernetes infrastructures of code can be a bit squishy a bit confusing because the idea is that you have uh infrastructure that kubernetes has to run on like virtual machines like cloud service providers selfhosted data centers but then there's kubernetes components like things inside the cluster and so i'm just going to try to help you understand that it's all infrastructure but we might just have to contextualize it so there's managing infrastructure of the cluster and there's managing infrastructure in the cluster and the tools that we use will be a slightly different because when people think of infrastructure as a code they're going to default to something like cloudformation terraform but um you know it's not going to solve all of our problems and so let's just talk about some of the differences here so if we're managing infrastructure of the cluster this is stuff that the cluster is running on the virtual machines or managed services like eks google cloud engine or or what have you and so those can be set up with a tool like a traditional tool like terraform or cloud formation or azure blueprints or arm templates or a deployment manager use the provision the cluster and manage services so like relational databases or things that you don't want to put on the responsibility of the cluster alongside the cluster itself okay so that's the recommendation now for in the cluster you can use terraform okay you can technically use terraform because they have this manifest module and basically you're just rewriting your manifest files like the yaml files as hcl which is the the base language or the the language of terraform files and so the reason you might want to use terraform is because you benefit from the state management provided by terraform and that way you'll be able to kind of detect configuration drift and things like that kubernetes already manages state within the controller manager and std so it's kind of like well uh terraform manages state but kubernetes already managed the state do we need two things in energy and state and there could be other ways of doing a configuration drift like using the admissions controller so it's debatable whether it's worth the extra complexity to manage things in the cluster using terraform but definitely of the cluster terraform or cloud formation or what have you is totally fine when we're talking about managing in the cluster um i like to put the thing in front of it application of infrastructure so things like pods services ingress anything that you would deploy as components and it's recommended to package these as helm charts and use that in your ci cd now you think well couldn't i just take all these manifest files and put them in a github directory sure but there are some synergies by using helm because it's just easier to like helm deploy than it is to put things in there and so this is kind of where we see a separation of those two could you use ansible i guess i wouldn't recommend it but these are the two that i generally think that would be good and i think terraform would be good for the cluster because if you use it once um it's that won't be too hard if you ever have to move uh to another provider and if you're using kubernetes that's one of the benefits so you know why not use it okay so there you go hey this is andrew brown from exam pro and we're taking a look at get ops so get ops is when you take infrastructures of code and you use a git repository to introduce a formal process to review and accept changes to infrastructure as a code once the code is accepted it automatically triggers a deploy so here's the general example here where you have some kind of iac tool like terraform and you actually commit the changes to your code and this also could be like helm packages and things like that in the mix to your github repository uh so a pr would be generated and it could be auto auto approved or someone can review it and press a button it gets merged into the main branch and then some kind of cicd tool this is github actions but it could be argo cd it could be uh jenkins x jenkins um flux and then from there it would go to the cluster's providers to affect the managed managed services or it could be going to kubernetes to you know affecting components or things like that or rolling out a hem helm package there to get our changes so uh the most important thing is that it's a formal process review and accept changes of infrastructure code by via a git repository so there you go all right let's take a look here at ci cd models and before we do i just want to mention a couple terms if you're not familiar with them the difference between production and staging so production commonly abbreviated to prod prod or sometimes even pro is the live server where the real paying users are using the platform and staging is a private server where developers do a final manual test as a customer like doing quality assurance qa before deploying to production and so this is our ci cd pipeline or life cycle whatever you want to call it deployment life cycle we have code build integrate test release deploy okay and so for these it's going to vary because there are some different terms like ci cd and and what have you so let's just take a look at uh what happens here and the first we're looking at is continuous integration and so continuous integration means automatically reviews developers code so it's this part of the pipeline where you you have code it builds it integrates a test but it's not rolling out uh for release okay then you have the concept of continuous delivery so automatically preparing developer's code for release to production that means doesn't mean it gets automatically released but it means that it's ready uh someone's got to press the button to get it out to the server and then you have continuous deployment so this automatically deploys code as soon as a developer pushes code if all tests pass so usually like you'd have a pr and someone would look it over but you could just have really good tests and if your tests say it's okay basically that's like the aokay to go roll it out so just understand that there's ci cd here and that continuous delivery and continuous deployment share the same initialisms and i think some of them uh might do like a forward slash or not a forward slash indicate the difference between them i don't do that they're both just cd um but just understand that um you know that these share the same one and continuous deployment is the one that does everything whereas delivery is like delivery as in here's the thing but you've got to open it up uh to do something with it okay all right so let's take a look here at argo versus flex so the cncf have two ci cd projects that serve the same purpose but take a different approach and both of these are in the incubating stage so they're all both safe to use unless you're super enterprise and you have some kind of concern but we have flux and argos let's talk about the difference so flux was originally developed by weaveworks it takes a cli first approach it's experimental web ui is a plugin so you can get a visual thing it supports rolebased access controls it supports multitenancy in flux too so if you're using flex one you won't have that it supports helm and customization and that's the logo for weworks uh it has automation of container updates let's take a look at what argo does so it has both a cli and web ui so it's had a little bit more to start with supports rolebased access controls supports single signon supports multitenancy helm customization case on it jsonnit manual commit and sync to update containers and so you might say well it looks like argo basically has almost everything so why not just use it well generally flux is simpler in design and cli focused so if you need to get off the ground real fast flux is very popular if you want a simple solution but you know they do have a lot of overlap and you'll just have to decide for yourself which one it is you want to use all right let's take a look here at jenkins jack and x and cloud b so jenkins uh what the you piece of garbage oh god all right let's take a look here at jenkins jenkin x and cloud b's so jenkins is an open source popular immature cic2 tool for any kind of workloads checkins can be used to deploy applications onto kubernetes because it's for any kind of workloads and jenkins is written in java and have many plugins for any use case now there's jenkins so then why is there's this thing called jenkins x so jenkins x is an open source cs cd tool for modern cloud applications on kubernetes compared to jenkins it's supposed to be much much easier to use and jenkins x may replace or merge with jenkins one day to only have a single ci cd tool for all use cases but you have two options here jenkins and jenkins x and it's good to know like who makes jenkins and it's called cloud b's this is the commercial distribution of jenkins and jenkins x for large and compliance first organizations clouds bees was acquired by infra dna yeah for dna organization created jenkins originally so there you go all right let's take a look here at circle ci which is a proprietary fully managed ci cd service to make deployments easy and seamless and can support deploying applications to kubernetes so the key part here is that it's proprietary but it is fully managed so it is one of the easiest deployment solutions out there but also uh you know it's just not open source so that's just something you'll have to consider i just want you to know a tiny bit about it okay hey this is andrew brown from exam pro and we are looking at deployment strategies so a deployment strategy is a way to change or upgrade an application and there's two terms that i think you should know uh regarding deployments and that is rollouts and rollbacks so a rollout is when you replace or update servers with new versions of an application and the word server could be just replaced with pods in this case because the the um nodes is may or may not be something that is getting rolled out it really depends on what happens there we have rollbacks and this is when you replace revert recently updated servers back to the previous version and so there is a um a rolling update but understand that rola is just a generalized term that applies to all of these deployment strategies so there are several different deployment strategies that can be utilized with kubernetes some of these it's a lot easier to do them if you have like argo or flux or jenkins but deployments are built into kubernetes you don't need to have an additional tool to do them but you might be limited in your options and some i'm going to list here are a little bit more advanced and probably won't show up in the exam but you should know them anyway and that's why i'll go through them but i'll point out which ones uh that you definitely need to know versus the ones that are nice to knows so the first is recreate so terminate the current pods and create new pods all at once normally we call this in place deployment but they call it recreate then you have a rolling update so replace one or more multiple pods at a time then you have canary so add new pods and route a subset of your users to the new server if no bugs or errors occur roll out changes to all the pods you have blue greens to deploy an exact copy of your entire infrastructure and when we say infrastructure it might not necessarily be cluster infrastructure but just application infrastructure because kubernetes is a bit different then you swap the traffic and then terminate the old environment or roll back to the old environment in case you know the the new environment's not working out you have a to b testing or red and black deployments red and black they won't come up but a to b might very similar to canary so the idea here is that you aren't necessarily tearing down um that alternate traffic you're leaving it up for a subset of users to test features um and so you know it's just the difference between temporarily or permanently then we have dark launches so this is similar to a to b testing except it's happening um like the a to b is happening at the application layer so um it's not it doesn't require a rollback or anything like that you're just flipping a switch in the code but we will go in detail and show you a diagram of each of these so that you thoroughly understand them but like red and black deployments dark launches that's not going to come up but everything else should and you should know them pretty well okay all right let's take a look here at a recreate a strategy so the idea here is that it will terminate all running instances and recreate with a new version when i say instances we're talking about pods and this is also known as an inplace deploy that is generally the term used in clusters providers and everywhere else but here they call it a recreate and so the way you'd specify that is in your deployment file in your replica set probably in your deployment file because i can't imagine you can do it other places but in your spec area that's where you define your containers you can add a strategy and just specify the type as recreate so i'm not showing the whole file there for you but it's as simple as that so let's just kind of visualize this so we understand so imagine we have an app running and i put argo cd there but you know it's not like the perfect diagram but the idea is that we're showing argo as the deployment tool but you don't necessarily have to use it um and so the idea is we terminate all servers and there's a period where the traffic is interrupted now recreate would work really really fast so the downtime would be very minimal but it would happen uh new servers are created and uh we route traffic to these new servers so users will experience downtime it can be very fast very simple but rollback is not possible i mean it's possible to roll back to the previous version it's just not possible to roll back to running servers so i think i should just clarify that because with deployments you have a deployment history so there's definitely something we can roll back to but it will mean that we have an interruption again right so it'll be uh you know rule forward interruption roll back interruption and not rolling back to something that's running so this is ideal for nonproduction workloads or where interruptions can be tolerated all right let's take a look here at rolling updates so these slowly replace pods one by one or based on as many as you want to specify and this is the default strategy of kubernetes so imagine you have um some instances there or pods running and you terminate uh an amount of pods so in this case we're terminating one and then we spin up a nude pod to take its place so as you can see this is the new pod here uh and then we tear down the old one and then replace that one there and so we are all done and so again you can do that in chunks or sets um and you can you can have it so that it either terminates first or if you want you can have it so that it spins up a new one and then tears down the other one which we'll talk about here in a second so with rolling updates you have reduced availability and i put an asterisk there because it really depends on how you configure it which might happen while each set of pods is taken uh taken terminated as the new ones are created rollbacks can be slow and hard deploys will be slow because it's doing one at a time or a few at a time so let's just talk a little bit more about availability because that's kind of an important thing that might get glossed over here that you need to consider so um available what is availability so the quality of being available to be used or obtained so if there's not enough capacity like memory cpu bandwidth to meet the demand of traffic then users can experience degraded delayed experience or no access to services at all so if you have two servers and you only have one then you know uh that means there's more traffic going to that single machine there may be enough cpu but you know it might make the machine slow or the the app might hang completely so that's where you have to be careful about availability so uh there are some values not sure about the graphics not showing that there we go that you can set the first is max surge and then we have max unavailable so max search says the amount of pods that can be added added at a time and so that means if you want to replace two at a time you can do that and then max unavailable so if you were to set this to be two and two that would ensure there would never be a drop in availability um but the deployment would have to first because the total would have to first create the new pods before tearing down the old ones so i don't know for sure 100 how this works like uh when i say that i just mean that i haven't been deploying a ton of these in practice i mean this is the default behavior but you know we don't observe it to say okay was this server there was this server not there but i do know that if you were set two and two or match it that you wouldn't lose any availability there okay all right let's take a look here at canary strategy so the idea here is we deploy a new version of the app into a new pod and serve it to a subset of existing users if there is no error or bug that has occurred then the rollback changes to all users by replacing old pods with new pods so imagine you have a bunch of pods let's say we're using argo because it probably make it a lot easier and the idea is you create a bunch of new pods now i show it like as if they're replacing ones but uh you know originally like when i think of canary usually the default behavior in other ones is that it tears down some and replaces them so might do an inplace or add new ones and stuff like that but the way it looks like kubernetes works is it actually spins up new ones alongside them so you know that's just kind of a bit of a difference there but generally the concept's the same so we'll be fine here but the idea is that uh we're gonna send a portion of our traffic um to these new pods and if we are happy uh with the traffic like there's no problems everything's going fine then we'll roll out uh to all the rest meaning like tear down the other ones and replace them with pods with new versions so we got fast roll out because you know when you look at um a roll with a like rolling update it's just like a few at a time where this one will just like completely switch all the ones over very quickly uh it has a slow roll back it should not have a drop in availability because even though it looks like it's replacing these ones here these actually just be additional to these okay if that makes sense um and so canary will use the load balancer weighted rules to only send an amount of traffic to the carry pods in the original pods just so you know how they're doing that balancing there but there you go all right let's take a look here at blue green deployment so blue green is when you completely create a new environment of all components and you send all traffic to the new new or green environment and if it's okay then you terminate the old blue environment if anything goes wrong you can roll back to blue and tear it down so let's kind of get a visual to understand so imagine you have uh two pods and you define that to being the scope of your environment right and that's the key thing with blue green is that um you know you could consider a set of pods to be an environment you could consider like a node with everything running it on a environment you could consider the whole cluster um a an environment so it really depends on your scope but the way that i was reading about blue green for kubernetes is it we're generally talking about in cluster components defining an application workload but just understand that scope's going to change and the mechanism that you swap on will change but the idea is we have whatever it is that we that we have as our our application and its surrounding components and so we spin up complete identical so we wait till it's ready and then when it's ready we switch over to it and if we like it if everything's okay then we tear down the old one if we don't like it we can just switch back to the old one and so the idea is that you might have to wait a little bit a while for the new environment to spin up but it's going to be faster because it's not doing one at a time and it's a lot safer to move back back and forth so blue green is very popular but again if you need to roll things out really really fast the deployment time can be possibly slower but it really depends like how many pods you have so zero downtime no reduced availability slow to deploy but faster than canary if something goes wrong larger impact to users immediately instantly roll back to previous infrastructure so if there's a problem they're going to be able to resolve it extremely quickly so there you go all right let's talk about a to b testing or red and black uh deployment uh red black i rarely ever hear but i mean the concepts the same here so the idea is that this is similar to a canary or blue green but the method of deployment uh is the same but it serves the new apps experimental features to a subset of users based on a set of load balancing rules so the idea is that you might you'll have 60 percent of your app on the old app and 40 on the new app but the idea is that you're not tearing this thing down it's not like okay this feature works related to everybody it's let's leave it up for a good period of time and so that's the the context or key difference between a to b versus canary and uh red and black versus blue and green is that it's your intent to test something over a period of time and you're not necessarily rolling back on it at least not immediately okay all right let's take a look here at dark launches this definitely will not be on the exam but i thought it was interesting and we should include it so similar to a to b testing except a to b happens at the application layer so within the app code so let's say you want to test a feature on a subset of users you code a feature flag in your app to turn on the new feature on and off if the users like it the feature you leave it switched on if they don't you just turn it off and then the next deploy you could remove the code if you wanted to doesn't require you to roll back at the infrastructure level so fast rollbacks so just imagine you have your applications running on pods and you could just flip things on and off when need be so there you go let's take a look at how deployment history command works so you can check the history of the previous deploys with the following command cube ctl rollout history and then the name of the deployment to deploy hyphen the deployment name uh you don't need to have a forward slash in the middle there uh it's just an alternative syntax i'm just pointing that out because sometimes it trips me up and so here if we did that we could see the previous revision and when i did this repo uh deploy i had to do this flag here even though this is deprecated uh because i uh otherwise it wouldn't show the cause of change so i would imagine that if you have uh automated systems deploying stuff like that then maybe this would be good in terms of description but this is a way to see your history okay let's take a look at deployment rollout status so you can get the status of your deploy with the rollout status command so very similar similar to history but using the word status and the idea there is you can see it as it's deploying so say waiting for deployment and then what it is successful say that it has successfully deployed all right let's take a look at deployment rollback so you can roll back to the previous deploy shown in the rollout history with a rollout undo so you type in rollout undo which essentially is rollback uh and then here what we'll see is um no output when we run the command but if we were to run a status you can see that it's uh waiting to do that roll out and finish and so the idea is we'll roll back to that previous version okay hey this is andrew brown from exam pro and in this follow along i'm going to show you how to set up your own kubernetes cluster for development using a lightweight kubernetes distribution so kubernetes uh generally is very hard to deploy and most people uh like in like for general use case do not set up their own clusters for production what they'll do is they'll use a managed provider like aws's eks which we might do throughout this course i haven't decided yet but if it does show up that might be something we do but that can be really expensive because you have to pay for the control plane almost all providers charge for the control plane the control plane node as we learned throughout this course is a node specifically intended to orchestrate and coordinate all your worker nodes and the pods and everything so it's like the brain of kubernetes and on aws at least it's 10 cents an hour um so there are providers like sivo and so civo is actually very cost effective pricing comparison i wonder if they have it here so if we go here they might actually compare the different prices between the providers so sivo the control plane is free and this is a guess right and this is at scale so if you're using this for learning like if you made a civo account they'll give you 250 dollars i think for uh to start with and for this course you will not end up spending all that so if you are really strapped you could use civo i'm going to be using aws but we aren't going to be launching eks right now what we're going to be doing is launching a virtual machine so that it's only the cost of the virtual machine that we're paying if you really have zero money another thing you can do is use catacota so catacota is a sandbox environment by o'reilly and so they have this section on kubernetes introduction it is not sufficient enough for this course but i'm just pointing it out if you are very very strapped and but you want to get some handson this is an option for you i would not recommend doing this as your uh as first because the problem with sandboxes is that you aren't going to experience the hard bits and the hard bits is the experience that you're trying to obtain if you go do these you're going to follow them blindly copy paste them and the problem with that is that you're just not going to absorb any information but i want to make sure that you're aware of these resources this would be good for drilling so after our follow alongs if you wanted to just make make sure that you cement that knowledge you could go back here and try catacota but i think we really should set up our own uh cluster so we have a few options we have mini cube con k3s and micro k8s and so uh there are four different ones and they are all a little bit different so minicube is known for being extremely easy to use but it requires the most resources notice it requires two gigabytes of memory 20 gigabytes of free disk space but they all kind of require at least 10 gigabytes then you have k3s and kind so k3s and kind are essentially the same thing but kind is uh it has a wrap around it so that you can deploy your cluster onto docker and so kubernetes k3s is designed to be directly on bare metal or virtual machines as you see edge iot whatever this is made by rancher so rancher is a company that provides kubernetes managed services and the thing is is that uh k3s is not uh or like the rancher's version of kubernetes is not the same as kubernetes it's not vanilla kubernetes you'll see components in the control plane here like kind and flannel these things don't exist in kubernetes so there's something slightly different here so there's that and then we have kind which is again it's like k3s it's just wrapped in docker so you can see the little boat here to emphasize that and there's micro micro k8s so micro k8 is by canonical if you don't know canonical they're the ones that uh create and manage ubuntu and so it's actually really easy to launch microcase on your ubuntu machine surprisingly because it uses snap but what's nice here is they actually will compare the difference between micro k8s k3s and mini cube and now i said k3 k3s is basically kind so just substitute that with kind because we're not going to do k3 3s we're going to do kind in these fall lungs here but let's just look at some of the comparisons so vanilla kubernetes so k3s is not vanilla as i as we saw in that diagram they had things that were slightly different that made up a different kind of control plane uh they mostly support the same things but here you can see the memory requirements micro k8s requires the least amount of memory minicube requires the most amount of memory then we have addon functionality so micro k8 is very very uh plugin driven so like it comes with nothing and you have to add everything it's very modular and that's probably why those memory requirements can go lower than those other ones mini cube is kind of like that as well uh k3s you just get what you get or kind then there's different container run times you can run on different ones there's different networking components that work with them storage is a bit different gpu acceleration is if you would want to do like something like machine learning um because you can run machine learning workloads on kubernetes there's a project for that can't remember up top my head so you can see there's kind of a difference there but they all generally kind of work the same um and it's going to be up to you to decide which one you like to use um i end up using my mini cube quite a bit but i generally like micro k8 when i can get it to work so uh just because it starts up so fast but yeah mini cube takes the longest to boot kind is very fast because it's in docker and then micro kx is like instant um which should be no surprise here but uh what we'll do here is um we'll make our way over to aws and before we can even run any of these things we're gonna have to have some kind of uh application to deploy and so what we'll do is we will create ourselves from scratch i'll show you how to do it uh a docker image and we'll host that somewhere in a container repository and we will use that uh as the thing that we will run in all these different uh lightweight containers um and so that's what we're going to do now okay all right so we are going to go build an application and we're going to use aws so you will have to go create yourself a bus account attach a credit card this isn't going to cost a lot i'm going to be very careful about spend here it will cost something but i really don't want the cost to go over 10 for the entire month while we're doing all these labs collectively like throughout the entire course but the thing i have to tell you is why i chose aws so the reason i chose abs is because it has this a service called cloud9 which is a um a cloud developer environment a cloud ever developer environment is a developer environment running on the cloud a very popular one is git pod and i would have used git pod because i really like them but the problem with git pod is that they uh use docker as the way that they run their environment and so um they're using docker with um c groups version one and in order to run kubernetes on docker you need c groups version two if you don't know what c group says don't worry it's covered in this course we do have to learn it but we could not use git pod so we had to use a virtual machine and we could have just launched any virtual machine up right we could have just went over to ec2 instances and launched one up or or azure virtual machine but the problem is is that there is more work involved launching those clusters like you have to specify the drivers and the base image and just be and you have to install docker and it became too complicated and so cloud9 just makes it really really easy for whatever reason they just have all the stuff to to to make the cluster set up uh super easy um and so that's why we're going to use it okay so what i want you to do is go type in cloud9 at the top here and we will go over to cloud9 we're going to create ourselves a new environment i'm going to call this k8s env and we'll say this environment will be used to run a mini cube kind and to build our docker application okay and so we'll go ahead and hit next steps and for this we'll create a new ec2 instance for direct access that just means that when we launch this and it has a terminal that terminal will be directly attached to the virtual machine and so this will have a virtual machine attached to it so we have to choose the size now remember minicube requires at least two gigabytes of memory and just to be on the safe side i'm going to go to a t3 medium notice that it's four gigabytes of ram uh so we'll do that and we have some options here amazon linux 2 amazon linux ami ubuntu and so ubuntu is fine but sometimes i run it with like pseudo issues and other stuff and so amazon x2 i would like to use it for everything but we can't use it for micro k8 because it we don't have snap installed so we're going to do amazon x2 for two of them and then for ubuntu we'll have to set up a separate one this will turn off after 30 minutes of you so if you forget to turn off your environment don't worry it will take care of it for you and so we'll go ahead and hit next step and we'll go and create this environment so if you've never calculated costs in aws we'll just go over ec2 pricing just so you know what you are spending when you are doing this stuff and we'll go over to on demand and we'll look up a t3 medium t2 probably costs the same thing but i'm going to look for t3 here so say t3 medium and this is the hourly rate and so the way i calculate anything is i take the adult the the cent amount that's the hourly and we go 7 30 because there's 730 hours generally in a month and so that would cost us 30 if we ran this nonstop which we're not right we're just going to use it when we need it it's going to shut off when we when we i don't need it okay so while that's spinning up we're going to see cloud 9. uh you might have this dark mode or light mode i don't know but we'll give it a moment and then we'll kind of just configure it so it's a bit nicer for us notice that the keyboard mode is vim you might be on default i'm using vim because i like that please say on default vim is very difficult to use if you're not a vim user you can also change the look of it so this is the classic dark theme there's a new flat one that i kind of like and so i want um i want this to be dark i just think it's nicer when it's dark so i'm going to go to preferences maybe and there should be themes here and if i drop down here i can't seem to find it but that's fine so there's just one that's really really nice that i like and it is flat dark but it's not letting me choose that for some reason here today which is totally fine so we'll just go back to um or maybe it's this one here uh this one ah we'll just stay on that okay we're just fiddling too much but we already have a virtual machine so this is an ec2 instance so if we go over to aws again just make sure you're logged in there and we'll sign in here we're going to go over to ec2 i just want to show you that there is a virtual machine running okay so there it is there's our virtual machine running and uh the one thing that we didn't get to choose usually when you launch an instance a virtual machine you'll get to choose so we'll say like amazon likes to we'll go to storage now we gotta go next here we just get to choose the size but it defaulted to eight gigabytes right and that's not going to be enough space for our virtual machine and by the way aws has like a new layout here so this is like another way of creating a virtual machine very confusing i don't like that they did that but um the problem is this virtual machine does not have enough storage on it if we try to create our cluster we're gonna run out of space um but we'll worry about that later on so i just wanted to point that out but right now all we care about is creating ourself a new docker image to work with so what i'll do here is i'll go to the left hand side and i'll create ourselves a new folder and this is going to be our app okay and in here what we're going to do is create a sinatra app so i'm going to do sinatra um uh docker okay there should be like a very easy tutorial for us to find here yeah this one's fine i just need the base code i know how sinatra works if you don't know what sinatra is sinatra is a very very um simple web server for ruby that's like this simple um and it's just going to be really easy for us to use okay so what we'll do here is we'll just go and copy this code here so this will be the sinatra app so we will call this server.rb oops that's a folder we'll go ahead and delete that but we will create a new file here this will be server.rb we will need a gemfile that's how we install our plugins gemfile and then we need a config.ru and i think that's everything and so we'll go back over to here and we'll just start copying so we'll go here and we'll grab the actual server code they call there's hello i'm calling mine server just because we should really call it the server you know and it's not copying here we'll try this one more time i'll right click sometimes pasting is kind of a pain here so we'll paste this again there we go and i'm just going to say hello world and that's not hello i'll be able to take that out of there we will need our gem file so if we go back to this little project here this is all we need we just need to specify a source we also need to choose a default web server so we're going to choose web brick for that i know that they left that out of that tutorial that or it's just an old tutorial configure you that is for using rackets away it's like a lightweight interface for servers it's not that important to know you just need to know that you need it so we'll include that as well and that looks pretty fine and then we just need the docker files that's what we're missing here so we'll say new file docker file and then we'll just copy the contents here actually i'll take the simpler one here i don't need all the documentation okay we'll paste that in there this is using ruby 2.7.4 so the great thing about cloud9's amazon linux is that it comes installed with a lot of stuff so if i type in ruby hyphen v it comes with a version 2.6.3 right now the latest version of ruby is probably like 3.0 or something so i go to ruby here and we go here we can see the latest version is 3.1 so just to be safe i'm going to update the latest one rvm ruby version manager is a way of installing multiple versions of ruby if i do rvm list this is the version installed so i'm going to go rvm install 3.1.0 this might complain and if the reason why is it because it doesn't know that it's up to date so i'm going to type in rbm update which is not the command but i wanted to get the actual command we have to type which is rbm gethead and that's just going to tell it to look at the latest versions of ruby so now if i do rbm install 3.1.0 it'll install that i just realized my text is really small so i'm going to bump that up for you okay so we'll go over here and i will try to find that font while it is installing maybe user settings um terminal okay so terminal we can bump up the font here okay code editor um is the font okay yeah it's fine but i think we can make it a little bit bigger this is terminal code editor here we go okay that's better all right sorry about that i don't think there's much i can do about these there might be something to bump that up but i'm not sure so ruby 3.1.0 takes a little bit of time to install um so we will have to wait a little bit here but let's just take a look at the dockerfile if you've never done docker before you really kind of need to know a bit about docker to work with kubernetes because a lot of times you are packaging um images to be used with kubernetes and docker is the most common one here but here we have the ruby version so this is referencing a base image from docker hub so if i was to go and type in docker hub ruby okay this is what it's referencing it by default it's going to pull from here and it's pulling from this here this 3.1.0 okay so if i click into here it'll actually show you the content so this is the uh the way they are setting up ruby for that image and then notice that it extends from bullseye so you can keep going down the rabbit hole but basically this is just a fancy back script for installing uh stuff and then packaging into a container okay we'll go back over to cloud nine it is still installing it what it's going to do is say look at the working directory and take all the code so um and then it's going to copy the code into that folder so create a new folder on the docker image called code and then copy the contents here all of these files into that directory and then do a bundle install so whatever is in the gem file install those uh bundles and expose on port four five six seven we can make this whatever port we want three thousand i'm gonna leave it as four five six seven and then uh when the docker image is is invoked like it's told to execute or run it's gonna run this command first bundle exec rack up so that's runs rack host bind on 0.0.0.0 that means to anywhere in the world you always need to bind to 0.0.0.0 very common for web apps and then it's going to uh listen or it's going to run on port 4567 so we're saying expose 4567 so that that port is open for the docker uh uh instance and then uh being able to access that okay so we will have to wait for this to finish um installing and also get back here in a moment okay all right so after a short little wait there it looks like our ruby version is installed so what i'm going to do is type in rvm list that's going to show us the versions there and notice this is the current version up here oh sorry it is set to this current one but i'm just going to say rvm use 3.1.0 just in case and say rvm default i think it's use 3.1.0 default i can't remember what it is that's okay we don't have to default it but now we have the right version of ruby installed so now i'm going to see into that app directory and we're going to do bundle install because we want to make sure that this works before we turn it into a docker image or file right so now that that is ran we can just start it by typing bundle exect ruby server.rb so bundle exec means do this in the context of what's installed in the docker file ruby to execute the ruby file server.rb and we'll hit enter and we'll see what comes out so it looks like it's running i'm just going to close that tab there and if we want to quickly test now we could open up a port to see if it's working but i'm going to open up a new terminal and what i'm going to do is type in curl localhost 4567 and it says connection refuse so i'm just going to check to see what port it actually started up it actually started on port 8080 okay so if we go back here and do port 8080 there it is hello world and that's totally fine um because when it actually launches we've actually told it to start on port 4567 okay but by default it's starting on port 80 but notice when we did a curl we got hello world so it's really copying the contents of what was being served which is html right so it's just a plain text string called hello world so that means this is ready uh to turn into a docker image so docker actually comes preinstalled on amazon linux 2 for cloud9 not amazon x2 itself but the version that is being used with cloud9 and so i can just type in docker build hyphen t sinatra sample and i think that's how we specify the name i don't do this every day so yeah there we go and that would tag it so sinatra sample and we'll hit enter and that should create ourselves a docker image so it's just downloading stuff and building it okay this usually doesn't take too long because we're really not making so much here but once that is done we will be able to go and do like docker images so i'm just going to type docker images for the time being see what comes up and so notice that we already actually have some preinstalled like python node.js and stuff like that and this finished off so fail to register layer no space left on the device ah so we forgot to resize our drive so if we type in df um h hyphen h stands for human df is like disk something i can't remember what stands for df is for report file from system disk so i don't know the name i don't know what df stands for but it's going to tell us about the disk so if we type in df this shows mounted volume so this is where our storage is and one of these is our um our primary volume and so i can't make sense of this so what i always do is i type in hyphenates which means human show it in human display and so here we see 10 gigabytes and then we have 9.0 gigabytes used and this is all that's available so based on looking at this this is the biggest volume here we know that this came with like an 8 gigabyte or 10 gigabyte drive volume and so this must be the one that we are using it's already filled up because docker can fill up drives pretty darn fast so what we'll need to do is make our way over to ec2 okay if you're not there we'll just click on databus logo here at the top type in ec2 we'll go over here and we'll just close that and we'll look for instance if you expand the name we should be able to see it kate's cloud9 that's the one there we'll checkbox it go over to storage drag up go over to the volume and we can see what size it is uh click into the volume again maybe and this has 10 gigabytes so that sounds right based on what we're looking at so this is where we're going to go ahead and modify this now if we were on the free tier video bus which we'd have to be using t2 micro which we totally can't uh there is a limit of like 30 gigabytes to stay in the free tier but we're already out of the free tiers so and we're not going to keep this around for long so it doesn't matter so just make it 40 gigabytes i know that um uh minicube said 20 gigabytes but four gigabytes because we're going to be running more than just um a mini cube on it okay so we'll go ahead and modify that and we'll click that now aws should automatically expand it sometimes you have to do some weird learning commands to do it or reboot the server but um elastic uh scaling should just happen like elastic uh volumes is something that works with uh supported images and so we're going to go here and just refresh until it's been modified i'm just watching it here this is probably the drive yeah here here's the new one 40 gigabytes and so i'm just waiting for it uh to do something so it's just optimizing i don't know if we have to wait for it to optimize but what i'm going to do is i'm going to go over back back over here i'm going to hit up and notice that it still says 10 gigabytes so sometimes i'll do this and it will show up and sometimes it requires a reboot i'm not sure if it's because it has to finish optimizing and i don't know how long it's going to take to optimize so what we're going to do is we're going to go back over to ec2 or instances and we're just going to to reboot it you don't want to terminate it that would lose all of our work we can stop this instance and we will not lose our work we can reboot this instance and we'll not lose our work but we'll go ahead and reboot sometimes reboot will take down cloud nine but if see how it's reconnecting but because reboots are so fast it should reconnect without us having to close this and reopen it okay so just give it a moment to do that we'll go back over here now if it still says reconnecting okay it reconnected we're fine okay so um sometimes if it takes too long i'll close it and reopen it i'm gonna close these things here and so now if we hit up we can see we're 40 gigabytes so we should be fine to go and build that docker image so we'll go ahead here and hit that again unable to prepare context unable to evaluate simulation docker file no such file or directory it's because we're not in the right directory when we restarted it put us back to the root folder there so we'll type that in and we'll hit up and we'll let it go again and so these were already downloaded so i guess it downloaded again but sometimes it doesn't have to download twice but we'll just give it a moment here so we'll just go over to here type in docker images notice that ruby's been downloaded says 2.7.4 um maybe that was already there because ours is 3.1.0 up here did we save this file i'm not sure if we saved it so if it wasn't saved we'll just have to do this one more time yeah so i think that was the problem there so if i hit up docker images notice like the sizes are showing up here so we might not want to have this image um let's see docker remove uh ruby 2.7.4 i don't use docker on a regular basis but i'm just trying to remember how to do it there we go and so if i hit up here just because these are all taking up space right and so yeah 2.0 2.7.4 is gone and now there's a 3.10 we'll go back here this has been successfully tagged and so this docker image if we go back over here is right here right it's somewhere on the machine um but we need to actually now put this into some kind of repository so we can use it and um there are things like docker hub and other things like that but we're going to use aws's solution because it's very cost effective and we're already using aws so why not so i'm going to go back to aws amazon.com we'll sign in here so i just have another tab i'm already signed in but just how their home page works at the top here i'm going to type in ecr and so ecr stands for elastic container registry that's a place that is like it's like a git repository specifically for docker images we're going to go ahead and hit getting started we want this to be a public repository just to save us a lot of trouble we'll call this sinatra sinatra example and uh notice we can upload this stuff so we'll say this for linux now this will be public facing so people will know where this is generally don't share that out because you don't want to be paying for a traffic going out of aws because that's something eight of us will charge you for but for uh for our purposes no one's gonna know what our url is so it's totally fine and we're gonna go ahead and create ourselves a new repository and it creates it pretty quick almost instant and then we click into the sinatra example and up here we have the view push command so these are the things that we will have to run in order to push it now when we use cloud nine this already has our aws credentials built in if you're doing this on your own machine you'd have to install the aws cli and it's a big pain in the butt but with cloud9 it's already directly integrated with aws credentials so it's going to be super easy so i just copy this line here okay and what that does is it gets our login password and stores some information for ecr okay so go ahead and hit enter so it has stored credentials so notice here it says your password will be stored unencrypted in this config.json so it's storing temporary credentials here for us to use ecr i don't think they're longlived i guess they're longlived because they're living on the machine then we do our docker build that's what we already did and then here after the bill complete tag your image so you can push the repository so we copy that we go back here and paste it no such image sinatra example so maybe i did not spell it the same way here so if we go back and type docker images and we'll just scroll up here so is this spelt the same way that's all i can think of that's wrong sinatra example so sinatra sample oh okay so what i'm gonna do because that's a bit silly i'm gonna just go ahead and delete this one we'll say docker remove image sinatra sample or sorry it's image remove i think okay and we go back to docker images and we can see it's been removed and we'll go back over here and this time we'll do example and actually just to do a sanity check will actually just copy what's over here to save us some trouble okay we'll paste that in there and this will be a lot faster because it already has the ruby 3.10 image so built a lot faster we'll go back over here we'll hit up to docker images you can see that it's there with the proper name and we'll just hit up until we get back to this tag so we're going to tag it great and we'll go back over dcr and we will go grab our next line and this will push it all right and so what it's doing is it's actually pushing it over here so if we go back and refresh we're going to wait till it shows up here go back here and watch it as well it shouldn't take too long and it's pushing all i think the layers so like every each one of these lines runs as a layer so that's all the layers that it's pushing and there's obviously layers within layers when we look in the base image there as well but it's almost done you can see it's a little bit bigger than i'd like it to be but that's what it has to be and it's pushed so we go back here we give it a refresh there it is so it's 361 megabytes so it's a little bit big we click into this here nothing super exciting you know it just shows that that it's there but now we have our docker application that we are going to run in kubernetes okay so we can go we'll leave this tab open we'll go back here and uh we'll start installing mini cube all right so it's time to go set up mini cube so what we'll do is go over to the mini cube website if you just type in minicube and getting started it'll have all the instructions here just note that you choose the right operating system so we'll go over to linux x6x8664 is the type of architecture we're using notice there's binary debian rpm we're using amazon linux which is based off of centos but we don't have that option there so we're gonna have to do a binary download which is not a big deal and we can copy it over here on the left hand side i'm gonna do each line at a time because sometimes when you do multiple lines it just acts kind of funny doesn't matter what tab we'll go to the first tab maybe we'll just cd back here for a second so we're at our top level here and we'll go ahead and paste that on in here and hit enter okay and so that has been downloaded and then the next line is going to now it's now that's down with the binary we can see that it's right here but we need to i guess extract it out and then put it in a executable location so we'll go back over to here and notice this is sudo install and it's going to install it into user local bin which is where we always put things on linux when we're installing binaries and so now i should be able to type in minicube good and so you can see there's a bunch of commands here um you know you can pretty much follow through this but i i we're not going to do exactly this but i know all the commands now since i've done this a few times so let's say mini cube start and what that will do is it will start up a cluster for us it'll do all the work for us notice that it detected that we're running on amazon linux 2 so it's that smart that even knows about amazon linux distribution it's starting up a control plane it's pulling that base image notice we didn't have to specify it's going to probably be using docker as the driver uh normally that's something you'd have to specify is like the driver in the base image but we didn't have to deal with that we didn't have to install docker so we're saving us some trouble but this would have been the base image that we would have specified as like a flag to it it does take a little bit of time to start up but it's not a big deal things that are really nice about mini cube is that you can actually use it to run not just a single cluster but multiple clusters so if you wander around more than uh one kubernetes cluster you just have to specify a different port number okay so i'll give that a moment to start here and this is taking a tiny bit of time it's not going to take forever but we'll give it a little bit of time here great and it's done so here it says cube ctl not found if you need it try mini cube cube ctl so cube ctl is what we use to interact with kubernetes um it's a cli interface and so you can install it independently i've been finding that it's been causing some troubles in some cases so i don't always install it but we can try to install it but i'll just show you here the fact that if you run mini cube ctl it will install it within the context of mini cube so we can type in mini cube cube ctl and do hyphen hyphen get pods okay notice that it's installing cube ctl for us but it'll always only work when we type in municube ctl we have no pods running if we do hyphen a it shows all possible pods so technically there are pods running because we have cube system namespace where we're running things like core dns at etsy or etcd api server control manager the q proxy the scheduler storage provisioner if you don't know them now don't worry throughout this course we're going to learn about all these components so if you come back into this tutorial uh later on it'll it'll totally make sense but anyway um so this is running and we can see all of them there it might be kind of a pain to keep on running mini cube cube ctl so i suppose we can try to install it so we'll type in cube ctl install and we'll see if we can just quickly install if we can't it's totally fine we'll just kind of dial that back there but it says download the latest binary here so we'll go here and it really depends on the version that you're using so this is stable and i'm hoping that it's the same version i don't know how to minicube how to check what version of kubernetes we're using if we scroll up maybe we might see it somewhere here so i have no idea what version is i'm going to assume that it's using the latest and we're just going to luck out and have both here if it doesn't you can always uh fall back to that mini cube qpctl so if i do ls here's cube ctl so it's a binary and so and i'm just going to make sure that we did it for the right environment so yeah linux okay we go back here we need to make it executable so we type in chamod chamod i can't remember what it stands for man chamod here it is change file mode bits okay because just because you download file doesn't mean that you can start using it you have to say that it's allowed to be executable so it's safe to use so use in the context of user a plus x is for it to be executable um as far as how i remembered anyway and we'll hit enter and so now uh it should be executable so if we type in um is it shown i just can't remember now oh yeah sorry it's just lsif and la if we type in ls hyphen la notice here that there's these things here and whether it's sex cable or not will flip i thought it was just yeah it's the x over here because there's three different user groups there's everybody there is um a particular user or something that's broken up into three that if you see that but the x's here mean that it is executable but we don't want to leave it here because then we'd have to always type this like cubectl pods which is kind of annoying like this because we can't just do this right so what we'll do is we'll move it into our use your local bin directory which is the standard place of putting things we'll need sudo because you need you need pseudo permissions in order to move cubes like a binary into that directory so we'll type in uh cube or sorry move sudo move cube ctl user local bin okay and now if we type cube ctl it's everywhere okay so that's going to make things easier for us so now we don't have to do that whole mini cube uh hyphen hyphen thing which is kind of annoying okay um but anyway we'll just clear that out there and we'll type in cube ctl get pods as you can see there are no pods running so that's what we want to do we want to deploy that and so sinatra is kind of like a ruby on rails app so i'm just going to go to google and we're going to type in ruby on rails uh kubernetes tutorial and i think honey badger has an okay one and they make it complicated because they have like all these different kinds of files so they have um like they show you how to start the cluster and they have a config map and a migrate job we just want to deploy it so what i'll do is i'll copy the deployment file they have here and by the way everything i'm doing here will end up in a repository anyway so at some point i will point us to that i'm just doing it on the fly here so bear with me here and so we have that deployment file and i want to just put it somewhere so i'll make a new folder here um i'll call this one k8s i mean you can call it whatever you want doesn't really matter new file and just say deployments the reason i'm getting confused is because normally i would call it app but i made that for the actual app itself and so this is for the kubernetes files but we'll go in here we'll paste that in and there's a bunch of stuff in here so if we go to the top here specifies api version and this is for a deployment now we could call this a pod right but generally in kubernetes you don't deploy pods directly which we will do in this course to show you but you always do deployment so that it can set up pods for you so you have an additional layer of stuff and then you will have a bunch of metadata so it's not you don't have to have this metadata but generally kubernetes recommends like they have recommended labels so if we go recommended labels for kubernetes they have a bunch here and they're actually supposed to have all of these here right like this insane amount here but um for our purposes we'll leave the name in because we will need some kind of label to do selection we'll delete out the process one because we don't need that we're gonna just call this sinatra sinatra sinatra and then we need the name of the actual deployment so the name of the deployment will be sinatra okay so you could do this to make a little bit clearer and um this should be sinatra as well and then we have our docker image which we'll talk about here in a moment so the idea here is that you have a deployment it's called sinatra we're going to provide a label as recommended by kubernetes so that if we need to select things or isolate or group things based on labels we have that then we define our spec so that's spec is specifically for pod spec files so we're specifying the template for our prospect file we have a selector so selectors are required in order for this to work because we have a deployment we need to select the correct um template and so this is a bit redundant but you have to have it i think it's like selecting from here to here we talk about selectors in the course so don't worry about it too much but the idea is we will need that then spec is actually where we define our containers we only have one container defined this one wants to run on port 3000 which is the default port for ruby ruby on rails or changes to 4567 because that's what we decided to make ours uh this is for config map we're not using config map that is for configuration uh environment variables we need our image and so for our image we'll go back to ecr and we're just going to grab the image uri so it takes basically these kind of urls here we can paste that in and we'll take that you can use that for docker hub or all other kinds of ones this one it's public so we don't have to worry about trying to authenticate to a private repository i don't even know how to do this i've done it before but i don't remember off the top of my head but i'm sure it's a pain in the butt so we'll skip that we have this image pull policy so image pull policy means every time this pod is deployed or do you pull the image every single time or only if we never have deployed it before pull it once i'm going to set it to always just because just in case we have to change it it's just going to make it a pain if we have if not present okay then we have our name here we'll call this sinatra okay and then we have our ports so container port is going to be port 4567 because again we said that that's what we're running it on and then we have a readiness probe so we'll learn about readiness probes within the course but readiness probe just says like it determines whether this is running or not so if for whatever reason our our application decides to not run like it's broken or something and the radius probe can't reach it so it's sending out a um an http request to this path and it doesn't return it a status 200 then it will fail okay so we'll do four five six seven we could not have radius pro but i'd like to have them in there so we'll leave them in there just because it was already there for us and so we have our first deployment yaml file okay super exciting again don't worry if you don't remember all the stuff even for this uh course it might be or sorry for the certification course the kcna it might be a bit overkill but you know the more we do the better the better we'll be prepared okay so now that we have our deployment file we are ready to do a deployment so we'll do cube ctl apply hyphen f and it doesn't matter if this is a deployment file a config set file or anything you're always going to do this apply hyphen f thing like apply and specify the file because kubernetes just has one way of of setting up stuff and it's always through this way here so we'll do app deployment oh sorry it's kate's right k8's deployment here we'll hit enter and here it says the deployment sinatra is invalid spec template yaml invalid selector does not match the template labels and this has happened to me before so let's just quickly carefully look at what's going on here um how did i fix this last time so the deployment sinatra is invalid spec template labels spec template metadata labels so spec template metadata labels hmm i don't see the problem here because spec template ah metadata labels invalid value map string kubernetes so it's saying this is invalid okay um what if we just delete it and see what happens okay i don't remember if i needed it or not spec template metadata labels all right so i can't remember what's wrong with it but the great thing is i already have a repository for this because i already done this ahead of time and so i'm going to go pull up our example repository this will be the repository that i will share for the course but it is just one i created temporarily as i was going through this so i'm just typing off on the side here to get to it okay and so it is this one here it's basically the same app um actually no it's not here so one second i got it on my other computer so i'm going to pull up my other computer and i'm going to just quickly take a look at what the difference is because i can't even tell here okay so i'm just going to look here starting at the top here this is fine and then under spec selector it has this so this is fine oh it's a spelling mistake and you're probably staring at this the entire time being like hey andrew you have an obvious spelling mistake i'm dyslexic i'm very sorry i cannot notice those mistakes so we'll go there but it was clear it says saying spect template metadata labels and clearly that was the problem there so we'll go ahead and hit enter and it says that it's created so what we can do now if we want to see this is type in cube ctl get deployments okay once you start using a little bit of ctl it's very very easy to use and we do cover in the course how this the the patterns to ctl but it's like if you want to know pods you type in get pods right it's very straightforward so here we can see we have a deployment says zero out of one is ready we'll go back to pods and we'll say that it is running it says zero to one that it's ready um i don't know if we can do probes we could try probes probe see i i don't know i just i was just trying there um but what we'll do is we'll type in deployment and we're just waiting for the probe to determine that it is working so now it says one out of one so it definitely is working and remember that is running on port um uh four five six seven now i don't know how to look up the ports here but i do know um that we can try to just kind of like ping it there so what we'll do is type in um curl localhost 8080 or sorry 4567 and we'll see if we can reach it okay okay so it says failed to connect to the port all right and that's totally fine because just because we set up a pod doesn't mean that the pod is immediately available in order to access that we need some kind of service okay so that's what we're gonna have to do next um or we might be able to just do port forwarding but we'll figure that out a moment i'll be back in a second okay okay i'm back so all right it was what i thought it was but what we were doing was we were doing port forwarding so when you have a pod running kubernetes you uh by default it is not available like it's uh externally uh the pod has a dynamic ip address and so normally what you'll do is you will attach a service to expose the pod or have a way of getting to the pod right so there's no port and load balancer and things like that but a very easy way without even having to use a service is to just use uh port forwarding so there's a command if you type in cube ctl and we type in hyphen hyphen port forward or sorry not we don't need the hyphen hyphen that was when we're doing minicube so we type in port forward and then we can specify that we want this deployment called sinatra um which is our deployment file yeah it's called that there here and what we can do is say forward port um i'm just trying to think like usually port 8080 is what you have to use cloud9 if you want to expose it to the internet so do 8080 and we'll say 4567 and we'll do address space 0.0.0.0 you have to bind to 0.0.0.0 or we can't see it outside of this cloud9 environment like into the web browser so hit enter and so that is forwarding for forwarding um that port okay i know it looks backwards because you'd think like four five six seven would point to eight zero eight zero but that's just how they showed okay so now what we can do is go back over here and if we do a curl uh we'll just say localhost 8080. now that's saying hello world i think if we do um hyphen l it might show us more information nope okay i was trying to see if we can see like html as well but i guess it really is just text but let's say we want to see this in our web browser so for cloud nine um we need to expose port 8080 so because by default the security group is not open on the ec2 instance so we go back to our ec2 again if you don't know where that is type in ec2 at the top here is that cloud9 environment we're going to go over to security inbound rules and this is all that we have is we can s or sshn which is not very useful so what we'll do is click onto the security group and we're going to go ahead and edit the inbound rules we're going to add a new rule and this is going to open on port 8080. now cloud9 can only run on like in the open on port 8080 and 8081 so we couldn't make this 4567 if we wanted to or 8 000 okay so here i'm just going to say my ip so the idea is that only this computer that i'm on right now can access this that's a very secure way of doing that and just say andrew's computer for some reason you can't put hyphens in here and exclamation marks but we'll hit save that rule and so now that port is open so we can do is go back to ec2 however you want to get there click on instances and what we need is the a public ip address of this so we go to the networking tab and there's that public ip address and we'll go to the top here we'll paste it in here and we'll do 8080. and so there it is it's exposed to the internet now this is obviously not what you do in production but this is just a very easy way uh to do that so we didn't cover services which is fine we'll cover that later because right now we're just focused on learning how to use uh you know uh like launching a cluster and stuff like that there's one other thing i want to show you with minicube and um that is the ability to run a dashboard okay so um there's a thing in kubernetes called the kubernetes dashboard kubernetes dashboard okay and it's like a plugin i'm just trying to find it here okay see this and it's not specific to minicube it's just it can run as a pod and then you can access this and it's just kind of like a nice way a visual way of seeing the stuff that is running in your kubernetes cluster and so uh you know we have figured out how to do that it was quite the pain to figure out how to get this exposed um on um cloud9 but i do know how to do it so the first thing we need to do is we need to stop our cluster okay so i'm going to type in cubectl or sorry minicube stop because we need to start this and make sure that it's listening on 0.0.0.0 remember i told you if you're not listening on that things can't reach outside cloud9 it's just it's a very common um thing that you need to do is bind on port zero zero zero even when you're deploying a real world web application you have to do that so what we'll do is type in mini cube start hyphen hyphen listen address and then here we're going to do equals 0.0.0 if you're running like how do i know all these commands i just go and type in mini cube mini cube start commands and then here this is how we figured it out we just went here into the docs nope that's not it maybe it's this one here here it is and it just tells you what all these flags did so we went through this and said there must be some way to bind right and so that's how we found out that you could do the bind that way okay so what we'll do is we'll go ahead and hit enter and so that will make it so that our um that we're binding on there notice that is restarting everything pulling the image i don't i think that since we stopped our cluster our deployment might be gone so we'll have to wait for that to start there but once that starts we'll have to also set up a proxy a kubernetes proxy so and make sure that that address is also on 0.0.0 and then also disable filtering because there's just so many preventative measures and proxima that we have to do so it is running i believe so we'll type in cube cube ctl get pods okay notice that our our app is gone which is totally fine so what i want to do now is i want to start up a proxy so i'll type in cubectl proxy oops address equals 0.0.0.0 and then we'll disable the filter we'll say true all right and so that's going to start up a proxy and it's going to allow us to be able to um get things out of the cluster all right because if we don't do that we're not going to be able to do much there and so now what we can do is we should be able to start in another tab um the dashboard so we type in minicube dashboard url and by the way like if you already had a regular kubernetes cluster you'd have to install it but this is just like a shortcut that currently does for you i'm doing i've been having url because if i don't it would try to open it up in the browser right away which it can't do from cloud9 so we just want the url and so here it's going to export expose that address and so we have this address and it's really funny looking which is totally fine um i think actually we can open up port 8001 8 000 i think again in the 8000 range we can open up on cloud9 so what i'm going to do is go back to ec2 uh we actually already have a tab open here so i'll use that one here and i want to go back to our security groups and i'm going to edit inbound rules i'm going to add a new one here and it's going to be for 8001 and this is for um technically this is the web app uh sinatra app might make more sense i wrote andrew because sometimes when you're working with multiple people you're running what this ip address is for right and then this one is going to be for k8 dashboard we'll save that rule there and so i think what we can do here is copy this go up here change this to eight zero zero one assuming that's what uh we specified everywhere else i think that's what everything defaulted to place that in there fingers crossed and there it is don't explain to me like don't ask me like how these ports and all these things work i'm not very good at networking i know enough uh to get around but i just have the instructions that we figured out that works but here we can see all of our stuff so we go to cron jobs and stuff or pause and see what is working and what is not so what i'm going to do now is go back to this environment here and this tab is taken up and this tab is taken up so we'll go here and make a new terminal tab and here we'll type in cube ctl and we're going to deploy our app again okay so we'll say deployment yaml and we'll go ahead and create that and what we can do is go back to our dashboard here and look we have a pod we have a deployment one out of one it started really fast okay so this is kind of nice as a visual we can click into it we can get some some kind of data with here which is really nice okay and so with that running now um it should oh we don't have port forwarding so if we went back to this here right okay and we tried 80 uh 8080. okay we're not going to see anything so we'd have to go back and forward that port so we'll go ahead and make a new terminal tab cube ctl port forward i'm trying to do based on my memory here um address actually would be we have to specify it so deployments sinatra and it would be addressed uh 0.0.0.0 and then it's 80 80 to 4567 okay um we'll just hit up till we get the right command so what did i get wrong cubectl port i forgot the r i always forget that and the flag is over here i don't think it really matters what side it's on we'll hit enter we'll go back over here and there we go so that is minicube okay so nothing super complicated there and what i'll do is i'll go back here and we will just stop this and we'll type in cube ctl i don't know if it's still running let's go back here is it still running does that stop the do that stop it cube ctl get pods okay so it still is running so we'll type in mini cube stop and i'll power that down there just stop these other tabs here we'll close them for now close and we'll close and we will close okay and so that's mini cube and so we'll move on to uh doing kind next okay all right welcome back and so this time we're going to go ahead and use kind so kind seems very similar uh to mini cube so it'll just be up to you what you decide that you want to use for your local development but what i'll do is just to make things a little bit easier i'll just close some of the tabs we have here let's just do a little bit of cleanup here i don't need a mini cube open up here and so we'll take a look at kind then we'll close some of these tabs here so kind as a tool for running local kubernetes clusters using docker container nodes primary design for testing kubernetes itself but may be used for local development and ci a lot of people like it because it's like a lighter version of mini cube and can be very very fast though a mini cube can be very robust and useful in a lot of cases it just depends on what you need and how small you want to go here so to install it we'll go over to the user guide here and i believe the way i installed it was from a binary but we'll go to installation here we'll take a look here so notice we'll see package managers like brew that's for mac os choco which chocolate which is like brew but for windows mac ports which is an alternative and then for linux it's just going to have to be a binary which is not a big deal so we'll be copying these lines here so we'll go back over to cloud nine here uh oops i forgot to copy the lines here but we'll go and copy one at a time so we'll first curl and make sure we're in the environment directory there you might have to cd back there to do that and just type clear again we're having some funny business where it's just showing that top line there we go which is fine i guess uh but we'll paste that in there and we'll go back to the quick start and we'll do chamond notices plus x you could do u u plus x that's just what i'm used to doing but it doesn't really matter and then notice it's going to say some kind of directory but the directory we want that to be in is going to be the user local bin so we'll go here and we'll type in user local bin and this is not going to work unless we do this okay and so i should be able to type kind and there it is okay and so now that it is installed we'll go back to quick start and we need to create our cluster so creating our cluster is very straightforward it should be a concrete cluster we can even name our cluster if we want but we'll just go ahead and do that so we'll say kind create cluster and we'll give it a moment there and it should start up a cluster now remember we have cube ctl already installed so it should be pretty easy but i think if we didn't we could probably do kind cube ctl as well um but anyway this is supposed to be faster than minicube um and we'll give it a moment here it's not feeling much faster but when i was using it before i felt it it felt really blazing fast but i'm not sure what's going on it is taking some time to create that control plane which is totally fine um oh there we go so installing c c and i storage class things like that so set cube ctl context to kind kind um oh it did okay so you can now use the cluster within here so cube cluster info to get some information so we'll do that so it says kubernetes control plane is running here core dns is running here to further debug and diagnose cluster problems use cube ctl cluster info dump so if we type in cube ctl and we were to type in um get pods we can see there's no pods we could do hyphen a notice that it has looks like a different amount of pods so we have core dns and then we have maybe this is a proxy i'm not sure here uh we have a control plane running uh we have kind net we have api server control manager proxy scheduler path provisioner local path provisioner not sure what that is although i guess oh storage path okay and so you know you can just see like it's not one to one with mini cube right so there's going to be a different amount of pods doing uh slightly different things but now that we have it let's just go ahead and deploy our application like we did before so what we'll do is we'll type in um cube ctl and that's in our k directory so we'll say k uh oops apply hyphen f k8 deployment right and so that should go ahead and deploy that and now if we want to see that we can type in cube ctl get deployment and if a deployment ever messes up what you can do is you can type in describe you can get more detailed information and you can type in something like sinatra works with pods works with everything and sometimes you get a lot more information so here we'll be saying like scaled up the replica it'll say what it's up to and a lot of additional information so if you ever have a failure always try to describe to help yourself out but we'll say deployment and we're just waiting for it to get ready and it is now ready um so that is started if we do a curl localhost 8080 um notice there's a connection refused so there's has to be some way for us to access that um i'm to remember cube ctl is a command that yeah it's not specific to uh to kind or mini cube so we can probably do that as well so we'll type in cube ctl port forward don't forget the r there i always forget it deployment sinatra 8080 4567 because that's what it actually runs on and then we'll have to bind it to uh zero zero zero so zero zero zero zero enter and so now if that's all set up we should be able to open a new tab here terminal and we'll type in curl local host 8080 andrew brown there we go i never tried to set up kind for a dashboard so i'm not even sure how to do it we solved it for mini cube and it was really hard to solve just because we're running it in a virtual machine to expose it um so i'm not sure if we need to do anything beyond that but that's just kind there so it's not super difficult i'm going to take a look here to see if i can figure out the dashboard i'll be back in a second okay all right so i looked it up and it actually is a bit involved so mini cube definitely makes it super easy but like here it's just a medium article where they go through that process and so they say okay we create a kind cluster then we are going to use helm helm is a way of packaging uh applications together so that could make our life a lot easier if we tried to use helm but that's a little bit farther in the course i don't know if i really want to dig into that but the idea is that you would add a repository like this github dashboard so kubernetes github is a repository like the dashboard is if we go like kubernetes dashboard dashboard git hub it should probably have one here so it's pointing to this basically right and in here there probably is a helm package somewhere here and but anyway so the idea is that you do that and you do helm install dashboard kubernetes dashboard kubernetes dashboard so it looks like we it also creates a new name space and then you'd have to set up a proxy and we had a proxy before and then you'd have this link and then you'd have to authenticate because you can't just log in you have to have a token or something and so then to log in the dashboard they're creating um a service account so that's like a type of role like permissions that's rbac and i would imagine that i mean they didn't activate it but they might have to activate rbac so you can see it gets really really involved right so i think that um i mean this would be good to do but maybe not right now because it's very complicated this is more like ckad or cka kind of material but i just wanted to show you uh the effort would be to go get that there so maybe that's one advantage of using mini cube and probably will stick to using mini cube but for kind we pretty much did what we wanted to do with it just to see that we can install and it was easy to use so i'm going to type in kind delete cluster i think that's probably what it is i'm just guessing and it still works that's great and so kind to solve the last one i want to do is micro k8s okay so that's what we'll do next okay all right so we're going to take a look at how to uh run micro k8s and i found this one like it was very fast i like how it's modular but i found it the hardest one to use to be honest i mean they're all generally pretty easy but the complexity comes because we're running in a virtual machine like a developer cloud developer environment in the cloud so there's additional stuff that you have to consider um and so you know if we want to install micro k8s we'll go to the docs and uh we will go and look for i thought it was like on the homepage they explained even how to install yeah you go to linux and right away they go snap use snap if you don't have snap get snap installed i'm not installing snap uh that's like a package manager i'm not installing it for our amazon linux 2 and since that sounds like a nightmare but what i will do is go to our dashboard and i'll spin up a new instance so um we don't need this one right now i'm going to leave it open and so i'm going to go ahead and create myself a new environment this is going to be micro k8s env this cloud developer environment env just to savor some trouble is specifically for micro k8s and we'll see if we have a little bit less problems ubuntu is great it's just sometimes they render problems with it so it takes the least amount of resources but still i'm going to run it on a t3 medium because i just don't want to have to deal with any kind of like having to delete it recreate it run any issues so here we have ubuntu server 18.04 lts so this one will have snap snap was introduced in like 14 it's like a replacement of uh get install or app install or whatever you want to call it so we'll shut off after 30 minutes that seems fine to me leaving that on the default there and we'll go ahead and create ourselves another cloud9 environment again i would love to do this and get pod but unfortunately can't run kubernetes clusters there right now but we'll take a look here and see what is being suggested so it says pseudo snap install micro k8 hyphen hyphen classic so if there's something that's not classic i'd love to know but i don't know much about snap so give it a moment to start up here and then we'll end up copying this command doing that command but notice here like turn on the services that you want notice with the other ones that we didn't have to turn anything on like we didn't turn on the dashboard we just said start the dashboard and maybe it wasn't installed it just pulled it at the time of when we needed it but with micro k8s you can't turn on the dashboard unless you enable it right so everything has to get enabled so when we start this up it doesn't even have anything right and maybe we'll see that when we look at the pods okay so we'll just wait here for cloud nine to start another way we can observe this is we go over to ec2 we go to our instances we can see them here and it gets kind of confusing because um it doesn't show like the uh what it is but like we don't have to go based on the name so i don't think it'll say here like ubuntu anywhere right so if we go to our ami or amazon machine image that's the thing that's running it oh down here it says cloud9 ubuntu so we do have an idea what it is okay but generally sometimes you can't tell what it is if it's not in the name like they didn't write it in the name go back over to cloud9 it is ready everything is nice and big which is nice this popup is driving me crazy we'll close that here we'll copy this and we'll go over to our new environment and we'll paste in sudo snap install micro k8 classic now this is an amazon x2 so it's slightly different this is kind of nice i like this experience with snap and it will install good like i think like when you install it it actually starts the cluster up right away because it says check the status of community service so you don't have to say start it just as it's installing and it starts at the same time so i'm surprised i would think they would just like install it and then you'd start it separately but i guess that was their decisions there and while it's waiting we'll just take a look here see if there's anything else interesting i don't think i dug too deep into the docks for microcades i'm mostly familiar with um mini cube and i think that's what we're going to be sticking with throughout this project nothing's super exciting here we'll go back over to michael k8s and it's still taking time so once this is done i will see you back here in a moment okay so after a short little wait it looks like it is installed so we'll go back over to the home page and look at the instructions here so we have micro case status weight ready so wait until it says it's ready and it says insufficient permissions to access it i like how they're like sudo here but sudo not here okay so we got to go ahead up actually has some instructions you can either try again with sudo or add the user ubuntu to the micro kh group i'd rather do this i did this before and i did not have much luck but i'm going to try to follow through with it okay it says after this reload the user group either via a reboot or running new grep new group micro k8s okay so we'll type in clear as i was talking about where i started having issues with sudo it was more than just this but we'll see what happens here so it is running and then it tells us what addons are enabled right so this is very addon based as you can tell so high availability it's it's no so it's obviously it's running on one node so that makes sense what's enabled an h a cluster high availability cluster which is interesting because it's not highly available and then what's disabled right now so we don't have ambassador uh uh i don't know how to pronounce that psyllium ci limb the dashboard's not enabled dns is not enabled we got nothing we don't have link or d we have nothing okay interesting that we can see some stuff so like kubeflow we talked about doing machine learning so it's really nice if you want to install kubeflow you could just enable that if you want k native which is for kind of like it like set does a lot of stuff where you like scale down to zero it's the kind of server stuff you want an open source framework so it really is good as as long as you can deal with ubuntu if i spend more time with it i think we'd be using this as opposed to mini cube but i just want things to be easy okay so anyway what we'll do is we'll go back to here and we'll take a look here so it says turn on the services that you want so right now i'm not going to turn on any services and i want to see if we run into any friction without having to turn anything on because obviously there's nothing but if we type in cube ctl actually that's not even installed on this machine so here it says pseudosnap install cube ctl sure why not so this revision of cube ctl is published using classic and thus may perform arbitrary system outside etc if you understand i want to proceed run classic okay i mean uh micro kh was classic so it must be fine to do this okay so we'll type in cube ctl okay it works great last time in this i hadn't i could not get cube ctl to work but uh we'll type in cube ctl get pods uh the connection of the server localhost 8080 was refused did you specify the right host or port ah interesting it's not working yet i wonder why so i think the reason why is because we don't have anything installed we don't even have a dns installed right so like dns is the way that outside traffic can communicate or just like traffic in general can communicate within a cluster so i bet we have to enable that so we'll go over here and we will only enable we'll only enable dns so i i don't have micro uh k8 down to a t here so i'm copying paste we'll type enable dns okay probably need a dns and we'll give it a moment here notice it's say look what it's creating a service account a config map these are all kubernetes components deployment service you learn about all the stuff in the course um core dns role based access stuff restarting cubelet and dns is enabled so now we'll get getpods and it's still saying this okay i remember this before what if we type in micro k8ss hold on here okay so i i think this is what i remember i remember i installed cube ctl i got this error i have no idea i was getting that error so then i just by default it to micro k8 cube ctl and then i was having less problems so i'm not sure if we had to have dns to do anything there but it's really hard to do anything without a dns in kubernetes so we probably had to enable it anyway let's do hyphenate let's see what we have look there's only three pods there's nothing here remember how much there was when we did kind and uh the other one like mini cube so there's literally nothing we have calico cube controllers core dns and calico we'll learn about core dns core dns is what kubernetes use it's the dns service here it is covered in the course we'll learn about it calico is a very popular um it's a cat i used to have a calico cat a long time ago but um calico kubernetes is a project by uh tigra tiger i don't know you call it but they are what is used for um uh i don't i remember cni it's in the course it's for it's for um it's for something i can't remember proxy or something i'm just i'm blanking here but anyway there's only three things here so there's not a whole lot so let's see what happens if we go and those some things are pending and some things are running so let's go ahead and deploy our application so we'll go here to the left and we'll go to deployment we'll have to copy this over here and we'll create ourselves a new folder k8s don't need a folder but i'm just making one for fun we'll say deployment.yaml we'll double click this we'll go ahead and paste that in there we will save it scroll on down and so now we should be able to do mini cube or sorry same mini cube micro k8s cube ctl apply hyphen s k s application oops deployment yaml it created it so now we can hit up because this i just can't type micro k so i can't remember how to type it get pods okay remember we do hyphenate shows us all the pods right and i'm just waiting for that to become available we can try deployments as well see if it was successful i'm waiting and we might be waiting on the the probe here so the probe is what's doing the business right it's the one that's saying hey after you checked a few times then we can determine it's ready if we didn't have the probe it'd probably start up a lot faster but i don't know if we're missing another component right still pending so one thing we can do if we're looking at this we could probably put the name in here like this maybe we'll get some additional information uh describe instead describe is your friend okay so we'll scroll up aha so we see something here zero out of one nodes are available has taint node kubernetes disk pressure the pod didn't tolerate if you look this up it might say like hey you're trying to deploy into a node that's not supposed to be there or the other issue is that there's we're running out of storage so let's see where it says disk pressure like the disk is under pressure it means the disk is out of space probably so if we do dh hyphen f oh it's not even installed oh sorry d f hyphen h i spelt it wrong um we can see that we have most of it used up so that's our problem here so it's that problem from before that we had no biggie we'll go over here micro khs that's what we want we'll go to storage we'll go to our volume we'll select our volume and we will modify our volume and we'll bump that up to 40. we probably don't need 40 but i just want it to work and it says it's in use so i think we're in good shape here and if we hit up does it auto expand elastic it's called like elastic something i can't remember the term 99 percent use so we're gonna have to reconnect reboot the server here by the way if you do have to like stop and come back to this you can always just stop these by the way you can always just hit stop and as long as you don't terminate you can always start it back up and we can always pick up for where we last left off or just let the um it shut down after a certain amount of time like if you go back to the cloud9 interface you know if you go here if you close these tabs you can always just go back to cloud nine and open them there and they will auto shut down right but anyway um we need to restart this so we'll go here or sorry reboot and we'll reboot that rebooting is super fast so hopefully we don't have to close this but when it does reboot um the cluster might not be started but i guess we'll see so i don't know if we'll have to because like remember we didn't have we didn't start the cluster we just installed it but if it's smart it might always do it you know what i mean so it is reconnecting go back over to ec2 we'll just check the status here no it is running if cloud9 has a hard time reconnecting you just close the tab we'll go back to cloud9 we'll just do it this way it happens sometimes and we'll go over to micro k8s and i'll probably instantly open and have no issues if we're lucky i suppose it's having some real trouble here go back over to ec2 i'm just going to check the volume maybe the volume is not ready i think the volume is ready attached doesn't seem like there's any issues here okay so i'm not sure what the problem is but what i'm going to do is i'm just going to take a break i'm just going to close this tab i'm going to take a break and then i'm going to come back to cloud9 i'm going to launch it and it should probably work okay if you ever run into problems like that sometimes it's just worth taking a break there but i'll be back here in a bit okay all right i was gone for like three minutes i went in here press this button it launched instantly so we're back in business here now is the cluster still running i don't know so we'll go back over here we'll say microk status because i don't know if it's just going to start up when we restart the machine and that is k8s we want this one right micro k8 hit enter and we'll see if it's ready it is running okay great so we're fine um i don't think our stuff is still there so if we type in micro k8s uh cube ctl get pods oh it's still running okay cool well that's cool like when we when we uh i don't know if we did it in this but i know that when i restarted um like kind or mini cube this stuff did not persist right they were lost so it's really cool that it just everything went back up i think that's actually one of the benefits of okay so high availability low ops minimal pr there's something about the features they say like that you know yeah autonomic automatic autonomous and selfhealing right so basically you know if you restart the machine everything's back in the shape that you wanted to be which is great um like the entire cluster like that's that's awesome um so now that that's running the question is okay can we access that of course if we do curl localhost 8080 we're not going to see stuff here so this is where we're going to do our usual um port forward so port forward i'm trying to do this from memory so deployments forward slash sinatra um 8080 4567 and then we'll say address address 0.0.0.0 enter and i forgot the r always forgetting the r on that one eh we'll hit enter and so that port is being forward and so now if we open a new terminal and we type in curl localhost 8080. there it is okay so it's the same thing as before it'd be nice to see we get the dashboard working um i'm not sure if that'll be easy but we can give it a go and see see what happens here so what we'll do is go back to the homepage here and yeah i think it's just because they had a couple lines here where is it where'd it go oops that is not what i wanted oh linux we had to click on linux um so we definitely need to have the dashboard installed we don't need to register our our istio so i'm going to go back here oops first second tab here and we don't want to turn off our port forwarding but we're going to type in micro k8s enable dashboard now if the dashboard is just accessible then maybe we'll just stick with micro case i really do like micro creates out of all these um but you know we'll just see what goes here uh lots of stuff about rolebased access if rbac has not enabled the dashboard using default token retrieved with this in rebac enabled you need to create a user with restricted permissions shown in etc so we'll see what happens here um so this is going to assign it to something uh what if i just paste that in there okay so we have a token now that i think we used to log in but let's see if we can actually get into the dashboard very easily i'm not sure if this is going to be hard so we'll go and type in the dashboard proxy so that probably launches the dashboard or something here checking if dashboard is running ah and i didn't get to show this earlier but what i wanted to show was the fact that uh the dashboard is just a pod so if we go to micro k8 cube ctl get pods and we do hyphen a we should see the dashboard here so the dashboard is probably metric server like when i ran it um for um kubernetes over here maybe we're still running the dashboard i'm not even sure no because we stopped my mini cube so it's not running but um there was two pods one was the dashboard and one was the metrics but i'm pretty sure the metrics one here is the actual dashboard even though it's not showing properly here um but we did start this here i'm just trying to find um dashboard ah so micro case dashboard wait for the dashboard the dashboard will be available here at this port number use the following token to log in okay so even though we have this on port 10443 i don't know if we can open that port we'll try it for fun we'll see if we can just open that port up but i don't think it's going to work i think there's going to be a lot more work there and honestly to get the dashboard working i i needed a lot of help i had to pull my cofounder who knows linux networking a lot better for me to figure out so um we will give it a go here but i don't think i'm going to solve it we're just going to try to open up the port so we'll go here into microkates we'll go to networking and we will choose oh it does say networking i meant security we'll click on the security group and we'll edit the inbound rules here we'll add this and we'll go and put in this port number here we'll say uh k8's dashboard and we'll make it my ip so we don't have to worry about anyone else trying to get into this thing and we'll save it and so now we'll go back to ec2 and what we need here is the actual ip address so we'll go to details public ip address we will paste it over up on to top of kind and then i need the port which is here clientside http gpus to an https server okay so it has to be https i i will try to do this we'll advance it's totally fine oh this is so much easier okay great because like with minicube it was so hard to figure out how to get that address to listen on that that port number but here's this token or uh cubeconfig so we can go back over to our michael k8 environment here we'll go ahead and copy this and i'm going to go ahead and paste that in there and we'll hit sign in and there we go so we're in we can go to our pods we can see that our pod is running here the usage and stuff this looks i don't know this looks any different is this the same as the other one it seems like there's a lot more going on here i can't tell because they're not both pulled up side by side but maybe we'll say with micro k8s maybe we'll use mini cube but the idea is that we have both our environments set up and that's all i really want to cover for now so to shut these down what we'll do is go ahead and just close these and i'll go over to ec2 and we will go ahead and just turn these off so stop the instance don't terminate them stop them otherwise you're going to be having a lot of extra work here and so we have two developer environments that we can use for the rest of this course so that should be fun hopefully that gives you kind of an introduction into kubernetes uh just obviously a lot going on there but we will unravel it piece by piece throughout this course and there you go okay hey this is andrew brown from exam pro and what i have here is kind of an amendment to our lab so i did a bunch of labs where um i'm typing constantly micro k8s as cube ctl because i couldn't figure out how to get cube ctl configured properly so when i run this like get pods i would get this error saying localhost 8080 refuse and i'd look it up and it'd say you have to configure your cube config file but i could not find like the examples they were showing did not work and so eventually i did figure it out but i shot a bunch of labs where i'm typing micro k8s cube ctl and just to save you some pain i just want to show you what you can do uh to fix that problem uh and so you just do this later on when you're doing a lab so you don't have to deal with this but i'm going to type in micro k8s micro k8 config and then right angle bracket and then uh tilde forward slash um dot cube forward slash config just going to double check to make sure i spelled that right there we go and we'll hit enter and so now if i type in cube ctl get pods we're all in good shape so um i realize that you know depending on when you're watching this video you might not know much about kubernetes but just when you start seeing me type micro case coop ctl if you just want to save yourself the trouble of typing that 100 times run this and then you can just write cube ctl and that will be a lot easier okay hey this is andrew brown from exam pro and in this follow along we're going to take a look at a variety of ways that we can expose our pods via kubernetes service and we're going to continue on using the environments we set up earlier in other follow alongs in our cloud9 environment uh and just looking at the costs here it's we're only at three dollars so we're doing pretty good for this account here but what i'm going to do is make my way over to cloud9 so just type in cloud9 at the top here and if you uh turned off not not terminated but turned off those environments shut them down they should still be here and so we have that k8 environment with both kind and mini cube and then micro k8 environment i'm preferring the micro kx environment so i'm going to go ahead and open that one up just because i'm finding that one to be pretty easy to use and we'll just give a moment for this to launch here okay all right so our environment has launched here i'm just waiting for our terminal to appear there we are everything is nice and big uh and so we had deployed something prior so micro k8 is really great because if you had something running prior and you shut it down and start the machine up it should still remain so i'm going to type in micro k8 uh k8s okay it's kubernetes or cube ctl and we're gonna say uh get pods and it would be really nice to use cube ctl directly i tried looking up the solution for it it has to do something with the config file but i can figure it out so i'm just going to keep typing micro case cube ctl it's not a big deal for me and so there's that sinatra application and it should end up deploying so there we go so now it's running again so we don't have to worry about deploying uh that sinatra app again i'm going to see what services i don't think we have any but we'll just double check so i'm going to type in cube ctl get services you can also type in svc which is the shortcut but i'm going to stick to typing things full out here but on the exam you might actually see uh initializations which is kind of frustrating because they really should just always use the full names for people learning but here you can see that we do have a service here which is for kubernetes and it's a cluster ip this is the system one that always shows up here so we're not going to touch this one but just showing you that it does exist and before we actually even go ahead and launch a service let's talk about pod communication so if we go back here and type in get pod we should see all the pods that are running and so we only have one which is sinatra uh but this actually has an ip address and so what we can do is type in o which stands for output hyphen o which is a flag type the word wide and we'll get additional information so here we can get the ip address um there's the uh the node that is running on things like that so every time a pod is terminated or or stopped or whatever it will have a new ip address and because dynamic ip addresses make it really hard to make microservices because your ip addresses are changing all the time it's going to be really hard to communicate and so that's the purpose of services is that it can have a static ip that's one reason to have it but let's kind of observe what happens if we were to um terminate this pod okay so what i'll do is i will copy the name of the pod here and we will put in micro k8s cube ctl and i'm going to say delete pod sinatra okay and what we'll do is just go to the top here and i want the wide and notice that even though the pod is destroyed because there's a deployment it's going to always launch a new one because it expects there to be one pod but notice that we have a new ip address so it's not the same one the old one was ten one twenty two nine the new one's ten one twenty two thirteen so that's an example of that dynamicism happening there now if we wanted to communicate with that um like right now the ip address is here but we are not within the node and so if we were to try to type in if we go up here for a second and we were to try to take this ip address and we were to write something like uh curl localhost 8080. okay it says connection refuse because you know these are internal uh internal traffic within the cluster were not inside the cluster so how could we communicate that without a service so one thing that we could do is we could um uh basically uh a remote login into a pod and that's something that you can do in kubernetes and so one thing we can do is uh launch a new pod and so what i'm going to do is just type in service debugging kubernetes because the commands that we want are there and so if i pull up the documentation here they have a couple commands so we have this one where it says if you want to run a pod you can run this one called busybox and if you've never heard of busybox we do cover it in the course and we might have already covered it before this tutorial but busy box is basically a um it's a container and it already contains a bunch of utility libraries that we can use right and so it's just a really easy way to get started so if we go back over here and we look at this what it's doing is saying create a new pod never ever uh restart this so if we if we terminate if we like we exit the command line it will or the shell it will delete and use the image from here so it's on the google's container repository here called busybox and it's going to launch bash so we can interact with it so what i'll do is go ahead and copy this go back here and we'll type in clear and we will launch that and so what it will do is we'll launch up a new pod oh sorry we're going to type in micro k8 in the front here but it's going to launch up a new pod and it's going to enter us into bash and so if we go over here we can now type in micro ka test if you don't have a new tab just go here and open a new one here and we'll type in cube ctl get pods and so we can see busy box is running all right and we could have written our own deployment file or pod file in order to provision it it's just a lot easier to just write it at this one line because as soon as we terminate this it's going to oops we'll type exit it will then kill the pod we don't have to worry about it so if i go back to here okay see the pod is gone but what i'll do is go hit up on my keyboard to bring back that command and it's going to enter us back not did not just launch the pod but enter us into the pod so we can execute commands so we are within a pod and so technically pods can talk to other pods because now we're inside the cluster and so if we were to type in let's say the other the other one runs on four five six so if we typed in localhost four five six seven let's see if that works okay so we'll say locals4567 and we will do curl here and so oh curl's not on here so we'd have to type in wget okay so w get and that same thing as curl and so it says connection furiously remember pods communicate sorry containers within a pod all communicate on localhost but uh pods have their own ip address so let's go back here and type in o wide and we can see there's an ip address here so if we were to copy this ip address okay paste it in here put w get in the front and then do 8080 oh sorry 4567 right 4567 it's able to access that index.html so if i just do ls which lists out all the files there it is and we'll check the contents of it by typing cat index.html and there it is so we were able from one pod uh to communicate with other pod by using its dynamic ip address okay but you know if it if we kill that pod or it gets replaced uh you know that ips is going to change and that's where that pain comes from so that is what i want to show you the pod so next we'll move on to our first service which will be cluster ip okay all right so what we're going to do is take a look at launching our first service so a service allows you to attach a static ip address and also can do load balancing to multiple pods that it selects and so what i'm going to do here is go to the top and we'll type in service kubernetes because most of the code is here we can just tweak it um and i always like to show you where i'm grabbing the code from so here's to finding a service and so this is an example of one but we want cluster ip if we scroll on down here um i mean that one up there is cluster ip but i'm just trying to see if they actually have a very specific one for cluster ip nope okay so that one up there is cluster ip okay so we'll go back to the top i'm going to go ahead and grab on this one here and we'll go back over here and i'm going to create in our folder here a new file i'm going to call this one service cluster ip.yaml we'll double click into this and we'll go ahead and paste that on in there and so one thing is that we will need metadata so the the metadata name will be the name that shows up when we do uh get services so i'm just going to type in service cluster ip and then we need your selector so uh i'm actually going to leave the selector incorrect so i'm going to go here and do sinatra uh i'm going to go over here and see what this is called so this one is called uh sinatra yeah okay so i'm going to call sinatra and then down below we have the port so tcp is great uh port 80 is the port that we want to access within the cluster so we'll say 8080 and the target board is four five six seven so very similar to when we were doing port forwarding right which technically probably was creating service and also proxy at the same time um and then down below here are we still in this environment let me just type exit if we didn't exit busy box there and we'll just type in clear and so a few too many tabs here i'll close the first one um i don't care about that tab but uh so the idea here is that we will um uh provision or deploy the service and so we should be if we're within the cluster access that port 80 uh um the pod via port 8080. so what we'll do here is type in micro k8s and we'll type in cube ctl and then we will type in um apply hyphen f k 8 s service cluster i p address now notice i didn't actually specify a kind and i just kind of want to point that out before we proceed forward because notice these ones have uh there's like a kind option here so normally you're supposed to do well maybe it's not there let's go down to uh node port will be an example that shows us here i'm just trying to find the code example so under type for spec it would be here okay so if we go under here and we set cluster ip okay that's how you know it's cluster ip but if you don't specify it will automatically be cluster ip so even if we don't have this here we dilute it out it's still cluster ip and this should be two tabs two space tabs otherwise we're gonna have a lot of trouble here um and what we'll do is hit enter okay and so created the service and so we can type in micro k8s um yes or sorry cube ctl get uh svc services and so notice here that we have our service cluster ip it's showing us it's cluster ip type and it has a cluster ip address so this service now has a static ip address of 10 132 183 and 8 and any um any pods that are associated with this like that are linked through them is going to be accessed through this static ip address that's never going to change okay we can terminate pods all day and they will be the same and so you can see that it's for port 8080 okay so that's the the port that we're going to need it on the question is did we properly select the pods and so one way we can tell is if we were to go ahead and type in details so if we go micro k8 or sorry describe and then go service and then put the service name here we can get some additional information uh that was described it is described i'm probably spelling around describe maybe services oh you know what i'm forgetting cube ctl here cube ctl okay and so we get some information so it's in the default namespace notes it shows labels none uh annotations none selectors name sinatra cluster ip single stack ipv4 that's its ip address but notice down below here it says endpoints none so in the course we'd say that endpoints is the way a service links to pod or pods and if there are no end points then that's an indicator that this service is not selecting the proper pause and so there's probably a mistake with their selector which there is i purposely did that because i wanted to show you uh that end point now what i want to do is i want to now go back to our busy box example that we had earlier so i'm just hitting up to grab that and if you want to find it again just type in service debug kubernetes and get that link and so we'll enter in here whoops did you specify the right host report oh yeah sorry micro k8 in the front again so we'll go ahead and enter here and so theoretically what we should be able to do is use the cluster ip address right and type in wget and do um 8080 and it should be able to download that page but notice it says can't connect to remote host because it's not linking to anything okay so we'll go ahead here and type exit we'll type clear just to clean things up here and what i need to do is i need to update this because this is incorrect so there's a couple ways that we can update this we can just change this and do an apply um or we can use the i'm to remember the name of the command because usually i'll just i'll just update it and apply it but it's oh i can't remember let's go take a look here so we say cube ctl commands it's in the course um we got viewing updating not replace patching so probably with a patch we could probably do it so i rarely ever run patches but let's give it a go and see if that works if not we'll just update the old way or the way that i like to do it but what we'll do here is type in micro k8s cube ctl patch and then probably the service name so we'll say service and then it's called service cluster ip just to make sure i'm going to go here and type in svc so i can see the name yeah it's service cluster ip if i just made a spell mistake i'm just double checking we'll hit enter and it says must specify patcher patch file containing so we'll just say patch okay that didn't work as i thought that shows you how often i do patches here but oh here goes hyphen p okay so basically apache what you can do is kind of change the value here so so do hyphen p and then provide json um right i guess we can do it that way it's not as clean as i'd like but i guess we'll see here so i mean the other way is probably edits but it's not great to edit things on the fly i'll try patch for fun okay so just to make this a bit easier what i'm gonna do is go over here and this is the actual proper selector that we need now you know it's too much work we're just gonna do an apply because i'll do this in another video where i'll show you how to do patch properly just because it's kind of a pain so i'll do hyphen f and we'll do k8's service cluster ip it says that's unchanged oh because we never updated anything okay so what we'll do is copy this because this is actually the way we're going to select it and we'll go back here and then paste this on here oops in here and so now we will then go apply that and if we go back over to here and we do get cvs okay and then we go uh describe cvs service cluster ip we've got a spell right if we want to work we can now see that there is an endpoint all right and so that should suggest that it's now pointing to the correct pod let's go take a look at the pods and see if there's any new information there so get pods um oh hyphen wide and so notice that it says that the ip address is 10 1 22 13. so if we go up here and look at the cluster ip its ip address is 10 132 8 um 183.8 but notice that this is the end point so this one is going to point to this endpoint okay so it doesn't matter that this ip address is not the same as the cluster ip because it will be because it'll just forward this when we access it through the service so what we'll do is go back over to here it will hit up until we hit micro okay it's cube ctl and we start our busy box and so this time what we'll do is go ahead and grab the cluster ip address which is this one here and we'll go ahead and paste this in we'll do 8080. we'll do wget and notice to download the page and so we'll just do cat index.html okay and so you know that's showing that we're we're able to access that um that pod through the service cluster okay so that allows other pods to be able to communicate with a static ip address which is really useful so type in exit i will type in clear and so that's pretty much all there is for the cluster ip now i do want to show you expose because expose is a easier way or just a like one liner of being able to launch a service so i'll type in cube ctl service command and we'll see if we can get some kind of examples here and type in uh expose and so create a service for this is replicated nginx and it should be easier than that um yeah replicate that's for replication so we should be able to do it for deploy so let's see if we can try to do this uh running this one command here because that's what a service is doing is exposing pods by giving a static ip and giving it a point of entry so what we'll do is go back over here i'm going to hit up until we get to rsvcs and all i want to do is go ahead and destroy this or delete this so we'll type in micro k8 cube ctl um delete svc uh paste and then we'll go back up here make sure that it's gone it is gone and i want to try this micro k8s um cube ctl expose so it's gonna be exposed and we're gonna what do we wanna expose want to explore a deployment and so we'll need to know what our deployment's called it's actually just called sinatra so i can just type in sinatra sinatra and um looking at our cheat sheet here okay so it's going to be whatever it is so this is a replication controller set or whatever and so then it's the name and so the port and the target port that's what we want to adjust here so we'll paste that in and so our port is the port that we want it to be on the cluster and then it is the port that we're listening to for the container or the pod which is four five six seven so if we hit enter and we go back up here and hit up we will see now that we have um a service called sinatra so it didn't change the name it probably is a flag for name not sure what it would be um but the idea here is that it has a service here with the cluster ip here and so we should be able to access our um our pod the same way so we'll go back up over to this on the left hand side we'll go back and launch our busy box and what we'll do is go ahead and grab this ip address here and actually before we do that let's go and do a describe on this i'm just hitting up to find it from before and we'll type in sinatra and notice that it's pointing the end point to 10 1 22 13. and its ip address is this and so we'll go take a look at our pod again or pause with the hyphen o wide and notice that is the ip address over here so definitely is working properly but just for fun in our busy box we'll just triple check here to make sure so i'm grabbing the ip address of the service and we'll type in wget paste and we'll do 8080. notice that it downloads if we cad it index.html it shows the contents of that file so we'll type in clear we'll exit busybox and so that's just another way and so expose can be done uh you know with any other one so if you want to do node part as well i think you just put the the flag kind hyphen have inclined but i prefer to uh do it always through the yaml files because then we can take that code and commit it to our repository and have infrastructures of code or what have you there which i think is just a lot easier to do but anyway we'll go back to our services here and i just want to go ahead and delete this one here so we will type in a micro k8 cube ctl um delete svc sinatra and i've seen some people where they literally just have to type in key uh k and then you know delete svc but you know just for us we're just gonna be typing it over and over again but that's okay because then you will know it uh by heart and you'll pass your exam no problem by being able to remember all these commands but anyway um so that was cluster ip and i guess we'll move on to node port next okay all right so we are back and this time we are going to have a service but this time it's going to be a node port so what i want you to do is go to the top here we'll leave our cheat sheet open just in case we need it i want to type in service kubernetes i'll go back to the documentation here and i'll scroll on down and i'm looking for uh one for node port so we scroll on down keep going here and we have multiport service which shows that you can have more than one port which is great uh and we'll make our way over to node port so here is our node port example and so i'll go ahead and copy that and we will make a new file here and we will call this service node port yaml okay and we'll double click into it and we'll paste the contents of it and i will just bring this down here for a moment so we can see what we're doing and so this one's going to be called service node port and the type is node port which is fine and then the selector needs to be uh this label here so it actually can match and uh that's the clustering p once we'll close it and so we will paste that on in there and then here for containers it specifies the environment variable port four five six uh seven we don't need to pass any environment variables so i'm pretty sure we can can we safely delete this there's a lot of stuff i always look at this and wonder if the indentation is incorrect oh whoops you know what um we are in the wrong file i was modifying this one by accident here just making sure i didn't mess up our deployment file sorry we actually want to go to this one here okay so um service node port type node port selector app my app and i was going over deployment because i just wanted to copy the contents over here so we'll go here they're all the same so it doesn't matter which we pull from from there go ahead and paste that on in there and notice that we have um three port numbers okay so these are confusing the text here so i'm going to delete these out and we'll write our own descriptions here so the port is the port that you want it to be for in cluster so the in cluster port so or we could say the port number that resources within the cluster we'll use to communicate okay and then you have the target port so the target port is um the port that we are targeting so that would be so this would be probably 80 80 is what we'd want and this would be four five six seven right so uh the port number the container uh is listening on right because we told sinatra to start on 4567 and then the node port is the external uh external port to uh connect to the node all right so i'm just going to make it 3000 30 0001 all right so hopefully that makes sense but it'll make sense here in a moment when we actually put it into action i really wish this would default to two spaces probably is the way here because the thing with the ammo files is they're highly sensitive to indentation and they're supposed to be twospaced by default so just to make sure we don't have any more problems here i'm going to just double check here and see if i can find um the indentation levels i'm just trying to think about where that would be we'll go to user settings interface maybe indents indentation uh indent uh cloud nine setting that's not useful i just can't remember where it is honestly oh anyway we'll just have to be careful about that it's not a big deal i just don't want to spend all day on screen scrolling here or stop the video just to try to find that option it's really not that big of a deal but anyway so the idea here is we want to be able to without even launching busybox we want to be able to do uh sorry curl wget is very similar but curl is just easier to use and we want to be able to do localhost 8080 or sorry 3001 right 30 0001 and so that's the idea here is that we don't have to we can access the service outside of the cluster right okay so what we're going to do here is go ahead and launch our new node port service so we'll type in micro k8s cube ctl uh apply hyphen f service i love to type it by hand here i was trying to hit tab to auto complete it and we'll hit enter and i forgot to type the folder name here k8s forward slash notice there's a folder we'll hit enter and created the service so we'll type in micro k8s cube ctl get svc for service and there it is notice we have a cluster ip address and so that is assigned right now there is no external ip address so i guess we'll have to take a look at that in a moment here um well i guess we don't yeah because we don't have an external ip address so external ip address would be for if we set up a load bouncer because we need an external ip address for external load bouncer to uh forward it's or like to find the actual service so that makes sense why we don't have an external ip address let's see if we can just get more information on this particular service to see if there's anything new here i'm not expecting anything but let's just take a look here and see if anything is different so whoops head up here and did i type it wrong micro k k disk cube ctl describe svc service oh i typed it wrong yeah all right and so we'll take a look here and see what information we have so default none none that's totally fine it's targeting this so the selector looks correct here it's node port single stack that is its ip address uh the port here and then here it tells us the node port we can see that the endpoint is working so it is going to something all right and so now what we should be able to do is type in curl uh we'll try the localhost here and we'll do the port number here okay and notice uh that we're able to access it outside of the cluster all right so that was what was uh very impressive okay um and so the next thing that we can do here is just check the cluster ip like just as we did before so what i'm going to do is go ahead here and launch up a micro kx cube ctl with the busy box and we will go and just test this cluster so we'll go grab its ip address and we'll type in wget and then we'll do 8080 and we can see that we can access that file again okay so that is node port all right and so now that we did no port i guess the next thing to do would be to do um the load balancer one before we do that let's go ahead and just make sure we get rid of that service so we'll say get svc i'm going to copy the name so i i have to figure it out we'll hit up we'll type in delete and we'll paste that on in there and then we'll go back and hit up again and see that it's gone and so now we're ready for load bouncer all right welcome back uh so we are going to do a service and this time we are going to use the load balancer type so what i want you to do is create a new file here we'll just type service load balancer and i want to tell you that we're not actually going to be able to complete uh this service type here at least we might do it in another follow along if it is easy because we might launch uh eks and if we do then it'll be an opportunity for us to use the load bouncer i'm having a really hard time typing this balancer dot gammel okay and so we'll go over here and again if you don't know how to get there we'll type in kubernetes service to go to the official documentation and see if we can grab some code to work from and so i'm going to just type in load balancer because i just want an example of one maybe click through here and so here is an example we'll go ahead and copy it and we will paste it in to the file here and we'll change the name so service load balancer and we need the selector to match our actual application here so we'll go back here and paste it on in whoops we'll try that one more time that doesn't work it was just right click copy right click paste gives us less trouble here and so there's a few things here we have the ports so again it's like what poor is it running on so this would be port 8080 for our service target port would be four five six seven um notice that it's specifying a cluster ip address and then we have a type load balancer and then we have status load balancer ingress ip address 19202 127. so what is going on here well the thing with service load balancer it's used for connecting to an external load bouncer so the idea is that you do something like um an abs would be elastic or elb so if we go here those are the load balancers for aws so if i go here and create a load balancer we're not going to do this right now because it's not going to work if we do it this way but the idea is that aws has load balancers um there's nginx right nginx has their own called nginx plus all right so that is a load balancer that we could use or if you're using azure it has a load balancer if you're using google it has a load bouncer and the idea is that you can have the load balancer from the service provider be used to distribute traffic and make sure your application's highly available which is recommended for production use case and if you were to use one you'd be using the network load bouncer because i don't think you can use the application load bouncer in this way you always have to use a like a t3 or sorry level three slash four load bouncer um because application load bouncer is a bit more tricky but the idea is that when you launch uh this here right and if you like if we if we were using eks right so we launched up an eks cluster which is a managed kubernetes service and we were to launch this load bouncer it would automatically provision us a network load bouncer and would connect it okay and so this configuration is going to vary based on your provider so if we go back over to here and look at load bouncers notice down below it tells you that if you go here and like you look at gcp aws there are some very particular configurations or for if you want logging or stuff like that so we're not exactly going to be able to be able to do this here but we could try running this i don't know what we'd have to do for the cluster ip here um i mean i guess it's the cluster's ip so if you go to our cheat sheet maybe we get some information here if there's a cluster commands maybe we can do cube ctl cluster info and see what there is so i'll go here and type in micro k8s okay and so i mean it's running 127.0.0.1 i don't know much about this because i haven't really done this yet but maybe we'll know more when we go through here but i figured we'd just try something here i don't think we always need this value here so i'll go ahead and delete this and we'll just see if we can actually run it if it doesn't work i don't care um but i just wanted to give an attempt here to show you but hopefully you understand the idea behind the service load balancer that it's for connecting to external load balancer service we'll type in cube ctl i'm going to type in apply hyphen f we'll type in k8 service load balancer yaml and we'll scroll up here and so we can see here is that it says um the service is invalid cluster ips invalid value failed to allocate this value the provided value is not in the range okay so what if we just go back and put this value in again i'm not a networking expert so i'm not sure what we can put in here i'm going to try it one more time and just see if it even works okay so we can't do that so i'm not sure what to set for this right now but if we do get ek set up we'll give this a go but i just wanted to show you the service load balancer or just make an attempt here and explain why we're not going to be able to do this locally now technically we could probably set up nginx plus because it is open source and you can deploy it so if we did like engine x plus kubernetes uh load balancer service there is a tutorial for it on nginx right so we could do this but it's a lot of work i just want to show you kind of the diagram they here have here so um you can see that you have nginx plus load balancer going to the controller going to the pod and i guess the svc somewhere here that's not a very good diagram but if we go down here it should have all the instructions nope this is not the right one let me use this one here i have the link on my other computer i'm just trying to find it very quickly here just to kind of show you how many instructions there were maybe try this one here they're not making it easy for me so i'm just going to pop on my other computer here just so i can see what the link was that i was looking at um so i'm just gonna double check here engine x plus load balancing kubernetes service and just see if i can find the link here i think it's this one yep this is the one so if you were to follow this i think this is how we could actually uh install a load balancer to test it out but it's a lot of work and it's out of the scope of this course so i just kind of want to show you this article showing what they're doing so here they say okay we have some uh nodes here and actually that's something that we haven't even done is ever ran this command let's go back over here and take a look the nodes we have um oh sorry micro k8s okay so we have one there which is kind of interesting and then down below we've actually never ran api resources might be fun to run again micro k8s we need that in the front this is a command that does show up on the um uh exam and so i probably will make a separate follow along and show this part but this shows us all the i guess api endpoints that are available things like that if we go back over to here uh first we choose the first node and add a label to it so they label it here they're using replication controllers which i think are old i don't think you're supposed to use replication controllers anymore in kubernetes they are replaced with replica sets which is what we cover in the course okay so they basically make a replica set uh and what that is doing is launching nginx plus as a pod in your cluster okay and i guess you label it because maybe you're targeting a very particular node for it to run on and then you what you're doing is you're grabbing the docker image so you're saving it i guess they're loading it um they're configuring nginx because you probably have to configure information so you put the stuff in there you go ahead and create the replication set it's a replication controller i'm going to say replication set you can see that the pod is running so that you have a load balancer then you are getting the node information so you're getting the external ip address of it and then you're able to access it and so i guess here they're creating the service so this one i guess would be the load bouncer one i don't see the type so this is a cluster ip address because we don't see the type there um this is i cluster ip none so that makes it a headless one i'm just looking to see if they do load balance here okay so this is definitely not an external load bouncer because if it was they would use load bouncer but i guess the load balancer can i mean i guess it is but i guess load bouncer can only be used with public cloud service providers like the ones that are defined over here if we go back up to the list right so we don't see the nginx plus in the list here so i think that nginx plus is a load bouncer that you can use but i guess that would technically be an internal load balancer because these are all external ones so just to confirm i guess i was wrong with the nginx plus but it is interesting to see that there but uh yeah so the service is really only intended to be the load bouncer type is only intended to be used with external ones there but that's all we need to do for that one and there's one more type for kubernetes which is external name and i guess we'll give that a go next okay hey this is andrew brown and we are looking still at service but for the last type which is external name so what i want you to do is create a new file here we'll type in service external name yaml okay and we'll open up that file and make our way over back to our service kubernetes page that i hopefully you still have open and we'll type an external name at the top you can just search it and command f to do that and here is that example there so we'll go ahead and copy that code in and so the idea behind external name is is that you have a service that is able to communicate or talk to you something outside of your cluster so maybe you have a database that you're using like you're using on aws rds which is a relational database and so if you had spun up a relational database it takes forever to spin one up so i'm not going to do it but it would have like a host name like a domain name that you could use because you can't um you can't get an ip address from rds it'll only give you a host name like a domain name and so this allows you to say okay i'll have my database here and so whenever the service is hit always send it over to that database okay so what i'm going to do is maybe we can try it with a virtual machine because that might be a little bit easier it will have a host name i don't know if it'll work but let's try anyway okay and what i'm going to do is go ahead and create myself a new ec2 instance and so we'll go ahead and launch an instance and we'll give it a moment here we'll say launch instance here i don't know why it didn't take and so the idea is i want to launch a new machine so let's say amazon x2 and i know so we're doing it with the old experience just in case it's the future and you don't have the old view i'm going to give you the new view even though this might change but the idea is that we need a web server so i'm going to call this apache web server and we want it to be amazon x2 i really do not like this but this is how it is it's going to be amazon's 2 instance it's going to be 64bit architecture x86 uh t2 micro is totally fine we do not need to log into the server so proceed without a key pair and we need to have let's see here allow us to say traffic we do not care about ssh traffic but we do want http so that's totally fine because we want to be open on port 80 and then down below the storage is fine and then we've got to go to advanced details and this is where we're going to uh go all the way to the bottom this is all new to me too so i'm a bit confused and this is where we're going to enter our user data so what i want to do is install a apache server i've done this a lot of times but i can't remember so what i'm going to do is just go to a github and go to exam pro code because i probably have an example because i've done it so many times we'll go here and i'm just going to search probably in terraform i've done it like there's an abus apache example so this actually this example probably would have it if we go to the user data yaml those are the two commands we want okay so i'm going to copy these over and technically this is a bash script so we're going to just take this out here whoops um i don't know if i have confidence in that i'm going to go back and maybe we've done it in the solutions architect solution free the free i have a bunch of free repos here they should show up so let's take a look at uh we'll open the developer i know it's somewhere in here maybe it's in the cloud practitioner the competition is i think pretty much updated here um we'll try vbc follow along is it in here no and we have this one that one's i don't see it in here here's the developer i'm just looking for that file i know i have it somewhere here user data.sh yep that looks kind of right to me yeah this one's right so this one what this will do is it will um we don't need the ec2 user here but it's going to install apache which is called httpd i know it's confusing and here it can actually create a custom file and then start it up so this isn't the free intimus developer associate i do have a repository for the project we're working on so it will be in there as well so if we go back over to exam pro here i'm just updating as i make the course here so it's not completely completed here but i have this repository here i don't know why it's blank because i definitely uh committed code to this the other day but that's fine this will get filled out and we'll have examples there okay but for the time being i'm just going to go ahead and grab this on in here and we'll paste this in and i don't need all this content all i want here is to um i need this line so that it knows that it's doing bash okay and so we have sudo yum install apache sudo yum or sorry sudo systemctl so startup apache and if the server needs to be restarted make sure that it's there and so that should be enough to do it and so what i'll do is go ahead and launch this instance and we'll just wait for that to work there okay and so we'll go view all instances and so i'm waiting for this to be running and so the idea is that we have this we have the public ipv4 address but that's not what i want i want the the dns name okay so like this is an idea where it's like kind of like a name domain name or something so what i'm going to do is go ahead and copy this and go back to our micro k8s here i'm going to just paste that on in here and i'm going to call this service apache here they have a namespace i'm going to take that out because i want it to be in the default name space i don't want to go anywhere strange and down below here which is something funny so i'm just going to type in clear and we'll go back over to our ec2 instance we're going to refresh here and see that it is running i'm i grab the public ipv4 and just make sure that it works okay so see how it's hanging if it's hanging it's either apache is not running or the port is not open oh no it works okay we just had to wait for it to pass so when you have an ec2 instance it has to be running and then the status checks have to pass if we refresh here well it's working but usually you wait till that you get two out of two but we're impatient here and we're just getting ahead of ourselves so it does work at this address and so the idea is um we are going to we pasted that into here and so i'm hoping that this will have an ip address or something and then we will just curl it or wget it and hopefully we'll get the contents of this page so now that we have this external service we're going to type in micro k8s cube ctl apply hyphen f k8s service external name yaml okay and then we will type in micro k8s cube ctl uh get svc to see what services we have here and notice that this one actually has an external ip address now if we got the load balancer to work in the last one which was uh not ready for that yet it would have had an external ip address where it would have showed that address so that you know for the load bouncer to connect to but here we can see that this external name doesn't have a cluster ip address but it points to external ip so i guess my thought is like okay i guess the other services would just resolve to this one if it knew how so i'm just trying to think here because it is external so what i'm going to do is i want to log into the the our busy box pod here just to see what we can see okay so we'll say wget uh clear and i just want to do like a w get on google here.com yeah that's external so i'm not really sure i mean i understand why we have one but i don't understand the use case so i'll be back here in a moment i just want to try to find the reason why okay um like like what can we test to make sure that this is working as expected because i know if we do a w get here this this is gonna work right we do wget oh this is 403 forbidden um is the port not open oh it actually is forbidden yeah so i wonder why we're getting a 403 on that type an exit here if i do a curl on this no that works here huh so we got a 403 forbidden okay well i'll look into it and i'll be back in a moment okay all right so i did a little bit of reading and says when looking up the host name so whatever it's called the cluster dns service returns a cname record with the value to that so it sounds like that we will have a um an address here and so what we'll do is we will go here and type in micro k8s cube ctl um and we'll say describe svc and this one's called service external name and maybe we'll be able to see that information because that's what i don't know um and so i might be t must be typing this wrong so we'll type in uh get svc for services oh it's called service apache i'll see if we get some more information here and so if we scroll on up here we have this here so what i'm trying to understand is how do we get this so when looking up the host because i don't know if this is always just based on this pattern here okay so we have my service prod so i assume that this is the namespace so we're in the default namespace and our service is called service dot apache or hyphen apache svc because it's a service it's in our cluster it's local and so this is what i want to know so we do curl could not resolve so i mean i understand what it's saying here but i don't know uh what to do with this okay so that's what i'm gonna find out next okay all right so after a little bit of digging on kubernetes we have debugging dns resolution so i think this would be the way that we would figure it out but there are a few things here that it says that might not work well so you have to have a kubernetes cluster cube ctl it's recommended to run this uh with at least two nodes and that you're not acting as the control plane host so if you do not already have a cluster you can create one by using minicube so the thing is is that there is only a single node so i'm not sure if we can just spin up another node um and everything i think is just one node because we're not running multinodes it's a single it's a single one there so i guess what we can do is we can try to launch this pod here so this is a dns utility and so what they're doing is they're using this and then they're doing an ns lookup right and so i don't know if i can do that from the c2 instance but um this is what i would give as an attempt even if this doesn't work that's totally fine uh it's we're kind of going way out of the scope uh for the actual kcna but if this if we can't get this to work i figured it doesn't hurt to try right so what we'll do is we'll go back over here and um this is dns utils so i'm going to go back over to this here and we'll make a new file here i'll call dnsutils.yaml and i'm going to go ahead and copy the contents here and we'll go ahead and paste this in so it will launch a pods it's not a deployment it's just a pod ds new tills um and here it's pulling a very specific image so it says e uh end to end test image jesse i don't know who jesse is but thank you jesse dns utils 1.3 and then it has a sleep and 3600 so the first thing it does when it launches it sleeps pull if not presence of pull once restart policy so always restart this pod okay so we'll go back to the debugging here and here we can see that it gets deployed in fact it looks like we don't even need to have this file here we could just hold on here we haven't we haven't done an external file so i'm going to go and delete this here whoops and we'll just actually copy this if because if it does that that's even easier right and i'm typing clear and what we'll do is go over here hit enter next to the server was refused um micro k8 we'll be doing that all day here so it's created that dns util and so if we go back over to this here we should be able to do a lookup so i'm going to leave whatever the default is actually we'll look up the pod first to make sure that it does exist and get some descriptions here i'm not sure if we'll find any info interesting information about the pod but let's just do it because they're doing it too right so there's the pod we know that it's working and if we go back over to the debug page here we're going to go ahead and copy this now this is for kubernetes default so this should show us something right that's not for the service that we're looking for but we'll go ahead and paste that on in there again micro k8s in the front there if you're wondering how like i'm jumping to the front there i'm actually hitting control a okay so ctrl e oh well it's not doing what i want but control a would jump to the the back control e would jump to the front but control e is overridden by this file command here so we'll type in micro k8s hit enter and so notice that it is resolving here so we have the server and it's going there all right so now what i want to do is i want to delete this out and we have this so this is what i'm going to try see if this resolves anywhere aha and it does and look what it returns our ec2 52 201 234 550 compute one amazoninbus.com and there's the ip address so that looks like that is our server um so the core dns the dns thing that is part of the control plane is definitely resolving to there so that is absolutely working how that works on the app like how we would use that within the app i don't know but that's great to see that that works so i just wanted to make sure that we had a way to validate there so what we'll do is go back to our ec2 instance and shut this down so we'll terminate this one here okay and we do not need um the that service running anymore for debugging so i'm going to go and type in micro k8 cube ctl get pods because i can't remember the name and we'll just hit up here we'll say delete pod dns utils and that pod will vanish because there's no deployment so there's nothing that will persist it there and so i think we pretty much covered everything i wanted to cover with um the service types because there's the four types and if we have an opportunity to test the load balancer later we will do that but we're definitely going above and beyond and i'm hoping that this really solidifies your knowledge of kubernetes um and i'll see you in the next follow along okay hey this is andrew brown from exam pro and this follow along we're going to learn how to set up a kubernetes ingress object so ingress as we covered in the course is for um exposing pods externally and so it's an alternative to use from using a service load balancer and actually generally recommended to use um but you know getting one set up is a little bit tricky uh so hopefully we'll figure that out here so i just launched my micro k8 environment because michael k it's going to make it really really easy for us to do whereas if we use something else i can't say for certain if it's that easy so we have our environment here and so what i'm going to do here is i'm just going to take a look at what we have running so we'll say micro k8 uh cooper cube ctl get pause we'll see if our sinatra pod is running and it is and then we will look if we have a deployment here and we do so let's take a look at what we can find about kubernetes ingress i'm going to tell you the documentation the main page not very good uh for helping you get set up so they do have this thing here and i guess technically it is correct so that's for the nginx but the thing is is that ingress also requires you to have an ingress controller and so you don't have one by default you have to set one up so if we go to ingress controllers here you can see there are a lot of kinds so we have way to bust gce engine x and then there's a whole bunch down below here all right and so we could probably figure out a way to hook up the aws one but that might be a little bit too hard for our developer environment be a lot easier if we were already using eks it doesn't say that we have to cks with this but i would imagine that it'd probably be a lot easier and so there's nginx which is an open source button you can see there's a lot of different ones that we could use but this is probably what we'll end up using which is the engine x1 here so we'll go over here i'm just trying to find the page that has the documentation uh yeah this looks like it so um if you're doing nginx they have a bunch of different ways that you can do it uh based on your provider so we have micro k8s and so you know i showed you all the lightweight distributions but i didn't tell you that you could actually use docker desktop or rancher desktop in order to run your developer environment without using a cloud developer environment um but i'm going to tell you docker desktop is such a pain like an incredible pain to set up um an ingus controller because you have to use i guess it's not that hard but you have this very complex template here i'm just going to show you this and we don't need all of this here it's going to take that out and so this is what is required as you can see there's a lot of stuff going on here in order to deploy our ingress controller so to kind of sift through that and make sense of it would be very hard but the great thing is since we're using micro k8 if we just go up here for a moment we click on the micro case it's as simple as enabling it and basically that script i just showed you that was for the docker desktop basically does the same thing but does it in one line and we know that it's going to work we could take a look at maybe like what minicube does so minicube also has an addon so that's nice to see i'm not sure what there is for kind or if they even have kind listed here go to aws that might be interesting to see so then it'll be to expose the nginx controller behind service load balancer type so something more interesting going on there but what we're going to do is stick to our micro k8s because it's super easy so i'll go ahead and copy that command we'll go over here paste it on in and we'll enable ingress and so that will install our ingress controller so you can see it's doing a bunch of stuff and so i bet if we looked at the contents of that desktop docker file where we saw that yaml file i bet it was installing all these things so we have a namespace for ingress it made a service account it made a cluster role rbac rolebased access control there so we have a role a role binding role binding config map and a daemon set so a lot of stuff is being created there but you can see we didn't have to uh worry about it i just get like uh worrisome when we have those external scripts and we run them because there's so many moving parts but this way it's all set up pretty easy so now that we have our ingress controller what we need to do is set up our ingress resource so if we go back to the tutorial here actually this looks like the uptodate version so this is the one we are looking for sometimes you might come across one that is older but this one definitely looks correct to me so i'll go ahead and copy this and we'll go back over to our environment i'm going to just make a new file here and we're going to call this um ingress dot yaml we'll go ahead and uh paste that in here so we'll say paste and we'll just take a look at what we need to change so up above we have the ingress type uh minimal ingress that'll be its name this annotation thing so sometimes annotation is just a type of metadata that is sometimes used by tools or providers so this one is particular to nginx as you can see this nginx ingress kubernetes io so there's something with rewriting so you're specifying the root path there then there's the ingress class name so we'll leave that alone that's totally fine then we have our rules so here what we can do is type in sinatra and the path type is prefix which is totally fine and then we're saying where's the back end so we are specifying something so here it's targeting a service so if that's the case we will need a service i'm just going to double check if we can target a deployment i'm pretty sure it always has to be a service because if we go up to the diagram yeah it always goes to here so we will need a service so i have another tab open here and let's see if we actually already have something running so we'll type in micro k8s um cube ctl and we'll type in get sbc and we do we have uh service apache but that is an external name on which we're not using anymore so i'm going to go ahead and delete that one just to kind of clean up what we have here if you have it you can go ahead and delete it too delete svc but what we'll do is whoops um cube ctl micro case must be spelling it wrong today micro k8s there we go and so what we'll do is use that expose command to just quickly create a a a cluster ip service so we'll have expose deploy and we need the name of it so it's called sinatra and then we need the port so the port will be 8080 and then the target port is going to be 80 or sorry 4567 okay and so it'll say here still typing micro k it's wrong micro okay eights there we go and so now what i'll do is type in micro k8 cube ctl get svc we can see that it's there i'm just going to describe it i just want to see that the endpoints are there so we don't have any problems there is an endpoint so it must be going to the correct pod so we are in aok shape and we'll just go back up there and just type in get again there so it's called sinatra so if we go back to our ingress we're going to target these sinatra and uh the port it operates on is 8080 right and so what we'll do now is just save that and we'll go ahead and deploy our ingress so going back over here to the other tab here it doesn't matter what tab we do it on it just that's the tab on so we want micro k8 cube ctl um apply hyphen f and by the way we've been doing apply this entire time we could probably use create so apply is just does create and modify and so create it's just if you only want to create um you know like if you ran it twice it would create two resources where uh modify if it already had the same name it would i wouldn't do it twice but i guess we can try that for fun just because i haven't been using that at all and so here it says that it's created the minimal ingress and so we will now type in micro k8s cube ctl get um ingress and so here we have our ingress here and maybe we can describe it a bit more and see if there's any additional information that might be interesting to look at and so if we just scroll up actually we don't scroll up at all but here it's launched the the default namespace default backend port 80 endpoints default http backend not found um which might not be a problem i don't know if this error actually matters um and then we have four slash sinatra and here you can see it's pointing to the back end sinatra's service name port 8080. so let's take a look and see if it works so this should be all exposed on the local host and so i think we'll be on port 80 or just localhost in general so i think that we can just type in his curl localhost and notice it says not found and so nginx is working it is going to the um ingress controller which is engine x but let's now go and type in forward slash sinatra sinatra okay and that didn't work so we'll type in now 8080 sinatra okay cinatra so it looks right to me i'm just going to go double check what i wrote oh you know what it was i think what we need we might need the protocol in here because it's not going to know what protocol if it's http or https so i'm just going to try this and it's not giving us what i want it's localhost sin not true i'm gonna double check making sure i'm not spelling anything wrong here sinatra hold on hold on hmm what did i do wrong here because i definitely definitely have had this working uh when i tested before so maybe there's an issue with our cluster let's just double check to make sure our cluster is working so we'll say svc this is a cluster ip here external ip that looks fine um so i know we have our busy box that we were doing before i'm going to see if i can find that here we'll say debug service kubernetes and we'll go ahead and grab our one liner here and we'll go on back here we'll paste that on in and it says connection refuse that's totally fine we'll type in micro kx and we'll do wget oh i don't know what the um oh yeah we do know it's right here okay so what we'll do is just type in wget 8080. okay so it does work cat index html so maybe if we go back up to here maybe this has something to do with it okay default needs to be backend 80 endpoints defaults to be back and not found all right so i'll be back here in a moment and see if i can solve it okay all right i'm back and so one thing i notice is that this one sets an ingress class name and when i compared my other code that i found somewhere else it didn't have this field at all and it defaulted to public so i'm thinking that if we change this to public then that will go to the 127.00.1 i'll be able to use it that way so that's what i'm going to try here we'll so we'll try to update this so we'll say cubectl apply hyphen f k8 ingress and we'll see if that will take um so here it says missing the cube ctl last applied configuration etc etc so it doesn't like it for whatever reason i'm just going to go ahead and delete that line for a second okay it says it's configured but did it change it no it did not so i think in this case what we'll do because i just i'm just curious if we set this to public if it will do what we think we'll do but maybe what we'll do is just delete the um the ingress controller and that might fix our issue here so we'll just say ingress i think it's short for ing if we wanted to do that so we'll say delete ing ingress okay this is all called minimal ingress all right and then we'll just go ahead and reapply this and then we will take a look here so the class is now public but it's not assigning the ip address so what i'll do is just remove this here just because i notice my other one does not have it and we will go ahead and just delete it because maybe it's doing more than just that if you specify the class okay and still no i p address interesting but even though it is a public class let's go ahead and just try it anyway for fun localhost sinatra oh it works there we go okay so i'm not sure why the ip address is not showing up here 1270.0.1 but i think that class has something to do with it there's another thing where you can set the class and the annotation uh so you can see there's some variation there notice like we can do curl we can do you can put http in front of here if you'd like this is running on port 80. so we didn't specify port 80 but that is just if it's public i suppose that's where it would go by default and we have rules for http and so that is port 80 uh which is what we'd be using by default here um so that's ingress for you so hopefully that wasn't too hard but uh we did get through it and we're all good okay hey this is andrew brown from exam pro and in this follow along what we're going to be taking a look at is jobs and cron jobs so what i want you to do is go the top here and type in kubernetes jobs and this should bring us to hopefully the jobs page here maybe we can take a look at this one as well uh and so we have some example jobs okay this one's a little bit more complicated which is fine but we'll also go to the cube ctl cheat sheet okay because in the um in the course i do use to the samples from here we just type in job and so here we have one where it says create a job which prints the word hello create a cron job that prints hello world every minute okay so what we'll do is we'll go ahead and copy this first one here and you can see it's cube ctl create job hello maybe we'll just type it for fun uh just so that we get better at remembering what is what so we'll type in micro k8s uh cube ctl create job we could do apply i suppose but jobs they run and they they die so i don't think you'd really be updating them maybe you'd be updating this one and using apply so they have a name called hello so we'll just copy the rest there to make our lives a bit easier okay hello hyphen hyphen image equals busy box so that should resolve probably go to docker hub and there's one called a busy box so a lot of times we were using the one that was sourcing google but i think we could just do that as well and so it will run the echo command and it should work so we'll hit enter and it says job.batch hello created notice we don't see where the output is i'm not really sure where we would observe that but let's just take a look here and maybe type in describe okay but you can see here it says job completed completed and it ran the command echo hello world of course we didn't see it but we can see that it did pass that in as the initial command so it definitely did operate that now the next question would be to take a look at um let's just take a look at all of our jobs here for a moment but to run the one on a schedule so here says hello one out of one duration two and age so go back over here very similar um but we have this very interesting looking thing and that's a cron job so cron job cheat sheet i wonder if there's something that we can just show you really quickly here so if you've never seen a cron so cron is part of linux so the idea is that it's a it's a um a thing called cron tab and uh what you're doing is you're saying okay run this on a schedule and there's this thing where you specify the time in minutes hours days month uh monday it's a month month and weekday right so if you have the value zero here which is uh minutes um i guess that's way hold on one two three four five one two three four five so this says every hour uh so zero i guess is one that's interesting you'd think it'd be like if you put a one that'd be every hour but i guess it's zero based on how the numbers are i'm not sure if it shows all the information and oh yeah here we go zero so minutes zero to you really think this would be one because it says hours why is this every hour okay whatever uh so then we have this one here it says uh uh four slash fifteen so every 15 minutes uh then you have zero whatever so you kind of get the idea here i don't fully understand the logic here i've done this for years but i always have to look it up but just to show you if you want to read that yourself but we'll go back to the cheat sheet here if we read this they say every minute so i guess that makes sense because if you took an hour and divided it by one did you get that well anyway that's what it says it does but you know this is not so important to know for the exam but just to understand that that's where that logic comes from if you want to learn it on your own you can totally do that so what we'll do is hit enter and oh we have to type in micro k8 all right and then we'll hit uh does that look great micro case yeah enter and so what we can do now is to get the jobs oh maybe we have to do crown job for this we do so there it is and so it says last scheduled it is active i assume it's active um but i guess we'll just wait here a little while and see when it does trigger while that is going on let's take a look at what it looks like to create a cron job file so you use batch version one you have a job metadata you specify your container image name command whether it should restart or not and back off limit so you know if it fails then it will retry after a certain amount of time it probably does exponential back off i'm not sure if it's uh every few seconds or whatever so in that sense you can make a job that is reoccurring in in some sense there but yeah that's pretty much all there really is to jobs but we just want to observe this thing works okay and notice that the one minute's going to come up here in a moment ah so it just ran so it says one active last scheduled so let's see if we can get a describe since there's only one it should pull the first one there notice we haven't been specifying the name at all okay because usually you'd write the name whatever it is right and i don't know what the name is it is called hello so we'd say like hello right and let's just take a look at what we have here so namespace is default no labels no annotations that's our schedule currency policy allow uh suspend false nothing super exciting here you can see what the pod is last scheduled time so last time it ran whether there's a job that's active right now and then we have events here so it says create a job saw a completed job i would think that if this runs multiple times we'll see this kind of like a list but um what we'll do is just go back to get so probably trigger very soon uh so when it's actually active it's one and then when it's not active it's zero so this doesn't count up like to say oh it's ran four or five times it's just true or false zero or one right um so we'll do is just hit up here go back to describe because it might have already ran again yeah so here it is you can see that it's starting to um pile on there i'm just curious like uh time to cron uh cron expression just wondering if there is a tool there that we can use okay so let's say we wanted it to be every single um every second is it changing real time okay so it'd be this okay cool let's say you want to do every minute which you want to do every hour okay so i'm not saying i can make sense of this at one point i did know how these uh chronic expressions work but i'm sure if you really wanted to know yeah here they all are uh you could uh look up and try to understand the syntax there but again out of the scope of this and we understand how to run a job in a cron job i think what we'll want to do is just destroy that crown job so it's not constantly running so we'll just say delete quran job hello i'm not worried about the job one because it ran and then it stopped so that's okay there and um i guess that's it for jobs okay hey this is andrew brown from exam pro and we are back with another follow along and this time we're looking at replica sets so technically we've probably been using replica sets if we go back to our deployment file originally here again we are in the micro k8 environment i don't know if we'll ever be using mini cuber kind now that microcades works so great but um here it's not specifying the replicas account but generally if you were just to add a value i think right here like replicas you could then say okay create a replica set underneath and that's how you would create a replica but let's go take a look at replica sets um kubernetes here and see if we can specify one manually now you never would do this in practice you'd always use a deployment but just to show you that you can do it right so if we go ahead and copy the contents here of this one here and we'll make a new one and we'll call this a new file replica set yaml okay and we go ahead and paste that in there we can kind of read it and see what's going on here so we have some metadata um so we have labels like names like go say name sinatra rc and we'll just say sinatra rc and here we can see how many we want to run and then we i guess we need to have a selector um and so i'm just trying think here match labels template oh yeah probably they just these probably just have to match the template like this okay and then we can go to our deployment and just grab our container image here and i think that should work so that should launch us three pods in a replica set that are managed okay so we'll type in micro k8s cube ctl um get or sorry apply hyphen f k eights replica set yaml and we'll see if that works it looks like it worked no problem there and so we'll type in micro k8 cube ctl get uh rc for replica sets or maybe it's rs there we go and so here's the one that's for the deployment right and then here's the one for rc so even though we never specified replicas originally here it did set up a replica set for one to one and then here you can see the replicas that are is set to i wrote rc i should have wrote rs we have the desired current and ready so generally desired means like go back to that's generally what you want to be running so um again we don't really want to be doing it this way so i just wanted to show you that you could do it that way so we'll just say cube ctl delete rs and delete that replica set and just so this is less confusing if we ever go back to this file we'll type in rs i think the reason i wrote rc because the old one was called replication controller so at the top here um maybe not here but notice here this replication controller this was replaced with replica set and that's probably why i wrote rc but rs is the new way of doing it so now let's take a look at how we would um modify this file here so if we go over here and we already changed the two so i'll just save that file and if we were to type in micro k8s cube ctl update or no sorry apply hyphen f because that will do a modify and update essentially and then we'll do our deployment here and that should update our replica set to now have two so see the desired is two and so if we go check our pods we should now have a second one trying to run and it is okay all right and so that's pretty much all i really want to cover about replicas that's right now of course there's more to it um and we'll see that when we look at scaling okay hey this is andrew brown from exam pro and we're doing a follow along and this time it's all about scaling and also using hpa the horizontal pod auto scaler so just make sure you launch your micro k8 environment that we've been using uh this entire time so i'm going to type clear here and so what we'll do is look up the scale command kubernetes and so i haven't been showing this uh throughout this but there is um like a cube ctl commands a reference let's see if we can find it here and so this is really useful if we ever want to just kind of see examples and so there is a scale command and then there's an auto scale command so if we look at scale down below setting new size for the deployment replica set replication control or stateful set so you know when we did it uh in the last um like the last volume of the replica sets that we ch we changed this to replica set or replicas 2 and then did apply so this basically does the same thing um except it doesn't update this file of course our original file but it will uh update what's in the cluster so if we go here we type in micro k8s cube ctl scale and then we'll go over to our reference here and take a look at how it works um here we can see hyphen hyphen replicas so we can just specify the new amount replicas and maybe we would say four and then i'm just trying to look so i'm assuming it's going to be whatever the resource is forward slash the name so we probably type deploy or deployment and then it's going to be sinatra and so it says that it has scaled all right and so if we go take a look i don't think you can uh yeah hold on here we'll we'll go get deploy and see what it says here so notice it says two out of four already so it is working on deploying the rest of those it says two out of four ready for up to date two available ah there we go just have to take some time and it will say get pods right so we see there are four pods and then we can go get svc and then from there we could um take a look oh sorry i wanted to look at the uh replica set and there's four out of four and if maybe we'll just describe it for fun replicas that okay and you can see some information here that it says replication controller under so i guess it was a replica set controller sorry i almost got confused with replica replication controller there so nothing super interesting here but um you know that's all great so that's scale uh one thing i'd like to show you is what happens when you have an svc that has multiple um pods uh just to show you that we have all our endpoints here okay because when we looked at it before we saw one but if we have four pods one two three four we should see four and we do which is great so we looked at scale and let's take a look at auto scale so if we go up here so auto scale is going to create us um a horizontal pod auto scaler so let's type in hpa kubernetes here and see if we can find an example that's a resource don't see one here one moment let's see if there's one in the walkthrough so here they have a deployment and a service so i want to show you the code ah here it is okay so here's an example of a horizontal pod autoscaler so the idea is you're setting the scale target reference you're saying you're targeting the deployment for it you're setting the min replicas the max replicas and then you can say all right you should scale up if the utilization exceeds whatever it is beyond 50 right and so that's an example there i thought i had more like a streamlined example here i'm just going to go double check my other computer to see what i have and i didn't save it but i know it's generally this okay so this is something we can write out but there's an easier way which is using the autoscale it's basically going to just create this file here so what we'll do is do a cube cube ctl autoscale so we'll go back over here and actually we didn't scale down our pod so what we'll do is go back to our scale we did here a moment ago and i want to just put this to one okay and then we'll go and make sure that it's scaled down so it'll say describe deployment describe deployment sinatra or i guess we'll just do get because it's a bit easier to see that okay so we have one and one so now let's say we wanted to set up auto scale so that's going to be micro k8s cube ctl um autoscale and then we'll need to figure out all the other options here so if we scroll up it probably shows us a very simple example oh yeah the name right what do we want to autoscale so we'll say deployment replica or sorry uh sinatra and then here we'll base it off the cpu percent and then we can set our min or max right and what i'm going to do is i'm going to set it to 2 so that it'll automatically scale up to 2 because it'll expect there to be at least 2 and we'll hit enter and notice that it created created an auto scaler so um what we can do is type in micro k8s uh cube ctl hpa for horizontal pod autoscaler um or maybe get in front of it first and so here it says unknown targets because it's still trying to figure out the targets i suppose two mins max pods replicas we can see that it is going to the right place okay if we know that this is working we can just go get pods and see how many pods are running so notice there are two pods i'm not sure why the target is unknown i don't know if we have to have something additionally installed for it to know or this just it needs to collect more information over a period of time but we can see that this works so it's as simple as that if we were to try to flood traffic towards those containers or pods i should say then it should spin up another one we're not going to do that because it is a bit complex uh to do that and it's kind of out of scope here and so what we'll do is just get rid of this hpa that we created here actually before we do that i just want to show you the edit command which we haven't really been using much so if we do edit hpa sinatra this will open in a local editor so it's i think using bim is this vim yeah it's vim and it looks very similar to our definition file but this is actually basically the definition file and active data here so you're going to see additional information uh here and so you can actually edit this uh like you i could change the value here maybe change um the minimum to three and i think that it will take effect right so i edited it and if i go hpa notice that the min pods is now three so generally you don't need to edit things on the fly like that but it is possible for you to do that if i go over back to our pods get pods we should see that there is now three running so what we'll do is go ahead and delete hpa sinatra okay and then it should go back to the right amount of pods let's see if it'll actually do that what if we have to scale it down okay and i think the reason why is because if we go to our deployment and we describe it again we only have one so we don't have to specify the name i think it has like a min and a max in here three desired three updated three total three available maybe because we scaled it to three from before i can't remember why so what i'm going to do here is just scale it back down to one so we'll type in scale deploy forward slash sinatra and we'll just say replicas equals one and then we'll go back to our pods and now we only have one pod so you know hopefully that gives you a clear idea how horizontal pod auto scalers work they can get quite complex because there is more logic that can be implemented with auto pod scalers i don't know if we'll do a vertical pod scaler i'm not sure if that's really that necessary uh i haven't decided yet but anyway you can see there's a lot more fancier rules that you can implement for horizontal pod scaling but yeah there you go hey this is andrew brown and in this follow along we're going to be taking a look at how to use config maps so just make sure you're a micro k8 environment and so i am in mine here and i think i'm going to actually need to use the k8 environment as well because that's where we created our original uh docker file we might need to do an update there but the idea is config maps allow you to pass data into multiple pods so you have consistent data across your containers so what we're going to do here is just type in config map kubernetes and we'll go here and we'll take a look here and so here's an example of a config map so we'll just copy the contents here we'll go over here and we'll say new file configmap.yaml and we will go ahead whoops uh reload and we'll just paste this content in here so this looks a bit confusing but the idea is that you have your data and then you can have a key and a value but then you have some values that are a little bit more complex and the reason why is that um these will turn into like json or yaml file formats not super important for the scope of what we're doing so i'm just going to simplify this so it's not as complicated and what i'm going to do here well we might leave actually it in because i don't think it's going to hurt anything if we do leave it in hold on here and i'm just going to go ahead and just leave it in maybe we can view it as yaml but i want to set a message here and we'll just say um uh i remember like how do you say how do you say hello in bar soon okay so hello okay so if you don't know bar soon barsoon is the uh mars books i think it's bar soon that's how you pronounce it and so we'll say bar soon and that's basically saying like hello mars okay and so the idea is that we will update our original file to consume that environment variable so if um we'll say like hello message maybe is a better name here and actually just to make things a bit easier for me so i don't have to kind of guess what it is i'm just going to type it in all caps hello message and so what we'll do is go back to our original k environment and this is where yeah we created our app and so in our ruby app what we can do is do something slightly different i'm going to do env oops single quotations here it's being a little bit silly and so if there is an environment variable it will use that otherwise use hello world i'm gonna just put an extra exclamation mark here so that we know that it's working and so that shouldn't cause any issues so we'll double check make sure this works we'll cd into that directory here we'll do a bundle install shouldn't shouldn't need to but we'll just do it for fun um okay let's see here um could not find bundler oh okay oh sorry rvm list so rvm used uh 3.1.0 remember i couldn't default it before i just didn't want to look it up because i figured we weren't going to be going back here too often and now we'll do a bundle install make sure everything's fine and then i'll do bundle exect ruby sinatra just to make sure that it or sorry a server just to make sure that it's working correctly and then on a new tab here i'm just gonna make a new file tab here i think this is running on port 8080 yes so we'll just do a curl oops that's a file i want a terminal we'll do a curl localhost 8080 and it says hello world the double exclamation mark okay now if we were to pass an environment variable we'll just double check to make sure that works so what we can do is go to the front of it and type in hello message equals new message okay maybe double quotations here i'm not sure if it'll take double or singles and we'll go back over here and we will hit up make sure that our app works it's not taking it one second here uh maybe we don't need the semicolon maybe we'll just take that out like that there we go all right so it is accepting um environment variables so now what i want to do is i want to repackage that docker file so i think we might have wrote it in here no we did not just came with it and so i'm going to write um bundle docker or sorry docker build hyphen t or sorry period hyphen t um we called it sinatra yeah i don't exactly remember so what i'm going to do is just go over to the console here open a new tab we'll go over and go grab it from ecr just because if i make a typo here it's always a big pane to fix so we'll go to our container registry here and we will go into our app here we'll view the push commands we'll take a look here and this is the one i want so that way i'm not making any mistake okay and i'll make sure i'm in the k8 environment here and we'll go ahead and just erase that out there hit enter and that's going to build that new one and then what we can do is then copy these next lines here and have a really easy time so go ahead and paste that in and we'll go ahead and paste this in okay and so now we have our your authorization tire is expired that is totally fine if that happens we just go back to the top here and we paste that on in okay hit up up to go to the last one there and now it's just pushing the updated version there so now that that's all updated what i want to do here is i want to double check my deployment file and see if it's set to always yeah always pull the public image and so what we can do here again we're back in the micro k8s we're going to go ahead and just delete our deployment so we'll say micro kx um cube ctl delete deploy sinatra and then what we'll do is say cube ctl apply hyphen f k 8 deployment and so that should pull the latest image it didn't seem like it did though usually it would say like downloading or something maybe it is pulling it but what we'll do here is just get our pods and wait for it to run so maybe it is pulling because it is taking some time so let's say describe deployment see what's happening i don't know if we can see the the images here docker images no it doesn't show them okay um also maybe this has changed so that's something that we should just double check okay so if i go into here oh yeah that's way different okay so what we'll have to do is just paste that on in there does that say latest it doesn't say latest maybe we just click in the latest there yeah okay so it should take it i just want to double check to make sure i think because like that was a very particular version and i would i didn't click into that we pasted that in there yeah this is the same thing so um i mean we should be in good shape so let's just double check here and say get pods i just can't believe it pulled that fast right like i figured we'd see like a message like pulling the image or something so then what we're going to know is if we kind of expose this and take a look here do we still have our ingress controller running ingress oh we do nice okay so if that's the case then what i'm going to do here is just do um curl local host forward slash sinatra ah we have our double exclamation mark so it is definitely uh using the latest image so now we are set up and so the idea with our config map is we're going to create this and then we want to get this message to show up here um this will just be like config map data here name that there and so what we'll do is type in um micro k8s cube ctl apply hyphen f k eights config map i'm gonna say okay so that created that config map there and um i'm just looking off screen here because i want to see if there's a way we can visualize it as a yaml file um i'm just looking here yeah there's like a way like output so what we can do is type in cube ctl get config map to look at it for a moment here oh maybe we should say oh micro k8 it's in front here okay so we have this one which is the certification root thing and then we have config maps so we'll say get configmap configmap maybe not a great name for it should probably called it sinatra but whatever and so um that doesn't tell us much but we go hyphen o yaml now it shows us in the format of a yaml file so we can kind of look at that data and see what it looks like so we have hello world equals that game properties equals this kind of see what the structure is i suppose i'm not sure if there's like a nicer way to look at it i'm just double checking here no that's pretty much it okay so now that we have that we need to now associate that to our pod or a deployment so if you deployed a pod you could specify it there but we're doing it through our deployment and so we can either mount it as a volume or we can pass it along as environment variables and so i think what we want to do is do nvar here so we'll go over back to our reference file here and we'll just say nvar and i'm just off screen here with like this one because i know i've seen it somewhere here environment variables maybe it's in this ultimate guide here to config maps because there's one where you can mount yeah this is what i'm looking for and and and env from okay so you think they'd have that in the main it's always a lot of work to figure stuff out here so what we'll do is go back to our deployment and so this one is showing with a pod but i'm going to assume that i can do that with a deployment because why not and so i think it would probably go in line here oh we have env up here so we could say env from see i don't think we need to set this yeah so like here we could probably override it first let's just do this as a test before we bring in our config map and so we'll just say um goodbye goodbye goodbye moon okay and then what we'll do is do an apply before we do our configmap we'll just see if this works deployment k8 deployments great and then we will just want to do our curl here to see if it would take it uh and it didn't take it hello message i'm just wondering here i'm just looking at our server file here yeah it's called hello message so it should load that instead all right well that's totally fine what i'm going to do here is i'm still going to specify um an and and n bar or n from i think the reason why i see this is confusing is because it's indented here but it's actually on the same line here so this didn't work but we will just delete this out or actually we'll just change this to n from and then we need to figure out how to map it so we'll say config map ref colon space space name example or sorry whatever the config map is called so it's called config map and so i think that's how we should be able to map it again i'm just trying this i haven't done it before so i'm just hoping that it works first time and what we'll do here let's go ahead and do a deploy here and we'll see if we get an error so um and from unknown field called name all right so that didn't work as expected so we'll have to go here and look up some stuff so config map for deployment kubernetes so this again this one's for pod that is frustrating it should it should be able to go in here it just says name though oh did maybe i put a hyphen in front of it so i made an array and that messed it up which by the way um no actually the other the other resource was fine no i did it correctly just make sure that we've done this right i'm just copying off screen the contents here making sure i typed it right looks right to me just double check here we'll paste it in paste um making sure i'm getting the indentation correct here yeah that's indented and we'll try one more time maybe maybe it's too far indented there there we go and so we'll try this again okay so it says unknown field name v1 env emv from source if you choose to ignore these errors hmm unknown field okay so i'll be back here in a second and figure out what's wrong okay all right so uh staring at it for a little while here i think that the issue is that this needs to be indented so we'll try that first because i just found a stack overflow that just said hey your indentation is wrong and they have an example here where this shows more indented from this one and this one is more indented from that one so uh we'll just do both of that so this one from from the e that's where we have to look maybe we'll just do that as a sanity check ah there we go so it looks like it has now been configured so oh but it's the wrong name so we'll have to type in configmap because that's not gonna work if we do that okay we'll try this again and so it says now it's now configured so we'll go back to our deployment here and just see if there's anything observable that's interesting here so you say describe deployment since there's only one it's going to select the first one i know i said 100 times but just so you know and here it says environment variables from configmap configure map optional false i'm not sure what the optional false is but it looks like it must be associated and so i'm just hoping that if we do a localhost that's going to work now i don't know if we have to restart the pods or if it would restart the pods so i'm just looking here so 12 seconds go scale down scale up so i think it did turn off and turn back on the pods so if we go here and do our um curl local host sinatra i'm hoping that it will show bar soon oh it does there we go so for whatever reason when we set an environment variable it didn't work maybe it was indentation or something but here it's definitely accepting the environment variable for a configuration set there so that is really nice now there's another way to uh use or to to pass them along and that's by mounting it as a volume which is kind of interesting let's see if we can do that really quickly here so volume okay and so what you can do is specify a volume mount and this is on a pod and assume it works the same way you just have it with the containers and then we specify a volume so i think what we would need is a volume mount here i don't know if that would be enough for it to work so go back to our micro k8s here and we'll just scroll on down here hit enter paste that on in and making sure that is right we didn't grab it all did it oh it's right here okay and so i think that would need to be the same level indentation here and so we'll indent here and we'll uh now that indentation's fine and so we have name config which seems fine uh we'll just have a config map we'll go to config actually i think we would have to associate so we have a volume mount and then we'd have to actually have the volume to associate with so we'll have to go back to here and then the this is the volume so we'll go and go here and then paste that on in and here it looks like it's just doing it in line so set the volumes at the pod level then mount into the containers inside the pod provide the name of the config map you want to route an array of keys to uh to convey to create as files okay cool so now going back to this it kind of makes sense that when you have this this would be isolate files and we have all those in there for example so we can give that a go in terms of indentation level we need to figure that out so this one is at the same level as containers so we will go up here and hit back and follow the line so it has to be here there okay and we'll go down and i think that's okay sometimes like when it's like this you have to have it the same line indication is kind of weird and so it says here so this one's called config map an array of items array of keys from the configmap to create as files okay so what i'm going to do is i'm just going to comment out our old one here so i kind of want to leave it in just in case we want to pull it back up i'm just going to put image right i'll put a name at the top here okay and then i'll just comment this out and then i'm going to go over to our config map well we'd have to update our config map for this to work so what i'm going to do here is go exclamation mark exclamation mark and then what i want to do here is um yeah this all looks fine so i don't know if this is going to work because we don't have we didn't specify this as a file since it says an array of keys from the configmap to create as files so i'm going to assume that it's going to default back to hello world okay so now that we have this and assuming we have got this all right um we'll go ahead and deploy let's see what happens so we'll type in micro k8 cube ctl apply hyphen f k8 and deployment.yaml and so that's going to update it and then we will do our curl it still it still says um caror bar soon because of course we did not push this so that's why that's not updated and i don't know if it would actually restart the pods for that if it mounted the volumes or maybe it's still doing it so let's go and look at what it's doing so we'll describe it and here it says nothing fancy i'm just taking a look here ah so it does mount to config which is great and we have config map here okay and so i don't think it's um the other original config map i don't believe is there anymore so it shouldn't be saying that so i'm guessing that it didn't restart the pods let's just try again oh there we go so it probably was just shutting down the pods right so restarted so it definitely is uh loading the other one and it doesn't have that environment variable so now what i'd like to do is see if we can get into this machine and be able to view that mounted directory so we'll look it up like kubernetes pod ssh all right and it's not ssh it's more like it's like execute here so this is the line that we want execute if we look over here you can see what it does so execute execute a command in a container and so the command that we're executing is saying uh get us into the actual instance here and i'm assuming shell demo needs to be a particular container just double check here so verify the container is running so get pod shell demo get a shell to the running container so probably one of the running containers which will be sinatra here so go ahead and paste that in and we'll just swap this out for sinatra and it was refused because we didn't have micro k8s in the front as per usual and i didn't type it right probably so we will oh and i'm in the wrong environment that's why so just be careful about this one and i don't need this anymore because we've already updated uh that and so we'll paste that on in here whoops we'll type in micro k8s and says sinatra pod not found maybe we called it sinatra example or something so we'll say micro k8s cube ctl get pods okay yep they're very uh they're very unique okay so then we'll have to actually give it the pod name here right and so now that we're here we'll do ls and we'll go to cd to the root and we have a config directory we'll go inside of there and notice we have game.properties user interface.properties so if we really did want hello world to be propagated over there we would have to write in that file you know hello world and then its path okay and if we go into game properties is that oh that's a file so we can go cat game properties and see its content and so there we can see the files so yeah that's pretty much config map two different ways to access it very cool and yeah that's it hey this is andrew brown from exam pro and we are taking a look at secrets so we did config map which uh are uh completely not encrypted in any way whatsoever and so secrets is a great way for us to pass our sensitive information into our pods or containers uh so what we'll do is pull up the kubernetes secrets now by default by default uh they're stored in unencrypted api api so in order to do that we'd have to enable encryption at rest for secrets rbac rules all these other things i'm not sure what we'd have to do for that i mean like for the ksna it's a bit out of scope but let's just take a look at micro kds and type in status and see if um if there is any kind of like extension just to turn that stuff on to configure it micro k8s status micro k8s am i crazy today am i not typing it right oh you know what it's because i didn't type exit i was still in um in our pod from before so micro ks status i'm just curious what we can see here in terms of options so we don't have rbac turned on so i'm not sure if i'll have to turn it on to use it but there's rbac and i don't see anything in particular for secret so you know there might be some things we need to configure but uh i don't particularly care if they're secured i just want to go through the motions with you to see how it would be done so there are different types of secrets that you can set so pay is what generally is for user defined stuff and then we have a variety of other ones here i would imagine that maybe the encryption is different or something i'm not really sure um but you know we'll go through the motions of setting up our own secret so it's very similar to doing the config map so here's an example of some data so we'll go ahead and grab this secret here and make our way back here and we'll create a new file called secret.yaml you can also ingest them from a file so even though we are placing uh the data right in here you can include additional key value pairs opaque so you can pass the data in just like that um say like username password andrew brown testing one two three and notice here it says service account token so if we go back here and we look at the types we can change it so maybe basic auth would usually have a username and password okay so go ahead and paste that on in here and then there's annotations for the service account name that's with role based access controls i don't know if we need this right now uh i have a feeling that we probably do but i'm going to go ahead and delete it out and see what happens so we'll just say basic auth say sinatra basic auth as the metadata name and so we'll see if we can actually create this here so type in micro k8s cube ctl apply hyphen f k8s secret okay so it says error when creating secret in version one uh cannot be handled as a secret illegal base64 data as the input okay so i guess it expects us to do something with it so we'll take a look here um oh they actually have an example right here oh string data okay so we'll change this over here so i guess it's going to vary based on what you're doing so this says data this one says string data because that's technically what it is i guess it was b64 we'd have to just encode it but we'll go switch that over like that hit enter and it created it so now we can type in micro k8 s cube ctl get secret secrets micro there we go uh maybe we get some detail so say uh describe secrets sinatra basic off all right and so notice that we cannot see the contents of it says 10 bytes and 11 bytes um still we didn't go through all those motions of setting up these other things so i'm just hoping micro k8 did it for us because i don't know if like we didn't set up secrets as it said uh above here with all these options if it would show plain or show just in bytes but at least it looks like it's obscuring from us so that's really great if we want to go fetch that secret that's a good question so we'll go back here and see if it allows us or shows us how to fetch that data it is not you can edit it using secrets i just want to see how we can see it okay so i know uh let's see this is secrets and this is on kubernetes as well this is managing secrets using a configuration file i'm just looking for the cube ctl command so we can view it so we scroll on down here so verify decode ah so maybe that's what we want so cube ctl get secret um yeah this one shows bytes oh did they do something similar to us like basic auth well let's give a go and see what happens if we run this but this we're not following this tutorial so i can't expect this to work the same way but we will try for fun and we will say here the proper name which is this paste that in and we'll type in micro k8s micro k8s oh it does okay great so that did work so we have this output json path so i guess what we're saying is get the secret and then we specify get us the data even though it says string data i think it's just saying like the data is being inputted as a string but we want to grab the data and so that is one way for us to access our data using cube ctl but how do you get it within a pod that's the question so it's no different than what we were doing we can i think we can mount it as a volume i think we can do that so we go over here um back over to this and we say volume i know we can do it with a volume whoops volume as a file volume mounted so it's going to be the same process as we did with config map as you can see here so nothing's super exciting we're not going to go through all that but we will look at how to do it for um just not like we did the environment variable so we can go secret um what's the one we set that we commented out up here config map ref it would be like secret ref secret ref yes so if we wanted to do it uh like pass it this way here we could do secret ref here and if we wanted to update this maybe we could try like hello hello message say like secret message i don't know if this will work but we'll try for fun and i'm going to go here and type in clear and then we will go ahead and apply this if we can find it here and so now we'll make sure that we actually have that data yeah it is here so notice that it is encrypted and so i'm hoping that we can just pull that information out so this is called secret base basic auth even though basic authors don't be using a password that's totally fine i don't care um just copying that paste that in there that looks okay and so what we'll do is we will update apply f will update our deployment and see if that takes and i got type cube ctl in there okay and then we need to wait for our containers to or pods to spin down and spin back up and by the way as we've been doing all this we actually haven't been using the um dashboard which surprised we haven't been uh it might be fun to go take a look at that just because uh we've been adding all these things and we haven't used it at all so i think the easiest way is to go to the home page here go to linux scroll on down go to micro8 proxy dashboard proxy and i'm going to make a new terminal here tab hit enter and this is the token we want so i want to go and find the ec2 instance and get its ip address because i don't know what it is and here it is so we'll go ahead and grab that we will override this one and i need the ip address here so go grab the port number there and it says cannot find the request http colon 10443 client sent hp request to an http server yeah okay it should be colon oh sorry we'll just put the s in and then we'll advance because we don't have a certificate attached and then we need the token so we'll go back here hopefully we've grabbed it in time i don't know if it's going to expire or something and we'll sign in and now it's just fun to kind of look at this so we had a config map here below you can see it was created over here can we edit these in place oh that's cool so yeah if we wanted to just edit this that'd be really cool to do um we don't have any persistent volume claims do we have or we have our ingress service just poking around for fun here we'll look at our deployments so what i'm curious about is like did our pods change so available progressing new replica set available so nothing super new here this happened a minute ago so i think it did happen one minute ago it did replace our our pods there so that means that we should be okay to do a curl local host and do sinatra ah there we go so it worked um so yeah that's secrets okay so there you go hey this is andrew brown from exam pro and we are doing another follow along and this time we're dealing with volumes and and specifically with uh persistent volume so we have our two environments and we're going to use our micro cage environment here and so what i want you to do is make your way back over to here and within micro creates we need to enable storage so we type in micro k8 status there is an option to uh have a storage here and so all this storage thing does is um i'm just trying to find it here here it is so it will create us a storage class uh and the reason why we're doing this is because when you usually have a storage class you generally are linking to something external so it could be like um aws ebs or azure disk or something like that and just to make this easy for development uh micro case has this storage class that is locally on the machine uh so we're not gonna learn how to create a storage class because it's gonna create one for us but we are going to use it and it's just going to make learning persistent volumes and persistent volume claims a lot easier so we'll type in micro k8s and we'll type in enable and then storage and that will enable our storage so it's just going to go ahead and create us a storage class there okay and so here it says it created an rbc authorization so a cluster role cluster role binding service account storage class stuff like that it says it'll be available soon so now what we'll do is type in uh cubectl and uh we'll say get maybe sc for storage classes yep there it is and so this is the name it's called micro k8 host path and so the idea is that on this node uh we will have um our volume or our or something mounted to our directory here okay and so now that we have that out of the way i'm just pulling up the instructions i wrote earlier and so what we'll need to do is create a persistent persistent volume so a persistent volume is basically uh it's saying okay this is the volume and it uses this type of storage class all right and so that's what we're going to be doing there and ignore my stream deck i really hate how it always pops up but i'm just going to uh say remind me later okay i usually don't get popups in my in my videos but there's one there so what we'll do is go and type in persistent volume and see if we can just pull some code from the kubernetes website to start with okay so we'll just load that up here and i'll just scroll on down and i'm just looking for persistent volume an example of persistent volume so we have uh here's one okay so what we'll do is just basically kind of grab this one this is clearly not enough but i do have my code off screen here that we will bring in here to make it a bit easier and so i'm going to make a new file here and we're going to call this pv.yaml all right and then we'll go ahead and paste that on in there and so we'll just take a look at what we have so the first thing is we'll need metadata i'm going to call this mypv i really don't like when people type foo and we're going to apply labels type local i don't know if we need to have the label there but that's the example that they are using for micro k8 so if we typed in micro k8 storage they might show an example there let's see if there's an example here no we might type in like tutorial and it's the same page here and it could be this one no but that's okay like i just have it off screen i just like to show you where i'm grabbing the code from so you get very good at googling too because really with kubernetes you have to spend a lot of time looking for stuff because the main docs is always kind of uh not that great so the storage class name is going to be the storage class that we want it to be so this is going to be micro k8 i'm going to copy and paste it so i don't type it wrong here and so we have cube ctl sc and we'll go grab its name from down below and paste it in the storage class here and then we need to specify the capacity so how much we're going to reserve of space for this and so i'm going to do capacity storage and we'll say one gigabyte okay making sure i split capacity right and then we might want to specify our axis mode so the axis mode is like can you read and or write okay and make sure you set those to two spaces over there and so we'll say access modes and i think this is an array so we'll do hyphen there we'll say read write once um claim reference we don't need that um but what we do need is to set the host path and so we're going to actually say for this storage volume where is it going to be accessible on our node or our local machine here and so i'm going to say mount data and so that means uh not within uh the pod but here in our developer environment our virtual machine on the forward slash mount data that is what will be will be shared to our uh pods okay so this looks okay to me but i did type a lot by hand so hopefully it is all correct and so what we'll do is go ahead and deploy this okay and so we'll type in k8 pv yaml and it's created our volume there and so what i'll do is go ahead and we will go view the contents of it so say cube ctl pv or sorry get pv i gotta spell it right okay so what we see is the name the capacity the access mode whether to retain it is it available but notice there is no claim so technically it's not linked to anything right now and so that will be what we'll do next with the pvc the persistent volume claim let's get a bit more of a description here to see if there's anything interesting and so we'll just expand that up and it's pretty much you know same information nothing super exciting so now what we need is a persistent volume claim okay and so what we'll do is just go back over here and see if we have an example and there is one so we'll do is go back over here and say new file we'll say pvc.yaml and we'll go ahead and paste that and we need our metadata i'm going to call this mypvc uh we don't care about namespace we'll put in the defaults here and we do need to specify a storage class so that one will be the same as the other one so i'll just go over here okay and we'll paste that on in there and we'll delete that and then we need to specify the resources i don't care about the volume name we don't have to write that in there it will just set it to my pvc i think and so we'll say resources and we'll tab in we'll say requests and then we'll say storage so we're saying how much we want to request so i'm going to set it the same value and this is not going to work because if this is one gigabytes and this is one gigabytes it's just taking up too much of the space and you have to be less than that but i just want to show you what happens if we match it one to one okay so what we'll do is type in clear and i'm going to do cube ctl apply hyphen f and then we'll specify k8 uh pvc gamble and notice that my pvc is in valid access mode required at least one axis mode so maybe my indentation is the wrong levels or i deleted it out oh no i didn't even specify it so we'll go here these darn spaces um two maybe this little cog will let me set it nope nope i just don't feel like looking it up okay so we'll go here and type in storage class name colon and then oops no no no no no we already have that what we want is our access modes it's going to be the same thing because what it's trying to do is say a pvc is saying like tell me what you want like this is what we want and then the pv uh the person's volumes this is what we have and if it matches up then they will uh match up via a claim okay so we'll go here and paste that in and i'll go back and hit up and so it created the pvc and so now what we'll do is type in cube ctl get pvc um type that right there we go and so it says status is bound and we'll type in pv and it's bound and it's claimed okay so it did match up so i don't know when i was doing this and i had it the the wrong side or one one to one it didn't work but i guess like if we were to set this to be like two then i would expect this not to work so let's just go and change that to do because clearly that's too large right and here it says forbidden only dynamically provisioned pvcs can be resized okay well we don't really want to resize it so what we'll do is delete it first so we'll say cube ctl delete pvc mypvc okay we'll type in clear we'll go take a look at the pvc make sure there's nothing claimed not sure how it's claimed when it's not there anymore okay well uh what i'm going to do is i'm going to go ahead and just delete the pv here and we'll just run them again okay okay so we'll type clear and i'm just going to hit up because i want to run uh pv and then i want to see that it is not claimed and then what we'll do is do pvc okay and then we'll go back to get pv i'm just going to stretch this a bit larger here and uh notice here i don't know why it's so hard to read i don't remember seeing this many columns here but we have the name i'm just going to try to expand this a bit more and run this again maybe i'll just bump down the font here to make it a little bit easier because i can't see what's going on here with this font this big so go back to our terminal here terminal terminal terminal where are you there we go and we'll just bump this down a little bit so there we go that's a lot more manageable so here we can see my pv and this one is not claimed and um it's i guess it's showing up the pvc that we have here but notice that it's not matching up because it's it's too large right so what i'll do here is i'll change this back to one i'm going to see if i can actually change it i know it said like before we can't resize it but maybe we can if it's not claimed yet so we'll run that field cannot be less than the previous value so notice here okay so i guess you can expand it but not shrink it so that's fine so what we'll do is just go ahead and cube ctl um delete pvc we'll say my pvc and we'll type clear okay and so now what we'll do is we'll go and then run it again and actually before we do that i just want to shrink it so it's smaller i'll say 200 megabytes there we go and then we'll go back there just trying to figure out the bounds of this stuff so it's claimed it's bound um so now the only thing that we need to do is we need to actually have a deployment that has a volume attached so what we'll do is go back to the kubernetes page here and see if it actually shows an example with the deploy i think it shows like with a pod maybe here keep scrolling down here yeah and so this is pretty much yeah so it says pod but we can do this with a deploy and so what we're going to do is go back over here to our deploy up here and there's a lot of stuff going on here so i'm just going to go copy the contents here make a new file and call this deployment with pv or storage and we'll paste that and we do have some old stuff here that i do not want so first we will delete uh and make sure you put the yaml part on the end of it or it's not gonna work properly it's not for coloring so obviously we've done a volume mount with a config map but here we have sinatra so i'm going to say sinatra with volume or storage okay and we're just going to keep up with that maybe just say pv like with pv just trying not to type a hundred things here oh we have our container name sure we'll just update that as well and this all looks fine we don't need environment variables ports are fine the readiness probe is fine so that's what we would have if we had our minimal one for before and so what we need to do is make it so that we attach a volume to it so we'll go back over to here and notice that we need to do a volume mount so we'll go ahead and copy paste that in that needs to be aligned with the name image within the containers the indentation is a little bit tricky but we'll figure it out so i'll go ahead and paste that on in there and we'll just fix our indentation i'm noticing this is all four spaces okay and so this should be i would say like that that looks okay to me and then what we need is the volumes part so we'll copy that that's going to be aligned with containers and we probably can just paste it even above to make our lives a bit easier okay and so we have the we gotta see what we need to match this up with so for the volumes part we'll give it a name like mypvc and then what we're putting here is mypvc so we're matching what volume claim we want to be attached or like associated with this actual pod okay and then under the containers itself this is what we want to define as the mounting directory so i'm going to just call this mount local data here and then underneath here we just want the names i'm going to say my pvc all right and so this should be the mount directory on the actual container okay and so now what we'll do is go ahead and deploy that so i'll type in cube ctl um apply f deployment or k8 deployment with storage and i gotta spell cube ctl correctly and so that's deployed and so what we'll do is just type in clear and then i'll just say um cube ctl get pods so we can see all of our pods and then i want to describe the one with storage notice that we have two of them maybe we had replicas on two here yeah we do and so i only want a one so i'm going to scale that down so i don't have to deal with two or actually we can just probably apply it to set up there update the value there get pods there we go and we'll just wait for that to connect while it's waiting we can just go ahead and describe it um so this will be for the deploy or we can describe the pod as well um we'll describe the deploy first no no we'll do pod we'll do pod and then we'll just copy the name here and just see like what kind of volume information we can see if any um so it says persistent volume claim my pvc not found so that's an error and that's why it's not proceeding forward so we must have spelled something incorrectly so we'll go back over here to my or our pvc here it says my pvc so that seems like that should be the name we'll type in cube ctl get pvc and there it is my pvc persistent volume claim not found let me just take a look at my instructions well it looks like it's totally fine let's just hit up and see if anything's changed no it still can't find that volume so let's take a look at the pvc volume itself and describe it and see if there's any issues so it has a volume it is bound and we'll go pv and that looks okay as well so i can think of is there's something wrong with my script here so i'll be back here in a moment and just compare my uh my old script with this one okay all right i'm back and so i just carefully stared and i can see what the problem is this is mv it should be m y so that's probably what our problem was and so what we'll do is just type clear and redeploy that hopefully it takes effect good and then we'll hit up until we can get to our pod the pod is going to have a different name so we'll have to go get pvc or sorry get pod and then from there we still have two one is running one is probably going to stop and if there's two deployments i'm just going to double check here nope we only have one we go and so i'm just going to grab that pod name and we'll just say cube ctl get pod here and we'll just describe it make sure there are no issues well gotta type it in the right place you can't do get described that makes no sense okay and so here we can see um there are no issues i'm just trying to see if there is the claim here it is read only false persistent volume so it's all in good shape so that's good to see let's go take a look at what it looks like for the actual um deploy so we'll say deploy and it's called sinatra with storage and there we can see the volume claim information again okay so now the next question is can we see if our volume is actually working so what we'll do is we'll go to that directory so i told you it was local here mount data remember this if we go back to our pv here we specified it's here so we actually have a mount a data directory there and i can just make a new file so i'll say touch and we'll just say hello.txt now we'll do a pseudo on that because it just doesn't like if we don't do sudo so now that we created the file what we should do is ssh or i guess um i say ssh but we're actually just remotely connecting to the shell so we'll type in cube execute hyphen i t and then we need the name of a container and then we'll have to have bin bash notice before we did like sh we probably do sh as well it's basically the same thing um in a new tab i'm just going to go ahead and grab the pod name so we'll say cube ctl get pods and that is the name of our pod double checking there yep and then we'll go ahead and paste that in here oh sorry cube ctl and so we're within our actual container now and so we mounted this at mount it's being kind of silly here we mounted this at mount m8 mnt local data so i'm just going to do ls on here i don't i don't know why this uh this is kind of being a bit glitchy here if you notice i'm typing it's kind of going weird soft to type it out by hand no problem and notice we have hello txt and that's within our container so that's pretty much all we need to know for our persistent volumes i'm not going to clean this stuff up i think it's totally fine it's not going to heat up a lot of our resources here but yeah that's it for storage so just note that you always have to have a persistent volume claim um in order to attach a volume there there might be an edge case where you can attach a persistent volume to like static resources with a persistent volume claim but you always have to have a pvc with a pv with a storage class okay hey this is andrew brown from exam pro and this fall on we're going to be taking a look at net policy so again i want you to launch your micro case environment that we set up earlier in the course and what we're going to do here is just type in clear and get started here so in order for us to use net policies we're going to have to have some kind of cni so um cni plugin right and so there's a variety of different kinds but for cube ctl they don't have a whole lot of options we just have psyllium so i'm not sure if that's the proper way to pronounce it but that's how i'm going to pronounce it here i'm going to type in cube ctl status or sorry micro k8 status and give it a moment here to load and so there might be other options here but the only one that i recognize here is called selem or cilium and so that's going to allow us to have uh full network policies all right so we'll go down below here and we're just going to type in micro case enable and then put in psyllium and then once that's in there what we're going to need this is taking a second here uh what we're going to need is to have some namespaces so with network policies we can isolate based on or like control traffic based at the pod level or at the name space level i'm going to do it at the namespace level for this example because we don't need to go too super deep into net policies we just need to get something working and i'll just pull up while that's going we'll type in net policies kubernetes and see what the documentation gives us all right and so here we are if we just scroll on down so here is a general network policy ours is going to be a lot simpler than this this says ingress and egress and it's quite quite fancy so what we'll do is go back here and see if it's ready it's not yet but i already kind of know what we need to do so what we'll do is we will create three different name spaces and we'll launch a server or a deployment into each of them and the idea is that we'll have uh one namespace that we'll try to talk to and one will be blocked and one will not be okay so what we'll do is create our name spaces we'll type in cubectl ns for short for namespace you can type namespace if you want but ns and we'll have one called fire and um oh sorry create and then we'll have fire ice and wind and so those will be our three name spaces that we'll be using then we'll have to launch some pods i don't feel like launching our sinatra app so i'm just going to deploy nginx all right and so we'll just put the image in here and say engine x and we'll make sure the namespace is uh for fire and so we'll hit up here and then what we'll do one is for ice and then we'll do one for wind okay and so now we should have all our pods so we'll just type in cube ctl get pods and ice wide hit up again um no resources found in the ice namespace what if i just type in pods here huh i guess the deploys are still happening let's just get deploys maybe just without the there okay and so we see that they're ready but they're not doing anything here so i wonder if we've like used up all of our resources on the machine so what we'll do is get deployed we'll say describe deploy hyphen end ice and we'll say ice engine x and we'll see what it's complaining about there's no errors so maybe it's just taking time i mean we are doing all this stuff over here so i'm not sure if that was conflicting with it we are seeing a connection refused issue so it looks like this probably didn't install properly okay and we'll go back here and do a get okay yeah so it just took some time here now this didn't work out i've never seen that happen before i'm just checking here created because when i when i did this it never failed a so we'll try one more time and see if we need to do that so it's already enabled so i'm not sure about all these errors i don't know this is going to run us into an issue but if we create our network policy and doesn't work we'll know that we don't have it properly installed but maybe we have enough installed and we'll be lucky okay so looks like all of our pods are running and stuff like that so what we're going to need to do is basically ping a pod from one other pod and so what we'll do i'm just going to write out a command here so we'll have to do cube ctl hyphen n choose um a name say so i'm going to do fire then we're going to have our pod name and then we're going to curl a ip address to somewhere and maybe i guess the idea is like this ip address is going to be like what we want to access so we say okay from from uh from an ice from an ice um pod hit this fire ip address okay that's what we're gonna do so what i'll do is just type in clear and the first thing we need is that pod name so we'll go and type in cube ctl get pods n ice we'll go ahead and grab that name on there i'm going to just move back there and then what we'll do is get fire hyphen o wide because we're going to need its ip address so there is its ip address okay and then we'll go to the right here and paste that on in and just complaining about this because maybe we forgot a word like exec there we go great and so the idea is nginx is running an nginx page if you hit this and so we can access the fire server from the ice server okay and just to show for the wind one so we'll do wind here and then we'll go ahead and grab wind okay we'll go ahead and delete this out great so right now we don't have network policy in place so it makes sense that they're able to communicate so now what we'll do is create ourselves a network policy i'm going to call netpal fire yaml and what we'll need to do is grab us some code so go to the network policy page we have up here this is a way too much we don't need this much code but we'll go here and i don't feel like dealing with egress so egress means leaving the pod ingress would be incoming from the potter container so we'll delete out the egress here and i'm just going to take a look off screen here to the policy that i created okay and we're going to rename this so it says net paul fire to ice so that's what it's going to be so we're going to allow communication from fire to ice or i should say ice to fire ice can communicate with fire okay but wind won't be able to communicate fire but it's all about fire so we'll just call it the pull fire here and then what we'll need is a pod selector so what we're saying is this is going to apply for our fire engine x right so select all those pods and then we need a policy type so we only want ingress so we'll just take that out for now and then from here they have like a from and they're doing ip block uh notice there's a namespace selector pod selector they're just showing a lot more information that we need this is going to be on port 80 because that's going to be how we're doing the curls just on a port http there and so we're not going to do pod selector we're just going to do namespace selectors so anything from that name space will be able to communicate and so what we'll do here is just change this we'll need to figure out what the label is we're not going to do ip so we don't care about that but what we'll do is figure out what kind of label we can apply here because that's what we're going to need so what we'll do is type in cubectl getnamespace wind actually won't describe probably here and see what kind of ener labels we have so this is a label so we'll do is just grab this part here and paste that on in and then this part is going to be wind all right and so that should work uh we have to specify this is the fire namespace because it's definitely not in the default namespace and double checking looks okay to me all right so let's go ahead and give it a deploy so we'll type in apply f k8s netpole fire.yaml it's created the file so hopefully it works we'll type in cubectl get netpol oh hyphen end fire and we'll describe its contents just to see what there is here and so it's saying um pod selector is for fire allowing ingress traffic to port 80 so traffic into port 80 to fire but only from wind okay so ice should not work and wind should work so now if we just hit up we already set this up before and we'll just hit up until we find the commands we want so we'll try wind first and wind was able to download the page no problem and then we will go over to ice it's working so if they're both working and the policies in place that means that psyllium is definitely not installed properly so what i'm going to do here is type in micro ks disable psyllium see now when i ran this it was smooth as butter i just enabled it had no problems so i'm not sure why we're having problems here but we'll disable it and then maybe we'll reenable it and hopefully it works but if the network policy is not in place then that's why it would not be working okay all right and so now we'll enable it again and we'll hopefully it'll just work because maybe when we were installing it there was just like network connectivity issues not relating to our environment but just what we were downloading it from and so i'll see you back here in a moment when this is done downloading okay and hopefully it installs properly all right so it looks like we're still having this here now again i did this uh same environment just a different account and had no problems with it so i just don't trust this environment i feel like there's something wrong with it so what i'm going to do is just type in i'm going to make a new volume so this will be micro k8 of netpol env and so this will be our environment specifically dedicated for testing this example here so i'm going to go back here again you might not have this problem it might work perfectly for you and you'll just proceed forward with no problems but what i'm going to do is go ahead and set up a new pod here so we'll go ahead or sorry uh environment so i'm going to spin up this new environment i'm going to make my way over to the aws console right and from there i want to go to ec2 and i'm gonna just see if there's any new machines here i'm not sure why it's not spinning up another machine not having much luck here today we'll go back to ec2 normally ec2 instances show up pretty fast so i'm kind of just surprised there it is okay and so we'll go to our next uh network policy environment just for this case we'll go ahead and go over to our volumes and i'm going to click into the storage i'm going to modify it we're going to set it to 40 gigabytes so we don't have any problems here and then i'm just going to wait for this environment to start up uh here then we want to install micro kh so we'll go micro i mean it's just snap so well we'll go to the website micro creates and just quickly reinstall micro k8 on this environment because it was so easy since last time we did it so we'll paste that on in there and looks like our volume has resized it shouldn't be a problem here probably shouldn't be running commands unless we know it's been resized but you know i just want to get it done and so michael kate's there we'll have cube ctl and then we learned from our fix that we can do micro k8s config and then do a cube config maybe about to touch that file first just make an empty file well maybe we type in cube ctl and maybe it'll generate out that directory for us all right remember we have to type these things in so we'll go and paste that on in then we'll paste that on in then we'll paste that on in then we'll type in micro case cube ctl get pods and then it'll download oh no it didn't have to download it great and so maybe we'll hit up now to um that line we have here there we go and so now it did that so now i can do cube ctl cap pods good and now i'm going to just make sure i have enough space on this drive here and i'm just looking for the drive usually it's 10 but we set it to 40 gigabytes so looking for it here it's still at basically 10. so we'll go over to our ec2 instance back to it and i'm just going to go ahead and reboot it and we'll give that a moment to reconnect and so if this works that'll be great if it doesn't that would suck but i hope it does worst case scenario i could always just open my other environment and show it there but it's just kind of frustrating if i can't replicate it here and if this hangs too long you can just hit refresh sometimes that helps here too when it's doing a reconnect here or if it does that another thing i do is just go back to the cloud 9 interface here and then i would open up netpol and if it is rebooting we can go back here and just double check so it's two out of two i guess whenever it decides to connect i'll be back here in a moment okay all right so i did a few refreshing and then finally let me back in here so it's not always perfect when reconnecting so we'll type in micro case status to see what we have installed because this is now completely fresh here micro k8 micro k8 status there we go we had to wait for it to get running there again i'm going to scroll up here and so all we have is the h a cluster so we'll want dns for sure so we'll go here and say enable dns and then um we want the psyllium right so we'll go here and grab psyllium and so i expect the nest install without issue but i'm hoping psyllium just works with a fresh fresh fresh fresh environment so this will take a little bit of time but i will wait until it's done here and i'll see you back in a moment okay all right so look it actually worked no problem this time so for whatever reason the old environments just kind of bust at least for that particular use case so we will have to uh back pedal here and redo a lot of the stuff we just did which is not a big deal it's not going to take too long to do so i guess we'll just go ahead and do it again and practice makes perfect right so we need our name spaces so we'll say create an s fire create ice create wind and then we need to create deployments so deploy fire engine x image engine x hyphen n fire and then we got ice okay and then we got wind great and so then from here what we'll do is we'll create ourself a new um file which will be our net paul fire yaml and we'll go back to our original environment here that we're running and i'm going to go ahead and just grab this file here we'll go ahead and paste that on in okay and that looks what we were using before so it's totally fine and so we'll type in cube ctl apply hyphen f actually before we do um let's go make sure we can communicate between our nodes so um uh let's remember what that command is so it's cube ctl whoops cube ctl hyphen n fire exec and then we need our pods here so we'll say new terminal so say cube ctl get pause and fire or sorry ice and then we'll say wind and then for fire we need hyphen o wide and so we'll go ahead and grab our ip address here paste that on in we'll go ahead and grab this one here and this is actually going to be for it was wind here a moment ago i already kind of forgot and i believe it goes here well the first one is ice wind oh sorry that says ice here so change that to ice and i believe it was uh hyphen hyphen curl and then localhost or i guess uh whatever the ip address is yeah there we go okay great so this one's set up and then i'm just gonna quickly do the wind so we'll do wind and then that one wind okay great so that works no problem now let's go ahead and deploy our network policy so we'll do cube ctl apply hyphen f net pull fire yaml and so it created it and so now what we can do is go hit up try out wind it works fine try out ice it works fine okay so one of those should have hung like it should have not worked but it didn't um so maybe there's something wrong with our selectors it does say roll up here which doesn't seem right to me so let me go back over here now like psyllium definitely did not install properly so i don't think our network policy was our problem i do believe that uh it wasn't installed correctly but let's just take a look at the actual selector name so we'll say describe pods and fire because i think it's just app by default yeah so it is so change this to app okay and then we'll have to redeploy our fire policy we'll type clear i'm just trying to find our commands that we type below maybe it's in this screen here there we go so does ice work notice that ice is hanging look it can't connect right because we said that wind is allowed to have access and not um not ice and so we go to wind and there we go so that's all there was to it unfortunately we had to take a bunch of detours there um but you know that's the learning experience and that's the hard parts that you're looking to watch so that's pretty much it and you can delete this environment because we're done with it so what we'll do is close the network policy environment and we'll go back over to our environments here i'm going to go ahead and just delete this one because we are done with it okay there you go and i'll see you in the next one hey this is andrew brown from exam pro in this following we're going to be looking at k native which is a serverless or serverlesslike platformer tool for kubernetes and so because this is such a heavy duty tool on top of kubernetes i don't want to pollute our existing micro k8 environment so i'm going to create a new environment here we're going to call this k or micro k8 okay native so it is a bit work to set up but in this case it makes sense to do it okay so we'll do env i'll do it next we're going to choose a t3 medium just so that we have sufficient space and or memory and here we'll choose ubuntu and we'll go next steps we'll go ahead and create that i really really wish aws would let us choose the storage size i'm just going to check if there's any additional options no they really should let you do that because it's such a pain that you have to go resize stuff we'll go ahead and create this environment and while that is creating i'm going to go over to a new tab we'll type in console.8bus.amazon.com we'll make our way over to ec2 and we are looking for this new environment because we need to increase that volume size as per usual so we'll give it a refresh here and is it here yeah it's right there so we'll go into here we'll go over to storage we will choose the volume and i will go to actions and is the volume ready yet oh we have to choose it modify the volume and we'll change this to 40 gigabytes we'll hit modify we'll hit modify we'll go back to ec2 and even though it is running we are going to restart it reboot it just to save us some trouble here and so now we just have to wait till this connects and then from there oh did it really reconnect that fast nope it's reconnecting okay great and so we just need to wait for that reconnection to happen as that's going let's go take a look at k native so k native here is enterprise grade serverless on your own terms so it says kubernetesbased platform to deploy and manage modern service serverless workloads if you're wondering what k native is compared to open foss or other serverless frameworks strongly recommend reading this article like versus open fos it is an article on the cnsf blog and it's very very good at explaining the stuff i like to the point that i actually grabbed this quote here and put in the course so we could understand the difference between the two but if you get an opportunity to read this whole thing and it doesn't just talk about those twos it talks about all the types of open source frameworks serverless frameworks for kubernetes but coming back over here the idea is kubernetes and just going actually to this article because it does outline it pretty well but k native already uses um components that we're familiar with like istio and it uses kubernetes underneath of course but there's three components to building eventing and serving and so venting serving is where we're spending most of our time but the idea is serving is the ability to quickly serve stuff that will automatically scale from scratch have routing and networking so the idea is that um when we are utilizing kubernetes we have to uh provision all these things like services and pods and deployments and so the idea with k native is that you just have one line and it does all the stuff for you you only have to set up you don't have to set up horizontal pod auto scale or anything it just all works for you does snapshots it has revisions for your code and then on top of that it has eventing so the idea is that not only does it make it easy to just provision things and have all those nice services around it but eventing allows you to trigger when these functions should run based on a bunch of stuff and so there's a whole ecosystem with that not sure why the redirecting is kind of messing up here so yeah down below they had on the website before i'm not sure why i can't seem to find it right now but on the k native page here they had a good description out here serving and so they talked about the integrations they had i can't seem to find it right now but the idea is that you can trigger stuff for that to happen so oh we're under serving i wanted a venting that makes sense now so the idea is that you have triggers channels descriptions there's all sorts of ways to uh trigger your k native events but now this environment's ready what we'll do is get micro k8 installed so we'll do the classic here okay and while that is going i'm going to install cube ctl just to save myself some time we'll just do cube ctl uh i think i might have just accidentally canceled that out there okay so cube ctl is installed micro case is almost installed here and then we'll have to wait for the server to spin up as that is going we'll go back over here and look at the tutorial to get getting started here look at the quick start here uh we'll have to also install um kn so kn is the um k native utility library so we'll have to go grab the binary as well um so the easiest way to do this is to go to the release page here and then what we need to do is download um the one that we need so amd 64 is x64 architecture x86 sorry and so that is the file that we need and i'll need to show that in finder and it's just downloading there as well while this oh now that we have michael k it's installed type in microcase dns and also k native uh enable assuming the uh micro kx is ready oh right we need to run these two lines here so we'll run this one and we will run this one and then we'll run this line here and then we'll go back up and we will enable dns and k native and those both will install assuming that the cluster is ready and i'm still waiting for this to download okay this is now done and so the idea here is trying to hide some things in my screen here but here is the file so i have it downloaded here and i'm just going to quickly rename this to kn because it is a binary right and then the next thing we need to do is upload it to cloud9 so if we go to file upload local files i'm just dragging that kn file it's just that file here right that we just renamed and it's going to be there so we'll go ahead and close that it uploaded pretty darn fast oh it's still uploading right here you can see the percentage so we're just going to wait for that to finish as that is going i'm going to go here and um make it so that we can use cube ctl so we'll type in micro k8 config tilde q a period cube config um maybe we'll close this tab here and reopen a new tab close and open a new terminal and hit up again we already did this so i'm not sure why i have to do this twice maybe i made a mistake maybe i forgot the s on there okay and then we'll go back there oh you know i forgot to put the um right angle bracket and so now we should be able to do cube ctl get pods clear we'll go over here and it looks like installation is still going and while that's going we should move kn into the correct directory so we'll go back over to k native and we'll have to chamod it so go over here and we'll check mod and then we want to move that into a location like user local bin so we'll say sudo move kn to user local bin great so kn is now ready to go there and if we go back here we're just waiting for k native to finish i'm just going to double check to make sure that it's anything else there we might check the version afterwards which might be okay to do so we'll go over here check our version cannot execute binary file exec format error i wonder if it's actually a binary let me just double check oh did i download the amd did i download the wrong one that's probably what it is all right well we'll go back over here and uh no amd is right yeah that's the one that we want oh but i downloaded darwin i needed linux for xxx darwin is for mac os i'm sorry so we'll download that one and i guess we'll have to go ahead and delete this other one here so we'll say remove sudo remove user local bin kn and now that's done downloading i'm going to go ahead and go grab that one so i'm just going to delete the other one here and we will open up our file upload here so file upload local files drag that on over we'll rename this to kn we'll drag that back on in there we'll have to wait for that to upload which shouldn't take too long we'll go over here and the k native should be installed uh we'll do cube ctl get pause and we'll just take a look at what was installed because everything's pods right so here you can see we have istio so istio was installed for it um we have cave native serving cave at native eventing just to kind of show you what's going on here and we're just waiting for this to download and then we can proceed generally forward but maybe i'll just take a look and see what kind of code i have here yeah we need to wait for kn so once we have kn installed we will then go towards serving our first service so here they have a hello world go example and notice we just do kn service create and it will do a bunch of stuff for us okay so that is now uploaded so now we'll just hit up and do the same thing we did before so we'll chamod our kn i'm just gonna close this so it's out of the way and then we will move our stuff into the correct directory okay kn version great so i think we're aokay um so go back over to here we'll go ahead and grab this now they do have yaml notes that's using k native dev so the specification is different but looks pretty similar to what we normally see so we're going to go ahead and create this hit enter it's going to go ahead and create the sample now i think that when i created this i remembered that this hung for a long time even though it did create the service so if this takes too long i might just cancel or kill it out okay so i'm gonna type clear here and as this is going i'm just gonna go back up here and type in pods and just see if that spins up so yeah it is creating the container and maybe we can just kind of inspect what that pods doing so we'll say um i was going to type um cube ctl get pods paste that in there and then maybe describe it let me just kind of read what it was doing so down below it's successfully creating the image it's pulling the image it's creating a proxy things like that and i'll just hit up here again okay so that pod is there so go back here and the idea is that when we run this we're expecting this output to happen which seems like it's never going to occur anytime soon but we did just describe the service well hold on here so okay i know that's never going to finish but i know that it's ran so what we'll do is we'll just proceed forward so i know we didn't have this and so i had to go look up kind of see what the command is so if we type in kn service list we can see all the services we deployed so there is hello don't worry that it says it's not ready because it totally did it did definitely work we'll type describe hello and as you can see here it says hello default one revisions okay type age reason so definitely is working but i don't know why it hangs like this all right so but we did install k native we did run through the basis of the tutorial so technically you know we we did use k native uh in terms of the kcna it's not like you really need to know how to do this i'm just kind of showing you so you get more domain knowledge on this kind of stuff and kind of show the experience of where you might hit friction there but i'm just going to stop there and that's all i really wanted to show you 4k native so we are done and what i'm going to do is go back to our um uh like our cloud9 stuff just because we have so many darn environments i just want to go ahead and delete this one so you are not having spend we'll type in delete and that's it for k native okay hey this is andrew brown from exam pro and i showed uk native now let's go take a look at open foss and let's go set that up and we will have to set up a new environment i know this is a pain but it's better to set these up in islip just because um it is quite the large install so what we'll do is create ourselves a new environment we're going to type open foss faas env maybe just put micro k8 so we remember that it's micro k8 and we'll go ahead and hit next step we will choose t3 medium then we'll choose ubuntu we'll scroll down and we'll hit next step we'll create that environment and we'll give that a little bit of time to create as that's going we'll type in console aws amazon.com make our way over to ec2 and i'm just waiting for that new one to spin up as that's going we have this nice article which is open fast k native and more just to kind of show the architecture so with openvos it uses docker and it could either use docker swarm or kubernetes it has prometheus function watchdog api gateway it has a lot of stuff compared to k native and this article explains like that open foss is easier to use than knative but you might have some issues moving from one provider to another so that is one concern there but you'd have to read the whole thing to fully understand it but what we're going to do is go to the open fos website and i think they kind of have a tutorial here that we might be able to kind of follow maybe getting started here deployment kubernetes ah here we are micro k8s nope that's no help i do have the link here off screen that i was using let me just go take a look um i think it's like first python function that's what we want so we'll type in open fos first python function that was definitely what i want to use here okay and we will have to install the fast cli well actually we don't because it's part of micro k8 so this environment is starting to run and so what we'll do is hit refresh here and we need to resize our drive as per usual so we'll go here to storage volumes and we'll go ahead and modify our volume and we'll change this to 40 gigabytes and hit modify and while that's going we'll install micro k8 so we'll just go to micro k8 website i think by now i have this memorized but i do not we'll go sudo snap install micro k8 it's classic we'll make a new terminal here and then we will do the same thing for cube ctl okay and we'll wait for that to install we'll check on our volume our volume is probably resized but we'll have to wait for microcase to finish installing before we do our reboot here there we go and then we will type in micro k8 cube ctl and then it'll spit out the stuff here that it wants us to do so we will paste that on in there we'll do new groups micro k8s okay and then we'll do micro k8 cube c or uh config port slash cube config and see if that works i forgot the um arrow micro k8 cube ctl get pods up again notice that it works now no problem okay and so we'll go back to our ec2 instance and we'll just reboot it so we'll say reboot reboot and that will have to reconnect and so yeah i mean it's not super tricky we don't have to install uh fast cli like we did uh kn because microcase has it already and it's kind of a pain to install so i'd rather not do it this way um yeah i think we could do microkids i can't remember actually now no you know what i think uh i think we actually do have to install it okay so we will install i'm just trying to remember right so i guess we can try when this is reconnected we'll type in microcades um and we'll see if it actually works but i think actually for this that i it is there but i ended up installing open fast cli because if we didn't i ran into an issue later on in this follow along and that's the reason for it but we have to wait for this to reconnect okay so i'll see you back here when this is reconnected all right so after waiting a short little while here it is ready again so uh we are in good shape so i'm gonna type in micro k8 dns and then we're gonna do open foss and i have to probably write the word enable in front of it so that it actually works and so that's going to go ahead and start to install open fast for us we also need to install the cli here so copy this line from where we ever found it here on the first python page and so it's saying running with sufficient privilege permissions attempt to move fast cli so we are okay okay for shape if i type it fast cli it does show up that is good and open fast is installing so it'll take a bit of time after it's installed we do need to verify it i'm going to close the micro cades page so it stops popping up and driving me crazy i'm just going to double check what we need to do for the next steps because we do need to verify the installation and i'm just double checking here to see if it's here i think um micro case is going to prompt us to tell us hey run this so you know that it's running yes it does and so what we'll do is grab this line to see if it works and the next thing is we need this password to actually use it so we'll hit enter and we'll get this nice little password so i'm just going to drop it in here i'm going to clear this out and then i'm just going to grab this password for the time being and save it here because we'll need it for authenticating to open bus and now what we need to do is actually go ahead and build our function so we'll go back over here and they have one here so they make a directory called functions and they put all these these files in here and stuff so let's scaffold a new python function using the cli sure why not so we'll go ahead and do this and paste that in and here it's provisioned us a new folder and hello python wow that's a lot easier than typing it all in by hand so if we open it up we can see the content so it says open fos it's saying where it is here is the function it's going to the handle is going to point to this directory which is going to open this handler.pi file if you have ever deployed an as lambda function or anything serverless this is looking very familiar right like a handler file and so here is our fancy handler file you have your requirements.txt for anything you want to install so what i'm going to do is go back here and take a look at the next step here and so it's just telling us to update the file with this print hello so we'll get back over here and replace the contents of this file as such and we will save that go back over here and i think that this stuff is showing up because it thinks that this is a lambda function which it is not it's an open fos function and then it's just saying look at the contents of that file it looks like it's correct because it set it up all for us which is great and then down below we can just build it as such so we'll go here and we'll type in fos cli build hyphen f it's going to prepare our function okay and i think that the image is being stored locally so it's not going out to uh because we didn't we don't have a remote repo i guess it's i guess it's building it here on the machine never mind so the image is built and by the way while we're at it we should do get pods hyphenate just look at all the pods that open fast added so we have prometheus basic auth alert manager gnats um q worker gateway and stuff like that and the gateway is something that we're going to have to use next because we have to have a way to connect to that pod so i'm just scrolling on down here and seeing how we can see the pod next because we want to deploy it right so we built it and then we want to actually deploy it so i'm looking for deploy ah here it is okay so now that we've built it the built the image let's go deploy it so we actually run the function and notice that we get an error it says connection refused failed to deploy with status 500 and so for this i know that we will have to do port forwarding because i looked up another tutorial on getting started with open fast and what they had to do here was to port forward open fast and specifically for the the service called gateway we do 8080 colon 8080 and so now that we have port forwarding i have to go over to our other tab here and then what we can do is try to deploy again and we don't have it in the context of this batch file so we'll just go ahead and copy this line here paste it on in it says unauthorized access so we have to log in first so we'll go ahead and do fast cli login and so now it wants the password so we'll have to do hyphen hyphen password and then we'll have to provide the password that we have it here okay and now we can go ahead and deploy and it says hello python so there you go that's how you deploy a serverless function with openfast so we're all done here and what we can do is go back to cloud nine and shut down this environment we'll go over here to open fast micro case environment we'll delete will delete and there you go again it's not needed to know the stuff for the exam um well there are serverless questions but just to kind of like really cement uh what these things are just by going through the motions of it okay hey everybody this is andrew brown and welcome back to another follow along and this time we're going to learn about helm so what i'm going to do is go to google and type in helm and here we go to the helm website so helm is a way of packaging your various scripts like your manifest files into a single package that people can use to quickly uh set up projects here and so i'm just waiting for the helm page to load i'm not sure why it's so slow here today but we'll click it again maybe they knew i was up to something and that's why it's slow but here it is the package manager for kubernetes so yeah just think of anything like ruby or python or node.js like npm install so you're installing basically a program but it's installing like those files into your cluster so above here we have charts and i'm not going to show you how to package um a helm project because that gets pretty complicated but we will install a project and take a look at the contents of one so helm has this artifact hub where you can uh view or browse a bunch of different kubernetes projects but i'm going to go up here and just type in postgres and um i just want to see one that actually has a bunch of code so we can just kind of look at uh what would be in a helm directory there's one for azure that is pretty good so i'll type that one in here and it's bitnami for azure okay so uh we'll go here and here is a project and if we go to the github repository they're almost always hosted on github but i'm just looking for the templates if we click into templates here it shows us all the files that are involved and notice that we have these like handlebar stuff here so you can see that you know you might have a template like a config map but then you have this pragmatic stuff where you can pass in values to helm and it will change that stuff but let's go find the repository if we can here that's what i'm looking for um where are you i know it's here somewhere so i mean there's the that's the azure uh uh uh container repository so i'm not sure if it'll show us that let's go ahead and grab that actually never go to the azure container repository there we go so we can't see it there so maybe this one is not a great example which is totally fine but here says this is the repo okay it is on github there we go so here it says postgres h a so high availability i assume and the idea here is you have templates and in your templates you can start to see we have a variety of different files you have postgres pg pool again i'm not going to get super deep into how this actually works but just to show you that the files are there then there's this values.yaml thing this defines the type of parameters that we passed into the template so there's a lot there i would imagine that when you use helm you you'd have a command flag like hyphen hyphen values and you pass the values you want or you could specify a file most likely but anyway that's the contents of a file but let's take a look at how we go ahead and install one so i'm just going to open up my off screen here my my helm instructions when i was trying to find something out i'm just trying to find an example of something that was um that worked very well and actually you know what i used juice box from oh wasp and that was actually really easy to use so what i'm going to do here is going to browse all packages and i think what i did is i went to like security or maybe i went to web applications that's probably what i did and if we go here we'll probably see the juicebox app so it is here or juice shop sorry and so if we scroll on down you have some instructions here but the real instructions you can see there's all the values um configured here but if you go to install it gives you the two steps so here it is but we'll need helm first installed on our micro case environment so launch your micro k8 environment and then once you're in there we're going to do ls we're going to need to install helm so if you do micro k8 status what you'll see here is that they do have a helm uh plugin addon whatever you want to call it module right here oh it is enabled on this cluster great so um but anyway the thing is is like we want to use helm 3. there is helm 2 in here so if you were to do micro creates enable helm that would give us two and the problem with that is that it uses a tiller server it's like this additional thing that's really complicated and so we'd want to activate helm3 now it looks like it got activated for another reason we could have been using something else and it had to be activated so i think we typed in like micro k8 micro k8 helm 3 that's how we would probably access it yeah but i'm going to actually go and directly install the helm cli without having to do through michael k8 and we can just do that on ubuntu by doing sudo snap install helm i'm not sure if it'll ask us to do classic yeah it asks us to do classic i don't really understand this whole classic thing but we'll go ahead and hit enter and it's going to go ahead and install helm so now we just type in helm okay and so if we go back to our juice shop we're going to copy the first line here because we need to add the repo to know where this is stored all right so that could be on github it could be on elastic container repository azure's container repository docker hub maybe i guess not docker hub because that wouldn't make any sense but some kind of repository right and so um i'm losing where i am here we are so we'll go ahead and paste that in there yeah i guess the ecr might not make sense because that's for docker containers but um anyway so if you add the repository we'll go back here and we'll go grab our install chart and if we go back and paste that in it will uh go basically set up that application so if we read it here it says um default namespace deploy get the application url by running these commands and so this is the command we can run so all it's doing is doing a port forward because it's already running so if we do cube ctl um get pods a bit we can probably see it running there so there is the my juice shop i'm not sure what else that installed but uh i guess if we wanted to know we would just probably go to the repo and take a look here so where is that repo where are you source code here on github and so this is the i think the the one that uh we're using here and if we go into templates so it deployed a config map a deployment ingress a service so it deployed a bunch of stuff and um what we'll do is go back here i really wish i had deployed that in a namespace so we could easily delete it but that's totally fine i don't think we're going to run into a lack of space here but what we'll do is paste this line in here that's the one they suggested but one addition i'm going to do address 0.0.0.0 so we can access this outside of our cluster and i'm going to change the port here from 3000 to like 8081 something like that maybe but clearly the app is running on port whoops uh is running on port 3000 so almost like a ruby on rails app it's probably php though when you talk about vulnerable apps php is usually the one to go with so anyway this is now running and i don't know if the ports are open on that so what we'll do is go over to ec2 and maybe just open all the tcp ports for our ip address so we don't have to worry about opening individual ports because this is just for development so we'll go to security we'll go to security groups and yeah so we only opened up that port but i'm just going to go ahead and say all por all tcp ports okay and that'll just save us a lot of trouble there so save that rule and then what we'll do is get the ip address of the cc2 instance that's running our environment here so if we go up here to details we're going to grab that public ip address we said it's on 8081. i'm just hoping that it works 8081 fingers crossed there it is and here's the awas juice shop if you've never heard of owasp it's um it's for security so it stands for open web application security project but really it's a organization that has um i think it's nonprofit yeah nonprofit organization that has not just one project but a lot of different projects and resources and stuff um to help you secure applications and they have like chapters like user groups meetups all around the world um and so i mean this is one of their apps or projects that helps you learn how to better secure apps so it's purposely insecure but that is the owasp app okay so maybe we'll just kill the olaf's app maybe by killing the deploy there just so it's not uh or run it in here but we'll do cube ctl get pods and i'm just gonna kill it so say cube ctl delete um delete pods this one okay and those other resources are still there but because it is an app that's designed to be vulnerable i know it's only access by rip address i figure we should just kill just in case but yeah that's all i wanted to show you with helm it wasn't that hard there you go hey this is andrew brown from exam pro and this follow along we are going to set up a service match using linker d now i did do this lab ahead of time but i'm going to tell you that i just couldn't get it 100 working at least in cloud 9 because when you wanted to see the viz dashboard you have to have an ingress controller and that has to run on port 80 and if we are running in a cloud 9 environment we can't run on port 80 we're just not allowed to open up on that port so um i mean we can do it in cloud 9 and kind of get an incomplete tutorial but i figured let's try to do it in ec2 and hopefully there we will be able to see the dashboard now if that doesn't work it's totally fine because we can see everything in the buoyant dashboard but we'll give it a go and do our best here uh for the kcna you're not expected to know how to set up a service mesh but i just want to go through the motions with it with you so you can kind of see and be able to really understand the purpose of a service mesh so i'm going to go over to ec2 and instances we're going to launch a new instance and it's going to go to the old wizard we're going to go to the new wizard even though i don't like it and we're going to type in here k8 or a micro k8 linker d okay linker d and we need to choose ubuntu and it's going to be on 20 which is totally fine 64bit architecture here we want to have a t3 medium or we can have t2 but we'll just do t3 because we've been doing that this entire time t3 medium we'll scroll on down here and we will allow ssh traffic that's totally fine we're going to allow http traffic from the internet that's going to be totally fine and we're going to want to have 40 gigabytes here and we'll go to advanced details and i'm just going to look through here to see if there's anything else we want i want to set a roll but where the heck do they put this is a totally new interface i don't know because i don't want to have a key pair i'm going to say no key pair here and what i want to have is attach a roll where the heck did they put it i'm looking for i am roll roll where are you is always changing the stuff on me like crazy shut down stop terminate detailed elastic credits capacity is it just gone hostname oh it's right here we're all for frick six and so we're gonna need to create a new i impulse uh profile get a new interface i'm not used to where things are and i probably already have one for ssm role but we'll just make a new one so we'll go here create a new role because we want to use sessions manager to connect to stuff and this is new too they just won't stop changing things on me so um they used to have like nice little icons so you could see what you're looking at so we are creating a database service account it is for ec2 oh my goodness what is going on here introducing the new experience what do you what do you think it's terrible like no it's like the the drop out the the drop down is it used to be very clear what to click okay whoever is doing their designs i don't know they gotta send back the uh the intern okay because it's you can't even revert to the old experience and it's weird because it's still like using old elements but anyway so what we would need to do is choose um the use case for the service so we are doing the ec2 i guess this is clear i just i'm getting confused we'll go ahead and hit next and so i want ssm roll so hit ssm and we are looking for uh the proper one i'm just checking here because this one says this is deprecated use this one so that's the real one i want to look for and that's going to allow us to use sessions manager so we don't have to ssh and do anything super complicated so i'll just check box those there where is it okay we'll type in ssm again it is being super glitchy today and i want is it this one i'm gonna double check here amazon ssm manage instance core i'll type core instead there it is okay so we'll go ahead and hit next and we'll just say micro k8 linker d roll ssm just so we know what it is click create roll and there it is okay so we'll go back here and now we'll type in linker d there it is and so now we can use sessions manager we'll go ahead and launch this instance a lot of work tons and tons of work to get this going here and now that that instance is launched we can go view it and the experience is not going to be as nice i'm going to tell you right now it's not going to be as nice as using cloud9 but we are going to get through it and you will learn how to use a virtual machine without a nice environment so we'll refresh here because i'm waiting for these two status checks to pass we don't necessarily need to wait for it i'm going to hit connect and if it works that's fine so we see we have four options here is to do instance connect sessions manager ssh client ec2 serial console they just won't stop creating new connection types we're gonna go with sessions manager here which is gonna be totally fine we'll give it a moment here to launch and hopefully we don't have to spend drivers or anything else with micro case but i guess we'll find out here in a moment um so it's connecting actually i'm just thinking this is an ubuntu instance so i i don't even know who i am so type who am i ssm user um good question i usually do those amazon links too so we'll type in sessions manager ubuntu user cause i don't know what the user's called like who's the default user okay so aws ubuntu default user it says okay so for amazon linux 2 it's ec2 user and it's ubuntu because we need to be the right user when we do this it always logs us as the ssm1 which is silly we'll type in sudo su hyphen and then we'll type ubuntu and we become the user that we want to be there we go we'll type in clear notice that this is a lot more limited and you'll just have to live with it it's not a big deal and so what we'll want to do is go install micro k8s and hopefully we don't run into any serious problems but we'll go here and hit paste we'll try again copy and then paste there we go hit enter and that will install it close that off there and we'll wait a little bit of time here shouldn't take too long to install actually while it's going let's go take a look at link rd so i mean there's istio there's linker d um we're doing liquor d just because that's what i do the lab with uh we could have done istio but it's just what it is so linker d is a service mesh it gives you all this cool functionality on top of it and it makes it really clear once you start to see the dashboard so if we type in like linker d linker d uh viz here i'm just trying to show you what it looks like here if i can find a screenshot maybe type dashboard the idea is once we get it running we're going to get all this kind of rich information about our network and stuff like that and that's really useful when you have a lot of pods and you're just trying to figure out what's going on like where are requests coming from what's the latency all sorts of information that's within prometheus okay and then there's buoyant which we might use here so let's go back here and see if it's installed it is installed so now we'll install cube ctl and now we will see if our micro ks is running so we'll say get pods see if it works that's okay we'll type in micro k8 in the front of it and we'll run these lines as per usual as it likes so it's a bit tricky here so you might have to right click go paste and then we'll grab this here if the font is too small i'll try to bump it up here there we go and we'll go back here and now we'll type micro k8 um config or actually we'll type in cube ctl first get pods we don't expect to find anything good and then from here we'll type in config lesson arrow tilde and then cube config we'll put pseudo in front of that oh it's really not liking today permission denied okay well what if we get the contents of that file because that's all we need really um cube ctl get pods no because all that's doing is it's probably piping this information into here so maybe we can just copy the contents like this and i'm going to type in vi tilde cube config it's in vi so by every key matter so hit i to go to insert mode right click paste okay and just make sure it's not cut off at the top i hit escape to go back to visual mode notice when i hit i it goes to insert mode i hit escape i'm out of here so now what i want to do is type in colon wq exit type clear then we'll type cube ctl um get pods okay so now cube ctl is working as expected so now we'll go and type in micro k8 dns and then linker d linker d and we'll have to type enable in the front there so that's going to go install a bunch of stuff it's going to create its own name spaces and stuff so you can see it's creating service count config maps deployments all sorts of fun stuff while it's going let's go look up linker d tutorial here getting started i'm just going to see if this is the thing that i use to go through it yeah generally i think so we'll have to do link or do check here okay we'll go back over here the only uh unfortunate part is that we don't have multiple tabs right so uh when we're running things here it's going to be a little bit hard hopefully we don't need to have more than one tab for this to work i'll see you back here when this stops i don't know this is just going to happen for a very long time or if it will finish after a while but i'll see you back here if and when this is done okay all right welcome back so it did it did finish installing it installed a lot of stuff like tons and tons of stuff let's just take a look at what we have um so we'll type in cube ctl get pods hyphen a and you can see that we have a linker d name space a linker d vis namespace identity destination proxy injector grafana prometheus web all this sorts of stuff um just to understand the architecture better we'll type in linker d architecture i don't know if it's going to help much but we'll take a look at it anyway because they have this architectural diagram so the idea is that it created a control plane and a data plane right so and these are um well i guess this is your usual stuff right this is your stuff but it created its own control plane uh in the linker d name space and here it has an identity a destination proxy stuff like that but what we'll need to do is we will need to run a program and then inject um the sidecar into the pod so that link rd can monitor it and we could be using our deployment yaml file with our sinatra app but because we're in this very constrained environment we'll just use the one that they have okay so they have one here for uh emoji voto and so we'll go ahead and copy this one and so it looks like they have their own yaml file so we'll go ahead here back to sessions manager and we will go and paste this in below and hit enter and it looks like we have an app deployed so then we will go back over to here and then it says oh we have port forward so we can do some port forwarding here to this unable to board for it because it's not running so i guess we're waiting for the pods to run so we'll go back here to pods maybe to deploy maybe it's still deploying did this deploy in a particular namespace i'm just going to double check yeah it does have its own namespace so we'll go here and then say n emoji photo and they're ready now so we'll go back here and we'll do some port forwarding okay so that's on port 8080 um oh sorry so um yeah i think it's forwarding to 8080 which is totally fine i don't know if we need to yeah it's going from 127 0 to 0 0.0 like anywhere do we need port forwarding i guess we do double check here what did it create because we don't have an ingress controller so it must be internal it's probably not exposed anyway so we'll go back to our instances and from here we need to find our ip address everything's kind of big because i expanded it for this page here it's just going to keep bumping down like that we'll go in here we'll grab our public ip address and we'll paste this in here and it shouldn't work because we need to open up that security group so we'll go here to our security groups and we'll go to security groups again and we'll edit the inbound rules and we will open up port 8080 so we'll say all tcp custom actually not all tcp yeah we'll do it for us because i don't care i don't want to have to open all the individual ports let's say andrew's computer that's our ip address so no one else can see it and now when we go here to port 8080 we should see there app give it a moment here i think it's just loading i think it is working if it's not we'll do http colon slash sometimes the protocol matters says it should work my thought is that it's not binding to uh the address a it's on localhost and i'm gonna bump this up here so we have port forwarding here but it's not creating a proxy per se um i'm going to look at how we did this earlier because we obviously did it earlier in our like mini cube and cube ctl stuff i just don't remember how we did it so just give me a moment here you know what yeah we need to bind on the address so it's address and then we'll say 0.0.0.0 okay and then we'll go back here hit enter cool we can vote for emojis um i always use upside down and it isn't an option so i guess that's how it is we'll do the leaderboard cool that's a nice little app they have there all right so our app is running and we will go back over to linker d and uh if you click around you might notice that it's a bit broken for example if you try to vote for donut you'll get a 404. sure let's give it a go where's the donut let's try taco that's fine vote on your favorite donut ah 404 okay so we got a 404 there these errors are intentional so we can identify the problem so with emoji installed running we need to add a mesh to it so we are basically injecting it so it says cube ctl get emoji deploy it's going to then do link or d inject and so liquid d inject is going to inject that data into there so this is what's going to install it like the sidecar so that we can track it okay so we'll go back here i'm going to kill this we'll paste that oops we'll paste that in there hit enter uh command not found linker d i think it's because we all have to do micro k it's like this micro k8s hit enter there we go so now it's been injected so technically linker d should be tracking it um it explains it that's totally fine and so then congratulations you now add link or d just as the control plane is possible to verify everything is working the way it should check your data plane with linker d proxy check so we'll copy that line we'll go back here paste it on in hit enter we'll have to do micro k8 in front of it we're getting a bunch of check boxes so i'm assuming yeah we're waiting for the data plan to be good it says everything is aokay so it looks like it's been installed properly i'm kind of curious if we take a look at we say um the pod so we say cube ctl get pods and what's the thing called emoji vote we'll do cube ctl get uh ns it's actually emoji photo and then i just want to take a look at the app because all these have been injected right so i'm just curious if we'd see anything in the describe i don't think we will but just for sanity i'm just going to check okay describe pod paste the pod name in here pods well the pod is right there so i'm not sure what it's talking about did i make a mistake emoji no it looks right all right well whatever oh you know what it's because we need the namespace emojivodo there we go um i'm just trying to see if like we can see anything because the idea with like link or d is that it's it shouldn't touch the code or anything here but we are seeing stuff in here so we can see things like linker d proxy so there's some volumes that are mounted to our thing and then there's a bunch of this stuff so look at the proxy so yeah i guess you can see stuff so i just wanted to see if that was the case we'll go back over to here and we'll explore linker d so we want to install a viz and so visit is an extension that allows us to see metrics so we'll go here and install that all linker d install is doing if you do vis install what it will do is output i'll just run it without the other one just so you can see what it does so we'll paste that in there paste micro k8 and all it's going to do is print out code so this is actually what it's going to run so when you see it piping here it's actually going to do an apply by taking that content so go back here paste the full line in here and run it we have to put micro case s in the front here every single time and so now it is installed vis metrics and then down below here it says to install buoyant and so we don't have to install buoyant right away because that is a thirdparty cloud services this is them being tricky to try to get you to sign up for um services so buoyant is a cloud provider that um tracks a bunch of stuff and it's optional and i might uh do it in this tutorial here but i just like the fact that they did that without telling us what was going on here but now that we have that running we want to be able to see the dashboard so i'm just looking up how do we do that to actually visualize that dashboard here i'm just looking at my instructions when i was going through it um we type in micro k8s linker d viz dashboard okay and so now we have the dashboard running on port 50750 it's not bound to address 0.0.0.0 so i don't know how that's going to work but we'll give it a go anyway and that's where i ran into an issue where i needed an ingress controller so that we could actually see it um i'm just trying to find where we are so if we do this notice that it's not resolving right so the problem is is that we are running the dashboard but the dashboard needs to be um exposed somehow and so i think the way i did it um was setting up an ingress so we'll type in expose this dashboard linker d this is the part that i'm saying that i wasn't sure if i'd be able to show you because i literally didn't test this in the ec2 i just said hey let's do this tutorial okay and so here they're saying like instead of using link or dvis dashboard every time you'd like to see the dashboard make an ingress controller but we need one because we need to run it on port 80 for it to work because i googled as well like other stuff and it says like if you expose it on an ingress controller it's not port 80 it will mess up i couldn't you can't send an ingress controller anything besides http https which is 18443 and that's where we ran into issue so down below here it says engine x that's what we were using before so we probably should just use it and um i guess what we'll do is we'll do what we did before which is uh enable um the index controller from our ingress tutorial so we're going to leverage that knowledge i'm just trying to find where we are here so i'm going to type in micro k8 status and we are going to need ingress so we'll go up here and type in uh micro qris ingress notice that like traffic ingress controller i don't know how to use that one and i only know how to use the nginx one so i don't want to learn something new right now so we'll go back up here and type in enable and while that's going on i'm going to go take a look at our ingress tutorial to try to get a refresher here on how do we set this stuff up so yeah we install ingress oh it already does nginx so it is installing nginx and so i guess all we need to do is now create an ingress yaml file so what we'll do here is it's kind of a pain to write it out so what i'm going to do is go over here back to our cloud9 environment because we should still have that file in our micro ks environment i'm just going to go ctrl 0 to go back to normal size here and we'll launch this here and we'll go grab that and then tweak it just so that it's a bit easier and so i mean we probably could use the one that they have here but i just don't know about it i guess we could try it also needs a password so that authenticates it passes it to basic off um it's also using secret i don't think we need to enable that i don't think we need our rbac this says it's on port 8084 dashboard i think this is fine so what we'll do is we'll go ahead and copy the contents here i know we're launching this but if we'll try to yeah give this a go first so um i'm just double checking here the host does the hostname matter this exposes the dashboard dashboard example.com and protects it with basic auth hmm i don't know if we need to have a host name so we'll just say cubectl or sorry um linkerd expose dashboard stack overflow because i feel like we might see an example that's not using that because this person probably wanted to do a particular port and see they're using the same code here notice that they took out hostname because we don't want to have that hostname and it says it's never going to be a path like that it needs to serve on the root of the url so change the route here and so that's what i remember i was reading here this is correct i've only added using the rewrite target there so looking at this one here i'm going to go copy this content we're going to go over to our micro case environment i'm just going to make a new one here new file called linker d um ingress yaml this doesn't work it's totally fine but i just want to try if we can and so i'm just going to see what they did we can put anything we want in here i think that's what it'll take and so i don't see the annotation on this oh i guess this is the secret so it'd be down below here oh yeah so engine x we've got a bunch of headers here this is this looks fine i guess 8084 that's totally fine so i think what it's saying is you can't do this you can't have link or d on that so i'm going to delete that out of there and then i'm going to go ahead and copy all this here i'm going to right click copy go back to our environment here i'm going to bump up the font i'm just doing control plus to do that and i'm going to touch a new file called ingress.yaml and i'm going to do vi ingress.yaml and this is vim so remember every key matters so hit i and then right click paste and then just uh hit escape and then what i'm doing is i'm hitting um k on my keyboard k goes up so hit k all the way up i'm making sure nothing's cut off and then hit j all the way down jj jjj and that looks fine so i'll do colon wq write and quit and so i have this ingress file and so i'm going to try and deploy this so we'll say cube ctl apply hyphen f ingress yemel and it says no matches for kind ingress in version beta 1. so i'm going to hit up again we'll hit vi ingress yaml and then we'll hit k to go all the way up down we're hit j to go down okay i'm trying to get this beta so if i hit h and l h moves me left l moves me right i'm sorry for all the vim stuff and so what i want you to do is move over to the v using the l button and then hit c for change and then w for word and now it's in insert mode okay so now the keyboard acts like a normal keyboard and we're going to type v1 okay because that code is just old and hit escape and then colon wq all right and then we'll hit up again and then we'll try to apply the ingest controller so it says no match ingress type so i'll go over oh you know it's even more wrong than i think it is we go head up here again sorry for revise so we'll hit h to go to the left all the way to extensions and hit cw when you're on the e c for change w for the word notice that it's an insert mode so now we're typing normally and it's actually networking dot k8s dot io and then hit escape and then colon wq hit up we'll try to apply it again oh it just doesn't want to work okay so error validating ingress yaml um validation unknown field service port in io k8s api networking if you choose to ignore these errors turn validation off i think the reason why is the syntax change as well so hit up again i'm so sorry for this and then hit j to go down down down down and this syntax has changed so instead of it being service name back end like this it's actually if you go over here to the colon or sorry go to this line here and hit or actually go to the sign here to sorry the back end here colon and hit o okay it inserts mode goes the next line and then hit escape and then hit i and then hit tab to indent and then tab again it's not indenting right so i'll hit i and i'll just backspace till we're in the right space and we'll type in service colon escape we'll hit j to go down d d d d okay and then hit enter or sorry o sorry uh i okay indent i know this is a big old mess name you know what i'm going to make it easy for you i'm just going to hit colon wq quit here i'm going to remove this ingress file we're going to just make a new one we'll just do it over here because i don't mean to give you all this vim nonsense and i'm just trying to get back to normal size i gotta go back to this tab here ctrl o you can't do it over here and so the problem here is this is just all old so this should be we'll open up our other one as a comparison here you can tell how tired i am because this is so darn complicated we'll go copy again if you just want to watch you can just watch me do this all it's important is that you see how it works so the back end here should be service and this should be port okay and so we'll copy the contents there go back over to our virtual machine i'm going to bump up the font because it's too darn small it's not letting me bump it up here we go oh hit plus that's why and i went too large now okay there we go and so we'll take touch ingress yaml vi ingress yaml i to go insert mode right click paste so we don't have any mistakes hit escape pro colon wq right to quit and then we will hit up to try to run this again and hopefully we have all the right syntax we still have a darn error okay so um unknown field port for ingress back end oh i made a mistake remove ingress yaml we'll go back over here we'll return to normal size i got to go off to another screen because i can't do control zero there go to the bottom here and what i forgot to do was do number this is on space is four we need this on spaces two this should be number like this indent and that is the right syntax so we'll go ahead copy this again touch ingress yaml we have to spell that correctly i'll bump up the font so you can see what i'm doing and we will do vi ingress again we'll hit i to go insert mode right click paste hit escape colon wq right quit hit up okay it looks a bit better so error validating ingress validation data unknown field port for ingas backend what are they talking about it definitely is a thing so go back over here and we'll just take a look at this back end service port port definitely exists for sure i know that it definitely exists is there something else that i'm doing that's wrong here i'm just going to keep looking it looks right to me i'm just reading it here invalid type got string expected map oh i know what it is if it's a map it's expecting well it's kind of confusing i mean i guess you could do this because that would be a map right hmm let's just see what we do ingress kubernetes just see if we can see an example there uh no we are doing it correctly everything looks fine i'll be back in a moment here and i'll come back with the answer okay all right so it looks like we've hit a dead end so i tried launching linker d in the background and doing port forwarding so by that i mean like i did an ampersand on the end here and so now it's running in the background i don't know how to shut it off and i could run on another port and i tried also an address so i ran on an address and you can do that but then it said hey you have to do dns binding and it's just like almost impossible so i think for the scope of this we don't really need to do more i could show you buoyant but i just don't care about buoyant cloud i'm not here to try to sell a thirdparty provider but let's just take a look at linker d linker d um viz dashboard and just so we can kind of maybe see what it's supposed to do but holy smokes it is super hard to get that set up and it's just because you know we don't have in the community that content that's just like available to copy paste in there i'd have to spend hours to figure that out or stuff like that but anyway for the kcna totally out of scope but i just wanted to show you that like the idea is if we if we were there we would see our grafana dashboard that's powered by prometheus and we get all these fun cool graphs okay so you know hopefully we'll have an opportunity to look at prometheus if i figure it out um if we attempt the managed service it might be a bit easier but uh yeah that is linker d at least service mesh how you set up with micro k8s with the exception of not being able to solve that last part there but again not super important for the kcna just going through the motions to see the components okay also i guess we should shut down that environment so you're not spending money so go over to ec2 and this one's a bit easier we're just going to terminate that one so we'll go over there and we'll look for our micro 8k8 linker d and we will um terminate that okay so there we go hey this is andrew brown from exam pro and in this follow along i'm going to show you how to set up uh kubernetes on a managed service like google cloud platform we're actually going to look at a bunch of them gcp aws azure sivo digitalocean just so you get an idea of what it's like actually setting these up for production um you know they're not going to exactly be on the exam because the exam is more focused on kubernetes cloud data itself so you should know about managed services but i'm going through this with you because in practicality this is where you'd actually deploy kubernetes and if you get to see what it's like on all the different providers even if you can't do what i'm doing and just watch through it you can understand the differences between the offerings how easy it is to do it but measuring my frustration level as i work through it and just kind of get a bit of a difference so we're starting with gcp because it's the easiest um and you know i just want to start off with something really nice and light so what we'll do here is make sure you have a google cloud account i'm not going to show you how to do that but uh once you do have your account and your your full access account there i'm going to go ahead and create a new project you can see i was doing one earlier here and it did not delete but we'll say new project and i'll say gke example 2 and i'm going to go ahead and go and hit create so that's going to create our project and then they usually create pretty darn quick notice it's still going there so we might have to wait a little bit there it is it's ready we'll click over to our gke example too and from here at the top we'll type in gke now if we don't want to do that we could also go on the lefthand side here if you click that button and go down below and go to kubernetes engine but i'm going to type in gke and google makes it super super super super easy to run kubernetes which is no surprise because they originally created kubernetes and then gave it to the cncf right so if they didn't have the easiest platform i would be uh surprised i'll go ahead and enable that and while it's going let's look up gke prices because it's actually pretty in line with aws now i used to say that gke had a free control plane but looks like they don't anymore the control plane is 10 per hour regardless of its autopilot mode or standard mode so you're always paying for that control plane all right difference between autopilot mode and standard mode well standard mode is just like how aws runs with ec2 instances it's just running on virtual machines um and then autopilot mode is running on uh uh cloud run so cloud run is like a bus fargate it's just a way of running serverless containers so this one you pay for what you use this one you pay for the underlying uh virtual machines that you spin up same thing um there are some nice things about google cloud where they have a service called antho so anthos is a way of google cloud anthos is a way of having a gke cluster but you can actually have um nodes running on different environments like aws on your environment stuff like that and manage it from essentially location so there's an additional cost for that i don't really know the use case for this because i just can't really think about it but um one nice thing about anthos is that it has anthous migrate so you go migrate um to gke for some reason like if you want to do it's always under that and what it can do is it can actually import a virtual machine from anywhere like aws and it will turn it into a container and then run it on gke so it's like this really easy way to get your containers or your applications into containers like it will generate out a docker file so i'm just drinking water here but the advantage there is that if you had a vm and you use that tool then you get the docker file you wouldn't have to write it yourself so that's kind of one advantage that i like about that but anyway we're over here we typed in gke we're on our cluster and so we have an option with quick start which is not bad but i'm going to just do it the oldfashioned way create we have two options standard autopilot standard is where you manage the nodes so virtual machines underneath this is using google or run cloud underneath right serverless containers so we'll choose gke standard and we'll need to name our cluster so i'll say gke example and it has to be lowercase of course zonal or regional we just don't want to render a zone we'll let it choose whatever zone it wants uh for the control plane we'll have it release channel we don't care about stack version we'll let it stay up to date and we'll go next and hit create now there was a bunch of other options but i didn't click them and maybe i'll go back there i don't even know if i ever looked at them when i went there oh standard mode configure here default pool yeah so i never even looked at this so it's going to launch up three nodes we really don't need three nodes we could have made it two which is too late now if you're watching this maybe before you launch this maybe you might want to make it two because it's more like think about every node that you run is going to cost money right so we don't want these running too long here it's using e2 e2 medium notice like when i launched up my um uh cloud9 environment i chose four gigabytes of memory because that's what i find it at like they say you can run it on to run it on a less but i would never run it on less than four gigabytes the the nodes 100 gigabytes so you know i run it at 40 but like this is kind of more for a production use case i'm going to go here to security that looks fine metadata that looks fine automation options networking options i'm just seeing if there's anything interesting here yeah the only thing i would have done is i probably would have made it two nodes okay or even one like i only need one additional node there but we're running three nodes which is fine uh for you i i'd probably reduce it just to save money but for me it's fine because we're not going to keep this up for long and so we are just waiting for this to provision and so it says it takes about five minutes it is like the fastest set provisioning i know that like digitalocean says there's four minutes but honestly gke or uh like provisions clusters so darn fast it's really awesome but i'll see you back here in a moment when it's done it's at 67 i'll see you then okay hey this is andrew brown from exam pro and we are back with gke and so uh again i'm telling you this this one's the easiest one to use all we got to do is click connect at the top and what it's going to do is show the command line that we use with gcloud in order to authenticate to the cluster uh and then cube ctl will work but it's even easier that we just hit run in cloud shell it's going to launch cloud shell it's going to establish that connection and then we can do cube ctl right away and as this is working i'm just going to maximize this window so we can see what we're doing and notice that it has a line here so now we just have to hit enter and then we'll say authorize you're gonna be so disappointed when we see uh the azure's uh cloud shell because it's so painful and so if we type in cue whoops if we type in i was just typing there i don't i don't know why i did that but if we type in cube ctl get pods give it a moment here to think so there it is connecting if we do hyphen a we can kind of see all the interesting things that it has here so fluent bit gke metrics connectivity agent dns all this fun kind of stuff metric server and so now what i want to do is just deploy something so let's say cube ctl um create deploy engine x image equals nginx because i just want to expose something so that we can see something um and i think we need a double hyphen notice like when you're doing this when i'm typing it's a bit delayed it might be delayed for you because it is a remote shell okay so it's created that now now what's interesting here is if we go to i don't know why it's on our gka thing but we'll go back to our engine maybe because i expanded it we can actually see our cluster if we go to workloads we can see it in here okay we can click into here we can see stats and stuff it's all hooked up and given us rich information go back to workloads even notice like we can hit deploy here and specify information so i i didn't have to do through here i could just went here and said new container image and just specify the repository and stuff like that so it does really add a very nice additional layer if like you're really not great at the scripting here this makes things so much easier and uh not not for everything though like and by the way we can go here we can edit these live too so we want to edit this server or like this sorry this resource it'll pull up the ammo we can edit it in line here which is really nice but if we go over to the service or service ingress we can't create a service we can create ingress if we already have a service created so not everything is great in google cloud world but it's still extremely good and one of the best okay so i'm going to do cube ctl expose because we want to have a service so that we can connect to ingress so that we can see what we built here so we're doing engine x port equals port 80 okay hyphen hyphen target port equals 80. and then we want to type equals node port because it has to be either load balancer or node port forward to work you can't use cluster ip in order to hook up the ingress to get a load balancer with kubernetes we'll go ahead and hit enter that is now exposed we can go up to our services here and then we can see the services there and now if we want to add ingress we can just checkbox it here and say create ingress and what it's going to do is set up an external load bouncer at a google cloud load balancer so that is fine we'll go down here and choose nginx we can't change either of these right there that's fine http protocol is fine we can preview the ingress here so this is what it's going to do very simple right so nothing complicated we'll go ahead and create i remember how painful it was when we were doing ingress like what we were doing with uh service mesh right so it's so nice that this is super easy to do because that is the honestly the the biggest pain for this stuff is um let me do k8 ingress i don't know if that's gonna be the name of the ingress oh it is okay um because that is usually the largest pain is just doing a deal with ingress and so google knows that and so they make it super easy so now what it's going to do is it's going to spin up a um a load balancer right so it's creating ingress and it says external hp lb and this does take quite a bit of time so i'll see you back here in a bit okay all right so we are back it wasn't a long wait but just know that you should hit the refresh button because i was sitting here it just looked like it wasn't done but it was done for a long time but i was fine because i was doing another follow along but if you're sitting here don't sit here for 100 years hit the refresh button make sure that it is done or not done so it is done and what it's done is it's actually set up a load balancer for us in this case so no if you scroll on down below it says load balancing resources we can click through those and you can see it actually set up a load balancer right here it is we have an ip address we have um probably a like a dns record something somewhere here but we have load balancer we click back here we can see it made a load bouncer for us all right so now if we want to access this i'm just trying to remember how or i'll just look up my instructions that i wrote um i didn't write write it down but you know what we'll just type in cube ctl um because it's an ingress controller so we type in get ingress and there's its ip address it probably is the same yeah it's the same one as here and so if i copy this whoops if i copy this and we go new tab here and we paste it in tada our nginx page works okay one other thing i want to show you is that it has a code editor if you hit open editor we didn't have a need for it but if we did need to use one just to give it a second to launch the reason i'm showing it to you here is because it is really really awesome it's basically all visual studio code we could have technically done instead of using aws cloud9 we could have technically used this uh but the only thing is that it only has a single context at least i don't know how to change the context uh and so the reason i went with cloud9 is because you can spin up multiple virtual machines and stuff like that and cloud shell is more like tied to a single uh virtual machine i don't know how to flip between them okay um but it is it's fine it runs really fast and google cloud is very easy to use but i was more comfortable with cloud9 that's why i used it but i just wanted to show it to you because when we go to use azure i just want to compare it to how awful their shell is like as their code editor and it makes no sense because they make visual studio code and it's terrible but anyway we've accomplished our goal here and so in order to get rid of all this stuff uh what we're going to do is go ahead and delete our cluster so i'll go ahead here and i'm going to go and delete our cluster and now it did create a load bouncer so i'm really hoping that it tears down the load balancer with it um right so we go to load balancers i don't want to kind of delete it and cause an issue here oh it did vanish right away so i guess it does tear it down right away uh and if you really really want to make sure that everything is gone we can go ahead and delete the project or like the yeah the project that we create so we go to manage resources and then you'll see gk2 example we'll go here hit delete and we'll put the name in and it will shut down so no we're not getting billed for anything in that project anymore uh it will be scheduled for delete for march 18th but we don't have to worry about it anymore okay so we are done google cloud we'll move over to azure then we'll do the really hard one at us and then we'll do some bonus ones okay hey this is andrew brown from exam pro in this follow along i'm going to show you how to provision um a kubernetes cluster on microsoft azure or azure i like to say azure it's more fun to say and so their service is called aks or kubernetes service azure kubernetes service and you might have to enable this somewhere in there that's not what i'm going to show you it looks like they got some free training which is kind of cool um but i just know the way that i know how to do it so what we'll do is type in kubernetes at the top we're going to go to kubernetes service notice that they have something called azure arc remember how we talked about anthos well azure arc is like azure's um multicloud service where you can manage uh nodes on other providers and onpremise in one place so kind of a competing service but works a bit different and so we'll do is create a new cluster and from here we will have our subscription and we'll create a new resource group because it'll make it really easy to clean this up afterwards so we'll just say um example so we know what to do there everything's gonna be put in there and the nice thing is they have some options so we are doing this for devtest so we want this to be nice and cheap i'm gonna say aks example here hopefully it lets me name it that way yes it's fine the region's fine i don't care about availability zones the version is fine the one thing about like different providers is that they might have different versions available and some might be more progressive than others i would imagine i would imagine that google is the best at keeping that up to date and the only difference between like some of these tiers is like the availability so we we want less availability because we just don't care for it because for dev and test down below notice it's giving us uh standard b4s 16 gigabytes that's quite a bit that is insane but that's what they're recommending i'm not going to leave that up for long i guess we could change it i'm going to leave it alone i just don't want it to break okay so because i i ran it on this um but to be honest it's kind of expensive i'm going to go with one node because i don't want a bunch of them because i don't know how expensive this thing is and at 16 gigabytes of memory that could be kind of expensive maybe look up the cost of this standard price usually they show you in the menu eh so if we go here to pricing oh i don't know i'm not going to worry about it but you know it's yeah sometimes it'll tell you right here if you open this up they'll tell you the cost 150 55 a month again we're not running this for long so you know uh i guess we could try i just don't want to break we'll go with b2s okay i just don't know if that's what's going to be on for the cluster right but i would imagine that's all the nodes for production workloads at least three nodes are recommended but we only want one so we'll say next and it's going to run the system on a b2 yeah that should be fine everything will be fine i just don't know if it's using like windows servers underneath because if it's using windows servers those always need a lot of memory okay so enable virtual nodes um nope we don't need that enable skill sets sure we can have that left on we have authentication it's going to have system assigned managed identity that's just how it integrates with azure rolebased access controls we'll have it enabled could we do azure active directory sure but i'm not going to turn that on we have the option between cubenet and azure cni if you choose that you have a lot more options but we're going to stick with cube cube cni and we can't use azure option unless we're using azure cni uh notice we can use uh calico but we're not doing any network policies so it does not matter here today um we probably want routing but we're not going to do it that way uh we don't care about container registry right now we don't care about azure policies uh we don't need to worry about encrypting the secret service kind of similar to kms so azure key vault would be the way to secure your secrets in a more secure way and we'll go ahead and review and create and so this will take a bit of time it takes longer than google cloud but way faster than aws okay so we'll go ahead and create this and we'll wait a while and i'll see you back here in a bit i'm just waiting for the next screen to make sure that it is working correctly there we go and so we'll wait till this is done okay so see you then all right so we are back uh didn't have to wait super long i don't know if it tells us the time here but um again not super long to wait maybe 10 minutes i'm not sure but down below it says connect to cluster and that's the first thing we want to do so let's click that and we get a bunch of instructions so that's kind of nice and so here it says open cloud shell and run these two commands so if we press this here it actually runs the first two for us which is nice and from the google cloud one you remember we had a shell so azure has a shell it's not as nice okay um it does have a code editor but like i don't know if we showed the google cloud run code editor i'll probably show it now because i'm actually jumping between all of them right now even though for you it doesn't seem that way but it has a code editor and it is a terrible code editor just awful and it's weird because like microsoft makes visual studio code and you think that they would do a much better job here for it but notice that it ran these two commands here so it ran them for us so we're in there and so now all we have to do is kind of run the stuff kind of here uh but looking through here this stuff isn't very particularly useful um so i'm just trying to think here what did i do i'm gonna go look at my instructions because i think they had a sample app and that's what i ended up using is there uh the app that they provide there yeah so i looked up i think what i did is i looked up ingress because it was getting pretty confusing how to do it because when i went over to uh micro microsoft here and by the way let's just test cube ctl to see if it's working sorry for the small font i don't know if there's much i can do about that not really font size large little cube get pods so we'll say get pods here right and we'll do hyphen a and we'll just expand this so we can at least see what's going on here okay and so a lot less is running here than what we see for a google cloud but you see core dns csi as your disk so it's that's how the disk is being persisted with percent volumes mesh your server tunnel front okay so what we'll do is minimize this for a second uh is it still here is this it uh what'd you do asher azure's interface is so janky uh but oh yeah we just can click it again we can reopen it i just wanted to make sure we could find it so here uh is our kubernetes cluster so like they're very good about showing us the latest stuff so we go there you can see it click into it and um it's similar to google cloud so we can see namespaces we can add namespaces we actually see a little bit more information so we see workloads these are all the system ones i don't remember in google cloud is them really showing us those at least they filter them out so we have deployments pods replica sets staple sets daemons uh json cron jobs and you can add stuff but it's not as nice so you go add with yaml right it's not like a nice big old interface so there is like with quick start applications but these are like applications so you would choose deploy an application voters sample applications yeah so that's the sample app that you can deploy so like we could deploy that but i'd rather just kind of go through the steps ourselves so we actually learn something and not just press a bunch of buttons and have it do it for us but notice like service ingress it's not gonna be as nice as a google cloud just period uh then we've got our node pools and stuff down below here but you can integrate like git ops into it so i whatever like github actions or whatever azure uses but um what i want to do here is bring back up our terminal here and we are going to look up ingress kubernetes azure or azure um or aks now the one thing about microsoft is sometimes their services are clunky like like they're either really good or they're just like there's nothing there but they're always really good about being able to copy paste and get things done via the terminal so i know the first thing we need to do is do a basic configuration so we need to have um an ingus controller and so this one's going to be the engine and nginx ingress controller that we're going to use so we're going to copy the contents of this and we're going to go back over to here we're going to go ahead and just paste that on in and it's going to use helm it's going to install the ingress controller like adabus has their own but for whatever reason kubernetes i'm using nginx but you can configure whatever you want so they have a more customized complicated one which we're not going to do but once our ingress controller is created we are going to then uh build up these demos so go back over here and it is running okay great so now the next thing is we need to get these two files because this is the app they have they have i guess we don't need both um because when i ran it i just did both because they said to do both we'll do both because they might have roots for both and i don't feel like copying paste them out so what we'll do is we'll need to create two files we'll have to do touch app uh app one yaml and we'll say touch app to yaml and then here we'll type in code app one yama to open up in the uh the editor here uh and i have the code here from prior so i'll what i'll do here i'm just going to remove my old ones so that you can go through the pain that i went through getting these going here okay and so just type clear here we refresh that they're gone good this this file's not actually here just pretend that it's not here uh so i'll do touch app one yemel and we'll say code app one yaml here okay it's empty right so then we'll go over here we will copy the contents paste it on in did i get it yeah okay right click paste or space control v i can't right click paste but i can go control v whatever and uh we'll go the top right corner save code app to yaml okay down here copy control v this is what i'm talking about their editor is terrible makes no sense and then we'll say uh code routes.yaml because we need to set up um is it ingress yeah it's ingress and they have a pretty complex routing for us so we'll copy that and we will control v and save it and uh so now we have our three now i won't get this code editor out of the way if it'll let me this is the best i can do and we'll need to run these now here's the thing they say to run these like this but i'll show you what happens makes no sense what happens here so i'm just going to paste that on in notice i have to right click paste there and i can't control v there it just makes no sense uh so if i do app one it does it goes cannot find the file and i i don't know why because if you go ls it's right there so what i did is for when we were doing our service mesh we learned that there was a way to pipe the contents of a file uh there and so i tried that and that works so instead of doing that we do cat app one yellow i'm just gonna show you that dumpster dumps the content there but we can make a pipe and then say cube ctl apply hyphen f uh and so the contents will be this hyphen here don't ask me how it works i just know how it works ingress basic because this is the name space that this this stuff is all being provisioned too if you noticed i didn't point that out but uh well we the uh the controller is in there anyway so we hit uh enter here that'll be one i hit up i hit ctrl a and i move over to two and that's two and then we do our routes can't always trust written tutorials you have to learn okay and so now we have all that stuff set up so now what i want to do is just see our ingress here so cube ctl get actually we can do all for fun uh get all uh hyphen all we learned that in our namespaces tutorial and do hyphen ingress basic and just see what we have here no oh maybe we don't need the hyphen knife at all that's probably for deleting yeah there we go okay so here we can see a bunch of stuff we have our pods our services our deployments our replica sets but it's not showing us ingress and that's actually what we want to see so we'll go ing and i think it's working usually what i'm looking for is if it has an address because if it has an address then ingress is working right if there is no address there then we have a problem and i know because i spent so much time doing this the wrong way and being like where's the darn address so if you got this wrong just shut everything down start over because it's super hard to debug but if we copy this we should be able to go up here just like paste it on in and there it is okay another way we could see this is if we um we're here and we go to our workloads and nope sorry ingress and you go over here and we can see the ip address there so normally when you set up something it's going to have a load balancer but for whatever reason it doesn't with this one doesn't set up a load bouncer so i don't know why so but that's fine like there is a load bouncer but i just mean like a managed load bouncer like we used whatever there's a called application gateway um the load bouncers in kubernetes or sorry uh azure are a little bit funny names like there's one called like they call them gateways a lot like application gateway and stuff like that don't call them load bouncers but it doesn't set up one but we're all done here we accomplished our goal so in order to shut this down what i'm going to do is go look up resources because it's all a resource group so it should be easy for us to shut down we go to resource groups and i called this one okay it's example so we click into that and there's our cluster and we're going to go ahead and delete the resource group and then we say k it's example go ahead we hit delete and that's it and just make sure that it deletes azure is a bit buggy okay you really don't want this running uh and causing you money but i'm shutting my down just make sure to refresh double check um azure is notorious for like being delayed in terms of updating so just come back after 30 minutes an hour just make sure that resource group is gone hit the refresh triple check just be sure okay but that's it we finished um our kubernetes cluster for azure hurrah hey this is andrew brown from exam pro in this follow along we're going to learn how to set up eks so that's awesome elastic kubernetes service to run a cluster and the goal is we just want to run anything on and expose it so we can see the app running in our browser i'm going to tell you right now eks is the hardest and takes the longest to set up it's not important for the exam for you to pass but it really does cement knowledge and give you a practical way of using kubernetes in production use cases and so um you know if you do get stuck don't worry just watch and just see what the process is to understand uh the caveats to actually getting a managed service up and it for me i can do it because i really know aws inside and out but like if somebody was new to it it's super super painful what we'll do is we'll go to the top here and type in eks of course you'll need an aws account and so we'll go over here and the first thing we are going to do when it goes here is we're going to add a cluster we'll go and create ourselves a cluster i'm going to call mine eks example all right and it lets us choose the kubernetes version we're going to need a cluster service role um i'm not sure select whatever let's just let us make one i i remember my other account i already had one so i must have created one uh prior but i guess what we'll need is we'll need that cluster rolled so there's a couple ways we can do it so here it gives you the instructions to do it or what we can do is use this cloth formation template so i am going to use this cloudformation template you think they'd have a link just to launch it prior to 2020 etc etc whatever okay so what i'll do is i'll copy the contents here tell you aws does not make it easy uh and we'll go to cloudformation it's almost like they want you to use their managed container services but we'll go over to cloudformation this is an infrastructure as a service tool for setting stuff up uh notice like our cloud9 environments are over here so that all got set up here we'll create a new stack and um we want to provide the template so template is ready i just want to i guess i'll have to save it so what we'll need to do is open up any kind of editor so i have a vs code on my local computer so i'm just going to go grab that if it will open here there we go it's opening off screen here i'll show it to you in a moment okay so i have i'm just saving the file over here you can see i have a bunch of other cool scripts there we're gonna go save that to our desktop okay i'm gonna go to my desktop here i'm just not sure what's on my desktop so that's why i'm just not sharing it very clearly and i'm just going to call this application.yaml okay just notice over here it says application.yaml right and i'm going to go ahead and save that and so now what i'll do is go back to cloudformation and i guess we'll just upload the template here on our desktop there it is we'll say open and we'll say next cluster or eks cluster i am rule it's so silly that it's like this i wish it was a managed rule it looked like they're saying that it was at one point just create it just make it for me here ada best please acknowledge yeah yeah yeah yeah all these warnings for a roll um prior to 2020 had an entry with the service link role the policy is no longer required okay well anyway uh we'll let that create there for a moment it should not take much time at all because it's a role those should be super super fast and we'll just keep hitting refresh still works and there it's created so if we go to stack info we can see that it's completed as well we'll go back over here and we'll refresh and there is our role so we'll scroll on down notice we can encrypt our um we can use uh encrypt our secrets using kms so that is adabus's key management service so that's a great way to encrypt your secrets then we have all these subnets and um i already know that's going to tell us to like pair this down it's not going to like them all but what we'll do is we'll scroll on past there we can see we can choose ipv4 ipv6 i'm going to leave it ipv4 public public private private we'll leave it public there's some advanced settings we're not going to touch that notice that it's using vpn cni so amazon has its own i guess plugin and it's going to automatically install that for us the uh for that then we can choose a particular version uh for this so it'll be whatever version there we'll have core dns which is a version there we can choose our cube proxy version we'll go next this is nice we can actually get all logging into cloudwatch logs but i don't care about any of this so i'm just going to say next and then here we have our tags our networking stuff like that i'm expecting it will complain okay we'll hit create because usually it doesn't work because it will say cannot create the cluster because there's not a sufficient capacity to support the cluster so it could be based on a does not have enough resources or that's probably what it is so what i do is i just like take off the last three and then i hope that it works just double checking there and we'll go next next create and there we go now here's the part that's crazy this is going to take at least 30 minutes to spin up all right it takes the longest i have no idea why but um i literally am doing this uh and and i'm going to be doing other follow alongs while i'm waiting for this to spin up and so i'm gonna stop the video here i'll be back in a moment but just go take a break watch your favorite episode of the show and also if you are spitting this up we probably should talk about cost so if we go uh aws eks pricing right because if you don't have a lot of money you might not want to be doing this but if you go to i already told you this but it's going to be 10 cents per hour just for the control plane node so that's going to be the one that controls all of our nodes and then we spin up other ones there and eks can actually run either on ec2 so you can run ec2 instances so you just spin up those nodes and they're always running or it can use a fargate so fargate is serverless containers meaning that you only pay for what you use so you don't pay for servers running all the time but that means that there will be cold starts you can even deploy this on outpost so outpost is like a a server rack that can be put in a data center and runs away specific stuff so if you want to deploy there as well you can do that but um that's all we're talking about for that for now and i'll see you back here in a bit all right so after a really really really really really really long wait the longest wait you'll ever have to wait for a cluster eks is ready uh just beware hit that uh refresh because sometimes it's ready it'll just not update and you have to hit that button once in a while okay so if you go here we have compute we'll have to create some node groups we have networking we have addons authentication logging it's a little bit more bare bones they're not really giving you a whole lot here we can see our workloads here if we click into them but you don't get the same kind of controls we saw in azure kubernetes service and um google cloud kubernetes service like where we could launch things and stuff like that so really it is cli driven again eks is the hardest to set up but it is pretty powerful but not from like an easy to use kind of uh perspective but we are gonna need some uh nodes okay so i'm gonna pull up my instructions uh off screen here because it is super hard super hard uh to set this stuff up okay and so um the first thing i think we'll need is we'll need to open up cloud shell so i'm going to open up cloud shell here at the top there oh it goes a whole new page you think like the other ones that they load in place but nope not aws it launches in a new one they were the they were the third provider in the first tier providers to get a cloud shell so no surprise there i guess theirs is a bit clunky that's fine uh and it doesn't have like a code editor built in but like you don't need it because you got cloud9 so it's not a big deal so we'll pull up the elastic kubernetes service again here just so we have it on this tab over here and um we're gonna have to really rely on a tutorial for this so um we'll just say cloud shell eks because aws does not have good instructions here this person here great instructions okay so i'm going to go here and what i'll do is i'll just first make a bin directory waiting for that cloud shell environment it's also the slowest to start okay and the font is like super small so we'll go here and we'll just go a little extra large okay and we're in better shape so i'll go ahead and paste those commands in good so it's going to create a bin directory for us we're going to install cube ctl okay we're going to um update config this this allows cubectl to talk to our cluster all right hit enter on this one sometimes when you paste it in the last command doesn't get executed you have to hit enter so just be careful there the name here i believe we call it eks example i'm just going to double check yeah it is okay and then we want to install eks ctl so eksctl is the um this databus is cli for eks and it's a way of making a lot easier to uh way of working with eks we could even provisioned our cluster this way but that's not how i did it okay so what we'll do is copy this contents here paste it on in hit paste enter and we'll go back over here to our instructions and helm because we will need helm because that is what we will need to install the um the ingress controller so remember when we were doing or whatever it's called the controller yeah when we were doing the uh azure one we used helm to install i i went through very quickly but we used helm to install it and so it's gonna be the same for aws so now that we have it we need to um actually create a node and or nodes and those are managed with node groups so back in our cluster we'll go to workloads or configuration i guess and we'll go to compute and we'll add a node group and we'll say my node group and then what we'll need is an iron rule so it's a little bit silly because we have to go to i'm console make our own now and we also have to look up how to make one so we have the i am cluster node we looked at up earlier but if you still have that link open you go under there there's the node item rule and they don't give you a cloud formation template for whatever reason but we can just create it in the cli but um i'm just looking here i did it this way i went through the management console because it was easier so here we'll click on uh or we already have it open up here and i'm going to create a new role i'm going to follow these instructions the thing is aws has great documentation but it's very easy to like glance over the most important information because it's so text heavy not broken up in a way that's easy but what we'll do is we will create one for i think it's for ec2 yeah so we'll choose ec2 the reason it changes this interface i do not like it why is it so light and gray it looks like it's uh doesn't work and then this interface is still doesn't make any sense but we need this one this one and this one so we will copy that name and we need that policy we'll hit enter we'll grab that then clear the filters it's still selected see that that means that there's a policy selected even though we don't see it we'll hit enter on that one we'll select that one so now we have two and we'll go back up here and then it says you probably want this one i go okay great and then we select that so we have our three we'll go next make sure you have the three that are there and we'll go back and then it has a nice name for us doesn't matter what we name it but i'm going to stick with the name that they have here paste it on in go to the bottom create the role and our role is created so we can close that tab go back over here refresh if you didn't know aws and you weren't like used to doing this like for some services this would be super painful for me it wasn't too hard but i can imagine hard we don't need a launch template that's totally fine labels taints tags don't care absolutes too notice it's using the t3 medium just like what we're using on cloud nine so that four gigabytes is a good option 20 uh this size i like to do higher like 30 well we'll leave it at 20. we'll leave it to 20 because when we're running it on cloud9 it's sharing both the control plane and the worker note like all the the data planes so the idea is that 20 is probably enough for our nodes i'm gonna put one minimum two maximum desired is one and one so that we are saving money here not like when we were doing it with uh google cloud or azure in particular um so there are three subnets those are fine we'll hit next this all looks fine we'll hit create and now we have to wait for those nodes to spin up while that is going let's go find our instructions for ingress so what i'm going to do is type in ingress eks the first one here and this is the right one and so when i first did this i did not want to read all this and so i kind of skipped through it and then i found out i had to do all these steps so these are some things that we're going to have to do so there's a bunch of prerequisites so the idea is that we are going to be using application load bouncer so that's aws's load bouncer and then we're going to use ingress in order to get to our app and they even have a fun example app that we're going to be using but before we can do that we're going to have to do a bunch of stuff the first thing is to install the application load balancer controller okay and so here we will right click and we have to do a bunch of stuff so the first thing is we'll need to create an impulse so we'll go ahead and the nice thing is they just have raw data here so we just hit copy we don't have to do a crazy amount of work we'll go back to our cloud shell paste this on in here we'll go back to our instructions so we've downloaded we just downloaded the file and so now we'll end up creating it so this is the same thing as if we went through the the im policy and did it all manually so we'll hit paste hit enter and now we have the policy created that's one step and we installed eks uh ctl because we need to um install the item service and actually when i ran this i forgot to replace the my cluster part here so i'm gonna make sure i don't forget to do that and i'm gonna copy this here and i'm going to paste and if you right click place it usually always pops up like that so we'll say eks cluster make sure you don't spell that wrong it's a pain to fix okay or sorry eks example see i'm already go i'm already doing it wrong so 100 make sure that's right because it's such a pain to fix um and then we need our ip our um account id so on another tab here if you drop down there's the account id as well so i'm going to paste that on in there okay and so that should be all good notice the namespace is going to be cube cube system so it's launching in the uh the systems namespace usually don't do that but for this case you do and we'll hit paste and that will create a service account it says unable to create imcount without an imoc id provider so it says eks ctl utils run this first so it's just that eks we haven't randy um ctl and so we just have to basically kind of authenticate and it goes here and says hey if you want to run this you have to hit you have to run approve okay because that was just a dry run so we do this okay and that worked and then what we need to do is go back and run this i'm hitting up making sure that name is correct making sure that account id number is right again it's big pain to fix run that it's going to go ahead and create us a service account waiting for cloud formation stack so it's actually spinning up a cloud formation stack so i think when we delete our cluster then we can just like tear all this stuff down which makes it a lot easier right so we are going to wait on that there for a bit we'll go back to eks and also see if our nodes are ready so i don't see our nodes yet we refresh here oh we just had to refresh there it is and these i probably show up on ec2 so if we go over to ec2 we should see there it is okay um so that is our ec2 instance running we do not see the control plane aws is managing it we can't touch it okay so go back over here this one has finished our node is ready so we can deploy stuff to it but we're still installing this thing here so go back over here and so now we're going to use a helm chart to actually install the application database load balancer now we have our permissions so we grab this first line here we paste it on in we hit enter oh it says it doesn't know what helm is what are you talking about because we definitely installed it so let's go back to this instructions here did i not copy and paste this maybe we didn't run the last line so we'll hit enter here paste enter helm okay there we go we'll hit enter okay so eks has been added so now uh we'll make sure that it's up to date the repo or whatever whatever it wants there repo update and then the next thing is to actually install it so we'll scroll past this stuff it's saying different use cases so we'll grab this one where it's actually installing it we'll right click paste paste it in hit enter enter oh stop stop stop this is where i mess up i i put the wrong name in here okay so i want to paste this in again like this and this is where i ran this issue where i didn't have the right name so i'm going to type in eks example so sorry about that and i'm going to make sure this is spelled the same way capital e is case sensitive i really should just made it all lowercase for us this looks fine we'll hit paste cannot reuse a name that is still in use oh no no no no no no this is a pain um now i don't know because we messed it up so maybe we can uninstall it i don't know like helm uninstall palm uninstall uninstall release so it is possible to do that so i'm really hoping that we can uninstall whatever mess i made there i should have just let it ran and then corrected it i don't know if we need um the second part here but i'll just do this not found oh i know it's going in the cube system ctl so we'll do dang it it's not gonna let me do it again is it let's try this again hit enter cannot reuse the name that is still in use okay so we'll do cube ctl get pods uh we'll say cube system you're never supposed to really touch cube system that's that's the idea here uh yeah cube system you gotta put an n on this so i don't see anything there that's important i guess we'll have to look at the helm chart and see what it's actually installing so we'll type in um this is where this is where the pain was and this is actually where i messed it up and it became really hard and uh i mean like i made it even worse than before so what we'll do it's not all lost it's just that we have to have a little bit of a detour now hopefully you don't have to do this and uh you don't make that mistake and you just carefully watch the video but there is this thing here that's on helm so if i type this in we go a helm chart it should be an artifact here and so the idea is that if we open this we'll see the contents of it right which we learned in our helm tutorial so just going through here i'm looking for the actual contents of the repo what does it install templates installs a deployment ingress class a bunch of things okay but what are they called though because maybe you'll have like the default name in here in the values anyway i'm going to be back i'm going to try to figure this out but i really wish i didn't do that all right so one other thing i might try here is by doing an uh i think i say an upgrade was it i'm just on stack overflow on the righthand side so use an upgrade and then do a hyphen i flag and install the release if it doesn't exist so what we'll do is we're going to copy this and i'm just going to see if this works as a fix right if you installed and you didn't make the mistake then you are fine but for me whoops we don't want to change the system uh the that there but this is just kind of painful so what we'll do is say eks example here make sure that is right and then we'll go ahead and hit paste hit enter well it's only going to work if i type it right you know so up grade eks example paste oh nice okay so we are super super lucky that we're back in business here um so we'll go back over to our tutorial and here we can verify whether it's working so we'll go over here paste it in okay that looks fine and then we'll go back over here and that's all there is to that part next part is we need to do this tagging for our subnets because if we don't do it it doesn't know where to use the load balancer so what we need to do is go back to eks if we go to details it should show us the um subnets here so i'm going to open one two and three and then we're going to have to add tags so we'll give it a moment there and so there this one is we'll check box it go to tags i told you a bus is a lot of work um and so we need i'm just going to open all three so i'm gonna have to like figure this stuff out here manage tags manage tags and okay so the first one is this so we grab this paste it in and this is going to be eks example shared okay i'm just going to copy that so i don't have to keep typing it shared shared and we don't just need one tag we need two tags so go here and then they say you need this one as well so we'll go here and then the value here is one and then here and then the value here is one and then we go here and the value here is one and then we'll go save save and save okay oh you know what this is for private it's supposed to be this one bro for frick's sakes no biggie we'll just go and add those you can just tell like eks just crushes my spirit okay we'll just go back over here copy that why won't it paste oh i can't actually remove it okay so stupid that interfaces um we'll go over to here remove and then one and hit save and then go back to here and then we'll add this here and then remove that one and hit save great so those are all updated so now we meet all of our requirements so the next thing that we want to do is run the fun app that comes along here so they have a sample app and they are using fargate but we are not using fargate so we'll skip that and we'll just go ahead and grab this it's going to launch right into our node and so we'll paste this in and we'll hit enter and it says that it's created the app um and if we want to see the contents of that file like we can just paste the link up here and if we just take this out we can actually see what's here so we have a service ingress a deployment and a namespace okay the game is 2048 you might have heard of this game before and i think that's it but i'll double check the instructions see if there's anything additional here uh no but it will say i guess we download the file we actually have to run it okay so we will oh no no i think we did yeah we ran it okay the instructions are really weird how they just kind of weird iterations there the same thing but uh if we go down below here and we do the get ingress right if we check our ingress right and if it has an address that's how you always know if this stuff is working so if we copy this and go anywhere up here it says cannot be reached okay uh but it has a port so maybe what we'll do is we'll just double check to make sure everything's working correctly i'm going to go over to ec2 here because that's where our load bouncers are because it created an alb right and if i go to the load bouncer here i was just hoping that it worked and so there is our load balancer and i'm just going to make sure that it can be reached it's also possible that the node is still starting and there's nothing wrong with it and so this would normally have a target this is an alb right so this should oh we actually have to go to target groups to see if we go to target groups here and select uh says healthy targets it has targets it says initial target registration is in progress so it's still making like uh determining whether to help you okay so now it's healthy if we go back here and try it does it work there we go cool and so we have a game so join the two numbers to get started uh uh okay use your arrow keys oh okay somebody in the bus had too much time on their hands they're like okay what excuse can i have to make a game they probably worked in like the kubernetes department or eks but anyway that's a fun game and it's cool that we get to deploy that so now we are all done that was the hardest one that was super hard and so what we'll do is go over to eks and we'll go ahead and delete it to confirm type in my node group sorry so just notice we are not deleting the cluster first go to eks carefully you don't click on ecs otherwise you go somewhere weird but we have to destroy the node group first i know this because it will complain if we don't so we call this my node group that's what it's telling us to type in and then once that is gone then we can go ahead and delete the cluster i can try this right now but i don't know if it'll take it until the node group's gone it hasn't finished attaching yet so i'll be back here in a little bit it takes a little bit time to delete all right so now that our node group has been deleted we can go ahead and delete our cluster so we'll go to the top here delete our cluster and type in eks example now aws does not like azure and gcp where they have resource groups or projects so you got to be really careful about what is lingering here so just make sure double check to make sure these things are deleted after it's deleted uh we did create a load balancer um but it was created through kubernetes so i'm hoping that it just deletes itself but if it doesn't you should double check because a load balance is going to cost you 15 bucks a month if you forget to delete it if you go to the lefthand side to ec2 and go to load bouncers you know it should be deleting or something like that and uh you know cloudformation some of these things were spun up there so sometimes that's a good indicator if they're all getting torn down so i'm going to go over here and just double check and so we have the cluster roll ah the load bouncer is here okay and so if i go to resources it's just a roll so there's no load balancer here so i'm really hoping that it tears it down so what i'll do is i will i guess i'll just have to make sure i'll just uh i'll have a i'll check back in here with you and just make sure that it's gone well here this one's gone so let's go check the ec2 instance i didn't think it would delete the cluster that fast because it takes so long to create clusters if we go to our load balancer it's still here so tricky tricky aws so we'll go ahead and delete it and that should be that i'm going to just double check is there any auto scaling groups there shouldn't be nope we're fine so that was the only thing i was really worried about was that autos or that application load balancer because that costs you 15 bucks a month if you forget about it but now you're aokay and we did the hardest one so uh feel proud uh that you've tackled uh some of the hardest managed providers and there you go hey this is andrew brown from exam pro in this follow along we're going to look at how to utilize uh ibm's uh offering for kubernetes so here i have an ibm account you'll have to go create one i'm going to go ahead and log in to my account now they do have a very sleek ui i have to say for um a cloud provider and so give a moment to log in here now i did already launch a cluster and the reason why it takes so long to launch them and i really don't want to have to launch one and wait so what i'll do is i'll kind of piecemeal this in the sense that i will uh show you kind of the creation process but i won't hit the create button and then we'll go to the active cluster and we'll see if we can kind of do stuff there so what i want to do is just make sure my running cluster is already there so i'm just kind of trying to remember i don't use ibm cloud too much so i'm not sure the interface too much but if i go to kubernetes here i should have a cluster already running because it did take a long time for it to spin up like longer than aws so i think uh kubernetes or ibm wins the longest time to revision control plane award uh if there is ever a reward not a good award to to win but the idea here is you'll go to ibm cloud lefthand side and you're going to go to kubernetes and then from there you're going to create a new cluster so i'll hit i guess i can't rightclick that but we click it and they have two plans and don't get shocked here i know this is 2 500 and that is a bit crazy okay but they do have a free plan and that could be the reason why it takes so long to provision maybe it's slower so you you have a free plan that lasts for a cluster that will last for their days and it'll automatically delete it's for uh just tinkering around and doing things with so notice that it is free but there could be some additional charges but let's take a look at the standard to understand the cost of kubernetes on ibm cloud so down below we have i classic and vpc and then you can choose your resource group um for whatever and then it just shows you geography availability metro symmetra is just i guess the town or whatever uh and then notice down below this is basically the cost here that's driving up is the amount of nodes but if we scroll past it for a second there's these integrations that they have which is nice activity tracking logging monitoring and so these technically add a cost so if you check box them off it actually doesn't really lower the cost there so it's not a true cost but if we go up here maybe the problem is our instance is too large so it's at 16 gigabytes we know that the minimum should probably be something like four so if we switch over to uh four here i'm just looking which one's cheaper this one's the cheaper one and so we will choose that one so save per tool and now it's down to 982 still really really expensive so let's go down to one node and look it's 340 dollars so for whatever reason i don't know the logic behind it but it's extremely extremely expensive but hold on here let's take a look here so this is 15 cents here it still says three workers three zones ah okay so that's the reason why it's so darn expensive so if we were to reduce the amount of zones there we go okay so that looks a lot more reasonable so this says a hundred dollars and i guess that actually is pretty cost effective so like if we had a two v uh c uh uh v cpus and four memory let's go take a look here so if we go as a comparison we'll use ewbs for pricing here uh and so we're using a t3 medium so if we go to ht3 medium and we look at the cost it says zero three three six and theirs is zero fifteen cents for something comparable at least that's what i it looks comparable to me we go 730 so it'd be 24 but like with the control plane it pretty much is the same cost but overall the worker nodes are more expensive if you had to scale up for whatever reason ibm cloud is super super super expensive uh all i can think of is that maybe there's something i don't understand about this vm so i'm just trying to look if there's something maybe more cost effective so like if we went virtual shared i'm just trying to find the lowest cost here no so i guess they're just generally expensive uh okay so but anyway the idea is you'd go free and then you'd uh create the cluster i can't make more than one free cluster but you create it and then what it will do is it will start to uh provision like it will get it ready to go and i'm gonna tell you it takes forever so it'll start the control plane and then it has to start the nodes so you will see it um if we go over here to the ibm cloud yeah you'll see like here it'll be like progress progress progress progress and then when that finish will say available and you go to the worker nodes and this will be like progress progress progress progress but they do have a really nice interface so it's not showing up here but it was showing the instructions before if i click help yeah there it is and so it popped up and it was like here's your instructions and i really liked that and another thing is it has the kubernetes dashboard so you press it and i mean i haven't seen it work yet but i'll click it and we should be able to get our dashboard so that's really nice i don't see any of the providers doing that and i don't know why they don't do that because it's super super nice um yeah i like that but uh let's see if we can connect never done it before so ibm has their own cloud shell so we'll click on that and add that there we'll see if we can get like an ingress controller connection we might not be able to do it on the free tier but let's see what we can accomplish and while that's going i'm just pulling up some instructions here to see if i have some reusable ones here there we go so this one's up i'm not sure if there's a way for me to increase the font here um i don't see an option i can try control plus which is kind of a weird way of doing it but it will work and so we'll go back over here this will be all over uh too large and we'll copy this line here that's to connect to the cluster so paste that on in we'll hit enter and we'll type in cube ctl get pods hyphen a just to kind of look at what there is now they got calico core dns dashboard metrics hopefully every time i do this with the providers you kind of see that everything's a little bit different and we'll carry that information forward uh there's olm operator i don't know what that is but yeah we have some nodes so i guess we can go ahead and start launching stuff so we'll do what we normally do so we'll do cube ctl create deploy engine x image equals nginx um and i think that is it yeah okay and we'll hit enter so we have our deployment look here deploy your app so set up an image repository deploy your app we already did that expose your app test access your apps create a node port to test your app if we click this where does it go i'm just curious if they like have anything built in i was thinking maybe we'd go like to the worker nodes or something and and uh you know like how we saw gcp no it's just instructions okay so if we go into worker note over here can we click into this or configure it no if we go to the worker pools um nothing super exciting they have this devops which is i guess like get ops it's probably like do continuous delivery so i guess that's kind of nice but i'm not really seeing any kind of like easy to use interface so i think like basically it's just a cluster with no additional fluff now what's interesting is they do have a bunch of these addons here so you can press it and it will automatically install so that's kind of nice um i'm just trying to look around here because i really haven't poked around it's all addons okay so that's pretty much it so what we'll do is we'll try to uh we'll make a service i guess so we'll need a node port service let's see what they say here so i guess they're suggesting to make a node port get a public ip address worker node in the cluster if you want to ask that we're going to from private get the private ip what i'm wondering about is like ingress um yeah i don't know i'm not sure let's say uh ibm cloud ingress kubernetes so here we have a service that's being exposed up then there's ibm provider domain so yeah i'm not sure i guess i could spend more time with it it looks like this is kind of the ingress here i wonder if we could just use this show more oh it does say engine x um i'm thinking about it yeah let's let's try it okay so we'll say uh cube ctl um expose sorry i'll bump this font up here i think i went to baker deploy um engine x port 80. i'm just trying to remember off the top my head target port 80 that's the way i learn i have to try to like recall from memory and get it right and then we have the type right and we'll say node port cool um and so then we need ingress so we'll go over here to this and i mean we've done lots of different ingress here and so um i don't think there's like an editor per se no but uh what i'll do i'm just gonna do ls i'm gonna say bye i'm gonna say ingress yaml and this is vi so everything's hard so i'm gonna hit i for insert and then i'm going to right click and paste it didn't like that so i'm going to hit colon control q it's all the way over here uh q q quit so i'm gonna try this again ctrl v maybe oops sorry i control v huh so i don't i can't seem to really paste this in here so i think we're pretty much bust here because otherwise we in order to get this file in here we need to connect cube ctl on my local computer or something else so i would say we're pretty much bust here and i'm not really that worried about trying to get ibm cloud to work but just more show to show you the interface we've seen a lot of times how this stuff works um but there is a little bit of trouble here so i think that's all i can really do here for ibm cloud but hopefully that gives you kind of an idea of what it looks like for ibm cloud but i was hoping what would happen is if we deployed that ingress that maybe it would just work but then down below uh exposing the lb to your kubernetes ingress cluster so there might be like some alb stuff so i would think that either maybe you'd have to like enable alb but like since we're doing this in the free tier i don't even know if they give us that so that's why i i don't have confidence that we can expose it but we would need ingress a so i think we're pretty much done here i'm just going to go ahead and click in here and we're going to delete this cluster and we'll i guess we'll type it again here and there we go and just make sure it is deleting i'm just going to refresh here there we go and i'll see you later okay hey this is andrew brown from exam pro and this follow along we are going to learn how to launch a kubernetes cluster on digitalocean so if you've never used digitalocean they're kind of like a cloud service provider i call them a third tier cloud cloud service provider because they were traditionally a vps a virtual private server basically just virtual machines and they've expanded to have a bunch of other stuff such as kubernetes so i actually already launched a cluster on here um but i am going to show you how to create one so uh digitalocean is known for being really really really easy to use so if we go here and create a cluster and we'll give a moment here the idea here is we'll select our kubernetes version so they have recommended version and then your your location so maybe i want to toronto we have our vpc network we'll have our node pool name and then here it's going to say how many nodes you want we're going to go down to one because we want to save money and notice that it's making the recommendation of about um four gigabytes it says 2.5 of ram usable so i guess that's fine um you know like i aim for like two to four uh gigabytes for for uh worker notes like if it's a single node like we're doing with micro k8 it's on cloud nine then it makes sense to have four gigabytes but here two two will do fine okay so anyway twenty dollars a month very very very cost effective i don't even know if the control plane cost anything digital ocean that well so i can't say for certain so i shouldn't say that absolutely but i can't seem to find the pricing but i think the the control plane tier is free i think um unless one of the nodes is actually the whatever node so anyway um friend downtime that makes sense so we'll have our name here whatever you want to call it then we go ahead and create the cluster and so we'll wait here a moment and so here it will start to spin up and it says it'll take about four minutes to create usually it has a message there yeah and this is what i like about digitalocean is that they give you all the information you need right in place so thank you for using oh thank you thank you for using manage kubernetes your kubernetes cluster is being provisioned provisioning usually is complete within four minutes you can configure the cluster as you wait and then go through these steps and it tells you like how do you connect and all that kind of stuff so that's really really nice of them and they have oneclick installs just like how we saw with ibm cloud um i don't think we saw this for their providers but like um i mean like google cloud had that marketplace so i guess they do um and i guess azure i don't know i don't know if we saw a marketplace for azure but it's nice if you want to just install a controller and if we want to do anything we probably would need an ingress controller if we could figure out how to use it um and so it's pretty clear okay so i already have one running to save us time because i don't want to wait a hundred years for it to spin up or stop the video and come back here uh so i'm going to go into here and notice i already spent a dollar after a day so it's not too bad but if we go to resources here we can see our node pool and we can see the two nodes i guess underneath and then there's a load balancer attached so low bouncers and volume should be managed etc we have uh any insights here just waiting for the load so we get some metrics so that's nice of them and there are some settings uh but and they also have a button for kubernetes dashboard so that's cool i didn't notice that before only other provider i saw that did that was ibm cloud so i like that so i like having that button there but anyway if we scroll on down um we might want to go install the ingress controller so because we if we want to reach the internet right so just press that button and it does take some time and again that's why i did this ahead of time just so i don't have to be clicking a thousand things here but when we want to connect um the idea here is we need to install the doctel kubernetes cluster config thing this thing here so in order to do this we're going to need an environment so i'm going to go back to my italy's account we're going to do it via cloud9 okay we probably could even do it um oh sorry wrong account uh this is developers uh andrew brown i should already be logged in here but i'm not just give me a moment here and i think this is the right password so we'll just paste that on in there is that the right account nope nope nope nope i have more than one account here and oh it's this one i have a lot of aws accounts so we'll get in here and what we'll do is we'll go back to cloud9 we could probably do this in the shell but just in case we need an editor could have went over to gcp but we haven't done much in gcp so i don't want to force you to be over there okay so we'll launch our micro k its environment and then what we'll need to do is install the uh the docker ctl right so they have a guide on that and digitalocean is known for having really really really really really good guides they're just known for it and so they have a snap install so that's pretty nice so once it spins up the idea is we're going to run this line and then using docker tl integration cube ctl requires cube config so i guess that would mess with our configuration with micro k8s um i'm just trying to decide here because you know what i actually want a fresh environment i don't want to mess with a micro case so we'll just say docker env environment unless docker has its own cloud shell i don't think it does it really should get one that would be really smart that's a record docker if you're watching or sorry not docker digitalocean if you're watching go get your own like um like get pods you know you can also do this in git pods but i don't want to switch right now so this we don't really need anything large because we're not actually um we're not actually running kubernetes on it so i'm just going to create this environment and then we'll spin that up and as that's way we're going to just take a look at what our next episode so it says sudo snap install that and i believe i choose ubuntu hopefully and then using this integrations but we could probably go over here just run the next line but we probably have to install cube ctl as well right snap runs in complete isolation you need to be grant permissions in your equity systems some doc requires this requires cube consumed okay well we'll figure it out i'm sure it's not that hard but we'll wait for this to spin up while we're waiting we might as well just poke around here so i guess we already kind of looked at all this there's nothing really new to look at come on cloud9 so we're going to have to download this config file for sure so i'm just going to go ahead and download that now and oh is it almost there it's connecting okay so now that we're in uh we'll have to do sudo snap install cube ctl uh and classic so i'm starting to remember the the pseudosnap commands and then for this one it'll probably just be doctl okay i guess this digitalocean ctl is what it stands for there and i guess we could probably do this even though i'm not really 100 sure on it because i know when i tested it i didn't i didn't do that i just i had cube ctl and ctl i ran the one line actually i'm going to run this one because it says i all have to do this access token is required okay doc ctl auth init and so i need an access token so from here what we do is we go to the api and from here you can see i have a bunch so i'm just going to delete these and from here we're going to say cube and we'll create it and this key is going to be temporary so i'm going to copy it and i'm going to end up deleting this so it's not going to be a big deal so i'm going to paste that in there hit enter and it says permission denied so i'm going to do sudo in front of there and then paste it again and that's fine so now if i do soup tpl get pods let's see if i get environment connection was free refused did you specify the host or path um oh you know what it's because we didn't run that other line so let's run this because i think this is just an alternate way so i think the documentation didn't we just do that what if we do sudo on this nope what if we try this again the auth token is valid grant oh so doc tl config maybe we do have to run that other line this one here okay paste and then we'll hit up maybe we'll take out the pseudo maybe it's getting confused because there's pseudo there now i did this on my local host and it was like super easy yeah i'm not i'm oh you know what um are we in the right cluster hold on let me just double check here go back to our kubernetes cluster i want to make sure it's the um the one that already is running uh which is i wanna do the one that's one day old here okay and we will copy that again and i will paste that in again okay and i will run doc ctl off in it again and i will go grab oh this is so frustrating it's probably you know my fault there we'll just generate a new token oh look there's tokens here maybe it created these tokens tokens you have generated with the api i'm confused okay what if we do docs ctl oops let's do doc ctl um cube ctl is that something we could do i don't know i'll be back here in a moment okay all right so every time we try to run doc ctl auth in it we get uh even though i have an api key here right i copy and we paste it here we get the silly error so maybe we just do mk dir home ubuntu is there a config directory there isn't so i'm just going to make one is that the one it wanted okay and now what if we round it again maybe that's the problem okay so doc uh ctl often it worked that time i'm not seeing any weird additional keys there we're gonna try this again using doc steel till snap grant it via the doc ctl cube config plug to use this command so sudo snap this so we'll try this now okay and i wonder if this no i'm just gonna refresh here because there was things popping up here i don't know how they should up in the api yeah so there's this now here i think this just was created when we ran that connect okay and uh so we'll hit enter again and for this this is permission to deny so i'm just going to make it manually i'm trying to copy it one second maybe just copy that i'll try this again there we go cube ctl get pods there we go okay great so now um let's launch ourself a deploy so what we'll do here is type in cube ctl deploy or create deploy engine x image equals nginx and then oh it already exists so maybe i made it prior keep see a tail get pods all right we'll do cube ctl get svc because i've already i might have already created a nginx cluster ip so i'm just going to delete those because i was doing that previously i didn't get rid of them so delete svc engine x um cube ctl get ingress i'm just wondering if i have anything there so i did make one before i'm just going to go ahead and delete it okay okay so the first thing we want to do now so we have our pod right so we'll do cube ctl get or create or sorry it's expose expose deploy nginx um port 80 target port 80. so no sometimes i do equals and sometimes i don't you don't you can do it either way it doesn't matter and then the port or the type or kind where's the type i think it's type is node port and then we'll just do cube ctl um get svc because i want to make sure that is a node port there we go i don't know if it has to be a node port but that's the one i made and so the next thing you do is install the nginx ingress controller okay and so then we need an ingress so i'm just wondering if we can look this up on the docks here so kubernetes uh um i keep on saying docker but it's digital lotion digitalocean ingress okay so here they have a service and this one's just cluster ips not even node port and then they have a deployment here which is fine and then they do a second one for some reason okay they install the nginx controller so that's something that we did so the nginx controller consists of a pod and a service the services the type load balancer etc so i don't think we need to do this because we press the button and that's basically the same thing okay and so then we need an ingress controller and the reason i'm looking it up here is because maybe there is a specific configuration to um digitalocean but it looks like just a standard one here so what i'll do here and i don't know if there's anything about this domain name stuff that we need to figure out i think that's where i kind of ran into some issues where i wasn't sure what to do um let me scroll up here is there anything about the domain name a fully registered domain name is available i don't really want one i just want to deploy one without one so what we'll do is we'll just copy the contents here and we're gonna go over here it's no different than any ingress controller for nginx i think okay and we'll paste that on in there and we're just going to take a look here so one thing i don't want is i don't want the host so what we'll do is take this out here and then here for the service i'm going to type nginx the path is fine there this all looks fine and so what we'll do is type in cube ctl um apply hyphen f ingress okay and if this works what we're expecting to see is an address here but i don't see one and this is the problem i was running into where i wasn't resolving okay so it says endpoints default they should be back and not found i don't think that matters and it is pointing to the nginx instance i just don't know why we don't have so like ingress address empty digitalocean so notice that they're not using host so they're saying it's empty as well if you are using nginx controller you don't need to see the ingress address okay when you install ingress controller to the cluster it creates a load balancer to handle all your incoming requests make sure the below part is completed so let's copy this because we could have installed it via helm or hit that button and so this will just confirm that it's installed i'm pretty sure it is installed trying to make our way back to the lotion so we hit this and so it is there we can see the load balancer i'm actually curious because i think that uh load balancers are in digital ocean i don't use this lotion on a regular basis so i'm not sure if we go to load bouncers did it create one so it did create a load bouncer for us i'm just curious doesn't have an ip address or a dns hostname um so i maybe made one for each one and there are ip addresses which is great and so down below here it says you should see an external ip address corresponding to the ip address of digitalocean load balancer okay and so there's one there one there but we're going to look at what there is here so this one has 10 24 oh sorry external 157 230 70 53 is that the one we're looking for neither of those match the load balancer they're all 68. okay well even if it is let's just go grab this one and see if that works because it says ingress engine x i'm just going to double check that this is ingress nginx controller it's not the same thing svc hmm okay well it's there so it does work and it is routing tangent x so it does work even though we don't have the address it's just a lack of confidence because i just kind of expected the address to be there but the digital ocean wasn't too bad so we'll go here and we'll go ahead and delete this and hit destroy and we'll put the name in here oh we want the load bouncer i didn't checkbox the bouncer but when we go destroy here let destroy that other one didn't have any resources because that was the one that i just spun up to show you and so because i didn't checkbox that thing i'm going to go to networking here and just make sure i remove those because this probably costs something i would think so we'll hit destroy we'll copy the name there and hit destroy i don't there's like any volumes lingering around so i'm just gonna double check nope there's no droplets or anything so uh yeah that's all there is to it um and there you go that's it hey everybody this is andrew brown from exam pro in this follow along i'm going to show you how to launch a kubernetes cluster on sibo so over here you can see i have a credit balance of 250 dollars when you first sign up you get 250 for free uh for a month and so you can see that i have a 0.3 spend because i launched one cluster uh sivo actually has a lot of great educational content for kubernetes um so they cover a lot of stuff here uh and so you know it's just if you need more stuff this is it mine is more certification focused where there's this general knowledge so maybe there's some places where you can fill the gaps in here by using their academy they also have docs here i haven't really gone through their docs so i'm not sure the quality of them but i do know their academy is is aokay and you'll notice that they have kubernetes but looks like they have virtual machines load bouncers volumes and some networking things so what we'll do is go to kubernetes at the top here i'm going to create myself a cluster and here i'm just going to call this cluster uh sivo cluster for fun we can choose how many instances i just want one we choose the default network we can choose an existing firewall and go default default whatever they have there whatever that means i'm not sure what firewalls are on sivo and then there's a different sizes so notice that i mean like if it was me i would go straight to medium and so if you go to extra then it goes hey you should probably choose the medium so i'm like okay cool i know i know what size of carbonate clusters i should be using so we go back here and it's 20 20 a month now i do know that the control plane for sibo is free so that is something that's really great i'm not sure again i'm not sure if they'll be able to do that forever but that is one of their positions when you go to sibo pricing sievo pricing here that is um their advantage where they go look uh you are playing for the control plane and all these other providers and for us uh the control plane is a hundred percent free okay so uh obviously these costs are are quite large but um but yeah so that is an advantage with sibo now what we'll do is go ahead and launch a new cluster oh sorry i was already launching one wasn't i uh so we'll go back to this tab here we have our medium you can see monthly or hourly you're not gonna get charged like immediately twenty dollars it's gonna be whatever it is percent and then we have some stuff in the marketplace so just like everything there are marketplaces might just wanna poke around here to see what they have so pretty pretty standard stuff i like how like the ci cds are here i didn't notice them on the other ones but maybe i just never noticed but anyway we'll go ahead and create this cluster and so we'll have to wait for it to create and has this countdown here saying how long it's going to take um and it's relatively pretty quick i think we'd say like a few minutes and so um i'll see you back here when it's done okay all right so after waiting a little while our cluster is running so we'll have to take a look at how to uh get working with this and the way it works is you'll need to have a cube config and just download that and use that so what i'm going to do is make my way back over to aws because we're going to use our cloud9 environment and um i have one provision from um before the digitalocean one i'm actually just going to go delete that and make a new one right now okay and then what i'll do here is create a new environment and we'll just say civo env just so we have a way of interacting with it i'm going to stay with the ubuntu just because i usually go with amazon linux but it says it's so easy to do that pseudo snap install for that cube ctl i figured we'll stick with it we'll do a t2 micro because we don't need anything else than that we'll go ahead and spin that up so while we're waiting there i guess the question is is like how do we do ingress because i didn't really do much uh with sibo so maybe what we can do is go take a look at their docs and see if they have instructions for that so i'm going to go here i just want to see um how we can do ingress so because that's always the questions like how do you do ingress how do you do load bouncing right so ingress controller with your own tls certificate um i want to know let's look at load balancing here so creating a load balancer relies on the sivo control uh cloud control manager sending the appropriate request to the cvo api to do load balancing and so then down below the low bouncy algorithm is provided in the round robin here's the annotation i imagine this is the annotation that we expect to see in our ingress controller or something like that external traffic policy session affinity firewall like yeah yeah what i really just want is i just want to know how to do it like what are the stepbystep instructions um for us to deploy so i'm not 100 sure okay and this kind of just says like how easy it is to uh utilize this stuff and that's always the question that i have here so here they have ingress and it looks like this but it's using tls i don't really care about tls all right so we'll go back here and this environment's ready and we will do our sudo snap install cube ctl classic to get that going there and then for civo we definitely have to download the config cubeconfig so we'll download the cubeconfig and i have to spell cubectl correctly it's not going to work and then what we need to do is we need to upload this file so i'm just showing this in my uh folders off screen here we're going to go to file um upload local files and then here i'm just dragging on in that file here and then we're going to hit close and did it upload yep so that's the contents of the file and so what i'll do is i'll say move to cube config we need to put the name of what we're actually moving so sivo this uh sure we'll say touch or make directory cube and then we'll move that into the right place and then we'll see if we can do this cube ctl get pods great and then i'll just do cube ctl create deploy um engine x hyphen hyphen image nginx and we'll hit enter and so that creates that there we'll go back to sivo and see if there's any way we can kind of like view our load like what kind of interface or stuff does it give us or is it really bare bones that's fine if it's bare bones but let's just see what we can see so we're going here and we'll go into our cluster okay and um we'll go to install applications marketplace uh i guess this one doesn't really show you anything and that's totally fine um but uh i guess the question is is like how are we going to network this so i'll be back here in a moment and i'll figure it out and i'll see you here soon okay all right so i'm not sure if this is going to work but because this one is specifically with your own tls certificate but i'm going to just try to maybe follow through with instructions here so we already created our nginx thing and then the next thing we need to do is expose it so what i'm going to do is go back over to cloud9 we'll type in cube ctl and by the way civo has their own um uh like tool like to create stuff so i mean that could be useful but i didn't install it so i guess we'll just do what we can here we'll type in expose deploy nginx and sometimes you'll see people do this and this is totally valid as well but i just like to have with no space or like no hyphen or forward slash there we'll do 80 and then we'll do target port equals 80 and then we'll say um kind or no it's type and we'll say node port because that's just what i've been used to doing and so now we have a service and the next thing we'll need is an ingress controller so i'll type in ingress yaml and then we'll go ahead uh yeah that's fine close tab because we deleted that one there and we'll go here and we'll copy the contents here and we'll modify it a bit so replace with your cluster dns name so we probably need that i don't care about tls this is going to go to engine x we're going to leave the path normal we're going to go to port 80 and it's fine if the path prefix it just looks slightly different because usually when we do ingress they look a little bit different here and we'll just type in ingress here and so what we'll do is grab our cluster dns name which is here okay and then we'll go ahead and post that we usually don't do a host but um for this since they're doing it let's give it a try and we'll do cube ctl apply hyphen f ingress yaml great and so now we can use cube ctl get ingress and it has an address so go grab that whoops did not work as expected 404 not found but at least um it's doing something you know what i mean so we'll go back here http colon slash okay what if we try with the uh the host and we'll advance oops um can i advance you cannot visit right now because the website uses hsts network errors in attack usually temporary with this page will probably work later so usually like if we have that we just advance right it's not a big deal what if i just try http can i do that or does it always forward no it does okay um so that's not going to work hmm so maybe we do need a tls but it does say like bring your own tls certificates so maybe we can just gener generate one in place um so save the cluster in the desired configuration so three node cluster greenstone create certificates so here they have a line to create a certificate okay and we'll go to the next place here and it says waiting okay so generated it out and then it says create a secret okay because maybe the requirements for civos you have to have tls or some kind of secure thing to connect i'm not sure but that is kind of my guess there maybe we should just follow through their tutorial here because they might have other things configured that i'm not aware of so let's just go all the way down the list and follow their instructions so we'll do this and then we'll go down and we'll copy the contents here um i'm just going to rename this to and then we'll just gut the content here well i don't have to get that cluster ip or dns again so i'm just going to paste that there and we'll scroll up and we'll just grab this value here cut and then we will paste this here and we will paste this here and then we'll delete the old one if we can find where it starts which is here i believe good and so then we'll do cube ctl apply hyphen f ingress two and then we'll do ingress and we'll have two there which is totally fine we're going to delete the old one i guess that's the one i made so cube ctl delete ingress or ing ingress clear and then check if everything works so there they are doing a curl there but it looks like you can also just go to the demo so maybe what we'll do is just copy this line here go to our browser type forward slash demo cannot be reached so i mean we can also try the curl right but i don't think that's going to make a difference because you know it's either right or it's wrong you know oh yeah so it's working it says it's here so if that's the case maybe it's just not resolving yet because it's like new or something so i'm going to grab the ip address instead okay and then i'm just going to try this instead 404 not found so it is 100 working it could be a firewall thing so they do have a firewall as we saw as the default so let's take a look at what's there um i'll just go back to command zero here to resize our stuff because that manage firewalls and i don't know anything about civo so i'm just hoping that's like open a port so we will go in and the default firewall has all ports open so if you install anything out of the box we advise to create custom firewalls so everything is open but technically according to uh to their tutorial it is working correctly now why i can't access it here i don't know okay but we do know that when we ran this command for ours and we could also probably even try with the ip address maybe i don't know if that would make it uh like if we still get the same thing so here it says 404 which is fine and we'll try this one again and here we get a 200 hello world so it is working um i don't know if i know enough about sibo to get beyond this point but it's totally fine uh and i think we are aokay here so i'll go back to cloud9 and we will go and clean this up but just do that can i get back to cloud9 easily yes i can and we will delete our sivo environment and we'll go back to our cluster here kubernetes and we will go ahead and delete our cluster and we are all done there you go hey this is andrew brown from exam pro and we're going to take a closer look at namespaces so we have been using namespaces throughout our labs but i figured i'd give it a little extra attention here just to talk about a few extra commands i think that we should know uh so you know if we want to and again i'm in our micro case environment so just make sure you have that spun up there in cloud nine so you can follow along here but what we'll do here is type in cube ctl get namespaces and so we can get a list of our namespaces the server doesn't have a typename system maybe because we got to type it right all right and instead of typing namespaces we can also type ns you can see we created those ones prior fire ice and wind depending on when we did the labs if i want to make a new one it's just cube ctl create ns um borg it would be our new namespace there okay and some interesting things is uh like if you let's say we launched a pod so let's make let's launch a or a deployment into this name space so we type in cube ctl um create deploy uh hyphen hyphen image nginx um n borg okay and so what that will do i really gotta learn how to spell keep ctl right what that will do is launch it in that name space so if we do uh cube ctl get pods that always shows for the default namespace right it's not gonna show us all of our apps it's gonna show what's in the default namespace all right so if we want something from another namespace other than default we have to put the n and then the name so we do borg and i think it's just not there yet because it's still spinning up so if we were to do deploy here then we could see it oh did that not work exactly one name is required okay so i'm not paying attention here i'm just checked out here so we will put the word engine x here great and so now if we go back here we can see that we have a pod but notice that if we uh do not have that it should not show up so all right so you need to have that hyphen end or if you want to be more verbose you can do hyphen hyphen name space it's important to list like the variants because when i was taking the exam like when i didn't know there was a difference between deployment deploy i couldn't tell if one was just like purposely wrong or there was an abbreviation or a shortcut for stuff so just understand there's a lot of shortcuts and i don't think the exam would ever try to trick you by having like fake fake abbreviations like that okay um so if you think it is kind of generally says deployment or deploy then it's probably the same thing on the exam so one interesting thing that we could do would be to um list multiple resources so uh and this is not namespace specific but what we can do is we can say get all and that will actually list out a bunch of resources so here we got our positive deployments or replica sets and some things won't ever show up there like um maybe like config maps secrets ingress okay and so that allows us to bring in a lot more stuff so not all of these things have stuff but notice we are getting things listed out actually they do we did get a lot of stuff here so just realize that that's another way with answers but now can you get multiple name spaces so if i say get uh you know pod and then i say hyphen or hyphen name um default and board just understand that you can only pass it one which is kind of frustrating i kind of wish there was a way to do multiples if there is i sure don't know how to do it but one interesting thing is if you want to delete everything in a namespace you can do delete all and so remember what all returned right so anything that all returned is what it would delete so i think if we wanted to get all of them we probably have to do like cm secrets ingress like that right but i'm just going to do all and then we would do hyphen n and then we would just say borg it says uh no name was specified oh sorry so it'd be all hyphen hyphen or sorry uh dash dash or hyphen hyphen all and so that way you don't specify name you can just say all the names right and so it's deleted those resources there i don't know if the namespace still exists cubectl get namespaces and that's still there as well so we'd have to delete that namespace cubectl delete nsborg and there you go so that's all i really wanted to show you is just some of those extra commands because it's nice to have that delete all ability uh when you put everything in the namespace and you just delete it okay there you go hey this is andrew brown from exam pro in this follow along we're gonna be looking at rolebased access controls also known as rbac i'm going to tell you right now they're a pain to set up they're such a pain and there's two types we have cluster roles like cluster stuff and then there's uh just roles and those are name space which we do cover in the course we're not going to do both because we just be here all day and we really don't need that for the kcna but i do want to set up one a role uh for a particular user and then restrict some resources okay so i'm just going to type in clear here make sure you launch that micro creates environment and this is a little bit tedious and painful but we will get through it um and we might have to kind of like piece it together because it was a lot of work for me to get a working solution here so if you can't get through it don't worry just watch along if you do get stuck and just understand all the components that go involved but i'll get through it here on video here so do not worry but what we'll do is type in micro k8 kate's status and we have the ability to enable it and i just want to note if we do rbac kubernetes here normally what you would do but we're using micro case but normally what you would do is you would have to specify when you start your cluster the api server you'd have to tell it to run an authorization mode of rbac now we don't have to do that we just have to enable it here so just understand that that's abstracted away for us but if you do see on the exam that flag just remember you know what it does okay and so what i want to do is just enable rbac it's right here there it is and we'll go ahead and hit enter and we'll give it a moment to enable it shouldn't take too long so it's reconfiguring the api server so it's likely adding that authorization mode flag so that it is enabled and there it is okay so um i'm looking for my super complex instructions for uh just the role so the first thing we're going to need is we're going to need um a way of identifying our users and there's a few different ways to do that if we go to authentication kerneubernetti's kubernetes don't worry if you can't say it i can't say it half the time or spell it but we have a few different ways that we can authenticate and it says it should say here the list so certifications bearer tokens there's three ways and it's not showing it here let me double check let's go to the top here i remember being the two of the top um maybe it's not this page maybe it's controls here well i'm gonna tell you i'm not sure where the page is but i can tell you that there are three ways uh at least at least three ways of doing it so one's using a certificate another one is having a bearer access token and there's another way they're not going to ask you on the exam so it's not a big deal maybe like the security exam that would be where that would matter but what we're going to do is we're going to generate out a certification because that was the one that i found the easiest to understand even though there's a lot of steps involved so what we'll have to do is generate out ourselves a certificate and so we're going to use openssl to do that so we'll type in openssl we're going to type gen rsa because rsa is the type of um cryptographic thing that we want to do there we'll type out to output um the uh the thing that we're generating here of the key and then we'll put the name so i'm going to call everything developer just to make things easier you could put the name like andrew or something here but just stick with me here and do developer because it becomes super hard to swap this all out so uh 2048 i believe is the length of the key that we want we hit enter and it generates out our rsa private key so now that we have our rsa private key um we need to uh create or generate a client site uh sign request a csr don't ask me what that is i do not remember but i just know that we need to do it so we'll type in openssl req new hyphen key developer.key hyphen out developer.csr that's what we want to generate a csr sebj and then we which stands for subject and then we have this very special syntax so cn is going to be the user's name which is developer we don't want to make a mistake and then oh capital o equals is going to be the group because remember rbacs can have users and groups we're going to call it developer as well we could call it developers but again for simplicity we're just going to call it developer okay i'm going to double check to make sure that's okay and if you're wondering like where did i get all these instructions oh also notice we've got this error so it's possible that this line didn't work i remember getting this error so let me look here because i did have a fix for it can't load root orange okay so i think it's like r d here let's just look it up and see if we can find it very quickly so they say try removing your commenting out rand line in this thing yeah that was the problem here so this is maybe a weird problem with ubuntu um and so i almost didn't catch that error there i'm gonna do ls did it generate out that csr it shouldn't because it wouldn't have worked so i'm going to go ahead and delete that and then what we're going to do i grabbed that line over here this one here and we're going to have to open this up so normally we could do like c9 and file name but unfortunately like last time i did npm install i could try it right now and see if it works npm install uh this this would install the c9 client i should be able to do c9 and then do this but it's not opening up so i think it's because we're on ubuntu and on amazon x2 so i'm just going to clear that out and so if since we can't use cloud9 to easily open it we'll have to open up invi and vi is a big pain because every key matters okay so i'm going to hit enter and before you touch your keyboard understand that every single key doesn't act like a normal key so if i hit j it goes down if i hit k it goes up if i go l it goes right okay if i go h it goes left all right every key is bound to something so you've got to be very careful so what we're going to do is hit j until we get down to this rand line and then we're going to hit i to go to insert mode look down below it says insert mode here and now your keyboard acts like a normal keyboard and your cursor even though it selects the r it's actually in front of it and we're going to make a pound okay that's going to comment it out now to get out of insert mode we'll hit escape on our keyboard okay left and now do colon to execute commands but then this right for w q for quit um and the problem is we're in readonly mode so i had to do sudo there so what i'll have to do is do colon cue exclamation mark and then hit up ctrl a to go to the front sudo enter and it brought me back to the same spot so we'll go i insert mode pound escape notice insert mode is out so now we're in normal mode colon wq i use vim as my primary um layout or keyboard layout so for me it's normal but you know a lot of people aren't and you know just got to be very careful there so we've updated the open ssl cnf and so what we'll do if it generated a key i don't trust it so i'm going to delete it because i don't want to see that message there and we'll hit up until we get back to this one okay notice no errors we'll hit up can we get this one no errors good now the question is where am i getting these crazy commands well the only way is to look up tutorials because if you go on the kubernetes docs they do not help whatsoever um so at least like the kubernetes docs is good in some places bad in other places um but i'm just trying to find the article ahead so it'd be like rbac rbac with kubernetes yeah and i think it was like a mini cube that i used so i had to adopt this rbac probably spell it right so we can find it easily because one of the key skills of learning kubernetes is being able to find solutions and so this is the fellow that made one i don't know if he has a video but generally like a lot of these commands are here um yeah i think he has a video i didn't watch it i just kind of like went through and kind of adapted it to to my needs um but you know a lot of that stuff you just got to look for it or understand how this stuff works so we generated our csr we fixed the rng problem and so now the next thing we need to do um is we need to generate out a crt okay and so that's just another thing we need to generate out but before we do we need the crt these are cert certificates um for kubernetes so if i type in kubernetes certifications or maybe certificates because it's not certifications per se um and we go maybe here nope that's not it maybe here that's not it i'm just looking at i got the links here i'm just trying to google it so we can find it best practice certifications okay so kubernetes best practices you can only imagine how much time i spent looking this stuff up and we go here and so what we're looking for is where these um certificates are so when you install your kubernetes cluster it generates at all these different kinds of certificates so these certificates are used to validate uh authenticity of things right so you say okay this is your certificate we'll bind them together somehow and double check and things like that so we need to have the crt and the ca key and so we need to know where these are now if you're using minicube they install in a particular place if you're using micro k8s they install a particular place but this is where they are by default so micro k8s is somewhere else and so if we want to know where it is i think we'd have to look up like micro k8s um off okay so if we go to the docs here i think it might be under service and ports and if we scroll on down here it shows you here var snap microwaves current certs so this is where they are so what we'll do is copy this directory and we're going to do ls and then paste it in to see where they are and we can see we have a csr conf cube etc etc ah there they are c a c a dot key and then cr dot cert so this is where they are and so now that we have that we now need to type this big old ugly command here so open ssl x509 which is a type of certificate or something i've seen 509 i don't really know what it is you're going to see it for years we'll say developer.csr because we're going to need that and then we do hyphen ca and then we have hyphen ca key and then we have hyphen ca create serial and then out um developer we didn't type everything and i'm just kind of just like filling out the the obvious stuff and then what we'll do is fill in these ones here so we got to fill in a value here okay we got to fill a value in there so what we need is and we'll open up our readme to make this a little bit easier i'm just going to go ahead delete the contents here but i need this address actually just to make this easier i'm going to copy this here because this is such a pain to type and we'll paste it in here and i'm going to grab the contents here just in case we make a mistake and we have to type this like 100 times or like execute it 100 times and so here we want uh ca cert and here we want um this is ca key yeah okay and so this is the big old line that we need so what we'll do whoops did i mess that up okay so we'll copy that line just clear this one out oops again i'll copy this line again yank paste looks okay hit enter and can't open for reading no such file or directory okay so i'll double check one more time i clearly made a mistake oops sorry ls oh crt sorry i wrote cert uh here okay so i'm gonna yank that line oops didn't get it try one more time it's a bit hard to copy i don't want to wrap the text there's a way of wrapping it but it's just like don't have to unwrap it so hit enter okay and generate out our crt are you lost okay so let's create a user now so the way we do that is we're gonna type in cube ctl config set credentials developer hyphen hyphen client certificate equals developer dot crt hyphen hyphen client hyphen key developer dot key if you hear rumbling i don't know if you can hear it but my my stomach's been like rumbling through a lot of the labs so if you hear like a rumbling sound it's probably my stomach um cube ctl config set credentials developer that's the developer that we want uh the client credentials developer crt client hyphen key developer dot key we'll hit enter uh and i typed it wrong so that would be a problem oh boy it's acting funny so what i'll do is i'll just copy the contents here and if we do want to make this easier i can just do this so my life is not so painful this uh backlash allows you to do multi lines okay so client probably should be spelled right paste that in there it didn't take it one second we'll try this again clear paste it in again it's not taking it okay we'll make it one line all right i don't know why it's being silly but whatever so we'll copy this again that or there actually is an error okay i just can't tell if it's a multiline error here it's missing the y here there we go okay so the developer has been set so now we have created our user and if you want to see the users there's a command it's called cube ctl config view hyphen o because like if you do view you get a bunch of this stuff right and so this config file shows us things so we can see admin and developer there if you want to be really clever you can do um parsing so if you do hyphen o you could pass it like jsonpath uh which is a i think it's like jq syntax there's a name for that syntax i can't remember what it's called i covered in one of my other courses asterisk and then we say dot name here okay and um did i type that right i forgot the equals here so we get the names really all it's doing is it's just parsing it through and grabbing out the admin and developer so we can see that we have um uh two users whoops i just wonder if this is actually the same contents as let me just type clear again so type in uh view again here one second so this is that file but i wonder if this is the same file as our cube ctl so if we type in cat which just means concatenate like to print out and we go cube config i wonder if it's just the same darn file uh it's similar so notice like there's some additional stuff here so we have certificate authority which is this big old string here server micro k8 we have context uh uh kind config user preferences but notice that when we do config view it's just a shortened version so ah here it is so look it says data omitted so it is the same file it's just that it's obscuring that just in case for security because i would assume you do not want to share your certificate authority data um i'm not worried about it because this is locked in on a vm and this will be deleted by the time you get to see it so i'm not too worried about it but yeah so all it did was really add it here and then provide those links there that was that big fancy link that we did with the set credentials there so now the next thing we need is a context so a context is to say what user are we right now so these are the users possible users and the the things that we need to uh authenticate them with notice this one uses a token to authenticate but here are the contexts so right now we are the micro creates cluster and uh like that's or sorry that's the context for this cluster because you could have multiple clusters uh that you want to connect to and then this is the user that we are we're the admin user okay and this is for uh micro k8s so what we'll do is um we will actually we can see our current uh contacts by typing current config current context it'll just show us who we are current contact so i don't see here oh it's right here right okay so if we hit that we are micro case right now that's who we are um and so yeah because the name is michael gates the context is called micro caves okay and so now what we can do is we can uh switch our context so or actually we need to make a new context sorry because we don't have a context for our developer yet so we'll type in cube ctl config set context developer hyphen hyphen cluster equals micro k8 cluster and then hyphen hyphen user equals developer okay and so now if we go back and we view you can now see that we have a context here and just make sure the cluster is correct if that's wrong we'll end up with an error so um we haven't switched context yet we are still this context here but um one thing we'll want to do is we will want to uh set some permissions here but before we do i just want to show you a command to say what do we have access to so there's a thing called cubectl auth and auth we have this thing this is can i and we can say list pods and so if i say this it'll tell me whether i'm allowed to list pause this is a way of checking rolebased access controls before we even set them okay and if we're a developer i can say as uh equals developer can i right now oops say uh hyphen hyphen as developer am i allowed to look at it it says no so i created this new user and it's not the admin user and so it doesn't have any access when we first created it so if we were to switch over to it like if we typed in cube ctl um to remember how we switch here oh it's config use context developer really got to learn how to type properly and so if we type in cube ctl get pods we have no access this is forbidden right you don't have any privileges uh you're not allowed to do this okay so what we need to do is create some role based access controls to allow them to have access to those pods so what we'll do is go back and switch to our back to our micro k8s let's type clear here and that's where we need to uh create ourselves a roll and a rule binding so what i'll do i'm just pulling up my example here and then what uh we'll do is type in roll rbac there's a role kubernetes and get some example code so we'll go to using rbac authorization we'll scroll on down and here is basically the same example that we want so i'll make a new one in our case folder here new file um role dot yaml we'll paste that on in there uh this one's called pod reader sure we'll leave it as is that's totally fine and then we need a um a role binding i don't know why they go like role in the role cluster this really should show you end to end but this is how they do it and so if we go down i'm just looking for one that looks like the one that we need these ones are all very silly roll binding i guess we'll use this one even though i really don't like it we'll have to adjust it where this is this one here as well this is the one i think we wanted role binding examples okay so we'll go back here we'll make a new file roll binding yaml okay and um i'm just going to double check because i might have modified this a bit yeah mine's a lot more easier to read so we'll just take out these comments here so that we can just see what we're doing i just don't want any of the stuff in so i can see what's going on just to make sure that it's right so the idea here is that uh we have a role binding kind and then we have our metadata uh read positive is fine the name space is fine so the subject the kind is going to be user the name is going to be developer because we called our developer user and so that's the subject now we need to um have the role reference and so i think it's called pod readers if we go back to the one looks good to me and then if we go back uh we'll just yep that's fine we'll save it make sure that little dot is cleared and so that should be good so we are good to set these up so we'll say cube ctl apply hyphen f roll.yaml i wonder if we can do multiple files like this i don't ever try does that work nope that i typed it wrong so i'll just hit enter oh you know what we probably can do it let me just try this let me just try this and see if this works all right well if there is a way of doing multiple ones i have no idea how to do it and then we'll just say rolebinding.yaml there we go and let's go take a look at what those look like so we say get cube ctl get roll and we'll say describe the role okay so get watch list pods all good there and then we'll say get roll binding okay read pods pod reader and we'll say describe it oops describe user developer that's who's allowed to use this role um so now let's go back up remember that uh auth command we had here it is as developer can they use it yes okay so now what we'll do is we'll switch context so we'll say cube ctl um i already forgot what the command is it is config use context developer i have to type use and we'll say cube ctl get pods cube ctl get pods you got a spell right for it to work and then we'll try svc oh our juice app is running again huh we had destroyed the deploy prior so it shouldn't be running but i guess it's still running whatever anyway um so notice that we get a denied access for services but we can get it for pods so it is working as expected i'm going to just switch back to micro case so we're not confused and that's all i wanted you to learn there are uh role based clusters and they're really a pain to set up and it's just really time consuming but if we go if you really want to know how i know there's a few tutorials if we say rbac cluster roll 100 days of kubernetes i know that um there is a good example there i'm just going to pull it up here if you're if you want to look at it yourself if you are on the example platform all these links are there for you just so you know but let me take a look here i'm trying to find the example um can't really find it but let's say rbac um micro k8s tutorial roll ah yes so anis i know i don't know how to pronounce her name i know her she's super nice she's on twitter uh and so she goes here and i think she's just created a role in cluster roles she does a cluster role one and so if we go through here um that's a role so they create a role i guess i guess he does both but let's just take a look at cluster rule here so for cluster role you have to create a service count um you'll have to set up verbs right so what resources uh oh yeah this is where the sticky part was so in order to um i guess it's not hard but she was using this thirdparty library and i just did not want to use it because then you have to install crew and do that and so probably there's a way i could figure it out but you know honestly we don't really need to learn cluster roles it's not that big of a deal they're very similar to roles they're just not names based right so you won't see a namespace in them and they're for workloads and not necessarily or sorry like they're scoped at the cluster cluster rules what was interesting was service account because service account isn't a user it is a like for workloads and stuff like that but i figured just doing a role and stuff that is fine for kcna and just conceptually understanding other ones is good enough so we'll do is close off these other tabs close this off here this one and this one and that's all i wanted to show you for rbac okay hey this is andrew brown from exam pro and this follow along we're just going to be looking at a bunch of random cube ctl commands that we just didn't cover through any of the other follow alongs but could appear on the exam or just help you contextualize a few things so i'm going to launch my micro k8 environment and as that is spinning up what i'm going to do is make my way over to google on a new tab we're gonna type in cube ctl commands and you know this is the one i usually point you to but i just also noticed if you click even if it's the cheat sheet right here left hand side you go to cube ctl commands and uh that's another way to get to it so there's just some things like api resource cluster management we might want to just take a quick peek at um i don't even look at top what's that the top commands lets you see resource consumption for nodes and or pods cool so maybe we'll just play around with a few of these just to see what we can see so i'm going to type in cube ctl get pods to see what we have here just to see if anything is running yet um and we can always just type in micro k8s as well i'm not sure if we have a permissions ah there we go so we are fine there um i thought we might have a permissions issue there but i'm actually surprised on the server that i don't have um the cube ctl installed because we are on ubuntu so um i can't remember what we did for that i think was something with the config file but um i think it was like cube ctl um config and then we would just do this dot cube uh config oh sorry micro cr8s right did not mean to open up that tab okay it's all just having clear and if i type in cube ctl get pods it's gonna save me some trouble oh cue get pods can i do that no so i see a lot of people alias the k command i'm not sure how that gets done but um let's go back over to here and just look at some of them here so we have cube ctl top and that might be fun to try so let's go give that a go and see what that does um qctl top so this top command allows you to use to see resources consumption on nodes so we need to specify something maybe like node um is unavailable to handle requests let's try pod okay so that didn't work but uh maybe just as a test we can do micro k8s there okay but we are we have pods right all right so i'm not sure as the reason why that's not working but i'm not too worried about it because it's not on the exam but i just saw it there so i just wanted to give it a go api versions is definitely something we might want to try out so we'll just type in clear cube ctl api versions versions and so here it's telling us all the api versions so if we go back over here we'll just give it a read print the support api versions on the server in the form of groups and versions so i guess the idea is that all of these things are apis so what i mean by that is like if we go create a file any kind of file like a persistent volume if you go to the top uh maybe that's not the best example but the idea is you have a group and then you have a version and so if any of the syntax changes because the functionality changes that might be version two that might be version three or down below like c auto scaling we have one two v two beta one v two beta two and so by copying this and putting this up here into the api version will change the nature of these files all right and i don't know if like it's you get what you get i would imagine that you install a version of kubernetes and then that's going to support different ones so as new versions of kubernetes come out i imagine they might drop older versions or things like that or maybe the betas vanish i really don't know um but i mean generally that's the idea behind the api versions let's go back over to cube ctl reference here cluster info might be one that we might want to run here so go ahead and just type in clear at the bottom and then we'll paste that on in there and hit enter so it says the control plane is running uh nothing super exciting now this is micro cave so i'm not sure if we would get more detailed information if we're doing a managed provider if we selfdeployed versus a lightweight distribution like microkates but we get some information there which is nice cordon drain those are really for managing nodes or spinning them spinning them down uncord on as well taint uh as well so these are we're not going to be touching at all this would be something you would do in the cka the administrator then we have other ones down below here i don't even know what alpha is these commands correspond to alpha features oh that sounds like fun let's see what alpha features we have so events is experimental so i imagine that maybe that can be turned on um so yeah down below here so cube cube ctl alpha events might turn it on i suppose or something like that or maybe you just have to put alpha in order to access the events that's probably what it is right um then there's api resources so print the supported api resources on the server so we'll go ahead and copy this we'll type in clear and we'll hit enter and so if we just scroll up here and just expand it a bit not too different from the api versions per se but like here it's saying what is the name of the api resource so like bindings events pods things like that what is its short name what version is this one currently on is it namespaced and what kind is it right so actually uh saying kind in terms of a component or stuff and i guess this would be useful if you wanted to know what kubernetes components are namespace and not right because we said that there are certain components in our namespace section that you can put in any states that you can't put in a namespace and this may also affect whether you use a cluster role or a role for particular things because i would think that if you have a role and then you have a component that isn't that is not allowed to be namespace you probably need to have to use a cluster role uh we don't really get that deep into that kind of stuff with our back or roll based access controls and that kind of level detail because again the kcna level we're not too worried about that information maybe that's the csk or ckad or the cka um config we definitely used and there's a lot of options under there explained uh we have not used so ctl explained uh the command describes the fields associated with each supported resource so that might be fun let's try pods over here enter and we typed in that here so a pod is a collection of containers that run on a host this resource is created by clients and schedules api version kind metadata oh so it actually explains all the fields within um within pods so that's kind of useful and maybe documentation that might not show up actually on the page but it's here in the commands so that's nice but really all i really wanted to do was show you api resources and api versions because i definitely definitely know that those will appear on the exam and you need to know uh what they're there for okay so api resources show you all the possible supported resources uh and then api versions is all the supported api versions okay