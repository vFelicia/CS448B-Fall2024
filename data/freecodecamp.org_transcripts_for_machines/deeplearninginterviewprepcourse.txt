prepare for a job interview about deep learning with this course from tdiv she goes over 50 common interview questions related to deep learning she's an experienced data science professional and has published papers in machine learning scientific journals I'm DF from luner Tech in this course I'm going to teach you everything that you need to know from the topic of deep learning I'm going to help you prepare for your deep learning interviews we are going to answer answer 15 most popular deep learning questions out there that you can definitely expect if you are going to participate in data science machine learning engineering but alsoo AI engineering and research scientist interviews deep learning is one of the most popular topics this days because it forms the Cornerstone of topics such as large language models but also the generative AI it contains many complex Concepts it combines linear algebra mathematics differental ition Theory and also uh Advanced algorithms and they all come together to form this field of AI which is the fundamental part of generative AI the large language models so if you want to get a job in large language models or geni then definitely uh prepare for this type of interviews because deep learning is going to be a concept that your interviews will test you for this course I have specifically designed in such way that we will go from very basic questions to more advanced one so let's first look into the first 50 questions that we will be covering as part of this course the first 10 will be the basic concept so we will start uh very basic we will talk about the concept of deep learning uh what is the difference between deep learning and machine learning we'll also talk about the concept of neural network we will look into its architecture also what is the concept behind neurons what is back propagation how does it work how it is different from GD what is GD what are those different op optimizers we will also be uh covering different activation functions which are really important and we will be covering them in detail to also understand the concept of saturation how this is causing Ving gradient problem and also we'll be covering the um different aspects of just the entire training and testing process behind neural networks and then we will move towards more complex uh topics and those will be the next 10 questions so we will be covering the concept of back propagation in more detail the relationship with the gradient descent and we'll be covering the vanishing as well as the exploring gradient uh problem we will also be covering various uh question specific and uh um problems related to deep learning models that you will be asked during your interviews we will also be covering the loss function and a various type of uh popular loss functions that uh you can expect questions about during your deep learning interviews in the next 10 questions we will be covering the following 10 questions so we will be covering the uh SGD algorithm so more Advan version of GD we will be covering the SGD with momentum what is the role of this momentum we will also be covering the different versions and variants of GD something that comes uh time and time again during the Deep learning interviews such as the badge GD mini badge GD and the SGD what is the difference between them what is the function of bed size another very popular question you can expect we are also going to cover the idea of uh Haitian uh how Haitian is being used as part of different optimization algorithms also what is the RMS prop and what is the U Infamous Adam algorithm then as part of the next 10 questions we will be covering bit more complex uh topics we will be covering the concept of Adam W and then we'll be covering the batch normalization what is the difference between batch and layer normalization uh how layer normalizations are used as part of Transformers what is the residual connection uh what is the function of residual connections and we'll also be talking about gradient clipping cier initialization as well as the uh the ways that we can solve vention gradient problem and the exploding gradient problem then we will also be covering the concept of overfitting definitely something that you can uh expect during your deep learning interviews then as part of the uh final 10 questions of this part of the course we will be covering the concept of Dropout and the regularization in neural networks we will be covering the uh overfeeding problem and how it can be solved in neural networks also what is the difference between drop out and random Forest so different tricky questions that you can expect that have the goal to confuse you but we will clear out everything such that you don't need to uh confuse anything during your interviews then we'll be covering the concept of uh the training versus testing the adjustment when it comes to Dropout we'll be covering the L1 regularization L2 regularization what is the difference between them and the impact on the weights uh the penalization impact and difference how severe they impact the uh moral performance we'll also be covering the concept of uh curse of dimensionality and how deep learning is solving this problem then we will be covering the uh concept of generative models honestly more and more people are getting into deep learning so whenever you want to get into AI or in machine learning you just want to start from the fundamentals and to understand those more complex models such as Transformers uh GPT series T5 in order to understand those mechanisms you need to know the basics and those are the Deep learning fundamentals something that we are going to cover as part of this course because once you know the fundamentals then you will also be prepared to answer more advanced questions and uh also to be able to answer the followup questions because during the Deep learning interviews you are not only going to answer the questions quickly and in a scripted way but you will most likely get those followup questions and I'm going to help you with my detailed explanations in such a way that you will not only be prepared to a those uh basic questions but also the corresponding followup more advanced uh questions do note that the idea of this course is not to learn all the concepts from scratch but instead to either quickly refresh your memory from Deep learning or to uh prepare for your deep learning interviews and you already know the most of these Concepts there are uh quite a lot of complex uh topics that we will be covering as part of deep learning and uh I would suggest you to First refresh your memory when it comes to statistics machine learning and also various type of mathematical Concepts such as the uh matrix multiplication with linear algebra differentiation Theory and then get into uh this interview preparation course when it comes to the uh fundamentals to statistics if you want to refresh your memory in this topic uh before getting into this course I will put a link uh that will forward you to the fundamentals to statistics course as part of lunar Tech where you will be able to learn the statistical Concepts or refresh your memory and when it comes to machine learning my fundamental to machine learning handbook uh from free Cod camp or uh fundamentals to machine learning course as part of ltech boot camp will be able to help you to either learn or refresh your machine learning memory and once you have done that all then you are ready to conquer your deep learning interviews I hope you guys will stick around cuz this course is going to be very helpful for anyone who wants to prepare for deep learning interviews so if you're ready I'm really excited so let's get started so the first question is what is deep learning so in this question your interviewer wants to understand whether do you have an understanding where the Deep learning comes in comparison to machine learning in comparison to AI and you can briefly mention about the idea of neural networks and how it is related to human brain so deep learning is a subset of machine learning which is then a subset of AI so a branch of artificial intelligence which involves training artificial neural networks on large amount of data in order to identify and learn those hidden patterns nonlinear relationships in a data which is something that the traditional machine learning models like linear regression or logistic regression or random forest and exus are not doing so well as the Deep learning models and the the heart of deep learning is the concept of layers because the artificial neuron networks and just in general the Deep learning tries to replicate uh the way human brains work and the way we intercept the and take the input signals from outside world when we see something or hear something or smell something and then use that information in order to learn and in order to whether to make conclusions or to do something and that's exactly what deep learning tries to do it tries to use this idea of naral networks which are deep learning models in order to take the input data in the input layer and then transform this through the activation functions to activate these neurons and then transform it into into activations and this will be in our hidden layers and the magic happens in a deep learning model to identify the nonlinear relationships in the input data and then what it does is that it produces then the output so this is at a very high level the interviewer doesn't want you to explain the idea of Weights the idea of parameters which we will talk about later but then the intervie wants to know whether you have higher level understanding what this deep learning is and maybe also to keep a few examples of deep learning models definitely mention this relationship between the way the human brains and the and the neural networks you specify this idea of input data input signals and activations and hidden layer don't go too much into detail of the architecture of neural networks and do briefly mention that the that these deep learning models examples of deep learning models are for instance artificial neural networks uh reur neural networks lstms the different sorts of more advanced architecture such as variational outo encoders they're also based on neural networks you get the idea so if I were to answer this question very short I would just say that deep learning models are a subset of machine learning models on its turn a branch of artificial intelligence that tries to learn complex data and large amount of data and discover hm patterns and complex structure in this pattern for various type of tasks including computer vision including a machine translation task lot of NLP models um the INF famous Chad gbt so gbt models or T5 all these large language models they are all based on Neal networks and they are deep learning models so the next question is how does de learning differ from the traditional machine learning models here your interview wants to understand whether you know what are the main advantages of the deep learning models compared to machine learning model models so do mention a few examples of traditional machine learning models and then mention a few examples of deep learning models and try to put the focus on one or two advantages of the deep learning models so a few examples of machine learning models are linear regression logistic regression the support Vector machines KN viation algorithm K means or clust string so as an unsupervised learning model theb scan do mention few boosting algorithms such as GBM XG boost you can also mention random Forest very popular for regression as well as classification type of tasks so in this way you will show to your interviewer that you know quite quite well those machine learning algorithms then do mention the Deep learning models which fundamentally differ from the machine learning models and I mention examples such ASN RNN LS grus and outter encoders all base or neural networks do also mention CNN so convolutional neural networks very much popular in the field of computer vision then do point out that the first biggest difference is that uh traditional machine learning models they rely on manual feature extraction while deep learning models they do this automatically and they automatically take the input data as it is we don't need to perform feature selection we don't need to worry about that because our model represented by neural networks especially with this D players is able to identify this patterns in all this data and then extract for itself the most important features that describe our data and then use it for different type of tasks whether it's for regression for classification it actually performs very well so deep learning models perform very well on Big Data something that we have seen machine learning models suffer so ml models are known for becoming worse in terms of their performance when the size of the data increases when the number of features increases they start to overfeed and they start also to become unstable and not accurately predict the output when we increase the number of observations in our data also the include complex features in the data that's something that definitely helped the de learning models outperform the traditional machine learning models especially in the test such as computer vision or speech recognition machine translation and the idea of deep learning models you can even see why they are better than a machine learning models by even looking at the most recent different sorts of applications in the across different Tech Fields you can see that most of them are based on some sort of Naro network based algorithm rather than machine learning models and this is mainly because of this various abilities of deep learning models including ability to handle large data ability to handle complex features and being able to do feature extraction on itself instead of doing it outside of the model or um instead of suffering from various type of problems like the traditional machine learning models do so the next question is what is a neural network so here your interior wants to understand the relationship ship between the neural networks to to the human learning the way we learn and whether you know this components and how the components uh flow so in terms of the training process whether do you know what the weights are what the parameters are whether you know what this hidden layers are whether they they are just one or multiple you understand the concept of deep neural networks and how this flow of information happens and whether you mention all the these elements of the neural networks so I would mention as an answer to this question that neural networks is a foundational Concept in the field of deep learning and in the field of artificial intelligence just in general and it's a computational model that is inspired by biological human brains and it tries to replicate the way humans learn the way they interpret information they take the input data and they they process that information so they do training of the on the input data and they also consistently update on this information so they learn something new they add new information and then they update the the possible outcome and the end result is to minimize the amount of error we are making when we are learning and that's exactly what we are doing in neural networks so the core of neuron network is made up of this units which we are calling neurons like in our human brains and uh these neurons are together together forming the layers we have input layer which is in our which represent our input data then we have this the the weights which showcase how much importance we need to put on different input signals and this weights are what we are calling parameter in our model which we need to consistently update when we are performing the training so the learning process so these layers include the input layer one or more hidden layers which is the middle part of our naral Network and that helps to learn these different patterns helps to understand activate certain neurons and deactivate or activate less the other input neurons and then transforms this information after this activations are done we go through this hidden layer and in after this hidden layer then we can have another hidden layer which will then receive the information from the previous hidden layer and will then continue the learning process on it certain it will be able to learn and understand the previous hidden layers information and as an example of this process think of that in the input we have the different pixels of the images and we want to learn these images and for the model for the nural network to learn it should be able to First learn s some sort of objects in the input data so in the first hidden layer it might discover certain signals certain edges and certain information and then in the next hidden layer it will be able to identify specific objects for instance the the ears of a cat if we want to identify where the image is of a cat or a dog and then passes this information onto your next layer onto the next layer and then we go on to the final layer when we produce the output now now if you are familiar with machine learning and if you are looking at this tutorial I assume that you are familiar with the idea of classification regression on supervised learning supervised learning so the in case of the traditional neural networks we then produce the output and it can be for instance identifying a probability a value between zero and one let's say for our classification task and then this value should be compared uh to the actual class so if we have for instance probability and we transform it to the class then we can update the corresponding so we have the output of our neuron Network in each Epoch and then we use this information to compare to the true labels in our supervised learning process and then we compute the error so we are trying to understand how well we did our prediction and then this information is being used to then understand how moral is being how accurately is the model predicting the results and this then this is what we are calling loss so this loss function is then used in order to understand how much we need to change the weights so how much importance we put on the neurons how much we need to update these weights as well as the other parameters in our model in order to reduce the amount of error moral is making and that's the underlying idea of neural networks to take the input put it through those hidden layers to learn from this input data activate the neurons and have either one hidden layer or multiple hidden layers continuously activate those neurons get the output understand what is the loss that our model is making the overall cost function and then compute the what we are calling gradients and do briefly mention here the idea of gradients but do not go too much into detail of it and then using this grip radians we understand how much we need to update this weights in order to improve our prameters of our model because at the end of the day what we care about for our final result is this model that has the parameters the weight parameters the paramet the bias parameters that will end up generating the most accurate predictions for us and for that we need to continuously learn go through this Loop in order to properly learn and in order to produce a model that makes the minimum amount of error so it's able to properly learn from the input data the next question is explain the concept of neon in deep learning in this question the interview wants to hear about the concept of Weights the concept of bias factors how they are used in order to uptain the socalled Z scores and how the activations are activ ation functions are used to obtain activations and what is the point of this weights and Bas vectors and relationship to the learning process so a neuron in a deep learning it is referred as sometime sometimes referred as artificial neuron which mimics the function of the human brain's neuron but it does so in mathematic and simple way and the idea is that in a neural network the moral gets different inputs and here this input signals we are calling neurons it takes the input data which we represent and call it as an X and we use this x to multiply it by weights because this weights can then tell us how much importance this neuron should have how much attention we need to pay to this neuron and these weights are then multiplied by this input data which you call X those are the neurons and we add a bias Factor because there can be always a bias introduced when we are performing our learning process and the the way it's multiplied by this input data X we are adding then on the top of that the bias factors and this will be our Z scor but then in the next stage before approaching the hidden layer we use what we are calling activation functions and activation function such as sigmoid function Rectify linear unit or leyu all this activation functions what they are doing is that they are mathematically introducing nonlinearity into our neural network and these activation functions are applied to all these different Z scores which is based on the weights and then input data and bias vectors in order to understand how much each neuron should be activated it can be that we want in certain when when looking at the object and trying to understand what is the object in that image based on the pixels it can be that we need to activate certain neurons related to that specific area or that specific object and we should not activate the other neurons and that's exactly what we are doing with this activations of this neurons so here then we are using these activations to introduce non larity for neural network to learn this complex structures in the input data so the next question is explain architecture of neural networks in simple way given that we have already answered the previous questions all mentioning some parts of the architecture and the training process of Naro networks this one will be easy to answer and I will use here a simple visualization for simple Naro networks that consist of just one hidden layer to just summarize everything that we mentioned as part of previous interview questions so the nural network is this multilayer structured model and each layer is transforming the input data step by step think of it like an assembly line so assembly line where every stage adds more complexity on the preview stage and the detail and the complexity is adding more value to the ability of the model to perform predictions so in the beginning the the model has this input layers as you can see here the input layers are represented by this Z1 Z2 Z Tre up until z n and in here you can think of this elements this inputs as part of input layer as features describing the data so it can be for instance that we want to use use neural network to uh estimate the price of a house we need to have certain features to estimate that price we need to learn and in order to understand how we can differentiate different houses and how these features influence the house price so Z1 can be for instance the number of bedrooms of this house Z2 can be the the number of levels of this house that tree can be the whether the house has a swimming pool or not not a can be uh whether the house was built so what is the year that the house was built what is the age of the house all these features they describe a certain element or certain uh characteristic of the house and this will help us and this will help the moral to learn from this feature and to understand what will be possible accurate price price for this house and then we have these different weights as you can see uh we have weight one one weight one two weight 1 three and then we have the corresponding weights for the second input the corresponding weights for the for our third feature so corresponding to Z3 such as w31 w32 and w33 Etc as you can see this weights they show a pile that goes an arrow that goes onto the next layer which is our hidden layer so in this simple architecture we have just we got just one hidden layer but it can also be that you have more hidden layers which is usually CA the case with the traditional deep neural networks hence also the name deep so what this weights do is that they help us to understand how much this input feature should contribute to First hidden unit second hidden unit and third hiding unit so as you can see in our hidden layer we have the three circles these three circles describe the hidden units in our hidden layer and we have the simple structure so we got just three hidden units but this is something that you can decide for yourself when training your model it's also what we are calling hyper parameter that you can tune so in here that you can see that the w11 goes from Z1 to H1 so this is the importantance weights that tell us and tell to the narrow Network how much the input layer should get attention when should get importance when it when it helps you learn the first heing unit so for H1 what is the weight that we need to put on Z one so our first feature then we have the W12 which describes the amount of weight that we need to put on the first input feature when identifying this when learning and putting the information into our second hidden unit H2 so you get the idea so then this this weights and the corresponding bias factors and the input data helps us to compute what we are calling Z scores and Z score so Z can be represented as the W so the weight multipli by X which is the previous in this case the input data plus bias Vector which we sometime refer as B then once we have the Z scores we are then ready to add an nonlinearity so the activations which will then help us to understand how we need to perform aggregation so you can see that when it comes to the first hitting unit H1 there are four different piles so arrows that comes to H1 it is the w11 W uh 21 w31 and wn1 what this basically means is that we have this different input features and per input feature we have the corresponding weight that helps us to understand how much attention we need to pay on these different input features when we are obtaining the H1 so our hidden layers first hidden unit and we can aggregate these different Z scores that we compute based on different weights different input signals and this activation function can significantly change on the way we calculate our activations in the H1 and that's exactly what the activation does so the activation functions then once we have this information we have computed our activation we are ready to compute our output and the output is done after we have computed our H1 H2 and H3 those hidden units have learned based on the weights and bi factors and activation function the information in our input data in the previous layer then we use the final so the final weights so uh wh1 wh2 wh3 in order to understand how much each hidden unit need to contribute to the output layer and in this output layer what we are doing is that we are getting the output in the format that is desired so it can be that we are performing classification and we want to get a probability uh therefore we need to obtain a value between 0 and one then in this output layer we will then have this volue we will transform it to a class and we will compare it to the true label such as zeros and ones you understand how well our simple neural network is performing so the next question is what is an activation function in neural networks so activation function in neural network plays a crucial role and the choice of activation function will uh Define the performance of your neural network we by now have already spoken a lot about the weights and how we compute the Z scores using the weights the input data or the previous layers input a previous layers output we add the bias vectors to obtain the Z scores and then we need to apply activation function to obtain the activation so how much each neuron should be activated when Computing the next layer so that is exactly what activations do so activation functions they uh in neural network they introduce the idea of nonlinearity if we were to not use specific type of activation functions they will our model will be similar to the uh linear regression model we will have a plain linear type of model that will be able to uncover the linear patterns and will not be able to uh discover these complex hidden patterns in the data and we have seen that in the true world the data most of the time contains nonlinear relationships therefore we introduce the activation functions which help us to introduce nonlinearity into our neural network and they are like aggregation functions that help us to understand how we need to combine this different input this this different neurons corresponding Z scores in order to obtain final so single activation volume so in here if you look at this figure here you can see that we are using the X1 X2 up until xn which are our input points and then we have the corresponding weights which tell us how much each of these inputs need to be activated and then we need to aggregate these values we need to obtain one single activation which will be then based on the activation function it will it basically defines how much we need to add a value that corresponds to that specific input when Computing the hidden units volume that we saw before and there are different sorts of activation functions do mention briefly that the four popular activation functions are sigmoid activation function the hyperb hyperbolic tank function uh which shortly is referred as Tang function also definitely mentioned about rectifier linear unit activation function so the REO and also mention the Leaky relo so the Leaky of the rectifier linear unit activation function you can also if you have time you can also specify that activation functions they act like a Gatekeepers so they decide how much information needs to be used from this from this state to be passed upon to onto the next state so that's basically the idea behind activation functions which enable the neural networks to properly learn nonlinear relationships in the data the next question is name few popular activation functions and describe them so we briefly spoke about activation functions in the question number six so here I will go into much more detail about these different activation functions and we'll also make a categorization between them when you are getting asked this question you don't need to give all these different sorts of examples just one example of out of this four and then provide this detailed explanation of this specific activ ation function but in this case what I would do is that I will provide you all four four popular activation functions largely used in the industry and then it will be up to you to decide which one you will give as an example so I tend to make this categorization behind this four activation function so sigmoid activation function Tang activation function rectifier linear unit or short relu activation function and the leak key rectifier linear unit Rel the activation function so uh I tend to say that sigmoid function and Tang function they are they can form a single category and then the rectifier linear unit and its adjusted version Le youru they can form the second category of activation functions and we will soon see why so let's now start with the sigmoid activation function so if you are familiar with the logistic regression then you most likely will recognize this activation function because sigo function is also used in logistic regression when we are trying to produce values that are like probabilities so values between zero and one so as you can see in case of sigmoid activation function we are getting the this s shaped curve when it comes to the diagram that describes the activation functions and so what goes into activation function we are Computing the Z scores which are them based on the uh on the previous layers data so output it can be for instance described by X and then we are multiplying this x by the weights let's say w and then we are adding bias Factor let's say B and then we this helps us to get the Z scores then the Z scores needs to be used and put given us an input into the activation function and this is exactly then what transforms the Z scores into activation function and here it is the sigmoid activation function so sigmoid activation function then takes the input let's say Z and then what it does is that it transforms into this value where the value is 1 / to 1 + e to the power minus Z so similar to what we do in logistic regression and as you can see what this graph represents which is a sigo function in the x axis we have all the Z scores basically and in the y AIS we have the corresponding activations and you can see that for all the values from minus infinity till plus infinity for the exis so for the Z scores the corresponding Activation so on the Y AIS we have values between zero and one so the minimum is zero and the maximum is one so you can see that the inflation point so the point where we go when we flip the uh the curve is at 0.5 and this activation function uh you can see that whenever we have a negative values then the corresponding activations will be between zero and 0.5 it is all these areas and whenever we have the Z scores which are positive scores the corresponding so if if we have positive scores for our Z scores then the corresponding activations will be between 0.5 and then one what is really important to remember and mentioned during your answer as part of the interview question is that sigmoid function is suffering from what we call saturation so what is saturation so saturation happens when when we take a very large negative values or very large positive values um then the activation or the volue in this case of a function converges to the extremes of the corresponding function and this is exactly what happens here if we look here if we take let's say the x equals to minus 1,000 then the corresponding y will be most likely very close to zero and which is the minimum so it's the extreme of the of this activation function and if we take the X as a very large positive number like plus 1,000 then the corresponding activation value will be very close to one which is then the global maximum of this function so this is exactly what we refer as saturation so it means that if we take very large negative or very large positive number for our Z scores this activation function will then provide us an output a value that is very close to the extreme so two extremes the uh zero and the one of the corresponding activation function and later on we will see why this can be problematic if we use this function as part of our activations for the hidden layers but of course when it comes to getting a value an output between 0 and one which is exactly what the sigmoid function is doing then the sigmoid function would be the goto activation function so the next activation function that we will talk about is the hyperbolic tank activation function ins shorted tank function so as you can see the hyperbolic tank function is quite similar in its shape to the sigmoid activation function only what it does different is that it transforms all the values to value between minus one and 1 instead of 0 and one like the sigmoid function was doing so the tank function as the name suggests it's based on the idea that comes from uh geometric topics if you are familiar with the idea of sinus cosine is T tangent and cotangent then this will be familiar to you because this function does exactly that so it's uses this tank function to transform all the Z scores to volue that is between minus1 and one so the F set which represents the dis activation function is a function that is equal to uh in denominator we have e^ Z minus e^ minus Z / to e to^ z + h^ minus Z so what it basically does is that it transforms it takes the Z sores and if the Z scores are negative it transforms to the values between minus one and zero and then if the Z SES are positive numbers from zero to plus infinity then the corresponding activations will be between zero and one and as in case of sigmoid function tank function also is sshaped uh only the inflation point so the point where it changes its pattern the way the graph behaves is not at the 0.5 but it's at a point of zero like sigmoid function tank function also suffers from saturation so for large negative and large positive numbers the corresponding activations will be very close to the extreme point so the minus one and + one uh for this function which can be problematic like in case of sigmoid function so don't use these two functions for your hidden layers but use them for the output layer and we use them for the output layer simply because the the value that we are getting from these two functions is very convenient to to transform it to the corresponding value whether it's for instance classification case because if we get a value between 0o and one for instance as in case of sigo activation function then we can interpret this as probability and this probability can be then used for getting a class so class zero and one for for instance and this then can be used for evaluating our model by comparing it to the actual classes and compute the cross entropy or any other uh valuation metrics can be computed based on this and later on we will see why this two activation functions should not be actually used in the hidden layers especially for deep neural networks that have many hidden layers but for now as this is out of the scope of this question I don't want to go into details as we have already covered quite a lot so the next two activation functions that you can mention when answering this question uh can be the rectifier linear unit and the Ley rectifier linear unit activation functions the two are very similar especially when it comes to the positive Z scores but they are bit different when it comes to the negative Z scores so what do I mean by this when we look at the rectifier linear unit so re activation function you can see that uh it looks like this so it takes for all the cases when Z so the activation if the Z score is small than zero so it's a negative number then the corresponding activation will be zero so basically uh this activation function will not be activating any of the negative neurons whereas the for for the cases when we have the Z scores positive then the corresponding activations will be exactly equal to the Z scores so hence the name linear so if you look at the Y is equal to X line and you visualize that line you can see that the positive part so for the part where the x is is positive from zero to plus infinity you can see that that Y is equal to X line is exactly like that exactly what we have here and that's where the the name linear comes from so it's like the linear function representation but this only holds for the positive numbers so for positive numbers we will get the the Z scores and we will activate them by their exact amount whereas in case of negative Z scores or negative neural then the corresponding activation will not happen and we will not be activating these negative values and for all these cases the activation function will set the activation equal to zero so this can be uh problematic for the cases when we do want to have the output we do want to take into account these negative values and we want to uh consider these negative values and perform the predictions based on them too in those cases we need to adjust this re so we can then better use what we are calling leaky relo and what this leaky relu activation function does is that it not only activates the positive sores but it also activates the negative ones it does that at a lesser extreme than the for the cases when d z is positive of course as you can see from this from this negative side so for from this third quarter of our graph but it still does it and you can see it also from the function itself because the activation function corresponding to Liu can be represented by this F set which is equal to 0.01 if Z is smaller than zero and is equal to Z if Z is larger than equal zero so for all the positive numbers the liid ru acts exactly the same as for Ru as you can see here but for the negative values the corresponding activation is simply equal to 0.01 so you can see that it is this part of the figure so both of this activation functions do not suffer from saturation something that is definitely different compared to the sigmoid and tank function and that's why it is recommended and it has been proven that this two activation functions perform much better when we use them as part of our uh hidden layers activations and they are not so much used when it comes to our output layer so think of like using the Leaky Rel and re for your hidden layers but not to use them for your output layer and the other way around for the sigo function tank function use them for for your output layer but not for your hidden layers so the next question is what happens if you do not use any activation functions in a neural network so the answer for this question can be very short because it is quite obvious so the AB of activation functions will reduce the narrow Network to a common machine learning model like linear regression something that removes the entire idea of using neural networks in the first place therefore in order to be able to utilize the architecture of neural networks and to be able to discover hidden patterns in our data nonlinear patterns we can use the activation functions and nonlinear activation function the next question is describe how training of basic neural networks work so in this case your interviewer wants to know whether you know the idea of a forward pass backward pass what is back propagation how this processes are connected to each other and how neural networks utilize these layers in order to learn these complex patterns in your data so you can start by uh describing the training process of naral network a very basic one uh by the process of what we are calling forward path forward path takes the input data and processes through neurons which we saw before and uses this weighted sum and activation functions to produce an output so we take the input data we multiply it by the corresponding weights and then we add a bias Factor then those are our Z scores then we apply an activation function to introduce nonlinearity and this activated values the activation scores then are used when we are passing information onto the next layer because then we are using this as our input we are multiplying it by the corresponding weights we are adding the bias factors and then uh we are using activation function for that hidden layer in order to learn from this complexities this process continues on until we reach the output layer where we compute our output so this output is then used to compare it to the True Values that we have for our data so it can be for instance the true labels of our observations in a classification problem then we compute what we are calling a loss function or cost function it can be for instance the rmsc or MSC for regression type of problem it can be for instance cross entropy for class ification type of problem and then using this loss function we are then doing what we are calling back propagation back propagation is then process of computing the gradients so the first order derivative of this loss function with respect to the weights and the bias Vector so with respect to the parameters of our model that we want to improve and once we have computed this gradient which tell us how much our L function will change if we make a certain change to our our weights and to our parameters so the bias vectors then we can use this gradients in order to update our parameter in our model and the way we are doing that is by the process of backward pass so this is the opposite of what we are doing in the forward pass in the forward pass we go from left to the right till the end of the model to get the output in the backward pass we do the opposite so we basically we're going backwards we have identified the gradients by doing back propagation then we are using these gradients we are giving it as an input tune optimization technique it can be SGD GD SGD with momentum RMS probe Adam you name it and then using that approach in order to update our weights and our bias factors uh layer by layer so then what we are basically doing is that uh we are taking the gradients we are taking the optimization algorithm that we will use let's say Adam and then we are updating the weights corresponding to the last layer we have computed that then we go on to the next layer that comes before so in this way we go from right from very deep layers to the left one so from using the multiplications between the weights and in this way we are then using the gradients and we are updating the weights and the bias factors in such way that we can adjust the weights and then reduce the loss function so this is very important to mention and to show the interviewer that you understand why we are updating the weight and why we are updating the bias vectors because mathematically the gradients show how much change will be in the loss function when we change a certain weight or certain bias Factor by certain amount it can be small amount and that is something that we are using the gradients uh in order to update the previous weight parameters and bias parameters such that we can reduce the loss that our motor is making because this will mean that we are one step closer to having more accurate model the smaller amount of error we are making the smaller is our loss the better is our neural network performing so that's the idea behind training of the neural networks and this Pro continues on and on we have forward pass back propagation backward pass and on and on until we have reached some sort of uh end criteria so stopping criteria let's say the number of iterations number of epo or some other stopping criteria the next question is what is gradient descent so gradient descent is an optimization algorithm that we are using in both machine learning and in deep learning in order to minimize the loss function of our model which which means that we are iteratively improving the model parameters in order to minimize the cost function and to end up with a set of model parameters that will that will optimize our model and the model will be producing highly accurate predictions so in order to understand the gradient descent we need to understand what is the loss function what is the cost function which is another way of referring to the loss function uh we need to also understand the the flow of nural network how the training process works uh which we have seen as part of the previous questions and then we need to understand the idea of iteratively improving the model and why we are doing that so let's start from the very beginning so we have just learned that during the training process of naral network we first do the forward pass which means that we are iteratively Computing our activations so we are taking our input data we then are passing it with the corresponding weight parameters and the bias vectors through the hidden layers and we are activating those neurons by using activation functions and then we are going through those multiple hidden layers up until we end up with uh Computing the output for that specific forward path so the uh predictions why hat so once we perform this in the for our very initial iteration of training the neural network we need to have a set of model parameters that we can start uh training process in the first place so we therefore need to initialize those parameters in our model and we have specifically two type of model parameters in the naral network we have the weights and we have the bias factors as we have seen in the previous questions so then the question is well how much error are we making if we are using this specific set of ways and bias vectors cuz those are the parameters that we we can change in order to improve the accuracy of our model so then the question is well if we use this very initial version of the model parameters so the weights and the B vectors and we compute the output so the Y hat uh then we need to understand how much error is the model making based on the set of model parameters that's the loss function so loss function or the cost function which means the average error that we are making when we are using this weight and the bias factors in order to perform the predictions and uh as you know already from the machine learning we have regression type of tasks and classification type of tasks based on the problem that you are solving you can also decide what kind of loss functions you will be using in order to measure how well your model is doing and the idea behind narrow Network training process is that you want to iteratively improve this moral parameters so the weights and bias factors such that uh you will end up with the set of best and most optimal waste and the bias factors that will result in the smallest amount of error that the model is making which means that you came up with an algorithm and with neural network that is producing highly accurate predictions which is our goal our entire goal by using neural networks so loss functions if you are dealing with classification type of problems can be the cross entropy which is usually the go to Choice when it comes to the classification type of tasks but you can also use the F1 score so F beta score you can use the Precision Recall now beside this in case you have a regression type of task you can also use the mean Square the MSC you can use the rmsc you can use the MAA and those are all the ways that you can measure the performance of your model every time when you are changing your model parameters so we have also seen as part of the training of of neural network that there is one fundamental algorithm that we need to use which we called and referred as a back propagation that we use in order to understand how much is there a change in our loss function when we apply a small change in our parameters so this is what we were referring as gradients and this came from mathematics and as part of the back prop what we were doing is that we were Computing the first order partial derivative of the loss function with respect to each of our model parameters in order to understand how much we can change each of those parameters in order to decrease our loss function so then the question is how exactly gradient descent is performing the optimization so the gradient descent is using the entire training data when going through one pass and one iteration as part of the training process so for each update of the parameters so every time it wants to update the weight factors and bias vectors it is using the entire training data which means that in one go in one forward path we are using all the training observations in order to compute our predictions and then compute our loss function and then perform back propagation compute our first order derivative of the loss function with respect to each of those model parameters and then use that in order to update those parameters so the way that the GD is performing the optimization in updating the model parameters is taking the output of the back prop which is the first order partial derivative of the loss function with respect to the moral parameters and then multiplying it by the Learning rate or the step size and then subtracting this amount from the original and current model parameters in order to get the updated version of the model parameters so as you can see here this comes from the previously showcased simple example from neural network and here when we compute the predictions we take the gradients from the back propop and then we are using this DV which is the first order gradient of the loss function with respect to the weight parameter and then multiply this with the step size the EA and then we are subtracting this from V which is the current weight parameter in order to get the new updated weight parameter and the same we also do for our second parameter which is the bias Factor so one thing you can see here is that we are using this step size the learning rate which can be also considered a separate topic we can go into details behind this but for now uh think of the learning rate as a step size which decides how much of this size of the step should be when we are performing the updates because we know know exactly how much the change there will be in the L function when we make a certain change in our parameters so we know the gradient size and then it's up to us to understand how much of this entire change we need to apply so do we want to make a big jump or we want to make a smaller jumps when it comes to iteratively improving the model parameters if we take this learning rate very large it means that we will apply a bigger change which means the algorithm will make a bigger step when it comes to moving towards the global Optimum and later on we will also see that it might become problematic when we are making too big of a jumps especially if those are not accurate uh jumps so we need to therefore ensure that we optimize this learning parameter which is a hyper parameter and we can tune this in order to find the best learning rate that will be minimizing the loss function and will be optimizing our neural network and when it comes to the gradient descent the quality of this algorithm is very high it is known as a good Optimizer because it's using the entire training data when performing the gradients so performing the back propop and then taking this in order to update the model parameters and the gradients that we got based on the entire training data is the represents the true gradients so we are not estimating it we are not making an error but uh instead we are using the entire training data when calculating those gradients which means that we have a good Optimizer that will be able to make accurate steps towards finding the global Optimum therefore GD is also known as a good Optimizer and it's able to find with higher likelihood the global Optimum of the loss function so the problem of the gradient descent is that when it is using the entire training data for every time updating the model parameters it is just sometimes computationally not visible or super expensive because training a lot of observations taking the entire training data to perform Just One update in your model parameters and every time stor storing that large data into the memory performing those iterations on this large data it means that when you have a very large data you using this algorithm might take hours to optimize in some cases even days or years when it comes to using very large data or using very complex data therefore GD is known to be a good Optimizer but in some cases it just not feasible to use it because it's just not efficient the next question is what is the role of Optimizer in deep learning so when it comes to to training a narrow network uh we want to find what is the set of hyperparameters and what is the set of parameters that we can use as part of our model that will end up making the least amount of error because when our algorithm is making the smallest amount of error it means that our predictions are most likely accurate and when using it on a new unseen data it will also perform in a much better way so it means that we can rely on this algor and we can apply it across different applications without worrying that it's making too much of an error so in order to perform this optimization we are making use of different optimization algorithms creating in descent that we just discussed is one of such algorithms that is used in order to update the model parameters in order to minimize the amount of error that the model is making in this case the amount of cost or the loss that a Mot is making by minimizing the loss function so that is basically the primary goal of the optimizers in not only deep learning but also in general in machine learning and in deep learning models so the idea is that we will iteratively adjust the model parameters in this case the weight parameters and the bias parameters in our model in order to end up with the scenario and with the model where the uh where the algorithm is making the minimum amount of error or in some case say it can be the objective function is of the nature that should be maximized it means that we will need to come up with a set of parameters of the model that will maximize that objective function so depending on the type of objective objective that you have as part of your algorithm you will need to then decide how your optimization algorithm should be applied you will need to minimize or you will need to maximize your objective function so when it comes to the optimization algorithms we have the very original optimization algorithms but there are also many other variants of GD that have been developed over the years in order to combat some of the disadvantages that the GD has but at the same time also to try to replicate the benefits uh of the gradient descent so the SGD is one example of optimization algorithm beside of GD uh there is this minib badge GD we also have S GD with momentum when the momentum was introduced to improve the optimization algorithm such as HGD uh we also have adaptive learning based type of optimization techniques such as RMS prop uh Adam and adamw also adagrad and those are all different sorts of optimization algorithms that are used in deep learning in order to optimize the algorithm so another aspect that we need to keep in mind is that the goal of the optimization algorithm is to iteratively improve the model parameters such that at the end we will end up finding the global opum so during various type of explanations when explaining the variance of gradient descend and auto optimizers I will be continuously making use of this term Global Optimum so what Global Optimum means is that it is the actual minimum or actual maximum of the objective function that we are using to measure the error that the motor is making because we can have a local minimum or local maximum versus global minimum or Global maximum what this means is that when we have this area consisting of many values that the error can take in some cases when the optimization is making those movements in order to reach that minimum it might confuse and end up discovering the local minimum or local maximum Maxum instead of finding the global minimum and Global maximum what this means is that for some area when the optimization algorithm is moving in order to understand how it should improve its direction to identify that minimum in some cases it might discover that well this is the minimum that we are looking so the algorithm will converge and it will decide that this is the set of hyperparameters and parameters that we need to use in order to optimize our model but in the reality it has confused the global minimum with with the local minimum and it has falsely declared that the algorithm has converged and it has found the actual minimum but it is actually the local minimum not the global one and Global one a global minimum it means that it is actually the minimum cuz the local minimum can seem as of it is the minimum of the loss function but actually it is just a volue that it is much smaller than the surrounding values but it's actually not the smallest one in the entire set of values so ideally we want to end up with a convergence and with a state of the model where our all the weights and the bias factors we found they result in the actual minimum of the loss function and not just the value that for a certain set of Valu seems the minimum therefore the goal is to identify the global minimum and not the local one next question is what is back propagation and why it is important in deep learning so back propagation is a fundamental and essential part of training of a neural network it helps you learn from the errors that the model is making in each iteration in order to understand how the change in the certain parameters of the model such as weight factors and bias factors uh will result in a change in the loss function so the back propagation algorithm is the middle part of the entire training process and what I mean here is that we first perform our forward path we go through all this transformation when fetting the uh input data into our Network and then based on this Transformations we then end up with the final output layer and we then can perform our prediction so we compute the Y hat once the Y hat is computed we can then compute the loss function or for that one specific iteration after that forward pass once we have the loss function then we can understand how we need to change our weights and our bias factors such that we can make less errors that in the current iteration the moral is making so the errors the average errors that the moral is making we are measuring it by the loss function now then the next step as part of this algorithm it is to obtain specific amount of change in the loss function when we are making a small change in the weights parameters and in the bias parameters and this is what we are also referring as gradients the entire process of computing those gradients is called back propagation so back propagation is basically a fancy way of referring to the process of calculating the first order partial derivative of the loss function with respect to each of our modal parameters including various weight parameters and the bias parameters so this back propagation is fundamental because it helps the algorithm to understand how much a change in L function we will observe if we change each of those different weight parameters because this is the way that we need to uh Supply this information to the optimization algorithm such as G Adam in order for the optimization algorithm to be able to to update the model parameter so uh basically the output of the back propop is the input for the optimization algorithm such as GD HGD adom Etc so if we look at the specific example that we have looked into as part of the previous questions we saw this uh very simple narrow Network where we had a small amount of hidden layers and small amount of hidden units and we had just couple of input signals so as part of the forward pass we then performed all the Transformations we have activated the neurons and then we end up with this y hat which is the prediction of the response variable so once we have the Y hat then we also have cached our final uh values for our Z scores so the Z scores were equal to the X multip by the weight parameters plus the bias parameters so once we have this then we can apply a various differentiation rules specifically the change rule but also we can make use of the differentiation rule of the summations and add one applied to the constant in order to update the gradients of the L function with respect to the activation so the da and then we can use this in order to obtain the gradients of the L function with respect to the Z and then we can make use of this da and then DZ to obtain the gradients of the loss function with respect to the weight so that is DV and then finally in the similar way we can make use of Da and DZ in order to obtain the gradients of the loss function with respect to the Beats so our bias factors what is important here is to keep in mind without going too much into details of mathematical derivations that in order to update uh the model parameters we need to know the uh gradients of the loss function with respect to the weight parameter so DV as well as the gradients of the bias factors which is the first order partial derivative of the loss function with respect to the B so the bias factors but then in order to compute the DV and the DB mathematically we also need to First compute the gradients of the activations and the gradients of the the z s and the reason for that is because the the weight factors and the bias factors they are part of the Z scor so we have this chain of Transformations we applied which means that when we are performing differentiation we then also need to perform this chain of differentiations in order to be able to obtain the gradients of the weights and gradients of the Bas factors in the first place which means that first we need to unpack what is the gradients of the activations then we need to understand what is the gradients of the Z scor we need to then use this applying the chain rule in order to obtain this DV as you can see here Z is equal to W mtip with the activations a plus b to be more specific the transpose of the weight Matrix multiplied with the activations Matrix then we are adding the bias vors which forms our Zs and then to compute the gradients of the loss function with respect to the weight so DV we first need to take the partial derivative of the loss function J with respect to the Z sores and then we need to multiply with the partial derivative of the Z scores with respect to the weight Matrix in order to be able to compute this partial derivative DV and then this is simply equal to the DZ multiply by the transpose of the activations Matrix and something similar we are also performing for obtaining the gradients of the bias vectors I will not go too much into detail of the mathematical derivations I will not go too much into details of the mathematical derivations of this gradients cuz you will need to dive deep into this various partial derivatives we need to also refresh our memory when it comes to using chain rules the differentiation rules when it comes to the sum of the values when it comes to constant multiplied with the target value and those are things that I would definitely recommend you to refresh when you are going through deep learning topics but just to say in scope of the interview question I just wanted to share this just to refresh your memory that um in every time when we are trying to optimize the algorithm in every step we are performing back propagation which includes calculating all these various gradients and in order to compute the gradient with respect to the weight and the gradient with respect to the bias vectors we first also need to calculate the gradients with respect to activations and the gradients with respect to the Z scores in order to then use them to calculate the gradients of the weights and then gradients of the bias vectors and then to finalize this the back propagation is simply this process of computing the uh partial derivatives of the loss function with respect to the weights and with respect to the bias factors in order to supply them this as an input for our optimization algorithm which is actually our next question the next question is how is back propagation different from gradient descent so during your deep learning interviews you might get a question regarding the difference between back propagation and gradient descent this is usually a question a tricky one to confuse you because sometimes it's very intuitive to describe the training process and to say that we first perform our forward path we then compute our loss function to see how well the model is training and the amount of air that is making on average and then we compute the gradients to understand how much we need to how much of a change we have in the loss function when we apply the uh change in various uh parameters and then we use this in order to uh Supply it to the backward path to continuously then update the model parameters up until the earlier layers in order to end up with a model that is performing better by coming from the deeper layers to the earlier layers so this sounds very intuitive and you might be still you might be still be asked a question but how is this then different from the gradient descent what is this different specifically when it comes to B propagation and GD well the answer to this question actually is very simple one and this is just in my opinion a question to trick the interview but I would just say that the back propagation is the actual process of Computing the gradients to understand how much a change in the L function is there when we are changing the model parameters and then the output of the back propagation is simply used as an input for the gradient descent or any other optimization algorithm in order to update the model parameters so the gradient descent is simply using the output of the back propagation as an input because it's taking the computed gradients from from the back prop as an input in order to then update the model parameters so the back propagation happens in the middle once we end up with our forward pass and then the back propagation is done and then we are performing our backward pass so we are using the back prop and then we are continuously iteratively updating our model parameters from deeper layers to the earlier layers and then this is what is done by the optimization algorithm the next question is describe what Vanishing gradient problem is and it impact on narrow networks so vention gradients happens when the gradients of a network so loss functions which respect to the modal parameters such as weights and the bias parameters they become very small and in some cases they become entirely close to zero which means they start to vanish as they propagate back through very deep layers to the earlier layers and the result of this Vanishing gradients is that the network is no longer able to learn dependencies in the data effectively and the model is no longer able to update the model effectively which means that the algorithm will end up not being optimized and we will end up with a model that is unable and was not able to learn the actual dependencies in the data of course that's something that we want to avoid and we want to have a proper amount of gradients that we will use as part of the optimization algorithms like GD in order to obtain the moral parameters and then do this iteratively and continuously such that we will end up minimizing our L function and our model will be at a state that it will provide highly accurate predictions so how do we get this Vanishing gradient problem and what is the reason of that so as part of training of a neural network we saw that in each training iteration as part of the gradient descent algorithm for instance we would be using the entire training data to train our Network and then perform these different Transformations from very earlier layers to up until the last output layer to take the activations and then multiplying it with the weight Matrix and then adding the bias parameters comput the Z scores in this way and then apply the activation function in order to activate those neurons and then after this is done in that specific hidden layer the network is able to learn those different dependencies in the D data and then learn the structure and then move on on to the next layer and then to the next layer up until reaching the output layer and then when we have our output layer we then are able to calculate our prediction so y hat and then we are able to compare the Y hat to the true labels or True Values and then understand what is this loss function what is the average error that the model is making with this set of parameters and then we are performing the back propagation to comput gradients then supplying it to the optimization algorithm in order to update the model parameters and the way the optimization algorithm is doing it is uh opposite of the forward path so it is Computing and taking the gradients Computing the corresponding updates and then updating the weights and the bias factors from Deep layers to the earlier layers and the problem with this is that we are performing all these Transformations and then we are cumulatively every time time multiplying those values what this means is that in some cases especially when we have deep Network when we have many hidden layers by the time the network approaches from Deep layers the middle layers and then earlier layers this after this multiplication of many of these weights and this gradients start to become very close to zero when the gradients become very close to zero it means that we have nothing than to update our weights and update our bias parameters when we can no longer update our weights in our model it means that the network is no longer able to properly learn especially from the earlier layers when the gradient is Vanishing and this is then the problem because we want our model to continuously learn and continuously update those weights such that we will end up with the best set of parameters for the Ws and the bias factors in order to uh minimize the loss function and then provide highly accurate predictions therefore ideally we want to ensure that those gradients do not vanish so we combat the vention gradient problem and our network is able to properly learn and understand this uh dependencies in our data independent whether it's in the Deep layers or in the earlier layers so when it comes to description of this problem try to focus on the weights because that's the biggest challenge when the gradients of the weight start to vanish as especially for earlier layers in a deep neural network and that's something we want to avoid because then the morel in case of Vanishing gradients problem is no is not able to effectively learn another important thing that you can mention as part of your answer is that there are certain architectures that inherently are subject to this problem and those are especially the RNs or reur Naro networks as well as lstms grus which are all inherently deep in their nature because unlike the original neural networks like artificial neural networks that have a single input layer they provide the input and then the neurons are being activated and they provide a single output and the hidden layers they are not interconnected so they are not like the RNN that take the input and then for each input they provide the corresponding output and then this hidden layer uh and then hid unit is then being used as part of the next step and the next step so they are not sequential based like rnn's then they are less likely to to be prone to this Vanishing gradient problem than the RNs or because rnms and other sequence type of neural networks they are inherently uh deep which means that they have this multiple layers uh based on the amount of time steps that they have which then makes uh this algorithms more prone to the vening gradient problem the next question is what is the connection between various activation functions and the vening gradient problem so Vishing gradient problem is highly related to the choice of activation functions because there are certain activation functions that automatically result in vention gradient problem because of their inherit characteristics and there are also activation functions which are known to not cause the Vishing gradient problem and of course we want to be aware of such activation functions and then not use them whenever we are aware that there can be Vanishing gradient problem so here I tend to make a distinction between the sigmoid activation function and the tank activation function versus the reu so rectifier linear unit and the Leaky relu activation functions and the reason for that is because of their inherit property what we are calling saturation what this means is that in case case of sigmoid activation function for instance when the activation is FZ is equal to 1 / to 1 + e^ minus Z it means that whenever the Z or the Z course is very large negative number let's say minus 1000 then the corresponding activation the F set will then be very close to zero as you can see here which is considered as a minimum for this sigmoid function and then when the Z so the Z score in this case from this graph the X will be very large positive number so it will be around 1,000 then the corresponding function the activation will be around one so very close to one and as you can see at some point after certain level of x uh for the positive values independent how large this x will be the corresponding activation will be very close to one and at some point there will not even be a change in this area this is exactly what we are referring as saturation which means that when we have very large negative or very large positive number then there will no longer be any change in the corresponding activation we will just end up approaching the maximum or the minimum of this function in this case for very large positive ex's the Y will be around one and for very large negative numbers the corresponding activ will be around zero therefore in case of sigmoid activation function we are saying that the sigmoid activation function suffers from saturation and it is prone to causing the vanishing gradient problem so it causes the gradients to vanish especially when we have this very large values as a z score so it can be large positive or large negative and actually the same holds for the tank activation function because like like you can see here Tang activation function like sigmoid activation function is this sshaped activation function and like sigmoid function the tank also suffers from saturation and in here you can see that when we have X which is very large positive number let's say 1,000 then the corresponding activation will be around one the same will hold if we change this X and we make it 2,000 or 3,000 so so independent how much we will increase this large number for X the corresponding activation will be the same it will be close to one which is the maximum of this function and then the same holds for the very large negative numbers for this activation function so let's say the x is around 1,000 then the corresponding activation will be around minus1 but the same holds also when the X will be around 2000 or 10K so this means that in case of tank activation function as in case of sigo activation function when we have very large positive or very large negative values and we want to see how much we can activate them using this activation function then the corresponding activation value is around the extreme of this function so minus one and one in case of tank function and the zero and one in case of sigmoid activation function so therefore like in case of sigmoid activation function we are saying that tank activation function suffers from saturation and this means that tank activation function inherently causes the vanishing gradient problem because it's causing the gradients to vanish and that's something that we want to avoid so one thing that we can keep in mind is that the sigmoid activation function and Tang activation functions are great when it comes to the output layer because they have this nice properties in transforming the values between certain C ranges like in case of sigmoid function it transforms the values any values between Z and one which is great when we want to have an output in the form of probabilities great for classification type of problems but when it comes to using this activation functions for hidden layers that's something that we want to avoid so my suggestion would be to use sigmoid activation function or hyperbolic tank activation function for your output layers but then do not use it for your hidden layers because they are prone to Vanishing gradient problem the next set of activation functions that we will discuss today are the rectifier linear unit and the Leaky Rel so leaky rectifier linear unit unlike the sigmoid activation function and the Tang activation function reu or Liu they both do not suffer from saturation this means that they are then no longer causing the vening gradient problem and they are great for using it as part of the Hidden layer it you can see that the for the cases when the we take very large positive X then the corresponding activations will change and when we go from let's say 1,000 to 2,000 from 2,000 till 10K we will have the corresponding activation values also change a lot which means that the activation function will not result in the gradients coming very close to zero which is what we are describing as a Vishing gradients so as part of this REO will not be causing the vening gradient problem which is something that is great especially when we are using them as part of our hidden layers the same also holds for the Le which you can see here because the two are very similar to each other and with only difference that Liu does consider activating at some extent the negative neurons which means that when ex is negative value so for from minus infinity to 0 you can see that the corresponding activations are not zero but they are equal to 0.01 so this means that unlike the sigmoid and tank activation function those two are not suffering from saturation so they activation values will change and will not be close to the same extreme of the function uh when we make very when we provide as an input very large positive or very large negative vales which means that they are ideal for using as part of the Hidden layers such that the network can continuously effectively learn those hidden dependencies in the data therefore we are saying use reu and leaky reu as part of your hidden layers to activate those neurons and whenever you approach this output layer for the output layer use the sigmoid function or the Hang activation function the next question is there is a neuron in the hidden layer that always results results in a large error in back propagation and what could be the reason for that so there can be many reasons that can result in this specific neuron or some of the neurons to consistently result in large error as part of the back propagation in the na Network so the first reason could be poor initialization of the weights and what this means is that in the very beginning when we are starting the training process we need to initialize the weight PR and the bias parameters such that the network can uh take this initial values and then perform the forward pass compute the loss and then use this as an input for the optimization algorithm like GD SGD in order to then update those weight parameters iteratively but if we do this initialization of the we weights not in a proper way which means that we are not using the right distribution to random from it to randomly sample from from it or we are just providing a wrong or not proper value to this weight and bias parameters it means that this will right from the beginning skew off the learning process for those specific neurons and if we right from the beginning perform the learning process in an improper way and we are sking those learning processes in every iteration this means that the next iteration that will be based in the first iteration that started wrongly from the first pass it will also be done in the wrong way and then the next time and then next time this process will continue to result every time in an improper learning and in the large error as part of the calculation of the loss and then back propagation and another reason for this can be the V Vanishing or the exploding gradients so if the gradients become very small uh and the weight of the neurons especially in deep network no longer start to update those different parameters this means that uh we can persistently make those large errors in specific Neons because we are no longer able to effectively learn those dependencies in the model and uh that specific name will not be correctly activated and will not be the corresponding weights will not be updated in order to improve the accuracy of the predictions for those specific neurons and of course the same holds for the opposite problem which is the exploding gradient problem if we are making too much of an updates and right and the neurons are not being properly activated the corresponding weights are not being properly updated this means that we will be consistently making those large errors for those Neons another reason for this can be inadequate learning rate which means that if we have too high learning rate or too small learning rate we might be overshooting the optimal values during the training or undershooting so insufficiently updating those weight parameters and those bias parameters and this means that we will then continuously make large errors for those specific Neons therefore it's really important to optimize these hyper parameters in our model such that the model can make consistent updates and consistently improve the model parameters which means also uh improve the learning process another reason can be improper activation function because activation functions can significantly impact the learning process the way we activate our neurons will be a defining factor for the neuron Network to properly learn those dependencies in data and then understand how the weight parameters and the parameters need to updated such then the such that the learning will be optimized the next question is what do understand by a computation graph so computational graph is a way to visualize complex operations that we are using as part of uh training models as part of various type of optimization processes and it's a great way to Showcase all the steps that we are taking when going from very simple variables or objects till applying various Transformations and then getting to the very complex functions so one way that we are using comp ational graphs are when we are visualizing the neural networks so we saw this a very basic representation and a basic neural network in the earlier interview questions and this one is a vivid example of applying computational graph in here you can see that we are using the the not and the edges in order to describe the computational graph and the in the computational graph we usually have the nodes representing objects or variables and then the edges usually represent the type of operations or Transformations that we are applying to those objects or variables in this case you can see that we have input signals X1 X2 X3 which are simply the variables those are the nodes at the initial layer and then we are showcasing that we are applying those edges this arrows that go onto our first hidden layer in this hidden layer you can see that we have the Z scores pair hidden unit and then we have the corresponding Activation so Z1 A1 corresponding to the first hidden layers first hidden unit and then we have the Z2 A2 and a Z3 A3 Z4 A4 because we have four different hidden unit as part of our single hidden layer then we apply another transformation uh when we go from the first hidden layer to the uh to to the output layer what you can see here is that we have this arrows that showcase how we go from X1 X2 X3 so our input data uh to the Z scores and then we apply the activation on the Z scores to compute our activation scores and then we use this activation scores from the final layer in order to go onto our uh predictions which is the Y hat you can see here that we are using this final output layer to compute the Y hat and uh this is something that we are doing to visualize this process of computing first the Z scores based on the XI the input data multiplied with the transpose of the weight Matrix W plus b so this set the observation layer observation level and then the uh y I had is then simply equal to AI equal to Sigma zi and what we are doing here is that we are uh Computing the Z scores and then after Z scores we are applying this Sigma function which is the activation function that we are getting and then after the activation function is applied on the Z scores we are then Computing and getting the activation scores and then using this activation scores to compute our Yi head and then once we have our Yi head which is our prediction then we are comparing it to the True Values so the y i and then we are Computing our loss so the amount of airor that the motor is making so you can see what are those steps that we are making and when we have more hidden layers not just one but let's say three four or billions of those layers then we will end up having a very hard time understanding those different steps let alone to visualize it or to Showcase how many layers we have and each time using different IND is each time performing the same Transformations from input data to Z scor and then to activation function for after activation function we get the activation scores you get the idea so it's then much easier to Showcase all these calculations using this computational graph so basically we instead of using all these Transformations these matrices that can be hard to understand uh we are using the computational graph to simplify the process and to visualize it to Showcase how we go from the input variables um and then we apply all these Transformations and then we end up with the predictions and then this computational graph can be made bit more difficult by also showcasing the the other side of the story so getting the uh predictions and then applying back propagation and then from back propagation to the actual gradiate calculations and then the updates of the parameters but then just for Simplicity I thought I will just use this simple neural network to Showcase this but of course the idea behind computational graph is not just to Showcase a forward path but also to showcase the entire network including the back propagation so calculation of the gradient what is gradient clipping and their impact on Naro Network so we just saw the residual connections and we saw that residual connections especially for deep neural network and specifically for the cases like RNN lstms when we have the sequential nature with too many layers uh we can have this Vanishing gradient problem and residual connections can help to solve this problem now gradient clipping is solving the opposite problem so it's solving the exploring gradient problem whenever we have certain deep naral networks like RNN lstms because we are each time using these Transformations and we are using a cumulative nature and multiplying all these weights towards each other through this process and we are using the previous hidden state to update the the new hidden state so next hidden State and next one it also means that when we are going through these layers during the uh optimization process after back propop is done we need to perform all this Transformations and by the time we come through the earlier layers it can be that we have exploding gradients so the gradients become so large that it starts to impact the entire performance of the model we are starting to make two big over jumps and the updates that we are starting to get and optimization process is starting to make too much of this oscilations towards different directions so we are seeing this tic Behavior as part of our optimization process and that's definitely not something that we want to have because exploring great Radiance means that we have unstable naral network and uh this also means that the update of the weights will be too large and this will then result in not well performing and not properly trained neural network now to avoid all these problems the way we can solve this exploding gradient uh problem is by using the uh gradient clipping what gradient clipping does is that it basically Clips the gradients at a certain level so there is a threshold which is the parameter that we are using for gradient clipping and whenever the gradient becomes too large so higher than the threshold then we will be clipping that gradient to ensure that this gradient is not too large and we are not updating our weight parameters or bios sectors too much so in this way what we are doing is that we are kind of stabilizing our neural network which is especially important for architectures like lstms RNN grus uh which have this sequential nature of the data with too many layers and uh in this way we will ensure that we do not have this ertic jumps in our naral Network optimization process and we will end up with a stable Network that is able to properly learn the interdependencies the next question is what is cross entropy and why it is preferred as a cost function for multiclass classification type of problems so cross entropy which is also known as log loss it measures the performance of a classification model that has an output in the terms of probabilities which are values between zero and one so whenever you are dealing with classification type of problem let's say you want to classify whether an image is of a cat or a dog or and the house can be classified as a old house versus a new house in all those cases when you have these labels and you want the morel to provide a probability uh to each of those classes per observation such that you will have as an output of your model that the house a has 50% probability of being classified as new 50% probability of being class ified as old or the cat has 70% probability of U uh being a cat image or this image has 30% probability of of being a dog image in all those cases when you are dealing with this type of problems you can apply the cross entropy as a loss function and the cross entropy is measured as this negative of the sum of the Y log p + 1 Y and then log 1 minus P where Y is the actual label so in binary classification this can be for instance one and zero and then p is the predicted probability so in this case the P will be then the value between 0 and one and then the Y is the corresponding label so let's say a label zero when you are dealing with cat image and label one when you are dealing with a dog image and the mathematical explanation behind this formula is out of the scope of this question so I will not going into that details but if you are interested in that make sure to check out the logistic regression model this is part of my machine learning fundamentals handbook which you can check out and this one includes also logistic regression which explains step by step how we end up with this log likelihood function and how then we go from the products to summations after applying the logarithmic function so we get the log ODS and then we multiply it with the minus because this is the uh negative of the likelihood function given that we want to ideally minimize the loss function and this is the opposite of the likelihood function um and in this case what the showcases is that we will end up getting a volue that tells how well the model is performing in terms of classification so the cross entropy then will tell us whether the model is doing a good job in terms of classifying the observation to a certain class the next question is what kind of loss function we can apply when we are dealing with multiclass classification so in this case when dealing with multiclass classification we can use the multiclass crow entropy which is often referred as a softmax function so softmax loss function it's a great way to measure the performance of a model that wants to classify observation to one of the multiple classes which means that we are no longer dealing with binary classification but we are dealing with multic class classification so one example of such case is when we want to classify an image to be from Summer theme to be from Spring theme or from winter theme given that we have three different possible classes we are no longer dealing with binary classification but we are dealing with multiclass classification which means that that we also need to have a proper way to measure the performance of the model that will do this classification and soft Max is doing exactly this so instead of getting the pair observation two different values which will say what is the probability of that observation belonging to class one or class two instead we will have a larger Vector pair observation depending on the number of classes you will be having in this specific example we will end up having three different values so one vector with three different entries per observation saying what is the probability that this picture is from Winter seene what is the probability of this observation coming from Summer theme and the third one what is the OBS what is the probability that the observation comes from a spring theme in this way we will then have all the classes with the corresponding probabilities so as in case of the Cross entropy also in case of the soft Max when we are when we have a small value for the softmax it means that the model is performing a good job in terms of classifying observations to different classes and we have well separated classes and one thing to keep in mind when we are comparing cross entropy and multi class cross entropy or the softmax is that we are usually using this whenever we have more than two classes and you might recall from the uh Transformer um model introduction from the paper tension is all you need that as part of this architecture of Transformers a soft Max layer is also applied um as part of the multiclass classification so when we are uh Computing our activation course and also at the end when we want to transform our output to a values that make sense and to measure the performance of the Transformer the Transformer the next question is what is a s GD and why it is used in training naral networks so SGD is like GD an optimization algorithm that is used in deep learning in order to optimize the performance of a deep learning model and to find a set of model parameters that will minimize the loss function by iteratively improving the parameters of the model including the weight parameters and the bias parameters so the SGD the way it performs the update of model parameters is by using a randomly selected single or just few training observations so unlike the GD which was using the entire training data to update the model parameters in one iteration in case of SGD the SGD is using just single uh randomly selected training observation to perform the update so what this basically means is that instead of using the entire training data for each up dat SGD is making those updates in the model parameters per training observation and there is also an importance of this random component so the stochastic element in this algorithm hence also the name stochastic gradient decent because SGD is randomly sampling from training observations a single or just couple of training data points and then using that it performs the forward path so it computes the Z scores and then computes the activation scores after applying activation function then reaches the end of the forward path and the network computes the output so the Y hat and then computes the loss and then we perform the back prop only on those few data points um and then we are getting the gradients which are then no longer the exact gradient so in SGD given that we are using only a randomly Selected Few data point or a single data point instead of having the actual gradients we are estimating those true gradients because the true gradients are based on the entire training data and in SGD for this optimization we are using only few data points what this means is that we are getting an imperfect estimate of those gradients as part of the back propagation which means that the gradients will contain this noise and the result result of this is that we are making the optimization process much more efficient because we are making those uh uh parameter updates very quickly based on pass by using only a few data points U and training a Neal Network on just a few data points is much faster uh and easier than using an entire training data for a single update but this comes at the cost of the quality of the SGD because when we are using only a few data points to train the model and then compute gradients which are the estimate of the true gradients then this gradients will be very noisy they will be imperfect and most likely far off from the actual gradients which also means that uh we will make a less accurate update to our model parameters and this means that every time when the optimization algorithm is trying to find that Global Optimum and make those movements per each r to move one step closer towards that Optimum most of the time it will end up making wrong decisions and will pick the wrong direction given that the gradient is the source of that choice of what direction it needs to take and every time it will make those uh oscilations those movements which will be very erratic and it will end up most of the time discovering the Lo opum instead of the global opum because every time when it's using just very small part of the training data it's estimating the gradients which are noisy which means that the direction it will take will most likely be also a wrong one and when you make those wrong directions and wrong moves every time you will start to ciliate and this is exactly what HD is doing it's making those wrong decision Choice when it comes to direction of the optimization and it will end up discovering a local optim instead of the global one and therefore the HGD is also known to be a bad Optimizer it is efficient it is great in terms of convergence time in terms of the memory usage CU storing moral and that is based on a very small data and storing that small data into the memory is not computationally heavy and memory heavy but this comes at the cost of this quality of the Optimizer and in the upcoming interview questions we will learn how we can adjust this SGD algorithm in order to improve the quality of this optimization technique the next question is why does sastic gradient Des sense or the SGD oate towards local minimum so there are a few reasons why this oscilation happens but first let's discuss what oscilation is so oscilation is the movement that we have when we're trying trying to find the global Optimum so uh whenever we are trying to optimize the algorithm by using an optimization method like GD SGD RMS probe Adam we are trying to minimize the loss function and ideally we want to change iteratively our model parameters so much that we will end up with the set of parameters resulting in the minimum so Global minimum of the loss function not just local minimum but the global one and the difference between the two is that the local minimum might appear as of it's the minimum of the loss function but it holds only for a certain area when we are looking at this optimization process whereas the global Optimum is really the mean the real minimum of the loss function and that's exactly uh that we are trying to chase when we have too much oscilations which means too much movements when we are trying to find the direction towards the global Optimum then this might become problematic because we then are making too many movements every time and if we are making those movements that are opposite or they are towards the wrong direction then this will end up resulting in discovering local opum instead of global opum something that we are trying to avoid and the oscilations happen much more often in the SGD compared to GD because in case of GD we are using the entire training data uh in order to compute the gradient so the partial derivative of the loss function with respect to the parameters of the model whereas in case of SGD we learned that we are using just randomly SLE single or few training data points in order to update the gradients and to use these gradients to update the model parameters this then for SGD results in having too many of this oscilation because the the random subsets that we are using they are much smaller than training data they do not contain all the information in training data and this means that the gradients that we are calculating in each step when we are using entirely different and very small data can defer significantly one time we uh can have One Direction the other time an entirely uh different direction uh for our movement in our optimization process and uh this huge difference there's variability in the direction because of the huge difference in the gradients can result in too often of this oscilation so too many of uh bouncing around towards the area to find the right direction towards the global Optimum in this case the minimum of the loss function so that's the first reason the random subsets the second reason why in HGD we have too many of those oscilations those movements uh is the step size so step size the learning rate can Define how much we need to update the the weights and or the bias parameters and the magnitude of this updates is determined by this learning rate which then also plays a role how many of this how different this movements will be and how large uh the the jumps will be when we are looking at the oscilations so the the third reason why the SGD will suffer from too many of oscilations which is a bad thing because it will result in finding a local Optimum instead of the global Optimum too many times is the imperfect estimate so when we are Computing the gradients of the loss function with respect to the weight parameters or the bias factors then if this is done on a small sample of the training data then the gradients will be noisy whereas if we were to use the entire training data that contains all all the information about the relationships between the features and just in general in the data then the gradients will be much less noisy they will be much more accurate therefore because we are using this the gradients of based on small data as estimate of the actual gradients which is based on the entire training data this introduces a noise so imperfection when it comes to estimating this true gradient and this imperfection can result in updates that do not always Point directly towards the global opum and this will then cause this oscilations in the HGD so at higher level I would say that there are three reasons why HGD will have too many of these oscilations the first one is the random subsets the second one is the step size and the third one is definitely the imperfect estimate of the gradients the next next question is how is GD different from HGD so what is the difference between the gradient descent and the stochastic gradient descent so by now given that we have gone too much into details of HGD I will just give you a higher level summary of the differences of the two so for this question I would answer by making use of four different factors that CA a difference between the GD and HGD so the first factor is the data usage the second one is the update frequency the third one is the computational efficiency and the fourth one is the convergence pattern so let's go into one of this into each of these factors one by one so gradient descent uses the entire training data when uh training the model and Computing the gradients and using this gradients as part of back propagation process to update the model parameters however SGD unlike GD is not using the entire training data when performing the training process and updating the model parameters in one go instead what SGD does is that it uses just a randomly sample single or just two training data points when performing the training and when using the gradients based on the Su points in order to update the model parameters so that's the data usage and the amount of data that SGD is using versus the GD so the second difference is the update frequency so given that GD updates the model parameters based on the entire training data every time it makes much less of this updates compared to the HGD because HGD then very frequently every time for this single data point or just few training data points it updates the model parameters unlike the GD that has to use the entire training data for just one single set of update so this causes then SGD to make those update much more frequently uh when using just a very small data so that's about the difference in terms of update frequency then another difference is the computational efficiency so GD is less computationally efficient than HGD because GD has to use this entire training data uh make the computation so back propagation and then update the model parameters based on this entire training data which can be computationally heavy especially if you are dealing with a very large data and very complex data and unlike GD SGD is much more efficient and very fast because it's using a very small amount of data to perform the updates which means that it is it requires less amount of memory to sort data it uses small data and it will then take much less amount of time to find a global optim or at least it thinks that it finds the global opum so the convergence is much faster in case of SGD compared to GD which makes it much more efficient than the GD then the final factor that I would mention as part of this question is the converence pattern so GD is known to be smoother and of higher quality as an optimization algorithm than SGD SGD is known to be a bed Optimizer and the reason for this is because that the efficiency of HGD comes at a cost of the quality of it of finding the global Optimum so SGD makes all the all this oscilation given that it's using a very small part of the training data when estimating the true gradients and unlike SGD GD is using the entire training data so it doesn't need to estimate the gradients it's able to determine the exact gradients and this causes a lot of oscilations Ina in case of SGD and in case of GD we don't need to make all these oscilations so the amount of movements that the algorithm is making is much smaller and that's why it takes much less amount of time for HG to find the global optim but unfortunately most of the time it confuses the global optim with the local opum so SGD ends up making this many movements and it end up discovering the local opum and confuses it with the global opum which is of course not desirable because we would like to have the actual Global opum so the set of parameters that will actually minimize and find the minimum value of the loss function and SGD is the opposite because it's using the true gradients and it is most of the time able to identify the true Global optim so the next question is how can we use optimization methods like GD uh but in a more improved way so how we can improve the GD and what is the role of the momentum term so whenever you hear momentum and then GD uh try to automatically focus on the HGD with momentum because SGD with momentum is basically Al the improved version of HGD and as far as you know the difference between HGD and GD it will be much easier for you to explain what is the HGD with momentum so uh we just discussed that the HD suffers from oscilation so too many of those movements and a lot of time because we are using a small amount of training data to estimate the true gradients this will result in having entirely different gradients and too much of the different sorts of updates in the weights and of course that's something that we want to avoid because we saw and we explained that too many of those movements will end up causing the optimization algorithm to mistakenly confuse uh the global opum and local opum so it will pick the local opum think that it's a global opum but it's not the case so to solve this problem and to improve the uh SGD algorithm while taking into to account that SGD in many aspects is much more much better than the GD we came up with this SGD with momentum algorithm where HGD with momentum will take basically the benefits of the HGD and then it will also try to address the biggest disadvantage of HGD which is this too many of these oscilations and the way SGD with momentum does is that it uses this momentum and it introduces this idea of momentum so momentum is basically a way to find and put the optimization algorithm towards better Direction and reduce the amount of oscilations so the amount of all this random movements and the way that it does is that it tries to add a fraction of this previous updates that we made on the motor parameters which then we assume will be a good indication of the more accurate Direction in this specific time step so imagine that we are at time time step T and we need to make the update then uh the what momentum does is that uh it looks at all the previous updates and uses the more recent updates more heavily and says that the more recent updates most likely uh will be better representation of the direction that we need to take versus the very old updates and this updates in the optimization process these very recent ones when we take them into account then we can have a better uh a better way of and more accurate way of updating the moral parameters so let's look into mathematical representation just for a quick refreshment so what the SGD with momentum tries to do is to accelerate this conversion process and instead of having too many of the movements towards the different direction and having two different ofen gradients and updates it tries to stabilize this process and have more constant updates and in here you can see that as part of the momentum we are obtaining this momentum term which is equal to VT +1 for the update at the time step of t+1 what it does is that it takes this this gamma multiplies it by VT plus the learning rate ITA and then the gradient where you can see that this inflated triangle and then underneath the Theta and then J Theta T simply means the gradient of the loss function with respect to the parameter TAA and what what is basically doing is that it says we are Computing this momentum term for the time step of t+ one which is based on the previous updates uh through this term GMA multip by VT plus the the common term that we saw before for the SGD and for GD which is basically uh using this ITA learning rate multiplied by the first order partial derivative of the loss function with respect to the parameter TAA so we then are using this momentum term to Simply subtract it from our current parameter TAA T in order to uptain and the new version so the updated version which is TAA t + one where TAA is simply the model parameter so in this way what we are doing is that we are performing more the updates in more consistent way so we are introducing consistency into the direction by waiting the recent adjustments more heavily and it builds up the momentum hence the name moment momentum so the momentum builds up this speed towards the direction of the global opum in more consistent gradients enhancing the uh movement towards This Global Optimum so the global minimum of the loss function and this then on its turn will improve of course the quality of the optimization algorithm and we will end up discovering the global Optimum rather than local Optimum so to summarize what this SGD with momentum does is that it basically takes the HGD algorithm so it again uses a small training data when performing the model parameter updates but unlike the SGD what SGD with momentum does is that it tries to replicate the gd's quality when it comes to the finding the actual Global optim and the way it does that is by introducing this momentum term which helps also to reduce consistency in the updates and to reduce the oscilations the algorithm is making by having much more smoother path towards discovering the actual Global Optimum of the L function the next question is compare badge gradient descent to mini badge gradient descent and to stochastic gradient descent so here we have three different versions of the gradient descent algorithm the uh traditional badge gradient descent uh often referred as GT simply uh the second algorithm is the mini badge gradient descent and the third algorithm is the SGD or the stochastic gradient descent so the three algorithm are algorithms are very close to each other they do differ in terms of their efficiency and the amount of data that they are using when performing each of this model training and the model parameters update so let's go through them one by one so the bch gradient descent this is the orig GD uh this method involves the traditional approach of using the entire training data for each iteration when Computing the gradients so doing the back prop and then taking this gradients as an input for the optimization algorithm to perform a single update for these model parameters then on to the next iteration when again using the entire training data to compute the gradients and to update the model parameters so here for the badge gradient descent we are we are not estimating the true gradients but we are actually Computing the gradients because we have the entire training data now B gradient descent thanks to this quality of using the entire training data uh has a very high quality so it's very stable it's able to identify the actual Global Optimum however this comes at a cost of efficiency because the bch gradient descent uses the entire training data it needs to every time put this entire training data into the memory and it is very slow when it comes to per performing the optimization especially when dealing with large and complex data sets now next we have the Other Extreme of the bch gradient descent which is SGD so SGD unlike the GD and we saw this previously when discussing the previous intrview questions that SGD is using uh stochastically so randomly sampled single or just few training observations in order to perform the training so Computing the gradients Performing the backrop and then using optimization to update the model parameters in each iteration which means that we actually we do not compute the actual gradients but we actually are estimating the true gradients because we are using just a small part of the training data so uh this of course comes at a cost of the quality of the algorithm although it's efficient to use only small sample uh from the training data when doing the back prop the training we need you don't need to store uh the entire training data into the memory but just a very small portion of it and we perform the model updates quickly but then uh we find the socalled Optimum much quicker compared to the GD but this comes at a cost of the quality of the algorithm because then it starts to make too many of these oscilations due to this noisy gradients which then ends up confusing the global opum with the local opum and then finally we have our third optimization algorithm uh which is the mini badge gradient descent and this mini badge gradient descent is basically the Silver Lining between the badge gradient descent and the original HGD so sastic gradient descent and the way mini badge works is that it tries to strike this balance between the traditional GD and the SGD it tries to take the advantages of the SGD when it comes to uh the efficiency and combine it with the advance of GD when it comes to stability and consistency of the updates and finding the actual Global Optimum and the way that it does that is by randomly sampling the training observations into two batches where the batch is much bigger compared to SGD and it then uses this smaller portions of training data in each iteration to to do the back propop and then to update the moral parameters so think of this like the kfold cross validation when we are sampling our training data into this K different folds in this case batches and then we are using this in order to train the model and then in case of naral networks to use the mini mini badge gradient descent to update the model parameters such as weights and bias vectors so the tree have have a lot of similarities but they also have differences and in this interview question your interview is trying to test whether do you understand the benefits of one and two and what is the purpose of having mini badge grade in descent so the next question is how to decide the badge size in a deep learning considering both twoo small and twoo large sizes this question is a very important question where the interviewer is trying to test whether you understand what is the impact of the badge size uh on your entire algorithm the quality of it so here your interviewer wants to know your understanding of the best size impact on the gradient noise on the bias of your model on the variance of the model on the generalization of the model uh the convergence of the model the efficiency especially the memory usage so uh we just discussed and compared the traditional badge gradient descent to the mini badge gradient descent and to the HGD and here we have already touched on this idea of using uh very large training data using small part of the training data like mini batches versus using even smaller like a single or very few observations randomly sampled from training data and this question is exactly related to that so let's say we have this mini badge gradient descent and we want to understand what is this badge size that we need to use what are the amount of observations that we need to put in single batch when performing the uh training process performing the back propop and then updating the model parameters in each iteration so let's compare the uh small bed size versus large bed size uh with respect to all these factors that I just mentioned so uh when it comes to the small bed sizes which means that let's say we have uh two up to 32 as the bed size so we have one till 32 observations uh that have that are in our batch so in this case the uh gradient noise will be of course very high and the reason for that is because we are using a very small part of the training data when estimating the gradients of our model the true gradients and this of course means that this estimate will Pro most likely not be the proper estimate for the true gradient because we are using just a very small part of the training data and this then introduces a lot of noise into our model versus when we have a large bed size then the gradient noise will be lower because we are using a larger training part uh training data part when performing the estimation of the true gradient then we have the convergence so when it comes to the small bed sizes the convergence tends to tends to be the quality of the convergence tends to be worse because it's like the SGD it tends to make all of these oscilations and though the convergence will be Speedy so it will be quick but it will most likely discover the local optim versus the global optim while when we have large bed sites then the AL it will often converge to sharper minimum so the the actual Global Optimum and the process might take slower but the quality will be higher then we have the generalization Factor uh generalization basically refers to the idea of overfitting how how much the model is actually memorizing the training data versus being able to perform equally well on the Unseen data now whenever we are using the entire training data which a lot of times will contain noise then the model by every time using this large portion or the entire training data when training the model it will be more likely to use this noise in order to obtain gradients and then perform modal parameter updates what this basically means is that for these large bed sizes then the generalization will be potentially worse because the model will be memorizing the training data it will be it will most likely uh catch this noise as a normal points and it will be much more focused on the training data uh whereas when we have a smaller bed size then we are randomly selecting a much smaller portion of the training data and the likelihood of having a noise in that small part of the training data in the small batch is lower which means that the like lihood of the morel following this noise and overfitting and Performing very worse on the Unseen new training data a new test data uh is lower so just to summarize when it comes to generalization because it's really important when we have small bed size then we have potentially better generalizing model because we will then using a small portion of the training data and the model will be less inclined to memorize the training data and it will perform uh equally well on the new unseen data versus the large B size which will intend and will have higher likelihood of following the noise in the data and then not generalizing well on the Unseen data then we have the bias so small bad size case then the bias will be potentially lower because the model will be less likely to overfit to the training pattern whenever we have a moral overfitting then the bias will be higher if we have lower chance for overfitting then the bias will be lower this also means that generalization and the bias are very much related as we also know in the traditional machine learning and the the opposite holds for the large bad size because for the large bed size we saw that it generalizes worse uh at the lesser extent which also means that the bias will most likely be higher in case of large B size so uh when it comes to the variance then in case of small small bed size the variance is higher because we are every time using a different part of the training data and small part so there will be much more of the oscilations and the exploration when it comes to finding the global Optimum whereas in case of larger bad size then we have a variance which is much lower because we are making less explorations to find the solution of the optimization process then we have the computational cost and the memory usage for large bed sizes when we have large amount of data then we will be storing much more data into the memory it also means that the convergence will be slower and the computational cost will be higher and the smaller is the bed size the lower is computational cost and the lower is the memory usage so the next question is how does the bedge size impact the performance of a deep learning model so bed size which is a number of training samples that we use pair iteration when performing the training of the model Computing the gradients as part of back propop and then updating our model parameters per iteration uh plays a significant role in the performance and the final outcome of the deep learning model there are few things that I would advise to mention as part of your interview answer to this question and the first one is definitely the training time and here it's important to to anal ize this with respect to the training time per iteration training time per Epoch and the overall training time because the two the Tre are actually very much different when we compare the bed size and here we would like to compare the small badge versus the large badge so when it comes to this smaller badge sizes which means that we are using smaller portion of the training data per iteration to compute the gradients and to perform the updates this means that per Epoch so it means when we want to pass the entire training data which is one Epoch it means that we need to have many of these iterations because per iteration we need to have we we just use a small portion of the training data and the smaller is the bir size it means the higher will be number of iterations per EPO to perform to ensure that the entire training data has been used for updating the model parameters so this also means that the training time will be longer per Epoch compared to the larger bed sizes where we will go through the entire training data so per Epoch we will have less number of iteration so it will take much less time per Epoch to train the model but the problem is that in case of larger bed sizes it can be Memory intensive and may not always even be feasible when we have a very small machine not a powerful one in order to perform the training uh so it comes also with Hardware limitations then another Factor you can mention is the memory usage so a larger bed size requires more memory whereas the a smaller bed size requires less memory so this is another factor that comes into the overall performance of your model and then we have the convergence quality this we have spoken a lot about so I will just briefly mention uh when we have smaller bed size the smaller is the badge size the higher is usually the generalization of the model but usually the lower is the convergence quality because it's more likely to catch the local Optimum instead of global Optimum then when it comes to the learning Dynamics then the bch size will impact the motor's ability to escape the local opum if we have the uh smaller bed sizes and we are using them when to estimate the the true gradient then this this gradients will be noisy which also means that the learning will be inconsistent and it is more likely to impact the quality of the optimization process and to catch the local optim inert Global Optimum and overall if your batch size is small then usually the algorithm is much more instable rather than when we have large badge size and the algorithm is much more consistent in the update of model parameters the next question is what is Haitian and how it can be used for faster training what are it disadvantages so if you got a course in differential Theory then you most likely know what Haitian means uh here I would just give you a very high level summary of what haian is and I will go in much more detail into the impact of using haian in deep learning so Haitian comes from a differential Theory and it is basically a matrix that contains the second order partial derivative of a function which back to the parameters in case of deep learning and as part of the optimization process Haitian can play a role of getting better and more accurate estimates of the gradients because it then will uh compute the second order partial derivative of the loss function with respect to the model parameters and the reason why we are using Haan in some cases in some adaptive learning optimization algorithms which are more improved version of SGD or HGD with momentum is to have a better estimate less noisy estimat of the true gradients and you might recall that when we were discussing the HGD and HGD with momentum we were saying that our primary goal is to reduce the number of oscilations as much as possible because when we are using a small bed size or just few training observations to perform our back prop and obtain the estimate of the true gradients then usually this ends up introducing instability in our model because this gradients this estimate of the gradients are not proper estimat of the true gradients and to improve this uh when we use Haitian Haitians are not to be much more accurate estimate of the gradients and they can help the algorithm the optimization algorithm to move towards more correct direction when we are trying to update the moral parameters so so using the same amount of data but having less noisy gradients and making more correct updates and take a smoother direction towards the global Optimum now this comes with certain disadvantages because it's not that easy to compute the haian so if we look at this example this is an example of an Haan Matrix and you can see that on the diagonal we have simply the the second order derivative of the function f with respect to the parameter X1 but then when it comes to multiple parameters so when we have multiple parameters in our model we then need to obtain the partial derivative of the function in our case the loss function with respect to the model parameters and when you do it this computation manually just as an example just for fun you will discover that in some cases when you have many parameters in your model then it will be sometimes super difficult to compute this second order partial derivatives beside of that this comes also at the cost of the computational resources when it comes to using hati as part of your optimization algorithm because this results in uh having this complicated functions putting into the memory and storing this information into the memory and this haian Matrix can be extremely large for or deep neural network models and this will then result in the high number of parameters making it sometimes computationally invisible or very expensive to calculate and to then invert the uh gradients then the other disadvantage of this haian usage is that we might have a risk of overfitting because when we are using this second order derivative the second order partial derivative um as way to estimated to gradients we might might be over relying on this haian for training accelerations and this might result in the moral memorizing the training data and overly relying on the training data when performing the parameter updates which will mean that the moral will be able will less likely generalize well on nonen data which is of course a problem because then our moral is overfitting and it will be able to generalize well on an in data which is our end goal the next question is discuss the concept of an Adaptive learning rate describe adoptive learning methods so here we need to talk both about the concept of adaptive learning rate and we also need to give you examples of optimization algorithms that are known to be of adaptive nature so when it comes to the traditional GD HGD or HGD with momentum so all the optimization algorithms that we spoke about before all these algorithms have single quality uh in common that they are using exactly the same learning rate when updating all model parameters independent whether those are weight parameters whether those are uh bias factors or whether they are different weight factors so weight parameters for different hidden layers now the Adaptive learning process and adaptive learning rate uh in neural network is quite different from this constant learning rate so this learning rate which we saw before defined by ITA in deep learning can play a crucial role when it comes to defining the step size so how much we need to update the weight parameters how much we need to update the bias factors and it basically defines the uh the amount of update we need to make so what learning the Adaptive learning process does is that it says we should not use exactly the same learning rate across all these parameters instead we need to look at the training data and we need to look at the feature and based on that understand and adopt the learning rate so the step size that we need to use when updating the model parameters because it can be that we have this different features and one feature can require a different amount of updates such as learning uh learning rate the step size versus the other features that will require different sorts of updates and that's exactly what this adaptive learning optimization process does so it handles diverse data it's able to look into these complex data sets and look into this features and it then tries to update the different model parameters based on the corresponding information that it's getting so from the learning process from Computing the gradients and then from this patterns in the different features so in this way we are adopting the learning rate accordingly and each uh model parameter will have its own learning rate so this will then avoid having a stagnation and will definitely uh reduce the oscilations because it helps you prevent getting stuck in a local minimum and not being able to discover the global opum which is a common issue when we have a F learning rate now when it comes to different examples of adaptive learning rate when we have different learning rate and the learning rate is being adopted based on the information that it's getting from the algorithm and training data there are a few that you can mention and one of the most popular optimization algorithms that is also of adaptive nature is the Adam optimization algorithm So Adam that specifically stands for adaptive moment estimation uh which is then the algorithm that adopts the learning rate accordingly and therefore is also known to have much better performance when it comes to optimization so another example is the RMS prop RMS prop stands for the root mean squar propagation and this algorithm has been introduced as one of the cuser courses about deep learning and from then on many people have been using it and experimenting with it another adaptive optimization Al Alm is the Adaptive gradient algorithm which is the aagr so this also adjust the learning rate based on what we are calling cumulative squared gradients and it's particularly useful when you are dealing with spars data so this is basically at high level the summary of the Adaptive optimization process what it means to adapt the learning rate uh the impact of it on the Deep learning model performance as well as the examples of few adapted optimization algorithm largely used across the industry the next question is what is RMS prop and how does it work so we just saw that RMS prop is one of the examples that that can be defined as an Adaptive optimization process and RMS probe stands for the root mean squared propagation and it is like GD HGD and HGD with momentum an optimization algorithm that tries to minimize the loss function of your deep learning model to find the set of moral parameters that that will minimize the loss function so uh what RMS probe does is that it tries to address some of the shortcomings of the traditional gradient descent algorithm and it is especially useful when we are dealing with Vanishing gradient problem or exploring gradient problem so we saw before that a very big problem during the train meaning of deep neural network is this concept of Vanishing gradient or exploring gradient so when the gradients start to converge towards zero uh so they become very small they almost vanish uh or when the gradients are so big that they are exploding so they are becoming very large and they result in a large amount of oscilations now uh to avoid this what RMS prop is doing is that it is using an Adaptive learning rate it's adjusting the learning rate and it is using for this for this process this idea of running running average of the second order gradients so this is related to this concept of haian and it is also using this DK parameter uh which takes into account and regulates uh what is the magnitude the average of the magnitudes of the recent gradients that we need to use where when we are updating the model parameters so basically what is the amount of information that we need to take into account from the recent adjustments so in this case this means that parameters with large gradients will have their effective learning rate to be reduced so whenever we have uh large gradients for parameter we will be then reducing the gradients this means that we will then control the exploring gradient effect uh and of course the other way around holds true in case of RMS probe for the parameters that will have a small gradients we will be then controlling this and we will be increasing their learning rate to ensure that the gradient will not Vish and in this way we will be then controlling and smoothing the process so the RMS prob uses this DEC rate which you can see here too this beta which is that a number usually around 0.9 and it controls how quickly this running average forgets the all these gradients so as you can see here uh we have this running average VT which is equal to Beta * VT minus 1 + 1 beta * GT ^ 2 so this is basically our uh second order gradient so then what we are doing is that we are taking this running average and then we are using this to adapt and to adjust our learning rate so you can see here in our second expression the Theta t + 1 so the updated version of the parameter is equal to the Theta T so the current parameter minus so we're subtracting the learning rate divided to square root of this VT square root of this running average and we are adding some Epsilon which is usually a small number just to ensure that we are not dividing this ITA to0 in case our running average is equal to zero so we are ensuring that this number still exists and we do not divide a number to zero and then we are simply multiplying this to our gradient so as you can see depending on our parameter we will then have a different learning rate and we will then be updating this learning so by adapting this learning rate in case of RMS prop we are then stabilizing this optimization process we are then preventing all these random movements these oscilations and at the same time we are ensuring smoother convergence we are also ensuring that our Network especially for deep neural networks it doesn't suffer from this Vanishing radient problem and from the exploding gradient problem which can be a serious problem when we are trying to optimize our deep neural network the next question is what is atam and why is it used most of the time in nural networks So Adam is an Adaptive optimization algorithm that is known for its Dynamic learning adjustments based on the first and the second moments of the gradients So Adam like RMS prop is another adaptive optimization algorithm which adapts the learning rate accordingly in order to reduce the amount of oscilations and to improve the optimization algorithms quality what Adam tries to do is that it tries to bring this benefits of two different algorithms together it tries to combine the idea of momentum from SGD with momentum with the idea of the RMS probe by using the running average of the second order derivative so the second moment and by combining the two it tries then to take the benefits of the Two Worlds so when it comes to the Adam it can be represented by this mathematical representation you don't need to know it but I always think it helps to bring perspective to how it combines the Two Worlds the RMS probe and SGD with momentum and why it is actually so useful so when we look in here we can see that the first one is the momentum term so the Mt is equal to beta 1 * Mt minus 1 plus 1 beta 1 * GT where the beta 1 is the firste which is simply a constant uh and the beta 2 is also a constant a hyper parameter both of which we can actually tune so in here you can see GT and GT squ which are simply the first moment and the second moment of our gradients where second moment is known to be better estimate less noisy compared to the first moment so what we are trying to do is that we are on one hand using this idea of momentum in order to reduce the amount of ciliation that the optimization algorithm is making and in this way we are improving the optimization process and getting a better quality and on the other hand we are also introducing this idea of velocity which comes from the RMS prob which then will result in better less noisy estimate of the gradients so by doing this what we are trying to do is that we are adjusting the learning rate throughout the training process and we are making it more effective across different scenarios so for scenarios where GD would momentum would have been better Adam will incorporate this and where it would be beneficial to use RMS prop then Adam would again the a optimization algorithms and that's why Adam has been proven across the entire industry to be very effective and people are using it across the industry uh for wide range of problems and applications so as you can see once we have our running average for the first order derivative and the running average for our second moment gradient then we are also adjusting so we are correcting this momentum and the velocity uh by dividing this Mt so we are taking the Mt and we are dividing it to 1 minus beta 1 T because we want to adjust for the bias when it comes to this momentum and then we are also adjust the velocity so VT head is then the adjusted version of the VT because we take the VT and we divide it to 1 minus beta 2 to the^ T by using the beta 2 as our hyper parameter as our second bias vector and then we are using this adjusted running average for the first moment and the adjusted version of the running average of our second order gradient in order to update and adapt our learning rate and as you can see here what we are doing we are taking the learning rate Alpha we are multiplying it by the adapted and adjusted momentum term and then we are dividing it to the square root of this velocity term or VT hat plus some Epsilon which is usually a small number just to ensure that we are not dividing this number to zero and we are using this to subtract from the Theta T so the current parameter to get our new updated parameter Theta t + one so so when it comes to the benefits of using the Adam first of all Adam incorporates this benefits from multiple optimization algorithms to ensure that the quality is in place and the efficiency is in place and the the optimization algorithm is able to handle different scenarios uh the efficiency with spse gradients is another advantage of Adam Optimizer because Adam is particularly efficient when dealing with this sparse gradient and uh the other advantage of Adam is that it's able to balance the speed and the stability So Adam generally converges much faster when it comes to the converging to the global opum compared to algorithm such as HGD and at the same time it also tends to be more stable because it's able to control the it's able to control the learning process compared to the algorithms that only use the first moment uh whereas the the atom is is also using the uh second order gradients so the second moment and the final advantage of Adam is that it is robust so it's known for its robustness in various conditions and various scenarios uh which also means that it is less sensitive to hyperparameter choices and this is especially important when it comes to the initial learning rate and this makes it much more easy and user friendly for large range of applications especially when you are someone who is just learning deep learning then it can be a bit difficult to tune your hyperparameters it can be difficult to identify this initial values that you need to use for your algorithm and Adam is then less sensitive to this choice of learning rate which can be really important what is Adam W and why it is preferred over Adam so here we are of course referring to the atam optimization algorithm that we just discussed and the Adam W is simply the adjusted version of the traditional Adam Optimizer so like Adam Adam W is an Adaptive optimization algorithm that is used to optimize the training of a neural network and to find the minimal point of the loss function by continuously adopting the learning rate and updating the model parameters so the biggest difference in Adam W and Adam is the way they perform this regularization so the traditional Adam has one specific disadvantage which usually comes uh when dealing with deep neural networks so unlike the uh traditional SGD with momentum Adam is unable to properly generalize the moral and most of the time when we are performing regularization with Adam so let's say we are using L2 re regularization uh with Adam in order to penalize uh large weights and in order to address the overfitting problem of the algorithm then Adam has shown in the industry when training different deep neural network to not generalize very well and to solve this problem of overfitting so that's exactly what Adam W tries to do it tries to address the specific problem of Adam such that we can have improved generalization and it can address the shortcomings of the traditional Adam making it one of the best algorithms out there and it's being used more and more in across Varian applications and this is especially true when it comes to finetuning already pretrained model because those type of models are known to have this problem of overfitting and we need to have General model when the problem of overfitting will not be S so absent so present and we will be able to to pretrain the model find H the model this pre uh based on this pretrained model and then use this on an unseen data and the model will still perform well so this basically solving the problem of overfitting and ensuring that we have improved generalization that's what adamw does and the way it does that is that like you can see here unlike the traditional Adam with L2 regularization for instance what it does is that it adds this DC so this penalization term right in the update process when updating the model parameters so the traditional Adam algorithm when combined with L2 regularization where Adam is the optimization algorithm and L2 is the regularization algorithm to solve the overfitting uh what we do is that we add this Lambda multiplied by Theta T minus one where Lambda is this uh penalization parameter we use to understand how much we need to penalize certain parameters and this is to ensure that we that the model doesn't memorize the training data and it ensures that uh the model the model is more generalizable now when we uh do this sometimes this doesn't help us that much to solve the overfeeding problem and the weight Decay so decoupling this weight decay from this gradient updates and instead of using this as part of this gradient update if we incorporate this as part of the directly as part of the model parameter updates then it has proven to have much better impact in terms of generalization of the model so when we add this regularization term Lambda multiply by Theta T minus one not to the gradient but we add it directly when updating the model parameter as part of this uh entire uh expression as you can see multiplied with the learning rate then this has shown to have much better impact in terms of solving the overfitting problem and making the entire training trained model more generalizable which will then ensure that uh we have better performance in specific scenarios especially when we have uh deep naral networks or when we are fire tuning pretrained model and this also means that the algorithm uh will perform equally well on an unseen data unlike when we are using the traditional Adam which in some scenarios doesn't perform generalization very well and It suffers from overfitting the next question is what is badge normalization and why it is used in neural networks so a b normalization to understand this concept you need to know what is a normal distribution so the standard normal distribution is a probability distribution function that has a mean equal to zero and a variance equal to one and what bch normalization does is that it normalizes the activations per batch hence the name bch normalization so bch normalization is this technique that is introduced to address the issue of uh this internal covariant shift uh in the neural networks and what this refers is that uh there will be a change in the distribution of the network activations due to the changes in the parameters of a naral network so every time when we are using the back prop we are Computing the gradients then we are using the optimization algorithm to update those model parameters this then will introduce this change in the distribution of activations which can then destabilize our naral Network and it's something that we don't want to have uh for better performance of our algorithm and our narrow Network we want our batches to have certain preferred distributional properties so we want it to be uh for instance normally distributed which is known to be one of the most goto uh distribution functions in statistics in machine learning and by uh normalizing the activation so once we compute our Z course we apply our activation function and we get our activations then we can normalize this activations by Computing the mean and by Computing the variance of these activations across the batch so for all the observations perir batch and then we can use this in order to normalize the activations now you you will most likely be familiar with this concept of normalization if you have used traditional machine learning models but let's quickly look into the mathematical definitions just in case you you also need to provide this as part of your answer so when we look in here you can see that we are Computing the mean of all the activations where XI defines the activations and M is the number of observations per batch we are taking the sum of all these activations and we are dividing it to the number of observations per batch this then gives us our uh batch mean so mu batch and then we can also compute the corresponding variance of the batch by taking this activations XI and subtracting from this the mean of the batch and taking the square of this so the square difference of the activations uh and the mean of the batch and we are taking the sum of them for all the badge observations so M elements and dividing it to the number of all observations per batch in this way we can find an estimate of variance of this batch and we can then use this to normalize the activations of this batch so as you can see here we are taking the XI so activations we are subtracting the mub the mean of the batch and dividing it to the square root of the sigma B squ so the variance of the batch and we are adding some noise so the Epsilon to ensure that we we do not divide this number to zero and then once we have this activations we can then use this normalized activations to obtain our predictions so this picture is a common picture that is used to visualize this idea of bch normalization because here we have all the observations of the bch so we have n different observations according to this picture and then we need to normalize this pair feature so you can see that we are normalizing pair feature all the observations perir batch so we are using all the observations per batch to find the main for that feature and to find a variance of that feature and then we are Computing the normalized activations uh now you might be thinking but why do we even normalize it beside of stabilizing the neural network and how does that even affect the the stability of the neural network so when we perform normalization using this bch Norm uh bch Norm introduces this two trainable parameters per Activation so one one for the scale and one for the shift so the scale is then this SMU and then the shift is then the variance so once we have this the step ensures that the normalization process doesn't diminish the representational capacity of the network so in this way what we are doing is that we are stabilizing the distribution of the activations across different layers so in each layer we don't have entirely different distribution for activations and when we have similar distributions and this is just normal then we will have more stable learning process and this will then allow for Higher Learning rates uh which will then accelerate the training process of a neural network and when we are accelerating the training process we are making a lesser amount of this oscilations which we saw was a big problem as part of the traditional sgds and the Lesser we are making making oscilations the higher is the chance that we will quickly and smoothly go towards the global Optimum and we will find the global Optimum of our loss function the next benefit of using bch normalization is that it reduces the sensitivity of the algorithm to the uh initialization so when we are training our Nal Network we need to have uh initialization for our weights and for our bias sectors and this weight initialization can influence significantly the final outcome of the naral network and it might even entirely change and put the algorithm towards wrong direction so towards local minimum when we are trying to optimize the algorithm and when we perform bch normalization then the algorithm will be less sensitive to this initial values of the weights which means also that we will have higher chance to find a global op op instead of the local opum then the final indirect impact of this bch normalization is the regularization impact so bch normalization is usually done for stabilizing the neural network but the indirect impact of the circularization indirect impact of this normalization is that it has a regularization impact so what then this algorithm does is that it slightly regularizes the neural network and it reduces the need for using other regularization algorithms that would uh try to solve the overfitting problem and the way moral does this is that by normalizing the activations it ensures that some of the neurons doesn't get too high activations uh and this will then be some noise points which will end up impacting the outcome of the entire neural network because if the uh training happens on some noise data points as part of a training data and the the morel focuses on this noise points and learns this memorizes the data points which are not supposed to be memorized then the updates that the model will make will be based on this to high but noisy points when we are performing normalization and we are reducing the impact of this noise points it means also that we are also reducing the risk of the moral following this noise too heavily and impacting the entire moral update which then on its turn reduces the risk of overfitting so it has a regularization impact for naral Network the next question is what is layer normalization and why it is used in naral networks so a layer normalization uh like veg normalization tries to stabilize the network and reduce the internal covariate shift of the activations to ensure that the neural network is able to properly learn the in a stable way dependencies in the data so we just saw that batch normalization uses the entire training data of one batch when Computing the mean and the variance and Performing normalization per feature now unlike the bch normalization the way layer normalization does this normal normalization process is that it computes the mean across all features not one but all features for a single training observation and does it the same for the variant so it computes the variant across all features but single training observation and then uses this to normalize these activations so the reason for this change is that in some cases in certain neural network architectures it is pH the feasibility is not there so we cannot even calculate it just doesn't make sense to perform normalization across all training observations because of the structure of the our architecture and also the uh input data nature for instance when we have CNN based on this architecture we can then use the bch normalization easily but when we have this architecture such as RNN or lstms or grus then using bch normalization is not effective and layer normalization has proven to be much more effective in those cases so therefore for this different sorts of architectures we can then use layer normalization whenever it is not possible to use B normalization layer normalization is largely used also in the stateoftheart Transformers which are a part of the trendy large language model such as gpts which are encoder only but also the decoder only architectures uh such as bird and if you're wondering why layer Norm is particularly used in architectures like RNN or lstms or grus or Transformers it is because in those kind of architectures the bed size is either small or it can vary and whenever we have this varying or small bch sizes then bch Norm usually is not effective and unlike V Norm layer Norm in those cases is particularly effective so like in RNN or in grus or lstms where regarding of the P size which can be different uh the layer Norm will still perform effectively when it comes to stabilizing the network the next question is what are residual connections and their function in naral networks so residual connections have a very important function in naral networks and specifically they try to optimize the performance of naral networks by combating the infamous Vanishing gradient problem the residual connections they play a key role in deep learning architectures like rest net Transformers uh the um the GPT series large language models as model so if you look at the paper called attention is all you need then you'll most likely see as part of the architecture in each of these layers beside of having the actual layer you will also see this additional layer which says at and then Norm so this is exactly where the layer normalization comes and the residual connections are added so what the residual connection does is that in this unlike the typical naral network uh where the input is simply transformed through the weights and the bias vectors into the corresponding activations to produce the output in case of residual connections we are adding the the original input directly on the output of this layer and this is often referred to us as a short cat or a skip connection and the reason why it is called a short cat or a skip connection is because the gradients are simply skipping through this transformation of this computational graph of being transformed towards the multiplication with the weights adding bias factors and then uh using the activation function as transform and instead of that it has also a direct shortcut towards contributing to the final outcome so as you can see here with residual connections we have the output equal to the FX which is a transformation so it's the output of that layer plus the actual X which is the input so going back to this example of the Transformer model in the original paper attention is All You Need You Might Recall from the Transformer architecture this part which is the left part so the encoder part of the Transformer model it's also used as part of the GPT Series so the general pretrained model uh where we are so the Transformer model where we are using the input uh so the input embeddings on the top of that positional encodings uh based on the sinusoids and cinos oids and then on that after that we are transforming this into cues keys and values if you are familiar with Transformer model and then this goes us an input to the multiad attention layer so if you're are not familiar with this architecture of Transformers just forget about that don't mind the details just think of like this multiad attention layer as a layer in your neural network that tries to learn the uh inter dependences between elements of your sequence so what we are doing here is that beside of providing this input those positional encodings added on the top of the input encodings beside of giving that as an input to the multi had ATT tension layer which then gives us an output the transformed version uh we are also giving this value this input you can see with this Arrow going to the on the top of that output so we are getting the output from the multi tension layer and on the top of that we are performing layer normalization and we are adding the input on this output and this is exactly what we are doing as part of residual connections we are adding the input on the top of that output now uh the reason why it is called shortcut it is because when Computing the gradients uh the gradient will then go through this Transformations we need to compute the partial derivative of the loss function with respect to the weights and for that we first need to calculate the partial derivative of the loss function with respect to the Z scores and then we need to compute the partial derivative of the Z of the Z scores with respect to the weights and then use the two and use the chain rule to compute the partial derivative of the loss function with respect to weights now this entire transformation of this gradients will result in many cases the gradient coming very close to zero and Vanishing so this is the INF famous Vanishing gradient problem and to avoid this what we are doing with residual connection is that we have also this shortcut so you can see that we are not only performing this different transformation so we have X then we have the weight layer then we have the rectifier linear unit activation then we have weight layer and then we are getting the output which is the FX and on the top of that we are also adding this x so the initial input on the top of this output this is what we are calling residual connections and the reason why you can see that we have this idea of shortcut so the gradient is able to flow into network not only through this Transformations but also directly towards the gradient is because of this mathematical derivation here so if we look at the output Y is equal to x + FX where X is the input that we are adding on the top of the output FX you can see that if we take the partial derivative of this function uh X+ FX with respect to the X then what we need to do is we need to apply the chain so we need to take the partial derivative of the E with respect to the Y then multiply it with the partial derivative of y with respect to the X which is equal to the partial derivative of e with respect to Y so exactly the same as we have here but then multiplied with 1 + FX or F first order derivative X I have to say uh and the reason why it is is this value because as you can see here we need to take the partial derivative of y with respect to X and as y isal to x + FX when we take the partial derivative of X Plus FX with respect to x uh using the um uh the rule for summation the differentiation of sums then this is simply equal to the derivative of the X with respect to X which is one plus the derivative of FX with respect to X which is f and then this not X which basically is the first order derivative of FX with respect to X so as you can see here when we open this parenthesis this is the same as taking the partial derivative of a with respect to Y and then adding to this the partial derivative of e with respect to y * f x and this is simply opening parenthesis so one multiply with this value which gives us the first order derivative of e with respect to one y plus and then we have this second part of the within the parenthesis which is the FX multiplied with the uh partial derivative of a with respect to Y which is simply the partial derivative of e with respect to Y multiplied by f x so what we can see here and the reason why I am explaining this mathematical derivation is that you can see for yourself that the gradient in this case uh with respect to X will be able to not only flow through this part which is the multiplied by FX which has gone through all these transformations of the weight and in activation function but also directly because we have here this term partial derivative of e with respect to Y and why this is helpful and how this is reducing thetion gradient problem is because when we have this multiple layers by the time we come from this very deep layers to the very earlier layers our gradient goes through all these transformations of the weights activation functions and then it ends up becoming very close to zero so it's Vanishing and if we have this opportunity for the gradient to go through the shortcut and flip through and Skip some of the layers we will have higher chance of this gradient to not vanish and to actually reach from deeper layers to the initial layers and that's exactly what we want to do and this solves the problem of Vanishing gradient what is gradient clipping and their impact on neural network so we just saw the residual connections and we saw that residual connections especially for deep neural network and specifically for the cases like RNN lstms when we have the sequential nature with too many layers uh we can have this Vanishing gradient problem and residual connections can help to solve this problem now gradient clipping is solving the opposite problem so it's solving the exploring gradient problem whenever we have certain deep neural networks like RNN lstms because we are each time using these Transformations and we are using a cumulative nature and multiplying all these weights towards each other through this process and we are using the previous hidden state to update the the new hidden state so next hidden State and next one it also means that when we are going through these layers during the uh optimization process after backrop is done we need to perform all these Transformations and by the time we come through the earlier layers it can be that we have exploding gradients so the gradients become so large that it starts to impact the entire performance of the model we are starting to make two big over jumps and the updates that we are starting to get and optimization process is starting to make too much of this oscilations towards different directions so we are seeing this ertic Behavior as part of our optimization process and that's definitely not something that we want to have because exploring gradients means that we have unstable naral network and uh this also means that the update of the weights will be too large and this will then result in not well performing and not properly train neural network now to avoid all these problems the way we can solve this explode gradient uh problem is by using the uh gradient clipping what gradient clipping does is that it basically Clips the gradients at a certain level so there is a threshold which is the parameter that we are using for gradient clipping and whenever the gradient becomes too large so higher than the threshold then we will be clipping that gradient to ensure that this gradient is not too large and we are not updating our weight parameters or bias sectors too much so in this way what we are doing is that we are kind of stabilizing our neural network which is especially important for architectures like lstms RNN grus uh which have this sequential nature of the data with too many layers and uh in this way we will ensure that we do not have this ertic jumps in our naral Network optimization process and we will end up with a stable Network that is able to properly learned interdependencies so the key ID behind cavier initialization is to keep the variance of the activations and the gradients consistent across layers the way it is done it is by setting the initial weights based on the number of input and the output neurons in a neural network so the way it works is that it looks into the n in which is is simply the number of uh neurons uh as the inputs that go into that layer and the N output which is the number of neurons or the output as part of that layer because in each layer we have neurons that comes in and the neurons that come out and This n in that comes in and N Out that describes the number of neurons that go out is being used in order to define the distribution of this weights so Ty so one way that the cier initialization tries to stabilize the network and keep the gradients as well as the activations consistent and stable is by using uh certain statistical distributions uh with certain qualities that will ensure that the variance of the weights is constant so we want to come up with a distribution from which we will sample our initial weights using the n in which is the number of neurons that go into that layer and N Out which is the number of neurons that go out of that layer using that combined with the statistical distribution so one way of doing that is by using the uniform distribution if you're familiar with the statistical distribution function uh the the way uniform distribution works is that it always has the two parameters it has a lower bound a and the upper bound B and the distribution always lies within this specific area it's like a rectangular area uh and the corresponding values so the upper bound is the probability is 1 / B minus a so the probability for all the values that lie in within that range is constant it's equal to 1 / B minus a now why this is important in our case it is because if we use certain A and B in this case you can see when using square root of 6 / to square root of n plus n out as a lower bound and is positive version so square root of 6 / to a sare root of n in plus N Out then the variance corresponding to this random variable that follows uniform distribution with this lower bound a and lower B the upper bound B will be then equal to 2 / 2 square root of n in plus N Out so the variance of this random distribution will be this amount and this means that every time we have this weight weight this weight will then be constant so the variation of it will be constant and what does it mean if the weight has a constant variance it means that it will always change in a similar amount of with similar amount so it will not change too much with too high gradients so exploring gradients it will also not change too low so the gradients will not be too small so we will also combat The Vanishing gradient problem and and that's why cavier initialization can be so important and so significant because it can help to stabiliz the entire network it can bring consistency into the variance of these activations and the gradients across different layers and it will also reduce this risk of Vanishing and exploding gradients which will promote stability and it will also improve the efficiency of your training process and this process it is particularly relevant for the sigmoid activation function and the tank activation function let's now look into the next question so what are the different ways to solve Vanishing gradient problem so by now we have discussed many topics and many approaches where I have briefly mentioned that this helps you solve the venish gradient problem for example so here I will just summarize at higher level very quickly what are the approaches that you can mention that solve the vanishing gradient problem so the Vishing gradient problem firstly can be solved by using appropriate activation functions so by using activation functions such as rectifier linear unit function or leaky rectifier linear unit function that do not suffer from saturation uh this activation functions can help to solve the vening gradient problem and ensure that your gradients will not vanish in your in those deep networks by the time you come from this very deep layers to the early layers uh if you use the so don't use the sigmoid don't use the tank but use the Leaky reu or reu in order to combat the vention gradient problem another way that you can solve this problem is what we just discussed in the previous question which is the cavier initialization so choosing an appropriate weight initialization technique can help you to combat The Vanishing gradient problem another way to solve this problem is by performing bch normalization so bch normalization indirectly will introduce stability into the network and by normalizing the activations it will also result in your gradients which will be more consistent and if your gradients are more consistent it also means that you have lesser chance of having a problem of Vanishing gradients then another way that you can solve the gradient problem especially when it comes to sequencebased architectures like RNN lsms grus uh in all those cases you can use residual connections so residual connections will help you to open the door for the shortcut for your gradients to flip through your network without per without going through all this Transformations and this will then on its turn help you reduce the risk of Vanishing gradients then the final way of combating the vening gradient problem is by using an appropriate architecture so whenever you have you have a choice of choosing certain architectures then try to make use of the ones that are more appropriate for your data more appropriate for your problem and this will avoid at least at some extent the chance of having Vanishing gradient problem if you are for instance making use of the Transformer architecture then in here we are automatically also adding this residual connections and layer normalizations after each layer and this helps you combat The Vanishing gradients problem at some extent and if we are for instance using the Adam V this also helps to regularize the network and in some way also solve this problem the next question is what are the ways to solve exploding gradients so one way of solving EXP floating gradient like we just spoke about is the gradient clipping so gradient clipping can help to keep this threshold and ensure that every time the gradients are checked with respect to this threshold and every time when we have gradients that are about to explode so they go beyond the threshold it will then clip the gradients and it will ensure that the gradients will not go over this level and this will then solve the problem of exploding gradients another way you can solve this problem is by using the weight initialization in a proper way so as we saw before cavier initialization will help us to not only solve the problem of Vanishing gradients but also the problem of exploring gradients because by keeping the variance of the weight constant we ensure that the gradi gradients will not go not explode so not grow too much also not grow too little so too vanish so in this way weight regularization and weight initialization will also help to combat the exploding gradient problem the next question is what happens if the neural network is suffering from overfitting uh relate to weights in neural network so there are two portions of this in this question and the first part refers to the overfitting problem so you need to show that you understand how overfitting relates to neural network usually this is similar to the overfitting of machine learning models uh and the second part is to relate the overfitting to the ways in narrow Network so let's start with the first part so what is overfitting overfitting happens when the model that we are using in this case a deep uh learning model uh it starts to memorize the training data and when training the model it tries to ensure that the model performs well specifically on the training data even though training data might might contain some noise points not General points some outliers and when we train our model on this type of data and model starts to memorize this and we use this fitted data a fitted model to train on and unseen data then what we will see is that this overfitting model is not performing well on an unseen data whereas our entire goal is to train the model on a training data in such a way that this fin version the fitted version of the model will properly perform on an unseen data that's our entire goal otherwise we would not be using predictions in the first place we want to have a proper prediction with high accuracy for a future data or unseen new data and that's something that overfitting causes a problem with because it makes the model less generalizable it will not generalize well on unseen data now now how it is related to the weights of the model so large weights in a neural network will make the model very sensitive to its input data which might also include outliers or noise data and when the model follows training data too closely which also includes these unusual points then this model will not generalize well to the new data and therefore that's something that we don't want to have we want to solve the problem of overfitting to ensure sure that our model is generalizable and will also perform equally well or at least at some extent very similar to the performance of the training data when we introduce the model to the new data the next question is what is drop out and how does it work so Dropout is a regularization technique commonly used in deep learning specifically in order to solve the problem of overfitting so we just saw in the previous interview question question that overfitting can cause a lot of problems uh when training neural networks and we want to have ideally a model that generalizes well on an unseen data and one way to make our model more generalizable and improve the performance of our model is by using a regularization technique and one very popular regularization technique in naral networks is Dropout so what Dropout regularization does is that it randomly drop out or deactivates a subset or of neurons in your neural network during each training iteration and the way it does is that it takes this probability or you can also say proportion and this p is simply a value between 0 and one and this refers to the amount of or proportions of neurons that needs to be randomly deactivated so p is the dropout rate and we are saying that the rgl which is simply the activations in the layer l that this random variable follows barol distribution with one minus P probability uh where 1 minus p is simply equal to the amount of OBS amount of neons that should not be deactivated so amount of errors that should be activated and if you have p as the deactivation r then 1 minus P will be the activation rate and why baroli because if you are familiar with some very popular statistical probability distributions you will know that barol distribution helps us to uh Express and Define random variables that have two possible outcomes there can be either success or failure so there can be either dropping out or not dropping out we can either activate or not activate so deactivate neurons and that's why we are saying that our activations in that specific layer let's say layer l can be described by a random variable that follows bare noly distribution with activation rate of one minus P where p is the dropout rate so think of it like if we want to randomly deactivate or drop during the training iteration 20% of our activation of our neurons then this P will be equal to 0.2 which means that our activation rate will be equal to 0.8 so our layer will follow a baroli distribution with 0.8 as 0.8 being equal the success probability so then what we will doing is that during only the training process in each tion we will randomly deactivate and drop out some of the neurons and in this this way we will ensure that every time our model sees only 80% of the activations and how does this help to solve the overfitting is because we reduce the uh chance of our model seeing the noise points and memorizing the training data because if our model sees every time 80% of the neurons and every time it's changing from iteration to iteration this 80% cuz the process is random then we have smaller chance for our Network to every time see the same noise and to memorize training data cuz we are kind of decorrelating the training process and what this then will help to do is that it will make our neural network more as table and less dependent on certain neurons and it also means that it will generally ize better it will reduce the chance of overfitting there is however a catch um which comes that uh we are performing the Dropout only at the training data and later on in the upcoming questions we will see how we can answer those type of questions what exactly we need to do given that we are performing Dropout only on the training data the next question is how does Dropout prevent overfitting in naral Networks so we already answered majority of this question so I will just at high level summarize what I just said as part of the question 41 so the way Dropout prevents the overfitting in the network is that it randomly deactivates p% of the neurons during the training process which then reduces the dependency of the neural network on certain data points and reduces the chance of the network to memorize the training data in this way the network uh encourages the feature redundancy and also helps to generalize the model better the next question is is drop out like random Forest so in this question your interviewer wants to test to very tricky way whether do you understand the differences between Dropout and random Forest as well as their similarities now let's first start with the common goal that the two algorithms have so the both drop out and the random Forest they aim to improve the robustness of your model and they reduce the chance and the risk of overfitting by introducing Randomness and diversity in the training process you might recall the random Forest algorithm which is an emble machine learning algorithm can be used for both classification and for regression and the way algorithm works is that it tries to use the boot bootstrapped B different bootstrap samples which are copies of the original training data only with replacement and uses this B different uh training copies in order to build B different trees but it does this by decorrelating the trees so making them uncorrelated unlike the backing uh by using randomly selected M features when performing the splitting of the trees so when the trees in random Forest are built because in this recursive uh building process in each step the algorithm randomly selects M out of all P features when splitting the tree so when making this decision uh of where we need to split the tree and to what region the observation needs to be assigned this introduces Randomness to the algorithm and this Randomness helps to decorrelated trees and then the algorithm ends up having B uncorrelated different decision trees and then it combines them all therefore it's called also emble algorithm and then takes the average across all these different B trees in order to get the final prediction so in case of classification that's the majority class in case of regression that's the average of all the outputs now this helps the random Forest to combat the overfitting unlike the backing and as the um average of uncorrelated variables has a lower variance than the average of correlated variables in this way we help it helps to reduce the variance and this then on its turn helps to reduce the risk of overfitting and makes the model more generalizable so Dropout also tries to kind of decorrelate process only the way it does it is by uh deactivating certain neurons and in each iteration use a different uh portion of training data in order to train the model and on his turn this helps to avoid overfitting and avoid the network memorizing the training data so in both ways and in both approaches the modals are introducing this random component Dropout by randomly dropping so certain neurons random Forest by randomly selecting M different features when performing the recursive splits and building decision trees in order to make the modals more generalizable and reduce the risk of overfitting now the key differences so how Dropout and random Forest are different well Dropout is a technique with a single model we have just single narrow Network we are using exactly the same training data and we are randomly dropping neurons in order to in each iteration have a different uh portion of training neurons in order to make the morel more generalizable unlike the random Forest which is an emble technique so it uses B different copies from the original data that samples it with replacement and then uses this B different data sets in order to build B different trees and then combined this B different models to create single model so this means that the two algorithms work entirely different also one is more a regularization technique the other one is an actual algorithm that can be used on its own the next question is what is the impact of Dropout on a training versus testing when we were discussing the Dropout I briefly mentioned that dropout is only applied on a training data which means that we are only randomly deactivating the activations uh as part of our training process but this also means that the neurons have smaller and specifically one minus P probability of being activated during the training process so we are reducing the probability of a neuron to be selected for being activated and this will then introduce inconsistency when it comes to the testing process because we are applying the drop out only during the training this also means that we need to address this reduce in the activation probability when we are performing the testing so given that p% of the neurons during the training are dropped or deactivated we need to compensate for this and we need to ensure that we have consistency in the input size and for that what we need to do is that we need to scale the activations in the testing by 1 minus p and in this way we will then compensate for the small probability difference that we were making when we were dropping out certain activations so just as an example if the dropout rate is 20% so 0.2 this means that during the training process the neurons have 80% probability of being activated and 20% chance of not being activated it means that during the testing we need to scale the activations by using 0.8 so 1 minus P the next question is what are L2 or L1 regularizations and how do they prevent overfitting in their own network so both L1 and L2 regularizations are shrinkage or regularization techniques that are both used both in traditional machine learning and in deep learning in order to prevent the moral overfitting so trying to make the model more generalizable like the Dropout so you might know from the traditional machine learning models what does L1 and L2 regularization do L2 regularization is also referred as Rich regression and L1 regularization is also referred as l or regression so what L1 regularization does is that it adds as a normalization or as a regularization or as a penalization factor that is based on this penalization parameter Lambda multiplied by this term which is based on the absolute value of the weights so this is different from the L2 regularization which is the reach regularization and this regularization adds to our loss function the regularization ation term which is based on the Lambda so the penalization parameter multiplied by the square of the weight so you can see how the two are different one is based on what we are calling the L1 norm and the other one is based what we are calling L2 Norm hence the names L1 and L2 regularization so both of them are used with the same motivation to prevent overfitting what L1 does different from L2 is that L1 can set the weight of certain neurons exactly equal to zero so in some way also performing feature selection whereas L2 regularization it shrinks the weights towards zero but it never sets them exactly equal to zero so in this aspect L2 doesn't perform feature selection and it only performs regularization whereas L1 can be used not only for uh shrinking the weights and regularizing the network but also also for performing feature selection when you have too many features so you might be wondering but how does this help to prevent overfitting well when you uh shrink the weights towards zero and you are trying to regularize this small or large weights then this methods such as L1 or L2 regularization they will ensure that the moral doesn't overfit to the training data so you will then regular regularize the weights and this will then on its turn regularize the network because the weights will Define how much of this Behavior ertic Behavior will be prevented if you have two large weights and you reduce them and you regularize them it will ensure that you don't have exploring gradients it will also uh ensure that the network doesn't heavily rely on certain neurons and this will then ensure that your moto is not overfitting and not memorizing straining data which might also include noise and outlier points now what is the difference between L2 and L1 regularization approaches so I did briefly mention the difference of the two now let's look into more detailed version of it so L2 regularization also known as Rich regularization at squared weight values to the loss function so it's also called L2 Norm so as you can see we are taking the weights wi we are squaring them we are summing them up for that specific layer and we are multiplying it by a regularization parameter also referred as penalization parameter Lambda hence the name ll2 which basically means that this is the regularization term for the L2 regularization or the reach regularization now when it comes to the impact of this L2 regularization is that it will shrink the weight weights towards zero but it does this in a very proportional way so it tries to make the weights more equal towards each other uh so it will have a bigger impact on the large weights because it will try to make it equal to the smaller weights but it will have lesser impact on the smaller weights in this way it will try to proportionally spread over the ways towards different neurons and it will try to ensure that all this different neurons have kind of equal proportionally equal uh impact uh on the training process whereas when it comes to L1 regularization which is based on L1 Norm also referred as ler regularization uh it adds to the loss function the added so the sum of the absolute value of the weights so you can see here that it's equal to Lambda multiplied by the sum of all the absolute values of w w i we of our neural network so the way L1 regularization works is that it adds these absolute values which then shrinks the weights toward zero and in some cases it sets certain weights and specifically those small weights exactly equal to zero so in this way L1 regularization punishes the small weights so it's heavily shrinking those weights and set them equal to exactly zero and it has less of a punishment effect and is less harsh on this large weights and in this way it also performs feature selection because all the neurons so the features that have a weight of zero so they have uh their weight become zero it means that they are no longer being no longer contributing to the network let's briefly look at the difference between the two in this table to clarify so when it comes to the feature selection process L1 performs feature selection L2 doesn't because L1 said certain weights exactly equal to zero when it comes to the smoothing process L1 it has a less smoothing effect because it's very harsh on the small weights it sets them exactly equal to zero and less on the large weights L2 is bit different it's less harsh of a method and it is more harsh on the large weights and much less on the small weights it kind of proportionally spreads over the weights across all the Neons it it then ensures that process is smoother so then finally the sparsity aspect when it comes to L1 the sparcity is high because the model also performs feature selection so it sets certain weights equal to zero which means that it also automatically perform swi selection and it then reduces that the dimension of the model and makes the network sparser whereas in case of a two or re regularization the sparcity is low because the regularization process is not performing future selection and it has no zero weights the next question is how do L1 and L2 regularization impact the weights in narrow network uh when it comes to comparing the impact on large versus small weight penalizations so in here we can just summarize at high level given that we went into detail of this as part of previous questions that L2 as the square of the weights as a penalty and as a result the large weights are heavily punished so they are reduced versus the small weights which are not so heavily and harshly reduced this means that the L2 will distribute the error along and among all the weights and this will lead to smaller but nonzero weights and it will kind of proportionally spread over all the weights across the neurons now when it comes to the L1 regularization L1 significantly reduces the magnitude of the weights so large versus small weights the reduction of the of the weights when it comes to the large weights is much smaller so the L2 L1 regularization is less harsh on the large weights compared to thisal weights so given that we are using this absolute vol Val of the weights it means that it will the algorithm will be effectively remove the features from the model by setting certain weights exactly equal to zero and those are usually the weights that have already been small so it will penalize more harshly those small weights and it will be very harsh on them but not so much on the large weights and in this way we will end up removing those small weights by setting them equal to zero and keeping only those large weights the next question is what is the curse of dimensionality in machine learning or in AI so the curse of dimensionality is a nonn phenomena in machine learning especially when we are dealing with this distancebased neighborhood based models like KNN or cines and we need to compute this distance using distance measures such as aine distance cosign distance Manhattan distance and whenever we have have uh High dimensional data so we have many features in our data then the model starts to really suffer from the Cur of dimensionality so the complexity Rises when the model needs to compute these distances between the the set of pairs but given that we have so many features it becomes problematic and sometimes even invisible to obtain those distances and and obtaining and calculating these distances in some cases doesn't even make sense because they no longer reflect the actual pairwise relationship or the distance between those two pair of observations when we have so many features and that's what we are calling the curse of dimensionality so we have a curse on our ml or AI model when we have high dimension and when we want to compute this distance between pair of observ ations and this can introduce data sparsity this can introduce computational challenges can introduce a risk of overfitting for our problem the model becomes less generalizable and it will also become problem in terms of picking a distance measure that can handle this high dimension of our data how deep learning models tackle the course of dimensionality so when it comes to the Deep learning models unlike the machine learning learning models deep learning models do not suffer from this problem so machine learning models they need to for them to work properly with high dimensional data they need to perform feature selection they sometimes recommend to use different sorts of feature selection techniques such as forward stepwise selection backward stepwise selection mix selections or other type of feature selection techniques or dimensionality reduction techniques such as PCA or factor analysis in order to to reduce the dimension of our features before even providing them as an input to our machine learning model this is not necessarily for our deep learning models Because deep learning models are able to counter this course of dimensionality by learning this useful feature representations and reducing this data Dimension that we have uh applying regularization and also using architecture specifically designed for high dimension data so basically we don't need to perform feature selection when it comes to uh using this complex and high dimensional data in the Deep learning Because deep learning models are specifically designed to be able to learn complex dependencies in the features nonlinear patterns so they do not need feature selection techniques to perform that for them and instead they just use techniques regularization techniques such as L1 regularization so L of regular ation to not only perform regularization but also to perform future selection as part of the deep learning models itself so few ways that the Deep learning models can prevent this curse of dimensionality and the way that they do not suffer from this curse of dimensionality is by performing feature learning as part of the actual original Deep learning process um so it also uses this dimensionality reduction techniques for instance uses outo encoders variational outo encoders to compress the input data and represent the input data with this compressed form like the dimensionality reduction technique but now as part of the actual architecture of deep learning model and then learn the hidden factors from this already compressed data with lower Dimension and then reconstruct from this to get the final output now another way that it does is by using regularization methods and I briefly spoke about this uh just before by for instance using L1 regularization because L1 regularization reduces the sparcity uh and introduces sparcity by reducing the dimension of the of the features by setting some of the weights equal to zero it indirectly performs feature selection and finally if we use model such as CNN and we use pulling layers so pulling layers in convolution neural networks for instance they automatically perform feature selection and they reduce the dimensionality of the problem and in this way they automatically combat the curse of dimensionality the next question is what are generative models give examples so let's first start with the generative models so generative models aim to model how data is generated they want to learn the uh joint probability distribution what we are often referring as P X and Y which is the joint probability distribution of two random variables X and Y where X represent the features of our data and why represent the labels so what generative models do is that they try to model the underlying data distribution so they want to figure out how data was generated in the first place and in this way they can then generate new data instances and they can also predict probabilities and beside this generative models are also great when it comes to unsupervised learning tasks so from machine learning you are most likely familiar with this idea of supervised versus unsupervised when in case of supervised we do have the label so the Y values but in case of the unsupervised case we don't have labels to supervisor learning process which means that we need to perform the learning only using the features think about clustering think about outlier detection or dimensionality reduction or some data Generation all these cases can be considered as unsupervised learning tasks and generative models are great in doing so now when it comes to the generative models they are particularly useful in scenarios when we want to understand and we need to understand the underlying data distribution or when we need to generate new data points so let's say you want to generate synthetic data or you have lot of images and you want to generate new images images that do make sense and they are similar to the input data but they are new they are noic they are fresh and this can be done by using generative models and all the models that these days you hear about for instance the GPT series uh the variational auto encoders the gens so generative adval series uh networks all these models they are generative models and they are largely used for generating new images generating synthetic data um for generating new speech for instance when there is a data that is there is a model that is trained on the large amount of of the speech you provide for examples of your speech and then the model is able to generate new voice so recording of based on the text based on your own voice but you haven't yet spoken that that's generative model that is doing that