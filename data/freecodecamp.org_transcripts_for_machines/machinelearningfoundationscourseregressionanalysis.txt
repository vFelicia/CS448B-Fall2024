welcome to this machine learning course for beginners taught by hayu Singh he is a data scientist and popular instructor this course is your gateway to mastering the intricacies of regression analysis in the realm of machine learning IUS has designed this course to provide you with indepth knowledge practical examples and HandsOn projects that will set you up for success with the curriculum that covers everything from the basics to Advanced Techniques this course offers a unique opportunity to not only learn the fundamentals of machine learning but also explore the often overlooked power of regression analysis so get ready for a deep dive into the world of machine learning what if I told you that there's a core machine learning course that does not have only detained explanation of regression analysis but also have bday lecture notes as assignments with personalized feedback on that and a scheduled learning plan for you well guess what I'm offering all of this but absolutely free hi I'm ayush and I'm currently working as a data scientist at Epic and has demonstrated working experience as an envelopes engineer at xenomen and a data scientist at artifact and a founder of Anton and I welcome you to our core comprehensive machine learning course out here the course is designed to give you a solid foundation of machine learning and tradition analysis which is often overlooked in most of the courses available right now we beg it by linking the downward for machine learning so even if you're beginner you are able to understand it you talk about what is learning we talk about generalization we talk about inductive bias and a lot of things initially and then we gradually move on regression analysis and recover as much as we go in depth and to ensure that we are quite equipped with the Practical applications as well I'm going to introduce two projects by the end of the course which you have never seen like that project before as well because I have worked personally at several companies and I know how we deliver data science process to them and the same way I'm gonna do it in this course as well but that's not all if you want to maximize your learning opportunity from this course then we are offering free updates to the course regular assignments with personalized feedback to you and a tailored learning plan for you to complete this course in just one month you can simply go on a GitHub page for the course and then get access to the lbs directly from there and all of this were absolutely free one of the major issues which are on a highlight you might be asking here use just for a question most of the instructor takes around only one audio of taking freaking so much of our super teacher I tell you that people forget the foundation people forget the basics of it and they'll just just strictly goes on Advanced topics like deep learning and then they are not able to secure the job and then they complain me that hey I used data science has no jobs plus actually if you think in a way that the talent is less than the actual drops because they are not going to pay you thousands of dollars for just three lines of code which you ride inside but anyone can do it and then crowd is increasing but the talent is decreasing they're forgetting what's the base of machine learning what's the base of data science before getting the solid foundation which was used to be given in 90s and all so that's what the course focuses on we make sure that we emphasize the core concept of machine learning and regression analysis so that you can even learn further things on your own our course is designed to put emphasis on core concept of machine learning and traditional analysis so that you can tackle critical business problem and solve it the way real data scientists solve so that's enough of it now let's get started with the course you can directly go to the LMS and then start from there by seeing the tailored learning plan or you can get started from here let me know if you have any notes in the comment section and find any errors or irata in this course please feel free to make an issue on the GitHub page for the course hey everyone uh so let's get started with our machine learning content and actually we'll get started with um a bit of discussion on learning the machines learning intelligence and give you some of the concepts which is required eventually to build up your ideas right so let's get started so I would suggest you to open up your notes if you have got it so once you have the notes that you can see on the front of the screen so we're going to start off with what is learning right so let's eventually talk about what is learning and slowly we'll get into the get into the concepts of you know a very very good examples of learning and intelligence Etc so the first one is that what is learning so I'll suggest you to write your answer in the comment box if you know so according to what exactly uh what exactly you know about in terms of machine learning we have the training data we take some sort of you know um the data don't worry what data looks like and everything will cover that so we have the data right and then we feed to a sort of a black box and then we get a trained algo that is trained on that data for example assume that you have a man right you trained that man with the with that particular skill and then whatever output after six or seven months you will be getting a trained man specialized in that skill right so this is how your model model gets trained your model gets the data gets the training from the data and then get it it gets trained by some instructor right or assume that in this case it's an algorithm and then after after after some time we get a trained algorithm right that in terms of a very simple or very you know mind understanding standing behind any algorithm but let's talk about a very nice example very nice example which is called which is known as rats learning to avoid poisonous dates so what is baits baits are some sort of you know food or Etc so the formal definition of beets is that it is it is some sort of you know um it is a type of prey that is used to attract when hunting so um sometimes you we use that baits to attract you know poisonous thing it it is a poisonous thing that you use to attract uh food right uh sorry a mouse is or any aerial things whatever for example fish we also have the beets right so in this way we have the baits so basically the problem statement the problem statement is that the rats are learning to avoid the poisonous baits so how do lats rats learn to avoid that poisonous Spades right so rats assume that rat came and then uh assume that rat game a rat game and that and then they given a no they there was a food available where this was a bait this was a bait right uh so it got they gone and then they um it was like smelling good and Etc right eight rat ate this food rat eat this and there's an ill effect on that game right there's an ill effect of that game there's a effect of rat game when they eat this food and and then rats will not eat this food because this is this is the learning mechanism which happened right now where where according to the past experience according to the history or the mistakes in the in in the past this rat did and he's not on rats will no more gone to repeat this step again this is one sort of a learning mechanism another learning mechanism which you can think of is assume that you you have done some sort of mistake in your past or a very simple example assume that uh you go for someone for guidance right why why they are capable of giving items because they have gone through the problems they have already gone through the problems so you go to them so that you get an easy path so that you get an easy uh guidance from them you you so to achieve something in your career right so in that way I'm saying that they already have an experience of something now they're capable of you to guide right just like I do I guide so many people right so we I got so so many of books right now so I am a capable because I've already gone through those problems now I'm capable because of my past experiences so whole point to take out right now I have learned what mistakes I what mistakes and what things I did that leaded me to success and failure right that I've well to learned that so now that's a learning mechanism for example you learn you know you you study some maths concept right you study quadratic equations or maybe some calculus concept so when you study that when you study that um what eventually happens is that uh you you take a look at that particular particular concept and once you take a look at the particular concept now you understand with the helpful examples and then you do a problem and if you did any sort of mistakes you you change it according to you know um Lex ask ask for help or see the solution but to when you s but but when you saw similar questions you will be able you you will be able to answer that why because you have learned from the past mistakes which you did right because you are not going to repeat the mistakes which you did in the past even while solving problems sometimes it happens as well sometimes it does not right so you you look that's on the learning mechanism that's why we say the failure is a part of success because it gives you the learning phases the learning of opportunities to actually go to grow forward right so there are several types of approaches which we go through the first approach is learning by memorization approach so this is something that's very interesting uh where we we say in Indian language it's ratification and in ratification what what we do we memorize things whatever we we we we want to study so for example I've seen students to literally memorize you know signs answers SST answers even computer science answers because they literally memorize code and they just go and write in the sheet of paper in uh and then they they memorize literally like even mathematical questions however it's so obvious that daily not will perform well but but but over here assume that if you memorization every concept each and every line of your answers will you be able to perform good in examination so even if one line you forget you'll mess up with all the answers you'll start integrating first answer to second answer second answer to first right so you have to apply so over here memorization literally fails in your daily life you can not literally memorize everything and then go and set an examination and you expect your that you'll score good it it works till class six or seven but it does doesn't work when you actually grow right so you can actually can't literally memorize each and everything whether it be answers whether it be mathematical Solutions or literally anything so I hope that that giving a good sense about what exactly I want to talk on is um learning by memorization approach where we memorize we learn some things by memorizing uh that sort of thing right so now so now what so now what what exactly I'm going to do is talk about some of the uh examples which we have listed out here for you to better understand things cool so assume that you have you you have a you want to build a system so you might have seen in your email you know uh Gmail where you where you have several photos promotions folders social folder Spam folders and it's an updates foldering Etc so you have these folders and this one folder called spam where all the spam goes in right where all the spam goes in so what are what are those spams right that's question that's a question so now now we're going to build a system you want to build a system you want to build a system that given the past data right first of all we if if you're learning something to identify a spam you need to have the data you need to have the you need to have some some sort of you know content to understand tantrum right so in this case if you want to build a system you'll build a system that given an email given an email it classifies that email is a spam or not spam okay that given an email it classifies whether that email is a spam or not spam right so now that's a question right so you you so now what you so now what you will do is that the first step is for anything for learning literally anything you need content for this so we need the data for it right so we are we are going to collect the collect the collect collect collect the data um we want to collect the data which is where where we have the spam emails right so we will collect the data which is spam emails out here so we can we collected the data which is spam emails out here now we have the content now we are now we want our machine to learn from this content so there will be two types of guys first who will memorize this so the four we'll talk with the first guy the first level will memorize this will memorize each and every email each and every email so machine will memorize each and every spam email right and now now you're now that machine is capable now with that machine is even if if that is 100 correct even if that memorized a very hundred percent correctly right so machine will be able to class will will be will will be able to memorize each and everything now if our an email comes em email comes through this machine and now this a machine is stored to classify this email to be as a spam or to be as a not spam right whether that email is a spam or not spam so you you go to new email and you classify it so how machine will classify machine will search this email into its memory whether this email is available into end to its memory or not right whether the words of this email is available to in in my memory or not right it if not then we say that it is nonspam but because the data is but because the the content which we learned is about spam right so if the if this is not available in that content in his memory what he what the model has memorized from the data then he then the model will be able to see that it is nonspam but if that email is available then it will be spam so what's the problem with this approach what's the problem with this approach right that's a good question what exactly the problem with this approach that's a good question to actually go forward to the problem with this approach the problem with this approach it is not generalizable it so why it is not generalizable so let's talk about a very instance example assume that you have got a new email you you've got a completely new email which where your model has not uh you know memorized for example you sat in your mathematics uh let's say Science examination you got your questions which was in a textbook which you memorize but you also got a tweaked question but you also got a very different thread set of questions so where you have to apply logic and you know apply several skills in that so he there you will fail right because you memorize only the consequences which came but you're not able to answer the questions which are new which are unseen by you right so in this case as well model will fail to gen to to to predict for examples or predict for emails which are unseen to buy the model which are not seen by the model right so that's that's the problem with uh with the approach right now right so that's one of the approach the problem is that the machine which we have does not have the ability to email any unseen emails to to predict for any unseen email so for example if any any email comes and let's assume that it is a spam but this is not available in this memory content so that will be flat as nonspam but actually it was a Spam right so this is one of one of the problem so we need what we need to do we need to figure out a solution to further generalize it further make into to to generalize means to have to give to give this machine an ability to give this machine an ability to further classify even unseen emails by logical thinking so I'm going to make this guy so the second number of guy wear it also understands analyzes the data and applies logical thinking in further to predict emails so that's where to generalize and this is called something known as inductive inference okay this is not nothing but called inductive inference so we will talk we'll talk let's uh let's talk about inductive inference uh in the in detail in the next lecture so now we'll talk about something known as a generalization which we had had a talk and then we'll talk about who and what's the issue in this as well right so we'll talk about that so the we we call that generalizing our model is known as inductive inference right please read the if you want to know more examples and all I have linked the very nice book the chapters and every chapters is also included in the in in the LMS so please go over there you won't be able to see everything uh um each and everything with the resources and everything uh and the reading materials regarding if if you want more examples but that's it you will stay here if you have understood it so generalization generalization is it's an ability to um to add the feature in a machine to actually improve right to to actually uh work on even unseen examples but to apply logical thinking so now let's take an example over here is that the the problem the problem which we had the problem the very basic problem which we had so I'm just going to take a very very nice example right now so now now over here over here this is right so in the end in this example This Thread if we have the generalization available if we have the generalization available if we have the generalization uh available where your rat will be able to decide whether to eat this food or not be whether to eat this food or not based on its smell right so rat will be able to decide to to eat this food or not with the help of its milk right so generalization how does this help how does this generalization have this had this right it helps it to decide and answer examples which is similar to the bad food so what is this saying that rat may have seen the similar similar food right so that may have seen the similar food so generalization is that that even that's even a similar something comes in that will be able to refuse okay this is a bad Pro this is a good food right so that should generalization gives an ability to even rat to decide on this particular food which has net never written before based on the based on the similar things which everybody eat which he has written in the past another example which which we have seen that the that we have a machine which scans the emails we'll scans the emails it extracts the words that appears to be spammy Words which is spammy words and then it says spam or Not by seeing the spammy word so basically that email comes and then see what proportion of the spammy votes is in that email and then classify it based on that but there's an issue so now by this if you have the if you have the if we have the words that is that is spammy and then we check the proportion in that or we give the machine the ability to even classify uh based on The Logical understanding of that and or a generalized understanding of okay this is this this is how the spam email looks like right so your model will is able to generalize accordingly according to the things right that's one of them but here your system might be able to perform their event right so in this case you might be thinking that you will be your system will be able to perform OMG right but actually it's not true um you have to give the ability to the model that to identify your nonsense examples and then you think okay it's over right it's not over so inductive inference might lead us to false predictions misleading predictions let us see how so this uh prison super session is an example business supercision is an example um I will which I have read in a machine learning theory from algorithms's Theory and also this is a very famous example on learning conducted by some sort of you know University which I don't remember they have written it very nicely is about how this inductive inference might lead us to bad incidents how this inductive inference might lead us to bad inference so let's talk about the very nice stuff as of now is this this particular thing pigeon superstition so you will be seeing a way you'll you'll be seeing a very nice uh I'll be linking a very nice video which you which I suggest you to go over there and watch that video first right so go over there watch that video first or I'll be giving a short um short experience on what exactly is going on so also you can actually pause the video right now go over there and understand what exactly is going in that video but I'll be explaining you in simple English terms so you can actually go over there and understand it by your own terms but I'll be explaining you in the very same simple English terms so you have um what what thought what the experiment was experiment was the you have you have set up you know you have set of pigeons you have set up pigeons which was contained in a cage so you have you you've taken a set of pigeons and what they did they have they've kept it in a cage right and that cage had a small hole has had a small hole where the food was coming and going at a certain interval of a Time assume that every three seconds four seconds five seconds ten seconds 12 seconds 14 seconds and all these pigeons are super angry okay so so we have a cage and that we have a pigeons which is super hungry and then we there's a hole where the food is coming at a certain interval of a Time okay at a certain interval of a time now now when smooth is delivered now they are angry right so sorry there the patients are hungry right so if the patients are hungry they'll keep on doing some things they'll try to Peak no they'll they'll try to peek and try to find you know some sort of um Foods you know so that they try to Peak or they try to you know wings as like spread their feathers and then try to uh go from here to the other Peak right so in this case as an experiment I'm taking example if the pigeons if one of the pages F2 two three are peaking right so they're they're searching for them they're they're active okay they're active and they're peeking like this which is my like searching for food uh they're peeking now so when food is delivered when food is delivered at every interval of a time they are engaged in some sort of activity that activity can be peaking or that can be Feathering up right anything literally anything so if you see the video we'll be seeing that some birds are peeking some birds are um this is only one body which the they're engaged in some activities now now now when the food was delivered they were engaged in some activity right so when food is delivered they go on and eat in that right and then again start doing the same action again started doing the same action in a hope that food will deliver it once again right so if they would like pigeon is doing some peaking and the food is delivered by doing that action so you're gonna need and again started peaking so he thought that if I pick I'll be getting um food to me right so so basically pigeons started doing the same action in a hope of food arrival but I'll come to that part later on so the pigeon Association of the delivery of the food with whatever actions they do peeking Feathering flying literally anything actions it with whatever transactions they had been performing when when it was first delivered so if the first time it were delivered so there were beacons so they continue to do peaking in order for the food arrival because they hope for it because they hope for it so what's the problem in this right what's the problem in this right so let's talk about what exactly the um uh problem which which comes so basically pigeon build an association pigeon built an association with pigeon pattern Association of the delivery of the food the delivery of the food with whatever chance or actions they had been performing right so they built the association between delivery of food with the actions like peaking or flying they did so they they tend to do peaking in Hope of food arrival right so very interesting so your your vision learned that this that they learned this right they learned it but what exactly but but but now now this now let's try to figure out what exactly the problem in this problem is that human Learners the we can rely on Common Sense filter right to fit to filter out meaningless sense for example for example assume that uh every time I throw this pen over here every every time I throw throw this pen over here right every time I throw throw this pen over here my food will be coming right like it is at a certain a certain in the end of a time in the morning and afternoon in the night it does not matter how much I do this I'll be getting that interval of time in this case as well it was delivered on the interval of a second it was not dependent on what actions patients are doing it was just lost intervals three seconds four seconds three seconds six seconds eight nine seconds 12 seconds um I mean 12 seconds and then it is going on the at a certain end Tower over time it is not associated with what actions patients are doing doesn't really matter this is so now we we can even we can rely on it is a meaningless right whatever pressure has understood it is a meaningless right this is a meaningless and this is something which is not so human Learners can filter out meaningless things we can filter out that's a coincidence or that's something that that's something super meaningless and meaningless learning conclusions I by doing this if I get money like it's one time assume that you you are throwing this pen out here right you're throwing this pen out here and you you any and you got money on your bank now you are now you cannot keep on doing this to get money till the end of Hope of money arrival money will come at a certain amount of time right um so this is something which this is meaningless conclusion but there are some humans who cannot apply the common sense filters to not take out the but never mind so and now now we now uh now over here region also learned a very bad meaningless conclusion where is where it he thought that peaking will get me a food right that's a bad so we also have to give this Builder to our machines right so in this generalization approach the problem was that your model may end up learning a Minix meaningless learning conclusions so we must give the well defined crisp principles again I'm reading we must give well defined crisp principles so that will protect our program that from reaching in senseless like What patients reached that will protect our machines to reach that to to to to reach that conclusion this excerpt is taken from understanding from ml from Theory to algorithm this is very nice that's why I've taken it from there what exactly it tells it tells that your that we need to beautiful gives filters to our model so that our model is able to identify the meaningless conclusions like this right meaningless conclusions so we have to give the well defined meaningless sorry welldefined Tris principles crisp principles to a model and those principles are only called and those well well defined crisp principles are called what that are called nothing but ah development it's it's that's what we learned that's what the theory of ml comes in theory of ml that's what the theory of ml where we give the welldefined principles to our model I hope that makes sense to you uh that give it a very nice understanding about whatever we want to have a talk on now there is a very interesting concept which is known as inductive bias inductive bias so let's talk about that and then we can end this video and then we can go to the next step which is exactly learning about what is machine learning and then now we know about learning and then we can go over a bit off you know learnings so look cool so assume that uh so sometimes um uh you you might be noticing that a new cat that a new cat that that a new sorry new rat whatever I'm saying a new rat might refrain from eating a food because they think that that this food is is is poisonous why because of the prior knowledge inherited from the previous generations so they have some sort of genes that tells us don't need this food this is poisonous because of the past Generations because of the habitual understanding if you've studied the biology then you have studied that there are there Evolution or the or the evolution is necessary for human beings if there is no Evolution if there is no you know chain of you know um for a whole food chain so assume that you have a you you have a lot lots of humans and they died from the disease but but there might be some some newborn newborn humans where or babies which later they would develop in humans where they are capable of fighting with those diseases they are capable of that right they have inherited they have built a very nice combination so fighting with the diseases we so in this case this rat has inherited from the previous generations previous generations of their ads to further to to further say that okay I'm not going to eat this food I'm not going to eat this food because it gives some sort of sense so you might be thinking why rat so this so why rat why rat was able to successfully classify to not to do not eat that food whether pigeons was not able to they just was reaching to a meaningless conclusion because drats were having the prior knowledge and prior knowledge is super important where the rats were having the prior knowledge so pigeons learning was not more successful as compared to what rats said because of the prior knowledge and prior knowledge is also called inductive bias prior knowledge is also called inductive bias I hope that makes sense um now I hope that that every very nicely introduction to learning in the next lecture what I eventually going to try to achieve is talk about a very nice introduction to machine learning with applications and will also do several examples as well that's something um isn't necessary for you right so let's go to the next lecture and then try to understand machine learning with a very nice definition with the very nice set of examples so yeah so now what I'm going to do is actually talk about uh start talking about you know machine learning stuff and that's something which is super important for you to actually go forward with it right let's talk about that so now what we're going to do we're going to actually talk about what is machine learning I'm going to talk about some of the applications of it and then we'll see some of the formal definition which comes in machine learning now you know what exactly learning is right now you know exactly what learning is and then you'll start referring that we have solved some of the problem now to div now just just to conduct the dots to connect the dots the dots how we're going to connect it the dot is first is there first where are the where we learned about learning mechanism where our rats were learning two are two two to further classify where the whether to eat this food food or not right so we've gone through memorization approach where um by taking a simple spammer ham examples and for him memorization we thought that generalization that we have to generalize it so that is inductive inference so we've got another another issue which we see in inductive inference which is the misleading or reaching to a senseless conclusions and for defining that senseless curve and for for refraining or protecting our programs for senseless conclusions or the principal conclusions we have something known as machine learning wherein this machine learning we Define a well defined welldefined crisp preface principles so that our model or model does not reach us to uh senseless conclusions cool um so let's get started so I suggest you to maybe have a coffee with you because that's something is super important to refrain from it so correctly I'm recording at 3am because I have boats as well and I have to prepare uh so uh I know how I'm managing this because I have a class 10 both class 10 examination in 20 24 days I guess and have to prepare a lot and uh I'm doing my day and night on that but eventually I have to also do for community and I'm doing it because I like it not something who's forcing me to do this but coming back to what exactly I want to do it is what tells machine learning question so whatever you know about machine learning write that in a comment box edit your comment or whatever you can do and just just write that in the comment box what is machine learning right so what is machine learning it's an artificial intelligence domain it an artificial intelligence domain where we extract patterns from the data and analyzes the data and make intelligent predictions on the new data according to the pattern your machine has learned a very nice and influential statements so let's try to talk about uh the very highly highlighted statement it's an artificial intelligence domain where we extract patterns from the data analyze the data make intelligent predictions on the new data this new is very nice keyword according to the pattern your machine has learned so these uh these you know keywords if you understand you know machine learning so let's try and understand each and every keyword first it's a domain of artificial intelligence so if anyone calls you know MLS AI say ml is AI of course it is but it is but it is a domain of Nei it is a subset of an AI domain right our way to extract patterns from the data where the exact patterns are data so I want to explain this to you with a very nice you know practical example let's forget you know bookish examples so assume that you um you wanna you are doing something like um yeah so you're starting some sort of mathematical Concepts assume that you're studying quadratic equations so you're studying about quadratic equations and in that quadratic equations you had something known as taking out the you know uh by you have some sort of quadratic equations from there's a form formula for taking out using loses um like the concept right so there are so many word problems regarding ages uh distance and speed Etc so now many actually when you actually solve those problems when you actually solve those problems you learn some sort of patterns that this is how I should solve it when you learn those chapters we expect those chapters when you actually understand those chapters you learn how to what what what patterns are there what patterns of questions are there and how the solutions how the solutions are framed right so in this way in this way we have to figure out in this way you are learning patterns from the from from the chapter and by learning that you're able to reach to a conclusion right so once you have the pattern now if the new question comes replies that whatever patterns which you have learned are analyzed your your your content now you use that analyzer whatever whatever you learned to answer those or predict predict them cqs right so answer those new questions right and what from whatever you have learned till now so if you if you didn't find the pattern well you will be not able to you will be not able to you know answer that well but if you know but you will be able to answer very well right good so that's exactly machine learning work so we're going to understand in that way so it extract patterns from the data so for example if you take an example of a Spam or ham example spam or ham example it extracts patterns from the data it extracts patterns from the data where how spam email looks like how hammy looks like it learns analyzes that whole data data means the collection of information in this case you have a Spam and Es spam and have emails that's the collection of information you extract the patches you understand that house fam looks like how ham looks like now once you have that I analyzed it now now when new spaniel comes in you use the model uses that analyzed or patterns to make intelligent predictions on the data which you have right according to the question your according to the pattern which a machine has learned that's what exactly machine stores so now you might have several questions by now I know it first is again how to extract patterns how machine extract patterns from the data how machines analyzes the data how machines make intelligent predictions from the data and Etc right so these are questions and in the whole course we'll answer this we'll answer this question answer this how machines extract patterns how we analyze the data how machines make predictions from your data right so there are set of mathematical as of now the answer is their set of mathematical algorithms that helps us to extract patterns mathematical or statistical algorithms it's from the data which we'll study throughout the course okay uh so that's an answer so let's go to a more formal definition which I really like by Tom Michelle and that's here's a nice book I recommend you to read that book really nice book if Tom is still is seeing this I don't expect him to be seeing this but hi to him I'm just kidding uh here's my one of my big inspiration and Machinery field to get started with his books on machine learning is really really influential and his lectures as well so nice a computer program is safe to learn from experience a with respect to some class of a task T and some performance P if it's performance p on on task t as measured by P improves with experience e if you got this very nice if you know if you don't that's totally all right because I was also the one even in in my middle of the faces I was not able to understand this definition this is by Tom Michelle okay so let's understand this definition by this flowchart which I prepared for you so you have a computer program assume that this is your machine learning machine learning uh whatever you can come to program what is model I've been taking model name right so assume that model is your machines as of now okay machines assume that as of now I'm just taking model just assume that as a machines this is not having so much of jargons so you have this computer program you have this computer program that computer program learns from The Experience e and in this case experience e is your data your computer program they learn or analyze extract patterns from the data for some task the data is for for some tasks right so for example if you have a Spam or ham data which is a collection of information of the spam emails and ham emails so that's the experiences available now the task was to identify spam so that's what for that task and that performance on the task is measured by P so we measured the performance and once we have the performance we improve and that performance is in being improved by adding more data or adding more experiences in that so that's what exactly it says hide highlighting the important terms it's safe to the learn from the experience e with respect to some task T and some performance measured P if the performance on T the task spam and ham measured by performance if the measure by this it improves to the experience if if you increase the experiment experience it will also be it it is going to be improved so this is a definition by Tom Witcher I hope that makes sense another example is Indian house price prediction Indian house price prediction system so here a task is to predict the house prices of Indian states where you want to build any ml system you need one of the key ingredients which is data which is in this case new experience so your trained model which is your computer program on the experience and this is the EXP on the pass data which is experience because password is experience right and here past data is your experience so the performance p is measured how well your model is performing on that particular task and if you add more experience in the the performance will be improved that's it that's definition very easy now I hope that you understood do it for other applications very very nice practice however if you if you want to do the practice all along please enroll in our official ml course as well as well as you can also go um and review some of you know uh um courses available the the three LMS which we ask you to enroll yeah good so what are the steps done by your machine Learning System so that's something what are the what are the steps for machine learning follows to actually do this I know this is early phase to talk about that that's something is super important which I want to have a talk hurt to her talk with my viewers on this however if it's okay if you don't understand it but I won't just want to have a do a talk with a very detailed explanation of everything and then we'll talk about data and then and after that we'll talk about some of the applications and then we'll um talk about a very nice application or the type of machine learning which is supervised learning we'll talk about unsupervised later on uh but as of now I'll talk on supervise only so hey folks welcome to this video in this video what I'm going to do is to talk about some of the steps uh which is done by usually done by Machine Learning System so what is the procedure what is the process for going through uh for for building a a good machine Learning System what is machine Learning System the system in which we give an ability we given ability to a two two two to a machine to be able to to learn from the data which which means to be able to learn from the data or extract patterns from the data and make intelligent prediction so we're going to make a machine a system what is the process of that system where we extract the data we analyze the data and make intelligent predictions according to what it has learned so um now so what the like everything for example while while creation of you know maggies or any sort of short sort of fools you have a processes so just like that like well creation of machine Learning System there's a processes created or recommended processes which I really want to talk to you guys so let's get started actually with that uh so it may happen some of you may not understand the fully thing that's totally all right totally all right if you don't if you don't understand it because I feel like it's it's something which you might focus on things like um definitions and the understanding core and a Crux of what exactly each step is doing as as there is a section in cs01 course so the the the course which I teach on Anton so there's a section called ml Ops where I cover these these things in great detail and talking about several case studies and all so if you're interested in something you know learning in depth and all I recommend uh see ESO one if you want to learn something fully so now coming back to this you have something known as scoping so you have something like that scoping and what that's what does this scoping means scoping is something like uh the first step in your machine Learning System is scope out the problem you're trying to solve scope out the problem you're trying to solve the first step is scoping where you where what you do you understand the problem statement and write down or jot down the key requirements for your ml project so basically whenever you start off with any sort of project or machine learning project machine learning any sort of machine learning project so what you do over there um you simply first of all understand the problem statement right you you understand the for example let's say you're building a Spam or a ham classifier so you have a system they have a machine M you want to build a machine M that that classifies whether that is a spam or ham right so the first step is scoping where you understand what exactly this problem statement is what what sort of data which you have right what sort of data which you have what are the key requirements like requirements what is useful analysts do you understand the problem statement according to given by the state stakeholders and the second step is you understand key requirements like do I require the how many data scientists to work in this what what should be the compute power of this what should the infrastructure required for this Etc so scoping out means getting a brief understanding what this project will be about and creating a brief plan of go to strategy for working on this project then after you have a brief plan and everything now what you do you simply go to the data part where you do where the examples of the steps is now now once you have the data you have to eventually clean it we'll talk about data cleaning and all in one of our project so when you have the data when you have the data you have to clean this data you have to clean the data your processes you have to process the data in a good way and Etc so basically what I'm trying to sell over here what I'm trying to tell over here that given the data you know you you want to you know uh understand it you wanna to uh clean it Etc so I think the and then you have the data then you cross collect and validate the data accordingly right and then you clean the data where uh where you have removing of missing values Etc so I know that this say this will not make sense if you're a beginner so it is something like you have the very bad data you convert that to a good data by data cleaning and Etc you validate the data whether that data is from truthful resources or not whether we can trust the data information or not now we have the data now now what now what we need to do we need to extract patterns from the data and for extracting patterns from the data we have something like modeling not something which actors do on a ramp work but I think on this modeling requires something like you know uh some something like yo your your your shifts your system tries to extract patterns from the clean data which you have right and then once you have the ones who wants once you're done with modeling you have now you're gonna put this you want to put this so that your users can make predictions from this model M so once you have the trained model get it low with learn to extract patterns so now we want to make predictions for it for example if this machine is of spam and ham classifier so I'm going to put this in production so that it is able to classify so that every email can get to this and the model is able to classify in couple of stuffs which is spammer ham so you know put this in production so that it can be used by folks um in real time and also we Monitor and maintain our system we see like how our system is performing what is the like whether a system whether a performance of a system is degrading or what what is the possible errors which are coming what is the etc etc so what is the wrong predictions which are coming Etc so we have to monitor our system as well so there's four steps for any uh this photo four to five steps of recipes of machine learning where you first of all plan out what exactly you need to do by understanding the problem statement and then you get the data and while getting the data you have to validate whether that data is from truthful resources and clean the data in a good data and once you have that good data now you model it which means that you extract you learn to extract patterns from the data and now you have the model which which has the patterns from the data it should be able to successfully use it in the production right and then and then you have to continuously monitor it so these are the steps for your machine learning applications now now let's talk about uh I hope that this this makes sense but now now let's talk about something known as data so what is data so data is info is in information contained in an structured or unstructured format so um so data is set of Records so you you might have seen in your you know machine learning lectures or sorry uh in your class eight or nine lectures of computer science where you might have gone through a data is a set of Records right it's a records or set of information available for the particular uh product so for example you have Amazon listing you have Amazon listing let's say you have you have Mouse right you have Mouse so that Mouse might have several several data right several data like what is the title what is the description what is the price so these are the uh set of records for the particular day Air Products so just just like that data is in information which is in structured and unstructured format okay and um what we'll talk about what is structure and unstructured in a bit but here's here's an example of a data where this is the example of a structured data where it is contained in a table where this data contains the the it is the data of house price prediction where we are given the some of the information about the house prices uh House housing price data where we given the sum of the information about house prices which is first one is floor what is the floor space how many number of rooms in that apartment how what is the lot size is there any other apartment attached to it right Row House Corner House detached so these are the information for a particular house right and this is the price and one thousand dollars right so this is a price for the for for this feature but this feature this is the price uh for that particular house then so we have the second data point this is the this is the this is the record for the second house this is the record for the third house so you got for the um this first house second house third house fourth house fifth house sixth house seven hours six eight and nine house so you have the nine thousand nine houses where for every house is you have your own infos using uh including pricing floor space rooms Etc so these are nothing but uh uh these uh these are nothing but your your data where we actually have something uh uh information contained right cool so now what we do now what we know what what you want to do is talk about a bit of application uh in machine learning right the first application which we really want to talk on is a loan protection system so if it it is an application so loan protection system is an application in which of machine learning so assume that you are in a banking sector you're you're in a banking sector and and Bank provides loans to the customers right so you might have visited loans and also your bank might provide loans and also the customers who require uh rights so now so now you assume that you are working in a bank and then that that bank gives loans to the customers who requires the loans so when providing loans there's a high risk that that that the person may or may not return the loan so this case that the person will not uh return the loan back with interest right it it may return loan back but will it are they going to return back with interests or not so given the bag down information about the particular customer what is the history of them like my salary etc etc we've given those infos we're gonna predict we're going to predict whether we should give loan to that person or not this is one example another example is the spam hand detection system where you promote another example is Spam ham detection system uh where you have where you given an email you know a classifier that that email is a spam or have recommendation engine you might have searches something like this which you're seeing in front of you so this is a recommendation engine uh in Google searches or maybe recommendation engine in your YouTube so these are a couple of applications which you might have already seen I don't intend to you know just just to give you a feel about it so I hope that you understood uh in a nice way about a very good processes of machine learning what I want to achieve in the next set of video is talk about the last thing of this uh to talk about the last thing of this lecture is to talk about supervised learning and we try to learn super supervised learning with a very Visual and very storyline of examples so that anyone even if a class 5 you will be able to understand very easily hello everyone um so now we'll talk about something known as supervised learning and hello everyone hello everyone we'll talk about supervise so now we'll talk about supervised learning and uh we don't want to directly get started with you know definitions and all I want to tell you a very nice storyline a very nice example relating into the Practical stuff so let's get started with that the first is for is to understand the concept so now so now whatever you're saying in front of me is to understand the concept for example for example you have assume that you are studying a mathematics book right so you're studying a mathematics book and there you understand some concept you understand some examples a date so you understand something so so you have a mathematics book you understand you know some concept you but you for example you have a quadratic equation as a chapters you do some examples you want to analyze some formulas you want to analyze some patterns for solving a particular questions and Etc so now you have once have the questions now what you do you're trained with the text to your train so now you understand the concepts you're trained with the from the examples and train for the exercise questions with the help of Supervisors so when doing exercise we are doing examples you're analyzing the patterns so you're getting trained with examples and exercise questions with the help of our supervisors so of course you have a question and this you're also seeing the solution if you're not knowing if you are if you're getting error in that uh question right you're not able to solve that question so you seek help from supervisors or whoever the instructor is out there so by now that our storyline is that you understand some concept you get trained from that examples you do also do the back exercises exercise questions with the help of your instructor and whoever now once you're trained with everything now you sit for an examination you sit for an examination and in that examination you're not you do you you don't have any help of Supervisors you don't have any help of exercise Solutions you don't have any help of answer sheets Etc right so you sit for an examination and if you were trained well then you will be performing well otherwise you'll be not performing well okay so this this was your uh this this will be condition after giving the examination so if you're if you are trained well with the help of your supervisors and everything you will be able to answer it very well so now now you can see that how you are trained you will train the examples questions where you're given question as well as the answer to it so that you can understand the pattern right in exercise questions you have a question as well the help of Super Soap supervisors which is question around so you have a possibility to see the answer right so just like this you the your train with examples and the questions now you sit for an examination you sit and give the examination and then that will predict whether you're going to really um pass the examination or not if you were trained well analyze the question well you will be able to pass and the whole concept is nothing but called supervised learning so let's let's talk about what exactly supervised learning is so in supervised learning you're given the question as well as the answer so that your model learns from it and get trained from that right so in supervised learning you're given the question just like this just like in this case you're in what for for training for this particular child you are given the examples and in that example of the questions as well as the answers right to uh to understand the pattern so in supervised learning as well you're given the question as well as the answer so that your model learns to analyze pattern from it and then whenever you set for an examination which is the new examples you and if you if you were analyzed well or if you're trained well you'll be able to solve it right but now but now what I want to do but now what I would really want to talk is on something which is which you which you might notice is that assume that you have an example over here for house price protection system so you so you have an example of housing price price protection system where you want to predict the house prices based on the given features so you know you know you know you know invest in real estate and you predict the house price in the coming 10 years by giving the following requirements for that reports for that particular house so this house number one is the house number two it's house number three so given this characteristics of the house we are we under predict the price of the house right given the characteristics of this house we want to predict the price of the house characteristics means what is the floor space rooms and all given all of this you want to predict what is the possible price if you have this so so now first of all we have to analyze it right so any for every way before sitting in any examination you have to learn it right so you're in supervised learning we are given the question which is a predict as well as the target which is the solution which from where we have to learn it so your model sees the question okay this is this is the question this is the information given in a question so any provide problems has the wrong information right and they ask us to predict something and also we are given the solutions as well so your model will find the relationship between the price floors your relationship between Solutions and problems and analyze patterns from the data right uh and then and then get trained get get trained so over here this solution the solution is acting as a supervisor for this model for this question so basically uh this will be this is this is this is helping this is helping your model to learn right if you don't have solutions for example if you sit for mathematic examination and you want to prepare for it if you don't have the solutions how will you understand the pattern right so over here you should have the solution you should that that that that is called Target variable right so you give some features based on that the target which is price and then algorithms learns from this and then you give new examples which will be without Target so now so now your model trained on this now you you want your model to set an examination where you only give this information you only give this particular a question and then say Okay predict the house prices based on the given info say for example this now your model will predict this price you don't have to give this because now now now it's our and now something will will introduce to evaluation system where it will check yes or wrong right which which is the which is the task of an examiner or a test Checker but as of now uh once the model is trained we can give only this question we'll have the answer later on where the algorithm tries to predict the answer so supervised learning is the type of machine learning in which models are trained using well labeled training data so labeled means every data is labeled well right so this this particular information is labeled well out here this particular information is labeled well outer right so you have the question as well the solution and machine learning tries to predict the output so here we mean labeled me so here we mean labeled means what does labeled means it means that let's say you're working on spam or ham protection system so for every email you every email for example email number one you have the label related so so that your model learns right so you have the you have the data you have something like this data and for every email you have the weather for the foreign first of all you have to make it learn by the historical data so you have the email number two it is ham even number three is Spam so the the the the label that label order or the thing which are to predict is given over here so this is what the label is in machine learning where it is label means that for every given input feature in this case the email we have to we have the correct we have the correct uh label for that whether that email was that and then we ask a model to learn from this data right and then and then after after more learned we just say we we just only give emails and then tries to predict what will the possible outer from that okay so uh super supervised learning problems can be classified into two ways so first ways is regression and second ways is classification so we'll talk about a classification regression in detail this course focuses on regression but uh if you if you want to learn more about you know complete ml with earning opportunities I recommend CSO one you can go and watch that but uh in regression in regression here here and supervised learning problems can be classified into two set of problems the first problem is the first problem the first problem is that your target variable which you predict is continuous values so assume that applications like predict prediction of house prices so in this house prices you have your target variable your target variable which is the house prices which are continuous which are continuous value so if you don't know what is continuous discrete Etc you can skip to the data fundamentals lecture where I'll be talking about data fundamentals in detail right so over here here a Target variable which you want to predict is continuous so for example the house price your pricing example is continuous right a revenue prediction sales prediction Etc so these are the examples of your regression values right there you're very the the problems where we want to predict are the uh when you predict where the target variable is of continuous value but in classification our Target value isn't discrete whether a person has a cancer yes or no whether the person whether a answer is correct or not yes or no whether this is this true or not yes or no so this is called sort of classification problems in this course we'll only deal with regression problems if you want to know more about CM things in real you can go to classification problems however we'll talk about each things in detail as we go now I think uh this is something which which we are done with whole machine learning now you may think here use various unsupervised learning where is reinforcement learning this course is intended to make you inclined towards the learning the core stuff of it so I'll just give you the idea what is unsupervised learning is and so while learning in which you don't have the solution you only have two you have only have to make this loss any sort of sense of logic from out of the data and then get the prediction anyway so in unsupervised learning you don't have any sort of supervisor in this case in this case in this case you don't get any examples you don't get any questions you just don't get any help of Supervisors you just strictly sell an examination and then you have to learn from it you have to analyze the questions and then make a pattern sort of it right so that's what answer revised which is the most challenging today and most of the work is being done in supervised learning but uh but unsupunt supervisoring is very challenging but uh that's something we'll take a look at later on right that's something which we'll take a look at later on at the end of this course but I want you to stick with as of now is with supervised learning and let's try to learn the core of supervised learning and master it hey everyone welcome to the another module which is linear regression so what we'll try to achieve in this module is we'll try to learn in depth about a linear regression we'll try to understand each and every concept so the table of contents and everything is already introduced to you so I hope that you know what you're going to learn so before starting up this video I would like to talk about the two terms right now is to talk about linear and then talk about regression as this course is only based out of you know machine learning code and linear regression so we'll talk about only these two terms which is linear and regression separately so what is linear linear is something which which is which which is like a straight line like you know so you you might have seen linear graphs where it is just a straight line right and then what is regression regression is or a sub problem of your machine learning you know it's it's it's it's it's a sub problem of a supervised machine learning uh machine learning task where your output variable is by output variable where your output variable of your so the output variable or the Target baby build which you want to predict is continuous what is continuous we have already seen in our previous lecture on data fundamentals so continuous means the height of the person if you're predict the height of a person you predict the stock or a stock of a stock of the particular company or if you want to predict anything which is which is which is uncountable or I would say nonfinite or something which is continuous like height a uh height age Etc but there's this discrete where if your target variable discrete where you have a finite number of possible outcomes over there then that's a classification problem so we are going to approve approach regression problem outer regression problem out here using one of the one of the algorithms called line iteration so now one of the now you might have seen in our definition of machine learning that we use um that that we how we how we extract patterns from the data so this is one of the ways we are going to study one of the ways for extracting patterns from our data and then this sets a very nice base so once you have this you you'll be easily able to learn logistic regression and other things but if you fully want to learn I suggest cs01 cores for you which is the best for you maybe yes so let's get started eventually and let's try to talk about what are the things which is needed out here cool so over here you're seeing something known as um we use ml for estimating or establishing relationships between two variables so that's something that's very confusing so I would like to take a time to make you understand what exactly I mean by that so over here over here what are what I exactly mean by that is that assume that you have um you have that you're working on something like let's let's take an example that you are working on cancer you're you're not you know predict you know predict whether the person has a cancer or not you know right so so you know predict whether person has a cancer or not so you're given them you're given some of the information about the person so you're given about the the B BMI info person which is blood which is BMI and then you're given the weight of the person you're given the height of the person so you're given these three features these three features you're gonna predict he ought to predict whether the person has a cancer or not I know this these features are not enough but bass assume that based on this hypothetical situation based on these given features you're going to predict whether that person has a cancer or not okay so how we're going to approach this and how exactly we are going to uh take up from here so one thing which I just want to tell you is how how we are going to approach this like why how can we relate this to establishing relationships so you have these feature you want to predict whether the person has a cancer or not so over here you want to establish a relationship between every feature you have so you know to be honest you establish a relationship between these these two things whether there is any relationship between BMI and cancer whether BMI has any effect on the person is having cancer so if BMI is high whether the person is having cancer ba BMI is low the person is having a cancer so we have to you want to identify the relationships out of it identify the relationships or something like I identify any sort of you know and any sort of um uh you know the any sort of relationship between uh over there out there so you know establish the relationships in um in various like you know the BMI and the cancer or maybe establish a relationship between feature number two which is weight of the person and the cancer so does weight of the person affect whether the person has a cancer or not does does this particular info affect whether a person is going to have a cancer so over here what exactly we we use ml for exactly identifying that exactly the same thing we use ml for establishing the relationship with this particular feature is going to affect my output variable in this case where my age or weight or BMI will affect whether I have a cancer or not okay individually like this feature the effects or not this feature if it's if yes then how by what units Etc so these these questions are answered so we'll take a very nice problem statement to make you understand this particular statement which would be easy for you so let's take an example your marketing strategist a data scientist for a company and basically this is what the data scientists do eventually have the marketing team would have the decision team to take certain decisions or marketing team to take some certain marketing steps so you prepare a marketing plan for a car company so you have a car company and then you prepare the marketing plan how does marketing plan should be there for car company or to help the marketing strategies out there with your data driven decisions so manifold of the first is a manufacturer of the car so so you're given these particular infos about the car the features about the car right so you saw you're given these one two three four four information about the car four features of the cars and based on this how to make a marketing plan okay so your make a marketing plan so in what way the marketing strategist asked you as a data scientist asked you that tell me which info we should highlight out of all these infos in our main ad so when you make our when we make our main ad which info out of all of this we should highlight so that we make maximum of sales for that car understanding what I'm trying to say what I'm trying to say that there is a marketing team sitting out there you're a data scientist out there they ask you that out of all these features can you recommend me which info which info I should I should take and show it in ad so that I have a most of the market uh most of the sales okay and just I don't want the prediction I also want to know the following questions so marketing's team is setting and asking asking this question so what are those questions question says is is there any relationship between N4 that particular information the car so is there any relationship between the manufacturer of the cars and the sales so of course as you think that let's assume that manufacturer is BMW but but it may have like there might be a very nice relation some it may happen that the brand manufacturer of the car is high but the sales might be low because the brand is super highly expensive right but if you have something wagonal or you know low budget the brand is Tata you know Tata Motors and all so they produce less expensive so the sales will be also very high so the manufacturer of the so we have to understand is there any relationship if yes then what a relationship is available between both of them and that answer is how strong is your relationship between that particular manufacturer and the sales you do for every model of the car engine size horsepower right so what is the relationship between a horsepower and the sales so it horsepower is high what is the sales if low what is the sales right if if engine size is highs size is highs what is the sales if the inner sizes though what is the sales Etc which info contributes to the sale so which information sometimes it may happen that this that that any particular information may not contribute to may not have any effect that does not really make sense just like in prison pigeon example which is senseless right how accurately can we estimate the effect how accurate we are in estimating the effect of each individual features each individual features on our output variable y and is the relationship linear between both of them and what exactly we say that the relationship is linear which means do they follow the linearity or do they follow the the straight line convention so what is linear things which are you know in straight line or something in any in any straight line it should be linear it should not be like this right it should be uh linear so you might have studied new lower classes about that so I hope that this gives you a very nice sense about you know uh we use ml for estimating relationships in the next video what I'm what I'll try to achieve in the next video what I'm trying to achieve is I don't know what I want to do is to work about geometrical understanding is to work about geometrical understanding geometrical understanding right sort of work about geometrical understanding of our regression so we'll actually go ahead and then talk about how we can estimate the relationship between features and outcome variables in easy way so let's go ahead and talk about that so now we'll talk about something which is now now once we and now we have studied about you know he this is how we asked this is what we have to do this is what ml is about estimating relationships so let's start talking about how we estimate relationships between variables right so let's let's get started and actually talk about that right um so over here what uh so now uh before going on that let's talk about what what it What does it means to say that data is linear right data is linear what is it what does it mean so data that can be represented on a live line graph right it's a data which can be represented on a line graph and there's a clear relationship between two variables between two variables is a clear relationship between two variables that the that that the graph can be shown on a straight line so there's a clear relationship with you which which you're saying and one unit one unit increase in X is showing a constant increase in y right that's what the data is said to be linear you might have studied in lower classes Just For Those Who would know who who just want to recap I just told about that this is an example of a linear data this is an example for linear data this is an example of a linear ignore the lines as of now okay ignore the lines as of now just go just go about the just go about the data which you have out here just just go about the data so you see that the data is linear if the data would have something like this you know very uh very random you know there's not showing any clear relationship there's not doing any clear relationships between between variables then it might have not been this is called a nonlinear so data is linear over here and over here over here assume that this is your data assume that this is your data so now assume that this is your data we're on x axis we're on xaxis you have something like the number of years of experience so you're going to build a system so uh let's let's formulate a problem right now you want to build a system you want to build a system that predicts the salary of a person given the given the um years of experience of a person so given the the years of experience of a person you know predict what to what will be the possible salary so if the person has a two years experience you have such such number of salary so you are given a data in a CSE and a CSV file load you give give given a data in an Excel sheet and in that data and in that data you have something like this uh record and your you have one information available which is the years of experience and assume three and there is the label a Target variable we're going to predict the salary which is in let's say five right which is in 5K dollars five k five thousand dollars to ten thousand dollars whatever per year according to Indian market um so over here you have years experience and the salary of that person right and this isn't a 5k okay so if there's five over here that means five thousand okay so uh and then you have the data and then you plot this data over here now you plot this particular data over here because you have only X variable you have only X variable and then you have y so you know you're given X you're gonna predict y out here right you're given X you're given X you're gonna predict why right so I just want to talk about one of the thing which is super important right now is is this something known as functions conscription to functions so in functions what we do in functions what what we do in functions what what we do say for example that this is a function which X squares the number which square is the number so this function takes the input value of x take the input value of x and outputs y and outputs Y which is a square of X right so when you plot this function you're going to plot this function this looks like a parabola this this this looks like a pattern this this is a parabola okay so X and then f of x then it takes the it takes the input value does some processing and Returns the square of it Returns the square of it which is a square right so in this case we want to tell we're gonna We want to build a function f so we're going to build a function f we're going to build a function f we're going to build a function f that takes up your Builder function f that takes up years to experience of a person that takes up years to experience of person and map this which we call this X in this case the input value the the thing which you're going to input to the model like give this input for example in cancer prediction we will give the input like BMI weight agent to give input and then as output we're gonna We want to predict what will the salary of the person that's the output out there that's the output out there which is the salary of a person so you this this does something so this does something with this input value and then returns you the salary so now so now we're gonna build a function now I'm going to build this black box thing this this this this thing out here this particular whatever processing that does with the user experience so we're gonna build that you know for for building that we wanted to learn it's the algorithm called linear regression where it helps us to building that particular processing that takes up your input feature X and does some processing and returns some values file okay so so how do we learn that function f that takes the value y um es expense and Returns the Y so let's start off with the geometrical understanding so assume that you have the data plotted over here on xaxis you have years experience and one yaxis you have uh salary of the person so this is the data which you plotted this is the data which you plotted I hope so that you know data plotting at least given the tables you're given the tables out there you should you should be able to uh plot data out here very easily right so when you plot this data out here when you when you plot this data out here we've brought this data out here now you look the ignore the black line as of now you plot this data now now your what what you do you find a straight line you find a straight line that best fits the data that best fits the data you try to try to find the line that best fits the data so why do we find the line that best fits the data right that's a good question right so assume that assume that you wanna you have um four years of experience your four years experience so what what will be the possible uh salary of yours so if you have the fourths four years of experience forced to experience the actual value states that the actual value states that you will be having somewhat like you know um I think eight eight thousand dollars for a new right but your model so so basically the intersection from X and the in the intersection point so basically the intersection point is this is this point so this is the intersection point and this intersection point is your is your is your model approximation model approximation so let's talk about what exactly this model approximation means model like proximation is something like your model approximated so this is there is a there as as we say for example assume that you have you have you have your examination and is the examination similar questions comes or exactly same question comes right so it might happen that your approach might be different from the books approach it may be similar question it may be similar question so you answer differently right so you over here don't exactly answer the same thing you written differently for example in science exam in science exam you don't write exact exact the same question exact the same answer there is some of course the your you have a proximate answer right not exactly similar to the original answer notice actually or similar to the bookish answer but similar to that so if someone asks what is photosynthesis you might ask you you might write your answer in different language but the original may be different so there's there should be some difference between both of the answers okay so in the same way if someone says that someone asked a question what is the sale what is the salary of person with the US experience for where they ask they give the years a four we have some processing done and then it results in some values okay and as of now we are trying to understand the geometrically how it should work if someone asks what is the what is this idea if if the person has the four years of experience that says that if if if I ask what is this year's experience when when when when the years experience is four it says that the the actual answer is this the when actual answer is this actual answer is this with original but there is the force the difference but there's of course the but the actual model approximated this this straight line as of now this is straight line so as this is a straight line so we found the straight line that best fits the data okay so we found a straight line and then we go gone and see okay this is the point of intersection and this is your model prediction this is your function approximated value not exactly same but similar to that the small difference between bulbots and then right okay so now you may may have served several questions please don't worry we'll tackle each and every question in detail okay so let's start tackling each and every question in detail now over here what you've seen is that assume that this particular information and this particular information so there is of course the difference there's a model approximation and your actual actual value so what is the difference between model approximation to Natural value as I again told in Science examination for a particular question there might be different answer but concept is same but different answer right in their own words in bookish language but you might have different answers in any other language so there's of course a difference so if the difference is high that means the concept is not same so it will so your your examiner will match your answer with the original answer and if the difference is similar then that's good but if the difference is high this this the the dissimilarity is high then the answer is wrong because you might have written in when you when when you were told to write the photosynthesis you you came and write written something like um respiration right so that's that that will be wrong right so I think that's that's a very very good example State over here so now your the actual answer though someone asked about photosynthesis so actual answer is the bookish answer which is so correct which is 100 correct we call that thing as a ground truth and then you you return that is a student approximated answer that that student might have written some answer so we we match both of them but take out the difference how how how similar they are if the similarity is if the Sim if if the similarity is if this if the similarity is less that means that uh that the answer is correct but if the simulator is high that means oh that something is away right that something is wrong that they have written as respiration instead of photosynthesis uh chemistry is the something never mind so this is what the difference between model approximation so in this case only your model says that for this much for this much of experience this is the particular uh value of this particular salary but your actual value states that this but there's a difference so higher the difference but why did I say that best fit line but why did I say this specific line so why did I say this best fit line because for example assume that your line is like this which does not best fits the data which does not fits the data well so now if you have four years to experience if your four years of experience you have four years of experience the model approximate is that intersection point which is nothing but which is nothing but four four thousand dollar but actually but actually it was six rights so the difference is high so the difference is super high right but but actually if you see that this is your actual value so this is your actual value right this is your this is your correct answer but this is your model approximated value this is a moral approximate value right so what there's a difference between so over here there's a small difference but over here it is a bit large difference so that's why we want to find the line which best fits the data so now for example so now for this so now for example assume that you go to this line or or the and this is the best fit line which is out there which is in over here but assume this this line we have four years experience we have four years of experience this is but actually the the correct the correct uh the correct is this one so of course there's difference but this particular but this particular line which best fits the data right which best fits the data uh where we are using that uh to make prediction right so now I think you gotta you got a way to um approximate your values you got a way to approximate your values now now let's worry about let's talk about um in detail okay let's talk about in detail what exactly I'm trying to tell is why exactly how exactly we got to our answer so what our problem was our problem was that we wanted to extract patterns from the data basically going to learn from the data and make intelligent predictions so we're gonna find the best fit line that means learning that find that best fits the data for example you want to learn your you know to train your mind that best fits the uh the concept the best fits the concept and around that concept you utilize that concept and that's called hypothesis you utilize that step to further make prediction to further make predictions or to further solve the question so in that end just in this way only it first of all it first of all makes the best fits the data best fits the data by a straight line it should be linear right it should be linear in a straight line and then it best fits the data and then we can use this best fit to see okay if we have currently we have only one input value if there is more then there's a several things but yeah so given that you can just go go and then this is the point of intersection so this is your y value which is in this case the Y values on Y axis so we simply predict that I hope that it makes sense if it does not please review the lecture again and again and if you still don't please go and review the concepts of Class 8 and 9. okay uh why to find the best fit line that's a good question about why exactly we want to find the best bit line um so I have given a very very nice example out here which I've just described to you so now uh you might you might have question on how do we find this best straight line how do we find this best fit line so let's try let's go ahead and let's try to find this um how exactly we go ahead and find the best fit line so so now you might have question okay now we have the data like how can we find the best fit like the first the first solution in your mind might come that we can plot our data on X and Y excess and then use our drawing skills to draw best fit lines so you can simply say and draw like this and then draw best fit line but the major defect is we cannot really make predictions from that so how can you make exact predictions from it tell me this won't defect how can you tell give me the exact protection you cannot give me the possible exact prediction you can just still approach I've just told over here proximated right not exactly the same what model approximate just approach are you sure approximated that's the first effect and the second effect is what if if you have more than one features in information for example currently it's X and Y only where you have years experience to predict the salary but what if your ears experience the designation the skills the ratings Etc so for Access four input values you want to predict one salary so this is a multidimensional thing you cannot really plot a straight line that's the first that's the first solution cut where you cannot use the joining skills to draw a straight line which the best fits the data so how can we get this a straight line so the second solution is that as of now let's go ahead and talk about only one only one thing which is which is you know you you you're only given only one information which is X which is the year's experience when you predict the Salvage that is only one so how can how can we uh do this so second solution might arise that you might have seen this in your early classes Y is equals to MX plus b why is it this is the equation for any straight line so y equals to MX plus b is the mathematical equation for a straight line where M indicates the slope of the line and B B indicates the intercept of that line slope means how much y changes when X changes and intercept B which is the yintercept which is the yintercept where your line cuts at y right if you don't don't know about the concept don't worry I'll just speak I'll just recapsulate uh a little a little bit on this so what exactly slope in this example so slope in this example states that how much my salary will change how much my salary will change if my years of experience will change that's the slope that's the slope which says how much your y changes the salary changes when X changes that's a slope so M over here is your slope and M is also the coefficient of your x and x in this case is a years experience plus the Y intercept where at exactly your y intersects and y y intercept is one of the major thing because we will talk about bias term later on but over here if you have this Y intercept over here if you've then white recept also plays a very nicer so over here this is the wine this is a varied white cuts on the yaxis so now there's one interpretation which we can get from this that if we are able so this particular line is fully dependent on this in these two parameters only these two parameters can change this line right only slope because X will be anything right but but m and b determines how this line should be the slope and the volume step so if Y intercept is different it should be like this it should be like this and slope is different it should be like different different shapes so that your your straight line your best bet line depends on the slope and the B which is the bias though so a straight line is dependent upon slope and the Y intercept so if you are able to find if you are able to find the slope and the best interferent in intercept that best fits the data we got a new line so very interesting very interesting uh um an analogy is why I am saying that they are dependent why I am saying that we are depend that our straight line is dependent for example so you have you have you have this data where I plotted the very small small amount of data and in this example uh and in this example assume that we have this this particular assume that we have three three different best bits lines let's go with the first one let's go to the first one this over here our slope is different and y intercept is different that's why R it is not perfectly fitting our data so now we tweaked in a way that we changed we changed the slope and The Intercept we got to this it okay okay fits the data but over here which again changed and seen that it it is also it fits the data very well right so so you're all use a straight line is dependent upon m and b so if you're able to find the best slope and the best yintercept for the particular data which you have you will be able to to um to have the very to to have the very good line to have the very good line right that's a that's a nice analogy I hope so that you're getting whatever time you tell is I'm just trying to tell you that you want to get the best M which is the slope and the best bee because it determines your values it determines your X values right so I hope that this this gives a sense that our so if you're able to find m and b we will be easily able to get the best fed line I hope that this makes sense okay I don't need to again repeat it again and again so this is a very dummy example which which which you're seeing in front of me uh is about is is about horsepower and sales so so you're you're given the horsepower of a car you're given only one information as of now as I said will only go with one information so you're given the horsepower of the car the RSI based on horsepower what is the sales for the car right you know you are identify that so to find the best fit line that best fits this data so if you plot this data it should look something like this so if you plot the data we have to best fit data so sometimes you don't need to really draw make your grass you to you know draw you know find the tuition you'll find a straight line or linear equation um not linear equation yeah so the the straightly straight line equation that best fits the data okay so your build is function f your build this function app that takes in horsepower and then and then thus and then multiply and then you have multi and then you have the straight line equation which is used for use which is used for making uh predictions so we need to build a function f that Maps your X which is the horsepower to the sales establish a relationship between both of them so here we are multiplying with M with X so that we can say that m is will will come will come to that but over here we want to we want to give the function horsepower and multiply a horsepower with a slope which is how much how much how much how much uh why how much house horsepower how much sales changes when horsepower changes plus Y which is your yintercept which is yintercept where it intersects on Y axis so now you might have questioned that how the why we multiply and how does this affect the output so when we multiply what effects does it goes on output variable what effect is Y intercept gives to the output variable that's something we should talk about in interpretation of our coefficients later on but as of now you can get that that this is the equation where you put in the values of X you'll get the particular value okay you can see over here that let's let's assume your slope is three and intercept is 4 which is dummy as of now if you take our function so now this is a function like this now you have built your function I've built your function you built your function built your function so you simply give the value of x and then get your answer and the regular prediction which is number of sales which you'll get okay so um now you might have service now now you might have uh several question right now like how can we find our best mnb like currently we have just taken a dummy example so how can we find best mnb what exactly this mnb contributes to the uh sales what why exactly we use that first of all at first place why exactly we use it Etc that that can be will that will be answered in later phases as you go but as of now what I just want to tell you you have got to know the very nice geometrical understanding of linear regression and I hope that you will really uh utilize this to recapsulate the concept so if you want this nose you can get that from my website you can also enroll in the course which I'm teaching in fully ml Pro which is from teaching from baby Basics to earning opportunities Etc so I hope that you will enroll in that let's catch up in the next video so folks we are going to continue our journey on regression analysis so in the previous lecture we had a talk on the some example and we have shown that how each like we have m and b and how this m how horsepower has its own weight and then we have an intercept we have talked about this if you haven't seen my previous video please go and see about that so our main idea was how can we find this main M and this intercept which we usually call as B okay so my question was how can we find how to find these two terms because your best fit line depends on this m and b only right so basically that that was the basic question that your best foot line depends on this so for if you want to get the good fit line then you need to find this m and intercept which is a slope of the particular variable how horsepower and The Intercept of that particular variable so I hope that that makes sense now what what you exactly going to do we're going to study about how can we find best m and b for your data which is the eventually in other words best Straight best fit straight line in your data right so what was the POS what is the sum of the possible way which we can think of so over here what is one of the possible way is to hit and trial different different values of m and b and see what works well right so for example for example in the in the in this example you understand you're gonna use the you know build a function f that takes the horsepower as an input that takes the horsepower as an input and predicts the number of a sales so if you want to promote your car with that horsepower what is the possible number of us is where you're gonna get right so you're going to build a function f that establishes the relationship between your horsepower of a car and the sales right so you know predict how how much sales would be done if the if we actually use the horsepower as an or how much sales are based on horsepower what is the relationship between them right that is the one question out there but what I want to convey is that over here for for building let's assume that X is a horsepower you have MX plus B and M for for finding the relationship right so here over here m and b we need we need to find out so over here you you cannot just plug anything like we have seen that if you plug anything we are going to we're not going to get a best fit line so what can we do we can we can try out several values of m and b so we can try at 1.1 or the and then some 1.2 and then with these values given the x value we took in test how well these values are by plotting it on a straight line like this so um we plot on a straight line and see how it performs by hit and trial right we first of all perform like this and then the next what we do is we conduct another we try another value and see if you're getting the best fit line so you've got to what eventually will do you will try several set of m and b right and then you will try to plot the best fit line or the plot that equation and once you plot it you will be able to see okay this is the best way I know this is not so you'll plot and see how is it the best fit line right so head and trial different values and see what works well but it has several problems the first problem is in it will be very hectic it will be very hectic and time consuming and time consuming okay first thing second is what if if you have a 3D or 4D um in higher dimension of data right you might have you might have higher dimension of data and you may not be able to plot every time and then see right so in that in that real world data will not look like this it will have a millions of examples millions of input features right so uh we exactly want to talk on uh that so we cannot really plot and then see against right so that second one is it will be very hard for us to eventually plot the data and see every time So eventually hectic and time consuming so we need to cut that approach out what's the next approach we have an optimization algorithm which is based on this hit and trial only which is based on this hit and trial only but we do it very strategically you learn how we do it very strategically using mathematical process right so I hope that will make make more sense when I actually go into this now so let's let's see let's see how to see what works well right so how to see what works really well so over here as of now we have seen that we can use optimal optimization algorithm to come up with that so uh but before that we need to see how what works will and what not then we can come to this optimization problem so if we want to see the graphical problem like over here we have two sets of data so the first set of data has this this this data and then you plot the best fit line and this is the best fit line available for your data and over here you you you don't have the best line so this this particular this this particular equation is wrong but this particular equation is correct but why because in this particular equation has a best fit line or this particular line uh fits the data best and this particular line does not fits the data valve right so that is one of them now over here you can see that over here you can see that it is not fitting the best fit line so what is is that and what is the conclusion we're trying to make is if you want to see the best headline which I've already seen is will will be this but by now we are only seeing or we are only telling by seeing the data we are only seeing uh we are only telling how is this the best fit line is the best fit line by only seeing the data but data will be you know you have only the horsepower as of now now there will be lot of a lot of input variables for example you may want to establish relationships you know you may want to establish a relationship between maybe horsepower and then you know on a on another one which is the brand of the car you know the social media you know engagement of the car and blah blah blah I'm going to find the relationship where all of these three to a particular variable sales so we have to find you will be getting three Deals we'll learn about this uh multiple uh multiple linear regression soon but you will see that we will have three dimensional data even four dimensional five dimension of data right but it's not always possible to plot it will always not get only some 100 200 300 or you'll get millions of data points right you cannot plot everything right you cannot plot and then see that how it's performing that is a graphical method for General understanding it's not for something which you have to do in real world but sometimes it also comes in handy if you have a small amount of data but eventually you'll have to deal with numerical way right so for example when you plot the when you when you plot the line you can for example assume that you have a equation x f f of x is equals to x squared so the the the graphical will look like this the graphical is Parabola the graphical is Parabola but you cannot eventually use this Parabola nicely to get your answers for larger values you know for larger way for subset you can do for two or three you can do but what if one two five nine nine and nine so you cannot even do that you cannot even try to use this Parable to do that you can eventually do that but it's very hectic and very not possible sometimes it's also not possible right so in that case what you will do you'll utilize is numerical way x squared f 2x is equal to the function numerical this is the function this is f of x graphical way and this is the numerical way this is the numerical way right so we'll go with the numerical way your plugins go one two five nine nine and then you just one two five nine nine times one two five and nine you just squared this and you get it exact value and then you'll get it exact value right so just in the case of linear regression okay we have the graphical best fit line to from at our data but this graphical line is represented by a numerical value which is MX plus b we need to utilize the numerical as well I hope that you understood why the importance of numerical is given over graphical graphical for understanding as well so now let's come to numerical representation that works really really well over here which is which you're seeing in front of me is that you have some data points out here so assume that this is your one data point is another this is another this another this is another so you have some data points out here if some data points out here right you have this blue points are data points in with labeled examples so this data point has x value this and Y value this this report so it has the labeled example y as well y as well please note that we're dealing with supervised learning algorithm and we should know that this is the part this has y right cool so over here assume that assume that your X is equals to 24. okay your X is equals to 24. your predicted would be assume that this is a straight line you plotted the best fit line you plotted the best fit line so this is the best fit line which you have in your data right and then X is equals to 24. this x is equal to 24 if you just see if you go above it will this is actually the this is actually the ground truth this is actually your label value but over here this is what your model predicted because model will predict at the point of intersection the point of an intersection is a model prediction right for example if we say x equals to 20 x equal to 20 would be gone gone gone gone gone and here it intersects right because we don't have any example we don't have any x equals to 20 a label a layer or data point in your data but over here the reason why you best fitted the line so that we can predict like this and then if we if we see the Y value there are 45 as the prediction right this is how we make sense but over here but something but let us assume that this value 24 for 24 your true value your ground truth means true value for example what is ground truth you have a score you have scored 45 out of 15 years test scores this is a 45 which is the correct value which is the truth but your model predicts that you have scored 57 out of 50. so this is your this is a predicted value or approximated value and that was your uh 45 out of 50 is your ground truth 45 is the ground truth so over here as well for this particular Podium X1 for this particular data point x equals to 24 x is equals to 24 so X is equals to 20 4X x is equal to 2024 your predicted value your ground truth is 38 right for therefore x equals to 24 but the model prediction Lies over here at the point of intersection so when you just go over go above you see the point of an intersection over here and the model predicted value is nothing but 47. right so actually the correct value for x is equal to 24 is 38 but your model approximated or predicted 47. so of course there's there's some sort of Errors you know for example you might have noticed that when you're actually worth in class 10 DNA you know when actually DNA copying is done there are variations right there's some there are some errors so when model approximates it comes with errors as well right so those errors between approximator and correct value 38 is the correct value is and third 47 is the approximated or the model predicted value the model what what model thinks what model thinks x equals to 24 and what model thinks decided by the straight line right uh so that's why we say that best with line freely works for best fit line we need to have for a line we need to have a best fit in the data then it will perform very well right so the difference between the predictive is approximated and the actual value the correct value which you call as a ground truth is called Nothing but residual or difference of Errors right so it's called residual or difference of Errors let's take a look at another example by taking a look over here so assume that assume that you have this particular data point you have this particular data point assume that this one this one so over here you have X is equals to maybe 12 and when X is equal to 12 your your your correct value your correct value as well as your correct value the blue point is ground truth as well as your moral prediction lie on the same point which means y i which is a ground truth which is ground Truth for x equals to 12 and Y hat I which is your which is your model predictive value are on the same point which means that they both are equal and assume that the models is that you will have 1200 samples 1200 sales your model predicted to 100 sales and actually 200 Sales was done which means that a model was perfectly accurate your model is perfectly accurate right so in this case the error will be equals to zero error will be equals to zero right so I'll take one example to make you understand assume that you're working on you're working in a date as a data scientist and ask him stop and over there you want to predict you're going to predict based on past data you're going to predict what will be the number of a sales Fund in the next day so your model says that there will be 200 Sales next day right but actually 200 Sales was done which means they are on the same point so over here with x equals to 12 then a y hat is equal to 37 and when y i is with 37 and Y hat I also has got 37 which means both are equal and the residual between them is equal to zero let's take a look at another example when your X is equals to 24 when your X is equals to 24 over here your the actual the blue point as well as the as well as the predicted value is on the same point which means that you predict it as well approximated is r equal let's take a look at another example when your X is equals to let's say 29 you have you have this as the predicted value sorry this has a correct value which is a ground throat and this as your approximated value this this has an approximate a value this has a correct value I'm not so the this the difference between both of them is a residual so your correct value is little bit more than the actual predicted value so what predict value is nothing but the point of intersection when we drop on the xaxis I hope you are getting what I'm trying to say I'm just trying to justify that that the difference between the ground truth as well as the approximated value nothing else I'm doing I'm just trying to justify that okay let's read something for x equals to 24 approximated more operative is 47 but the ground rule is 38 so the error term on error between Y and Y hat which is why hat means which is approximated in ground truth is nothing but 9. so the error term is nine higher your error is better approximation is what does this mean high order error is in bad Deb approximate is for example assume that you are in an ice cream shop and then you pre and your model predicted 1200 sales which which will happen the next day since the 200 Sales will happen but actually only 400 sales happen so the difference between you know for example you're giving a match test right you write a very different steps right to different different times a very different answer rather than the actual answer so you're absolutely wrong right you should be at least close to it when you're giving a Science examination you written something you written some texts which is the actual which is the your answer which you think is answer and there is a correct answer in a marking scheme right there's a correct answer so your your what you what your teacher will do your teacher will match with this if the error is so much in this is so much the the difference between so much then the error is bad so that the bit difference between the why why I and Y hat or sorry y hat i and y i which is the moral which is the actual value and there's a more applicable value if the error between in them is high then battery approximation is why I have given you a very simple example one example is ice cream shop which is 1300 and 400 so error between them is very very high right and that's why we don't have that's why the higher the error will be that your approximation will be right so I hope that makes sense about this higher error and bad approximation is but what we can do we can for evaluating how well our lines are but why are we learning all of these things the reason why we are learning you know R and Phi how well this line is for based on numerical representation by seeing if you can tell okay this is good but numerically we should be also be verified right so what we do we do for every sample which we which you have in your data right so what you do for all these Blue Points you take out the difference of Errors for all these data points for example for example y1 which is this y one my y hat one minus y1 which is equals to 9 right which is equal to 9. so you taken out for the first for the first one and then you do for the size second one y two y two so if you see over here Y2 where is Y2 yeah this is Y2 so this is your predicted value this is your predicted value and this is your ground truth so the difference between them is 44 which is a predicted value minus 45 you get minus 1 right so this is the this is the difference between predictor and the actual value right and then you have per Y3 for Y3 you have difference of Errors where you have y 3 which is the actual value and the predicted value which is minus 4 and then you have y 4 and where the predicted actual value are on the same point you have 0 and you do for all the data points okay this this blue lines and blue points are nothing but the data points these blue points are nothing but the data points right so you do for you calculate the for example what you do eventually so let's take a scenario to make it little bit more easy to understand right when you actually let us assume you have 10 problems in an exercise 10 problems you have the problems one two three four five six as well as you have the solutions so what you do what you do you make your own 10 Solutions you make your own 10 Solutions based on the questions what you have learned you you make your own 10 Solutions right and these Solutions let us assume that this is your name is model and you approximated this or predicted these Solutions now what you do you match it with this you match with the correct Solutions given in your book right so higher the difference is bad your answer is right well that's what we are doing over here to evaluate how well our paper gone is we we match each and every Patriot value with that respective correct value for example question number four we match the question number four with question number four the correct answer as well as the so we just match and check how where it is and we give our mark space on that so just like this for evaluating how well the best vet line our are we just have this straight line line and given a straight line employee we evaluate more operations minus the correct value for all data points available in your data right we do not do for data points for data points which we don't have any label for that for example we cannot do for uh assume that uh we cannot do for let's say two we cannot do do for let's say two right we cannot because we do not have data point for that but we can do for the maybe even seven right we can do this because we have the prediction over here and then we have the model practice the correct value over here okay I hope that makes sense and then you then you take out for and then you have a n number of errors or we can say e number of Errors Eis so you have e number of Errors e will be equal to the number of data points which you have in your data right so it will have the E number of Errors how N1 minus N2 and the one denotes y hat I and N2 denotes y i and then you have negative values as well somewhere so there are two issues which we come we have m n we have several number of Errors how can we combine all of this into one single number to better describe our model and then second one is some of our error terms are negative so you can see that they some are negative right so what we do to take care of this so we can solve the problem we have the negative sign we can get rid of by two processes by squaring the number or by taking out the absolute absolute of that in absolute of that number absolute of that so for example you can we could simply take out the absolute obvious whatever the answer which integer the absolute or the squared absolute or this Square so Square will just cancel out as well as absolutely we will just cancel out so we can we have two approaches either Square to the differences or absolute differences so what we usually we what what we will usually do is we are going to use the square of the number I'll talk about why we are not using absolute but there is an apps we also use absolute as well but I'll tell you later on we'll just wait for my confirmation for some for some maybe next set of sections and then the first problem is all negative we can either use any any of the approach to fix the negative sign but but even after fix it we have certain number of M number of Errors m m is nothing but equals to the number of data points and every as I said your error should be equal to number of data points because the difference is where because you'll be only to your only ask your model to take out the answer you ask your model to take out the answer as uh for the questions which we already have the correct answers so that we can match and test how a model performs so what we can do we can take that average of M terms we can have take out the average of all the errors E1 E2 all over e m but then what we can do we can put that in a submission format so that we come up with the formula so formula is this 1 by m summation I equals to zero all the one or the M4 every Eis we take other every errors hence the expanded Formula 1 by m i is equals to zero all the way out of the m so you do so you go from I equals to all the way out of the M right y i y hat I minus y i squared so over here you're taking out the differences and differences you're squaring that up and adding the another I equals to 1 then I equals to 2 and then I equals to three so please note that you have already studied the summation format right so how it will perform for example assume that you have a date you have a data point you have a data point something like this where you have one two three four five right you have one two three four five so I equals to one is equals to I equals to I equals to zero I equals to one other M right so you have y one y one minus y a y one that's quick right so you do for y equals to one I is equals to two I is equals to three I is equals to four is equals to five so you do for every data point for all the and for all the detail points in your data you add a squared to remove the negative sign and then you app and then you have the approximation value by uh sorry the difference of errors this is called the residuals which you will get and we divide the value by by all the summation all the uh sum by m right I hope that this makes sense this this formula makes sense so this formula should make sense to you you know people usually learn these type of formulas but it's eventually what you're doing you're just thinking of the differences of the multiplicative value and the actual value is squaring it up to remove the negative sign and then you're taking and then you're taking the average of all of them you're taking the average of all of them and this comes up with the name of cost function or we can say mean squared error for evaluating how well our best fit line is how well our best fit line is we will work on a worked example so why is it useful first it is way to evaluate the best fit line it measures the performance of our model so higher the MSC is badder model is why because higher the MSC will indicate that differences are also higher so as I said that if the differences if the differences are higher then it's not a good model right we had to talk on that right so this is one of them this is one of the key criteria which you should know so it is way to evaluate the best fit line and this is way to measure the performance how well so higher the error is battery so you have to make sure that your cost function is approximately or is or around zero so now we come up with the more conventional formula which is J of m and b so basically we're evaluating our m and b is equals to 1 by 2 m i is equals to zero all the way out of the m y I have minus y i which is the difference of headers off you take the average but what is 2 over here what is 2 over here what is 2 over here it's because of the convention but usually people say because of convention but eventually when you learn about this optimization algorithm which I was talking about which I was talking about this too really helps in getting written when you actually do the calculation stuff you know sometimes researchers find an easy method to easy to calculations and these two really helps in that optimization algorithm we'll see how this helps when you actually derive the formula but as of now you can just remember it okay now to recapsulate what what we have studied recapsulate what you have studied is what we can eventually do is that we can take out the differences between more addicted and actual value moral predicted and actual value and then Square it up to get rid of negative sign and we have several values now now we have to Simply divide the term or by the number of values in your data set which is take out the average of them higher the error is that your model is because the higher the different it will showcase the higher the differences is right but there are two questions now why we don't use absolute and what is 2 what is the 2 over here what is the use of two over here and we will talk about the real reason of why we don't take out absolute and why we don't use two over here right why why we are not using ingredient descent or optimization section tool so let's quickly work on a verb example of mean square error that will make more sense so you have your height over here you have your height over here and then you have your weight over here so basically given height of a person you're gonna predict you want to establish a function f that takes the height of person and then predicts what's the weight of that person what's the weight of that person right so given this height and predict the weight right so now over here you came up with this you came up with 0.8 X which is M plus 9.0 which is B so you came up with these with these formula so as I told as I told that you have your uh now what you do you will put in the values of X for the known for the only axis for which you have the known label right or the data points only and you may figure how we came up this zero point is 9.2 it's because I just randomly written it because I want to show you not how we get this but how we evaluate its performance okay so what we do we have 0.8 we have we have zero zero zero point eight which is over here multiplied by this 43 plus 9.2 which is you just exchange the value of x every time 43.6 44.5 for every values for every excess values now now what we do we compare the correct value and the model approximated by the correct value in a model approximated value correct value and model like rocks correct model correct model we take out the differences errors okay we have these errors out here now what we do first of all we Square all the terms and then we sum it all up and then divide by m which is one two three four five five and then 6.08 is your square error so when you apply this formula you will get that okay so basically what we are doing over here is nothing but taking out the answer for every axis and then comparing with actual answer okay cool I hope that this worked example of Ms really helped and everything is pretty much Crystal Clear let me know if you have any questions in the descript Community or in the comment box and try to answer each and every question now what you're eventually going to do is talk about the relationships talk about relationships in terms of mathematics not in terms of life because I pretty much stash up in relationships and life so please ignore I'm not good advisor on relationship in life but I'm good advisor on how can we establish relationships okay we'll talk about the definition of relationships we'll talk about some rotational changes and then we talk about how to find that beta0 and beta 1 which I've been waiting for a long time we have several ideas they build from a plain English language and then we go to a level but prior to this I would suggest that you're pretty much comfortable with derivatives the geometrical understanding of derivatives as well as a bit of you know formula and rules and derivatives it will really really cool so now what we need to do is get started with a bit of next sections about relationships till then I'll talk later so now what we're going to do is talk a bit about relationships relationship is one of the most uh important thing to talk about because usually people tell you know establish a relationship or establish a relationship and blah blah blah and eventually you you get confused what exactly this term the relationships means so we have even just clear about that what exactly this relationship means so that it makes much more sense out of it right so over here whatever what I'll try to do is tell you what exactly the relationships are tell you what is that relationships are and then I'll I'll tell about two types of relationships which is deterministic relationship and second is a statistical relationship and why we want to focus only on statistical relationship over the course right so let's talk about what exactly the current thing which is studying right now the current thing which is starting right now is a linear regression which is a statistical method that allows us to summarize data and study relationships between two quantitative uh which is the quantitative variables which is that type of continuous variables so we have seen it that supervised learning supervised learning has two types of problems one is regression problems and another one is classification problem so we have two types of problem classification and regression problem in terms of regression problem how can we identify a problem is regression problem if the target variable or the thing which you want to predict the thing which you want to predict the thing we're going to predict is continuous is continuous data or is continuous data or some sort of a noncountable or nonfinite or something which is continuous like the age of a person the height of person the salary of a person they are the con their example for continuous value so if the target variable is is is of con is of continuous data then um then the then it then we known then it's it's known as the regression problems then it's known as the regression problem right cool so linear regression is a statistical method we'll talk about what exactly the statistical method means that helps to summarize data and study relationship between two variables now you you may ask hey ayush what do you mean by study relationships and summarize data so you'll study you'll you'll understand more practically later on but assume that you have a function here function f of x your function f of x that tells you that tells you about the the price of a house in a certain area given its size right so you are given a size of the house so you're given the size of the house it will make you it will make you it it will predict it will predict the price of the house this is the output variable which which you wanna get right so given the size you know predict the price of the house right so what does this F tells F will tell you f will establish the relationship as F will tell you about what is the relationship between these two right it will tell you what is the relationship between these two right that's that's the first thing which the tries to answer this and second it will summarize by how if we increase size by this much by some unit then what will be increase in price so if we make price if if we increase size by 10 units what will be the increase in price right that's what it tells it's it tells you about the relationship it tells you about if one increases what will happen to another right they will study about interpreting the coefficients soon interpreting the coefficient so but that's what the that's that's that's what it uh conveys that you have um uh relations between two quantity variables where you have a particular over here which is it tells you about the two uh quantitative continuous variables or the features okay so assume that assume that you wanna predict the sales of a particular car given these particular features so you know build a function f that takes the manufacturer that takes the model that takes the engine size and that takes the horsepower that takes all these four field input features and predicts the sales of a particular car it predicts the sales of a particular car so basically we we we tell okay this is a manufacturer this is more of the guy this is the engine size is a horsepower and predicts the sales so how this function f which it will eventually help us the function f will tell that is if if we how this manufacturer is affecting the sales how this model car is affecting the sales how does engine size is affecting the sales and horsepower is affecting the sales so we can figure out which of the following is affecting more of the sales like if we focus more on manufacture then for example assume that model of the car affects the sales the most so we can focus on model of the car a model of the car to Brand it up to get more sales right because this is this is generating more so the sales for us or if this is generating then we can focus on showcasing engine size or begin to go so focus on showcasing device power or we can focus on showcasing manufacture according to our analysis that's why I say it helps us to summarize and study relations between these variable and output variable y okay so these variable which we give as an input are called explanatory variable or independent or predictor variable okay or value so why we call this as an independent independent or predictor or explanatory first of all every each individual feature each into each individual feature or variable are independent of each other for example this particular variable is not dependent on this manufacturers is not dependent on this for its own existence model the car is not dependent on the engine size for its existence horsepower is not dependent on the engine size for this that's why you call the independent but this particular sales variable is called the dependent because this this is something which you're trying to predict we cannot predict sales if any of these variables are not here right so sales is dependent on manufacturers sales is dependent on model the car for its own existence right um you know I I hope so that that is clear there's something to be removed over here that this is not the predictor let me just remove this maybe it makes more more sense yeah so now this is this is a response or dependent variable this is a response or dependent variable explanatory variable means it itself explains it like it it is explanatory by itself manufacturers we know that what are the card that's why it goes explanatory variable right or independent I hope so that tip mixture you can see our data fundamentals lecture for more info cool so the algorithm let's first talk about what is an algorithm algorithm is in stepbystep procedure is in stepbystep procedure to solve a particular problem and we and we have an algorithm in machine learning to study relationships and summarize data so I hope that this gives you good sense of what this various examples we have a talk okay so there are several types of relationships which you can come across first type is deterministic relationship deterministic relationship so you might have coded in Python that you have forecoded a function in Python that converts Fahrenheit in so your DF code in fudge that converts Fahrenheit to Celsius right so you have code in Python that converts Fahrenheit to Celsius right so that is the deterministic region why I am saying that is a determinist relationship the formula for converting your Celsius to Fahrenheit so we can build a function f that takes in the value of and that uh for the that that function f is a fatter nine that dates in the value of c and nine by five C plus 32. so this is this is an example of a deterministic why it is deterministic because you can simply plug any value of C you'll get exact for X you'll just you it will just be converted into fahrenheit so there is a defined relationship already there for F and C so if Celsius is this much then it will just calculate things up and get you out of f if you know the value of C if you get the value of C you can easily get the value of f exactly okay because this is already defined formula which we have this is already defined relationship which which we already have and it is linear sort of thing for example if if your cells is 0 then your Fahrenheit will well well with this if your Celsius is 10 then your uh then your value will be also on Y axis which is the Fahrenheit so you if you know the value of C you plug in the value of c and then you get exact F other examples are if you want to calculate the if you if you calculate the the the area of a circle which is pi r squared right or Pi d right so this is one this is the area this is a defined you plug in the value of pi and diameter you get the area right 1 by 2 times BH Define relationship you plug in the value of B and H you get the value so these are defined relationships right this equation exactly describes this exact these equations it exactly describes the relationships right plug in the value it will multiply with some factor and then except and then are you it to a a to 8 will multiply the C with some factor nine by five and then add 32 to it right and is the exact or defined relationship between the two variables and this also defined relation between the two variables right but in machine learning we are interested in statistical relationship right in statistical relationship with the relationship between two variables are not perfect please know that deterministic candidate and statistical indeterministic your relationships perfectly defines the particular uh variable between two variables but the statistical it does not perfectly defines it okay I hope that it that that is much more clear so you know find so one one such example is you're gonna find a relationship between skin cancer mortality and latitude right so you want to find you know find what is the what is the what is the relationships if the if what is the ratio between latitude of a particular country and skin cancer mortality so if latitude increases whether it is skin cancer decreases or increases this is a question right this is a question so this is the data which we have this is the data which we have on xaxis we have the latitude at the center of a state on our yaxis one on yaxis we have mortality deaths per that's per 10 million this is the mortality rate this is a mortality rate and this is your latitude and all the blue points are the data points now what you did you you you model the your straight line on this which best fits the data and that straight line is of this where your intercept is 389.2 minus um I think yeah yeah that this this this this particular thing is bit wrong over here this this should be yeah so which is three three point three eighty nine point two minus 5.98 X where this is uh assume that this is your um straight line this is your straight line okay this is your is straight line out there this is your equation for this straight line this equation for this straight line I hope that it makes sense and this is the negative relationship this is a negative relationship first of all we'll talk about the positive negative relationship and further on later on but uh but as of now you can see that this this equation is defined by this um this to stay straight line defined by this particular equation now how can we interpret what is going on over here how can we interpret what is what is going on here so assume that you can see the higher lat altitudes of Northern us the less exposed you did due to the harmful rays of the sun therefore the Lesser risk nevertheless risk so let's uh let's figure out it is saying the higher latitudes of the northern us higher latitude means oh over here High higher latitude which which are going to the the right or the side then you then this 45 uh 45 to 50 so higher latitudes higher latitudes means your legs exposed to the harmful days of the Sun so when you go over when you when you go this side in the highlights so if you see there is no data which suggests which suggest that any any uh values of cancer mortality right cancer mortality is it there's a less or even for Town whatever there is a less not too much not too much 150 there is nothing but on so if you go to the right side road or the more the higher the altitude is less the more mortality therefore less the risk okay but over if you go over here down 35 there is a lot you know there's a whole chunk of uh information out there or the mortality rate out here okay so I hope that this gives a good sense about it so but over here you can see the relationships are not perfect sometimes over here there is this much right but the relationships are not perfect because we don't get exact value we get approximated value but will not get exact value so if you if if if you have seen over here if you have seen over here that we have that that we were getting some sort of residuals that that we were getting some sort of residuals right so your model has approximated but didn't got the exact value this is called the exact value but your model is approximated and the model has approximated it ok so I hope that this gives a good sense about what is why exactly it's not perfect it shows some Trend it is showing some Trend as we go to the SVS increase the latitude your cancer mortality date is it in decreasing okay and it also shows the scattering relationships you can see that all the data is a scattered and it's the scattered relationships so as you can see that this is this is what the statistical and over here over the course we are going to talk about state of modeling the statistical relationships okay cool so if you are interested in finding the statistical relationships so now now uh we'll some we will summarize what exactly we studied after changing some rotational changes so that it makes sense in the next lecture what exactly we are going to do if we are going to talk about how can we estimate the coefficient how can we estimate that m and b in that equation so you have X is y equals to MX plus b how can we estimate m and b by giving you some rotational changes and then we'll talk about how what is the interpretation of that m and b in different terms and then we just and then we'll just recapsulate whatever we have studied so that it makes much more sense and then we finally go on understanding how we can get that in simplest terms so everyone uh now what you're going to do is we're going to talk about a bit about how can we estimate m and b what are its interpretation what is this interpretation what is this interpretation like what exactly it means and Etc after some rotational changes so yeah so first of all what are we given with so you so you're given the data so you're given the data X1 then you have y1 so basically it is a supervised learning problem so you have input value as well as its label y1 X2 Y2 X3 Y3 all the way down to the x m y n so you have particular input value according with with with these labels X2 with this label X3 with this label so you so you're given the data and we try to find the best fit line based on the given data which we have that best fits the data that's our end goal so why best fit line a very nice question to tackle why do we want the best bit line right so line which best fits the data have n errors and they're small as possible in overall sense okay so be on a fine so we wanna find a line which have n number of Errors which are n number of errors and there's small as much as possible overall sense well so what does this mean overall sense in overall sense means that we have n number of Errors N1 N2 N3 N4 N5 n6 N7 n8n9 right so this is the this is the so over you can see that we have one two three four five six seven eight nine ten eleven twelve so we have a total number of error 12 errors so 12 means when we take an average it should be very small so overall means average so it should be as small as possible and then and then we can simply and then we can simply approximate by going off from the xaxis at the point of intersection that is the Y that is the that is the coordinate which contains y as well and then we get a production y okay so we have a way to achieve it by least quiz criteria so least release quiz criteria is something which which will study as well later on but we have a way to achieve it which will study later on but what you can ignore this as of now so this forms the best fit lines so our equation for best fit line X1 is m x i plus b so let me just make sure that this is something this is something which we should ignore yeah uh so which is H of X is equals to MX plus b m x plus b this is the this is an equation you plug in the value of x and then you get a prediction and then you could get a prediction for the straight line so our goal is to find that m and b right right so we can evaluate our best fit line using our MSC only Square criteria so we can simply evaluate evaluate our best value line one by two m I equals to zero all run out of the m h of x minus y minus y squared Why are we squaring because to to avoid the negative as we are taking the average and adding 2 as a convention but will but we'll see why this two really helps while uh in latest pages of gradient descent we'll talk about that but you can assume that as a convention but we'll talk about really we will talk about that why is that two because I think this is important to talk um so you can use simply use this equation to take out some errors and higher the error is where your model is so you use MSE to calculate how your how you are performed so how you're good at errors if error is small so errors depends on the straight line so if the straight line is bad like this is like this is a this is something straight line this is your assume that this is your data and this is a straight line so this is this straight line is bad right so your errors depends on straight line and your straight line depends on two values m and b right because this particular this particular value has its own Y intercept and has its own M so B and M right it has all so it is dependent on M which is the slope and a y intercept B the slope and The Intercept so what we want to do we're gonna We want to reduce our errors right so if you want to reduce our errors what we should work on we should work on straight line we should work on getting good straight line and if you want to get a good straight line we should work on what we should work on in getting the good values of m and b we should we should find good slope and Intercept in order to minimize m and b that with you in order to minimize your cost function to find mnu so in a fine m and b that minimizes your cost version which is yo you have a cross version J of M B which is equals to 1 by 2 m i equals so J of M comma B uh 1 by 2 m i equals zero all the one out of the m h of x minus y squared so Square for negative sign average Tower and 2 4 as a convention so to find mnb that minimizes the cost quench so what are its notational changes so what are its notational changes so again to to to repeat your summary we we have a hypothesis hypothesis give some error like to now here but now we say it how how the hypothesis is this we get some error and then we and then the error depends on straight line the straight line depends on m and b so the final good slope m and b together and if you get the good straight line it will automatically reduce the error a whole story is represented on a screen I hope that this gives a good sense about whatever I'm talking about so let's do our goal is to minimize this particular thing which is J it's it should be J I'm so sorry I'm very sorry some of the um I'm I'm very sorry about this about sometimes you get you you have a bit of you know on errors and all you can definitely put in a Errata page we can fix this up right it will be much more helpful if you help us to complete it okay so what are some of the notational changes so over here right now we are using MX plus b as of now h of x h of x equals to MX plus b as if now for as for our prediction function what we can do we can assume we can assume B to be beta0 and M to beta1 okay these are the parameters so why are we doing this the reason why we are doing this in industry in Industry you will see mostly this beta 1 and beta0 and also it will help us to be bit more clear about you know industry or the online articles so when you go to online article you make sure you're comfortable with the beta 1 and beta0 sometimes instead of beta 1 and Theta 1 and Theta 0 or sometimes you have some other notation but we'll use this beta 1 and beta0 so over here you might assume why are not we taking M as a m s beta1 Ms beta0 beta0 but as as a convention we always take we always start off with our bias term B B beta which is B plus m x so B is assume beta0 plus beta 1 times x right okay I'll just make sure about this so your M becomes beta 1 and B becomes beta0 okay so we can write beta 0 plus beta 1 x that resolves that is equal equivalent to B plus MX these are the parameters but the meaning stays the same beta 1 is nothing but the slope and beta0 is nothing but your intercept so now there are other names as well so over here over here when you multiply 2x 2 is 2 over here it scales somewhere it scales your X right so 2 over here is a phase of weight given to this x just like that X has a weight we have a feature which is X for example the horsepower horsepower has got in weight what's first gotten beta 1 or we can see even in this case horsepower X has gotten weight M so in this case the horsepower Garden uh weight beta 1 and beta0 is intercept or biased we'll talk about what is this bias term means later on but this is a yintercept okay from where it should intersect at y axis I hope it it uh gives you a good sense about it cool so I just want to talk a bit more about uh these things a bit more on your uh hypothesis uh so your hfx is equals to beta0 plus beta 1 x right and beta beta 1 is a feature weight and X is a nothing but your uh but your uh but but beta1 is nothing but a coefficient of your feature or the feature weight or a parameter beta0 is also intercept we core by convention we have beta0 x 0 plus beta 1 x 1 or we can say beta B times x 0 plus M times X1 okay so what does this mean they both are equivalent but but we are going to use a new notation which is beta0 X 0 plus beta 1 x 1 okay or you can say b x 0 plus m a m x 1 both means the same so I'm going with the beta 0 x 0 plus beta 1 x 1 by conversion x z is not but equals to 1. so whenever you multiply anything with one it just remains that number so beta0 so that's why we write beta0 alone plus beta 1 x 1 okay so we have so we have x 0 to be equals to 1 right so that's why we don't usually write x 0 okay so I hope that you understood will uh it will be more handy when we'll talk about multiple linear regression we will finally understand it but as of now just keep in mind that we have did the bit of rotational change everything Remains the Same just our notational changes introduce beta 1 instead of M and beta0 instead of B all sorry makes sense I hope so that you that that you understood about relationships in detail uh and now you can you can easily ignore uh there's a direct formula but I just want you to ignore it because I don't want you to focus on that uh I hope so that you will not focus on that now what we will do we'll talk about we'll talk word so we have a hypothesis h of x h of x equals to beta 1 times X1 Plus beta0 so this is your hypothesis so what does this beta1 represents what this is beta0 represents in terms of you know predict the price of a house given a size you're given the size of the house so size has beta 1 times the size of the house plus beta0 so what this beta 1 represents or how does this beta 1 affects your output variable if you introduce B beta 1 how does it affect on your output variable and what is beta0 effect on your output variable we'll talk about that later on so everyone we come back to our next section of our video which is talking about uh which is talking about um interpretation enter production of coefficients of coefficients parameters or slope and intercept both all mean the same so usually just just to make a remark just to make a remark over here your f i i sometimes use f of x is equals to V beta0 plus beta1 so function of baby on a builder function f f that that Maps your X to your output variable uh h f x means the same and Y so they both they all mean the same eventually at then you're taking out just an output variable y right this you're taking the dependent variable right so all mean the same value interchangeably just to make sure that you are on the same page so we are going to use this uh this this the particular formula uh eventual equation uh where it's going to take out the Y which is the dependent which is a dependent variable and beta0 is an intercept term and you have a beta one is a coefficient of x or feature weight of X or we can say slow okay and so now so now so now let's start talking about so now let's start talking about about what is the significance of your beta0 and beta 1 in terms of problem statements that that will make much more sense that that will make much more sense so let's get started now so assume that so assume that let's start talking about beta0 so beta0 is nothing but call The Intercept the Y intercept the Y intercept term the Y intercept term or buy a storm by the septum and the biostore so this is what the beta0 is so what this is beta 0 represents what does this beta0 affect the output variable how does it affect the output variable right so beta0 represents the value of y when X is equals to zero when X1 is equals to zero so your beta0 comes into play when represents the value of y beta0 represents the value of y when X is equals to zero so when 0 multiplied with beta 1 that will be nothing but beta0 plus 0 so Y is nothing but equals to beta0 that's the first scenario when beta beta0 really helps is when x is by chance if when X is not there whenever when you don't have an information we just go with the Baseline this is what called This is known as a baseline this is known as Baseline so when X is equal to 0 when X is equals to 0 then when X is equal to 0 then your Y is equals to beta0 so that in other words The Intercept of the regression line with the yaxis okay so which means which means this comes into play when X as X is equals to zero okay and it's also The Intercept it's also The Intercept it's also The Intercept of the regression line on yaxis so it's a y intercept right so if beta0 is positive if beta if beta0 is positive so let me just make it clear so if beta0 is positive is beta0 is positive if beta if beta0 is positive it means that it's it means that the equation line starts above the yaxis above the yaxis which means it's it's above the y axis of over here something like that but but if it is negative then it says below the yaxis so you know uh it's it's like um if if your y if if your uh intercept is positive if your inters a bit positive it will stay some somewhere over here but if it's negative then it will stay somewhere over here right so that's one of the that's one of the property so let's take an example to make sure that we better understand this like uh to what exactly when x equals zero so suppose suppose was going to build a simple linear regression model where y represents the price of a house and X represents the size of the house so you know build a function f that takes in the size of the house and and predicts the price of the house so the Builder version F right uh you know window function f that takes a sign and predicts the price so beta0 beta0 would be the price of the house so so this equation will be modelized beta 0 plus beta1 X1 so beta 0 would be the price of the house would be the price of the house when X is X is equal to zero so beta0 plus beta 1 x 0 which is which is beta zero zero multiplier beta 1 is also of course zero so beta0 would be the price of a particular house when price of particular house when when the size of when the size of the house is zero when the size of the house is zero or nothing okay that's when the beta0 comes in play that's when the beta0 comes at play so let's take a very simple example let's take a very simple example to make you understand one more thing so assume that that that that you want to predict uh the the application is predicting exams course exam scores predicting exam scores so in a build a function f that takes in the number of hours you study number of hours you study now what was the r study and predicts the exam predicts this exam scores of that particular child so that's that's the basic uh that's the basic function and you and you build and you build a fortune we'll talk about how we come up with this mnb later on but to build the function f of x which is nothing but equals to beta0 plus beta 1 x so f of x number of our study which is equals to assume that your beta0 is 60 plus um plus beta 1 is 5 and X so we have just taken random one we are just taking random number for a second for example talk about how can we take out this beta0 beta one later on but just assume that you're taking the random one right so over here over here your your intercept term your intercept term is a number 60 and your and your coefficient is nothing but uh uh uh the five as is as this five so so your beta0 is 60 this means that if if this should if the if this shouldn't have been studied anything if the student haven't started ending which means it goes to 0 60 plus 5 times 0 is another 60 so if storing has not studied anything at all which is a in with that is equals to zero the predicted X exam score would be nothing but 60 would be nothing but 60 which represents the average which which is nothing but the average score which is nothing but average scores of students who do not study who don't don't study so 60 is nothing but the average score of the students who don't sell that it's a bit higher but that's what the beta0 represent beta0 represents the situation when your x0 is nothing but equals to x x 1 is in nothing what equals to zero which means that that in this example the person has not studied at all or in on a previous example the size of the house is zero which means the average of the size average house of the price when size of the house is zero okay I hope that you understood what exactly this beta0 means if this beta0 means and it uh yeah it it makes sense but in case of sometimes video that does not make sense but it acts as a biased term but it acts as a buyer's term the average term uh that's very well this is a bias as well Okay cool so we have studied about beta0 now let's talk about beta1 now now let's talk about beta 1. let's talk about beta 1 in in predicted so beta1 so beta0 represents so when X is X x is zero but beta 1 is the coefficient of x right beta 1 is a coefficient of x so we are multiplying x with something beta 1 that means it represents that how the how this beta 1 will affect this output variable y so it says that it is repeater one represents the change in y for one unit increase in X so if X increases by one unit what will be the change in y that's what the beta 1 represents that's what the beta 1 represents beta 1 represents what the change in X the change in X the change in X for a word sorry the change in y the change in y what how how much y will change if we increase X by one unit let's take a very simple example let's take a very simple example to make you understand about this for example if we have a simple linear regression model where y represents where your particular y represents the number of hours of sleep a person gets okay the number of hours you're going to build a function f you know you want to build you want to build a function f that you're gonna build a function f now in a bit of the Builder function f so let me just clear this so over here we build a linear version vary on a predict the number of hours a person will sleep the number of hours person will sleep uh the number of hours the person given given that then given beta0 plus beta 1 the number of a cop then the number of coffee cups the number of coffee cups like the the problem statement is going to predict the number of hours the person will sleep given how how many number of a coffee cups he has drank okay that's that's the function which you know uh me so if you have a simple linear Vision model where y represents the number of R so y represents the number of R's a particular man's lead we divided this is the number of ask and gets it with the sleep a person gets and X represents your X is nothing but the number of a copy cops a person had right then the beta one represents the beta one how this is how what does beta 1 Test B data 1 represents the change in hours of sleep for 1 cup increase in a coffee consumption so if we increase the coffee consumption by one hour what will be the change in number of hours of sleep of a particular person okay I hope that your understanding over here the number of coffee cups assume that number of how if we increase if we drank one more coffee what will be the effect on this output variable that's what beta 1 says about it that's what beta1 says about it let's take a look at the previous example which which we had so the example was the predicting service exams course we're going to build a function f that takes in the number of study hours the particular person predicts so beta0 plus beta 1X 60 plus 5x so now we have now now we don't have x equals to Z now we don't have that that is worth a beta0 but now assume that assume that um what this is beta1 Tells over here beta was tells that for every additional hour of a study if we increase X by 1 or which is one unit which is every additional hour of study X increases by 1 the predicted score exam score also increases by 5 increases by 5. so this P this 5 represents the rate of change in Y which is the exam score the rate of change in exam score what will the change in exam score given the given there is one unit increase in x one unit one unit increase one additional hour of study so in this case for every additional hour with study which is X increases by 1 the predicted exam score increases by five increase by five this is the rate of change in y change in y with respect to change in x by one unit increase in X with respect to X okay so assume that if a student studied if a student studied three hours so now F of 3 which is nothing about 60 plus 5 times 3 which is 60 plus 5 times 3 which is 15 which is 75 which is 75 so student has studied three units and three three hours extra three hours additional hours so three times five so it will increase by 15 times the Baseline will be increased by 15 times so we can study if a student has studied for three hours the predict exam score will be 75 which is 15 points higher than the Baseline if this one has not studied at all which means that that does the if your X is equals 3 then there's a then there is increase in X okay then then if for every additional hour whatever is additional hour the his or her score increases by 5. you can see clearly that how this uh how this 5 is affecting your output variable how this 5 is affecting your output variable how this 5 is affecting your output variable how this five is affecting your output available oops 5 5 is affecting the output variable out there I hope that it makes sense to you as well okay currently it was positive but sometimes you you have negative as well sometimes you have negative as well okay sometimes you get negative uh scores as well so let's talk about negativity as well uh which will make much more sense and we'll uh I'll give you a sheet where we have much more examples of interpretation that will make much more sense as well right so assume that uh assume that if you have a linear regression model where y represents the number of R study and X represents the age of a student okay so you you want to predict you want to predict the the number for us a particular student will study given the age of a age of students so you know given the age of person to predict the number password student will study you know given like higher dates higher than high higher different study and the coefficient for X is minus two so you have got beta0 plus beta 1 x and x over here is the the age of a person right so if your beta 1 is nothing at minus 2 which means it's different so what does this indicates this means that every one year increase in X so X is nothing but age right so every one year increase in X the predicted number of hours of study decreases by two you can see our misconception was that it is that that higher the age higher the study hours but over here it decreases so minus 2 represents so if your beta is positive this means your If X increases then beta absorbs increases in this case a predicting stress exams course if the study hours increases then the exam also because the beta was positive but in this case if beta is negative my beta 1 is negative which is minus 2 so which means for every additional hour which if X increases sorry if beta 1 sorry if if if if if X increases your output value decreases but in this case if X increases outer values also increases when beta 1 is positive but in this case in beta 1 is negative when X increases output output decreases one such example was that that the the the the in this case beta 1 is 2 which means for every one year increase in age the predicted number of study will decrease by this factor which is minus 2. okay I hope that this makes sense so we have studies we have written over there here as well here as well that if there is a positive relationship there's a positive relationship X increases leads to Y increase as well as well okay so you can see increase in size also also in reason uh price so I hope that this gives a good sense about whatever we are talking about I'll take one more example maybe to make you understand uh in much more detail so that we are on a good page okay so what exactly what I what I want to do is uh take an example of predicting sales you know to take an example of predicting take an example of predicting sales and this is this is a nice example to start talking on so suppose you have a data set that contains information on Advertising expenses and the sales of a company so so you know you know make a function f you know make a function f that that you're going to predict the the advertising expenses of the you know predict the revenue you know predicted a venue the the sales of a person based on the like you know give the advertising expenses like how much you expect how much you spend on advertising and and and predict what and you know predict why which is the sales you know you know establish a relationship between advertising expenses and the sales advertising expenses means the the money you spent on Advertising of of a company okay so you have only one feature X and you're gonna predict to buy and predict why so your so your uh f of x will be nothing but X is because advertising expenses which is equals to beta0 plus beta1 X so assume that your beta0 is nothing but one thousand plus beta 1 which is 0.8 x 0.8 X so what does this mean what does this signify so this signify that your beta0 is thousand beta 0 is thousand what does this mean it is thousand if the company does not spend any money on sales if the company does not spend any money on sales which means X is equals to zero your sales with the the revenue the revenue there's the sales revenue would be one thousand which is the average which is the Baseline when your Baseline of Y which represents the average revenue of company when they do not spend without any advertising okay let's beta0 but what about beta 1 but what about beta 1 beta 1 over here is 0.8 so this says that for every additional dollar we spend for every additional dollar dollar wage we spend which is one unit increase in X will lead to increase uh the total will lead to the sales revenue increase by 0.8 dollars so if increase one dollar on Revenue then 0.8 then we'll or will get a raise of 0.8 dollars in Revenue so this is the rate of change so this is a positive relationship so it means the the the the the advertising uh revenue is in coupled increases uh then the sales also increases okay so assume that your company spends five thousand dollars on Advertising so your if five thousand okay this is equals to beta0 1000 plus 0.8 5000. so this is nothing but your your total answer would be tot your total so your total answer would be five thousand which is five thousand so your total uh your your your answer would be let me just calculate so it will much worse 1000 plus 0.8 times 5 000. 5000 which is nothing but five thousand uh dollar is the five five thousand which would would be the sales so basically if your company has a spend five thousand dollar on Advertising the predicted the predicted sales or the revenue would be five thousand which is four thousand higher with the five thousand or with the final sales if you if if you uh spend five five thousand dollars the the five five thousand advertising which means that there's a four thousand higher four thousand higher than the Baseline Revenue so this is four thousand higher than the Baseline Revenue right so this is a positive positive guy so it says that for every increase in for every in for everyone you increase in the advertisement every one dollar increase in the advertising your Revenue will increase by 0.8 dollars I hope that this makes sense now I'm now I'm not going to talk more in that uh will you be I've given you in enough examples for you to explore by your own uh I hope that this really helped I'll be catching up in the next lecture when we'll talk about how can we estimate our ex how can we estimate our beta0 and beta 1 in 3D let's get started in the next lecture hey everyone welcome back to this lecture in this particular lecture what exactly we are going to cover is uh we'll just recapitulate whatever we have learned and then we'll talk about how can we find beta0 and beta1 in the in like our whole you know discussion is uh if we don't have better beta bet Azure and beta1 we'll be not getting our best line eventually we'll be failing to get the good predictions so that was one of our one of our uh goal so let's recapsulate whatever uh we have done so we haven't our hypothesis function or a prediction function our hypothesis function or prediction function and over here the hypothesis or prediction function states that you have a hfx and a builder function or maybe um FX that that you're in a better version F that takes in the value of x and then and then uh taking value of x and map it to Y and Y can be any any problem for example you want to predict the sales given uh given the advertising expenses or you want to predict the exam scores given the number of hours study so you have input value of only one in one input feature as of now you have only one input feature X you have only one input feature X and given uh and and using this one input feature X sort of predicting what is the uh y right but it may happen that you have several input features X2 X3 all that are out of the X and you have several input features and based on the several input features you will predict the Y but we'll take a look at that several one later on but first of all let's build our Baseline so whenever you work on any type of problem the first you should think about how can we how can we build the Baseline build the Baseline and a working solution then after building a baseline we can extend it okay usually I have seen a lot of people you know initially they started focusing on Advanced things and eventually they they end up losing time money and Etc so what I recommend is start with the very basic or same Baseline and then build upon that so just like this we are doing over here we are currently taking only one feature as of now only one feature and then we are trying to predict and then and then you will see that how will extend it streamlining it very nicely to other features as well two several features several input features as well okay cool so um let's get started so over here which you which which you're seeing over here which is the hypothesis function which is the prediction function you put in the value of x and we get a prediction and you have to learn your beta0 and beta 1. you have to find USB resident beta 1 and then you have your MSC which is which is used to calculate which is used to calculate your uh how well your best fit line is how well your best fit line is which is 1 over 2m or I equals to alternate the m h of x i minus y i where we are taking the difference of residuals and the squaring to to remove the negative sign and then we are averaging it up so that we we get an overall sense and we're adding 2 as a convention or you will see that how this two will help us later on but as of now assume that this is a convention so what's what's our goal is our goal is to minimize a mean squared error which is mean squared or Y divided mean square error because it's we are taking mean of the squares of the error because we are taking squares of the error because these are called errors because these are residual the differences are errors and they're squaring this up and then you're taking them in that's that's where it's called the mean squared error and our end goal is to minimize this mean square error minimize this mean square error in order to find your bl0 and beta 1 because our best fit line depends on this blog beta0 and beta1 so we'll see approach that is commonly finding these so we have to find this beta 0 and beta 1. so what are what are your initial ideas right so let's talk about what are initial ideas I would like to give you some seconds five seconds from now on to think about what what ideas what idea you can come up with what idea you can come up with to actually to actually get this beta0 and beta1 so you can take five seconds from now on to think about what exactly you want to achieve and you can actually take a break as well if you are just feeling overwhelmed but I'm trying to make an explanation so easier so I hope that you have understood now over here in I what is the idea number one the idea number one is to try out several values of beta0 and beta 1 and see if with that particular parameter whether your cost function is decreasing or not so what I'm trying to say that you can try out several first first what you what the idea is that you can try out random say for example you guess beta0 oops what happened to me OMG or give it uh yeah so you can try several values let's let us assume beta0 to E1 and beta1 to be zero you've taken these values and then you plug in the value of over your beta0 beta 1 and then you evalu and then you take in the predictions for every X's X is out here for every predictions and then you've taken out the MSC and then we have taken we have done this please don't uh interfere if you don't please see my past lectures okay so we have we will take out the MSC now we'll take out the MSC so if with the 0 and 1 if our MSC is decreasing first of all if if our MSC is decreasing if our cost is decreasing if our error is decreasing then what we did then then we'll update our values of beta0 from for example assume that the first in the when when you get started you say every to be 0 and 0 you initialize beta 1 and beta 0 to be random values let us assume bit 0 and beta0 will be initialize the zero and beta 1 is 1. so first of all initialize and then you try a different value okay and then you take out the cost function with this m a mean square error the error from this particular uh betas and then what you do you oh you you randomly take another value you just take another value the X 0.25 or beta 1 0 1.75 you take the value and then you check MSC and then you check your MSC if the MSC is less than the previous one then you update the beta value okay then you update the beta value right so you update the beta value if with this with this new value if your Ms dick is is less than the previous one if your error is less than the previous one right and then you update your beta value to be the new one whose error was less than the previous one okay and then after this you have beta0 to be 0.25 beta 1 episode so it's 1.75 now this updated one right where MSC something MSC C okay now you update we now again update 0. maybe 95 and beta1 to be zero Now Beta 0 is 0.95 and beta1 to be 1.95 so you take to you you take these two values and then you check okay now if I check our MSE is less than previous one if yes then we have the then we again update the value then we use the not previously but the current which is 1.95 and beta beta 1 equals one point uh 995 beta 0 is equal to 0.95 okay so we use the updated one so what exactly we are doing we are trying out several values of beta0 and beta 1 and see if with that particular value of the parameter our cost one should decreasing as compute previous one or not if it is then you update and if it is not we just stick to the previous one okay so our first step in this idea what we would initialize your beta0 and beta 1 and we evaluate with these uh how do we evaluate using this cost function where you predict on every samples of x i and then you compare it with the y i and y hat I and then you simply predict it so the cost function will be super duper high right so the cost function will be super duper High because when you have beta0 to be 0 and beta 1 because so it's the cost function will be super super high now you try different values of beta0 and beta 1 like 1.4 and 2.5 irrespectively which is beta04 1.5 beta 1 through 2.4 and see if the cost wasn't decreasing so if the cost function is decreasing as compared to previous one you update the beta value from 0 to these values and keep on doing this until and unless you get low error you get low error right so you have to try out several values of beta and beta B beta 1 beta zero and then until next to get a lower error so now what's the big deal in this the big deal in this is that we can keep on trying so what to do like this is something which you have to think of right so we need a strategy right so we can keep on trying but we should know a strategy to keep on trying we cannot just think about any any value and then just go with it right it will definitely not make sense right so what exactly we want we want to think about in terms of we want a strategy for us or we want an algorithm to try for us this is a good a good good uh this is a good strategy but with an algorithm to try this strategy for us to try this strategy for us in an efficient and timely manner within strategy so what's that strategy the strategies we want we want a system which automatically tries out several values without any human intervention tries out every values every in every iteration so what is iteration in this case you might be confused in this so when your beta 1 is equal to zero and beta0 is equal to zero so you have these betas right this is now when you update now you now you update the value so the first iteration would be now you update the value and then check the Mac then a second iteration you update this the value asked about the previous one and then check MSC is less than if it is then you this is second iteration the third iteration means the third time you're changing the value and seeing if the msc's so that's iteration okay so we want a system which automatically tries out the values every iteration in such a way that your iteration at nth value you iteration at the last parameter value which is the last iteration or something like that would assume that you want to make a 20 iteration so at 20 the iteration is better or maybe this your second iteration at the the the error at the second iteration where you have updated beta 1 and beta0 should be less than the previous one okay it is what is telling the iteration at the nth value where for example third iteration in the in the third iteration the parameters value of the third iteration should be better than the second iteration and how to check betterness by MSC it's better than that before iteration oh so that you got it this is a strategy you want now how can we come up with this strategy this is something you know this this is something which you should be able to uh confront by yourself so how what is the exact strategy for this to occur so the strategy is the strategy is uh the strategy is we can make use of something known as uh let's not go to what exactly I have a very different mind to teach you all so assume that assume that uh we we want to know so what exactly we we want to know so how can we check how can we check we want to check that that that whether by cost function or whether my mean square error is decreasing if I change beta0 us um if if I change the beta 0. so what exactly you wanna know whether whether my cost function cost function is decreasing if I change my what if I change my beta0 or and beta1 so I want to check this right I want to check this so you might have heard about uh let's let's let's start going again a mathematical term so we wanna we wanna check this right so what is the week and we can check this so you might have heard about um slope okay so let's talk about change in y over change in X how much y changes how much y changes when X changes that's that's what is change in whatever change in X so how much y changes when X changes so what if I can write how much J changes how much your cost function changes when you change when you change your beta0 and how much your cost function changes when you change your beta1 so can we get our change the rate of change right so if my cost function decreasing as compared to be as cash come to the previous one this is good we can check the rate of change by nothing but the slope by nothing but the slope uh we which which you have heard now now here comes an interesting part now over here how can we take out the slope of this because we eventually want to take out the slope of this particular uh function how can we take out this a very interesting question so the though let's let's let's worry about first of all tell me what is the if you if you can applaud this function f of x how what is the what what will the plot looks like so I would recommend you to plot this function on a graph right now where you plug in the values of x and x squared plug in the values one one two four three nine plug in the several values and then plot it the plot which you're going to get the plot which you're going to get is a nothing it's nothing but something like this but something like this okay uh parabola okay or something like the uh some the so if not not exactly Parabola but uh you if you plot this on if I I just plotted it on terms of you know a positive one but if you plot this uh but the but the parabola if you if you eventually plot this up if you eventually plot this up so let me let me let me just plot this up since the plotting would look like this which is the function x squared this is a this is uh this is a function this is a graph of your x squared this is a graph of your x squared now this is the graph of this x its weight so can you tell me how can you take out slope in this because at every point every Point uh your slope changes so there is not a constant slope right so assume that you wanna take a slope at this particular point so at this particular Point what does it tell what what does a slope will tell how much uh that how much it will how what what is at this particular point will will tell how much your x square will change when you change this a little bit let's forget it you can though let's say you wanted to take a slope at this particular point you want to take a slope at this particular point because slope pattern is not is not constant right it's not same all over the graph it keeps on changing right it keeps on changing so now now it is not possible so what we do there's a concept of derivatives which takes out slope of a curved line okay of a curved line I hope that this gives a good sense now this Parabola why does I introduce this Parabola to you interesting question right observe carefully observe carefully can you see this this is something assume that this is something a x and then you're squaring this up isn't your cost function will look something like parabola right because we're squaring directly squaring this up right it will look like something like Parabola so when you plot this equation up and you plot this equation up you will get you will get something like this something like this something like this okay you will get something like that now on xaxis you will have your betas on yaxis you have your errors so now again it might can confuse you for every point of the betas for example let let us assume this is a beta0 okay so in beta0 beta0 over here if beta0 is uh if beta0 point is over here so your error at this point where error with this beta0 is very high error with that particular beta0 is very very high if beta0 is over here if beta0 is over here beta0 over over here error is bit low the previous one if beta0 is over here error is bit lower than so this represents this uh the x axis represents the beta the values of beta 1 beta sorry beta0 and Y represents the error y represents the error okay so over here what our goal should be our goal should be to to get the beta to get that beta value to get that beta value over we want to get that beta from here to here so that our error is very very less approximately equals to zero So eventually we're gonna get so assume that this is a beta 1 this is a beta1 and then over when when the beta1 is over let us assume beta1 is something like um four is something like 4 okay beta1 is something like four so what will be the error error will be super duper High beta 1 so now you have beta 1 at a little bit less than or maybe uh something like that so b b beta 1 is in this case if beta 1 is less than the previous one and the error is also less so 80 I'm just taking an example it can be more as well it can be more as well don't worry don't worry about anything currently it's a very simplest example which I can plot for you okay which is now beta1 you try you wanna reach to a global Minima you wanna reach to a global Minima not local Global Minima where your error is very very less where is negligible so you're going to start with the beta value whose error is very very high and then you have to learn the beta value on a chain keep on changing the beta value so you're seeing that we are changing this is definitely beta values aren't in unless our error is nothing but equals to zero okay I hope that you're getting what I'm trying to send me I'm just trying to convey that you're gonna keep on changing beta value you keep on changing the middle value that will help us that will help us to get your beta that that will help you that that that will help you to get beta 1 equals two so beta 1 the error of beta 1 is equals to zero okay I hope that you that that you're getting so how can we take out the rate of change how can we take out the rate of change so how much your error changes so when when your beta beta0 changes how much you're at how much your error changes when betas are at this particular Point changes beta0 will be saying will will be not same it will be changing right so what will the what will the change in error when that at this particular point so what is what is the derivative of your error with respect to this beta0 at this particular point okay so let's uh uh so it will be more clear once we uh understand more in mathematical perspective but eventually what our derivative is telling it is telling that how mature Dairy how mature uh cost functional error will change if you change your beta a little bit okay let me change your beta a little bit so we use the concept of differentiation which is how much y changes when X changes Matlab key how much your error will change when your beta changes so in a nutshell we are looking for this strategy one we are just looking for the strategy where we can take a look if this something increases or decreases we're just looking for that strategy and here we go here we go so here we go where it tests the d y by DX where you're taking out that where where we are when when you take out the start when you you have to do for individually for different different parameters because you have two parameters beta0 and beta 1 right beta beta 01 is slope and one is Pierson but it's not something like it's the best of now but over here it's the stock the optimization problem so you eventually take out the derivative of your cost function with respect to beta0 and derivative of the COS function with respect to beta 1 because we have to do so it tells you it this tells you how much your cost function will change if you change beta0 a little bit how much this cross function will change when you change beta 1 a little bit so if that changes if this it it it will happen in every iteration so the second iteration assume that it changes and it decreases then it then it if and then compares to from the previous one and see if there's something low yes then you update the value of beta0 so there's a change in a when beta0 changes same here as well same here as well okay I hope that you're getting what exactly I'm trying to convey now here's your algorithm here's your algorithm here's your algorithm um for updation like you get a change now you get a change now you have to update no you have to update your values as well and then you look for the another beta value right so you update the this is this is called the update rule this is called the update update rule where you first of all update beta0 with old beta so basically you have a new new beta0 new beta0 and then we'll assign this we this is assignment operator where you have new beta Nu beta is equals to Old beta minus the learning return I'll talk about that times the change in X with respect to the the the change in your cost function how much your day of the how much error changes when beta0 changes we'll expand this will will take out the derivative in just a second but assume that so we multiply the learning rate for this so what is learning rate what is learning rate so but but before that uh we'll we'll talk about that learning rate later on but let's let's stick at some point over here beta0 is is the old beta0 like over here if you have beta 0 to be 0 and beta 1 is equal to 0 then this is the O this now we have the second iteration second side second iteration here beta 1 uh beta uh so you have been new beta and you have to take out the new beta so you have the when you actually convert beta 1 is you assume that you have a zero minus the linear Alpha with respect so basically this is the old old beta has covered the previous one the pre the previous beta okay the old beta minus the learning with Alpha and this change change of rate of change of rate of cost function with respect to this beta0 and then you do for beta 1 as well I'm so sorry this this should be beta1 this should be the one okay this should be beta1 uh now over here over here so now you may have several questions the first question is would be why are we subtracting our old beta the reason why we are subtracting because we're going to minimize our function right one way to think about it it's it's it's it's not something uh uh legal but we can think about that we want to minimize our function right when we want to go down and minimize our function so that our error is less so we're gonna minimize that's why we subtract I mean if anything you need to minimize sort of subtract and then it will tell you it will get this this change this this calculates the change it calculates the exact points using derivatives at the current points okay and of course there's the change and then you were learning rate Alpha which is nothing which is nothing which is nothing which is nothing but the learning rate Alpha so over here over here you are going down at a certain speed right at a certain rate at a certain rate you're going down at a certain rate so the rate of going down the rate of going down the rate of reaching to a global Minima is nothing but called The Learning rate okay it's nothing but called The Learning rate the rate or by which we go in we we learn with the the read by which we learn is also known as learning rate we'll take a look at we'll take a we'll have a separate video on learning rate we'll talk about some of the best cases and cases of learning in great detail but you can assume as of now it's case the back it scales the change by the factor of Alpha and that is the rate of learning in a geometrical perspective I hope your understanding cool let's get started top so the what are the steps to to summarize you initialize your beta0 to band beta 1 equals to zero I assume that U is ratio of beta 1 to any number any random values and we there is a initialization techniques we'll study in deep learning not as of now but you can ignore it you calculate and gradient at the current point with the current current value of course the error will be high you scale it a factor of X when minimize it by subtracting you subtract the old betas here the update rule app happens and they repeat two and three points two and three points right you repeat you calculate you you and in the first iteration you calculate the gradient with the previous now in the second iteration now your betas are updated now it's not the initial but it's the updated one which you which you which which you get after applying this update rule right and then you apply the derivative and then you uh get the and and then you get the new beta and then you go to third iteration and then you now you it now you beta R of the second iteration so it's okay so you repeat the two and three points how much by how much we're going to repeat first of all we can do it for like Beyond a repeat for 20 iteration third iteration for the iteration or space we can we can learn this when you're when you're uh when you're uh when you actually converge when you actually when your beta when your beta is actually when you're uh when you're when your algorithm is converged so what what we say can burst is that your step size is smaller like a step size smaller than the tolerance and then the learning rate stops when it reaches the global Minima okay and you will learn about this you can ignore this as of now we'll have a separate video on learning rate to make you better understand about that okay so and then we do for beta0 and then we do for and then we do for beta 1 as well and then we do for beta 1 as well okay doing the same thing we are updating the beta1 the new beta1 the old beta minus learning refer the change okay and then you repeat until some conditions what is the number of hitters should be 24 or 20 iterations where you I for beta I equals to 0 and 1 we apply the folder for beta0 and there is update with simply simply do for all the betas out there whatever beta 1 and beta0 you do for both one okay now now your question would be how can we calculate the derivative of this So eventually you don't use usually use the derivative you use the partial derivative of jio bi with respect to bi b b i so this this exactly means that the in both means exactly the same but this is a partial derivative which is applied in a vector Calculus if you want to learn more about you know uh calculus and everything if you haven't yet I recommend you to watch this playlist which I created over a year ago which is quite famous playlist uh not by so much of people but eventually it is very nice if you if you wanted to learn about calculus and great detail I've tried after I've considered everything which I know about calculus over here so how can we take out this partial derivative of J of uh how can we take the partial derivative of your cost function with respect to a particular parameter this exactly means the same then how much it cost function changes when you'll be the beta changes okay I hope that this makes sense so you see that they do the same sort of work overall you seem simply same sort of work but this is all the vector calculus so how individual parameter like eventually how individual parameter affects your MSE and over here I have taken out the derivation I have taken out the derivation I have taken out the derivation for your model now you might be confused how this derivation came we'll talk about that for a single training example uh in a free form so I'll just talk about when when will I should do a real world example no then then I'll show you the derivation ignore this as of now this is just for notes purposes I'll show you a worked example HandsOn so that it will it is more clear so now when you when you actually calculate when this will yield to this formula to this formula we'll learn how to utilize this formula in for uh we'll now you plug in the values now over here this this will give you the rate of change okay listen to the rate of change so we'll talk about what is the interpretation of this what why do we use derivative what is the interpretation of this number and various things this is just a starting point my aim was to make you understand that we have an update Rule and then we have this uh give a storyline for what exactly how do we learn and this is the story behind the the learning point okay so you can see that we are having the this is the derivation when you calculate the derivative of the cost function the cost function is which I've seen with respect to particular parameter you get this particular equation okay so we'll put we'll talk we'll talk about this in just just in next section we'll talk when we'll work on work the example please ignore others now but just remember that this is the derived if you don't want to do the calculator stuff you can actually ignore the next section as well but I really really recommend if you want to uh fully understand greatness and you should be able to learn about it okay now I have listed some examples of learning rate so over here which which I've seen learning rate Alpha is a learning rate which decides by how much you should Converse the rate of learning okay so we have learning is 0.1 starts with this and then it slows down and then it converges slowly so if you see that at this point at this point it is very very slow why because it is at as it is learning the end and and it no you can understand this today it knows that the con it knows that it is a my my conversion rate is coming so I have to be very cautious because I am so I have to be a bit slow because when you have a when you drive a car you don't drive at a same speed right you don't have it stays same speed you strive same speed you drive that fast bits whenever required the breaker comes you slow it down right or whenever the stop is going to slow it down that's why the stoppage is slows but if but if your error if you if your learning rate is too high you this is the learning rate which you have to choose according to your problems statement according to the data which will talk about that later on will actually the practical but there's something which you have to tune like try out several and see what works well so if you're learning that it's too high it will mostly diverge so for example if if your car is going on okay with a high speed so if you push a break if you push like 100 kilometer per hour so push a break car is most likely to fall down right well like it it will most likely to fall down it will most like to crash like hell so if the learning result will diverge like this it will never converge to the Minima it will simply keep on diverging you'll it will never reach to a stoppage and if your learning rate is too small then it will never converge okay because it will be two percent super duper so if you drive at one two one meter per se per per kilometer you know it's it's uh I mean I I just mess it up so if you try slowly you'll not able to reach okay cool this these are some of the learning rate variations uh we'll talk about that later on but I hope that you understood what grade is it in upper level now let's go and do the work example and the derivations and all to make you more uh understandable about all of this thing so hey everyone I'm back uh with work example of trading descent learning and we will eventually take a sample data and perform a real time learning on that and then I'll show you how it is super simple to understand query in descent even with mathematical reasoning uh if you're if you're not comfortable even with math I would recommend you to watch it right it will be much more better for you so let's get started um over here I wanna I wanna take a sample data so sample data which I'm which I'm going to take is you have a one input feature well whoops my mind is so diverted I'm using yellow color and white so X and Y and in this X and Y you have 1 2 2 4 3 6 4 8 please note I'm not going to take a huge data otherwise it will take me years but I'm going to take a sample data but I'm going to take a sample data this is a sample data where you have only one independent feature independent independent feature and then you have your output variable output uh variable or Target variable okay you have one independent feature and then you have output variable and Target feature Target variable which predicts in a builder function f in a bill of function f or takes the value of this x is the values of this and Maps it to Y so to build a function f that RFI is any sort of relationship between these two right so basically this for for you it may be super simple right to understand the relationships but eventually I'm going to take an example to show you how our machine learning algorithm will interpret this how our machine algorithm will go on understanding the relationship okay because I am always a Believer or of understanding from the Baseline so that you want to send a core of it you will understand even the big things of it so your f of x is uh when you when you're taking a value of when you put put in the value of x you'll predict y so build a function f so your Builder function f so the equation it's equation so sorry so your equation for this line so equation for this line or the Y equals to MX plus b or Y is equals to Beta is 1 times X Plus beta0 so your point is to take out beta 1 and beta 0 out of this using great in this set so what's that first step the first step of ours is nothing step number one step number one initialize initialize your beta 0 and beta 1. so I'm going to initialize beta 1 to B1 and beta 0 to be zero so this is two two things which I'm going to initialize initially okay because an N thing you have to initialize first then the second step which which you have to do the Second Step which you have to do the Second Step which you have to do is calculate the error is calculate the MSC calculate the Mac with the initialized with the initialized beta or values with initialized beta values and so how how can we take out first of all let's say it's very simple you have your uh Y is equals to Beta what what was the beta 1 beta 1 with 1 times X plus zero this Tesla's equation you plug in the values of every X over here you plug in the value one two three four right so uh the Y bread one so y red one which is equals to nothing but uh one times one plus zero which is nothing but equals to one so you keep keep on doing for every sample so I press two wiper three the prediction which is one times two plus zero equals to 2 1 times 3 plus 0 equals to three and one times four plus zero equals to four okay so now you have to calculate the step now you have to after taking the predictions now what you have to do is calculate MSE is calculate is calculate MSC calculate MSE by nothing but 1 by m I is equals to 1 all the way out of the M as well all the all the way around to the m so what exactly we can do we can simply uh say that one thing is we can also use our formula which which we had of that 1 by 2m which is nothing but 7.5 divided by 2 which is something 3 plus 35 yeah so what exactly we can do what exactly we can do is we can either use 2 over here but as of now let's ignore 2 as of now what do you say let's let's ignore two as of now uh because of uh and just just just for sick of Simplicity more but eventually we'll use this too so maybe let's add 2 over here as well 1 by 2 m but uh but eventually we are we're just going to take the squares nothing else more so let's stick to this only what do you say let's stick to this one it's much more uh better so 1 by M of course we will change it later but my point is not to make it perfect accurate but as of now uh if if you see in your machine machine learning libraries they're consistent enough but let us use Simple it's very simple without using 2R and anything which is one by uh and you have your edge of x i h of x i minus y i it's great for the spirit for removing and the difference is it's great for removing the negative sign so now it will yield to 1 minus 2 squared plus 2 minus 4 squared plus 3 minus 6 squared plus 4 minus 8 squared divided by divided by 4 and that will yield that will yield to nothing at one plus four one plus one plus four plus nine plus sixteen divided by 4 and that will the third P plus 4 which is nothing but 7.5 okay this is your error this is your error if you utilize these two values now uh let's add one more page let's add one one one one more page now once you have the once once you have the MSC now you have to use gradient descent now a step is you have to use use Grid in descent use gradient descent to minimize to minimize your uh your cost function which is G of beta which is G of beta so to use Grid in descent to minimize that cool so let's eventually talk about how can we eventually do it so your cost function is so I'm going to write the conventional cost function to show you what exactly this 2 means in uh how how exactly this two eventually help us so uh this it it will make you more clear so 1 by 2 m your I is equals to 1 all the amount of the m h of x i minus y i squared so this is your cost function so basically what our goal was to take out the partial derivative of this of this because our what was our grain initial formula formula was this formula right so we have to take out this we will take will take out the derivation we'll we'll take out the we'll do the calc will do the calculation of this how how we even do you will see it right now so we are going to take a derivative of J of beta with respect to with respect to beta0 first of all let's let's do for beta 0 okay so what this is split will tell it will tell how much will cost until changes when your beta0 changes so it is nothing it is nothing so what what will be undertake of the derivative of this whole set of functions this whole function so how how does it sell so which is nothing but derivative of of the whole functions we will write that with respect to beta0 so the whole function is one by two m I is equals to one all the one out of the m h of x i minus y i squared okay this is your uh this this is what which shown us all so what I'm going to do I'm going to take out the sum rule uh some sum sum outside so I'm just some some rule outside which is 1 by 2 m then we take out the derivative then we take out the derivative of your of your uh this function which is over here you're going to apply the change audio over here you're going to apply the chain rule on this particular function so X of I minus y i squared okay now you're going to take out the derivative of this how you're going to do it we have a two Function One inner one out Outlet so you will take out the 1 by 2 m 1 by 2m I is equals to 1 all the way out of the M derivative of of this particular pass function that derivative of this this particular over here you was the outer function multiplied this is over here we're going to apply the chain rule over here I'm going to apply the chain rule of calculus so we are going to take out the derivative of outer function leaving as it is inside times the derivative of the in inner function the derivative of outer function is nothing but equals to 2 comes over here by a by the power rule and then we subtract minus 1 from here so 2 uh and we leave the inside as it is as it is for outside derivative and over here 2 minus 1 is nothing but equals to one well times your derivative of your inside function which is beta0 and say and say inside function is nothing but minus y i okay so this is your uh this is your uh equation that that you're trying to solve Now 1 by 2 m 1 by 2 m i is equals to 1 all the way out of the M all all different out of the m and then you simply solve for it which is nothing but your 2 and 2 cuts out so you see that 2 and 2 cuts out so now we will not write two and then your final answer would be 1 by m h of x i h of x i minus y i this is your derivation this is your derivation you might think yayush what happened to this this becomes equals to 1 this this becomes equals to 1. why do we say that this is equals to 1. so let's talk about why exactly this is equals to one so when you take out the partial derivative of this function h of x i h of X I A minus y I with respect to beta0 with respect to beta0 what is that if we expand this h of x i is nothing but is nothing but let us assume the beta0 plus beta 1 x i minus y I with respect to beta0 right so over here we're going to take the with respect to beta0 so except beta0 every except this except this everything is constant everything is constant so that is that that that that that will be nothing but equals to zero so it will everything is constant and that derivative is nothing but equal to so everything is zero okay now now over here now over here now over here now if if you go uh and if you see that everything will be 0 because it is except zero which is nothing but oh we and the derivative of a constant is you know it's it's zero so now now you see now you add then derivative and now this this one because this is respect this this the whole equation will come P equals to 1 y because because when taking the partial derivative with respect to all the letters except the except beta0 are treated as constants and their derivative constant is nothing but zero and then you have beta0 when you take out the derivative of that is something which equals to one and we have one over here and when you actually do that because it's it it does it does not have a constant and even if it as as we know that a we it it has a constant x 0 and x 0 is nothing but equals to 1. right so I hope that did make sense to you as well uh if it not please see my calculus lectures please see my calculus lectures out here it will make much more sense if you see I'm not going to go in depth explanation but you can understand this way that we come up with when we actually take out the partial derivative of jio beta with respect to beta0 is nothing but 1 by m one by m I is equals to 1 all there are the m y hat I minus y i and y hat is the regular model predictions plus okay so this is your a beta0 now we'll do for beta1 and now we'll do for beta1 okay beta1 exactly the same the only thing changes instead of one noun so when you actually uh every Everything Remains the Same so whenever when you actually take out the derivative of beta with respect to beta 1 is nothing but equals to everything Remains the Same everything Remains the Same over here everything Remains the Same 1 over m one over M uh I was to one other the m y hat I minus y I multiply multiplied by x i multiplied by x i y multiplied by x i because now in this case now in this case when you actually take out the derivative of this take out that derivative of this you will notice that now h of x i is supposed to Beta 0 plus beta 1 x i minus y i right so now take the derivative of this everything except this one is constant this one this one everything is constant now over here we actually have beta 1 times x so the constant is the this is the coefficient of the curve x i so this is X x i would be over Dash okay so X I will be written over there please see my lectures uh if you are confused like how this x again this is a rule of calculus and all if you don't know please ignore uh now over here you have you you got this for beta1 as well right now what you need to do we had an update Rule now we have to update beta0 it all beta minus learning that Alpha the derivative of beta with respect to beta0 and then we have beta 1 beta 1 minus learning Alpha with respect to beta y right you you have this you have this so let's calculate this one let's calculate the first one the partial derivative of beta with respect to beta0 is nothing but we have already taken out I've already taken out the equation this was our equation right this this was our equation so first of all our prediction our prediction one minus 2 multiplied by multiplied by uh one minus sorry yeah 1 1 minus 2 V and we are multiplied by one so it's it's we we should not write anything 1 minus two and and then you have 2 minus 4 which is your prediction minus the actual Value Plus 3 minus 6 plus 4 minus 8 and then you divide by the total number of values which is 4 which is nothing but which is nothing but minus 2.5 which is the rate of change okay you do the same for J of beta with respect to beta 1 which is nothing you will get nothing but 1 minus 2 but this time you're multiplying with x i and the first of the X5 which is X1 and this is for X1 right the X X1 is that nothing but equals to one so if you go and Z your X Y is nothing but it's now you now you do for SEC six second one you do for second one which is a 2 minus four usually simply add plus 2 minus 4 multiplied by two plus three minus 6 multiplied by 3 plus 4 minus eight four minus eight multiplied by 4 and then you divide the total by four you get nothing but minus seven point five you get that thing but minus seven point five minus seven point five when you calculate so your uh with the current value of beta0 to be zero and beta 1 to B1 is only about minus two point five and minus 7 which is the rate of change now once we get it now once we get it we can simply apply our update rule our update rule was first of all let's let's do for beta0 our old beta0 which we initialize was 0 minus 30 let us assume that your learning rate is Alpha we'll talk about learning the alpha in great detail uh multi multiplied by learning will also multiply it with the rate of change and the rate of change for beta0 was minus two point five right and the beta 0 the updated bit as beta0 is 0.25 and then your beta1 you do the same you have one in this case because the old beta was one or minus 0.1 multiplied by minus 7.5 you get nothing but 1.75 that's an output okay this will be down this is updated B doesn't updated beta 1 after in the first iteration okay as because it now you what now what you do now this this is now you're done with the baby beta0 and beta 1 in the first iteration in the next it's done in the first iteration you calculate the MSE and the updated beta0 and beta 1. calculate the message you'll be noticing that your Mac is low is low so when when you calculated when you calculated the MSC with uh with this uh with beta0 to B with beta0 to be one and zero it was minus it it was seven seven point five but when you'll calculate your MSE with 0.25 and 1.75 it will be less as compared to previous one please note that you can try it out as well and then you do the next iteration number two maybe an iteration number two you do the same step now what you do you calc you calculate them you see you plug in the values of you plug you up again you again make the predictions you again make the predictions with new beta value so again with make the predictions with the numerator value and then you uh take out the error and after taking out the error you simply apply the update rule after calculating this derivatives once again how this derivative is calculated you have your new predictions right if your new predictions and new predictions you will get once again and then you have to calculate that but once again you're getting new derivatives and then you can and then you have the iteration number two iteration number two as well iteration number three you do the same and all until unless you reach to 20 or whatever number of iterations which you have fixed up okay I hope that you're getting what I mean whatever I'm trying to tell you can try out this priori you may be confused about iteration number two so let me just be clear about it what exactly I want to do what you can do you can first of all you be you you have got new betas right you make your wipe red once again once again now you get your four predictions after getting four prediction you take out MSC with these four predictions you've taken out good now what you do you have you all do already have your uh partial derivative of cos function with respect to beta 1 and beta 2 you have some equation you plug in the values now the now that derivative you'll get is very different why because your predictions will change because you have because you're using new beta value new parameter value your predictions will change of course so beta that derivative also change and then you plug in the derivative over here with now beta0 and beta1 with a different bear beta0 is 0.25 and 0 beta 1 is 1.75 minus 2.5 and beta 1 to be 1.75 right now again you will be getting new betas where your MSC will be low as compared to previous one I hope that this gives you good sense and I really hope that you understood this as well and now I'm just going to uh now we'll ask you to take ring because this was something you know uh very tiring as well for me and it may be tiring as well for you in the next lecture we'll talk about uh why does gradient recent use derivative what is the significance of it and I'll give you a Google sheet where we have we have one more solved problem of it and then we are finally done with learning and then I'll recapsulate you everything make your summary so that you understand it and then you go to other forms of integration to study more about a lot of things so let's catch up in the next lecture hey everyone welcome to this lecture uh so basically what I'm going to do today is talk about some of the key terms which may be uh useful for you and then we'll talk a bit have a bit of discussion around um some of the types of regression algorithm which is a uni varied or simple integration or multivariate and multiple linear equations we'll talk about the other differences and all in great detail okay and then we'll get started with extension form which is quite easy to understand if if you have understood the previous one so first of all let let us understand why does gradient descent use derivative of the cost function so first of all let's figure out that and so any so basically in linear regression so in linear regression what was what is your eventual board what is your eventual goal that's a nice question to start forward so your eventual goal is nothing but to minimize but to minimize your cost function or your MSE so that that is a goal and and and your error between all to minimize the error between your predicted and actual Target value so that we take that partial derivative partial derivative of that cost function with respect to a particular parameter or a coefficient assume that in this case beta0 Okay so particular parameter coefficient and which so what does that derivative gives us is of course that gives us the rate of change but what in terms of how does this help us to go down or how how does it help us to reach to Global Minima that's a very nice question to ask the first it gives the first is the direction to move your data be right whether to increase the beta or whether to increase decrease the beta right so it also gives whether we should increase the beta increase the value from 1 to 12 increase or decrease the beta that's what it gives so direction to move your coefficients or parameters in in order to minimize your error so for example if you have a current zero should we minimize or macro so should we move should we decrease or should we increase this so This exactly the derivative tells and it tells in such a way that it minimizes the error minimizes our MSC okay so the derivative of your mean squared error with respect to a point square with respect to a coefficient or a parameter value or any kind of beta 0 and beta2 sorry beta 1 and beta0 provides the rate of change of the MSE with respect to particular so how much that changes when this changes and M so for example if your derivative if if uh derivative if your derivative is positive if the derivative is positive which is the rate of change is positive then increasing the value of coefficient will decrease the error okay first please note that whatever I'm telling if the derivative is positive with respect to this so it tells that if we increase the value of beta if we increase the value of beta if beta is increased then the error will decrease so if it is positive then we increase the value of beta please note that this is the most famous question when when we talk about the interpretation because this this derivative also gives the direction to movie whether to increase or decrease uh degree of theta in order to minimize the error right so if your derivative is positive then if you increase the beta your error will decrease with means if the if it is positive if the derivative positives it increases your beta okay and if your derivative is negative then increasing the values so for example um yeah so just just just for a note uh over here I think guys I I had a very long interpretation I'm going to retake it I'm going to retake it okay so now the derivative of MSE with respect to a particular coefficient provides the rate of change how much your cost function will change when you change the coefficient so if your derivative is positive if your derivative is positive so whatever the derivative which are taken whatever the derivative derivative which you which which I have taken if this is positive which means that if increase the value of beta so basically derivative what is it gives it gives the direction to move to increase or decrease the data that's that's what it gets right so if you increase if if the derivative is positive the derivative is positive so if we that that means that if we increase the value if we increase the value of beta our error will also increase so we'll have if the so so in this test if increase the value of beta a revelation 3 so so in so over here we decrease it okay but if your derivative is negative if your derivative is a negative which means that if you increase the value of a coefficient if you increase beta by some Factor by 3 is beta and you can increase beta from the previous one it will decrease the error it will decrease the error right so over here we just go with this change okay so basically this derivative gives us the direction to move your beta in if if the derivative is positive that means if we increase the beta our error will also increase but if the derivative is negative it means to increase the beta then the derivative will decrease okay I hope so if the derivative is positive then we decrease the beta so that our error also decreases okay I hope that that this gives you a good sense so by Computing the derivative of these uh of this MSC with respect to every individual parameter we can determine the direction in which each coefficient should be changed like to increase the degrees in order to minimize our error okay I hope so that you got it and now our gradient descent uses this derivative advertis uses this derivative of the of the mean squared error to update the parameters in the direction in the direction to update the parameters in the direction it minimizes with this we will use the graduation tool in the direction that it minimizes the particular error so at every iteration to at every iteration the the gradient of a m the the gradient of mean square error is computed and the coefficients are updated in the opposite direction of the gradient as we have already seen how exactly gradient descent works and and and it keeps on it keeps on you know changing the direction increasing or decreasing keeps happening a positive of the gradient opposite of the gradient and keeps on the come come coming up until the unless we converge to the global minimum so basically the whole idea behind first it is the rate of change derivative is the rate of change second it tells you the direction and second like how big the step is in this case if you see that it is starting in the big big step but eventually it is started small right so if your slope is large if your slope is large we want to take a large step because we are far from the minimum we are farm so if this over here slope is large right so over here it is far from the minimum so so we take the big step but as we go towards the minimum our slope becomes small and that's why we have to take a little little step so the your derivative tells that how big this step we should take I hope that this gives you a very clear sense about what exactly our derivative means and I hope that this gives a good sense about why do we use study where if for the cost function cool now we'll talk about a couple of types of regression problems in great detail I think this is important to talk and then we'll end the end about this section so there are two types of so now in the next in in the next video we'll talk about two types of regression which is univarial submultivated multiple there are a lot of regression we'll talk about that it's one and then you put the uh in you you make a you you make a vector X you make a vector X which is which contains the information vector and then and then and and then you have a X1 X1 x0 X1 and X2 and then you make a and and then you make a parameter vector and then you make a parameter vector and then you make a parameter Vector beta0 beta 1 and beta 2 and then you take the dot product between both of them dot product between both of them dot product between both of them so when you take the dot product between both of them what does it does X zero beta 0 plus x 1 beta 1 x 2 Beta beta2 so we started the linear algebra this heavily reduces the computational time because it follows something when it's broadcasting where it's simply multi multiplies and uh adds at the same time and the calculation the dot product which will perform the way of taking the vector x x 0 x 1 and X2 and then beta 0 beta 1 beta 2 they take the dot product it will be super duper efficient so we have several Frameworks to do this which is numpy framework you know tensorflow which which provides the optimized versions for the vectorized option this is called the vectorized operations that's why linear algebra comes into place okay now how can we write the dot product so if you have the beta if you have the beta which is the parameter Vector which contains the beta0 beta1 all the way around to the beta n number of a bit the number of beta is nothing because the number of features which you have and x x my X Vector contains your X1 out of the uh x 0 all done or the X N then you take the dot product by Beta transpose X this is this is how you can write or you can say beta dot X okay you can write in this way so you basically take the dot product but what will come to dot product we'll come to dot product later on when we solve a real world example but as of now this is how it works now let's get started with more detailing thing okay first is let's prepare our design Matrix let's prepare our design Matrix so this is left repair design design Matrix is nothing but your data how can you convert your data into a matrix a CSV data into Matrix so in this case in this case you have your design Matrix X okay X contains your data okay so assume that you have X1 X2 X3 all the number of features so extra size of the house price the number of bedrooms above the house the number of Earth so it it can have any number of p number of information so here we have written p in the K instead of hence we have p number of information okay and the for and the first data point can be represented as if you have seen The Matrix lecture you could have understood this very easily so the first data point in your the first data point in in your data is X11 which represents that it is the first row and First Column which means is it's the first it is the first it is the uh for it it is available in the first row which of of the first information for example if there's the size of the house this is the first okay now this is the second row first information this is the second data point of the first information which is assume that this is the size of the house the first information of the size of the the house second information of the size of the house so you have same information but different different samples then you have price then you have number of bedrooms so first information of the second second feature which is the number of bedrooms second information about the bedroom so there's several samples for the same right so you have so on over here you have different different features or as a number of columns and then on the number of rows has an N number of a number of rows okay first data point First Column first row First Column second first row second column so First Row Third column all you know the first row P column okay and then at last you have an nth row First Column and a throw second column and a throw Peak column okay so it says that it can have p number of columns or n number for an N number of rows okay so this is no nothing known as design Matrix this is nothing known as design Matrix okay and then you have this Y and Y then the the why why n will be equals to X N okay so the number of rows then the number of elements which you have in your parameter y Vector is equals to the number of rows which you have in a design major because it is a super supervised learning problem and every individual row has its y1 Y2 all the way around to the Y N every has its own label so it it it it may be it not be smaller than it x n is equals to Y and the number of elements in y n should be equals to the number of their data points in x n okay cool now what we can actually take out we can actually take out our y hat i y hat I which is the predict prediction so now you might be thinking that okay every individual parameter has beta0 beta1 all there are the beta P okay p number four now should we are we going to multiply beta 1 with all the variables over here beta beta beta is the two with all the variables and how exactly the hypothesis function how exactly is will this work a very very interesting question so your prediction your prediction why had I why had I was a prediction first of all you have beta0 beta0 by default comes in beta0 times x zero so basically sometimes you know what we do we actually have a n times P plus 1 10 and N times P plus one in this case we add another column of 1 1 1 1 1 1. we'll talk about that why do we add one okay so your you get a predict you you actually have this prediction y hat I which is beta0 plus beta1 x i One beta 2 x i 2 plus beta all that are beta p x i p what does this mean this will you you'll only have to understand if you understand this thing okay so you have your Y and Y how you can get your prediction you can get your prediction which is y hat I which is why hat I you can get your prediction you can get your prediction you you can get your prediction from your model by for example eventually you have to make you you have to make a prediction for calculating how well your equation is right so you actually have a parameter Vector which is beta0 beta one all done or beta P which is the which is of size of p plus one p plus 1 because there's a p number of features plus one which is beta0 okay so you multiply it so basically in this case we also add there of one over here so I'm just going to add to make sure one one one all the way around to be one so only one so this this indicates X zero okay so that beta0 multiple gets multiplied with this we'll talk we'll talk about that uh in a bit so we have this particular and now when you multiply this Matrix with a vector it should lead you to a vector of your predictions so every individual sample have will have it because uh because a row is a data point second row is another data point so basically it's a column columnar difference you know what I'm speaking about is assume that assume that you know make you you have a data like this you have a data like this you have data like this okay data like this and data is mostly you know like this so you have uh so this particular Point has this label Y and now you have to make the prediction for this y hat I so that you can compare the both and then the take out the differences so take out the Y hat for this particular data point and then for this particular data points so for this part because every row is a data point every row is a data point okay what is it that that will eventually help us in taking our this or learning from our data okay hope so that it makes clear we'll talk about this uh this in a bit but before that whatever exactly want to convey to you all is you can actually convert this you can take this X1 you can take this X1 you can take this X1 which is all the uh this this column into into a a row Vector you can actually convert this so X1 transpose which means eventually x x one or S like like click this we can convert like this and then you have X2 transpose X and transpose you take this and convert this into a row Vector take this converts into a row Vector this this this has its own benefits we'll talk about that and your cost must become like this we'll talk about this all of this in retail just will will come come to this please ignore as of now cool so you have X1 x2ox X3 all the way it's it it has some feature we have the x0 which is beta0 and then you get a prediction y using nothing okay categorize as I've already talked about this your p number of samples are generalized to P samples and the parameter Vector p and we have y Vector Y which is the ground truth okay you have a matrix distribution which is nothing but which is also the data Matrix and the design Matrix now based on this now based on the every individual data point will have its own prediction uh will have its own prediction and then we compare with the ground through the actual prediction and then take out the error and then take out the error okay so every data date data point has its own prediction and then and every column has its own B2 which is the beta values okay understood in this case you are seeing that exorcism being ultimately aggregating of the information now let's get started let's get started in how can we actually how can we actually take out the Y hat one y hat two y hat three y hat I all the way around the number of features which we have let's exactly work on that so y hat one which is equals to beta0 which is equals to beta0 which is uh equals to beta0 so where I am going to write it up I don't know yeah in this case see this example uh now over here I think I read I written sorry now you want to make now you know now you want to get a uh predicted prediction you know a prediction vector and then you have your y uh and and then you have a ground truth over here put your prediction vector and first of all you have this one you you made a a column where you have one one filter okay why you'll notice and then a parameter Vector B okay when you multiply this Matrix this is this should be equals to this should be equals to so when you multiply this Matrix with a multiply with this Matrix of the product The Matrix is n times P plus 1 y p number of columns plus one column which we added right now times you have P plus 1 times 1 is B number to columns which is one and will there is only one column so when you when you actually we under take out the take out the multiplication between both of them the dot product between both of them multiplication with the product okay in this case what we'll do you have beta0 as I said that every individual Point has its own prediction okay it should not be like this it should be like this every data point is is is a row okay so beta0 times 1 beta 1 times X One One beta 2 times X12 all the way out of the beta P times x 1 P so basically you take the first row you take the first row of the vector multi take the take the dot product between the first row of the design Matrix or data Matrix with the first element of the vector and multiply with every element of the first uh row of that every element sorry sorry I I think I just mess it up I think I just made up you take the first row of the I just I'm so sorry let's you take the first row of the design Matrix and then you take the vector so basically you have your you you have your row Vector you multiply with the you you you multiply with the column Vector so you take the dot product between the row vector and the column Vector row vector and the column Vector which is nothing you'll come up with One X One One X one two all that are the uh this X 1p so X11 x 1 2 all X1 P all another X and there is one so take this and then you have a column Vector which is beta0 beta 1 beta take the dot product between both of them so dot product will yield to something like this so beta0 This beta0 multiplied with this one beta 1 multiplies this then plus of course beta 3 beta 3 multiplied with the third information beta 4 multiply the fourth information fourth fourth information the I did uh fourth information a particular data point in that fourth information okay I enter I in ith index and then you have got your prediction you just take out this and then go to your prediction now you go to Y2 which is the which is the prediction for the second row okay for this second row so basically you do the set you take the second row you take the second row and then you take the uh and then you take this Vector do the dot product with both of them it is beta 0 times 1 beta 1 times this is the same information this is the same information it is the x21 is it is saying that is the second data point of the first ever of the same this if the size of the how does the second data point okay second data point of the first information so beta1 times x21 beta 2 times X2 beta 3 times 6 2 x 2 3 so you have this b y two all different out of the y n all that are the y n you you do for all the round of y n which is a y uh n number of uh predictions which is beta0 times beta 1 x and one which is the nth data point for the first and for any data point for the first information the beta 2 times x 2 x and 2 which is nth information for the second sorry NF data point for the second information any data point for the third information and nth and for n and a data point for the piece information okay and between you have y i which says that you have beta0 times x times of course one times of course one because everything is one in the end and that first case beta 1 times x i one I three data point for the first information either a point for the second information either a point for the third information and either point for the beat information right and this is nothing but your prediction function for nothing but your simply uh multiple linear regression hypothesis function okay you can write this in a coefficient in a vectorized format which is y hat is equals to X B and this will yield to n times 1 vector I hope that this makes sense okay now over here now once once we have this now you can I actually uh now once we have the multiple you have the hypothesis function uh we'll do a verb example just like we did for linear regression simple line regression we do a simple work example for you to be better comfortable in um okay cool yes so now you have this now you have this now you now what you have to do now you've got your buy now if you compare this y hat I with Y how well your how well your this particular uh equation is so this can be which you have you you have this J of PETA which is your cos function which is nothing but equals to this which is for defined for simple animation but eventually you will notice one thing you will notice one thing you have your vector in this case you have your vector in this case you have Vector in this case you have Vector in this case right of course you can write your vector not a big build you can actually what you can do you can actually you know take out uh something you can take out something like this you can have the Y hat and Y and then subtract it and then you're getting another vector and then you simply add all the elements in that vector and divide by the number num number of terms you can actually do this but this is computationally computationally expensive and computational expensive we avoid in machine learning because it's already two not too much you know um what do you say but you know very every time taking so what we do we have a vectorized as we have the vectorized hypothesis function if someone is vectorized which is computationally good we have convex vectorized cost function as well which is defined by this which is Geo beta which is 1 by 2 em X beta which is your prediction function which is the prediction which is why you had eventually y hat minus y transpose X beta minus y hat well sorry y not this is your prediction and this is a ground truth prediction ground through so you have this Vector this this also use the same Vector this this yields the vector this yields the vector this is the vector this is the vector okay so how do we how do we derived this cost function so let's let's talk about how do we derived that okay so let's let's break it down let's break let's let's not take this let's take a very simple example where you have very where you have a vector X where you have one one and then you have first first information first first data point of the first information so first second item one for the first and if you have only one predictor which is an independent variable and then you multi and as you have only one uh information you have beta 1 and beta0 as as as a simple integration but let's frame this on a multiple things okay so when you when you want to take the prediction it's nothing but X beta so you take the beta0 and beta 1 this this the first row multi take the dot product between these two the the vector which is the parameter vector and the first row uh and then when you take the dot product and then the and then you write the equation over there and then take this this particular vector and then multiply with this Vector sorry row Vector then you dot the dot print between both of them the dot rate between the row vector and this Vector so you'll get the three data points out here now what do you do now you have this now the what will so you have the prediction for you have the pre action from your model now what you have to do as I said that you want to you want to identify what is the differences between both of them right as please note that if you want to know the geometrical perspective of course Solution please see my previous lectures because I've already told this I'm not going to tell again and again so you have now you take out the errors the residuals how how apart from they are so you simply have the prediction function the printed the prediction which which you got X like XB which is equals to nothing but the back Vector minus y hat which which is nothing but what which is nothing but a ground truth okay so when you see this when you see this you will simply have this and now we have to as as as to as to remove the you know the squared sign here sorry the negative sign we Square we Square so this is going to be square so if this is going to be square so every individual parameter is going to be squared so when every individual parameter is going to be scaled now you can see that this is going to stay so you can actually you have X beta minus y beta sorry X beta which is the which is nothing but which will yield the prediction function which Vector minus the ground root transfer so basically let's remove transfers as of now so you have a couple of times x beta minus y so you say simply take the transpose for better multiplication of both of them and eventually you will be getting a very nice so you have noticed a pattern you know using a vectorization it will eventually helping you it is helping you out it is helping you out very very easily it is helping you out so basically you you you're simply multiplying You're simply multiplying this two times the this square of this so basically you have this so you have this transpose for better computation you can see for that if you a field we'll do one sample example just to see how does it work uh so now rather than doing this extensive computation you can simply you know multiply the subtract the vector and then take take out the dot productivity in both of them to actually get the answer okay hope so it makes sense okay yeah the dot for a bit in both of them to actually take out the answer and then you divide by 2m or 2 over here is added because of the green decent convention but you can actually remove these two as well to take out the what average I hope that this gives a good sense about how to how do we came with this uh cross function uh efficient cost function um apart from that I think we covered pretty much very very well about this multiple line regression uh and then now once we have this cost function now you're able to evaluate it now what we'll do now we want to minimize minimize the cost function you'll see that how I do that how I do it using again gradient descent I'll choose I'll showcase you the difference between the partial data as we are using the partial derivative why we are using a partial derivative a very interesting question but I'm just I just wanted to be with the convention I just don't want to use that d d is used for scalar programs but this partial that depth derivative is used for what is used for um Vector formula but but basically uh I think uh I'll just note it down for you for for you all that okay here we are using uh in terms of future notation the the reason why I use that uh partial derivative in that uh when actually we're solving green descent is for so that it so so that it works perfectly for a fusion notation which we'll study right now so it makes you be a bit handy otherwise I could have introduced video so you might be confused so that's why I didn't introduced over there okay so cool I think it's now in the next video we'll talk about how can we optimize our cost function and apart from that I'll show you an analytical analytical solution how can we without any procedure we can just take out our beta's value in matter of no second in in a matter of no time with Pro with calculating and then we'll talk about how can we evaluate uh and then we'll talk about uh some of the hypothesis testing uh which is ttest ftest and a lot of things going to come up and you will be eventually enjoying all of this thing and after that what we'll do we'll talk about something known as uh assumptions of linear regression some of the assumptions and then we'll do another project as well uh you'll be super duper enjoying that as well I if I'm enjoying to teach you all of this so let's catch up in the next lecture London byebye hey everyone welcome back to this lecture uh so basically uh we are at linear regression where we'll be talking about multiple linear regression I already have the notes for you but that's bit less so I just want to elaborate with you uh to you with some exam which may help you to understand much more better so we'll do a we will have a set of worked set of examples for you to better understand all of these things okay so let's get started now so basically uh let's have a let's have an example so that I can show you the vectorized operation and everything so assume that example over here is X1 uh X2 X in X1 you have one two three and four and X2 you have two three one and three and then you have the you have a two independent variables and then you have one dependent variables which is three the four point five four and six so you have the data available you have a data available like this so let me just draw a straight line you have a date you have a data available where these two uh these two variables which is out here these two variables are independent Independence this is no dependent output variable and this is nothing but your white or your dependent variable or your dependent variable okay so this is the data now so now so now you have the so now you can make a design Matrix so you know so now you can make a design Matrix first of all uh first of all we'll we will calculate the we'll have the hypothesis function out there okay so we are now we're gonna train our linear regression model on top of it so how will train first we will uh put put that in form of a matrix which is nothing but um one two two three three one and four eight and then we have one one one I'll show you what is the use case of this one just in some just in some you know uh moments which you will see that why we are adding one more over here so let me just have this in a nice way if x equals to one one one so with the we have the design Matrix we'll talk about this later on we'll talk about this later on and then you have beta and then you have beta so basically every individual feature has its own beta right so beta1 and beta2 beta 1 and beta2 right beta 1 but but we also have something else beta0 right so how beta0 basically when we do the multiplication so basically we take the first row multiply with the uh take the dot predicate in the first row and and the vector B so basically the here is here we have only two for example assume that we haven't taken this particular one one one then we would have only one two and this this would have been left so we have to multiply beta 0 to 1 as well right because beta 0 times x 0 x 0 is equals to one that that's why so you assume that you that you have initialized your beta with uh maybe a zero so but before that you have the beta over there and then you have your ground through ground throat Vector which is three four point five four and six three four point five four and six okay so you so you have this uh ground Truth uh over here now what you do now let's now with the available settings now let's let's do one thing let's form a hypothesis function so you can get your prediction vector by multiplying Your Design Matrix with the with the parameter vector and this will yield to to um this will lead to a vector this will yield to a vector which is equals what what was our design Matrix design Matrix was one one two one two three one three one one four eight and then let's initialize our beta value beta0 beta 1 beta two all the all the way to zero because we have to you have to find the best setup so in the first iteration what you do you just initialize okay I hope that this really makes sense uh to you now once we have that now once we have that what we will do after initializing our beta values after initializing our beta values to zero what we will do we'll simply have this zero zero zero and now we'll take the field we will take the dot review the first row and the column of that the first row The Dropper when both of them will yield nothing but Zero The Dropper between the second row in this Vector will lead to zero this and this is zero and zero so predictions your predictions are zero which is a y hat so once you have the predictions from your model when your betas are all set to zero you'll be having that okay and then and then now we have to compute the cost once you have the model prediction I have to estimate how well how well so when you when you have the H of X in this case is nothing but beta0 plus beta 1 x 1 plus beta 2 x 2. so with B resor is nothing but zero times X1 plus 0 times X2 so how well this this equation or best fit line is of course this is super bad because we have initialized as this is the first iteration so let's compute the cost so when we compute the cost when we compute the cost of it cost function so cos function is computed by this so we have already had a talk on the derivation of our cost function which is nothing but 1 by 2 m X beta minus y Vector transpose X beta minus y okay so this this was our equation so let's let's first of all compute let's first of all compute X beta minus y okay so when you how can a cow can you come come compute this we have model prediction minus the ground truth moral position was 0 minus 3 0 minus 4.5 0 minus 6 and 0 minus uh sorry 4 and 6. okay this was 4 and 6 so you have your model friction minus the ground truth which will yield to a vector which is minus 3 minus 4.5 minus 4 and minus 6 this will be your uh this this this will be your when you calculate this now what we will do we'll take this we have calculated now we'll take the transpose of this so when you take the transpose of the transpose of this which is nothing with equals to minus 3 minus 4.5 minus 4 and minus 6. we have this this particular transpose Vector now we have the transpose Vector now what you have to transverse Vector multiplied that multiplied with this particular Vector then you will have Simply Now you have uh one by e 2 m so here you have here you have four examples m is just one two three four four date four data points one two two by four is equals to eight one by eight times one by eight times minus 3 minus 4.5 minus 4 minus 6 and then you have the non transverse Vector which is minus 3 minus 4.5 minus 4 and minus 6 when you actually calculate this up and you can actually calculate this up which is nothing but which is sorry so this is nothing but um nine plus uh 25 so 9 plus 20.5 plus 16 plus 36 as you multiply the element wise operation and then what do you do and then you simply add it up and that will jio beta will heal to 10.156 so you'll have this um okay so you have seen that how we have taken an account of this hypothesis function after that we have had this taken out the and and we have taken out the cost pension out there I hope that this gives you pretty much a good sense about how exactly we cover up things now this was our vectorized operation out there right so how this vectorization really helps so this is a very important topic to talk on maybe it's very useful as well to talk on like how vectorization question how vectorization how vectorization helps how vectorization helps so if you were to do this normally you know if you if you were to do this without any vectorized operations first of all let us assume you know you don't want to do it with any vectorized operations first of all even with calculation of hypothesis will take too much of time no nested Loops will come in will take too much of time so when you have this particularly your Zen general form which is the word h of a h of X is equals beta 0 plus beta 1 uh X1 plus beta 2 x 2 right so you put in the value of x you would have put in the value of x and then calculate you put in the value of X2 calculator okay and put it now that's for that's our first data point and then you do first second date I'm going to third port so you do for all the four data points okay now after this now after this what you what what you will do you'd have take out the diff that that you have to take out the errors for every for every individual right so that would take a ton of time right so that's why we we over here you have seen that how Simplicity by at the only one time it just calculates our error okay cool so um so what are the what are the operations involved what are the operations involved which you have studied right now so operations the first operation which is involved is compute the compute your hypothesis or compute XB which is nothing your X is nothing but M times n plus 1 Matrix where you have one included for 1 1 column and then you have a beta which is nothing but n plus 1 Matrix and and then you have nothing but n plus 1 Matrix okay and then you have a beta which is nothing but n plus 1 Matrix uh sorry uh n plus 1 times 1 Matrix or the vector where you have betas are also included that's why we we have this plus one okay so compute this so computation takes the the complexity will take uh some order so that's the first and operation is involved second operation which is involved which is the which is taking out the error like this and then what you do and then we take the transpose of this and then we take the transpose of this and then we multiply it okay so this is the second method information and after you multiply it you will be getting your answer so the total time takes it's very less as compared to the you can also take out the runtime complexity you want so now I hope that this gives a good sense about what exactly we trying we're trying to offer now what we'll do now we'll go forward and talk about uh how can we perform a gradient descent how can we perform gradient descent outer okay so that's the first question to tackle so let's go ahead and have this cutting template and let's get started how can we perform gradient descent okay so basically so basically um how can we perform limits right gradient descent amazing question right so how can we perform so you have J of beta your cost function is nothing but 1 by 2 m X beta minus y transpose X beta minus y so this was a cost function right so now this is a COS function as I've studied that you have to take the partial derivative of a COS with the gradient of a COS version so there's a difference between partial derivative and derivative so there's a difference between partial derivative of cos function with respect to a parameter and uh derivative of so previously so basically this we use if we have the scalar program if you have a scalar program just just like we saw in our uh previous simple simple a linear regression but in this case but in this case but in this case um this particular thing we have a vector program which we saw in a multi multiple linear regression but I just introduce you this into the first only so that you'll be comfortable with what exactly both mean the exact same thing but this is for Vector programs this is what Vector programs and this is for scalar programs so basically whenever you have whenever you have betas in Vector you actually use this and then when you have the same scalar you actually use this okay so what you do so what you do you take out the you you take the debt you take the dairy partial derivative of G of beta with respect to a particular beta why we take out and everything we have talked that it gives the rate of change it gives about everything the whole story of geometrical we have already talked about so you take the partial derivative of it so that you can identify the rate of change how much the cost which is changing with a beta is changing so that when you take out the derivatives which actually gives 1 by m x transpose X beta minus y so when you actually take out the derivative I'm not going to talk about the um how can we derive it it's not required maybe for you if you're not a calculator student but we of course have the reading materials for you to get started with it okay so so what is what is the Grid in descent algorithm so once we have the gradient once once we have the gradient calculated out here what you can do we have the algorithm following so go for I for I in for I in range number of iterations you want to perform you wanna perform you want to change the variable so that you so so so that you can converge you take out first of all the hypo if using hypothesis function you take the doctor between NP dot dot X and beta and then once you have the ground truth now what you do you take out the error now you take out the error which is why you had minus y this y hat is nothing but your modification and Y is right and then you have a gradient now you now you take out the derivative of the gradient at that particular point and over here it will sort of vector will do one solved examples to better for you so we take out this so we take out the derivative of the gradient and that particular point which tells us how much how much uh change we are getting in a cost function if we change the beta a little bit which which was nothing but which we have derived this is nothing but 1 by m times the doctor between the dot product between your your views so basically this is the docker between the error this is the error and it's the transpose let's take out the top between the transpose of your X which is this and the error which is this so just simply multiply okay at now once you have the editor once you have once once we have that once you have the gradient we can actually use the update rule so your beta new will contain beta old minus the learning rate Alpha which is nothing we're learning it Alpha multiplied by the gradient multiply with the gradient which is okay right this is the derivative which here another partial derivative constantial with respect to particular uh variable okay so I hope that this the this is the algorithm which is for multiple line regression let's try to do one thing let's try to solve solve a very simple example of using a grid in descent okay so assume that I'm going to take a very simple data set which is X1 X2 and Y which is 1 2 2 3 3 and 4.5 So based on this on a predict the output variable y so your design Matrix will conclude f 1 1 okay because this is important x 0 and then one two and two and three this is a design Matrix this is a design Matrix okay and then you have a output the ground root Vector which is 3 and 4.5 and then you have parameter Vector which is 0 and then you have bit 0 0 so beta0 beta 1 and beta dou beta0 as a proper bias term beta1 is the parameter of X1 and we advise the parameter of term of X2 okay assume that your learning rate Alpha is 0.01 and then now what you do now you compute the gradient so now what to know because but before that what you do but before that what you do you compute the you you run this algorithm the first step in this uh when one running this algorithm you're going to calculate your y hat and the calculator y hat which is nothing but X beta which is nothing but 0 0 okay you calculated now you calculate the error which is nothing over there which is nothing but your 0 minus 3 and 0 minus 4.5 which is nothing but Z minus Three N minus 4.5 which is nothing but this is nothing but the error the the difference now once you have the difference you calculate the gradient at that particular point where you calculate the gradient in that particular point which is nothing but one by two one by two which is over here m is equals to 2 that's why one by two multiplied by your your dot product between your X transpose the design Matrix and the error the doctor between the design Matrix which is the design Matrix over here is one one okay which is one once you attempts the transpose of it will be one one um one two sorry sorry I guess I did a bit wrong over here it should be one two okay so one two and two three okay so one one one two and two three so when you take out the transpose of this particular Matrix you'll get this multiplied by multiplied by oops and multiplied by your uh error error or difference Vector which is this okay so when you calculate the gradient you calculate the gradient you will end up being well you'll end up being with uh this particular variable which is nothing but it'll have c c let us assume that you have variant now what you do you have a gradient so when you actually work on it when you actually work on it so this in the new set of beta so I'm going to come to the gradient here this is a good this one even when you solve for it you'll you'll automatically get it okay you'll automatically get it just when you solve for it okay so you get all the values and you solve for it and then you have the new beta is equals to Old beta minus learnings and grad okay this this particular grad and this is also a vector okay so your new beta will be a Vic Vector which is a three elements which is the old beta is also three zero zero zero okay minus 0.01 multiplied by your vector multiplier by your vector which is the C1 which is C2 and C3 in this case if we exactly want to focus on exact numbers this will be nothing but minus 3.75 minus 6 and minus 9. okay so this is the basically uh the the the C will when calculate will come out which is nothing but your new set of betas new set of betas will be zero zero zero minus 0.01 times minus 3.75 minus 0.01 times minus 6 minus 0.01 multiplied by minus 9 so you have this one and then you actually calculate this 0.0375 0.06 and then 0.09 so this is your new set of vector after first iteration okay and this is the first iteration now now you now after calculating the new betas now betas are updated now you what you do you you you run this once again with the new B test calc with symmetrical gradient and then get the new beta and then do the same thing unless you the number of iterations reached I hope that this gives a good sense about how exactly we work through and um for geometrical perspective you have already talked a lot cool um so I'm just trying to make sure that you have every worked example into your hand so that it's much more better for you to understand it next what we have next we are done with mostly um this we'll talk about a bit about analytical solution of linear regression and then we'll talk about how can we evaluate our model uh specifically and then after that we have a lot of things layer so let me just go there so after the calculation we'll talk about hypothesis testing which is one of the most important thing over here which is the how can we test our model the significance and everything and then we go to the next step which is nothing but um actually talk about assumptions of the model and then we'll do one project and they'll wrap up this course in this video what I what exactly I'm going to do is talk about something else analytical solution of linear regression so basically what exactly we'll talk about the anal what exactly we will talk about in the analytical solution of linear regression is you have direct formula you have direct formula for taking out your beta values by putting your X and y's into it so how this is derived and everything I'll be linking a very nice resources in this particular reading section of this or you or you can see in the resources section for this particular lecture I'll I'll give you how do we derive this formula but it's pretty easy if you get to it you'll have the assignments as well for on it so basically we have the direct formula when we plug in the values of X and Y and then we get our output variable y so sorry uh betas right so we have an optimized beta which is converged because it's the best beta right so there's a direct formula which is which we call analytical solution that so basically uh so basically the formula is X transpose X then we take the inverse multiply with X transpose y so so let's talk about let's let's talk about um and let's try let's talk about by taking one particular example so assume that this is a particular example you have one one one one as X zero and then you have one independent feature and one x0 which is nothing but for beta0 and then you have output and then you have Target variable uh which is Y which is the ground truth now we utilize both of them to actually take out the analytics to take out the beta's values right so basically so the first step is to calculate the X transpose x x transpose X so when you take out the sodium take out the transpose of this so This column this column becomes a row and row becomes the column so basically one one and five seven two zero and then you have the particular Matrix Matrix out here which is two uh two by five Matrix multiplied by five by two that will yield to two by two Matrix so when you multiply this up you will get a two by two Matrix which is nothing but this one okay now the next step what you do is take out the inverse of it so what is the inverse of it or inverse of any particular thing is one by this okay one with determinant of the inner value which is one by determinant and when you take out the determinant of this particular which is nothing but which is nothing but multiplied by this minus multiple minus 54 times 54. so this is determinant this is which is not not nothing but the area under a parallelogram we have already talked about the terminal everything in great detail please see the linear algebra lectures so when you take out a determinant you will have 674 and then what you do you take out the adju gate of that and then and then you take out the aggregate of that so educate of that would be nothing but um this one this particular Matrix and then you multiply with 1 by this with the the as you get Matrix where you will get the your inverse of a particular Matrix now once you have the inverse of a particular Matrix now what you do you compute you compute X transpose Y which is this one you now we compute X transpose I you'll get this particular thing and then finally multiply this particular Matrix this particular Matrix and this particular vector by far getting beta0 and beta 1 and that's the required beta0 and beta 1 which is your regression equation okay I hope that this makes much more sense uh now uh so uh as the as this was very simple example uh for a calculation of this but I actually want you to just don't uh remember how to how exactly those this is something which is uh softwares are made to do it for you so you don't need to worry about this what you need to worry about is uh uh is this particular thing like what exactly this trust tells is here we have the formula which directly gives our beta values okay uh if this if this is not if you if your this meter is not invertible invertible it cannot be inverted then you need to remove the extra features to ensure that your D is smaller than or equals to n so here the number of features is less okay so uh this is a very general General thing which you need to know but uh but let's talk about what is the difference between this this is important this is usually asked in interviews what is the difference between gradient descent and closed form Solutions gradient descent requires multiple iterations okay so you have one iteration second and it requires time and you need to choose the alpha value as well and it extremely works well when you have when you have a large number of features it is not it does not works well when you have a large number of features okay it supports the incremental learning it suppose the incremental learning okay it is a non we can close from which is non iterative it's just plug in the values and then we get up values and then it's no need for Alpha values we don't need that it is slow if n is large if a number of features large it is slow because it will compute all the operations which is linear algeb operation which is quite expensive and then you have this particular thing which is nothing but a you can you compute X transpose x uh which is roughly order of NQ which is extremely extremely time taking so if your if your data is small and is small then I suggest to go close form but if data is you know large then I support incremental learning which is great indecent okay I hope that this this gives a very great sense what exactly is an analytical solution means now what we do now what we'll do we'll talk about how can we evaluate our machine learning model by going on to the next slide so let me just make a template let me just make a template uh over here so toilet paper and over here what I'll do I'll talk about some of the methods how we can eventually uh talk about how can I evaluate our machine learning model in a right way okay so uh so the first uh the first measure which we have already have seen earlier is nothing but known as mean square error so the first measure for evaluating how well your model fits the data which is nothing but mean squared error so what exactly mean a square error is is the average of this great difference between the predicted value and the actual values and emphasizes the impact of larger errors on overall error so basically what exactly this test tells you have the MSC which is nothing but 1 by m some sometimes you write 2 over here as well but let's ignore two ways of now so 1 by m multiplied by I equals to 1 all the way around to the end all day round to the end all the way around to the m you do you you you do for every example for every example for every examples you take out their prediction take out their prediction uh sorry you you you take out more prediction minus y i which is minus your ground truth and square to remove the negative to remove the negative sign once if so to remove the negative error if if there's any right so basically what does it tell what does it do it gives you it gives you it gives you the the the difference is the overall differences the overall difference is the overall the overall differences the overall differences in your model but that's a one note to be taken out here one note to be taken out here you see that we are taking mean you see that you are taking mean so mean is highly affected by outliers so assume that you have a 2 3 2.56 and then 99 you understand this this can easily affect your energy however models performing best value one example is a perform very worse right so over here your MSC will yield to very bad example to to a very misleading you know misleading uh error because it is it is waiting eventually taken a mean and this is something giving us a misleading outcome right this is something which is to give giving out the misleading outcome so basically we say that it emphasizes the impact of larger errors on the overall sense so if you've got the if you if you if if you're getting that a large MSC that means that it is not something like every error is performing it emphasizes it emphasizes what is emphasizes the impact of large errors the impact of large errors like this like this in overall sense in overall in overall sense but I just told about the MSC so that's one of them another one is another one is is about mean absolute error mean absolute error so it's the average of the absolute difference between the predicted and the actual values it measures the average magnitude of errors in a set of predictions without considering the direction so basically so basically what it does tells that you have mean absolute error and what does it tells that the first of all the formula for this is a nothing but 1 by m I is equals to 1 all the way down to the m and take an absolute value of y i minus y i okay and not squared over here please note that we have that you don't need to square so basically what what we're doing and rather than you doing Square to remove the negative sign you're taking the absolute value and so that we get rid of negative terms so basically that means that if we get rid of negative terms if we if we get rid of negative terms if you did get rid of negative terms what does it mean it means that if you don't have if you don't have any direction it it does not if while you take out this then it does not a negative a positive reduction it just says we don't have Direction so if we get the rate of negative Direction which also means that we get rid of the direction without the considering the factor of whether it is positive or negative and let's not focuses on that okay um so uh it does not consider detections but it takes the average of the apps when you take the average you'd be positive positive B positive C take the average okay it is not taking out the differences that consider that is going in other direction it all in the like consistent like it's it's like we're not considering Direct in this case so that's mean absolute error that's one of the way to think about it okay and that and another one which we have is nothing but called root mean squared error which is nothing but called root mean square error so let me let me just write root mean mean squared error or somewhere we call rmse so what does rmse means this is a very important uh thing to talk on rmsc is nothing but a square of your mean square error square root of your mean Square so when you take out the square root of Mac that will give you rmse which is root mean squarer okay that is the square root of that it has the same units as the dependent variable so why does we um why do we take out MSE because the units are same as the depend the target variable that's why it is easier to interpret you know some sometimes we take out the MSE because we want to come back at this format where it was okay you can understand in that way here we squared it no we're going to come back at where it was without considering the negative sign so here it tells you the it it converts back to the units which was previously in okay cool though so these are very small errors which I really want to talk on uh one of the important and most important is nothing but called are r squared error r squared error and in that we have SST which is uh which is total sum of squares and uh residual sum of squares which we'll talk about that today okay Okay cool so let's talk about these both in great details so that um you have a very nice understanding of what exactly you need to do okay cool so um let me just start off with one example so let's say let's consider a very simple example to illustrate the the the the the R square error so assume that you have a data set with one independent feature okay and one dependent feature okay so let's consider your the index number the observation number which is observation number is one so we have X which is the in index which is 1 and Y which is also which is the target which is three okay so two two 5 and then you have three three seven and four four nine so you have the index number the the OBS the observation number the observation number and then you have and then you have the independent feature and the dependent feature so basically on a map of function x to the output variable y okay so now so now you've built a linear regression model on top of it when you built a linear equation or model on top of it uh basically we need to do the hypothesis test we sorry we know we need to predict we need to predict the Y hat right so that we can calculate so that we can so that so let's take an example beta0 is something there 0.1 and beta 1 is also 0.2 then you want to test it right so you have to take you have to say you have to perform the predictions on the X and then compare the prediction y hat with Y right so your y would be nothing but so your your actual y was 3 5 7 and 9 and your model predicted 3.5 5.5 7.5 and 9.5 which is the model which richer model has predicted okay now we have to compute the r squared we have to compute the r squared so the formula for compute we'll talk about the geometrical meaning in just in some minutes but the formula for computing the r is squared the formula for computing the aspect is nothing but 1 minus SSR divided by s s t okay this is the formula for calculating the um what formula for calculating your r squared but let's talk about let's break down what is SSR and what is SST so SST is nothing but it's the total sum of squares so SST is nothing but total sum of squares which is total sum of squares what does it tells SST represents the total variation in the dependent which is the variation in this so let me just highlight this the variation the total variation between the dependent variable is represents the total total variation in the dependent variable uh difference is what what difference is what variation it shows between whom so it shows between the it shows that the total variation how much variation you get into this okay how it shows that by showcasing the difference between the actual value and the mean so your SST your SSD formula is nothing you'd go for ISO soon all around the m y i minus y which is the mean device and and and squared okay to get rid of negative sign so what does it tell it is explaining about your why it is explaining about a Target variable it is where the where we take out the differences between the differences between your uh prediction your your action not a prediction ground explains the variation between the ground and the mean of this so what is the average so assume that average is six so the what is the so three three minus 6 which is minus three right so the variation between your variation between the actual values and the means what is the most common value value in this and what is the What is the who who are in beside of them what is the variation is over there so that's what it tells so basically it is saying the very we have already talked about variance a lot I don't know if this this customer once again so it measures the variability of the target variable without the considering the influence of the independent variables over here are you seeing I'm taking y hat I'm not taking my hat I'm just taking out I'm just taking out the variability of my target there how much my why is spread out how much my why is Target variable which is dependent variable is varying out right it's wearing out without considering our betas wouldn't beta1 because over here this is this is our data right we are not considering anything we're just taking the Y uh which is we are just taking the Y uh which is the mean and then we are just trying to take out the variance we are just trying to take out the variance between uh with with that mean so basically and then we are not doing any sort of if there is no any influence there is no influence no influence of your independent variables we are not used there is no influence of the these independent variables or in literally anything over there when we calculate so so your formula for that was SSD was this where to summarize that you're taking out the differences between your predict your your not predictive the ground truths values the target where the the actual Target values and the mean of that to take out the difference uh so to take out the variation in that particular or the variability in that particular variable okay which is why then you have nothing but something known as SSR is nothing but SSR so which is which is the full form is nothing but residual sum of squares so the formula for this is nothing but SSR is equals to uh I is equals to 1 all the one out of the n y i minus y hat I squared so this is something which if I'm which you're familiar with here we are taking out the taking of the the variability of the dependent variable so basically we're taking out the variability of the dependent variable considering our y hat considering our ads but you can take out the variability of this here we are adjusting out a variability without considering the Y head here we are over here taking over Liberty whether considering the yhat because we are going to definitely taking the residuals through it explains it represents the unexp uh the it so basically higher the variability May higher the variability method more the difference and more the difference back the model because your model could not be able to predict so higher the variability explained in this which means that the more the model is not able to capture which means that more is not to explain again I'll summarize in very simple terms the SSR Is Nothing But whichever studied so I'm just going to reframe in different terms that you're taking out the variability of your dependent variable taking the variability of your dependent variable considering the effect of Y at how you're doing that previously in SST what you're doing you're taking on that variability of Y hat without considering of Y hat just sticking out the mean of that and then just taking out the the way the very with the variation but of yeah you're thinking the variability with the considering so higher the variability means that your three is far away from 3.5 that's what it means you know so if the variability is high that means the errors this how spread they are from the mean so basically the the variability is also which means the way the variability is high the model could not capture it the model could not learn it in a nice way okay so SSR over there is indicates that this is the formula so now once you about know about SSR and SST now you can easily come back to formula and what does it tell let's let's talk about that so your formula tells me your formula tells me so assume that we want to take out of R square so first of all let's let's take out wire y uh which is let's let's put let's take out the r squared for this position number three point three plus five plus seven plus nine divided by four which is number six and then you have s s t you take out the SST which is which you'll get a 20 and SSR which is residual which is nothing but two okay now we can easily compute it now you can easily compute it which is r squared which is nothing but 1 minus 2 by 20 2 by 20 which which will get 1 minus 0.1 which is 0.9 which is when you convert in percentage term which is 90 so the r squared value is 0.9 which means that 90 of the variation in the dependent variable y can you explained by the independent feature X using linear regression so if we are able to explain if if our if if there is a 90 variation in the dependent variable that means that 90 of the answers are able to correctly are your email generation means model is able to give 90 of the correct answers you this is ninety percent this this is your model is able to correctly identify ninety percent of the examples you'll understand in this way ninety percent of examples are correctly identified okay but it's not exactly what I'm saying is something like 90 variation ninety percent variation is explained by your model which means the ninety percent of your variation which is which is perfectly explained so the if if the variation is low if for example it's 20 20 so this is pretty pretty much low so your model is not able to uh exp your model is not a best fit because it is not able to explain your model if your 99 90 by 90 you model your line vision is able to explain the X explain the in the is explained okay I hope that this gives you goods and what exactly this is higher the R square value good the moral is because it's able to explain more variations in the dependent variable y given or explained by the independent variable X so X is able to uh which is your linearity model is able to uh explain your why okay but one thing to be noted over here there is one thing which I really want to write one thing which I really want to write over here over here maybe let's write it out so the r squared which means that the X percentage of X percentage of the variation of the variation in the dependent variable in the dependent variable y can be explained can be explained by X using LR so this is what is said so if the variation is large then then your model is good but if the variation is less than your model is not but it depends on problem to problem as well okay but but over here see over here that you have X so what is it tells then there's if there's 90 percent of the video for in this example the 90 the 90 of the variation in this particular Y in ninety percent of the variation ninety percent of the variation in this particular Y is explained by X is explained by x x seems to be useful feature is explained by X using our linear regression okay so if 90 of variation is explained you can understand your your you are able to your you've given the information you're able to correctly under 90 of them okay so this is this is one way to understand it but yeah this is a very messy thing to help you understand r squared we'll have one practical example to have a one one more lecture on square and there is something whereas adjusted R square we'll talk about that buckets and the end of this lecture I really hope that you really enjoyed this and now we are coming to the ends of our lectures specifically cool so uh we'll talk about uh hypothesis testing which is one of our last phases and then we'll talk about assumptions uh which is one of the my favorite Topic in the world do project and then we'll end up this lecture hey everyone welcome to this video is uh in this lecture what exactly I'm going to do I'm going to talk about one of the second last topic of whole regression analysis course is about hypothesis testing and hypothesis testing is one of the most you know ignored as well as one of the most asked questions um that the topics from which the interviewers asks integration analysis and we'll talk about this in pretty much very detail with several examples as well like how do we use hypothesis testing a linear regression we'll have the Practical implementation of it as well in various terms and uh and then after and then after uh working on the hypothesis testing we'll go forward and talk about uh I will go and forward and talk about something called as some something known as assumptions of linear regression where we'll talk about something but that's for later purposes currently our aim should be finished this hypothesis testing in very detail now one thing which I recommend over here is sometimes you will see formulas I would suggest you to completely ignore those formulas okay um sometimes you know there is a some some formulas will come up the main idea from which which you have to take up is how you can understand the concept the core of it formulas can be seen later on as well so how can you understand the core of it by understanding the Crux by several examples which I'll try to take Okay cool so now what so now what we will do we'll try to start off with this but before that I just want to clarify some of the things that uh as as I say formulas will come please ignore if you if you're not able to understand it and at any point if you're not able to understand I suggest go to a Discord server go to a Discord server and ask directly by giving the time stamp you should tag the help or create a ticket directly out there and then ask the question uh in a machine learning section about uh any such questions if you have so let's get started eventually uh with this with this um basically uh you you have you must have studied about hypothesis testing in statistical and probability lectures uh if you haven't please I consider to watch a little bit of lectures around hypothesis testing because I assume that you know a little bit about hypothesis is testing however I'll I'll I'll also describe over here if you don't know but uh I if if you're not comfortable with Statistics and probability I would suggest the highly suggest to go and watch our stats and prop lecture stats and prop lecture which is one of the most important you know uh lectures but it is available on our course which is cs01 or you can say machine learning course machine learning course by Anton you will be able to see everything out there but let's get started with actually this this lecture so basically what is hypothesis testing hypothesis means some sort of statements or to be made some thought some sort of statements which is not yet means this is the hypothesis which we want to test we see now this is a hypothesis which you want to test so the formal definition if we go to hippo this is the formal definition if you go to a hypothesis so hypothesis states that the the hypothes states that is what that the statement or the explanation like for a statement is made based on limited evidence or maybe small amount of evidence for example for a for example one or one of the example that that one of the example can be if you stay up late then you feel tired the next day that's that's one of her hypothesis that's one of our hypothesis right um so these are some of the hypothesis so how can we test that hypo how can we validate that hypothesis is valid or not how can we validate how can we valid the current example which you have taken the current exam what example which you have taken the example which you have taken to see we say we we are saying that if you stay up late then you feel tired the next day that's one of the example so how can he add if I stay up late if you stay up late how you'll find like how can we validate it so hypothesis testing deals with validating your hypothesis yes um using several tests you know you conduct tests for identifying that but but never mind let's actually talk about uh let's keep talking about hypothesis testing so it is a statistical method in terms of statistical maths it's just a statistical method which helps to make decisions about the data eventually for example that statement what was that if you stay up late if you stay up late you feel tired if you you you feel tired okay um that's one of them another one is let's say let's say for the sake of the example another hypothesis can be daily exposure to Sun leads to increased level of Happiness that's one of the hypothesis so previous hypothesis was that the previous hypothesis was if you stay up late if you stay late if you stay late you feel tired you feel tired you feel tired second example of a hypothesis exposure to sun exposure to Sun what does it leads it leads to increased level of Happiness increased level of Happiness increased level of Happiness so this is one of them so these these are some examples so basically hypothesis testing what does it do what does it do it validates it validates whether this statement is true or not based on the data which takes the decision whether this statement is true or not based on the available data based on the variable evidences okay so there are eventually two types of hypothesis which we generally know something else a null hypothesis and alternative hypothesis or or you can say H1 so s0 and H1 so what does it mean like null uh or an alternative and blah blah blah so let's talk about uh both of them in great detail so that you you you could understand in pretty simple language um so let's take an example that he that uh that that your hypothesis is that that your hypothesis what is your hypothesis which you're not validate you're gonna validate whether whether you're let's take in a pharmaceutical pharmaceutical company uh invented a new drug of weight loss drug okay so they they invented a weight loss drug they invented a weight loss drug and they want to test they want to test this hypothesis that the weight weight loss is effective okay and if the hypothesis is true then we say that if this this is the hypothesis what is hypothesis weight loss drug is effective okay so we're gonna test this happens whether this weight loss drug the company invented with the new weight loss which they have invented is it effective or not so this is a hypothesis which we want to test which we're gonna test okay so there are two types of hypothesis the first one is no line second one is alternative so what is the null hypothesis in this particular example the null hypothesis in the in this particular example the new drug the new weight drug which is which is which is published or which is evented has no effect on decreasing your weight okay so we say that so we say that your null hypothesis the in the in the population this is the population mean which is that that indicates in the pop in the pop with the drug effect on the population is zero means the drug effect on a lot of people is zero okay and that's null hypothesis what exactly no hypothesis States it says that a drug has no effect but an alternative hypothesis what is it states it states that your drug has an effect your drug has an effect on decreasing the weight loss which means that a population the the in in the population this is working which is not equals to zero what does it mean ecosystem and what does it mean to mean the population mean equals to zero which means that it has no effect population which is not 0 which means that it has some effect okay because if the mean is zero the on the number of people for example this this is going to assume the number of people which for which this drug if affected is zero which means that the drug does not affect but over here it is so the drug eventually affected the population okay so how can we value how can we test it so basically so basically we want to test we want to test whether whether whether we will consider null hypothesis or alternate and uh a particular example this particular can be either classified in null or hypothesis or alternate hypothesis so in a context we want to conduct a test we're going to conduct a test we're going to conduct a test to identify whether it is null hypothesis which means the drug has no effect and an alternate hypothesis which means the drug has an effect okay so this is two of them which one to go forward so we have several tests in our statistical tools to conduct whether whether the drug has effect or not some of them are ttest ftest and there are several other as well so we'll study about ttest in real detail F test will be given to you as in homework uh to actually work on your by yourself because we want you to be not even spin food like this we just want you to be export and everything by yourself as well so you now now you're at a journey where you have to explore things by your own right if you have came over game over here this means that you have achieved a lot in your career as of now in terms of learning okay so now so now as you so of course when you conduct a test it will give some sort of number assume that c right so in in what and then what we do we call for example you've got C so so a week so we know you're going to classify you know you're going to classify whether whether this is a null or hypothesis so if if that I assume that you conducted a test as in though don't worry we'll talk about Easter in the great detail you conducted a test and then you got some number c okay you got some number c so we check we check with the significance we three with a threshold and we say that if your if your if your test value is is smaller than is smaller than is it smaller than this particular 0.00 which is five percent which is five percent smaller than five percent then what we do we reset the null hypothesis and then we accept the alternate hypothesis which means the drug then we re then the drug has an effect so if if your if your TV if your value from the test which you conducted is smaller than the significance value which is 0.05 this is a standard threshold which we keep foreign test will give you some number and then you and then you compare with it if if the test value is smaller than this then we say that the drug has an effect why we'll talk about that later on great detail as well about this don't worry okay and then if if it is if if it is greater if it is great out your T value is greater than this then we say that it is a alternate hypothesis sorry if we resect the alternate hypothesis and then accept so basically anything a widget anything above the five percent and your your validity value or the test value is greater than five percent then we say that the drug has no effect and then we say the drug has no effect it is smaller than five five percent the drug has an effect okay and that that was one of one of the example let's talk about in great detail this this was just an example to Showcase you the hypothesis testing but how we use the hypothesis testing in linear regression that's what that's one of the most famous questions to talk on um I just I just was uh getting a Red Bull I'm so sorry for that okay so uh how can we think about in terms of linear regression and this is something which everyone needs to know about this this is something which every everyone needs to eventually know about so let's get started with that as well yes cool so how can we use that in uh linear regression in linear regression so basically so basically um I'll just go through a past example so that you could be understandable in the easy way I rather prefer all of this thing okay so this is this is one of them this is one of them where we talk about F tests Yeah so basically how does we formulate our hypothesis in terms of linear regression so in a case of linear regression the claim is made that there is an a relationship so what is an hypothesis hypothesis is that there is there's exist a relationship between a response and a predictive variables now you might be thinking why do we need hypothesis testing why do we need hypothesis testing so basically how and how does it helps and how does it helps now this all of this might be in your question but first of all I I understand that your question is so let's go ahead and talk about why exactly we need hypothesis testing so assume that assume that um you you want to build you want to build you want to build a function f that takes in the number of hours of study number of hours and the attendance per percentage of a particular child and then predict what what will the exams go so so it takes the X1 and X2 and predicts Y which is the exam score okay so it takes X1 and X2 and predicts Y and this is a nice uh this is the function which you have to build this is the function which you have to build up okay so you have um now now now it may happen now it may happen now what what do you want to do you want to interpret right is this significant to outward variable is this information is this particular independent X1 is significant what does that mean significant significant means important okay like you can understand is is it significant is it even reliable for this so basically what we do and the first hypothesis formulation is is there exist any relationship between both of them yes or no how can we identify a relationship between both of them how can we identify is there any relationship between both of them uh the input variables and router variables by calculating the significance okay how much this value X1 is significant to this output variable y okay because this is important in terms of the the reason why it is important the reason why it it is important see example X1 and X2 and you're getting the Y so so basically you identify the relation between both of them so if there is no relationship then your machine learning model will be not able to perform well so you can tell to stakeholders that there is no relationship it with the in into the number of hours a particular student study that that whatever number of hours to do in a particular study it will not affect exams course exams course will be the According to some of the factors okay so this can be one of the possible factors so if we should first of all uh assure ourselves that there is a relationship okay this is this is one of them so there is a relation how can we identify there's a relationship between input variables and output variables we can identify the relationship between the input variables and output variable by identifying the significant say the significance of our input variables to outward variables okay so we can say that okay okay so this is uh is this significant to our output variable yes or no yes or no is this significant to our output variable yes or no X1 is significant so if a is is so question is is your is your number for our study is significant to your exams course so this is the this is the hypothesis formulation hypothesis formulation what does it says that there is a relationship there is a relationship there is a relationship between input and outer variable between between input and output variables or independent features and dependent features okay and then we go to the X2 and then say okay is this something related to is this something is the X2 which is the attendance is also is is also significant talk about yes or no okay so this is how we this is this this is why important because we want to distribute into identify is this very even relationship and assume that X2 does not have a significant so we can drop this column we can drop this feature if you drop this feature it will definitely help us in getting the robust model because this is just unnecessarily over here because it's not contributing to your output variable does not have any relationship with output variable understanding what I'm trying to say I'm just trying to convince you just I'm trying to convince you that there is a relationship there is a relationship there is a relationship if there is a relationship that's that's the build of a hypothesis formulation we'll talk about all of this in great detail but you can understand this exists a relationship between the Institute between the response which is the Y and the predictor so if that's the hypothesis formulation okay so what the the formulation of hypothesis times says that if the claim is represented using the nonzero then then the alternative have this is the uh this is Alter alternate hypothesis and if your beta value are zero then this is a null hypothesis I know you didn't understood anything let's talk about uh what is what exactly that space statement says so what will be our null hypothesis in this claim and what will be our alternate hypothesis null hypothesis null hypothesis means that there are no relationship between whom between your independent X1 X2 and um y there's a row no relationship between X1 and X to Y and what is the alternate hypothesis alternate means completely opposite okay that there is a relationship there is a relationship between X1 and X2 and Y okay so in so we now now how can we how can we say that how can we even make it more mathematical so null hypothesis we can can we say that null hypothesis where your beta1 and beta 2 are equal to 0. or we can say all your betas are equal to zero or your all your beta 1 and beta 2 are are equal to zero null hypothesis means so if so for example let you I know it's it's very simple to understand the concept please apply your mind now over here that you have your hypothesis which is beta0 or um beta 1 beta X1 plus beta 2 x 2 okay you're understanding so if we say that if we say that that if if if there is if there is no effect if there is no effect of this an output variable which means that this is equal to zero this is equal to zero all right bro I'm just trying to say that if if this is equal to 0 that means that will do yield to 0 and that will it does not that will not affect your output variable but whatever eventual null hypothesis is saying that your X1 X does not have any significance right so where beta 0 is equal to zero that means that X1 does not have any significance if your beta 2 is not is equal to 0 that means X2 is also does not have any significance in output variable okay so here all betas will be equal to zero a null hypothesis all betas are equal to zero in a null hypothesis an alternate hypothesis says that alternate hypothesis which means that all the betas are not equal to zero are not equal to zero that's your hypo this is your hypothesis now you might be thinking a bit of off key how this is how this is coming again I'll summarize it for you that your h of X is nothing but equal to beta0 plus beta 1 x 1 and beta 2 x 2. so so null hypothesis what does it say is that there's no relationship if there's no relationship that beta will be down if we make beta 1 to beat a beta 2 to b equals zero that means it will hold yield to zero and this will hold you to zero which means it's not affecting any output variable right but what if if there's if if the if there's something out of zero that is alternative which means that that it is affecting some sort of outer it is affecting some sort of output a variable so a null hypothesis we say that all R beta value are nothing but zero but alternate all our value is nothing but not equal to zero this is our hypothesis for emulation in linear regression I hope that this gives you a very good understanding about what exactly we aim to start solving about uh this okay so so now you can see that if everything is equals to zero if everything is equal to 0 that means this is a null hypothesis which means that does not have any output that does not have any consequence or output variable but if there's not even result that means they have a consequences on output variables so there are three types of tests we should uh talk on uh which is test for significance of regression is it a regression problem or not maybe it can be easily classified when we'll talk about linearity assumption it can be easier talked about a linearity assumption which will see that later on one is test ttest which checks the significance of individual to coefficient regression coefficient so what is it tells so what does it tells I think I did a little bit wrong yeah yeah so it what is the T Test us T Test as I said that we have a hypothesis where our s0 where what does null null hypothesis indicates there are beta values nothing but equals zero and an alternate hypothesis what does it indicates either beta value is not equal to zero is not equals to zero this is our formal formulation apart is testing which on a test original test so in a test we want to test whether there's exist relationship or not so we have a test right we conduct a test and then it gets at some certain value let's assume C so if that c is a smaller than 0.05 it's more than five percent we'll talk about that this this again significance just in a second but if this model then then we say that we say that we reject the null hypothesis that means the there's a no release except that there is a relationship but if it is greater than 0.05 that means that we reject the alternative with that there is no relationship between the variables okay reject this in order and then we accept this uh we'll talk about the significance just in a second so this test check the significance of individual regression coefficients individually okay not something like over it's it checks that whether it be whether this X1 is is six is significant to Output variable or not or whether it's X2 is significant to Output variable not how does it checks that if your beta 2 is equal to zero so your your hypothesis will be a zero where beta 2 is equals to zero NH hypothesis with beta 2 is is not equal to zero so this is a hypothesis it checks individually it checks individually and once it checks individually we can easily take out the overall overall significance okay so so so if we take if we take out individual uh significance then we can easily is okay this X1 is not in the not significant then we can remove this X1 how how can if the beta 1 of X1 is a zero that that means that it is not significance and then we have something known as ttest and then we okay uh sorry F test and F test is used for taking out the overall significance the overall significance overall significance of your regression barriers overall which means whether this overall equation is significant or not okay significant means overall so the hypothesis formulation would be the null hypothesis would be beta of all the way devices and an alternative which is all the video value is not not equals to so this is the hypothesis form formulation three test let's talk about the first test ttest def test will be left left upon you to explore I hope that I'm giving you a very great knowledge is out of it and I really hope that you'll utilize this information because for me it takes a ton of time you know to develop this and I hope that you will uh that you will that that you're enjoying this course out till here and if your till here that means that's something you're a special guy you know uh that's you're a special guy and you're always up for very big challenges out there okay cool so now now I've been talking about something known as significance value so we'll conduct the ttest so how can we how can we conduct the ttest so there are some steps for conduct for conduction of a ttest I'm directly going to the you know ttest uh formulas so that it is my it it will make much more sense to you only when we'll do one example okay so let's go to uh one T Test example yes so you're a so your first so you're basically again I'll just write the ttest example so that it makes much more sense yeah so your so your hypothesis formulation is hypothesis for emulation is so assume that let's take an example because I really like to take examples in every bit whatever I'm explaining because that something is makes me motivated to like your and you're able to understand by the simple examples so example what this example says example says that you want to build a simple linear regression model we build a simple linear regression model build a simple linear regression model and what is that same simple integration more model it will build the function f build a function f that takes in the the text and the square footage the the the the the square footage or or or you can see the size of the house size of the house and predicts your price and predicts your price so given the x value is going to predict y I'm just taking a simple linear equation to make you understand so in this case what zero okay and Alternatives means that your beta1 which means the beta1 which is the beta 1 is the say is the coefficient of this X1 right beta1 is not equal 0 which means that there is a this this means there is there is no relationship no relationship relationship between X1 and Y which means that no religion resizes the house and price the size of that does not affect a price and this there is this alternate means there is a relationship between X and Y that means that that means that there's the release the size of the house definitely affects this price okay so when you fit when you fetch the linearization model and you fit the linear version model you have the following equation now what you do you calc you do the test now you do the T Test how will do will talk about just in some sentence but the formula for a calculate for the formula for doing the for the formula for doing for for for doing this test is we as we do individually for every parameters right if we do the individually for every parameters so the T we calculate the T statistic the tst statistic for whom these statistical for the size of the house because it does individually assume that it assumed so we'll take the take out the T for every individual parameter So currently I'm going for only one so it takes a t statistic for the size of the house which is estimated coefficient estimated coefficient the coefficient value so in this case it will be beta 1 minus zero beta 1 minus 0 divided by the standard error of that beta 1. a very nice equation of very confusing to a lot of people now let's talk about why does it confusing and let's talk about each and every Integrity of it okay each and every Integrity of it so uh we'll talk about why are we subtracting it why are we dividing with sound error what is a standard error and yeah so and and and and after conduct after conduction of this test what we do so that it makes much more uh sense to you in your initial stages as well okay so let me just uh let me just open my uh from a calculator because this is something which I'll be required which will be required so let's first of all talk about let's first of all talk about the general formula so the general formula for calculating the t t value to statistic for a particular beta for a particular beta is nothing but estimated coefficient that is coefficient which is estimated because we have to take every this happens stress testing only can be tested if you if you have a fully trained model so that you can test your hypothesis okay estimate coefficient minus 0 divided by standard error of that coefficient standard error of that coefficient so what does it measures what does it measures so what we do what we are doing over here so basically we're calculating the T statistic and then we are subtracting the hypothesized hypothesized value from the official from the estimated coefficient so so you might be thinking what is hypothesized value you this is something which is uh confusing to most of the people so hypothesize the value is the value of a coefficient under the null hypothesis so we we first of all assume okay in uh basically what basically what we do we hypothesize something we make a we make something like true like there's a no ratio we prior to assume that there is no relationship okay and then we disprove that and then we disprove that so hypothesized value is zero in this case because your hypothesized value is zero okay so estimated minus the hypothesized value which means the value of the coefficient under the null hypothesis okay this is the hypothesized value which is if the beta 1 is equal to five that that means I'll write a five over here already assume that there's no relationship now we disprove that uh with with with t statistics so if you're not even disprove that that means there is no relationship but if you're able to disprove then we accept the alternative hypothesis okay so over here this actually usually that there's no relationship so basically we subtract with the hypothesized value and then we divide by the standard error so what does a standard error means what does standard error means so let's just go and talk about uh let's just go and talk about a very nice explanation of standard error because this is something also also people you know uh worry a lot about so standard error is nothing is nothing but um uh it's it's nothing but accuracy the accuracy of an estimated value the accuracy or how much uncertain or how much uncertain your regression coefficients are so I'll take a very simple example to make you understand about this um so basically assume that uh so assume that you built a linear regression model and the standard error is the measure of uncertainty of the estimated coefficients and then describe the ratio basically uncertain uncertain uncertainty means the student standard error so I'll just write it over here so standard error it measures the uncertainty uncertainty uncertainty or the accuracy accuracy or preciseness or the preciseness of the estimated coefficients of the estimated coefficients this is a very important uh to talk on that that the standard is nothing but the accuracy uncertainty preciseness so that lower the uncertainty means better your coefficiency so basically it evaluates basically it evaluates how your coefficients are uncertain how how your estimated coefficients are accurate how your uh how your uh how your estimated coefficients are precised and all so these are certain uh certain words to talk about so basically so basically you calculate so basically you calculate and say that okay this is something is used this is something it is used to indicate how precise or or precise or accurate your estimated value are okay so again you might be your might be thinking so you might have beta 1 equals to two beta is equal to 2 by estimated by your gradient descent okay by estimated by your gradient recent that's one of them so when you estimate a beta1 so how precise how much how much precise this is how much precise is how much accurate this is is how much earns how much uncertainty it contains how much unsureness how much it we are sure about this we quantify that so we quantify how much we are sure about that we quantify how much we are accurate about a particular beta value that's what the standard error means standard error means how much we are sure about that particular beta value okay how much you're sure about that I hope that it that I hope that it gives you so if your standard error is high if your standard error if it's time if you if your standard error is high that indicates that there is more runs under uncertainty more uncertainty a less accuracy less preciseness okay so if your uncertainty is high which means not sure about that that means that is bad no if you're not sure about if you're not too much sure about the particular coefficient that means that that that means that there's something wrong with it if it's super accurate with that and you're also not accurate so if your standard is small that means that your alternate is low which means you are too sure but you are able to quantify things you are able to quantify how much sure and unsure you are so in this case if you are sure if your unsureness is too low which means that you're sure of that beta values to that that means that is uh that is good so your assignable error should be less so basically what we are doing so basically what what we are doing we are simply subtracting we are simply subtracting sorry we are simply first of all the now now you understand this full equation you have you calculate the T value calculate the T value which is beta 1 minus 0 divided by the standard error so now what does the division gives you what does the division gives you this is something which is usually taught in the statistics class classes and I usually recommend you to not even uh which is the which is the which is not even recommended for you even if you if you if you're just curious you can just go ahead and talk about things uh but what exactly what I'm gonna do is talk about my interpretation from what I under understood because that's something you know I I also have to take care whether whether I understood it nicely or not right um so uh let's talk about that uh otherwise it will be bit off to me as well so the T value which you're going to get over here the pvalue which is going to get over here which will give you which will give you a number so that we can compare with the threshold we'll talk about to the threshold just in some second just just after this topic so the T this is called the T statistic this is called the T statistic is that is what is the what is this test it helps us to assess the significance of the coefficient relative to the variability or the the estimate or the uncertainty of our estimates so what does it tell again I'm going to repeat it tells you please remember the significance it tells you I'm just taking in same very I'm not taking very mathy you know if you if you go online no everybody is taking so mathematic things I'm not able to even understand why they are taking mathematic things but it tells you the significance it tells you the significance of a coefficient of a coefficient relative relative to the uncertainty uncertainty uncertainty of the estimate of estimate that's exactly what it what what what is it ttest gives you in geometrically if you understand it geometrically it can be thought of as how many standard errors the how many standard errors the estimated coefficient is away from the hypothesized value so again geometrically meaning and geometrically meaning I know this is something uh this is something unconfusing as well I can I can ask you to ignore it as well but uh but if you just want to understand it much more depth it will be easy for you so it can be thought of as a it can be thought it can be thought as uh it can be thought of as how many standard errors how many standard errors as standard errors are away are the the the the coefficients the coefficients the standard errors of the coefficients is away from the hypothesized value is are we from what is hypothesized value which means that we already assume it which is beta is nothing which is which hypothesized value which is equal to zero so what does it get now this is something which which how many standard errors are away from that away from that okay so now what does it give the taste test the the what does it give is it gives uh so if one once you calculate your T once you calculate the T statistic so we're gonna understand two ways either by this particular example of this particular what does it give is significance of a coefficient relative to the uncertainty unsureness what is the significance of particular coefficient with respect to the relative to the there is uncertain uncertainty of the estimate higher than certainty like it's not good right so the last T statistic so now let's talk about what does it mean if you have a large t statistic if you have a large D statistic what does it mean if you have a large D statistic so if you have a large D statistic you it indicates that your estimated coefficient is different from the hypothesized value is different so last is statistic means that your that your estimated coefficient that your estimated coefficient is different is different from the hypothesized value is different from hypothesized value which is s0 which means that there is a relationship that there is a relationship between your h j which this is between your beta 1 and y okay so this would last the statistic mean you will will be talking about it usually it depends whether if you have the P values we have to compare with our P P value but as of now why do we do why do we divide we divide it because because it tells you the significance of our equation now once we have the significance now how can we say that okay this is the perfect like how can we identify whether to accept the null hypothesis or whether you expect the hypothesized value which means that the preassumed thing which is there's no relationship or should we accept it or should we reject that there is that we have an Evidence to say that that there is a relationship between that okay uh let's talk about that this is something important to talk on as well I'm just talking uh too much and so sorry for that but it's actually 30 minutes but that but this is something which is important as well uh in terms of understanding level as well okay so let's talk about uh P value let's talk about pvalue so pvalue over here what is it pvalue means so so once we have the t t statistic we once we have the T to if we compare with the threshold value usually this threshold value 0.05 or 5 and we say that if you T statistic is less than five percent is less than five percent then there is a then we can resect our null hypothesis and then we can reject a null hypothesis and say that there is if it is greater than five percent then we say that okay this is something um we can reject the alternative that there is no relationship then we can say that there's no relationship if your T statistic is greater than like the three critical value the T critical value okay uh currently T statistic is different from T critical value D critical valuables will talk about it just in a second but assume that you got a some critical value and then you compare with a 0.05 okay okay so uh you can take a break as of now even I'm speaking for the last 40 minutes but that's all right this is something which we have to do if you want to understand uh hypothesis testing in detail um so so basically what is the so so how can we say that how can we say that that if your T critical value the critical value the the value which you will get the probability the the the critical value which you'll get is if it's more than five percent then only if it is how can you support that statement we can say that how can we say that how can we say that as I said as I say that the null hypothesis is a statement that assumes that there is no effect there is no effect on the relationship so there's there we introduce the concept of something known as P value and threshold value odd is also called nothing but pvalue so what does pvalue means it says that it says that the observed result is unlikely to have occurred by chance alone and so basically what does it mean I know it's something uh chance and all it's something which is also very great to understand it uh but I'll take a very simple example uh so that you have uh a nice interpretation so assume that you want a 300 test if new weight loss supplement is effective on weight loss or not again the same example which I've taken up so you conduct experiment on two groups of people so group number one and group number two conduct on two groups of people and then once you are one groups that takes the supplement and other takes the uh place which is the other takes the supply once one group takes a supplement other does not fix that supplement okay so hypothesis what does null hypothesis States null hypothesis is that there is no effect of that supplement okay that there's no no effect and and no difference there's no effect so basically there is a two groups again there's a two group group number one group number two so each taken their own treatment affect the supplement those the supplement number one and supplement number two okay so the null hypothesis states that there is no difference between these two so in identify if there's any difference that's a hypothesis and uh the the hypothesis uh the alter alternate hypothesis says that there is a difference which means that if there's a difference that means there is something cooking in that so we can of course use that for treatable effect but I bought the null hypoall state that there is no difference so after you conduct the experiment you calculate your critical value and then what you have a significance you value and then and then your critical value comes critique critical value 0.02 0.02 okay and 0.02 so your value is 0.02 which is the T critical value by by doing the bike taking all the T statistic you calculate 0.02 now now your threshold was 0.05 0.05 right so so what this means that that means that there is enough evidence to support that the weight loss supplement has an effect on the weight loss so there is a Soviet X we reject the null hypothesis we reject the null hypothesis represent so if if your critical value is is a smaller than your significance value then we say then we reject then we reject our null hypothesis and accept our uh alternate hypothesis now you might be thinking why the why does it happen like that why does it happen like that uh this is something uh this this is something which also you need to understand so this is a general thing if a critical value is smaller than this then we say that okay reject the null hypothesis and then we accept that there's a so but if but if you got 0.07 that means that you have to resect but you have to reject your alternate headphones except the null hypothesis okay I know this is something hard to understand as well in the initial level but you have to understand it anyway assuming that you have the name for this cool so how can we connect this how can how should we connect this P value the concept of pvalue to linear regression so the so in linear regression your known hypothesis states that the independent variables and dependent variables has no effect your independent variable has no effect on the depending which means so all your betas are equal to zero and the P value associated with every coefficients help us to determine uh that has a significance with us so basically you have a critical value in this case you will you'll be having critical value for every uh betas okay so correct currently we'll be having critical value or the P P value for this size which states that which states the significance of our size of the house on pre on the price and so in this example in this example as we have seen so you obtain so you obtain the pvalue or we can see so there are P value or we can say critical value we can say critical value by taking the T test and then and then comparing with the ttest distribution we'll talk about that we'll talk about this you calculate the T test after calculating the test over here after calculating this after calculating this you compare after you take out you know after you take out some number you compare with the table you compare with the table and the table you it is a table which is already uh published on the internet okay you you don't even remember everything you don't need to worry about how this came you compare okay this this is something over here now this is uh we have to we'll we'll select the how do we select the p p values as well later on but you you calculate how much significant how what is the probability that this is that is that size of the house is significant okay you calculate that P you calculate the P value for the coefficient of what over the coefficient of your size of the house and now we say that if if your P value is less than 0.05 is a threshold which is the threshold you reject the null hypothesis where you say that there is not enough evidence to accept the null hypothesis so you might be thinking what does it mean bro why do we why do we don't don't cities if it is if your baby value like if your probability is higher that means that it is something significance that is not okay pvalue is always related to the null hypothesis so higher the pvalue is the more you're providing evidence that there's no relationship Lord is that there is an Evidence to reject them there is evidence for alternative that there is a lower chance that there is a lower chance of um of there's a lower chance that it occur that it is a a null hypothesis okay I hope that you are understanding what I'm trying to say if you are not then please watch the lectures once again so that uh you are able to understand in much more easy way um so if if you want I can actually take an example I can actually take an example to help you understand there's a what is chance and all but let me know if you want to understand about that like how can we there's a support to statement how can we say how how can we say how can we say that why not greater why not greater uh currently you can understand this way that if the P value is smaller if the P value is smaller than 0.05 you reject the null hypology except the ordinary hypothesis okay if you if you wanna know please let me know about that as well because I have a very nice explanation but that is bit of out of the context of the course but it's all right I can actually understand like uh I can actually understand this this something which people face as well cool I hope that this gives a good sense now let's do a very nice example a numerical work example so that you understand it nicely and then uh I will be done with the ttest you'll be left with ftest by your own and we're left with the F test which you have to explore by your own I'm not going to talk about that so let's talk about uh so let's continue our previous work example our previously worked example was that your you have a square footage you have a square footage which is one thousand two hundred to one thousand two hundred one thousand two hundred and then you have 1500 1700 2100 2300 2600 and 3000 okay and then you have nothing but the Y where we have the price of the house you can understand some sort of price okay I'm not writing the data but I'm just making it like this okay so your equation will be beta 1 plus beta0 sorry beta0 sorry I think I did it wrong beta Y is equal to beta0 plus beta 1 x okay x x y so assume that your beta0 as you assume that your beta0 is nothing but 5 50 000 okay fifty thousand and your beta 1 is nothing but equals two hundred okay so we can so first so let's State our Nolan uh alternate so no less as H 0 is H should do is where all your beta values H1s the Alternatives they are not equals to zero that there is a relationship existence you calculate the T statistic you calculate the T statistic T statistic how how it is calculated estimated we under calculate for beta 1 okay okay because we're gonna take out the significance of this x one you calculate the beta 100 minus this is the estimated minus zero why because because the knowledge because minus the hypothesized value in this case it is equal to zero divided by divided by the standard error the standard error of that coefficient the standard error is also is a it's it's a nice formula for a calculating of a standard error but in real world you don't need to worry about the formula you can actually look on the internet to calculate the standard error formula so the standard error which which you're gonna get the standard error which which you're gonna get is nothing but the the whole calculation will yield the whole the whole calculation will yield to 100 divided by 10 which will not nothing but equal to 10 so your t t statistic is ten now you might be thinking here use this is the greater than this but in most of the case this will be greater right no once you calculate the T statistic now you take now you convert this into you're going to understand you convert this into a probabilistic value understanding this way I used to understand this whether you convert this into some sort of proper some sort of probability value so that we can compare with a threshold okay so the four calorie for calculating the threshold for practic for uh for calc for calculating the threshold of critical value for calendar in the threshold critical value so that we can compare with our significance so which is 0.0 so that it can compare because we cannot compare this we should compare properties values we calculate something else degrees of freedom degrees of freedom degrees of freedom is calculated by n minus 2 N minus 2 the number of a fluid number of rows which you have in a column the number of rows which you have in your data minus um which is number of parameters of your model in this case it is seven minus 2 which is nothing but 5. now you may be thinking what is degrees of freedom again new question asked so degrees of freedom is is nothing so if you if you also review the notes I have written a very nice and detailed uh notes on what is degrees of freedom so that you also understand it so if you go down so degrees of freedom degrees of freedom is nothing so assume that your data set contains 10 observations and regression model has two parameters the intercept and the slope okay in this case as well it was intercept and the slope the degrees of freedom is equal to eight so what is degrees of freedom his use derives the number of independent ways a system can change number of independent ways the system can change this is used for calculating if you're not able to understand I I would say ignore it but eventually even a but eventually this has a statistical meaning where it says that number of a number of uh independent values or the independent values in a data set so basically beta 1 and beta of course is dependent right number of independent values of your model the number of independent why we say that this number of independent we can search online about more about degrees of freedom but this number of independent value which uh so basically the two are dependent because it's slope and intercept which is dependent so we can subtract it from our whole data number of observations will eventually degrees of freedom degrees of freedom so once you calculate the degrees of freedom in this case is 5 okay now now so now now your significance value is 0.05 degrees of freedom is uh 5 degrees of freedom is 5 degrees of freedom is 5 okay so your significance is this degrees of freedom is this now let's go ahead now let's go ahead and talk about something known as uh now let's talk about something known as uh T table distribution so ACD open up the T table distribution to do that you'll need to be online oops I'm not online that's pretty bad so yeah so I'll put up the I'll I'll put up you you just you should open it online by your by your own I'm not online as of now so I suggest you should open by your own so that you understand it by yourself as well open that online and then just go to a table and then if you see and then if you see on x axis on the top of it yeah you have nothing but uh you have nothing but T values the the the probability which is you have you have to go to two tails in in regression you always use two tests not one test the difference is listed in the reading materials please see that so in two tails you see that 0.05 you see that there is a 0.05 and then if you see the fifth the degrees of freedom is five that is the 2.57 2.571 2.571 is your is your critical T value is 2.571 is a critical T value is a critical T value now you compare the Cal the T statistic T test statistic statistic with the critical value with the critical value you so it shows that it shows that if you your of course T is greater than 2.571 these gradient of course 2.571 so we resect the null hypothesis and because this means that there's a significant relationship between that so you you you might be saying that here you should just some some seconds ago you told that that it should be smaller than the critical T value so as not not about that but in this a t test of course we do in several examples but in ttest we simply do this is we simply reject the null hypothesis in the favor if it is if a test statistic if a significance a level is greater than the approximated uh critical value because of course it changes no it changes so as I said your T statistics should be greater than should be sorry uh should be greater if the if the significance is greater than your approximated value then we say that okay this is something to be predicted nicely so I hope that this gives a good sense uh if you want to know more about all of these things I would really suggest uh go forward and talk about have also given several examples above so if you go and see several examples of mine of ttest of ttest you can go over here and talk about like for every individual predictor variable for every individual predictor variable you should be able to view it nicely but my point was that you can easily use the materials to actually uh ttest to calculate ttest statistics and then approximate the critical value and then compare the and if the T Test with an approximately critical value then you go forward and accept that okay cool but uh but you know but you but you have to also make sure that uh your Trail but but it depends on several other problems from the test to test uh so that's that that was pretty much about uh these statistics statistic um now what we'll do now we'll now we are done with almost hypothesis testing now we'll go ahead and talk about assumptions of linear regression and then we'll wrap up our whole course with the one project hey everyone welcome to this one of the our last reflectures I'm super excited that we came along over here and I hope that you really understood each and every Integrity of everything which should which you should know uh we have talked a lot of things as of now and it's still not ended here let's say we still have a lot of things to explore we still have a lot of things to cover up and one of the things to cover up which is one of the most famous interviews kind of thing is assumptions of linear regression so basically what I'll do is I have tried to make it in a unique way or structural way okay so basically uh we have several sub topics to cover in this that's our sub topics are linearity then you have you know Independence then you have normality assumption and there are several other assumptions listed out here so what I'll try to do what I'll try to do I'll make sure that you understand each of the substation great detail and I'll make sure that you also do your substantial research by yourself okay cool so basically uh let's start off with the first assumption let's start with the first example these are the notes for you to you know review quickly but I prefer using my own hand right now so let me just open my new hand which is uh this one let's open a dotted paper uh once you open the dotted paper let's get started now so the first uh so the first assumption but but what exactly assumption is now you might be thinking hey are you showcase this is assumption but what exactly assumption is and why do we need to study about this so assumptions assumptions so what is the definition of assumption so tell me the definition of assumptions so first of all let's get it clear about what exactly the formal definition on dictionary says because anything which I learn about anything I just make sure that I know that definition of the topic so uh thing is the the what is assumptions the the thing that is accepted as true or as certain to happen without any proof a very nice uh definition so what this says when you apply a linear regression model this linear regression model assumes a several thing without any proof it assumes that your that your data should be linear it should follow linearity assumption almost sky or no constant variance assumption are there a lot of assumptions so you're so your assumes all of these things and assumes and assumes all of these things and then and then you can expect your model to work fine so if you're so if any of those assumptions fail like if any of the assumptions fails then you then you cannot explain the LR model will only perform best you'll only perform best if and only if these assumptions are met assumptions means certain things are met like for example for example uh when when we have something like let's let's take an example when you prove something or uh or whenever when you say now assume that this is something and then you put an argument so when assumption is that not true then how can you say that particular thing uh will work right and so uh assume that uh assume that we have some certain uh assumptions to be there and then we expect these assumptions to be true for our LR model to work perfectly fine okay so this is this is one of the definition of assumptions in a great detail now what I'll do now what I'll do I'll I'll talk about there are several so so as as I talk there are several assumptions so I'll talk about some of the most important assumptions for you to explore it out right away and then we can further get started on uh onwards okay so the first assumption which is which which I'm going to talk about is something known as linearity assumption linearity linearity assumption please ignore my handwriting I have to write it super quickly that's who that's why I'm writing it super super duper quickly so linearity assumption says that it what what does it say is that it says that the relationship between the dependent variable independent variable is linear okay so what does it says the the relationship between as this linear regression and so it expects the linearity Assumption where it says the dependent the the depend the relationship between the independent and the dependent variable are linear so what exactly linear is as I've already talked about it means that we can have the we can have this straight line which can represent or approximate our data so we can have a straight line that can a reasonably approximate our data okay so uh over here this is a very nice example of the linearity Assumption this data where we have only one paint put feature in the output variable you plot this y onto this and X onto this now over here it you it can be your data is linear why because it can be approximately on the straight line data but in this case this is not a linear over here you cannot make a straight line to say to data you it it forms something like this which is not linear okay it cannot be reasonably approximated as a straight line so that's what the linearity Assumption says okay so this is an important why this is important or by why this is important because accuracy of the linear equation model relies on the underlying relationship which is nothing but linear okay so if your model is not linear assume that in this example assume that in this example the so the errors will be super super high as and if the errors are super high which means that your model is not performing well and it can fit in any way you're not able to perform well you have to use extensions of linear education to work on this data but we'll talk about that later on but over here it will not definitely not work well pretty well okay so this is something which you should know about like this is an important assumption because it will lead to unaccurate predictions because of high error Okay cool so once we have those assumptions like once we have those assumptions so which which you can see if you plot the data on a graph the point the point should show that it should be reasonably represented in straight line and if does not form the straight line this means that they are not significantly appropriate or we should use as other significant model which is a probably normal regression or other regression to actually make it more appropriate Okay cool so this was on another example now as I told once we have these assumptions once you to okay this is the Assumption now how can you test how can you test if the if if that particular data set follows linearity assumption so no not linearity assumptions or not we have to test no so we have another sub so how can we test it over here what we did we just did a visual inspection we just did a visual inspection okay this is something which you can draw a straight line but what if you have a higher dimensional dimensional for data so there are several ways to test your model okay the first test which you uh which is uh which is very popular is nothing but visual inspection visual inspection so in that visual inspection in that visual inspection what you do what you do the first thing which you do is Scatter Plots the Scatter Plots the first way to test if that if your model for if your data follows linearity assumption or not is apply scatter plot so what exactly Scatter Plots does so what exactly Scatter Plots does what is it does please we simply take and add independent and or dependent variable we have to of course do it in a 2d plane of course so when you have the on yaxis you have the dependent variable x axis we have independent variables and you plotted so if the relationship shows the straight line or the or the linear data that that means it is volume but if it does but it does not then then it is not near assumption is not satisfied so visual inspection is one of the most common ways as well but over here the only downside though is if you have more number of variables large number of variables input features independent features here to plotted against every individual and then see if it works but but you know it's sometimes not good as well but eventually it helps you to get a better understanding but this is a Scatter Plots to uh eventually help okay so uh so this is this is one of them another one is which is more common like over here we have to plot for every individual independent feature X 2 and then Y and X3 and then Y and all we have to do for Scatter Plots but in but another example is let's say how can you test with residuals plots residual plots so residual plants plots is one of the most common ways to um to test those assumptions what you do you simply put you simply um you simply take the you simply take the residuals and then put it on a yaxis against the predicted or the uh predicted values or the independent variable so basically you take you on yaxis you have the residuals on by sorry on accesses we have the prediction you have the prediction okay on why because you have the residual so why uh on Accessory Y axis will prediction sorry other situles and put on the over here prediction so you might be thinking here how does it even make sense right so let's uh let me show you a very nice example of how what exactly residual plots looks like so that it makes perfect sense to you so I just going to uh take a very nice example maybe that will uh make much more sense uh so I'm just uh drawing out a very nice example so that I can actually send it from Mac to my iPad that really works you know uh sometimes it works and sometimes it is not so let me just open up a very nice so open up a safari where is my Safari yeah so yeah so how can I download load this cool so I send this on my laptop from laptop to over here now I should be able to show it to you all so this is the example of a residual plot this is an example of a residual plot and what does this plot says on on X on on xaxis you have the temperature and then yaxis you have the residuals now if you see that now if you see the when you when you plot the related values against the residuals against the residuals the odd or even attempt or even the independent variables against the residuals it should show what it should show it should show a very Cur a very random scattered point so over here you know you see they are very randomly scattered there's that that's not the residual plots there is the on yaxis you have residual and x axis you have that independent variable so this does not forms any sort of you know a pattern like this uh what what patterns I'm talking about like this or like this or like this it is not forming it is it is a very random one you cannot even fit any sort of models is very very random and they're around zero only they're around zero okay so so if if if if the linearity Assumption holds if the linearity Assumption satisfies the residuals are the residuals are scattered randomly are scattered randomly okay are scattered randomly and they're around zero they're around zero okay I hope that you you are unable to understand this pretty well uh this is this is the visual inspection this this is what you do for visual inspection another way is something known as uh is is something known as partial regression plot partial regression plot partial regression plot and this is something which you have to explore by your own this is something explored by your own I'm not going to teach teach about that but you have to explore by your own uh because this um because now you are at a stage where you have to explode by your own as well I cannot spoon video everything but this this is one of the way which which you should explode right now by yourself another way to test another way to test it is is nothing is nothing but something always statistical tests statistical tests so we have seen our our hypothesis testing in that hypothesis testing we have several tests which is ttest ftest you know now so there are several test for States testing the significance of our um coefficients or independent features so just like that you have a tests for testing the linearity assumptions for example one of the test is nothing but a rainbow test is nothing but rainbow test the rainbow test I'm not going to go into mathematical details of it because this is something uh you're not studying statistics in detail if you want to study statistics just go and search more if you're interested you know but I don't suggest you to go in so mathy details of it because nobody usually asks you just need to know okay there's something just available which we have to use for testing is linearity or maybe something known as har we call it test which is one of our one of the most like uh statistical tests which is often used for testing this other like how can we eventually uh talk about things how can we eventually test if if the model is linear or not okay uh so just you can you can explore the mathematical diseases how do we test exactly but it just follows the same thing which I've studied the test assesses we have a null hypothesis the null hypothesis states that the null hypothesis states that what is a test it states that the there's a the relationship between the relationship between your X and Y is linear so it it assumes that it already assumes now we have an alternative it says that it is not the relationship is not so so so what we do we conduct the test and once we have the critical uh the pvalue if the P P value which we get from after conducting the rainbow or the these test if it's smaller than 0 0.05 which means which means that uh which means that you can reject the null hypothesis uh if if it is more than you can reject the null hypothe which means the resumptions is is not satisfied because you are saying that it is not linear okay so this is one of the this is the the same same concept holds for every hypothesis testing another another example is another example so these are some of the some of the ways to summarize up linear regression assumptions holds for you know there should be linear significant relationship between flow two variables uh we have scatter Scatter Plots then we have residual plots and then we have partial regression plot and statistical plot and that we have uh hypothesis and tests uh sorry there are several hypothesis tests available to test with the darts whether it follows statistical or not okay I hope that this gives a good sense about our linearity assumption now what we'll do we'll talk about something known as another assumption which is nothing but called as uh the remedies of it yeah yeah this is this is one of the way to think about it I just forgot like we have some Revenue remedies about how can we fix so once our assumption so assume that your assumption is violated so assume that your assumptions is assumptions are violated assumptions are violated now if the if your if your model does not satisfy linearization which means that your model is not going to work well right so so we keep so you told about that this will not work well so what we should do if this does not work so I will say that fix that assumption make it the make make it from nonviolated to nonviolated assumption okay and this is what you have to do so there are several remedies for it you know remedies means something to fix if the if it is if it is being violated we have to fix that as well hashtag fix extract hashtag fix is also very famous thing so you have to fix that assumption so there are several remedies the first remedies is nothing but called transforming variables the first remedy is we call transforming of our variables transforming of our variables over here you can see that we transform our variables so um this is one of the Transformer variables so what does that mean transformation of our variables which means that if the relationship between the predictor and the response is appears to be you can try transforming the predictor or to see for example you can take the square root or log or or Square of a variable for example assume that you have a house price square and then you want to predict based on the so you know based on the size and predict the house price you have uh you have something to one thousand then you have 200 000 uh then 250 sorry 1 to 200 square feet and you have to 250 000 so use what you can do you can apply the log you can apply the log or the square root you can apply the log or a square root on you know on maybe on uh on on variables okay on on variables so that it comes on a it it becomes linear okay so you apply the transformation on the variables you apply the transformational variables used to study more about feature in genetics if you want to study more in degree level or all of these things I suggest you take the course which is cs01 bo4 which is one of our famous sports available online um so you can actually uh talk about this so we get we we take this log square root or Square of the predictor variable so independent variables out there so that it works perfectly so that you bring that bring them back into the linear way so when I take the log it of course comes in linear uh so this is a very nice uh note to make sure to so that's one of the transformation of Remedy uh variables another one is ADD higher order terms had higher order terms or interaction terms which means that if you have your X variable then you make another variable X but you square that but you squared that okay so you add the so you add the cubic per Cube cubic terms or quadratic to help capture the even the nonlinear relationship So eventually this introduces a concept of polynomial regression in this Century entries concept of polynomial regression but that's something for later on where you introduce the higher order of you know uh the variables where you just take the X and then take the x squared and then you have x 2 x 1 take the X1 squared so you just introduce the higher order uh terms for example quadratic or cubic which helps to capture the nonlinear relationship another one is like if nothing is working then you should start going on to take out the different model okay different model means you can consider using Gams or decision trees models like random Forest which can better capture if you're if if you data is nonlinear too much in order to satisfy the Assumption then I think you should go to random forest or somewhere like that to actually uh cover up our assumptions of uh random Forest you know okay cool so I hope that gives you a very nice understanding about a linearity assumption what the next switch we'll talk about is something known as uh Independence assumption which is also a very nice assumption and then after that we'll talk about Homo scarcity and normality assumption and then we'll wrap up the lecture on this what's up hey everyone welcome to this video in this video what exactly I'm going to do is going to talk about independence assumption this is one of the again the second most famous assumption available right now which is Independence which is independence assumption which is again I said the second most Independence assumption which is like important so the what does independence assumption States independence assumption states that what is it states it states that that the errors or the residuals which you have the residuals which you have the residuals which you have there is the the residuals which you have are independent of each other okay so for example you have X1 and then you have y and then you have a b c d and you have y one y two y three y four and then you have y hat which you've taken from which is y hat one y hat two y so your error will be will be Error 1 is equals to Y hat one minus y one error 2 is equals to Y it had two minus so so the error number one should be independent of area number two or error number two two should be independent of error number one okay so errors or residuals associated with the in this the data or the observation should be should be uninfluenced by any other observation out there so error number two should be uninfluenced by any other so error number two should not get affected by the error number one okay or error number two should not get affected by an animal else so I guess the the the individual assumption states that the Assumption switches holds that that your observations associated with one observations should not influence the error associated with the another observation okay if you don't fix this you will end up with biased results uh unaccurate estimates unreliable predictions and a lot of things can happen uh so this is the this is what the assumptions say that you're uh that your error should be independent so let's take a very nice example so example states that that you have you have something known as electricity so you want to predict the electricity uh based on the uh based on the temperature in a city based on the hour okay so we collect the hourly data for entire month so you have hourly data which is and then you have temperature okay and then you have your electricity demand temperature electricity and demand so one two three dot dot dot 15 16 17 dot dot dot and 80 85 and 90 dot dot so this is this is one of our you know uh this is one of our um this is one of our data which we have in our annual test whether this assumption has a wholesale Independence assumption or not so if you see that the demand for electricity at one hour at one hour at one hour might be related to the demand of the previous one right so so basically so basically what what is it trying to say you it's it's something which is trying to tell you that assume that the electricity demand at on up at our two might be related to the previous hour right might be related to previous hour electricity at three might be related to the previous hour hour number three multiple to the previous hour so you see that there is some sort of relation and as we said that there should be not any relation of course if this is written then of course errors will be also related okay so electricity uses strengths to show a pattern so when you when as we are using electricity it is being so that we have it is greater than the previous one that's what it is showing okay it is as the hours increases your electricity demands also in which it is showing such some sort of pack pattern over the course of a date so it is of course showing the pattern over the course of the day because it is related to each other so what is it showing for example over here higher the demand is higher in the day and lower demand at night or lower demand at night so this is something which is which which can be showcasing the patterns so over here this is showcasing the trends which is not good right this means that our that our points are independent violating the independence assumption and that's one of our important major issue so that's this this is what assumption indicates how can we test our assumptions for testing our assumptions for test testing and assumptions we have couple of things again we can talk about residual plots and residual plots so again you you can take your residuals and plot it and plot it or like like this you take the residuals and plot in on a yaxis and you take the fitted value the prediction value product on xaxis right and when you plot it and when you plot it you will see you will see that you will see that uh you you'll see that uh you will see that there is no patterns or trends of the residual so there is no patterns there is this there's no such certain patterns uh of over here this over the line they are distributed randomly or scatteredly if there's some sort of patterns like this you know like this so we can say that that's that something does not hold true but over here this does not have pattern but but in the case of this it will hold a particular pattern when you when in this particular example to hold a pattern okay uh so you this is a visual inspection uh from which you can see the another one is nothing but call statistical tests statistical tests and statistical tests what does it say is that you have couple of tests which is Durbin Watson test the urban Watson test which will give you some sort of number and then we say that okay that that number is greater than the significance to 0.05 which is a P value then we say okay we can reject the null hypothesis and say okay this something is correlate so we'll talk these are tests which have already seen the way we conduct tests and all in previous lectures but Darwin Watson test and uh versus got Freight rest God Freight test so let's talk about a bit about uh Durbin but your your stance should be that you are making it researching a bit more about it okay and so how can we so that so there is one of them is um is what we can do so what exactly the Durbin Watson test means the Durbin Watson uh is specifically was introduced what Hawaii why it was introduced it was introduced to detect the auto correlation Auto correlation Auto correlation it it was it was came on to reduce the auto correlations in the residuals in the residuals in the residuals of Allah so what is means autocorrelation the residual so which which directly means what is auto correlation first of all autocorrelation nothing but the correlation between the residences correlation means is there any relationship between both of them so if they're highly highly correlated which means what they are indeed they're dependent on each other so so basically it tells you the correlation between two and the two residuals at different time points so basically it's it it means that it checks with the errors are correlated across observations or not so if they are that means they don't they don't they don't satisfy this that the independence assumption and if they are not correlated then then they satisfy the Assumption okay so dermin Watson the Durbin Watson test which you have it lies between zero to four zero to four and if you turbine Watson gives test of a 2 value 2 which means that there is no autocorrelation and if there is no autocorrelation between residuals that means the Assumption satisfies because there is there is no barriers correlated with each other okay so but if the values are less than if the values are less than two that means there is a positive correlations which means there's a positive quarter we have already talked about positive and negative correlation please see the lectures of probability if you haven't bought the course please now go and just enroll in a course if you haven't studied about problem statistics so value less than two suggests that the positive correlations are nothing but are nothing but um the positive correlations which you have is nothing but uh a value less than two if your turbine was to test statistics give less than two that means they have a they hold a positive correlation but uh which means the residuals are positively correlated across observations but your value is greater than two suggest negative correlation which means that residuals are negatively correlated so your value should be two if they want to satisfy this assumption if they're less than two that means they are positively correlated if they're greater than two then they are negatively correlated across the observations okay um yeah so uh how can we perform some of the steps so what you do you first of all fit your data and then you calculate the residuals you calculate the residuals you calculate the residuals after calculating the residuals what you do you take out D in in T Test you're taking out the T value over here you can take out the test statistic which is D um you you can search online for the formula but formula is not even required you know if there are Auto automatically tools which does for this now once we have this D now you what you do you compare the test statistic to the script critical values for the Durbin WhatsApp the the the table which we have okay the tables are statistical software come you compare it and if your D is less than that lower critical value or greater than that then we resent the null hypothesis of no autocorrelation which means assumptions is likely violated for example what does it says what does it say that you conduct the test and then if and then we again our null hypothesis and Alternate hypothesis comes into play and if it's greater than that and then we reject it and then so this resection automation will eventually happen Okay so but uh but mostly what you do if you could be statistic gives two that means there is no autocorrelation but however you can perform test as well for detecting autocorrelation but one of the few more drawbacks of it the Durbin Watson test is is more for you know what it's more for time series data it's more for time series data okay so that's why uh we have to use a you have to use proper you know tests for all of this thing I hope that you understood about how can we test our independence resentment so there are several remedies for it so there are several remedies if the assumptions are violated how can we fix it so one of the way to fix it is add additional variables so you can actually add additional variables you can actually add the first one is add additional vars add variables so for example if there's any underlying factor which is causing for for example in this case which you have taken example that your this this this has a this boil it's independent y because it might be related to previous one so we can find a factor which is causing we can try to find a factor which is causing that dependence and then we can you know add this Factor as an independent variable okay to uh we can add this Factor so you're adding more variables definitely helps definitely helps for for example in this example what we can do we can introduce another variable we can enter introduce another variable we can introduce another variable what variable time of the day time of the day in this case the time of time time of the race we will introduce another variable so time of the week which is night night morning so how does it help how does it help this helps how does it help if if we convert we can actually take it that we can actually understand the pattern your model could understand the pattern okay this is something is um uh daily practice you want to be able to capture the daily patterns that's that that was the factor so you can consider adding more variables according to what what is causing this this Factor so basically at night time so we can actually explicitly add this to actually help you so that your model does not be depend so this is one of them you can add more variables uh we'll do one proper example as well don't don't worry you can use time series model for time series model drill relatively works well in these type of examples and then at last you have nothing but called mixed effect models but that's something it's like that's something you can use hierarchical hierarchical linear models but I suggested to export by that alone it's not too much use but it's exactly that's what people actually use sometimes if if you have hierarchical data but I suggest you to learn about hierarchical linear models HLN a very nice uh concept around it okay so adding more variables you know uh time series using another algorithm all of this really helps good so I hope that you understood about independence assumption now we'll talk about uh homo schedule assumption uh which is also known as nonuh constant constant variance constant variance across it so I'll talk about that uh where where your variance of all the residuals are constant across all the levels so we'll talk about that homo this no constant very uh constant various assumption uh and then we'll take one example and after that we'll have our simple last one which is normality and then we have no multiple linearity which is left upon you for you to explore so let's get started with um what homo scarcity assumption or even say no constant variance assumption no okay cool so what does this homo scarcity assumption states that I I really have a hard time understanding you know pronouncing this so please ignore if I sometimes pronounce it wrong so most characteristic assumption is a statistical assumption that states that the variance of the eritream also with this spread or very what does it means the spread of the values variance means spread of the values okay uh how how does it spread across the mean so the mean of that how does the variance is uh in a regression model is constant the residuals or the variants are constant okay they're constant across all the all the predictor y variables you may be thinking yeah use what the okay so what what what just you told okay so uh you have uh you you you might be confused how what exactly does it mean so let's talk about what exactly does it mean so you can assume that it is a it states that there are variance I assume that you know variance the spread of values of the researchables is constant across all levels they do not change or you can say this spread of irresiduals should be approximately same for all values of the independent so you have so they should be constant across all independent values okay this should be constant if they are not if the variance of the residuals increases or decreases transmitter Dr domestically for example at at Road row number one error is something C and over row number two error is C plus hundred that that means that is that increases and at around row number three C minus 100 that means it decreases you know so it increases this decrease dramatically which means that it is not following that assumption that is not constant the variance is not of constant across the individuals of your samples then we say that it is not following our assumption that voltage the Assumption okay so uh I hope that really helps about that again over here for testing it we have nothing but called residual plots we have nothing but called residual plots which really helps to test the homoscadicity Assumption so what is uh so let me let me just uh again if there's too many patterns then you can conclude that there's no such assumption so just going to uh make you aware about like a very nice example maybe for you which will be super helpful if you understand it a nice way so just going to have it over here the PNG file uh uploaded right here share done and then I'm going to share again airdrop and iPad oops Yeah so this is this is one of the nice image which I usually use to explain it so this is this is called a residual plots we have a residual plots we plot and the yaxis residuals on xaxis we have some sort of uh fitted values you have to put if we have fitted values or the predicted values or the model pretty values on on yaxis you have the uh on yaxis you have what on yaxis you have nothing but uh residuals or Nexus we have fitted values and when you plot it if you it shows some sort of patterns over here you see that it is showing some sort of patterns over here it is showing some sort of patterns but over here it is not showing any sort of patterns right so in this example if if your model shows something like this that means the assumption is satisfied you can go ahead with it but if this like if there's some sort of showing patterns that means that assumptions does not hold this is this is example of a heterostasis heterostaticity and this is also example heteroscadicity but this is an example of homoscadicity which means that it was a homoseconditional assumption is true which is consider your variance your errors are constant across the uh across the independent variables okay and there's no Trends or there's no Trends over here it is increasing over here it is decreasing now where there is it is constant across all variables okay uh this is what this is a visual inspection another one is nothing but called uh we have several you know uh statistical tests just like as pre previous one which is called the Pagan test Pagan test or a white test white test and all of this you don't really need to you know understand what exactly uh it does but again the hypothesis testing formula looks in you have the and null hypothesis alternate and then you perform the test and then checking the significance value and then you say okay there's something greater than then we reject or accept statistical test for all the statistical tests which you have to understand by your own by searching online I'm not going to do with it because it is out of the scope of the course as well it is not required as well for for initial steps at least so and once you once you assume that your your model does not satisfy so what are the remedies how can we fix it so fixing can be happen in two or three ways two or three ways okay so uh two or three ways so let's talk about what are those two or three ways the first way is we can transform our variables transform transformation can happen transformation can happen so you can add you know you can apply transformation or dependent on an independent like a log transformation or square root transformation or inverse transformation and then see if your assumption is being fixed or not another one what you can do which is weighted linear regression weighted linear regression what is weighted linear regression says that instead of using ordinary which you use right now you can use weighted which what it does it assigns weights towards you have a nine observation so it assigns the weights to every observation okay based on their variance based on the variance but weighted linear regression is something which is which which you should learn in the extensions of linear regression um which is upon you to understand it much more graduated I'm just doing it because I want you to and make explore by yourself so not empty and giving you that nobody gives all of these names but I'm giving you to explore by your own and the another one which is we should explore which is uh more sort of you know uh used to remedy is something known as hover regression or quantile regression quantile regression content regression or hover regression and these two are used you know for uh techniques for you know what it is used for uh checking whether whether uh and and this this really really helps in fixing those quantile regression and hover so I suggest you to explore these three types of extensions of linear regression which helps to satisfy this assumption so I really want you to go ahead and then stop this lecture and then go over and search about this learn about this make a note and tag me on LinkedIn Okay cool so I hope that you're done with this homo scarcity assumption uh now at last we'll talk about nothing but call normality assumption which is our most you know as well as it's not you know what most it's something which is required for everyone you know cool so let's get started with normality assumption so what is it normality assumption states that normal resumption states that normality assumption states that that the residuals now previously in Independence and the constant and homoscope is the constant you know Independence independent of each other uh constant and now we talk about they should be normally distributed you know that word is normally distributed you know what is said to be write me in the comment box what is it said to be a normally distributed a very nice uh so I've just wait for five seconds what does it means so tell me what is it means your residuals to be normally to be normally distributed so let me let me just write the question so that at least I can check for it and you and in the comment box uh yeah so please go ahead and invite it um so that I could understand foreign so what does it say to be a normally distributed which means which means that your mean is that that that your distribution is following a normal gaussian distribution okay your mean is following a normal gaussian distribution that your residuals are following the normal when you plot the residuals they should follow something like this where most of the values should lie between this range okay should right should uh this should be symmetrical this should be symmetrical most of the value should be around the mean your mean is something that nothing mean is equals to zero and then you have the bell shaped curve which is symmetric which is which should be which where the mean should be close to zero in the case of the residuals in the case of residuals your mean should be close to zero in the case because residuals are of course zero right because we don't want to be residuals to be high so it should satisfy our normality assumption it should satisfy our normality assumption I hope that so basically uh normative assumption states that and what is normality assumption say it's that that your normality assumptions refers to assumptions that the errors are normally distributed in other words that your uh it should be well shaped or belt shaped curved with a symmetrical positions with most of the errors clustered around the mean and fewer errors so most of the errors so even even plotted so most of the errors should be over here so it should be constant so you see that it is related to the previous one this constant show here most of the error should be constant it should be independent and there should be around they should be similar it should not be this it should be very one one or two error should be at extreme like very different but it should be most most of them should should be uh between like it should be around the mean okay uh over over here you can see that if you violate this it can affect the validity and the conclusions your model is going to make and it can also often result in bias and inflated standard deviation errors so again which can generally affects the accuracy so there are several ways to test the and the easiest way is to plot a distilled plot is a normal plot which is nothing but a histogram or something like you know histogram with a belt shape curve uh which which you usually do so let me just introduce you with uh a very nice image for this as well maybe for you which will help you we have two two types of plot which we can use the first one is quantile plots the second one is uh histogram plot so assume that I just want to take a very nice example in which example I should take I don't even get it um so I just gonna use this so assume that assume that you have this particular example where you plot the residuals on a yaxis where you plot the residuals on a y axis and then see okay your eventually your your your bell shape will shut form like this it should it should form like a gaussian distribution that's one of them when you plot the where you plot the histogram where you plot the histogram and then what you do when the plot the histogram with the Bell shape curve and then and then you apply the distal plot on top of it so that it looks like a a density plot you know the density plot and it should look like a gaussian distribution or you can what you can do you can create a quantile quantile plots what does quantile content plot means it means that what is content quantile means that it means that if the residuals are normally distributed should fall approximately along a straight line now this may be a bit of you know a bit off to you so let's just go ahead and talk about it so what I'm going to do is have it like over here quantile plots I'm just going to send it out to you over here you usually you know what I do I usually teach along with it I I just don't like you know to teach it like uh like like like just having a preparation because I feel like having a preparation really affects you know uh the way we teach so of course I have a preparation but not too much because I like to be very natural so over here you can see that you have a QQ plot and UV plot if so what is QQ plot indicates the QP plot sees that that if your distribution are now if your errors if your residuals are normally distributed then the points on the the point it should lie approximately which is approximately over here this normal distribution their lies approximately on the straight line but over here this is not a normal distribution log normal distribution so your points are so points are in a diverse the points are not on the straight line over here this is also not on a straight line but over here this is on a straight line so if if you get the content plot like this then you say okay this satisfies our assumption and then you have several other statistical tests just like um previous one like which is over here we have sapiro will test or you can say Smith Smirnoff test where it is used but will what will perform the test one of the test in practical but you there's that something is according to you if you want to do it but I like to do it by normality assumption which is just my visual inspection what are the what are the remedies for it transformation is one of the most popular remedies removing the variables which is which is causing to be it in nonnormal so you can remove the variables which is causing to be nonnormal or you can talk about or you can use different other algorithms such as quantile regression as I told that you should be comfortable with that um and actually in the course you know the course which which we have CSO one we teach about all of these extension as well but as of now I feel like as we made it for fee we just wanted to explore by your own as well okay so this this is about normality assumption where we where we are ask our residuals to follow a normal distributed plot Okay cool so I hope that really makes sense uh now what I'll do I'll just wrap up so now we are we are done with almost all the assumptions now what is left is something known as no multicollinearity assumption which is for you as a homework to uh understand it's very easy and very very tricky as well if you don't understand the right way so but that's that's something which you have to deal by yourself or talked about 10 hours of lecture that's something which you have to do by your own as well right and we have a talk in very things in great detail and I really hope that you understand all of these things you have what we have came to an end actually so we are done with the assumptions and everything now what's next the next is nothing but we'll go and do a sample project one simple linear regression and one multiple linear question to Showcase you several other things as well so you don't need to worry about a lot of things let's get started with the final section of the course and then we'll say goodbye foreign welcome to the mostly the last section I've been telling in the Practical session so we'll so uh so we'll now start off with the practicality stuff get you comfortable with what we have learned and how to implement that as well okay so we'll we'll definitely do that but but prior to this you have a programming assignment for you so that you could you could Implement linear regression from very Scratch by your own I'm not going to do that what I'm going to do I'm going to utilize libraries teach you the core and Concepts and cruts which nobody else teaches that's my aim so uh we'll be we'll be doing a short a couple of lending regression project one of the one of them is using simple linear regression to give you a basic idea about how we go about building a very nice report how do how we go about interpreting the results of linear regression and Etc and as my as as my strategy suggests we'll start off with the Baseline and then we will slowly extend to the to the multiple variables as well right so so what so what we can do we can actually uh do that in terms of uh having more sort of you know adding more sort of uh projects in future if you want to go with the free one but as of now let's let's get started with the very simple example on how we can use linear regression for identifying predict advertising prediction but before that I'm pretty much sure that you might be confused about what exactly the project is and how we're going to go forward with it so basically let me show you what exactly the project I'm going to start over the problem statement and then after the problem statement I'm going to make you familiar with something known as uh with with some with something known as um like what a simple and linear regression and then we'll do a couple of stuffs like 40 for example data ingestion and then we'll talk about how we will do the model how do you test the assumptions and Etc so I hope that that really makes sense uh apart from it and uh and on the other other hand uh let's get started I eventually talking about first of all what exactly the data is so first of all we have the advertising data set as they have already seen a lot uh and you'll also seen the programming assignment which you'll be doing uh please see the course website where we have listed for the same so over here of over here you have three variables you have three variables which is TV a radio and newspaper and these are listed num listed listed it's listed um expenses which which a particular company has put on for example they have they have spent around 200 that 230.1 uh on uh on on expense on TV at advertising on TV 37.8 000 on Radio and 669.2 on newspaper advertising and based on Advertising they got 20 20 22.1 this much sales okay so so basically we had this is over here we have the expenses in which every company has did on TV radio and newspaper and we are going to predict uh what is now what is the significant number of a sales so now your task as a data scientist is to identify which which in which mode of advertising is much more liable for is which which a company should increase the expense to get the sales and which one um which which one source which should completely vanish it like we should completely ignore it for example in some cases okay you uh you you might say okay for this particular example you might want to go with TV because it is more significant in getting more sales as compared to newspaper or radio right so so your comp so so that's that's what your work should be as a data scientist it's helped to take help the stakeholders to take better decisions out there right so what decisions they they have to take they have to they have to take the decisions up about like whether to use TV whether to expand and spend more on TV radio or newspaper or couple of them so we have to identify we have to identify which variable independent variable is giving most of our most amount of contribution to sales which particular medium of source is giving most amount of informations to the sales and that's our problem statement so what I'll do for for being super simplistic as as I told it's not a fullfledged project it's just for the learning purposes not for the putting on a portfolio so basically uh over here what exactly gonna do I want to take this TV and I want to take one of the I'll take one of the advertising source and understand his impact understand its impact on the awkward variable sales for example I could take TV and then understand its impact on sales so I can build a linear regression on top of it we're given the TV the expense curve given on TV it will protect me the sales so that I can and if I open this much and then I can do for separate variables and then I can do for separate variables and then see which is which is having the most amount of contribution however this is not veto it we actually use the multiple linear regression where we'll use a three of them to actually predict the sales but that's for you to try out not for me as a programming assignment we have listed detailed programming assignment for you where we will do a small thing and then you have to extend it to further okay so over here or over here what we'll be doing we will be making the data into a simple linear equation which is a simple PDF where you take the only one column as of now so that you have X1 you have one a one into an independent feature and then one dependent feature and over here I'm going to RFI the significance of TV onto the sales the significance of TV or to the sales however you can also include other variables if you want to understand the significance but as of now let's let's get started with it uh you'll be also doing a very nice programming assignment where you'll be actually understanding answering that question of which one to go forward with which one of the medium source to go forward with so let's get started with actually talking about uh the how how can you code in a computer so uh everyone back to this lecture uh to this project so I'm going to start off with the data injection so the first step of any Machinery project is to ingest the data and I really like it to be in certain sort of class and everything so basically over here over here we have um we have some we have a class and we have a little data ingestion class which ingests the data advertising and then lows it down to only one to uh to a simple for to a data set where we can use Simple linear regression on where we have only one independent feature and one dependent feature so the first step is to initialize the class for the file path so we required a file path when someone instantiates this cloth class and then what and then what we do and then we have a method called load data which reads the CSV which reads the CSV from the specified file path and then we get the and then we get our specified as I said that it that we want the specific TB and sales only so that it is for learning purposes and then you concatenate both of the X and Y variables and then we return that data frame okay so this is a basic data ingestion class not a big deal to understand it okay now we can actually use this now we can actually use this over here over here which is uh where where we are importing so ignore everything just just go over here you can you can say from source which is the source folder dot data ingests with the file name import the data ingestion class and then you say okay this is inside instantiator class and then um and then Source it to the or the make it to the advertising.csv and that gives your simple DF that that that will give you the data frame which is a simple DF which we want to perform on okay now in data ingestion is done so let's run our file to actually see okay that will make much more sense and I'm going to use something like uh very nice over here so I'm just oops a little clear LS and then I'm just going to have this uh if I have actually an environment fiction so I'll just uh environment variable so what was the name LR Raj maybe I'm not sure yeah uh so Honda deactivate so that's clear okay so uh I have activated my environment if you don't know how to activate you will also seeing the reading materials for that or the other otherwise in the next set of projects I have showed you how to do this okay so I'm just going to remove this because it's not required yes over here now what we'll do over here now what we what we will do is we will simply uh run this python dot python main dot pi and I'll just I just want to run that uh just gonna make it comment and I I just want to take a look at what exactly the data frame looks like so it's going to print that data frame print that data frame it's better it may seems good to you so that you can see what exactly it's working on so so when you when you run it you'll be eventually getting a date a data frame where where you have only one column which is an X and Y over there okay so it eventually takes time initially and then it works fine so you have the TV which is that expenses on TV and then the sales which happen if you have advertised on TV so that's a data ingestion is done now the next step is I'm going to do a basic data processing please note that in the next project which will do we'll have the very extensive data processing thing but as of now I would suggest you can ignore this as if now okay uh maybe uh you which it it just for information purposes if you want to know how we are performing basically what we're doing we are we are importing a class known as data processing class and in that we are instant sharing uh instantiating with the data frame object sorry uh ends with which which takes an input which is data frame and then performs couple of methods the first method it applies it identifies if there's any outlier the box plot we box plot and see if there's any outlier in that particular TV's variable because linear regression is it's sensitive to outliers so we have the TV and then we just see if there's any outliers if there's any we identify uh who who are those outliers numerically okay and there are several ways to deal with it capping trimming and several ways to identify as well we should be seeing in the next lecture please ignore that as of now so when you're going to go to main.pi now uh now you can just import the same thing you can simply import the same thing which is the data processing from that data source folder in the data data preprocess file and then you simply run it so let's when when we run it it is okay let's have a fi if there's any outlier so it says that there's no one here apply so let's print it out let's let's print out books I haven't printed that so outliers so if I just print it out you will be able to see that there's zero outliers in that trigger column okay of uh using zscore but you will identify your method to actually uh NF which are outliers and I would recommend to if you don't know about outliers you should wait for the next lecture as of now you can ignore this this is not the scope of this uh process the scope of this process is focus on model building only so now once we have the bit of Crossing I just want to show you the process nothing else like okay after data ingestion we process the data in a good way and then what we do we go on how can we build a linear regression model so how can we build that so we'll be using stats model API because I really like that as computer as a scikitlearn because um I don't know why but yeah it it is more interpretable it gives more results it gives more explanations when we off after after building up the model okay so basically we'll build a class which takes in which takes an X and Y which is the independent and dependent variable and then we and and then and then what we do we add a constant which is one one terms you know in X is a design Matrix and then one one one one one uh a column of one so that we multiply with beta0 and 1 which is SM dot add constant we have SM as an allies for this a for for this library and then what we do and then we make a method called fit and that fit what it does we are calling or we are calling orderly squares which is equivalent to linear regression and then we're calling dot fit and when we're calling dot fit by giving your independent variable and dependent sorry dependent variable independent variable where we were saying okay this is something as a dependent independent and this something is dependent so more of the relationship using linear regression first first what this fit fit will do it will perform all the steps First Step hypothesis second calculate the cost third is perform the gradient descent on top of it until unless your model is come first we have already talked about what exactly fit method does it fits the method for example we have already seen what the algorithm is take out the hypothesis take out the error take out the dead take out the derivative and then perform the gradient descent n number of times just after you updating and updating beta's value so that's what fit does okay fit means it trains your linear regression model by the number of iterations so when you actually go and see what it whatever exactly isn't that our OLS is you will be seeing all this stuff but but basically it trains your using gradient descent algorithm cool and then return the model and then we can print the summary of our model what exactly after training our model what exactly the summary of our model looks like what exactly the summary of our model looks like and if you if you're getting confused what this dot fit method does please take a look at online from for a scratch implementation of limitation if you're still not sure about how do we programming it up but uh if but you have to attempt a programming assignment for this for better understanding you print the summary of the line regression model which is model is equal to self.fit first of all you train the model and after you sell those that you train the model and then print the summary of the model and return the trained model so that anyone can use it for prediction so over here when you actually uh when you actually uh use that so let's let's have a basic simple integration model over here and let's print it out let's print print that stuff out this model summary so um basically we have we're just calling dot summary we are just calling dot summary and when you're calling dot summary it first of all trains the model prints out somebody and then return the model so how does that summary looks like which is the most important part of this lecture is we have this OLS regression results which is telling okay this is the dependent variable what model we are using and a lot of things we shall talk about that in just a second about how we can interpret the OLS results regression and then once we have our uh oh what is results now we are we have a model trained with a full report of how it is performing how our model is performing how they're performing in different different tests and Etc and Etc so this is something which will which which we'll talk about okay uh each and every Integrity what exactly this what exactly this what exactly this report says and everything in Greater detail so let's talk about how can we interpret each of them in a nice way so I would suggest if you go to interpret.md so when you when you go to interpret.md you have pretty much everything listed out there so I'm just going to focus on important you know explanation of which is required for you as of now and which is in the scope of the course uh like confidence the interval switcher which we haven't had a talk so I'm not going to talk about that but we are going to talk about most of the things which is the starter which is also very very important cool so let's talk about that so first of all let's let's assess the results so now we have trained the model that can take in the TV expenses and then can predict and that can predict what that what what will be the possibly the number of the sales which are which you're gonna get so what you can do you can simply go over here you can simply go over here and then say okay here and then say you will model dot predict model dot predict you given though you you you're given the value of TV sales and then that will give you you're given the value of TV sales and the previous year through the particular variable which is the uh which are TV expenses and that that will give you the sales okay so this is what what you're going to model or predict but that's something for you know I just like you can easily do that I assume that because that's something which I will also explore by your own okay cool so over here you have the first first and the first one is dependent variable and dependent variable is a dependent variable in the model where in this case it's the sales going to predict the sales given the TBE expenses second one is r squared as you know we had a talk on r squared right in Greater detail how do we evaluate our how do we evaluate our linear regression model so over here so over here our r squared our r squared which is nothing but 0.812 which means that 81.2 variations of the sales is explained in our TV that means that is pretty nice which is 81 of variations in the sales whatever the variations comes in in the sales is being described by your TV expenses and that's pretty much put it's good so higher r squared indicates the model is better fit to the data where lower is not good okay uh but but it also depends on problem to problem so you'll so you might have to worry about that as well but but basically over here it explains pretty well uh what is adjusted r squared adjusted our r squared in this case is almost the same is almost the same which is 0.811 and what has what exactly adjusted R square is it's just it's nothing but a modified version of your R square modified version of R square so what is modified version of r squared for example it is a measure of a goodness of fit for the linear equation model that adjusts for the number of predicts now you now you might be confused in this so let's consider very very simple example consider that linearized model that predicts the sales of a product for the amount of money spent on Advertising okay so r squared will give us an idea about how more how well the model fits on the data how well your sales is being described by your independent variable okay now let's see now let's say that you add under additional independent variable such as how much your how much your company spent on radio spending okay so the r squared value will increase if the radio for for example r squared which which we have will increase if the radio is pending improves the model OKAY like if if the radio spending improves the model R square will also increase right so basically so basically it will increase even if one of the predictor is not improving the model okay so you put another variable X and then if it is if if it is not improving but a radio is improving then increases so r squared might not be significant if you have a more lots lots of predict independent variables okay lots of independent variables so I just R square is pretty much same because we have only one variable and they are almost same because over here we have only one to check the the two to check how much they're explaining to the output variable y but for example if you introduce to one of the variables like radio newspaper then you will see a massive difference because because if you have a large large number of even if one of them is increa is improving the model you're asked whether it will increase but r squared just increase the RS but I just ask it what does it do it only increase R square if and only if if and only if if your overall it just if your overall model is improving okay so that is our adjusted R square you can take a look if you want to know more about it method what method which you're using B squares method where we are using greatness uh grain is almost same but little bit of difference and all but we are using gradient descent algorithm to optimize it and then we have something else F statistic and so F statistic which you have already talked about in hypothesis testing and F statistic which tests the overall significance of the regression model in this case the F statistic is this much which tests how much your regression model is good so basically uh so basically of over here which you see the app statistic which does overall significance of a particular model and if you convert that to a probability probability of say F statistic which is around 7.9374 which means the model is highly significant how can we say that the model is highly significant so you might be thinking here you 7.93.4 is values very very small like very very large right but it's not actually if if if you know math then you'll be able to understand this that actually a very very small number okay so so what we'll do we'll compare that value the probability will compare that value against our uh against our uh PP value and then we can see use and and and then we can see over here and then what in the in the pvalue we can we can we can test okay in this case we we can have a null hypothesis that the co if that the model coefficients are equal to zero okay a low P value indicates that the evidence again so we can so P value which is the P value P value lower than 0.05 which so that will if if it is then will be resected so over here it's actually lower than 0.0.05 then we'll say okay this let's reject our null hypothesis the null hypothesis was that all our betas are equals to zero so that is not so so we can see that it's pretty very zero that model is highly significant to our uh so your your DV is highly significant in predicting that but but uh but the this this F statistic can be for several other values as well it tests the overall sense so if you also see the highly significant then if you use three of the variables of radio newspaper and uh what this TV then if it is significant you say Okay three other variables are very highly significant as well in predicting the sales okay uh number of observations there are 200 observations as of now as I said there's degrees of freedoms so degrees of freedoms in this case it's 198 because you have 200 200 number of rows and then you simply have number of rows or number of data points minus the number of parameters in this case only beta 1 and beta 0 beta 1 is related to TV and bitter is the constant term uh it's there's the biased term so what exactly degrees of freedom is so what exactly this does is it represents the number of independent values in your data set number of independent values okay this actually helps in calculation this actually helps in calculation of T statistic or F statistic and lot of other way but this is what the DF residuals is it's saying they are DF residuals it's nothing but here 198. okay so covariance type so what is covariance type covariance type over here is nothing but uh nonrobist we'll talk about that what does that what does that mean what does it mean but the covariance type is the type of the covariance used to compute the standard errors of the coefficients again the blah blah blah it's very difficult to understand but I'll I'll explain in a nice way so that you understand it so over here what is a telling that is your standard matters us first of all what is standard errors we have all drugged the other how efficient how efficient or accurate or how reliable our coefficients are for example a very nice example which will destroy or say you want to predict the mean height of all adult males in the United States of America we can take a sample of adult males and calculate the mean height of the samples will be from the sample we to calculate the mean height so how much so we say that because the sample is only small portion of the whole POS population so the mean height of the sample will not directly match the population sample that's true right that's true this is mean height will of course not matter so the standard and what is it tells it tells the measure of the variability of the height of the sample which provides what is the what is the it provides an estimate of the uncertainty so how much we are sure about how much our sample mean is accurate how much a sample mean is accurate accordance through the population mean so small on me is smaller than standard error is more precise the estimate for your population estimate so you're based on Sample mean you're predicting the population mean which means that you're using uh there is some sort of standard error switch tells that okay how much precise we are in getting the population mean given we have the uh sample mean and in this case covariance type is non robust which means we calculate the covariance of our Matrix without any elimination of the data without any elimination of the data so what exactly does this tells what exactly does this sense I'm just just going to go over here and then tell you about that so over here if you read read this out as typically non robust where there is no elimination of the data to calculate covariance between two features but what exactly covariance is covariance is a nothing but it tells you the relationships between two random variables so for example you have two stocks which is S P 500 and ABC Corporation you're going to assess the directional relation what is directional directional relationship so if SP is increasing what is happening to ABC Corporation so what is the covariance between SP and ABC so it tells either one is squeezing or one is decreasing so basically this is the data is an example and it's extra and the variable is y random variables over here both are increasing that means they have a positive covariance this one it tells you the direction of the relationship not like if one is the wood then what is happening to another so if both of the stroke decreases then we have certain things so that's for the covalences it tells you the relational uh it tells you the relationships between two random variables okay uh cool so I hope let me just ensure that yeah it's running cool so go variant shows two videos between uh how to move with respect to each other and blah blah blah it just tells you about everything over here okay uh no that's known robots which means that we don't eliminate any data okay which is most common way coefficient so coefficient is coefficient is nothing but coefficient is nothing but for constant which is the bias term which is beta0 is 6.978 which is the average when X is your X is equals to zero and TV is 0.000.055 and constant for the Santa error for the constant is 0.323 which is how reliable how reliable our uh our constant or or the our how accurate or precise our estimate is for bias term and how to estimate how good I estimate for TV which is pretty much very low for both of them and then you calculate the T statistic which we had a talk which which we had a talk which is T statistic which is a look at each statistic for constant like how because this is what is it it tells you it tells you the a signal the significance of the individual parameters it it tells you this it it tells you for individual parameters okay so we just calc which is calculated by dividing the coefficient by the standard error of that coefficient so how so basically that's the T statistic now what you do you compare the T statistic so this is the basically you have the pvalue uh you have the pvalue for each coefficient which measures now if uh and you can see that both of the P values uh both of the P values are close to zero which means that they are that they are pretty nice you know they are less than 0.05 and they're both significant to the model okay we have Omnibus test which test though now now over here we are done with a couple of things which is the testing significance of a model now what we do we have the Omnibus test what exactly Omnibus test does it tells us that we have absorption which is normality assumption in a linear regression which tells whether our data follow our residuals are normally distributed or not so Omnibus test statistic gives you exactly the uh to that helps to validate the Assumption that's why I really like stats model API because it tells you everything which you need you don't need to calculate further on so Omnibus statistic which gives you 0.013 and the probability which get the prob the P P value which is 0.993 which suggests that residuals are normally distributed how can we say that they are normally distributed so over here which which which you can see the the the the the probability value the probability of Omnibus value is the pvalue associated with the test statistic which was calculated okay so t t statistic you know T statistic statistic which we can calculate and other things so here the probability common Omnibus is the if your if your value of the prop the probability of Omnibus value types of the test is close to one that means our likely normally distributed and there if they're less than that they are if they're less than 0.05 which means they're not normal distributed right so over here which is 0.993 which means that are close to one that means it's a normal distributed another one is Durban words and test which was used for testing the independence of residuals whether one into one one error doesn't not affect another which we had a talk so it usually ranges between 0 to 4 and the value indicating two which means they'd have they they follow the assumption but less than they are you know uh positive and greater than they are greater than to their negative correlation right so um so uh but so in this case your double Watson test is around two which means that they also follow the independence assumption now we don't need to check for too much and then value close to as you can see that you can uh read over here and then we have another test which is another again for normality assumption which is Jack beta test you know there is a lot of pronunciation uh for the same like harik beta you know so it depends on what you use so you you have another test you don't need to understand how exactly the mathematical working was setting it's just for statistics purposes so which tests assumptions of a normality of the residuals in this case your beta statistic as you know first first you have a t value and then the critical value right so first of all calculate the beta statistic which is this much and then you calculate the probability of that which is from the table distribution which is 0.979 which says that residuals are normally this what is the probability which is the so so that we have the problem so that we can compare with the significance right Alpha value otherwise how can you compare this this is this is not a probability convert that probability so we suggest that the residuals are normally distributed how can we say that you can read that how can we see that we have actually written you everything which is over here uh that that also tests suggests now now we have skewness is skewness means that skewness means so how can I explain your skewness is skewness is a statistical you know uh I think that you have to enroll in my course for learning about all of these things but but basically um but but basically skewness means that excuse suggests some normality assumption okay uh so basically it has also also helps to calculate our normality so over here in this case is minus 0.08 which means the residuals are close to being normal distributed I suggest you to you know have a look at in detail about skewness and kotoris because this is something which you have to and it won't indicates the validates our normality assumption over there but I suggest you to explore by your own because something is takes a long time to explain that is that that I explained in my probability course cool and this is the condition number uh this is not important you can actually ignore but actually condition number if you if you want to know about like uh and so you have a condition number over here condition number what is it states it represents the sensitivity of the predictions of the model predictions to small changes in the value of independent variables so how much your model prediction will change if you change the training data and then retrain the model okay so if you reach if you change the training a little bit and then how much that will change so higher the higher the condition number is better uh sorry uh small kind of it's a small condition number indicates that the model is a relatively insensitive to changes which suggests are relatively stable okay so they are not sensitive so basically smaller condition number which means that even if you change the data your predictions will be remained stable it will not drastically change I will not trust it it will not drastically change so I hope that that really helps um that really helps now uh I hope that this this provides a very nice explanation about our condition number and the summary of everything now what you can do now you can have now now we have now we are done with the interpretation now I can actually go to something known as buy your own which is explanation and explanation consists of everything which we had talked on a lecture like I had uh of over here again what exactly the report is what is the small summary of the model over here I have interpreted I've interpreted every the important you know tests and everything I've also validated the assumptions I've also validated the assumptions if you you can also go on uh images and then see figure one figure two figure three and all which which will give you a very nice as uh for for every assumption by your own okay if you want you can actually tell me in a comment box if you want to if you want me to develop for it but you can see that I've written a very nice blog for this so that you don't need to worry about too much and also just what are the further improvements or limitations which can be listed out there for you to work on and that's pretty much it about this project I'll be catching up the next project till then bye hey everyone welcome back to another lecture and section which where we are going to eventually talk about one of the last projects for the midterm so on regressions on on regression part and then we'll move on to classification projects so basically what this project indicates this this project says that that in this uh in this we'll try to predict cancer mortality rates for the U.S countries right so what exactly this uh what exactly the problem statement is ETC so here's the walkthrough of the project which will go through it first we'll start off with the introduction which is this lecture so we'll start off with the introduction lecture and then give you some motivation to start with it and then we'll uh go through through the Rick what are some of the required installations and what is the virtual environment setting up the virtual environment setting up your workspace Etc and then we'll go forward in understanding exploratory data analysis so inex in exploratory date and Analysis we'll try to do some you know a Basics Ed of the data set and we'll try to understand uh more about the data and then after that we'll go through uh preparation of the data what is preparation of the Dead data means which means cleaning up and then feature engineering Etc and then we'll model our work uh where we'll Implement our linear regression model to predict what is the death rate for that particular country in that particular country for the different different states I hope that makes sense now now uh what I'll do is make you make you make you go through all the the first part of this project work through is Introduction section where we let's talk about uh what exactly we are going to solve in today's lectures so basically you can go to the uh given link so Wireless regression challenge so you'll be having the link in the readme of your project and so if you go over there so basically this is a challenge listed by data.word which is one of the good challenges which I've ever seen and basically this challenge says that that you want to predict that your task is to build a model to predict what is the rate of cancer mortality what is the rate of the the person dying with the cancer for U.S countries okay so if you go and see the data so the data the data dictionary where you have several independent features and then you have one dependent features so what is let's let's talk a bit about on this is you have the data dictionary let's let me show you first of all the data the data which you'll have is over here this is the data which is which is in CSV file okay so you have a total of 3047 rows and 34 columns in which in which there is one column which is a Target variable so let's talk about what is which is the color which is Target variable which is Target death rate with the target is the death rate which is a dependent variable it says that mean per capita which is what is the death rate uh in per capita which is 100 000 cancer mortalities and then you have again the same thing average mean number of reported cases of cancer diagonal diagnosed annually so you have these features which is average counts if you go down you see average count and then you have average dates per year which is mean number of reported mortalities due due to the cancer average death per year then you talk about incident rate mean per capita and the median income per country then you have population of that country percent of population country so you have the features out here you have the features out here and every feature has its own description listed out here so I suggest without doing anything go through the understand every feature and try to first of all understand what exactly each feature is trying to tell you okay um so basically the the years of data it contains is from 2010 to 2016 and between that you have several number of information out there right for example you can just go and see um some birth rate which is the number of live births relative to number of country women in the country which is the person of married households person of country listening who I don't find a category which is not white so basically these are the information for a particular observations okay so let's go with that example so first example is where your average Union count is this much and then you have a death rate where given all of these information you have to predict this particular information which is the death rate so where is the death rate um death rate yes so you saw you have to predict this target variable so this is this particular is the target variable where you have to predict this given all the information out there given all the independent features so these are independent features and you have a one which is a dependent feature I hope that makes sense now um so what exactly we will do let's uh let's go to this now given the data so basically we need to predict the rates so cancer mortality rates for the USPS come countries out there right um which is nothing but how many people died uh we have to predict the death rate using of uh death rate which will happen uh using cancer okay with the cancer diseases so build the task is to build a multivariate which is multiple um Lane regression model to predict the death rate for the US countries right so what what should be the deliverables when when you when you will complete this task what should be your following deliverable it's like what you're going to deliver to the task master whoever is checking your assignment is first is yours should be model equation where you're where you should have a model you should have a statistical software where you're telling about R square mean squared your code file and the model Diagnostics whether your model follows the linearity Assumption Independence assumption Etc and then your interpretation of the model and there are other factors to also consider in this okay so I hope that this this will tell you a lot and now I hope that this gives you good sense about what exactly we have to do now we you you may be thinking we'll be covering each and every part as you might have already noticed that we have covered this part which is assessing them which is a model assumption test as well as the evaluation part so these two things are in your assignment however we'll do the big chunk of portion of this project to help you better understand mainly the data processing steps and all so I hope that this gives you good sense about what exactly because we are going to do um so I as I told our our aim we have come we have given you the short introduction about the problem statement problem statement is nothing we have to predict the death rate per me uh which is which which is the uh death rate and what is the forecast using which happened with cancer mortalities okay now uh you might have some issues like Understanding Variables so here's you have to do if you're having a very uh height a hard time and in understanding cancer mode uh whatever the independent features then suggest you research that particular term online on Google and then you'll be automatically understanding what exactly you need to do right but as of now we want to predict what what will the death rate in that particular country given that in that sort of information so basically if you go out here if you go out here you will see a geography in that geography in in that geography you have the geography as well as for for that particular place and for that particular year you have all the information available I hope that this gives you a good sense what exactly the problem statement is now in the next set of lectures what I'll try to achieve is I'll make I'll start off with a very basic introduction of what is the require required installations which you need to do what is environment variables so we'll start off with the basic introduction to environment variables and then we'll move forward accordingly hello everybody um now we'll talk about one of the theory theoretical concept today is a bit about on programming basis but this should already be covered in your python if but if you don't know I widely use this uh virtual environment uh in this project so if you if you don't about virtual environment at all uh this lecture is for you maybe uh but if you don't know completely python this this lecture is not for you please complete your python stuff click in Python you you're taught you taught in these all these things so please make sure that you have some idea what exactly these things are but if even if you don't if you have basics of stuff not done then you will be able to solve things up cool so let's get started uh actually is first is I will to what our agenda today to talk about a virtual environment variables in Python our agenda is to talk about this especially in this particular lecture and uh first of all let's so let's talk about uh what exactly they are right so virtual environment in is an isolated working copy of your of your python version that allows you to install any sort of packages or dependencies which you have for a particular project so I know it's something which is a very big definition let's let's say one thing you're working on Project number a you're you're working on Project number a and also you're working on Project number B project number B you have a python you have python installed you have a python installed right and then in that python you have several libraries installed such as numpy installed panda is installed matplotlib installed blah blah blah so in that python version you have several dependencies and packages are also installed right and now now now you now now you have this package packages and all so your project a also downloads in your base version your pro in Project a you download your whatever first of all down download your package is required for project from these um which these and every library has its own version right they have the V1 V2 V3 Etc and Project B might also have the uh might also have some some sort of similar libraries or different different packages installed okay but but but you may be thinking but over here but over here project a is utilizing my it might happen that project a might have different versions requirement different versions requirement and Project B might have different versions requirement right that's one of the way to think about that if your project a requires if you project a runs on numpy 1.0 then project if Project B works on numpy 2.0 then that's an issue right you you cannot uh run Project B on that first library right on the other hand you might be having questions like um so so so so we need uh so you you may you might be thinking how can we solve this there's one way to solve this is that we can have every project separately so we can have a project project number a we can have a project number a with with its own python version with its own python version with its own libraries version so its own libraries versions with its own dependencies right so for example numpy can be dependent on several other uh packages to work right pandas can be dependent upon numpy matplotlab right so a single Library can be dependent upon several other uh packages so one way to think about why do we need first of all what is virtual first of all the the problem which arised that a particular project project number a might have different requirements of your python version they require 3.8 but Project B requires 3.9 and they might require several other other set of versions of your libraries and maybe there there might be different different dependencies so for example numpy can be dependent on pandas and password but pandas can may not be dependent on matplotlab right so this is dependency conflicts which means which may can happen and Etc so what is the ideal solution we can have a a container we can have a container sort of thing we can have a container sort of thing or the variable sort of thing a variable sort of thing or an environment sort of thing for every project so we have two projects so project number a so we create an environment for that particular project number a and in that project number a you might install your own python version your own python version you might also install your own uh libraries your own libraries so now you can run that project a using this and whatever versions you have and then you make this another environment for project number B for a project B and in that project B you can have different different versions so you've given a numpy 1.2 number one 1.3 and then you can activate this environment if you're running this particular project you can have project C with different versions with different versions of your libraries or Python and all right and then if you want to if you're into runs from number three you can activate this environment if you have project number D then you can have several other versions and then you can actually if you want to run this project then you can activate this particular environment okay so this is one of the way to to think about it right uh so basically you can you can you can have a different different versions of a packages and different different for the different different projects for the different different uh environments and and and and it will and we have and we can create this and this will not affect anything on a global global means where you have a globe global winds which is which is out of which is the which is setting over here in that Global you have certain packages installed python installed on a path environment but in that a global one you have several of the virtual environments available right there are different different tools that use to create these sort of environments one of them is when another one is conda another one is uh one is when another one is conda another one is uh let's let's say for us for a second example uh pinev so we have these three where you can simply create your version bar and environments out of these tools right so let's talk about a bit about you know a bit of reasons why exactly we need a virtual environment in our daily life as is something which is super important so uh there are several reasons which we might use for virtual environment the first one is package package management so this is the first one which is in package management you can you what you can do you can install your packages you can install your packages you can install and manage your packages or libraries libraries for particular projects you don't need to install in such a way that it should be applicable on every project it can be for a particular project and this like you install it like you install you manage your project you you have a project and then you have several other dependencies so you have to only in practice this you can package this without affecting the project number two on which you're working so you might have noticed in my previous projects I always create a budget environment so that it does not affect my other projects and dependencies right another way is we isolate our projects which is another one is um is isolation another one is ISO isolation isolation means that if you have lots of environments you have lots of virtual environments for all for project number a project number B project number c so you can have these virtual environments so changes in the project number a does not affect the changes in Project number two even they if they have a same line because they have all installed separately they haven't all are installed separately by creating a folder so everyone has their own folders and in that they have their own packages so if every environment has their own different different versions or it may be same as well but uh one is that one does not affect others so in covid19 used to isolate people so in just way over here as well we isolate you guys okay now uh you might have another one is which is the another one is reproducibility okay so this is another reason of reposibility which means that let's take an example you run a particular project number a you run a project number a and that there has own requirements you know that has short should have numpy should have pandas in that project should have C born should have uh this this particular requirements all the all the technical requirements right so when you have when you package this up you can exactly set up in such a way that it will work on other systems as well you can also deploy to production so whatever you change change it over here changes the production as well so you can actually have a very good reproducible pipelines out here and I hope that this gives you good sense about what exactly uh you need to you know you should be able to but the reproducibility is not nothing but sharing of your uh project to others or deploying your project to a production that's why because it may happen that you have a global slate safe say for example of global environment and in that you have hosted project number a so even if a single dependency error occurs in the system which is running for users and production will fail right so you have the separately so that your model is also reproduction is contained in a single container or a one environment I hope that this gives you good sense about what exactly we wanted to talk on packages and all now what we'll do we'll try to get into bit of coding part so now we are compared to complete a bit of theory which which was needed before it now what we'll do we'll try to install our packages now and then we'll get started with a basics of you know exploratory data analysis and try to understand from there hey everyone welcome to this uh another video on what what you're going to do we are going to set up our project and make sure that you're also following you have to downloaded materials and all so you're also evolving to that throughout the course um so what I'll do today is make you a bit comfortable with um a bit of you know setting up the environments and installing required libraries getting it through the notebooks which we have getting it through the important steps out there right so let's start with that first of all I want to show you one thing which is a documentation of python weight where it says that we'll be using van we'll be using when to actually create the environment variable out here so um so now if if you want to create the virtual environment if so if you if you if you really want to create the virtual environment then what we can do we can simply have this maybe this one which is python mvnb which which is that so basically I have opened my UK when when you unzip your file when you unzip your file you will be having this stuff okay so now you open your terminal you open your terminal and go to that folder so when you clone that when you when you when you'll be downloading the zip file when you'll be downloading this ZIP file open that zip file and open go in that folder so it should be easy right so CD and write the go go into this file and open your vs code in it right and once you open your vs code um write python um Dash M and when it's in the case that we're going to create a virtual environment and let's name that as a OLS regression challenge so just write OLS which will be the name of our environment and then let's click on enter and that creates and that creates uh we noticed do you want to select it for the workspace folder yes I want to select it for workspace folder now we have created a virtual environment we have created a virtual environment named OLS where it says that you can see that a new folder has been created where all our files will be installed or our packages will be installed now it's containerized in a particular environment so now project will only run if that environment is activated if this environment is activated okay so how can we act how can we activate this this is this is pretty much uh like like I'm I'm totally you know uh trying to make sure that how eventually if I'm if I'm uh you know in initial versions how I'm going to make sure that you're able to um get it so so basically uh if you go to the you know so if you go to the bin for bin file and then you have something which is activate which is something which is activate so you can either go there and then uh confirm it right either you can go there go there like CD and blah blah blah you just go there or you can just have a sets like let's say let's say uh you want to go at uh in the folder of CD OLS Ed OLS CD OLS eventually I I actually use pi and V to be honest because of my organization because in previous organizations they were using pi EnV but I recommend you to use when because it's just initial version of yourself that's why they come in to use that then I go to select go to bin and then click on source Source activate or maybe you can run simple simply the activate as well it should run very fine so now when you see over here OLS which means that now you're particular now you can run your project with that particular environment okay now you can go out of that however you can also write source and then you write you know OLS OLS and then you simply write our bin and then you simply write activate Okay it should work perfectly fine right so I actually gone and showed you this step by step uh solution of that right let me just zoom in a bit maybe it may be beneficial for you uh to notice which is out here so let me just zoom in a bit Yeah so now we have now we have activated environment now what we need to do now as I said now once we have activated our environment now what we have to do we have to um let's let's go through the uh project let us go through all the files right that's much more better rather than showing it like this so now you have the now you now will see something like this after every in after we have activated your environment so though so I'll recommend you to go to readme.md and it's in readme.md you'll find everything which you need which is what exactly the problem statement Etc now now what we need to do we need to record and we need to install the required libraries which will be going to use in this project so we'll be going to use this this project as well as we have some other libraries to install so we'll install that for sure so uh as of now we'll install these libraries frequent which which have created the requirements which says that these are the requirements for this particular project which means these are the libraries which are packages which needs to be installed for this project so when you will go to there and then what you can do you can simply add first of all which python so let's type think about which is the python which which you're using over here we are using the python which is created for this environment and then let's talk about which pip you're using using pip for this particular you can see that OLS bin and pep so pip is also for this particular environment now what we can do we can simply install pip install R Dash R requirements so what what this does what this does so it you you can simply name all the file name all the libraries with with the specified versions which you need specified versions which you need for that particular Library will come to that how we got how we created this at the end but as as of now you can simply have this pip install and this this indicates that we want to uh you can also install it individually but I was I have added in a file so so I can install it all at the ones by a single command which is in the requirements file and then it goes in the first install the first second third fourth all around the end and then simply click enter once we click enter it takes it takes a bit of you know um time to do that and then you will see that it is being all the packages are being installed so all the packages are I think uh being installed over here where they're installing matplotlib they're installing numpy they're installing scikit loan they're installing stats model and then we are done so we got a bit of warning over here what that warning says that that the version which we are using the paper version which we are using it's not upgraded so let's upgrade our pip version so let's let's upgrade our pip version so we can simply add Python and then use this particular command so it is saying that you used to use this python which is a python the the absolute the the path of the Python python is over here but you can actually absolutely use that python because you have just checked that python the which python the python is like the environment python only and then you're installing uh install and then you upgrade the PIP you simply upgrade your pip and then click enter now it upgrades your pep to the latest version which is 23.0 now that warning can be removed so I hope that this gives you good sense uh about what exactly we wanted to wanted to do now we have the working we have the working uh available things out here where we set it up our project now we are we now we are ready to start off with our explanation of the project so uh if you if you open our notes if you open our notes we had this notes out here we had this notes out here let me just move it a bit you have the introduction we have done with the introduction all the setting up everything however we haven't covered this setting up get repository which we'll do at the end node right now so now we have the required instincts now what do will now we have the data so when you when you when you open this data folder you have you have a lot of things available out here you you know you ignore this these two as of now because this is something which will be built in the project only you will see that is the data and this is the original data right so I suggest to go in this data and this data you will find out the data which is available out there so let me just open that so I think uh when you open this in CSV file it will look much more better I have already shown you in the introduction session very very if you go over here you will be having cancer regression dot CSV you will be able to see the data out here okay now over here the first step is will the first step we should see a very simple way where we had a class where we are taking out adjusting the data and second what we'll do we'll start off with understanding our data okay so I've created a very nice notebook where we did a very extensive Ada dealt with you know outliers we deal with a lot of things out here so we'll cover that in reread it so I hope that and then we'll after the after the after the Eda will cover some preprocessing techniques we'll cover some feature engineering techniques then we'll cover some preprocessing steps Etc I hope that this this guy too super well let's catch up in the next set of lectures to understand the Ed and everything out there so everyone welcome back to another lecture especially on uh some theory part which we'll try to discuss you know some I will start off with this basic Eda and then with the basic idea we'll try to figure out what is the possible you know uh data uh cleaning techniques and data preparation techniques which we have to go forward with right so let's let's start talking about actually about all of these things but before that we're going to review a quick concept which we have to go through is about quartiles right you can read about deciles percentiles in the previous in the lecture start in the probability and statistics part I would like to go through only one concept of over here which is quartiles so what is quartile quartile is something which basically quartal is a part of quantiles where we divide our data into some sort of distribution which are equal sized right or subgroups okay so we divide our data into subgroups right so we have already studied that it divides our distribution into four equal parts so quartiles divides are in distribution to four equal parts uh and there are three quartiles so here assume that you have this data this data so to assume that this is a column two four five six seven eight so divide this data into equal equal parts where you have first first second third and fourth so you'll see that the all are equal equal parts and this is the quartile number one this is quartile number two and quarter number three so quartiles are at the cuts where we divide our uh equal parts right so the first step of is for taking all the quartiles so what is quartile quartile is where we divide a distribution four four equal parts and quartiles are like Q one Q two Q three are the quartiles where which are present at the cuts and they first of all we have to put our data in a ordered format it should not be unordered otherwise it will not work right over here over here you have for your quarter number two is the median quartile number two is the is the medium so let's talk about quarter number one and quarter number three in great detail uh so basically you're seeing and you you're seeing in front of you that you have a date that your distribution of your particular data right and this is hotel number one where and this quarter number two and this quarter number three so given this particular information q1 is the central point between your smallest value and the median of your column okay Q3 is the median the Q3 is the median and Q2 is the highest score so basically what given this example so assume that this is this this is an example of your marks you got right this is this is an example so we put in order format now you take out q1 by this formula you take out q1 by this formula you take out um Q3 or sorry uh wait you take out Q3 you take out Q3 which is the middle value by this formula and you take out q1 Q2 Q3 which we have already taken out right and now what q1 tells you q q 1 tells you that 25 of this course are less than 68. so if we have the quarter number one if we have the quarter number one which which is at the position number five which is 68 so we say that there is 25 percent of the values there's 25 percent of values which are less than 68. quarter number two tells there are 50 percent of values which are smaller than which which are smaller which is which are smaller than um you know so whatever median is there so there is 50 value which is smaller than this particular median like 50 values like this 50 and the fifty percent who are greater than this so that's why uh Q2 is called a median which is the middle value we're 50 greater than this and 50 smaller smaller than this you have Q3 where we say that Q3 which says that which is a 75th percentile or you visit where it contains the top 25 percent of this course which are greater than 84 right so basically it is saying that uh whatever is called which are greater than 84 that's what the uh greater than 84 are the uh 25 of the values which are 20 last 25 top 25 percent of the values right so this is also called the 75th percentile where it says that 25 percent of the scores are greater than 84 and 75 percent less than 24. so basically out here out this we say that there is 25 percent of the score which are greater than 20 which are which are greater than 84 because that's the quarter number three for this particular example greater greater than 20 84 and there's 75 percent of the examples which are less than this okay again I'll repeat quartile number one says that the 25 of values are smaller than this q1 whatever the value over here 50 of the values are small and and why once is the 25 of values are smaller than this particular value and 75 sorry and 75 percent of values which are greater than this particular quartile and you how you can take out you can take out by this formula okay I hope that you're that you're getting one way to interpret it q1 is the middle point q1 is the middle point between the lowest value in your data and the median of your data right median of a data that that that is the because 50 what is the half of 50 because the medium is called the 50th percentile so what is the half is two which is a 25th so you have you have to take the median this is the central point between both of them and Q3 is a central point between median and the largest value maximum value in your data okay this is the one way to interpret it another one which which you are going to talk today is IQR what is IQR IQR it tells us how far apart your first and third quartiles are okay indicating how spread out your 50 of your data is so what it is telling so what it is telling that um um what it is telling that you IQR is the difference between your quarter number three and quartile number one okay where it says that because most of the most of the data lies between these two right because most of most of the data Lies over here right so we're gonna take out the range how how do you take out the range first of all is the simple range is taken out by maximum minus minimum right but that is that is that is that is um not resistance to outliers out yeah we have we have a talk we we had a talk please see the problem lectures please again I'll repeat it is if we use our formal range where we take out the maximum value and maximum and minimum value then it can be told Liars y you can see over here which you have listed it out for example you have this data out here right now you have portal number one 3.5 quarter number two six and quarter number three eight okay which are the cuts so what so we can say that there's 25 percent of the values which are less than 3.5 fifty percent of values which are less than six and eight um with 25 of values which are greater than negative okay so so assume that assume that you can take out IQR IQR which is which is the quarter number three which is the which is the range of the range of the range of the values where your most of the data are present okay where your most of the data present you should think logically as well in this over here it is we are taking out the range of this particular where are most of the data line right where are most of the data lines so they are not likely all outliers outlier are the one where they are out of the distribution where they're out of the distribution right so IQR is the it tells it tells it tells the range it tells the it it it describes the range between these two uh quarter number one and quarter number three where it says that how much your data is um uh you know spread out just like the range where you talk about number three minus quarter number one because we just take out the range of this particular part of the data that's why score number three minus quarter number one V2 so you get 4 and range S7 but why we don't use range why we don't use a range because let's take an example we introduce 100 over here 100 over here so 100 minus 2 which is 98 right but but we if we take a but quarter but quarters will not change over here the portals will not change so our IQR will be resistant to even these outliers but range will be affected right so that's why so that's why where are most of the data are present we take the range of that that's what the interquartile range I hope that that gives a good good sense of what exactly we wanted to have a talk on now what I'll do in the next uh after the review of this we'll talk about a very nice stuff which is box plots so uh let's go into the next lecture and talk about something known as box plots so hey Vox welcome to the another lecture on box plots so uh eventually we have I think we haven't had a good talk on box plus and all but it's very easy relatively easy if you know about quartiles and all right so assume that assume that you have a data assume that you have a uh data out here so let let me just first of all give the title as box plots right box plots I know my handwriting is not good but that's something which you have to bear with and we'll extensively use this box plot to do video night to to do a nice thing okay um so basically assume that you have this data 18 34 and then you have 76 and then you have 29 then you have 15 then you have 41 46 25 54. 38 20 32 43 and 22. so you have this data available out here now the first step as I told the first step the first step is to sort it all right in an order so when you sort it in an order you'll have 15 okay let me let me use another pen um assume that you'll sort it on another order which is 15 18 then you have uh 15 18 is 20 then you have 22 then you have 25 then you have 29 then you have 32 and then 34 38 41 43 and 46. right so this is our assorted data now so now once we have the sorted data now um and then we have 54 sorry uh then you have 50 equivalent 76 so now we have this sorted data now what we need to do right so now once you have that sorted data we're going to take out the quartile number one quarter number two and quarter number three right that's our goal to take out so how can we take out this as I said divide your data into equal equal parts first is divide your data into equal equal parts so first of all let's find the middle value so middle value so first of all let's let's count the number of values one two three okay so let me just one two three four five one two three four five six seven eight nine ten eleven twelve thirteen fourteen so you have an even number of values right so that's something so basically I've already conducted it so when you take out the median of even then it then then I think you should tell me the formula in your Discord so go over there and tell me the formula for median you should you should be able to have already covered this um so the the median will come out will come which which is over here which is the quarter number two which is around 33. so the values over here are the the values at this side on the left left side are the 50 of values which are smaller than 33 and the value switches over here are greater than 33 which is the 50 of the values I hope that that makes sense now so now once we have this quartile quarter number two now we have to go forward to quartile number finding one so quarter quartile number one we have to find we can use that formula to find the index I have already used it so this particular is the quartile number one this particular is quarter number one this particular squatting number one right and now and now we we want to find the Portal number three which is over here which is over here which is volatile number three so now you see the that we have divided our data set into four equal parts one two three and four right one two three and four so we have already divided our data into four equal parts so now your portal number one is quarter number one is twenty two quarter number two is which is the median which is 33 and quarter number three which is 43. so you have fifty percent of the values which are smaller than twenty two sorry twenty five percent of values which are smaller than twenty two fifty percent values are smaller than 33 uh and then and then there's 43 or sorry 25 percent of the values which are greater than 43 okay so currently you have this particular example uh with its statistic we which you have now what what you want to do as I told let's take out the IQR as well IQR right uh what is IQR IQR is the range where all the data lies in right which is Q3 and minus q1 where all your data lies in which is 50 of the data which is 43 minus 22 which will give 21 okay so that's your IQR so that's your IQR now what we need to do now what what you need to do okay first is we can go forward we can go forward with taking out the maximum you know we can pick the minimum and then take out the maximum and then make a box plot out of it but for outlier identification for outlier identification we need to make whisker whisker okay we're going to make whiskers so what is Whiskers Whiskers gives you that a maximum a maximum and the minimum value in your data and a maximum and the minimum minimum value data according to the IQR range right according to the IQR range so it gives the maximum and the minimum value in the data according to your list value 21 according to this Mass so basically we have the maximum and then if you say if any Delta points go above this maximum then that's an outlier or any data points which go below this minimum then that's also an outlier so for outlier and the identification with respect to our IQR because IQR is resistant to outliers so we're going to take out so we have a viscous which gives us the um uh the the maximum minimum values which are also resistant to our data which which helps under the detection you will get to know just wait for a second second now we want to take out the maximum and a minimum value so minimum value can be taken out by the minimum value can be taken out by Q um I think we can take out q1 minus 1.5 times IQR right that's the minimum value that's the minimum value and then maximum value can be taken out Q3 Q3 plus 1.5 IQR okay so this is the formula for minimum value for taking the minimum value in the date of with respect to this out IQR range and this is for maximum now you may ask how did how does this came this is something which you can go forward even if you want to research how does this derived but that's a very general uh explanation behind this but that's not a scope on this my scope on this to tell you that we want to find out the maximum minimum values with respect to the outlier range if there was simply a range or something you would have just taken the minimum and maximum values right but that cannot be used as the outlier I'll tell you the at last why why we cannot use the minimum maximum why we need to take out the minimum Maxima with respect to IQR so if we do the calculation to 21 minus 1.5 times your IQR which is 21 and then comma because that's for minimum and this for Q3 which is 43 plus 1.5 multiplied by 21 you get you get nine points minus nine point minus 9.5 and then you get 74.5 so this is your minimum value and this is your maximum value so if anything goes above this sorry if anything goes below this minimum if any data points go low go below this data point we say that 9.5 minus 9.5 if any goes above this this is called the upper limit and this is called the lower limit so if any any goes above this we say that that's thoughts on outlier so let's try to do with the help of a diagram so let's try to do with the help of a diagram so I'll make a very nice diagram right now uh so assume that you have a diagram over here you have the diagram over here a number line so let me just make a diagram like this and let me just use another pen yeah so we have zero then you have 20 uh 30 40. 50 60. let's make a let's extend this uh 70 80 right so you have time so you have this num num number line out here now now basically what what was our outlier what was the outlaw our outlier our sorry what was our quartile the quartile number one was 23. number two was what 33 and quarter number three was 43 so it is slightly over here so you simply make something like this okay I know this something is not great but this is this is what it looks like and this is your portal number so now this particular is quarter number one this particular is quarter number two and this particular is quartile number three as it makes it it is 22 it is I think uh 33 and this is 43 okay so you have these three outer quad uh quartiles as of now which I've already seen now we wanna we wanna now one thing which we can do we can simply say that something like this we take out the minimum and a maximum and then just add it for example in this case the minimum is in this case the minimum is a 15 so we can have over here you know we can have like this minimum case and then what is the maximum maximum is 76 maximum is 76 so we can have uh something like this okay we can have something like this but it but I'll tell you why it will not work first of all you see that 76 is out a bit bit out of the distribution is bit outlier just by seeing it right so so I'll tell you why why it will not work first as we said as we say that IQR is resistant to outliers and IQ represents the 50 of our data so if we take out the maximum minimum value using IQR that makes makes more sense because we'll be able to identify where are most of data lies so if any of this if any of the data goes out of that light spreading out we'll consider that as an outlier right so now so now I think it will makes more sense if you just try to now mix and make make it more sense sense sensible yeah so now we wanna we have this minus 9.5 we have this minus 9.5 where we say that where we say that we have we make a whisker we make a whisker we make a whisker which is minus 9.5 which is minus 9.5 so and then you have the 770 I think what was that yeah it will be something over here so anything anything which is uh now now anything which is which this is for this is for the maximum it's for the maximum value and this is what the minimum value okay so maximum and this is for minimum value okay so anything anything any art data so how can we use this to remove our data how can we use this to remove outliers from our data so we say that anything below minus 9.5 anything which below minus 1.5 is outlier ending above minus above 74.5 is outlier because this is the distribution this is the range of our of our maximum data where are most of the data so anything above this so if we see our data anything below minus sign to 9.5 there's no anything below my n minus but there is um above 74.5 which is 76 so we simply say that we consider this particular thing as an outlier this particular thing as an outlier okay this particular thing is an outlier this particular thing is outlier and we remove this data from our plot right from our data so we remove that outlier right there you know the linear regression that other that lots of algorithms that are sensitive to outliers okay so you have to remove these outliers we'll we should study later on as well so we'll remove this outlier by a box plot where I'll some summarize the concept you take out the you take an order you take out the quartiles you take out the IQR you take the maximum minimum value by this formula by this formula which is which is out here q1 minus 115 Times Square IQR Q3 plus 1. maximum is for minimum anything above this is bad anything above this is outlier anything below this is outlier I hope that makes sense let's catch up in the next lecture to actually see the Practical effect and how we actually do it to identify outliers in your data set hey everyone welcome back to another video um so now what we'll do we'll start off with you know Eda stuff and uh processing stuff which we eventually need for a project and I'll make sure that you understand each and every Concept in great detail so you can actually read all of this thing which which you have done done it for you but let's start off with uh we have we are done with the injection of the data now let's start off with this file don't know what is what's this file don't worry we'll figure it out the first line of the code states that the first line of the code says that we ingest the data from our data ingest file so it goes in data and just file it ingests this class so if you click over here if you click over here you'll be going to this ingest data class and over here in this class you have the init method which is a Constructor and then you have a method which is get data which will where we ask for the path of the data where the data is present in your folder and then we read the CSV if it is a CSV file we read that okay and then we return the data frame right so let's see it goes and changes data it's it just Imports that class then it goes in processing data it goes in processing file and then it imposes the following columns which we'll take a look in a bit you have to just wait for a sec right which will take a look in a bit and then you have for from future engineering we are import where there is another feature engineering file where importing some functions which is important to us as of now right which is important to us as of now right so now you see over here we have ingest data we have ingest data and it's we are uh we are you know what what we are doing we are simply um instantiating a class and then using that class as we said we are using the method of the class if you see that method methods get data over there so using the method in just database is the object which is the blueprint of the class we say that we will get the data from the following uh from from the following folder so we go to OLS we go to data and then simply copy the relative path of it right and then we import that and then now once we have the data now it's ready to start with some of the processing which is required as of now but before processing uh what what I usually like to do I usually like to you know go through all the basic data you know see how how's the data looks like you know see some of the missing values if if I have any and talk about distribution right so let's let's before going on the distribution one let's before going on a processing one I would like to take a time to explore a bit of you know explorated data analysis and Analysis and all so what's what was our project our project was to identify the number of debts which was due to the cancer in a specified population for over a period of a Time typically one year so mortality rates are nothing but a number of deaths are good for a period of time in this case we have we have given already in this competition you have to build a multivariate least squares competition to predict the cancer mortality rates for U.S countries right the first step is of course we are going to import the libraries of course sometimes I have imported in a bit between as well so I'll do it as we go along so we import our libraries as of now we need pandas and numpy we pfpd as an ally so that we don't have right pandas dot every time to use Panda so we have PD where we can use this PD now one thing that you which would do using a way we are not importing that class just for sake of you know Simplicity I just added it you know we can give it to any people which is for Simplicity however we can use that we can simply import the you know from data from data ingest from data ingest and then we simply import the simply import the ingest data file and then we sing assembly maker you know um interest data which is I'm just going to comment it out you know ingest data which is in Jets data and then same simply say key uh data is equals to ingestinal so you simply have this to actually import the data but as of now just important that because of simplicity so that you you could be on the same page rather than going over this so if anyone is seeing ADH they should be able to understand it very well so now once we have the data so now let me start running it so before that I would like to just tell you that we have to over here you have to activate your environment in jupyter Notebook as well in in uh in vs code we have this we have this select selecting of our kernels but in vs code you can simply write you can simply write like this a source and then you simply have the following command which we have used earlier so now you see that we are using source and then it is activating it so you can simply use this as well you can simply use the this as well source and then use simply give the activity it will be able to activate for that particular environment however you can change the kernel as well or call in your vs code if you have issues if you're having issues over there please let me know I'm just deactivating because sometimes you know conda I have also a conda installed so I have to um it's my system thing which I have to deactivate it doesn't really matter it is a base thing but I deactivate for a safer thing but as of now you just simply Source then you have uh which which you have your source and then you simply activate it okay and then also this uh base base also gets activated but it will it mean it may be not Vision but but it is not going from here it is my system things I don't know what's what what happens but I think I need to uninstall this Honda but never mind on that but uh you can you can first of all will activate that environment whenever you enter the project First Step activate that environment because that's what the project right once you have activated the environment now you should be able to run it so let me run the first first cell I usually use vs code because it's much more simpler so it says that for vs code we are going to download VR4 for this particular environment we have to also download the IPI ipinel IPI kernel however however if you're using vs code you should you don't really need this okay you don't really need um to download every time but I don't know but yeah for free score we have to now it runs now it runs now you see that it takes some seconds to run it and then we simply say achar now let's let me import my data so this is what your data looks like this is what your data looks like where you have the 34 columns and five rows where it where we are asking to show the five columns you can also specify on a series 10 or 5 it is just for the top five columns is a target variable the death rate in that particular span incident rate median income rate uh Power type study case study recap bind dink which is and media range so you see that this is something unusual you have to work on this so I'll come back like it's something you have Nan values OMG you have missing values available you have you also have you know um what you also have if you've explored the data if you explore the data you also have you know over here which is the English which is the categorical values which is this text value so how we are going to deal with that right missing values categorical values in in type of interval in type of interval type of thing how are we going to deal with this because we cannot just view it to them because model will make because this this acts as a string right now so that model will not make sense so there's couple of things which have to take care missing values right the first is missing values second is over here which we see that this is something unusual which is something unusual where you are the lists in every written column and then you have you know um text variable okay object variable we can use simply data dot info to take out things out here to take out who geography is in a string and then you have enough all all values are float Target there is float and then you can take out the null and then it gives you good information so now let's take out so now once we are done with exploration you can also take out describe you can also write describe so um data dot describe and then over here when you do the describe then what it does for every column it describes some of the useful statistical analysis for average a n there are 3047 count with the number of values in that particular column what is the mean mean is 606 is standard deviation 14 and 16 minimum value in that column 6 25 of the values which is 76 where we see that there is this 25 percent of the percent of the value which is smaller than 76. 50 value which is more than 100 171 and 75 percent of the and less rest 20 and more than and so and 25 percent of the values which are greater than 518 or we can say 20 75 percent of values which is lesser than 518 all right and maximum value in that column is 385150 so you have now for every column list list it out here and then now now once we have the basic things and now what we do we simply go ahead and simply take out some of the ask because we have noted on three three things that bin uh that binwala thing and then you have Nan values right and the categorical so let's take out let's take a look what number of missing values which we have in which we switch columns so if you run this if you run this we say that null values we we put the null values and then we say data dot is null dot sum so what does this gives um uh this gives um you know when when you print out normal values this should give you know this every column with the number of a null values present out there with the number of present or not there are only three columns where your null values are present there are only three columns which are where your um null values are present so we say that you print the null value you print the null from from the null values from that series take the null values take the null values whose known as great greater than zero right so we simply print that and then we see there are three columns post ECT and over here so now now there are several ways to deal with missing values which we'll talk about in later videos so I'll give you quick solution in this but we'll also talk about there are several ways jnn computation simple imputed blah blah blah so there are several techniques for dealing with missing values with their pros and cons which will deal with in very detail that we have now once once we have this let's worry about other things let's let's start exploring each and every variables so once you have the basic you know missing values exploration of the data now let's go ahead and talk about how we can um uh now you have to analyze hnl variables in your data to further analyze your data so now you may be thinking hey ayush how are you coming up with these these things the first step is an in anything is to do a basic exploration okay have a template basic exploration where you talk about what are the number of there is there any missing values is there what is the data types available for every column what is the maximum minimum values what what values are greater than this odd value is smaller than smaller than and the next step should be to analyze each and every variable okay each and every variable out here that will give you a very good set of thing first is we are going to analyze our Target death rate we're going to analyze our Target variable right target variable in which we say that plot.histogram you're plotting the histogram bins should be 20 color should be blue and H should h of the S2 histogram should be black and then we say on xaxis label that as a death rate on yaxis river that frequency and then title should be histogram of the day three and then we show that Plot show me the plot right now that's what it is saying and I'll talk about what is the entity interpretation of this so when you when you actually take out the histogram of this target death rate you take out the histogram of this particular column A of this particular column this will result in this particular histogram so you may have high time in uh interpretation of it so let's talk about interpretation of your histograms the values which are grouped into bins along the xaxis so the values the values are grouped along with bins so you see over here you see over here on the values on xaxis they're grouped in bins they're grouped in bends so uh we will talk we'll draw we'll talk about the bins first of all uh as of from which you see over here in this histogram it shows all the threads which are date divided in the bins and yaxis on the in the counties you know the frequencies in that bill okay so we say that zero two hundred zero two hundred is the I think uh fifty two hundred not zero two hundred uh zero two hundred is the sorry 5200 is one bin it's 500 to 150 is another but over here you might have noticed that we have 20 bins as of now but if you notice it very nicely that is this the the the range of this the you see you see the mouse right now you see the mouse they are where I'm indicating so this is 20 you know 20 bits so we say that that in this particular bin what are the the number of frequencies in this for this particular bin what is the frequency right for this particular bin what is the frequency so that's what it is telling that it shows all the trades in xaxis with the bins over here we have 20 20 20 bins uh 20 20 and then another 20 right and yaxis is the counties in that bend so for the for this particular bin this will be the counties on WE simply have this 100 we have this this bounties for this we have this much versus 550 around for this particular okay so for for 150 to maybe let's take an example of this particle bill so we have this uh but if you have a type of thing so now this is the simple interpretation of histogram which you've already seen but but it is but now once you have the basic understanding of histogram what will how this will help us right first it help us to identify ourselves how this help us to understand a particular variable in terms of statistical format first it will help us to identify what is the center of the data what is the spread of the data and what is the verb is eventually what is the range and what is the shape of the data right so let's talk let's answer these three questions what is the center of the data but but before that lets to note something number line spams from the minimum value to maximum value the number line is broken into equally sized intervals you see that all are equally sized these are nothing but called bins covering the range of values over here it is 150 150 to this will be uh maybe we have 20 bands so in this particular we have all the values all the values which lie in this range page right which lies in this range okay um there will be where is the number line is broken equal called bins and it have the every win has the range of values in that a histogram shows how frequent a values occur in that particular bin so his histogram it shows how what is the how frequent the the range of values occurs in that particular how frequent on bikes is for this particular bin there is uh 6780 that is the frequency right so that says the frequency the height of each bar represents the number of values in the data set that fall within a particular bin and when the the higher you know the higher number of you know thing is and then you have yaxis which is the counter number though which is to be discrete which which represents the amount of data Falls in that frequency of course and the histogram make it easy to see which values are common and which values are less common or which range of values are less common in your data set okay yes now now once now once we have that let's go to some of the observations so I've linked the resource which is good for you uh first observation is it is slightly positively skewed data set so if it is not completely normal it is very very slightly positively skewed data set right and then there are outliers as well yeah so you see that over here you see over here you see over here you see over here say this these are out of distributions and this outliers nice so we got an idea this is an SQ data set or slightly and you have the outliers and this is also not this is not a completely normal distributed this is it is not completely normally distributed and is mean is greater than median in slightly positively skewed data set your mean is greater than median and you have three peaks in the data set so your buy model data where you have three peaks in your data set right for that particular range this is the high this is one two three these These are three picks that's why we say buy model data but the most important key takeaway which I'm going to take from here which I'm going to trans translate is the is we have this The Columns which is slightly secured we have the columns which is normally distributed we have the outlier in that column as well like however um we are not going to uh we have we we will just take care later on about the outlines but so that that is for this particular variable so we do this for every variables but that for example if a thousand variables how are you going to do it that something is a uh talk to talk on specifically but I hope that this histogram gives you a nice sense about what exactly we're going to deal with the next so next what we're going to do as we've identified we about lies we're going to use box plot to see whether we really have outliers in a great sense right so we're going to use that and then have a have a very nice understanding of that and for every variable we'll do the nice exploration and then we'll learn about outlier detection removing and lot of things to talk on you know there's a very lot of things to talk about so let's go and go to the next lecture and talk about that so everyone let's get started with our data processing techniques which we wanted to so you will find a file uh which is data processing test and data processing data processing test is a file where we are testing our all our data processing you know um our methods out there so we'll try to uh go through go through a step by step what exactly we are doing and why we are doing that right so first of all let's get started with what exactly things which you're going to use first we ingested the data we get the data we imported data using CSV file and then what you're doing you're finding the constant column so this is not appropriate for every case so basically this fine consists constant columns what this does this function takes a data frame and it turns the columns that contains a single value so if your data frame if you if your column has only one single value that's the very very bad thing which can happen right so it will remove it will find the columns which is constant right which has the single values which can indicate either of categorical variables or the variables which does not provide any sort of values to your um to your data right so we find the constant values constant which is that a column column set contains only a single value either a11 either 222 or whatever so constant columns you make a v we we make a list to empty less constant columns and we iterate through the the column state of range rate data frame dot column where we ask for the data frame as an input to the function and then we iterate to the columns and then we say that find the number of unique values in that column by using using unique method and then if that unique method is greater than one then append that particular column in this constant columns which are which which we are doing over here and then we are returning at the end the the constant columns right so that's the first step which which we are doing over here so it says the columns that contains a single value and then print out the constant columns and then let's try to run it so uh whatever what exactly what I'm going to do is Maybe go to run my you know uh interactive window so I'll just run my interactive window so we have OLS activated in this as well so you see our act in Virtual environment which is OLS is activated in this now so we have that let's wait for a few seconds then it should work really fine so over here which you're seeing in front of me is it is running uh the 30 lines of code which I've selected you might use Jupiter notebooks or you might run the whole notebook step by step the the way I like is interactive you can also go online and search about virtual uh Visual Studio code interactive notebook you will be able to find out so if if you go and then search out uh virtue sorry Visual Studio code interactive interactive notebook then if you go and see then you'll be able to see that python interactive notebooks where you can actually download each and everything and then run the run that thing interactively by select selecting the number of uh cells so now it says the columns that contain the single value is nothing so you have a so so you have a column which has at least more than one value so at least it has a good spread so that's that's something which you don't have to worry about sometimes it may happen that your columns can be a single value and then you want to take a look if your data set is too large enough now you go to columns which contain the thing a very few few number of values so I've also written delete constant columns so these uh this what what this particular method does this drops the columns which are constant but in this case we don't have any constant columns now we search for columns which is um with few unique values where we see that function takes in a data frame and a threshold as an input like what number of constant values and the return so for example some columns may have few and few fewer number of values in that column so we say that we what we do we simply say that we make an empty list and then we Loop then we iterate through all the columns you know date data set and then Define the unique values for every uh data frame and then we say if the unique value is smaller than threshold so for example you say that you want to take a look at the columns which has which has less than three less than three unique values in that particular column right so if if that is smaller than threshold then it says that append that column so you basically have the columns which has few fewer fewer unique column so it should also give you a empty slide which you're seeing if you print this out so I can just print it print this out it works like a Jupiter notebooks so it says that there is there is no such column so the even few unique values with a threshold 10 okay with that threshold then you can you can change your threshold so similarly I have several methods for as a template for you to make this work in your further projects like find duplicate the rows so it may it may sometimes happen that you have a duplicated rows as well so you can actually delete the duplicate rows by using these two methods where you first of all find the duplicated rows by finding dot duplicated and then you simply drop the duplicates by keeping the first duplicate so so you might have first duplicate second duplicate third where the second and third are the same as first then it keeps the force and drops the second and three right so you delete the duplicate Etc so you have couple of methods now here but but we are not but we are not using them so now you have the basic you know basic things which is just for us exploring things there is lot to left in data processing now we have all of this left now what what we'll do we'll do some or sort of a basic feature engineering or feature construction okay that's called feature construction rights feature splitting and construction this whatever I'm going to teach right now is called feature construction feature is splitting so let's do that in the next video so now what we're going to do we're going to talk about something known as feature construction and features splitting in our data so one thing which I would just want to highlight for you is if you if if you see our data which we have if you see our data which we have is let me let me just go to desktop and then let's let me go to my OLS regression Challenge and then you have the data now let's go to data and then let's open this data and then worry about two of the columns out here the First Column out here is our Bend you know you we have seen where the columns were in a in terms of interval format so you've seen build uh bending where it says that your column is an interval format 6194 so this is your lower bound and this is your upper bound right this is a lower bound and this is your upper bound right so you have this column so so it will it is of object type and the model will not make any sense out of it if it is left alone so what we need to do we're going to take the first element and you we store the first element in another column and take the second element and store that in another column this is called the first lower bound and it's called the upper bound right so what we'll do we'll split out this we'll split out the the values from this column into different different columns by making different different columns right so let's see how we are going to do it so we're going to use uh so first of all let me show you first of all what exactly that uh we are doing we are using DF data frame and then assessing that column and the assessing the first value from that and the first value is this if you see the type of it if you see the type of it so type of it is nothing but your string I guess so if you see the type of it it is nothing but a string so now you have that string now to make the column so that it should make sense otherwise it will not it the model will not accept it foreign so this function is made in feature engineering.pi file and in this what we're doing we are making a empty file bin dink where we are iterating in the in that column so we are iterating for I in that particular column so it will go through first value second value third value Etc and then first of all it will remove all the parentheses my point is that it will it will remove the parentheses and the square bracket so it is saying I to strip and then um and then this particular parenthesis and squared brackets so it will it it will remove it so if you see dot strip function in if you see dot strip function in Python if you see dot strip function I mean um dot split which helps to remove the characters from the beginning or the end of the string right so it strips out it removes the white space and any sort of values which we wanted to remove so it's play it removes the parenthesis and brackets in this case the parenthesis and brackets now we have removed the parenthesis in Brackets now we have this um and then what it does it split the string into a list so we can actually uh split that string into a list when where you see so it is it says whenever you see this column split that split that list split that string into a list buy this so when when when the when the push program sees this particular column it says okay this is the first element and then splits these two differently right and then what it does it converts the list to a tuple um considering that we should have a unique stuff and then it converts the converts individual elements to float right so it from the Tuple it Maps every individual to float so each and each individual is mapped to float and then finally convert that list to a tuple to a list and then you append that particular Uh I that particular value into this so if I could show you if I could show you what exactly each steps looks like its steps looks like so let me just show you each each and every step this should make much more sense I guess so let me just print each and every eyes uh whenever you actually uh go ahead and then see that right that should make much more sense for you um let me just go to data in chest state of test and then let's run it first of all we have to you know save that it will not work if we don't save it so we have to save that feature engineering and then just save it first of all and go to data test and then now run it from here to there in an interactive window it will take some time to run it so now you see now you see the for the first it was this it was this it was this then it then it converted into this particular thing that then it converted into this particular thing by removing all the parentheses by removing all the parentheses right now by removing all the parentheses over there so basically you can see now um where I was Data engineering so you can see over here we are first printing I the first printing I with the removal of all the values out here with all the values out here and then we are getting the value of I we are getting the value of I which is you know um specified value right which is the which which we wanted now you can simply say you can simply what what you can do once you have the value once you have the values which is remove all the processing stuff which is converted into a list which is converted into a list which you can see over here which is converted into a list now that list okay now that list can be added instead of those strings right and then what we do we say that we we make a new column data which is of lower bound make a new column and then we say that for I in that bend deck take the first element in that list take the first element in that list and then you in up in Upper bound we say that take the second with the zero the first element in the in the lower bound zeroth and the first one first so we say for every each and every values right we also make a column called median where we say may lower bound plus upper bound divided by 2 that will give us median and then we finally drop that column which was useless in this case so we simply drop this column bending right so this is how uh basically what you're doing we're simply taking this value putting this another in one column named lower bound taking this value putting this on another column main named old upper bound and then we are taking the median of both of this like this the first element plus second element divided by two uh and by storing that in another column that might be useful right so now this will start making sense so what we did we splitted the feature we splitted the intervals and then we used accordingly according to a statement and then we drop the columns which are useless so now we have the information which is good so this is what what we did now if you go and see our data which we have so let me let me just show you the data which we have you should wait for a second still opening that's very big to be honest yes and then and then we'll go to categorical to column and then we'll talk about uh categorical encoding in the next set of lectures so if you see out here that spreadsheet now if you see over here now if you see over here you have something known as upper bound and lower bound so let me just show you to you first lower bound so that lower bound have this particular values upper bound has this particular values and median is this right so we have we we removed that because that was we're not picking sense because of interval that was a string and model will not make sense out of it we made something like this which will make sense to our model right now we have something else categorical so now another issue which we had another issue which we had I just want to show it to you what issue which we had is this particular column where is that column uh data and then we're just going to show it to you that's much more better yeah so see this now we are done with this now you see the geography column in geography column we have the county and the state right County and the state so both are the same point right so can't we have can't we split this feature again and then have County in another column and state in another column that will make much more sense right so what we do we simply say we simply say County we split that with that comma because comma is there so we split we're using the comma and then we say that uh for for for every column in geography and then we say that take in in this case we asked to take the zeroth element because this will be converted into a list now now we take the zeroth one because it was splitted in the enter in with the help of comma so it it will be converted to list and then we take the first Score first first element and then we take the first element and store that in a county column and then we take the second element of which is of course worse because it starts from zero one so let's take the second element store that in a state column and then we drop the geography so now you have another column for kids app for counties and states right so that that is feature splitting and reconstruction this is this is the processes called feature splitting and reconstruction I hope that you got it very nicely and that's it for this right now um so what I'll do in the next set of lectures is actually talk about we have this something now we have the columns which is splitted now we now how can we convert how can we make use of the state and the counties and how can we convert that how can we make sense out of it by different different methods of categorical encoding that's also super useful for you to understand let's go back and let's go to the next lecture so hey everyone so now uh in this what I'll try to achieve is uh just what I'm going to achieve is try to give you a very nice and complete feature engineering techniques which is uh till the previous lecture we have completed this categorical uh sorry to the in geographical column we converted the geographical column into two sets of color which is county and state what I'm going to do next it is talk about one hot encoding which is um so you have seen the the the the the data where we had the values which is categorical so now we have the two categorical values which is this County and state in County we have some other end State we have some other set of values now we have to come convert this into some sort of integer or numeric that should make sense to the model right that's your your model should be able to interpret it right so now what we need to do we need to um uh you encode those categorical variables right so we learn about several other encoding techniques later on but we'll encode those category variables or try to make sense out of that English otherwise it will give at a very a good error because your model will not accept the English stuff it will accept only the numeric and you have to make in such a way that it should make sense Okay so so what you what exactly I'm doing it over here whatever you seeing in front of your screen is you have a categorical columns in that categorical categorical columns you are selecting if in that data table so selecting the data types which are of object and then selecting columns so basically what this category column states that this is a list of category columns whose object is data type so most of the categorical categorical columns whose data type is object right so that's what we first of all select the categorical columns and then what we do we had the one hot encoder you know uh instance we create an instance with the following parameters so let's try to go ahead and then talk about what is what exactly one hot encoder so I'm just going to go ahead and then talk about uh show you a very nice visual a quick uh understanding of that so if whatever you're seeing in front of me is my screen so see this you have the categorical vary variable you have a categorical variable which is color and then in that you have red blue green uh blue so as of now instead right so we in in this case we have a categorical variable which is color so what this one hot encoding will do one what will coding will create the three other the the the the number of unique um values in that particular column will be the equals to the number of new columns which will be created and now for this assume that color is uh for for example in ID number one ID number one we have color red where it says color red is present but color blue is absent green absent and zero a blue is absent right same goes with color blue and then when first article a blue is absent but you see the color blue is friend so that's why it's one so basically what you what what we are doing eventually is removing uh is is converting that into a dummy variable type of thing we're creating creating some sort of dummy variable for every values and then we are seeing if that particular variable is available or not in that right so uh I'll tell you a very nice example in there so you can also go and see the other set of examples so let us assume that you go and see this one assume that you have seen this example and in the in this example you have I land as a categorical variables and then for every categorical variables you have the you have the columns created for every means the uni categorical uh counts in that so Biscoe and then in bisco the first ID is one because it is present but tungsten is not present that's why zero in the first ID in the in the first ID right and see my mouse so in the first ID bisco is only present then a second ID at the last column dream is present and the third we have second tongue strain is present right so this is how we create new set of this this is how we convert this particular one column to the number of occurred to the number of a column which is equal to the number of unique values in that particular categorical variable and in that what and and how we do it we have the particular ID and then we see that ID is present in that particular column or not right so I hope that this makes sense I'll be giving you a very very nice example as well so this is what the M1 hot encoding will do which over here what is this parts is equal to true and then handle unknown is equals to true so what is this uh variables so handle unknown is nothing but um I think that there is a very nice set of yeah so handle and uh an unknown which is this specifies the way I think there should be some uh yes so when giving an input so when given an input uh if there is any unknown values comes in so ignore that for example for example in that categorical variable apart from this apart from red blue green you apart from this some other you know a voided color comes in but that's not present in your data so we have to it is asking to ignore that part okay and the sparsity we are seeing that as a false where we we don't have a sparse data right so now we instantiate then we say fit transform fit transformative fits and transform so basically fit the Transformer to X and Y uh which is a fit the data and then transform it like first of all fit the data like have the uh all the all the cat all the things out here fit the data and then transform it like convert that into the categorical variables over here right uh for all the category variables in your data set and you have one hot encoded which is the data set which is one hotter encoded now you convert that data into a data frame but and then you name that feature by ticket getting the get features names out using the one hot encoder um object and then using get features names out by giving that particular categorical column right you'd name that as a columns and then what you do you drop the categorical columns which are now you have the geography now you don't need the English column you already have the numeric converter new encoded column so you drop that and then you concatenate the numeric which which was there and then the official data set okay so I know it it might make sense to for you to maybe understand it but let me just make you clear with things so let me just open that OLS and then let's go to uh processed let's go over there it will take some time because it is a very huge data set now which is which is being created and I'll explain you every part which is happening over here so let's let let's just wait for opening yes so you're seeing in front of you're you're seeing in front you're seeing in front of me which is uh your first categorical column which is that geography that geography is now converted so now you see that we have County uh abelia County and that first ID it is not present so every unique ID of that particular County uh that's that every unique value of that County variable is converted into the columns now if they converted into a columns is now if you see the if that I I believe is present in the first row or dot it says no it's not present but if you see now this is there there are so many unit values that's why there's so many counties are created though so many variables are created and there's a downfall of it that's a downfall it but we cannot do anything that's for the one hot encoding says then you have state then you have you know State I think it it it is not shows full but you have a state and for State as well for state Washington States Los Angeles and Etc it is showing off as well and then you see that that it is it is mostly you see by zero but at some point you will see that there is one as well this this one as well where it indicates that that particular county is present in that that particular variable okay I hope that makes sense I don't want to repeat it once again so this is how it got converted this is a very huge data set which is being formed right there's a 1935 columns and only few rows right but we'll deal with that so this is how what a one out encoding is where you convert our numeric to this one hot encoding but but we'll take a look at other encoders like you know um there are other in encoders as well where we'll take a look at that and then now what you can do you can simply say that one hot encoding and then you simply encode the variables now you have that now you have also seen now you have converted the to the numeric now what you want to do you want to um you want to fill out the missing values so missing values in the data set however we'll be having another section of dealing with missing values but currently what I'm explaining you see first is first of all we take take the columns which which we need to drop right so here's my plan what I'm going to do if the missing values exceeds 50 percentage of the whole data then we say that remove that particular column right but if that is below 50 we'll say that let's deal with it with our uh techniques which we have so the first of all calls to drop first the variable contains the call columns which we have to draw so we select the columns from the data frame where there are null values and where the null value is the mean of that null value the in that particular column the mean is greater than 0.5 if that percentage is greater than 0.5 then we simply drop if that's what the particular columns with that particular column in the data frame is if the known values in that particular column is greater than 50 percent we say that drop it which is drop X is equal to 1 because it is a column if if this was a row then we would have written X is equal to zero then if the values which are smaller than 0.5 the missing values are less than 50 then we say that fill the missing values with that mean of that particular column so for example you have one two three four Nan and then six seven so now you have only one value so you fill this by the mean of all the values in that particular column right you have other methods to simple imputer K and an imputer there are several other techniques for imputing missing values which is a whole set of interview questions but we'll deal with that in the next section uh like we'll study in detail right so we don't have to worry about that now this is something which is dealing with missing values now now I hope that now you now what you can do you can simply use this uh in one hot encoding drop and fill and then print out the shape and then you can simply convert that now we have the process data where what we did we first adjusted the data we found basic values which is converted that intervals to bins categories called categorical some some call it splitted the feature reconstruction feature splitting feature engineering of converted categorical variables into numeric and then we recently dropped and filled the missing values now we have the perfect data to do the analysis on it to do the analysis on it where now we'll deal with something known as outliers so that's something is the whole set of discussion for the next set of videos so let's catch up in the next video okay bye oh so now what we'll do now we have done with the preparation of a data set now once we are done with the preparation of the data set now we'll go and explore some of the most amazing things in your data set which is working or dealing with outliers so this project I would really like to focus on dealing with outliers so uh let's go let's go ahead and let's try to figure out lot of things out here right so um in outliers we have seen a very nice explanation of the outliers but um we'll talk about some of the methods for dealing with outliers and I think this will be a very long video uh so bear bear with me up right so first is what we're doing we're taking out the box plot of our Target death rate where we say that we I'm using plotly so I'm using plotly where we say that we're going to create a figure in that figure you know on a yaxis we should have the target death rate and then it should a point pose Jitter box points to be outliers Etc so this is a I'm using plotly to plot my box plots so out here out here you can see my first quartile my first quartile which is um zusic 161.2 and then my second quartile is 178 and my third quartile is 194. right and then you have something upper fence up upper fence is something which is your maximum and there you have lower fence which is this much so whatever goes above this are are say to be an outlier and whatever goes beyond this whatever goes um below the minimum I'll say it'll be outliers but this is this is what we have learned right but this is something which we'll take a look at later on but as of now let's go ahead and let's talk about uh let's analyze rather than a Target column right let's analyze other The Columns as well rather than that so first of all what I'll do what I'll do is we have a column known as average a n count right so this is a column in our data set now with what we're doing and think of the mean of that column and then taking the standard deviation of that column right and then we're creating a this histogram to visualize what is the distribution of that average a n and then we plotting the box plot using uh plotly so uh for line by lining ice I'll be giving a resources for learning plotly okay so I suggest you to first of all learn broadly because that's very pretty much easy so you can first of all I suggest to learn plotly where you're saying create a figure for that data and then that data I'm into we're going to have the box plot of this particular column and the power it should be outlined Zeta is equal to 0.3 and Point post is equals to 1.8 however you don't need to remember all of this what I usually do I just copy and paste things so even I even sometimes I forget lot of parameters so it's organization eventually so what I'm doing right now I'm taking out the now I am going each now why I'm doing this first of all let's understand why I'm doing this the reason why I'm doing this I want to identify outliers in my columns so just for explanation sake of thing I am giving the tools how I go about learning more about how my individual variable looks like how my how this average airline columns are is what is the distribution what is the mean what is the standard deviation what is the what is the correlation coefficient correlation with my target variable what is the number of outliers present etc etc etc so in this this is a step which I follow is a framework which I follow is the reason why we are doing this we need to summarize this is some summarize this column average a n and count statistically so what are you seeing in front of me we we have peer print printing the mean of that particular column and then you're plotting the histogram bills is equals to 20 which you have seen the way in interpretation of the histogram then over here we calc and then you simply box plot it and then over here you calculate the correlation between the averaging and count and Target which is we are saying correlation is equals to this particular column this is assume that is a vector dot correlation with whom Target death rate right so into identify what is the what is this the direction the strength between your average and account and the number of death threats in that particular country so the correlation it will be given by this and then you create a scatter plot to visualize the relationship so uh yes what I here's why I did first of all I care first of all I did two basic statistical analysis of this numeric column mean standard deviation I I I wanted to know the distribution that's why I added a histogram I already know the outliers that's why I added you know box slot I want to know the correlation between both of them that's why I added this now you may think are we going to do this for every column if you wish to analyze any particular column you should do it right if you wish to analyze if you wish to know more about the data you should do all of this stuff for each and every numeric variables right that will give you a better understanding about holistic understanding about data plotting the scatter plot to visualize the relationship between AF averaging and governance Target their threat so let's first of all print this out from starting then it will make more sense so let me just run it eventually so it will take some time to run it so we'll just give it give it some time to um run it I think yes plotly no module name plotly so basically it is saying the plotly is not installed so let me quickly install plotly so plotly install pip and then I'll just install plotly by using pip install plotly by making a nice pip install plotly let's wait a few seconds to get it for getting it installed I think it's get start so once it gets stalled we'll just run this you should wait for some time whoops it is saying a minimum type of rendering requires an NB format 4.2 but it is not installed interesting question so let's go and search for it so this is how you should search if you I don't know so currently I don't know the solution to this problem okay now let's try to debug it so it is saying that there's something to do with this format right there's something to do do this format I think that you have to update your kernel you have to upgrade your kernel I have to upgrade your jupyter notebook that's why it was saying you can simply go and then simply run it and then you will see that it will work completely fine then you see that it will completely fine so let me run this out and then you see that and then you first of all restart the kernel so you have to also restart the kernel to be honest restart because once you install the libraries you also have to restart so that it makes sense yes now we are done with that now what we do we simply have this um so let me just first of all run run it out so it says that what it says it says that for a particular average a n account you have mean of average count standard deviation histogram you see it is a positively skewed it's not normal it's a positively skewed data set where most of the data lies between your zero to five thousand because there's some outliers also present you see that there are a lot of outliers present you see that that this is your upper upper phase lower fins and in this upper fence you have 117 is the Upper Front so whatever values Above This are outliers and whatever values below the six are outliers but you see the most of the value slice between uh you see these are the values which are OMG these are out outliers and then correlation between the target is negative correlation where one increases then other decreases so it is saying that um the the the the negative correlation what it is one increases and other decreases right right so as uh assume that um so now you see over here now we see now this is scatter plot where on on xaxis we have average in encounter Y axis we have a Target death rate you see that there is some sort of a giving a very nice stuff but you see over here that uh but but let's let's in let's interpret it so it is saying the mean of average a n and count is 606 which tells us on an average there were 606 cases of cancer diagnosed annually Per County right six and average the standard deviation tells how spread out your data is indicates it is quite spread what in 216 is quite spread with counties having relatively low low numbers of with count some counties having relatively low number of cancer diagnosis annually and some count is very relatively High that's where the standard deviation is high because the spread is high in this case and this is further confirming which you can see that is further confirmed in your histogram or box plot you see the correlation is minus 0.14 which tells us the strength and then strength of a linear relationship correlation of negative one indicates that the perfect linear relationship which means that one increases then other variable decreases a correlation of one means that there is a perfect positive relationship between both of them right so this is a very correlation about how can we interpret it so if in this case if one increase then other other decreases but it does not imply cause a cause I should please see my lectures initially on correlation does not imply causation but only thing which I want to tell is to this is how you can visualize or summarize your particular variables by dealing with all all of the shutsticks you have several missing values as well um I think you have several several missing values however we are going to import the process data to deal with anything literally anything right so you can actually ignore this as of now so what you're going to do we're going to import our process data but now over here but now over here you will be a bit shocked to uh see the outlier detection so let's try to learn about outlier detection in the next set of lectures uh on how we can utilize how you can utilize how we can utilize several tests for outlier detection how we can eventually utilize other things like um uh like SQ how can we perform outlier detection skewed data sets how can we remove it right how can we remove our uh out outliers from the data set by and how can we use our trimming capping this over several the techniques for dealing with it we'll deal with that and then at the end we will just save it and then we'll go to model building which is super duper easy part in this where we send we have our model to eventually train a model but the most important part of this project would be the outlier dealing with outliers and working with data so let's meet in the next lecture talking about things which you have not imagined bye so hey everyone uh what I'll do today is talk about um outlier detection which is the most important you know uh lecture till now which we which which you're going to deal with today is that the most important lecture and we'll try to achieve uh talking about outlier detection using zscore method we'll try to achieve outlier detection using uh IQR you know uh interquartile ranges we'll also try several other techniques like and how can we how what is streaming what is capping Etc in these sort of lectures so so let's get started actually with all of this so whatever you're using in front of your screen is for normally distributed data so um uh so first of all we'll try it off try and try it try it try out outline detection using outlier detection using um zscore and over here first of all we import a process data the one which you process very nicely and then we take a look at the head of this and then we take a look at the tail of this so let's try to print out our head our head of the data and then let's first of all head of the data and we print out the hair of the data this is how it look it looks like but I think which we want this to be in a form of rather than printing let's print out something like this it should look like this where you have five rows nine out of four columns right because why why this is nine over four columns because every value of that categorical values B has become a new categorical column now now uh now for zscore where we are going to use out we are going to use Z scores we have studied zscores in our probability uh lectures so please go there and review it back so we also these scores are you and we are going to use zscode for outlier detection right we are going to use Z score for outlier detection so basically so basically what what we are seeing over here is that um there's one condition for zscore that your particular column or data should be normally distributed so we on a deal so we first of all want to select the columns which are okay normally distributed okay it is not like HiFi and all so you can go and see over here as well how do I know if my distribution is a gaussian if you want to go in math depth but as of now we don't need to go in it you can just use tests like you know normal tests from scifi to understand it so first of all we select the numerical columns because why you want to select the numerical columns you want to select the numerical columns so that only because you want to deal with numerical columns and then you have gaussian columns and nongaussian columns we make a two empty list and then we say for column numerical columns for polynomial columns we're taking out the stat and P P value for normal test which is which is conducted on that particular column which is going to on that particular column and then you're printing out statistics and pvalue and if now this now our PSR significant alpha alpha 0.05 so if a P value is greater than that Alpha they're greater than the alpha we we append our column in that uh so it it so if so what we say we resect our null hypothesis we reject our null hypothesis and then we say okay that if that P there's a significance of that particular column is greater than 0.05 we say that that the particular variable has the is the caution called variable and otherwise it is nongaussian so uh we we we run it then we run it it will take some time to run it so you see that we ran it now if you take a look at the length of the gaussian columns there is only one column which is normally distributed there is one only one column which is normally distributed which is PCT public coverage now with this what you now we have now let's check with that's really you know uh gaussian columns out here so when you subplot it and then here's the explanation of this code so what you're doing you're creating a subplot with 10 by 10 size so this is what your figure is create is of 10 height and 10 width using a subplots function from the matte library and there's Androids and Androids and and callers parameter so you see there is something known as you see a few this is how so you have and what is the number of rows and what is the number of columns so if there is more columns if there is more columns which are gaussian gaussian or normally distributed a normal distributor we most likely go with this you know specify the number of rows and columns and there is only one column in one row because there is only one value of gaussian columns and then we go to your use for Loop to iterate over the colors there is only one column in the gaussian calls that's why we just iterate once time and then we plot the density function then density plot of that function right density plot of that function the X parameter is set to access which is which allows the plots to be displayed on the same plot and the subplots parameter set to True which specifies that each iteration of the loop it should create a new plot right that should create a new plot that's why subplot is equals to true and then a share X parameter is set to false which means that each plot should have its own x axis so each plot should have own axis that's why share X which is sharing the xaxis should be false right and then we tightly out tight layout means to automatically adjust to fit nicely within the figure so it it ought it should automatically adjust so it fit nicely on the computer then we run it now we see that this is the only color which is normally distributed which is normally distributed now now what now what we're gonna do now we're gonna take now we have the column now we have the money the column right now so we print now I'm going to take the whatever data present in that column right because we want to identify outliers in that particular column right so we are going to take out the data from that column and then we're going to describe that data so when you now you have the now you have the gaussian data if you print out the gaussian data so you have only one column pcdi which is gaussian because we want to process data and then we just have taken the coil caution columns out there right so it is just giving the column columns which are gaussian gaussian so now we have the gaussian data now we can perform our outlier detection on this and basic info which is basic info which is mean standard deviation count you know 25 percentile so 50 Etc which is the good statistic which you'll use later on then what you then what you're doing we are taking the basic info so that we should which we'll be using in our Bay info which is means standard deviation minimum and maximum and then what you're doing and then and then over here now here the part come as I told you that in as I told you that we need to identify the highest and the lowest right highest allowed in the lowest allowed and we'll be using the Z score and that right so what what eventually we are doing what eventually we we are using zscore for eventually taking out you know uh highest and lowest so what what you're doing on this case we are taking that particular column and then what that mean because this is a basic info basic information taking the mean of that minus sorry plus that's the maximum thing plus three times the basic input then the standard deviation okay this is the the currently we're using zscore that's why we have this formula why this three three can be changed we give into uh no no the you know values which go outside of that so if you go and see you know uh zscore outlier detection Z score outlier detection let's go to this so look you can see that anything which goes and anything which goes beyond this three then that's outlier beyond beyond minus three below minus three that outlier and Beyond minus B Beyond three and that's also my outlier right so the formula what's this formula was this but you can see over here anything Z score anything below minus three anything above 3 above standard division three they are outliers right so if you see over here that this that the nice any Z score greater than plus three which is standard deviation or minus 3 is considered as a less than sand minus is considered either as the outlier okay so which is pretty much same as this random deviation this is also called the standard deviation method some people call it but okay we're not going to call that so we have the we are going to take out you know uh for PVC column not foreign this column right uh this is the this is the highest allowed so anything which goes up above this anything which goes beyond below this so this is for plus three values and minus three standard deviation so anything which goes this is the my this is the top of the plus three and this is the value of the Tom below of the minus three so we say that highest allowed in that column is 59 which is the plus three which is the zscore and this is plus three and this in this case a minus three lowest and where we subtract it and then what you're doing we are saying we are saying he in process day now we are going to print out so we say the process data so you can see out here that what exactly we're doing we are using the function uses Boolean indexing to select only rows in the data frame where the values in the column are either greater or the low or greater than the highest allowed or less than the lowest allowed and this results in the new data frame operation and this is called the data wrangling this is called the data wrangling and in this case you have Pub processed and then we go go to go to that particular column and therefore Loop iterate to every rows and then say any rows which is greater than that print out or any rows which is smaller than this particular value print out that and that prints out very nicely your columns and there's only five rows which are which are acting as outlier there's only five rows which are acting as a outlier so what we do we make a function we make a function we make a function where we talked or where we make a function that checks that deals with outliers that checks whether that for that function is outlier just implement the advanced version of it where anyone can put your own column name and then he'll be able to able to get your the the data frame right so over here you have deal with outlier so it deals with outliers where it Returns the out there it Returns the data frame which which is the Outlook outline data frame in that particular column okay it is for that particular column so it is highest allowed lowest loud as we talked about and then the data frame which we where that we select the columns the rows where the highway the where the values are greater than high slot and select the rows which are smaller than high lowest allowed and then we return the data frame okay now be honest we want to get the columns from a data set so we have the gaussian columns so now you have the gaussian columns where all all our columns which are normally distributed as of now we have the gaussian columns which are there's only one column in that which is normally distributed but we're going to go forward and talk about to the all the there might be other columns as well when you're working on a project right so iterate through every column in that gaussian columns and then you say deal with outliers you call that function and then you say whatever what are the what is the data data frame you have is gaussian data which you have right now is the gaussian data where we have only one column so you have to also convert that to uh gaussian data which should be the gaussian data and in that gaussian data you have these columns these uh this this particular column because we're iterating to gaussian columns and the basic info which which we wanna guess right the basic info which we have to give as of now we have already taken out all these three and when we take out the shape and if that shape is greater than zero then we see that that callers have the I have the outliers so as of now the as we expect PCD public coverage has outliers okay so what so now if you print out this data frame now if you print out this data frame so let's print out that it uh let's print out the data to the frame eventually so you see that we have the data frame we have the data frame which is a five rows and five rows columns so you have this particular data frame okay in that particular column you have this this value which is acting as outliers which is acting as a outliers but over here you see process data is showing all the values but we are interested in only this put this particular you know uh columns so we just remove we just remove those trimmed trimmed those values from our origin so we removed this outlier this outlier this outlier which means removing whole rows so there are couple of methods while dealing with outliers first one is streaming second word is capping trimming is a technique which is used to handle outliers in a data set which means that outliers have values that are significantly different from majority of the values in a data set when dealing with it is it is we're dealing with outliers the goal is to identify or remove them in order to more accurate representation of the data otherwise there are there are algorithms which are prone to this thing so trimming means removing the values that are far the removing the removing the outliers simply removing that's called trimming trimming means removing the outliers so when it is useful where it is useful when it is useful when it provides uh when when when there's a when we know uh it is easy to implement to be honest and it and for interpretation of it it provides a simple way to handle outliers without having to make any sort of assumptions just about removing like any missing values so here's what you do we go to the now we have the curve now we have the calls that have outliers right and then we print the data frame where it has the outliers right where we can clearly see then it be iterate through all the columns which has outliers then we go to highest allowed and lowest loud and when it trim the data trimmed data what does it mean trim data which means we go to the process data right and then we say that and then and then and and then we say that yeah so over here over here we say that that particular column is more than highest allowed or I think this should be or this should be or or um that particular is smaller than lowest allowed is Greg is greater than slowest allowed so if if that happens removed remove the column right it is a trim data where you're only selecting rows only says only selecting rows which satisfies these two conversations which we already seen which is great greater than values and uh smaller than values right so if you see the process data dot shape you're 47 so I think we we have to have this both the conditions right we have to add this both of the conditions to further successfully do it very now it removes all the values which are you know uh satisfies these both conditions it has to satisfy with this this both condition right now that's streaming trimming means removing of the outliers by by your data wrangling conditions where we see if that particular value in that particular column is greater than highest load for that particular column we remove that and then if that particular is less than that lower allowed then remove that as well gapping capping is another technique which is used to handle outliers which means that uh which means that to handle out Lies by replacing the values that are farther away with the maximum or minimum value so we just remove the you know value values of that particular one with the maximum or minimum so you can see that highest and then we create a copy and then we see that in that we block loc we go to the values in that particular column as we saw that column in this case a public PC to average then go to this column and then go to the if there's anything which is greater than high is allowed and then we simply go to that column ABC is equals to the highest that's a maximum and the same goes for minimum so if you simply have this capped data very simply remove are we simply remove our uh you know what our outliers with the maximum or minimum accordingly you can see uh anything which is greater than highest loud which is go is going going to be Mac going going to be replaced with the maximum anything which is smaller lower slot is going to be replaced with the lowest allowed now we have the capped now we have the capped data now we have the capped data now you can see that this Gap now this cap the data will not have any such um outliers in this any such outliers in this right and always you you can see we do we are not removing any call and any rows we just capping it so this is useful when you don't want to lose or lose lose of your information okay I hope that really makes sense now I hope that you want you you understood everything which I want to teach about outliers and in the next video what what you're going to do I think it's pretty much understandable from now on but I'll again make it for those who didn't understood again so I'll make another lecture on outlier detection if there is a skewness in your column so folks welcome to the next lecture on um whenever many many of you have requested me to work to actually complete this stuff and that's why I'm just going to complete this and you might have questions around like uh why the hell um like uh on outlier detection like uh first of all we have used you know for we have separated our normally distributed data so you so you might be seeing over here we have separated our normally distributed data for example in this which is which which we had outlier detection I think there should be one yeah uh which is outlier outlier detection using on normal normal distributed data so what you what we were eventually doing in this um approach is we were trying to uh from our from our uh whole data we were trying to extract columns we were trying to extract columns which are of uh which are of which are not normally distributed okay so uh for example there are several columns that from that we were trying to extract columns which are normally distributed um and you if you don't know what normal distribution please see the lecture so normal distribution which I prepared in a very nice way so then we use some tests to to determine uh we use some statistical tests to determine whether that is the whether that variable is normal distributed or not and we have used and then we use you know the thing which is p values to to say to to identify it that particular variable is a normal distributor or not using the certain test however you don't need to go in depth about that how do we identify that is for more like who wants to go in in depth but usually in interviews they don't ask like in depth all of these things but it's good if you if you know if you know about that but that's something for statistics majors and all but not for uh I guess for machine machine learning so now I'm so only we have we only we have analyzed only one column so the the reason why we are categorizing it first is for um for the for for example for normal normal distributed we use zscore um for zscore for outlier detection for normal distributed data right but but there are several columns which are not normally distributed as well as they are um like they they follow other the distribution so what should what what should we do how should we handle should we go to Every individual variables and then identify uh and then identify what is there in what is the uh outliers in that particular column or or we or um what or what we do there is something called as IQR interquartile range which we use that for outlier detection IQR for outlier detection IQR for outlier reduction so let's let me just tell you the IQR for the outlier detection yeah so we use outlier for uh iqr4 outlier detection and then uh that IQR works on skewed data skewed means a positive shifter or negative is negatively shifted we have already talked we have we have already had a talk on this please make sure that you actually um go towards it now now coming back to uh one of the questions which may arise wow like like a student asked me like hey ayush how can we identify which columns to test with them whether that column has outlier or not just I'll let you know one thing is that this is very very you know a big problem this is a big problem in um like you you can't just go to Every variable and then do things according to your own pace every each one column here might follow normal distribution and another column might fall is Q distribution right so they have two different distribution and normal normally their zscore works and on a skew there is other method works right so that's a defined set of rules which you already have and and what we do is simply classify in different different categories and then we apply the required all outlier detection techniques for the same and mostly our data has this skewed data so first of all what what we do uh you you might have seen our data has 1900 columns our data has 1900 columns so what we do so what we do is we simply say that um like 19 why why they have 1900 columns because we have encoded our categorical data that's why it went from you know from small amount to large amount so first of all we identify columns which are categorical because they are of course cute right they are of course at once or another side so we don't want to touch them because they are categorical so we separate the categorical values how what we do we simply say calls to remove we simply make a list and then we iterate through our data columns and then we say what is the number of between in that particular column we say if the unique number of values in that in that column is less than 10 which means they're categorical however there is only two two unique values if they're categorical but still I for a safe journey I just added 10 and then I added cost remove a pen now I'm just using that list to drop that so there is 1870 columns which we are going to going to drop because their categorical columns that's it's a nice strategy that you might have other strategies to remove the categorical column because now they are not catech now they are not string type or object type they are numerical type and I'll deal with nicely to numerical types data but make sure that you don't apply the basic processing like remove one single values of few unique values don't apply that because you have to know that this is the categorical data like that's for numeric not for category now over here the code defines as you can see that uh over here else now what we do now what we do we identify as skewed columns as I said now we have the normal normal distributor now we identify these cubed columns so whenever you try to solve for outlier detection I did recommend you do couple of things is normal normal distributed data and skewed data right so there are different different distributions which I might start it right binomial uniform Etc I suggest to to take out to to research on that okay this is the distribution this is distribution of my data what outlayer detection techniques which I can apply to that so currently I have choose a skewed data I have choose a skewed data as Q distribution and skewd you might have it shifted positively or negatively anyway right so we identify that that sort of data and then what we do we simply say the uh over here what we over here we have a function that takes in the processed process data frame and then the columns so what does that columns form columns means the columns in the data right so call our columns in there that's it which is Wiki May we make a list and then we iterate through columns we take our dots Cube using dot skew method we just take out the now the skewness score and if that skewness score is greater than 1 or if that screen score is negative one then we append that uh then we append that particular column in that skewed calls and if this and if there and if that does not passes this condition then we won't append right um so you might have questions like how how exactly is skewness and well all works I did recommend to see how the reading materials which are which we have released for you please see that that's the best thing which you can do right now and then you visibly uh run this one to add in five skewed columns giving the data for skewness and the data for skewness columns and you have total number 16 columns 16 columns such as Q data right and then what you do you simply uh you simply make it just like you created the gaussian data you create the skewed data and then what you do you simply create the to see the distribution so over here you see over here you've subplotted it where we said we where we have I think for 16 and weight columns so you have to sub plots 16 right so what we do 4 by 4 16 so we can have four by four plots to put two two to plot every 16 variables which you can see and I've written this explanation of the code for the same please see that which you can see now if you just uh if you just maintain it let me just say plot image output open it please yeah so first of all see this first of all see this this is the distribution of averaging and conditioner distribution of average death so you have one two three four four columns one two three four four rows and you have all the 16 variables which you had now now you can see that there they are at this their their skewed data which you can see over here possibly negatively and you can see they are putting this is negatively this is positively this is this is slightly positively sorry negatively I'm sorry uh I think I did wrong it is slightly positively and this is positively so it negatively positively and and you can actually see that we have positive and negative if it is going from this side then this negative if this is going to this side this is positive right so uh and then you type layout so that it adjusts Auto automatically and then show it now you can see that you're confirmed okay these these these are my columns just skewed and then what you do you simply box plot so that you see that if they did they do have outliers by using this particular code where you're saying uh figure where you're giving the column and the Box points outliers and then your title is box plot what were the number of columns to be added and then you say okay go to and then you apply this function to each and every variable for that particular data for skin skewed columns and then a column and then skewed columns where you have this cute data and here you can see that OMG we do have this queued we do have outliers we can see maximum and then these are the information which are of outside the way and this is you can see outside the way right and we have seen the interpretation of the box plots as well now you can actually see that this is also the outside the these are the variables Etc so we have this now how can we deal with this the what what we can do let's take a particular column from this let's take a particular column I am going to take um this cute column which is uh for an example as of now study recap and then we take out the quantile range 25th cup percentile the 25th percentile and uh and then we take out 75th percentile okay we take a 25th percentile and then we can take out 75th percentile which is a quarter q1 quartile 1 and quartile three and then we take a IQR from that okay and then we have upper limit like but the maximum value which which which we should have and anything goes above this our outlier and lower limit which is minimum and and things so you can see plus minus over here we are multiplying this and this is the Strand is strand deviation oh upper limit and lower limit and then you can simply say the process data where you have it where you just trim the data out here where you just say take out the columns here which are the outlier which you're seeing our upper limit or they're of lower limit they are below the lower limit upper limit for this particular column is 209 and lower limit for this is minus one through five and these are the columns which you 504 rows in that particular column has outliers and then you simply uh and then you simply apply this and then you simply apply this column where we just had listed this particular all the code in through a sorts of the function a Define and column for that particular element is checking the quantiles q1 Q3 IQR and then upper limit lower limit and then a and then taking out that and then what you what you're doing you're iterating through all columns and then applying this function on that giving the process data and that particular column and then you simply have the uh count then you simply have the columns which has the outliers right now now you have the column switch which has the outlier so you have uh one two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen all 16 variables have their outliers so what we've what we do first of all we take out trimming there's two approaches we have to talk trimming and capping trimming removing capping means um updating updating my uh values like you know what what values updating my outliers with the maximum one and the minimum right so if that a maximum then we if if if the values above the maximum then you updated that with maximum if the value is up lower then minimum then we update that with the minimum you might be clear if you have seen the previous lecture otherwise I'm not going to repeat once and once again right so uh going to this code on trimming first of all identify what rows to remove and that can be done by this data wrangling or the data extraction where you're saying in that particular column in that particular column if that is uh that if that is greater than lower upper limit and if that is smaller than lower limit then these are the rows to remove and then this is a trimmed data where we actually this is where we actually talker the trim our whole data and over here we are tapping it so here we are capping it out there and uh and over here what this is just an example as of now just to Showcase we'll convert that to a function right now and then we simply lock loc to identify the rows and then we identify the rows where your upper limit uh for that particular column right for that particular column for that the the upper up any any value in that particular column is greater than upper limit update that with upper limit and any value below that lower limit update that with lower limit which is the minimum value you see that your data is maintained the same but over here your data is trimmed gapping gapping means um you see over here again the same thing you have converted the code to see over here and over here we will cap will not remove cap by removing we lose whole chunk of lot of information that's why we are tapping it over here and now you can see we are simply say go to the particular column sorry Row in that particular column if that is greater than upper limit for that particular column update that with upper limit and if there is more than lower limit update that with lower limit okay I I hope that that you're getting and then we apply this capping this function on everywhere every variables we have outliers and then we see that we have the data without trimming and we are just going to cap it and then you can now box plot on cap data now your capped data has that now if your box plot on that for every columns now you see there there are no outliers and that's right you want it you want the challenge I hope that you get it and then we say now we have we are going to use the cap data only we are not going to use something then we save our capped the data into something else okay I hope that you I hope that you got one point there is one as high uh assignment related to it there are lot lots of things to try out you can actually try out outlier detection using percentile method that's your thing now I think we are mostly done with the outline detection on regression model on on regression model uh let's talk about uh in the in the next set of videos around a regression model and then we will actually talk about that in great detail let's let's go in the further video and let's talk about that cool uh so we'll uh now we'll come to now we have done with the data data part and my whole point from this project is to tell you about data stuff now we'll just go and quickly review because we have already talked a lot on you know how to build regression model and all and that's not my point on talking in this project my point was different to teach you several techniques feature engineering collection and all techniques to actually go forward with that now uh what now what we'll do we'll try to model it very basic you know uh just to let you know you know we have already tried it out and then now it's now in the assignment you'll be given to model it accordingly so I'm just just going to give you a very small Baseline and it's a assignment will actually further update this uh stuff now which which you're seeing in front of me is we have the first function but ignore all all of that let's go to the let's go to over here so what what I did instead of classes or everything I just created functions right I just created function so that it is easy for you to understand but in but usually I suggest you to work with classes first of all we are importing our Gap data from the cap data which we saved so you can actually use the df.2 to CSV and give that particular path you'll be saving that your file and then if you go to the data data folder you have a cap data over there and cap data is where your where you have dealt dealt with you know missing values and everything out there outliers and everything now uh what you will do is you simply uh first is you have this particular thing where you have the correlation first of all you have the read read it and then you and then the second thing which I which I really like to do is identify the correlation among numeric features among numeric features and then right right now it is eight it has you know 1900 columns and it will take so much of time and the reason why I want to identify the numeric correlation because there is one assumption which you have saw that um the code the code they should not be correlated to too much if they're correlated then they're not providing any sort of information to the output variable and when I apply my correlation techniques when I when I apply my correlation techniques and and not have in not in every process you have to do you have to do this in exploratory data analysis part where you are understanding the correlation between variables this should be done in an Eda part right I'm doing it right now because I think it's much for me I think as of right I've just added added over there just to make sure that we are on a page like like why we are using this we are using just to know that we can ignore our highly coded correlation coefficient and then you see our result right so I just add it over here for for a for a sake of learning but this is a part of your EDM um so you have a correlation among numeric features where you are going to the where you're given the data frame and The Columns of the data frame and then in a numeric column you select all the columns here numeric and then to be honest every column is to American this right now and then you take out the correlation it builds a correlation Matrix with we have already talked about correlation please we don't want to get it but I'll tell you a small uh stuff so let me let me just go to correlation Matrix and then let's talk about that a bit so so this is a correlation Matrix so this is a correlation Matrix so I'll just open that in another tab so this is a correlation Matrix and in this correlational Matrix you you find heat map like this you find heat map like this where where on on on on over here on yaxis you have the variables on xaxis you also have the variables let's survive the correlation between the same columns is one they are highly correct highly coded with each other but the correlation between survive and P class is negative the correlation between survive and this variable is this much the correlation between servers so you have the Matrix for every VR caviar taking out the correlation between every variables out there right so it goes to it takes to the correlation and then it's sick it it it it takes it it has the empty set and then what we are doing we're going to for I in range for that correlation all the coral and then we go to J and then we are saying if the correlation of two variables so the correlation between for example survived and P class is greater than 0.8 is 80 greater than 80 percent we simply append that it's highly correlated with each other right we simply append that we simply append that column so that uh we could know that they're highly correlated features so we were using that I when when I when I ran that method when I ran that method by um uh by applying this so basically what you're seeing over here is I had the columns and then uh over here I had the columns and then over here you can see correlation among numeric features Gap data and cap data dot columns so I've just given the cap data and then Columns of that capped data and then I I was I was able to take out the correlation features and these these were the most highly correlated highly highly correlated feature uh features among themselves and now what now what what we can do we can we can first of all uh remove the remove the columns from the data first of all but I don't really like removing it so what I'm doing is I'm making a list where I'm telling call for call in cap data dot columns if the call is not in hike highly coded so we are just saying key for every column in your cap data which is your data which you've imported for every column make sure take give me that column names but also make sure that these are not in these high in highly query if the if the call also not in this so this is a list comprehension which you might have learned in your Python programming language and I said please see the resampling methods for this and we're going to explain the single word diet right right now please see the resampling method what we are doing you're dividing our data into training and testing I've already uploaded lectures on resampling please go and see that first before coming to this split the data and then we train our model I'm not I'm not going to go with this we are adding the constant why we're either adding the constant because we have to multiply with that bias term that one you know x 0 SEC 0 is equal to one so we are adding one to every rows so that whenever b0 multiplies with one only right which is the biister and then we have um and in the constant and then we have LL model we are training OLS from the the stats model API uh with the Y train and X train within to intercept and then you simply take your lard or summary and then it is printing out the summary and then you have the same goes for significant variables so what we are doing we are identifying the significant variables so um let's talk let's talk about that like I'll talk in a bit but let's run this whatever we have as of now so just to make sure that you're on the right page so so it will take some time to run we'll just chill we'll just chill out right now uh so it will take some time to run and so we should just stay calm stay nice do whatever is going to happen right now because it because the columns are super high and you and you and first of all it might be not as much valuable to deal with that categorical but to to have those categorical columns so could the correlation between categorical columns I'll be giving you a very nice article which talks about correlation between categorical columns which tells you whether that whether like and and and will also uh worry about key what are the variables which are significant enough to make our model better like we cannot go with these much variables to be honest because it will lead to several other problems as well so first of all Let's uh let's let's go with this now now if you see over here it printed out the columns which is of highly correlated so if you see out here that we print out the highly correlated features and then now it goes and training of the model so it is training of the linear regression model and it will take extreme y train this training on and then it is going to print out the summary so let's try to take out what exactly summary looks like oops yeah this is the summary and the summary is too big because you have 1900 columns bro you you understand your 1900 columns and to be honest we'll and and the most of the night allow around 99 percent are of out of encoded columns that's why uh that's why in this case we but that's why we in this case you have to remove some of them so for first of all see that we have dependent variable the the thing in which you want to predict R square at just a r Square F statistic you know number of observations residuals you have moral covariance type and then for constant which is the bias term we have the B beta0 is 94 with instant deviation the TV and the P value for that average and for every variable you have this this much information a left a for every variable you have this much information left and there is so much of information and it is not feasible to go through every information so what I do I just go through what are the features which are numeric which are important which are not categorical as of now so what I'm doing I just could go on through you can go through numeric and then talk out talk some things but before that what I can do is talk about things which is you know signal we can take out the significant variables we have learned about that so let's let's do one thing let's print out the significance variable so let's print out the variables which are only significant enough so from 1900 we got to 144 columns which are significant enough we have done a lot of tests we're using p values you can take out the significancing significance of a particular variable so we we using t t statistics using T statistic we are taking out the significance of each and every variable over here so we have we have using LR dot P values and that P value is this is this is this particular thing is greater than this particular information these statistic we have ttest which you which we are using right now this particular thing which you're seeing right now p is greater than test so we are using for that if P value is smaller than pvalue threshold and threshold is 0.05. okay I hope that you're getting that we are taking on the money columns which are which are which are which which are greater than that so that uh sorry I think uh so the values which are smaller than zero zero zero point zero five so that we can resect our null hypothesis and say that okay these these variables are important so we have only 144 or 1900 to 144 we've reduced the complexity of the model understanding what I'm trying to say now what you're doing you're simply training the model now with significant variables though the reason why we are removing this I'll tell you the reason why we are removing this but first of all uh you will get to know first of all let's run this you'll get to know you got an error you got an error what you got an error in that significant variables you also have the constant column a constant means Constitution which has one right so whatever what I am going to do I'm just going to remove it as of now there is actually a lot of fixes right now but I don't want to extend that you can fix it by your own so what you can eventually do you can create uh you can create actually X in Extreme as well you can create a constant column as well so that it appears that so you can actually what do what you can do let me do one thing right now I think that's something which which we can eventually do extreme and then just go ahead and then add X strain and then constant I guess what we can eventually do in this we can you can add the significant you know constant columns rather than removing that so I think what you can go and then add significant verbs and then significant Vats we can add constant yeah so you can actually do this but how however that significant natural action should be having your favorite you know what you that should be having um constant columns or you can what you can do you can simply have this SM dot you know SM dot add constant and then you can simply add that constant as well over there right so it is very easy to do do things you can actually go to uh xtrain.act constant and then the constant will be also added right so you can actually go over there and then simply add SM dot add constant right SM Dot add cons because in extremely 8080 it is not added right so we can actually go ahead and then add the constant audios as SM dot at constant and then we simply have this x string so it adds the constant you can actually use for loops and all to also added but this is the best way to actually do do that rather than removing that so you actually rather than moving that you can actually add this and then once you add this so let's run this so you might be having a nice set of information so now it it is complete now you run this now you run this you should be able to see a very nicer of info so now let's go and let's print out the summary which you can see so now if you run this now if you run this you have less information as compared to the previous one you have just only few set of information to go forward to so yeah this is the one and then you can also see the the R square is 0.68 however it is it is decreased but it is a it it is a model which we looked for right or however it reduced the complexity of our model right our complexity of our model the more important is not to get the accuracy but also the robust model the model which is able to perform in most of the cases very nicely so this was it for the project and I hope that you really like this particular project and um you now you'll be given assignments for the for the same for rest of the things which you have to do in the project uh now you can get started with making your own projects Havoc try to explanation everything from very scratch now I suggest to watch resampling methods video regularization videos which is going to come so please watch that and thank you for the lectures