In this course, you will learn the basics of reinforcement learning and how to implement it using gymnasium. gymnasium is a software library by open AI that provides a collection of pre built environments for reinforcement learning agents. In this course, Mustafa will show you how to use gymnasium to test and improve your reinforcement learning algorithms. So what will be the objectives of the course. So what we'll be doing is, we'll be learning the basics of reinforcement learning and opening a gymnasium, you will learn how to use the gymnasium interface to interact with different environments. We'll explore various gymnastic environments, so environments could be like a taxi environment, it could be like a gadget environment, you can have different Atari games as well. And we'll also understand their characteristics will build and evaluate reinforcement learning agents using given as prerequisites knowledge of Python programming language, so you need to be familiar with basic loops. What is a class we'll have different methods. So I'll explain it to you while as we go along basic understanding of reinforcement learning concepts. So whatever the topics are that are required, I'll be explaining it to you. While we are while we start the course. Introduction to reinforcement learning, what is reinforcement learning, we will learn the RL technologies. And also we will understand the reinforcement learning workflow. My next section is introduction to opening a gymnasium. So I'll introduce you to open a gymnasium and also we will set up the gymnasium environment. And I'll also I'll explain you the basic concepts that are required for using gymnasium. So it could be the environment agent observation, action and the word. The next section is gymnasium environments. So we'll understand the types of gymnasium environments. We will understand the difference between continuous and discrete actions and observations understanding the properties of different gymnasium environments and also the games classic control Atari back to the robotics we got different types of the environments available and there are many more available so like Jack we have taxi environment we have frozen like environment. So now let the next question is the gymnasium interface will be interacting with environments using the gym interface will understand the gymnasium API. And we'll create some custom environments. Now the next section is building reinforcement learning agents. So we'll be implementing RL agents using gymnasium will understand the algorithm Q learning will be training and evaluating RL agents using gymnasium. And we'll be improving RL agents with policy search and other techniques. Basically, we'll be solving blackjack using Q learning this will be the next section. So let me just write it down that will be the tutorial and these are the advanced topics. So after you get a basic understanding of these of their of reinforcement learning gymnasium, and solve some server problem, then it can go and explore the other advanced topics. So the these are the using a porn AI baselines for benchmarking RL algorithms, creating and using rappers to modify environment dynamics, combining multiple environments to create more complex scenarios. So we can have two taxes coming at each other. Or you can have multiple players in a poker environment. So basically, you can simulate those environments. Also, we'll be integrating with other tools such as TensorFlow and pytorch. Okay, so basically, this is the course. Now let's look at the conclusion. So by the end of this course, you will have a very strong understanding of reinforcement learning and open air gymnasium, you will be able to use the museum to build and evaluate RL agents. And we'll have a very good understanding of the different gymnasium environments available. This code is an excellent starting point for anyone interested in developing or working with reinforcement learning system. Let's start with the basics of reinforcement learning. It is a type of machine learning in which an agent learns to make decisions by interacting within the environment. So basically, as you can see, I have an agent and I have an environment. So basically, let's take an example of a robot to what will happen is that the robot when it takes an action Suppose it takes a step forward. So, we will reward either we will reward the robot for that action or either we will penalize it for the action. So how will you decide that? So what my basic policy would be that suppose if the robot takes a step and it banged into wall. So, we will have to penalize it and we will deduct some point for that. Or if the robot takes a step, and it doesn't bang into anything, that will be a good action, so we will have to give it some points. This is basically that's what will happen. So, what is an agent? An agent is an entity that interacts with an environment in order to learn how to make decisions that will maximize a specific goal or objectives. So, basically, in our case, when we have a robot need it to take something from one place to another place to the city, it has to reach the particular endpoint. So, we will have to train the robot to reach that goal. So, it will take the decision autonomously as it receives input from the environment, perform the actions and receive rewards or penalties based on these actions. The environment on the other hand is the external system or context in which the agent operates. So basically, in our case, everything that is nearby to the robot is the environment. So basically, it could be a warehouse to the full warehouse will be the environment and it will be interacting with the robot will be interacting with the environment. So it can be a simulation of physical system or any other system that the agent interacts with. The environment provides feedback to the agent in the form of rewards or punishments. When the agent uses to learn how to make better decisions. The agent receives feedback in the form of rewards or penalties. So as we discussed earlier in the robot kills, when he takes a step forward, and it doesn't matter to wall, we will give it a reward. And our goal is to maximize the total reward. So if it takes a step forward, and doesn't bind it to anything, it will eventually reach the goal. So it is a trial and error process, observing the consequences and adjusting its behavior based on the rewards it receives. Over time, the agent learns to identify the actions that lead to the highest rewards and avoid those that lead to penalties. So as we keep training the robot, it will eventually learn what decisions to take autonomously. So it adapts to new situations and environments. And it will adjust its behavior based on the feedback it receives and improve the performance. And it will become efficient decision maker, which leads to better performance as well. So let's also look at some real world use cases of reinforcement learning. So basically, the example that we covered is robotics. We could perform tasks such as grasping objects navigating and interacting with humans. For example, RL has been used to train robots to play tennis, table tennis, where the robot learns to adjust its movement based on the position of the ball. So let's look at another example of which is the game playing. So like the most famous example is of Atari Games. So like DeepMind launched Atari game module. Reinforcement Learning has been applied to game learning, particularly in the development of artificial objects. Or we can also talk about Blackjack, we can talk about chess, so a lot of games, Atari Games AlphaGo, a computer program developed by Google DeepMind, which is reinforcement learning to learn to play the game of go at a world class level. Also, we can use reinforcement learning for autonomous driving. Reinforcement learning can be used to train autonomous vehicles to make decisions in complex and dynamic environments. For example, reinforcement learning can be used to train a self driving car to navigate to the traffic, world obstacles and make safe and efficient driving decisions. Also, another very good example is of personalized recommendations, which is basically Netflix or Google recommendations. Reinforcement learning can be used to provide personalized recommendations to user based on their preferences and behavior. For example, we can use RL to optimize the recommendations of a video streaming service, learning what content to recommend to the users and to maximize user engagement and satisfaction. So now we have learned about the agent and the environment. So now let's move on to gymnasium, which is a toolkit that will help us simulate these environments for our agent. So basically, it is a set of a collection of environments, or tasks that can be used to test and develop reinforcement learning algorithms. These environments are typically game like with well defined rules and reward structure, making them useful for evaluating and comparing different reinforcement learning algorithms. So what I'll do is I head over to the documentation page and show you the different types of environments that are available. So this is the introduction page. So basically, it is just two lines of code to set up an environment and you can interact with the environment as well. So I encourage you all to go and check out the documentation and also explore the different types of environments that are available. So we have a Cardpool environment we have a mountain car. Also interesting environments. We have a car racing environment as well. Basically, for gymnasium, there is only one single agent. But if you want multi agents, then we have another library called petting zoo. We have a bipedal Walker here. And another interesting environment is obliging environment. So in this tutorial, I'll be covering blackjack environment. And another environment is a taxi environment. So basically, this is like our cell running example. Also, we have a frozen lake environment, forking, these are all the very interesting environments. So it could range from games like Pong, or record to more complex simulations like robotics or autonomous driving. The NASM environments are designed to be easy to use, and come with a standard interface for interacting with the environment. Now, basically, what all steps do, we need to start interacting with gymnasium. So let's check it out. So we need to define the environment that we want to work in P to create an instance of the environment. So that's what we did here. So basically, we use them that make and use the environment that we want. We will define the agents policy, how it decides which action to take, interact with the environment, taking actions and receiving rewards, update the agents policy based on the reward it receives. So just to go back, we have an example of a robot. When we take that example and apply these rules, we'll be able to interact very easily. So let me just to show you how we can do that to an environment will be the warehouse environment, an instance of the environment we'll create first, we'll define the policy. So in our case, the policy was that once the robot takes a step forward, and if it doesn't crash into a wall, or even cat class into another word, that is a successful step, and then we reward it, otherwise, we'll have to penalize it. So basically, this is the fourth step, we have to update the agents policy. So what we have to do is we need to update it that once you take a successful tip, you need to keep taking successful steps and you will in the end, and we'll keep repeating it until our performance is satisfactory. Some of the main concepts we will need in open AI gymnasium. So the first we have is observation and action spaces. And our observation space is the set of possible states that an agent can observe in the environment. And Action space is the set of possible actions that an agent can take in an environment. So let's take the example of our robot. So what is the thing is that I have my robot right here. And the possible actions that it can take is either it can travel in the front, or it can double to the right or to the left or take a step back. So this is the possible action space that can take also, in this case, the observation space will be an ideal place to be 567 action space is different than the observation space. This is the full action space and observation space will be changed. This is action space. This is the observation space. Let's look at episode. An episode is a complete run through of an environment starting from the initial state and continuing until a terminal state is reached. So let's look at that in our example as well. So I have my robot right here. And I wanted to reach here. So this would be an episode and I can train the robot to run multiple episodes. So it will start at a particular position and it will reach this terminal position. So that will be one episode it will start there and could go here as well. Basically, this could be a terminal location or it could this could be the start location and this could be the terminal location. These are all the different episodes that we have. Each episode is composed of sequence of states action and towards rapper rapper is a tool in open engine that allows you to modify and environments behavior without changing its code. So we'll look into that later on. Wrappers can be used to add features such as Time Limits, reward shaping, and action must be benchmark open and Jim provides a set of benchmark environments, which are standardized tests that can be used to evaluate and compare reinforcement learning algorithms. So if we have multiple algorithms for a particular environment or a problem, we can use the benchmark to compare them We are done with the basics of reinforcement learning. And also we have learned the agent environment and everything that is needed to get started. So now let's implement the game of blackjack using gymnasium. First, let's get introduced to the game of blackjack. So what are the basic rules, so the basic rules is that I need I will have two cars and the dealer will have two cars. Now in this scenario, both will be AI players. So the thing is that at each turn, I will have to decide that I need a new car or do I want to keep my current set of cards, but the dealer has to keep playing until he until the E reaches over 70 The sum of the card which is over 70 And if I have to when I need to be a bigger number, then my some of the cards have to be greater than the dealer's card. So let's go over all the basic rules of blackjack. This game is played with one or more decks of standard playing cards. Each dealer is dealt two cards and the dealer is also day two cards with one card facedown. The value of each card is determined by its rank. Aces can be worth one or 11. face cards kings, queens and jacks are what 10 and all other cards our their face value. Players have the option to hit and take additional cards to improve their hand or stand and keep the current trend, the dealer must hit until their hand has a value of 17 or more. If a player's hand goes over 21 The bust and lose the game. If the dealer's hand goes over 21 the player wins the game if neither the player nor the dealer bust the hand with the highest total value that is less than or equal to 21 wins to basically these are the rules now let's head over to the gymnasium documentation and check out how blackjack is implemented. So here we are in the gymnasium documentation now let's check out the action space and observation space here. So basically we need to import blackjack we want to create the blackjack environment this is the rules that we discussed no action space the action space is one comma in the range of zero comma one indicating whether to stick or hit. So it's a player's decision with a dealer has to be played the observation consists of a three tuple containing the Players Current sum the value of the dealers one card one to 10 where one is is and whether the player holds a usable is one or zero. Basically this will be the observation space our starting space will be four between four and 11 dealer car has to 11 and other usable is if I ever usable based on what will be the rewards in this particular reinforcement learning problem. So we'll have win game as plus one lose game as minus one draw game is zero and win game with natural blackjack plus 1.5 If natural is true, and this one if natural is false. So basically this is another type of set that is the natural environment. So basically we can understand that with natural is true whether to give an additional reward for starting with a natural backdrop that is starting with an ace and 10. So we can specify that when we set up the environment. Now let's see how we can implement it in Google collab. Now we've covered the basics of blackjack. Now let's understand how we can solve this game of blackjack. So let's first discuss a book that is very popular regarding to blank that it's called reinforcement learning and introduction by Richard Sutton and Andrew Bartow. It covers the basic applications of reinforcement learning. And also there is a chapter on Blackjack, and it also covers other games. And they have modelled it a Markov decision process, and how every action influences the outcome of the game. Now also they explain how reinforcement learning algorithms can be used to learn optimal policies for playing blackjack based on maximizing the expected return over the long term. So now this seems really complicated, but this is the beginner course you don't need to learn this. Once this course is over and you still have a curiosity you can go check out this book. And because why should you check out this book because it presents several approaches to solving the blackjack problem including Monte Carlo methods and temporary difference learning. It also covers value functions and policy improvement, which are a very advanced topics. So maybe we can create a course on that but as this is a beginner course, you do not need to know all these. So let's start with the basic implementation. What I will be doing is I will be importing the libraries that we'll be needing for us to install I'll be copying this multiple times and I plot lib, then let's do NumPy. See one. So basically NumPy is used for data manipulation. As we import I just explained to you when we bought what each library is used for, take UDM we are in synchronism. 0.2 7.0. So basically, this is the LT S support Long Term Support version. And let's do matplotlib in line. So, as you all know, when accurate lib is used, it opens a new window. But in collab, Google collab, there is no window. So basically, the plots should be in line. So basically we'll do this percentage matplotlib in line 30. So all the plots below the cell. So yeah, it's done, let's do pip install. So I already have it installed, but in my machine, it'll take some time to install by till it get installed. Let's also import all the necessary materials that we'll need. So from collections, I will import defaults. So why I am using a default is because it allows us to access keys without checking that if the value is accessed, its value exists from input map plot lib.pi plot as PLT. So this is just also write down why we'll be using access to you find the keys keys that do not exist. Check for us, instead of us having to always check if that keys exist, it will just take it for us. We'll be using this for going clots from Mac lot plot lib dot patches. So basically this is for creating shapes or shapes. Let's import NumPy. So NumPy is very popular. Michigan NumPy is for array manipulation and image manipulation. Manipulation there's two input C one C one is a very popular data visualization library as soon as and from TQ DM so basically this is for a loader. So whatever training that happens, it will show it as a loader. So to be very easy to understand cleaning covers. So I think we have completed the installation of the basic libraries. Now let's so let's so what I'll be doing is I'll be using the Jim that make command to create a environment so it will be env is equal to Jim. So I've already imported the gymnasium. Okay, I forgot to input let me import it as well. So from import gymnasium. Now let's do gym dot make. So our environment name is Blackjack, we won. Check we want. And what I'll be doing is I'll be setting the SAP parameter to true what is SAP parameter it is the natural environment, basically the default state of certain and the book that we discussed. So what I'll be doing is and also I'll be setting the render mode is equal to RGB grid because I'll be showing you the training states so exactly what is the position of the card at that particular moment in training? That will be interesting to watch. That is why I will set this render mode to RGB array. Now let's go and see how we can set up the environment. So I will be observing the environment what I'll be doing is I'll be setting this as Let's observe the environment so we'll be using the NV dot start method to observe an episode so basically we discussed what is an episode in is basically all the steps that it takes for me to reach from the starting point to the endpoint once. Basically, that is an episode. And for sitting in Episode, we do it as observation, comma info is equal to en v dot reset. So basically, we use the reset method to reset the full board, or you said the particular environment. And we'll be keeping done as far as because we have just started the training, let's do let's have some comments. They said no environment first observation So, and what will be the output of this, so, observation is equal to it is a tuple. And as we have discussed before, it will be 16 Nine and false. So what is this tuple. So basically, this is my hand, what I have been dealt, this is the dealer's hand. And this is the number of if, do I have an ace, so, I don't have an ace at the start, that is why I caught the output as false. So, it is a Boolean and it is like the this value deposits that if I have a as usable without busting So yeah, that is the thing, if I can have an ace, but I can also bust as well. So, this is a an ace which is usable, and without busting. So let's write that down as well. So, what this output let's consistently when is the first value in the current some, the second will be value of dealers sub called the third will be Boolean whether the player holds, holds usable is and what is a usable is where it is usable. Yes, it comes without busted. Basically that is let's see how we can execute that. So, for executing an action, first we'll receive our first Object Observation should be basically that will be the starting space, we'll be going to then after that we'll be using the end third step. And we'll be giving the action to interact with the environment. So in whatever example of the blackjack, it will be that if I want to take a new cart, or do I want to keep my current set of cards that will be passed in the action. This function takes an action as an input and execute it in the environment. Because that action changes the state of the environment, it returns four useful variables to us these are next state. This is the observation that the agent will receive after taking the action. It will also give us the reward the Agent will receive after taking the action. Either this environment has terminated or it will give us truncated that if the episode ended early, or the time limit is reached. This is a dictionary that contains additional information about the environment. The next reward terminated and truncated variables are self explanatory, but the info variable requires some additional explanation. This variable contains a dictionary that might have some extra information about the environment. But in blackjack v one environment you can ignore it. For example, in atari environments, the info dictionary has a fairly short lives key that tells us how many lives the agent has left. If the agent has utilized then the episode is over. So basically, these are the four values that are important. After we take a step two let's see how we can take a step now. So let's take a new cell and let's take the action. So action equal to NV dot action underscore space dot sample now what is this? This is a sample action that will be taking sample a random action Basically, this is the training loop. So that's why we are taking a sample exam. All valid actions. Now we have hash action is equal to one. And what we'll do is we'll execute, execute the action, we see how we will be executing it. So we'll be taking observation. So we discussed all the parameters here, right, so we'll be taking all the four parameters, we'll be having reward, we'll be having terminated, truncated, and so forth. So we can ignore info but I be taking input, just in case an action so action is the sample exit from here. Execute the action. So we'll be receiving input and receive info after taking it but taking the step. So what will be the output if we run this so the sample output will be the division is equal to So basically, we have learned what this tuple means. This is a 24 is my current sum to basically I've gone bust. Now, also, first, I do not have any usable. Basically, this is just a sample start. So I have some values here. So I'll explain to you all these values. So and reward is equal to minus 1.0. So basically, I've lost, so terminated will be true. We have truncated and hash info, we call to length. So yeah, this, this is how it's going to be for this particular one simple step. So once we get the word terminated is equal to true, what do we do then the current episode, we have to reset. So if terminated, I had to take a text. needed is equal to true or we should stop the current episodes and begin a new one using en with a reset. So basically, I've talked about this method before as well. And this is what is a single action that we take in this particular environment. Now let's build our agent. So now we have committed the basic setup of the environment and our agent. Now let's understand the approach that we'll be using to solve like that. So let's look at the epsilon greedy strategy. This is a very optimum strategy to solve that. So let's understand it. In this strategy, the agent takes an action that is either the best action based on the current policy with a probability of one minus epsilon or a random action with a probability of epsilon. This approach balances the exploitation of the current best policy with exploration of new policies, which can lead to better rewards in the long term. So basically, it is a very strict policy. Either I take the current best policy or current best approach, or action, or either take a random action. So how you set the policy let's let's look at that later. But let's understand the epsilon greedy strategy. In the context of blackjack, the epsilon greedy strategy can be applied to determine whether the player should hit or stand at each step of the game, the agent, that is the player can choose to take the action that is either recommended by the current policy, or it is a random action. The policy is learned over time by updating the action value estimates of each state action pair based on the rewards received during the game. As the game is played repeatedly, the agent learns the optimal policy that maximizes the expected rewards. Basically, we as we keep on running the iterations and the agent keeps learning, it will understand the optimum policy that will maximize our expected reward. Initially, the agent may explore by taking random actions to discover new strategies. However, as the game progresses, the agent will start to exploit the best known policy, which should maximize the expected reward over time. So this is the approach that we'll be using. Now, let's build our agent using this approach. The epsilon greedy strategy Okay, so let's take a code cell and create our class. So basically, we'll create a blackjack agent class. And let's do def, we define the initial constructor that is a default constructor. And what all parameters do we need for this, so we'll take the self betterment of the learning rate. Basically, this will be passed as epsilon, this will be a float, stick initial epsilon be a float as well. Let's take epsilon decay is a way of float as well. Let's take final epsilon should be a float as well. And let's take the discount factor and this will be 0.95 in this case, so we have completed the initial Garaventa. Let's move ahead and create our initial complete the initial constructor. q underscore value is equal to default IC so we'll be using default because we do not want to always check if that key exists to be a lamda method, lambda function and P dot zero Yeah, so we can just initialize it as np dot zeros. Let's set it at E and V dot action space dot n. Now let's read some comments as well just for explanation. So what are we going to do we are going to initialize or also whenever I say RL it is it is a reinforcement learning agent with empty dictionary of state action values. This is the key values learning rate and epsilon. Arguments let's understand all the arguments as well. So we'll be calculating the learning rate. So, I think these are pretty self explanatory, let me just explain to you the discount factor. So, the learning rate is the learning rate itself, the initial epsilon value, epsilon decay, the decay for epsilon and this will be the final epsilon value and the discount factor is something that needs to be understood. Let's understand the discount factor discount factor for computing the Q value so I'll explain to you what the Q value is just first complete the basic class of our agent. This is the approach that we are going to be using using Q learning to solving blackjack using Q learning. So, now let's complete our consulter basically, this is the learning rate. discount factor is equal to discount factor epsilon be able to initial better know also good epsilon is equal to initial epsilon loop self dot epsilon DK single equals final epsilon going to final underscore epsilon cuts to the training error says training error is equal to blank correct. Now let's create another method that is the get action method basically, I need to complete the integration. Let's do that. It may complete the end condition yeah I'm just looking at this is a tuple that we discussed observation tuple that we'll be getting, which is intelligible and the output will be an integer. So this will be the get action method where we'll refine the policy. So if np dot random dot random maybe doing random is less than self dot random dot epsilon so I think I could do return in v dot action underscore sample. So we do taking the sample action instead of taking the correct action. Yeah, else if the probability is, is good enough. Let me take the return of n p dot arg max dot q. So basically, this is the key values that we'll be using your so I'll explain to you in just some seconds. Me complete all the methods. So let's let's give it some parameters here. Basically, this is this will return return the best action be at one minus epsilon otherwise random action with probability explore for exploration. So yeah, this is the basically the approach that we understood before. And short exploration. Let's create another method first printing of what are the Q value is, so basically, let me just write it down here. So the Q value function is used to estimate the optimal action to take in each state, the optimal action in a state is the one it is the one that maximizes expected long term rewards. So basically, we've understood the epsilon approach to basically this is how we'll be maximizing it. Now, this is a bit complicated the equation there is an arg max, which will return the maximizing value. So I encourage you guys to go and just understand the logic behind it. It's pretty complicated, but it is this is basically the optimal action that we have to take in each step. Basically, this will be calculated for us in our in our game, pls. Let's move on now we need to complete the update method for it's complete that you have our def update and it will have self observation. You see this is the operation tuple. So like you might be having some errors. So when we compile it, we'll get to know all the actions that do reward the workload to terminated boolean and do next observation. Again, and to kill it. So we've understood what the cue function is basically we'll be updating the cue function so let's understand it. Future. value is equal to not terminated into np dot max self dot Q values of next observation, we will be taking the Q value of the next observation and multiplying it with the terminating method. So, if it is terminated, then we have to start again, if it is not, then we know, we know what need to calculate this equal to. So, he has the reward. Basically, this is the method that is a function. So, explaining to you how we calculate it discount factor into future Q value minus Admittedly, this is the literal formula that we use to calculate it will explain to you, but let's go ahead and complete this method. Observation of action. Three, and this is done the queue method, key values done. Let's do the training. But now we need to update the Q value. Now, we have completed the difference now let's do the self dot q values of observations and of the action 2d array is equal to self dot values. So, basically this is what we have to do again, you score values I'll be copying this plus self dot x revisit the learning rate into the temporal difference and this will be self dot training dot append temporal difference now, let's compare the last method that dT d k epsilon. Lot salon minus salon decay was ugly we have completed our class like the agent. So, now we've completed the class of our agent three and we've defined all the actions that the agent can perform. Now let's see the training of the agent. So let's start the training. We'll be taking it one episode at a time. So let's do the parameters for servers we'll keep the learning rate 0.01 Visit the learning rate we have the episodes equal to 100,000 we can change it like just keep it 100,000 Because that's will give us a very good trained agent, but I'll just reduce it for the tutorial. This is the standard that you should keep if you want to get a real world agent to start shorts by two so we reduce the exploration what time we wanted to take the appropriate path without having to explore too much to this will basically reduce it. Then we have the final epsilon which really this is where we need to get by the end of the training. Let's create our agent this is this these are all the parameters the hyper parameters doors, we have agent is equal to have a black agent learning rate. learning rate we have the initial which means you can just pop up on intubated is equal to start. Start if you don't have one, and basically we need to get to 0.1 from one and we have epsilon decay is equal to epsilon decay. It's past that. Now also let's do it the final epsilon. So this is the agent that we need Hello. So now, let's turn the training let's turn to training now. So basically we have completed the agent setup now. Now let's start the training. So what I'll do is I'll just set up the environment in v is equal to gym dot rappers, not record. So we will be recording all the statistics and I will just show you after the training, how the agent improves over time. So will you plotting some charts as well. So let me complete the training part first. And then we'll talk about what the training has, how the training has improved over time, and what kind of all the statistics would mean. We'll be putting charts and also I will give you the statistics we have episodes and episodes okay, it will be recording the statistics for episode now let's do that. So we have under 1000 episodes, let's make it 10 For this tutorial, just to that it will run pass. So I will also output the environment at particular point so I will so every time that a card is dealt or anything that is decision is made by the agent will be plotting that particular environment and see the what is happening in real time. So I'll be doing that as well. So it's like watching live play live poker in real time. Live blackjack and play time. So let's do that. Let's let's do that. Now. We do end with a reset. Now why do we do in winter reset because as we discussed, after every episode, we need to reset the environment. Because I have traveled from one point to another point I need to reset the environment. Okay, so done is equal to false because we need to complete the challenge need to get to the endpoint need to clear up now I need to import. So what I'll be doing is for every episode, I will be carrying the output because I'm only interested in the current episode and what is happening right now in real time. So I'll import it IPython display input clear output. Basically, this is what I need to import. So basically for every episode, this will clear the output. Now let's complete third training for one episode. result we play one episode as do while not done, so is also while we are not done, let's run this loop. Let's do action equal to agent dot get action of observation. Let's do next underscore OBS comma toward comma terminated whistle all the information that we'll get after making a particular step should be truncated the info equal to E NV dot step. Let's update now let's do agent dot update. We'll be passing all these parameters, we have to pass observation action reward terminated and the next position. Next observation frame. So basically I will be rendering the frame so at particular instance what is happening we can just output that as well. So I'll be doing that we will able to end the Render loop PLT dot I am show frame from Python, I'll be showing you the exact location of the cards and exact position of the gods exactly what is happening in real time. Now, what we have to do is we have to check if that episode is terminated. So how will we know that we have a parameter called terminated or we have truncated so truncated is that we have a bust or terminated is that the time limit is oh observation is equal to next observation. And we have to do agent. So I am excited to follow me to agent dot d k, epsilon. So over time, it will get smarter. So this is it. So I'll be using the package called annotations because I'll be using subscripts in my methods. So I got an error here. Now let's run it and check again. If you're also getting an error, make sure to import the package annotations. Now everything's done fine. Let's do it. Yeah, basically. Basically this is like running an ad is not defined. So it is not defined. So I'd say okay, it should be brutal basically these things happen yeah this sounds fine now that's tribes but agreement equal to equal to where is that here let's see what I can do record episode so it is en mi comma non thought basically these things happen so so there is nothing in the gymnasium that happens so it is rappers no rapper also has no attribute record episode statistics so it is Np p Saudia obviously, this is episode statistics now in Opie is not defined for is not OB, this is our OBS, this is observation space. Now, we're gonna continue to add episode when they already exist. To see why that happens. Everything is done, let's run everything through run all. Now if you get some errors, just make sure to check out the integrations or I just link my Google colab notebook in the description, you guys can go and check it out. Because there were a lot of integration issues before running it. Now that's it. So as you can see, for every episode, we are able to visualize it. So divisibility when you 121, then it's a drop in both are 21. Let's see it is 12 It's pretty fun to watch. So now currently, I've said the episodes over 100,000, I can change it and keep it as 10. But I'm just gonna let it train. So once that is done, I just come back and let's just visualize the training part of them. Because we need to understand the error losses and we need to understand the policy that we have used. This is running, let's just complete the visualization code. So what I'll be doing is rolling land. So we will be understanding all the parameters and all the data relating to our policy and understand what is the appropriate amount of episodes that we couldn't use, and also the loss function. We'll plot it and we'll understand it that way as well. So let's do that. So what I've done is I've set the rolling length is 500 and I've done the finger sizer so I let me just complete the code and I'll explain to you everything what we've done. Yeah, just give us five subplots, not subplots. Just to access of you know that set title, so we'll just give it a title as episode rewards. So what how does the reward change in each episode we have plotted let's plot that episode rewards. Now let's do reward the score moving score average equal to NP dot convolve. We will plot a colored graph in lieu np dot array E and V dot return Q dot satin so we'll flatten it to dot Latin comma np dot once and let's do more is equal to valid and let's do rolling length divided by rolling this will be for every 500 iterations we're doing this now this is completed let's do x of zero dot dot array range then the word moving average the moving average of the reward now what I will be doing is I will be putting it against the reward moving average to that value. The value here so we are going to plot it now let's do excess of one two is the y axis got set title. So lengths this is done let's do length underscore Moving underscore will be our variable that can be the same way to a carnival within do np dot Cornwalls. So I'll explain to you why we use the Commonwealth method as well we need to do a blanket dot plugin, then we need to do np dot once. Rolling and again and mod is equal to same so instead of valid we'll do same now it's to the window same close the brackets, divide this by rolling in not clot length training, moving average three out only ever talked about training error. Score moving average comma, training underscore underscore knowing no score. Loop PLT dot tight layout basically, I've completed the training or visualizing of the training parameters and how the graph looks like. So what I'll do is I'll just wait for the training to complete and then I'll explain to you the graph. So the training is complete. Now let's look at the different parameters that we have plotted. So they have three different types of charts. Let's understand each one of them. So first one we have is episode rewards. So the episode rewards in our case of solving Blackjack, and episode is a can be defined as a game of blackjack played from start to finish. And the episode rewards is a total sum of the rewards obtained during that game, it could be the sum of the numbers of wins and losses. And we can add any bonuses or penalties for certain actions. So as you can see, for each episode, there is a very drastic change in the rewards. So basically, this is very interesting to watch that it is not a flatline, basically, we do not have any rangebound moment between it just just too volatile, just keep changing it this is very interesting to watch. So now let's look at the episode lens combat which is plotted against each episode. So basically, this is one complete sequence of playing a hand taking a hit or a stand and receiving the result. Now the length of an episode would depend on the rules of the game, the number of players and other factors. So in our game, it could be just two player, but you can have more number of players as well. And in a game of blackjack you could consist of it would consist of a single hand or multiple hands with different lengths of each episode. So basically, you can see, in our case, it would be more than one only. But it doesn't go over two. So basically everybody will find in between 1.4 1.3 basically 1.3 and 1.4. So each episode is just between 1.3 and 1.4. That is also very interesting to watch. Let's look at the training error. The training error is a measure of how well our model is performing on the training data. Now in our context of solving blackjack using Kulani. The training error would be difference between the expected value of the Q function and the actual Q values obtained during the training. The goal of Q learning is to minimize this error and converge to optimal Q values that maximize the expected reward. A high training error indicates that the model is not learning effectively. And adjustments may be needed to divert the motoring data. So as you can see at around 3000 around 3000 This is pretty good. Our training error is pretty low. Now after 3000 goes pretty high. So that's it now we are done with our training of the blackjack agent. And we have successfully solved the blackjack game using the Q learning algorithm. Now let's quickly revise all the things that we have learned while we solve when we started solving that chapter. So we started with the reinforcement learning basics. Then I also explained to you the role of an agent, what exactly constitutes an environment, what each action is? And how do we keep track of each each state that the agent is currently in. Then we also learned about the observation aspect, and also how we use rewards to keep track of a successful step and also give penalty to the agent when it makes a wrong step. So basically, we have covered that, then I'll also explain to you the real world use cases of reinforcement learning. We saw basically there is robotics game playing autonomous driving personalized recommendations. Then we started by with the gymnasium libraries basically, it gives us a collection of environments that can be used to test and develop reinforcement learning algorithms. So here we have made use of the blackjack environment. Then we started with the setting, then we started the setup for the gymnasium environment. Also, I've walked you through all the processes that we need to successfully solve any problems. This is true for any reinforcement learning problem. Then I also explained to you the main concepts of open air gymnasium. To be started with observation and action spaces episode, what is the wrapper benchmark. Now we start with the introduction to the blackjack game, basically are the basic rules. The game is paired with one or more decks of tender playing cards. Each player is dealt two cards and the data is also dealt two cards with one card facedown. The value of each card is determined by its rank. It says Can we work 111 face cards, that is kings, queens and jacks, what 10 and all other face cards are worth at face value. And basically, we have the option to hit and take additional cards to improve our hand or stand and give the current time the dealer must hit until there were and as a value of 17 or more. If a player's hand goes over 21 They bust and they lose the game. If the dealer's hand goes over 21 the player wins the game. If neither the player nor the dealer busts the hand with the highest total value that is less than or equal to 21 wins the game. So, this is an example road. Now we have learned about the action space, the observation space, the starting space, what the rewards and what constitutes an episode and how does an episode end. So, we have the termination conditions here. Now, we have started with the solving of blackjack, I also introduced you to a very good book that is reinforcement learning and introduction by Richard Sutton and Andrew Bartow. Basically, this is the the summary of the book. Now, we started with the installing of the So, basically we have started with the installing of all of the all the libraries that we need for the tutorial. So there is matplotlib, NumPy, Seabourn then we started with our import statements. So, we have imported all the libraries that we need. Now here is where we have created our logic we want environment then we have set the environment and started with the first observation then we have observation tuple basically this is the tuple. Now then, we start executing the actions, we have the next step reward terminated state truncated state and the info. Basically, this is the step. Now, we have used the epsilon greedy strategy to solve blackjack. So, what is the epsilon greedy strategy? In this strategy, the agent takes an action that is either the best action based on the current policy with a probability of one minus epsilon or a random action with a probability of epsilon. This approach balances the exploitation of the current base policy with exploration of new policies, which can lead to better rewards in the long run. Basically, this was the epsilon greedy strategy that we use. So I explained to you the what is the Q value. So, this is the class for our black jacket, we have all the methods we have to get actually meant that date method. Now we started with the training. So we have set up some hyper parameters here, I also explained in detail what each parameter does, so you can just play around with these. Now we start with the training loop, we have the episode, we have created a number of episodes. Now we what we do is while not done, we play the episode. And basically this is an area where I got to we started with the rolling length. So this is the plotting of the graphs that we did here, too. This was the code for that. And also I explained to you the URL, what are all the graphs that we have plotted here? So now we've solved logic using key learning. Also. Now let's look at some other methods that we can solve that check. So the first one is Monte Carlo method. Monte Carlo is a more model free method that learns from experience. In the context of blackjack Monte Carlo methods involves playing the game several times, and in keeping track of the rewards obtained for each action, the agent then updates its value function based on the average of the rewards obtained for each state action pair. Monte Carlo methods are suitable for problem like episode tasks like blackjack, so you can use him NASM and try this approach as well. Now let's look at the other approach. temporal difference method. It is another model free method that learns from experience. In the TD learning the agent updates its value function based on the difference between the predicted and the actual reward. TD methods are suitable for problems with continuous tasks like blackjack. Q learning is something that we've already solved blackjack. But let's just quickly revise what Q learning you're learning is a model free reinforcement learning algorithm that learns to optimal policy. By updating its Q values for each state pair. The agents selects the action with the highest Q value for a given state. Q learning is suitable for problems with finite states and actions like blackjack. The fourth method that we have is deep Q networks. So basically we will be solving so in the next example, we will be solving card poll problem that is like an Atari game where you have to balance the card poll, and we'll be using deep Q networks. So I also explained in detail what are the deep Q networks. So let's look at the brief. Deep Q networks combined reinforcement learning with deep neural networks, DQ NS learn the optimal policy by approximating the Q values using a deep neural network. DQ ns are suitable for problems with high dimensional state spaces like image based games to it like an image from these are suitable for image based games. Now, the fifth example that we have is actor critic example. Actor critic is a model based reinforcement learning algorithm that uses the two networks an actor and a critic, the actor network selects the actions while the critic network evaluates the actions taken by the actor, actor critic is suitable for problems with continuous action spaces like that gap. So these are all the different approaches that you can use to solve blackjack. And I highly encourage you all to use gymnasium and try these approaches. So you will find you will, you will have to use some math for that. Also, you will find that online as well. But these are the different approaches that you can use. So now let's go to another example where we'll be using the environment of the carpool. So now let's understand what what what is the carpool environment, and then we understand about deep Q networks. So in the carpool environment, we have an agent that can decide to actions moving the cart, right or woman the cart left, right or left, so the poll attached to it stays upright. So as you can see in this clip, we have to balance the poll. And these are can be random actions, so it could just tilt left twice, or it could go to the left five times, and it could go to the right one. So we need to have all the possibilities covered here. You can find more information about the environment at the gymnasium website. So I have also linked the website link here. Now let's look in detail. As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action in this task reward plus one for every incremental time step. And the environment terminates if the pole of the pole falls over too far, or the cart moves over 2.4 units away from the center. So basically, we do not have a lot we cannot allow the pole to fall 2.4 units away from the center. And for every incremental time step, we have pressed one reward, so we have to maximize the number of times you can balance this cardboard. This means meter performing scenarios will run for longer duration, accumulating larger return. The Cardpool task is designed so that the inputs to the agent are for real values, representing the environmental state position, velocity, etc. We take these four inputs without any scaling and pass them through a small fully connected network with two outputs one for each action. The network is trained to predict the expected value for each action given the input state. The action with the highest expected value is then chosen. So now let's look in detail at how We can implement this network as well. Now let's look at all the methods that we'll be using from Python. So the first is the neural networks, which is imported using torch.nn. Now, in general, this module provides tools for building neural networks. It includes a wide range of layer type, wide range of layer types, such as fully connected layers, convolutional layers, and recurrent layers, as well as activation functions and loss functions. Now let's look at in perspective of a DQ n, the torch.nn model is used to define our neural network architecture. Now let's look at the other method that is the optimization method. Now this module provides a range of optimization algorithm for training neural networks. It includes classic optimization algorithms, such as stochastic gradient descent, as well as more advanced algorithms like Adam and RMS Pro. But let's look at in context of the DQ N, we'll be using towards the optimization module to optimize our neural network weights. Next is torch dot auto grad, this model now, this model in the context of DQ n is used to compute gradients during back propagation. This module provides automatic differentiation functionality which is essential for training neural networks. Why am back propagation, it enables patterns to automatically compute gradients of a loss function with respect to all the parameters of the network allowing optimization algorithms to adjust the parameters in order to minimize the loss. Now let's import the modules that we need. So the first is the gymnasium classic control. Let's import that. Basically that convert that contains a model that we need to cut pole v1 model let's install that pip install Nazeem classic underscore control this is the name so now let's also import all the other methods that we'll need so let's import import NASM as gym mat. So basically these are all the inbuilt models method inbuilt is doing random randomization what method is matplotlib this charting library plot lib it's used for making charts there's import map plot dot pie plot as PLT I think one of the most common code lines in Python code named tuple and that so basically we'll be using these data structures to manage the training maker tools input count meter reading this now let's import all the torch models for torch torch.nn has an OPCOM OPCOM import.nn dot functional functional f Now let's also create our environment so we need this will be gym dot make card poll click poll new one now also let's set the inline setting for matplotlib. In AD lib.net I think I forgot to import this little read that quote because I think we can directly make it in Leno's in collab. If we end Google collab, we want pyplot.to be plotted as inline. So that will be displayed below the cell if is IPython so basically this is Google collab again from ipython. Input display because we need the display to display the charts do PLT that ion. Also, if you do not have a GPU, we can just add a condition as well. And if you do have a GPU, then we can increase the training. So let's set the variable. If we have a GPU, this CUDA is the art.cuda.so. Basically, this is the method to understand if we do have a GPU available. And you can just go to settings in tools, you need to go to settings, and Google editor in the run time, I think you need to send it around. So it will change and time time. And you can just add a GPU or so I will do that, I'll do that as well. So it will basically give us a lot more computing power. So now we've completed the basic in code where we have imported all the libraries that we will need. Now let's look at another concept that is very important in this in this example, or to solve cardboard. So now let's look at replay memory. So what is replay memory, it is a technique used in D enforcement learning to store and manage the experience of an agent during training. The idea is to store the agents experiences as a sequence of state action reward next, state tuples. So basically, we have just four values, which are collected as the agent interacts with the environment. During training, these experiences are used to update the agents policy and value function. So basically, this is the we collect all the actions performed by our agent. Now let's look at what the importance of replay memory. The replay memory allows the agent to learn from past experiences by randomly sampling a batch of experiences from the memory buffer, rather than just learning from the most recent experience. So it's like the, so we instead of just learning from our recent experience, we take a sample of all the past experiences and make a decision based on that. This helps us to reduce the correlation between subsequent experiences, which can improve the stability and convergence of the learning algorithm. In addition, by storing experiences in a buffer, the agent can reuse past experiences to update his policy and value function multiple times, which can further improve learning efficiency. The replay memory is typically implemented as a fixed size buffer or queue that stores the most recent experiences. When the buffer is full. New experiences overwrite the oldest experiences in the buffer. During training, a batch of experiences is randomly sampled from the buffer and used to update the agent's policy and value function. This process is repeated iteratively until the agent converges to an optimal policy. So basically, this is an example it's a basic thing. It's a very basic concept, we store all of the training data until the queue is full. And the buffer memory that we have will keep using it as we go ahead. Where does the concept now let's look at how we'll use the concept of replay memory in order to implement our DQ n algorithm. So what we'll do is, let's understand that so we'll store the transition that the agent observed, allowing us to reuse this iterator by sampling from it randomly the transitions that build up a batch or D correlated, it has been shown that this greatly stabilizes and improve the deployment training procedure. Now let's also understand how we implement this in code. So we'll have two classes. One is the first will be transition class it is a named tuple representing a single transition in our environment. Basically, this is our sample input. So this will be represent one state and essentially map state action pairs to the next state reward result with the state being the screen dividends image as described later on. Next will be the replay class, a cyclic buffer of bounded size that holds the transitions observed decently. It also implements a dot sample method for selecting a random batch of transitions pertaining. Let's implement our replay memory class. So the first will be the transition state that we have which is a tuple which is the named tuple by the name of transition and then we'll have a state action next We have recently this is our condition tuple. Now let's also create our class, which is the deeply mini class. So it will be a class we call replay memory, then object. Let's do def underscore underscore in it, self, comma, capacity. And then we'll have self dot memory. So basically this will return the full memory buffer. So it is equal to that. I don't know how you pronounce it, but I pronounce it as that it could be called as DQ also, depending on the accent here, max length is equal to capacity. So basically, this is our max length. After that it it will overflow and then we'll reset it so we'll use one transition into this memory was here it says no memory will have self comma REM sleep are some arguments as well. Basically, it is also comment save so I think it is double save a transition save raishin. Memory node append transition star arcs Okay, so basically this will append to our memory buffer, let's create another method of the sample. This method will help us sample our buffer remember, before memory cells come up bad size. So we have a capacity and the batch size. So it the capacity could be 50 and the batch will be 10. So this is how the there's no return. random sample. Okay. Still not memory. Let's size define underscore underscore length may give us the length of the buffer at any point. Return length or size but memory. So yeah, I think we have done it completed. Let's also look through the code again. So we have defined the transition tuple and replay memory class. Now we have fulfils your state action next state. I think I need to do that this year awards. I think it is rewarding. And we will have other class as well. So we'll have the push method. This method takes in a state which is state action next ID tuple as input and creates a transition object from it and append it to the deck. If the deck is already at maximum capacity, the oldest element in the deck is removed. So this is how the append method works. Let's look at the sample method. This method randomly samples a batch of batch size, which is the batch size parameters that were passed experiences from the deck and returns them as a list. So this is the sampling method. Let's look at the length method. This method returns the current length of the deck. Now let's understand the DQ n algorithm in detail and how we use the algorithm to solve the Cardpool problem. So it is a reinforcement learning algorithm that uses deep neural networks to approximate the Q function in a Q learning algorithm. So in short, basically it is that but we need to understand the steps involved to solve the Cardpool environment. Let's look at that as well. First we need to initialize the Q network with random weights. The second is sample and action using an epsilon greedy policy. Basically, we have covered this in our blackjack tutorial, which selects the action with the highest Q value with probability one minus epsilon and the random action with a probability epsilon. Execute the action and observe the next state and reward. Store the experience to Built in a replay buffer, so we have implemented the class already sample a mini batch of experiences from the replay buffer, we've already created the method for that, compute the Q target values for the mini batch using the Bellman equation. So basically, this is how we calculate the t Q value I'll explain to you when we read the code, compute the Q values for the mini batch using the current Q network. So we'll have that value. And we just need to compute the Q value for that. Compute the loss between the Q values and the Q target values, and update the Q network parameters using gradient descent. And we'll keep doing these steps until we reach convergence or we reach a fixed number of episodes. So I will explain in detail what what we are going to do to the end of the cardboard. The DQ n algorithm uses a target network to stabilize the training process. The target network is a copy of the Q network that is updated less frequently than the Q network. This helps to prevent the Q values from oscillating during training. So that's what happened in our budget. When we are solving that check. In Cardpool environment, the DTN algorithm learns to balance the port on the cart by moving the cart left or right. The queue network takes the state of the environment as inputs and outputs the Q values for each possible action, the DQ n algorithm learns to maximize the Q values by updating the Q network parameters using gradient descent. With enough training, the DQ n algorithm can learn to balance the pole on the cart for extended periods of time. So basically, this is all the steps in what and how we are going to solve the Godbole. Example, we are going to balance out the card pool for a very extended period of time. Now let's go to our cue network. So let's take some notes here. So far model will be a convolutional neural network. If you take the difference between the current and the previous screen patches, it will have two outputs representing Q, S comma left and q s comma right, where s is the input to the network. That, in effect, the network is trying to predict the expected return of taking each action given the current input. So basically, this is our cue network. This is the full implementation will have two outputs. And the network is trying to predict the expected return of taking each action given the current input. Now let's implement our DQ N class. Let's add the class here. That's right class, the Q n and n n dot module. So let's do def, underscore, underscore init, underscore underscore, self comma n, underscore observations. Comma and underscore actions. So I'll explain to you in detail that just complete the class first, and then we'll read the comments as well. So it will be very easy to understand. We self dot underscore underscore in it. Self thought we left three layers, let's do all the three layers here to three, the equal to and then thought linear and underscore observations 128. So we have sizes and 128. Let's do that. We'll the next one will be 128 by 128. This is how it will be now and the other one will be the exactly the opposite of this will be 128 by the number of actions comma and underscore options. So we've implemented the new folding in it consulted. Let's go and define our method. That is the forward method. And we have to do self dot causes comma x and x is equal to f dot value self dot layer 1x And then we have the layer 2x. And then we'll return the cell got layer 3x. So we'll basically go to three layers what the code now let's understand what are we trying to do here? So we have the cue network which is Mighty perceptron, so let's just keep writing it down yet mighty per mighty layer or scepter on any point, you can just use perceptron with three layers. So, this is what we are trying to achieve here. Now, we have what the input is it is a tensor. So, the what will be the input, so it will be a tensor of the size and underscore observation is our size which is single this is what we passed here. Actions is our input to the network this is basically the state input that is the state of the environment, state of environment to the network. So, we have the fully connected layer with 120 neurons, followed by a value activation function. I'll also explain like why we're using that the second layer is also a fully connected layer with 128 neurons and a ReLU activation function. The final layer is a fully connected layer with an actions where an action represents the number of possible actions in the environment, which represents a certain number of possible actions. environment now, let's look at the forward method what we have implemented here. So, if you take the input as the tensor that is the X tensor, as it passed through the first layer of the network using the value activation function, pass, take input, we pass to two or three layers on the network when. Basically, this is what we're trying to do here. And what we are also trying to do is, so, we'll pass it to the state to the network. And what we will get this, the output is the corresponding neuron in the final layer. So, when you pass here, we'll get an output we'll pass it to the next year. And in the third layer, we finally return. So and what will happen here, so during training, the Q network is updated using the Bellman equation that we have discussed already to minimize the mean squared error between the target values and the Q values. Yeah, so let's look at that. Now let's look at the training of our DQ n. So what we'll do is we'll have a Q network, which is updated using the Bellman equation to minimize the mean squared error between the predicted Q values and the target Q values. The Target Q values are computed using the Q network but with the weights frozen and not updated during the current iteration of training. This, this helps to stabilize the training process and prevent the network from overfitting to the data. Okay, now let's look at all the other parameters that we will be have. Or I can say parameters and utilities as well. So first, we'll have select action, it will select an action according to the epsilon greedy policy, we have again covered this in the blackjack tutorial. In the back when we were solving blackjack using toolani. We simply put will sometimes use our model for choosing the action. And sometimes we'll just sample one uniformly. So there is no steps for that. But we will see how we can do that. Now, the probability of choosing a random action we start at EPS start and will decay exponentially towards EPs and EPS decay controls the rate of decay. So, basically, this is the set of we do not have any particular target value. So as we go along, the values will keep changing. And we have to decide whether we want to take a random action or we want to take from the action that we have on the board. Next we have plot durations. It is a helper for plotting the durations of episodes, along with an average over the last 100 episodes. The measure used in the official evaluations. The plot will be underneath the cell containing the main training loop and will update after every episode. So we'll plot this training durations as well. Let's now go to the code. So let's implement all the parameters that we are going to need so I'll have pad size, that size equal to 1.8 comma You can do zero point. So I'll also explain like this, just complete all the parameters for your start. Start is equal to 0.9, we have EPS, and you will do 0.05. So basically this is when we will end. This is our epsilon and lbps decay rate, which is going to be 1000. And we have the tall, which is the one who is 0.005. And our learning rate is going to be one e raise to minus four. Yeah, this is our this is what is going to be Yep. Now, let's read the comments as well, what is going to be the batch size number of transitions sampled from the labor force it is grandma this is what is the discount factor as mentioned, we will do the discount factor. So what is going to be the EPS third quarter is the final value of episode yesterday the starting value of the epsilon basically, we can just write it next to it as well. So we don't have to waste the time just writing now, it is a starting value of epsilon. Let's look at the EPS. And this is the ending value of final value of epsilon. Let's look at the EPS became in EPS API educate controls to equal mediaspace. controls the rate of exponential decay decay of epsilon higher means a slower rate. So first we'll start with a very slow rate. And then we'll keep improving on our learning rates. So there is going to me higher means slower and slower decay. Let's look at our this as well is that is the update rate of the target network? What is tau? Tau is the update rate of the target network. And what is going to be the LRT it is going to be a learning rate. So this is going to be the atom optimizer. So let's look at now what is this is the perfect time to understand about the atom W optimizer, Atom optimizer and atom optimizer. So basically now let's look at the atom optimizer. So basically we've defined it you're eliminating all these parameters and eliminating it for the atom optimizer. So this is the learning rate for the RM optimizer. So Adam, adaptive moment estimation is a popular optimization algorithm that is commonly used in deep learning. It is an extension of stochastic gradient descent. If you do not know about that, it's okay, you can still watch the rest of the video, which is the most basic optimization algorithm used to train neural networks. The main idea behind Adam is to combine the advantages of two other optimization techniques, add a grad and RMS problem. So if you do not know about that as well, it's okay. In the DQ n algorithm. We use Adam optimizer to update the weights of our neural network based on the gradients of the loss function with respect to the parameters, so we'll define our loss function as one minus epsilon. Specifically, we use the Adam W optimizer which is a variant of the atom. So we have defined it here which is a variant of the atom that also incorporates weight decay regularization. weight decay helps prevent overfitting by adding a penalty to the last function that is proportional to the magnitude of the weights. By adding this penalty the optimizer encourages the network to learn simpler and more generalizable representations. The learning rate is a hyper parameter that controls the step size taken during optimization. It is an important parameter to tune as a high learning rate can cause the optimizer to overshoot the optimal weights and lead to divergence while a low learning rate can result in slow convergence and getting stuck in a local minima. In DQ n algorithm, we set the learning rate to one e raise to minus one. So basically, this is what this is why we're going to use that optimizer. So we don't get stuck in a local minima. So, we need to control the learning rate. And for the content to the learning rate, we need that and W optimizer. In summary, the Adam W optimizer is is a widely is a widely used optimization algorithm in deep learning. And it is used in DQ n algorithm to update the weights of the neural network based on the gradients of the loss function with respect to the parameters, while also incorporating weight decay regularization. So this is the very critical role played by Adam W optimizer. Now, let's so now we will learn about the atom optimizer. Now let's learn about now let's read the code for the other two other utilities that are pending. So the first variable that we have is the end actions, this is going to be NV dot action, the action space. So basically, this is going to be the action space of the carpool environment. So let me just keep it up, let's just keep writing the number the comments as well. Number of actions from them action space. Also, now let's do that net number of state observations, which is going to be state comma info. So, basically, this is going to be that tuple that we talked about the content all the information related to the current state, the current state that we are in and of the reasons is going to be length of state. Now let's create our policy net, which is going to be a class it is going to be an instance of the DQ N class. We have this Ruby and underscore operations comma and underscore actions not to device specific devices that what we have set up to target net as well. Targeted net is equal to same thing. Now let's load the state target dot score net not load underscore state underscore the policy this coordinate got state I think it is policy and I understand the state. Now let's complete the optimizer so I'll just come back and explain to you everything is just complete the optimizer also now basically, we this is going to be the torch method OChem but we added W we discussed about atom W I also explained to you the atom W optimizer and our atom optimizer is going to be policy underscore net dot parameters. Okay parameters needed we have already defined small error is equal to capital error. Total Error is defined here. one equals two minus code from AMS guard AMS grad, so like listening to you what this means as well. Let's complete the code memory we'll be taking the last 10,000 episodes for our memory. Do replay. Memory 10,000 We passed in to our buffer size is going to be 10,000. Let's define the steps done as well. With me, this is where we currently are. Okay, let's let's go ahead and just complete all the methods let's do the Select action as well. Basically this is going to be the method that will help us select any action. So if I have a state, it will just tell us which methods are and steps underscore done sample is equal to random sample I think it is wrong it is random not random and with the PPS on the score threshold is equal to EPS score and plus EPS underscore start we have defined all these parameters before VPS the scope and into we have to divide it by offering when you go to the next line so with me goes less mad not mad dot exponent of minus one towards star off steps done by up to K yeah this is going to be the equation and then let's do steps done once we're done with this little loop do steps than possible will rent payment from those sample to them UPS underscore threshold and do with torch.no Grad so basically we'll be doing that we'll do T dot max one minute and the largest column of each row this column value okay what does it mean this is a row we have the second column second column on max next up where Max element was was fun so, we pick action with largest yeah so, we need to like we need to optimize for the larger what will basically this is what we are going to do expected reward let's do return policy score net state dot max not view one comma one now we need to do the as of this as well is the ELS dot pensar en v dot actions so we'll just return the action space action underscore space dot sample response. So basically this is our we're going to do is we'll see if we want to take a sample one already to keep continuing the path that we are on. Do that okay, now with the device is equal to device and command D type call sheet.com Okay, so I'll explain to you all these utilities let's just complete all the methods while we're at it. So does equal to Episode underscore durations Okay, So what we'll do is just go over all the methods that we completed. So, the N actions is the number of actions in the environments, no environments action space will be the gym environments, state comma info are obtained by the setting the environment and observations is the number of features in the state. Let's just drill down as well in the state let's do the target net is initiate with the same weight as policy net Okay. target net initialized with the same weight as policy net then we have the optimizer which is not an optimizer Adam W optimizer optimizer is LM W optimizer. Adam W. Normally optimizer pytorch with the learning rate and other hyper parameters specified earlier. So we use it to optimize the weights. Use to optimize weights, we already passed all the parameters that we need. Let's go to the memory. So the memory is an instance of the replay memory, and it has a capacity of 10,000. Okay, so let's do a site wide recall you can store intense experiences which will be for training for training just ever done. Because it is it is used to keep track of the steps number of texts keep track of number of texts taken by the date. We're going to start with we're going to start with zero. Now let's understand the step action method as well. So it takes the current state as input, let's just write down and code this current state and returns an action so it could be either that the action is it could be selected by choosing the action with the highest Q value. So this is going to be the ISP value. Or it could be that we have taken a random so how we're going to decide so we're going to decide by this expression. So we had discussed earlier that we're going to be calculating the threshold value and if there was a sample within the threshold value, and we have no grad then we go to the maximum or too high SQL that we have that we find or we take the random random approach. So basically this is going to be the envy that action or sample everything the sample and the device will be either a CPU or a GPU in our game it is the GPU that we have defined earlier. And last we have episode durations interviews to keep track of the duration we can often each episode so now let's also complete the plot durations method which will plot all the duration of each. So net is taken by the agent we have shows multiple calls. Start with the PLT dot trigger. Mainly we're going to plot we're going to be using biplot here when durations underscore t is equal to torch dot tensor. We have the episode underscore duration. Format D type is equal to torch dot float. If so underscore result The PLT dot title wizard, we're going to be keeping the title and result as a rule that we have also basically the first time we change the title and then we'll plot the crosshairs so that now training painting graphs PLT dot x, x level is going to be episode and then we have PLT dot y label, which is duration. Then we have PLT dot plot durations. Obviously, we think this munition under different durations might have taken not NumPy will convert it to the numpy array. And then we'll take 100 Episode averages. So let's start it on. First, let's do that clip length greater than 100 doesn't equal 200. Then means durations underscore t naught unfold zero to slice it and we'll take the mean and then we'll take the other viewers what little means is equal to Torstar cat it's going to be towards our cat towards third zeros 99 And then we have the means then we have PLT dot plot dot NumPy let's add a pause when you have PLT dot 0.001 pause so that plots are updated. And then we if we have a display so basically I just write it down so that it works on all the environments. If we just run it in your local VS code also it should work right. So it will be show underscore results display dot display dot DCF and display display dot here or wait is equal to display dot display PLT dot GCF. So I'll explain to you what all these plots mean. We're done. So we have completed the method. Let's now go and write some comments. So but it basically it will be very clear for all of you. So this method is used to visualize visualize the training progress of the DQ N basically, this is the method here. Now first we'll create a tensor from Episode durations. So basically this is the keep track of the lengths of intuition. And then we'll get into using matplotlib. And if we have the show result as true then we'll label label it as dessert or as it will be training. And the x axis is labeled as episodes the Y is labeled as duration. And we have a function that is called we have a variable that is called durations underscore t is a blue line and overlays With a red line that shows 100 Episode moving average of the durations, so let's just write it down. So the 100 moving average of the durations Yep, so while we wait for just a second so that the plots are updated it'll come in real time. And if you have a Jupyter Notebook, then we'll use can use this method as we'll just use a normal method so now let's start our training loop. So, let's create our optimized model here called model from spaces if length match network size then return we do not need to optimize at the start then we have the transition when we take a memory that sample basically this is the sampling we will do the sampling okay, then we have the batch match is equal to transition so, visibly this looks a little bit complicated, but I'll explain to you like why we do this basically we transpose the batch and it will convert the batch array of transitions to transition a batch or is it converts batch or a batch of transitions to trans transition of batteries now, let's compute the mask of non final states 30 being non underscore final underscore mask equal to torch dot insert then we have that tuple do map iterator than the continue number is says not none match.com device this is done so let's write down what this method will do what we're planning to do, we're going to compute our mask off non final states and the batch limits okay and the final state will be where the simulation and basically that means that let's do I think we have to provide a parameter can be D type SQL to towards that Boolean now let's do the non final next states recall that cat as far as that's not next step is not null. Then we have the state underscore batch equal to torch dot cat batch dot state And we have the action but the action bets New Order batch Yeah, this is bad dot action this is bad got reward so now let's compute the Q value now state score action let's call values equal to policy net basically this is the policy that we're planning to use policy net state school beds not the other one comma action we'll compute the key value here it's now compute the next state values also. Next underscore state which will be the final state if it is ending then it will be zero in case the finest gets the state was the final state will be taught not zero match score size comma device is equal to device us now with no no torch or that no grand do next underscore state underscore values non underscore final mask SQL to target net it looks a little bit complicated but trust me when I explained to you after writing the code they were pretty clear what's missing there looks code looks intimidating but what we tend to do is pretty simple got Max one of zero now let's also compute the expected value expected underscore state underscore action value I think we I forgot that yeah we need to do this first you can do this first cannot underscore values equal to next state values values to gamma let's see what we what that about losses well I'll explain to you what police teams losses criterion is equal to n dot mood one loss that's the name and also the call to state score action score I use expected the school state school action expected sports score action score values squeeze one. Now let's optimize the optimizer dot basically our query and the way to do optimizer dot zero underscore grad loss dot backward is in place so it must be looking like It's pretty tough but trust me I'll explain to the Medicare then dot dot utils not clip grad value add value underscore policy not net that parameters commander then verse will keep the value now let's do optimizer dot now, let's look at this. So, first we check if this is a mini batch, so, we have the Optimize method right. So, this is deep Q learning that is working what we already know. So well let's check what we do. So first we replay memory. So, first we check if the replay memory contains enough samples to fill up a mini batch let's write it down if check if we have enough samples for you if you do not then return nothing then do nothing okay and then let's transition. So, first then we what do we do is we take many bytes extract extract extract I mean us me myself transitions from Epson comma reward comma next step from that now then also what we do is we calculated calculate the expected Q value and it is the sum of the immediate reward. So, basically we have it here too, we have the gamma as well I cannot see grandma Yeah, so, basically just search for Yeah, this is where we do the expected state calculation. So, what we do is okay good calculate the expected Q value Expected Q value you value for each transition using the target network basically we have created our neural network here. Now, if you do not know how to create a basic neural network, no issues so, basically it is the sum of the immediate reward and the discounted Q value of the next state. Okay, now under this contract is controlled by the gamma. Okay, now, let's see the function. Now what we'll do is we'll calculate the q value predicted by the Policy Network for each transition. And we'll also select the queue corresponding to the action taken by the agent. So, basically, what we are doing here, we are also selecting now, we need to calculate the hover loss as well. The upper loss is a smooth approximation of the mean squared error loss and is less sensitive to outliers. So, we need to be less than smooth. What is our loss it is a smooth approximation. We do not understand how the loss knows us you can just look it up. But it's a pretty good metric for calculating data loss. Listen that basically, this is our losses. And now finally we back propagate now to the Quality Network and update the parameters using the Adam W optimizer. And we also have clipped it to a maximum of 100 To prevent the exploding gradient problem, so maximum value is equal to under to prevent exploding gradient problem so now, let's go on to the main loop now maintaining loop. So we have completed the training part of it now we need to create, we need to start implementing the algorithm. Let's do that now. So what we'll do is if torch dot CUDA, basically, I'm just making the code in such a way that if you do not have a GPU, you can use this code, you don't have to make any changes. So then you need to make number underscore episodes, you can just keep it 50. And also, if it is available, then I will keep it somewhere else. I'll do it now let's loop over the episodes for I underscore episode in range of number of episodes to date, so I don't I think you all know what this does is really set the environment and get the current state and the all the information that we need. Yeah, let's just write it down again. Visualize the environment. Get state to state is equal to torch the tensor state comma D type is equal to call that float 32. Comma device is equal to device and let's do an squeeze zero. I think you all know what we're doing here. I've already explained it once. Let's look let's create another loop. Now what do we have the encounter? Number of counter the reaction is equal to cylinder since we need to select an action here. Select underscore action false was for the first time we do not want anything with by the state and we want observation. Comma, reward comma terminated comma can get it any other parameters that we get is equal to NV dot step is equal to send your items where you could take action on items. And then at Maker step forward, so we'll need a new observation they were terminated and you're truncated well. Yeah, now let's give the final reward is well, we've taken a step forward now let's look on the reward or the tensor reward karma device, welcome device. And we have done is equal to terminate it or truncated. And we have if Yeah, so we can either have that we have completed the number of steps or that we have the card if the card pool has fallen off 2.4 meters away from the center to easily that could be that next underscore state is equal to none. As we need to do you find the next state? We'll do that. One next state will be we call it a tensor tensor observation variation comma V type V Going to torch dot float 32 comma device, the call to device dot and squeeze, zero now let's add this next step in memory. So we can do that using manager push, we have created that in our class action, we already have all this action. Next, we have the next video. And we have the reward. Now store, just keep that in comments as well store the transition and then move to next, let's move to the next as well. So basically, we move to the next step, let's optimize the model. Now. We have already created the model for that, the method for that. Now let's do the soft update of the target network weights. So we already have that new target underscore net underscore state. The score did this mean we need to do it for both the target and the policy next. goal to target and the score net dot state underscore dict. So if you find it pretty difficult, it's okay. I happen to be the first time. But when it comes into code, I just go everything once again. So everything becomes very clear. For Kean policy net, I think we have to take this one is the policy net, that's to training that state. And then when to do key. The call to policy net underscore net underscore state underscore of key swing into to multiplied by the tau tau plus target net state t. So the old value also into one minus tau. Basically this is what we're planning to do now. One minus tau is what we're doing. Now let's see if we are done. We are going to do a software update. And then we are done. Episode dot.so We are completed one episode two, I think I've explained to you what is the difference between Episode and an iteration. Basically, an episode is the one sequence between a start and an end to basically in the cardboard instance, the start will be when we started the initial state. And then if we fall of 2.4 meters away from the center, or that we have completed the game, basically we have achieved the maximum state that we could we have a number of times we could have, we could have played this game. So this is one, this is going to be one one time that we are going to play the game basically. And we are for if you have a GPU, we're going to do this 600 times. So basically, we have to play this game 600 times. There's going durations and there's to break out. Now let's complete it. So finally, we have completed then we are going to plot all the durations. Basically what I encourage is we can just keep changing the number of durations and check the results. Basically, that is something I want you to do. And I'm not going to do that. I'm just going to stick to the 600 episodes, because I have a GPU I have and PLT dot show done with the cart pole environment as well. Let's quickly summarize everything that we have learned. So what the agent has to decide it has to decide between two actions moving the cart left or right. So basically this is so we do a loop here. So as you can see, either the carpet will move left or right and we need to balance it out. So now I've explained all this in detail Let's look at all the methods that we have imported. So, we have imported the torch that nn which is used for our neural network, we have imported that thought is that octane method, which is for optimization imported towards an autograph, which is for automatic differentiation. And also I've explained in detail what each of this is you would extend each of this in detail as well. Now, we have started with the import of the module that we need also we have started. So, basically we have installed it and here we have imported all the methods that we need, here we have set up the environment. And also I have also made it such that you can run it in the Google collab notebook as well also in just any python script as well. So, just accordingly, then we have detected if CUDA is available, which is a GPU. Now, I have also explained to you what is the concept of replay memory, basically, it is a technique used in reinforcement learning to store and manage the experiences of an agent during training. So, basically, we will make use of this, we have saved all of the steps that the agent takes, and use that to make a future decisions. So we're going to make two classes one transition class and one replay memory class to basically we have to transition here and then we have the replay memory class. And we are going to use the DQ n algorithm. It is a deep Q network, which is a reinforcement learning algorithm that uses deep learning deep neural networks to approximate the Q function, I will end with the Q function and we have also learned about the Q learning algorithm when we were solving that chapter. So, we have basically I have written all the steps that we will be using to solve the Cardpool environment using the DQ n algorithm. Then we have implemented the class for that as well. Then we come to the training section. During training the Q network is updated using the Bellman equation I have explained the Bellman equation as well, to minimize the mean squared error between the predicted Q values and the target Q values. The Target Q values are computed using the Q network but with the weights frozen and not updated during the current iteration of training. Now, we have some important concepts here. So basically, this is the Select Action, maybe P start and we have plot durations, where we paste the case what I have explained to you all these hyper parameters as well. Let's go to a very important concept that is Adam optimizer. So Adam adaptive moment estimation is a popular optimization algorithm that is commonly used in deep learning. It is an extension of stochastic gradient descent, which is the most basic optimization algorithm used to train neural networks. The main idea behind it is to combine the advantages of two other optimization techniques added an RMS block. Here I've explained in detail how we'll be using it in our DQ n algorithm. Next, we get the number of actions from the geometric space we set up, we set the environment and get the initial state, we set up the number of features that we have, we have the atom optimizer, we have initialized our training neural network as well, we have the replay memory which is a 10,000 buffer, which is contained 10,000 last 10,000 episodes, then, basically this is a training loop, I explained in detail as well, then we have the optimized model, this method will optimize the model. So basically what we need to do is we need to just take a mini batch. So and also we will be using sampling as well. To have gone to that now you have started with the training. So we'll take that if CUDA is available that is a GPU is available, then we'll do six minutes because or else we'll only find it episodes. So yeah, this is the training loop. And then we have plotted the end result. This chart I've also explained in detail how we use this chart to understand the performance of our algorithm. Now we've completed our solving of logic using Q learning. So basically, let's move on to the advanced topics that we are going to do go to cover and get you will explore after learning this beginners tutorial. So you could explore deep reinforcement learning. Deep Reinforcement learning is a combination of deep learning and reinforcement learning that uses neural networks to approximate the Q values in Q learning. Deep RL algorithms such as deep Q network and actor critic have been successful in solving complex tasks such as Atari Games robotics and autonomous driving. So, basically, this is what you can explore, you can dive into autonomous driving now. Policy gradients policy gradient algorithms directly optimize the policy function by adjusting the parameters of the policy to gradient Set. This allows the agent to learn a policy that maximize the that maximizes the expected reward without competing the Q values. The advantage of policy gradient algorithm is that they can handle continuous action spaces, unlike Q learning. So this is pretty interesting. You can also explore this. Now let's move on to multi agent reinforcement learning. Yeah, so this is pretty interesting. Now you can have two three cars in an environment and you can simulate that. And you can use petting zoo for that. So petting zoo is a brother of Guinness. And it allows it gives us an environment for multi agent reinforcement learning. Multi agent reinforcement learning extends reinforcement learning to the scenario where multiple agents interact with each other in a shared environment. In multi reinforcement learning, each multi agent reinforcement learning agent learns his policy based on the joint reward received by all agents. The opening a gymnasium toolkit provides environments for MRL such as the classic prisoner's dilemma game and cooperative navigation tasks. So there's some pretty interesting imitation learning, also known as learning from demonstration is a technique that trains an agent to mimic the behavior of an expert. This approach has been used in various applications such as autonomous driving, robotics and game learning. And genbrain Open AI gymnasium toolkit include environments from imitation learning such as Motoko humanoid locomotion task flow, this also sounds pretty interesting. Now let's look at the last topic that you could cover in the advanced topics to discuss transfer learning. transfer learning is the technique of transferring knowledge learned in one task to another task. In reinforcement learning, transfer learning can help speed up the learning process and improve the performance of the agent. The Albany gymnasium toolkit provides environments for transfer learning, such as the classic inverted pendulum does and card polls wind up just so now we talked about multi agent reinforcement learning, so let's check out putting zoo documentation that I mentioned. So what we can do is here you can check out all the different types of environment that are available. So what are the games we have pulled up? Here we have Mario Bros for multi agents of Luigi and Mario here on the action space is defined so I've explained all the concepts here so it wouldn't be very difficult for you to start implementing these. We have Space Invaders will be basically these are different types of games available. Let's check out classic games. We have chest Solibri chest as well. So I highly encourage you to go first Lex was when you understand the basics, then I highly encourage you to go out and check out the petting zoo documentation and study creating your own environments and your own agents and solve some real world problems.