hey this is Andrew Brown your favorite Cloud instructor bringing you another free Cloud certification and this time it's the adabs ssops administrator associate certification also known as the S SOA C02 and the way we're going to obtain certification as always is going through the lecture content uh doing the work for real in our own inabus account utilizing the HandsOn Labs instructions and as always we provide you a free practice exam so that you can go aset exam get that certification put it on your LinkedIn a resume to better get yourself a job in cloud or in a Dev hops rooll I just want to remind folks here that uh this content is made available free here on free Camp though if you want to support more free Cloud certifications just like this one I strongly recommend that you purchase the additional study materials which comes with things like additional practice exams layered content technical support and more um you know me probably by this Point I've taught a lot of different types of uh courses around Cloud here adus Azure gcp terraform kubernetes Linux and more um so you know you're in good hands but let's just jump into the course uh and get to it ciao hey this is Andrew Brown and we're at the start of our journey asking the most important question first which is what is the sis Ops admin associate so the ssops administrator systems operation administrator is an administrator certification when I say administrator I mean like I think it administrator uh Cloud administrator um and it's going to teach you things like automation of cloud infrastructure via scripts troubleshooting Cloud networking issues uh performing automated or or Endor no touch maintenance on compute uh monitoring and observability of cloud resources and the course code for this certification is the s o s03 I'll get my head out of the way so we can see the full uh code at the time of this video the um solution Architects been s03 for quite a long time why is this not s03 I don't know ad us really should have updated the course code by now um but I just want you to know that if you are taking the SO3 I bet this course isn't that old but as always make sure that the course code matches for the certification you're taking so you have the latest content uh to best pass the certification um this specific uh certification the ssops admin associate has has been considered the most difficult general knowledge associate exam for ads at the associate level um I've always said it's the developer but it really depends on your background I I think a lot of people struggle with um it networking understanding uh how networks work and so that's where this becomes a challenge for a lot of people but if you have a developer background you might find this harder because you don't have the networking background if you have a networking background you might find um the developer harder but uh this one is generally the one that is is least taken but has extremely valuable knowledge so I really strongly recommend that this one's taken alongside the solution architect associate and the developer um but who's the certification for well consider taking this specific certification if you want to work as a junior devops engineer um ssops administrators is not a common job job title I will see it sometimes but generally we call these Junior devops Engineers um so you know just look for those rules online if this is the kind of stuff uh that you want to do um this is a role if you enjoy maintaining existing Cloud infrastructure and providing technical support um think of what a junior devops engineer does a junior devops engineer supports a senior devops engineer or just a devops engineer so um a lot of people want to get a devops because it sounds fun automating all this infrastructure but understand that this is more of a supportive role um to other devops engineers and uh it can be a customer facing role um that would lead into that so just understand that difference there so if you enjoy the following tasks like Building Systems that support applications working with Linux working with Cloud networking or uh learning just enough coding to work with basic scripts well this is the rule for you but let's take a look at the full path of um certifications so even from the solution architect associate which I just released less than a month ago this has changed dramatically uh because adus has done a bit of a shuffle um they used to have more Specialties but what they've done is uh they scrapped like I think like four or five of their Specialties and they've now uh introduced uh the AI practitioner the data engineer and the Machine learning engineer as of this video they're all in beta but I suspect that they are going to come out so I'm going to treat this as if this is what it actually is now because I think that's what's going to happen um but there's a lot of paths that you can take uh when you are thinking about your journey and there's no wrong path these are just suggested paths that I I think that you could take for specific roles so I going get my pen tool out here the idea is that you almost always want to start with the Cloud practitioner never skip the certification even if you don't want to set the exam do not skip the content um in my courses I make a lot of content that I do not bring over into the associates other creators will uh pack that stuff in there but the problem is is that we need to spend as much time at the associate level doing labs and that fundamental content needs to stay in that fundamental certification so if you have yet to do so please go do my cloud petitioner first it's not as long as these courses but we're going to cover stuff uh especially pricing um uh in that one pricing and billing so make sure you take that one first uh there's obviously the new AI practitioner which would lead you to maybe data engineer machine learning but let's focus on the ssops administrator where we are here uh so the idea is that when you take the cloud practitioner a lot of people will go take the solution architect first there is no wrong one to choose first whether it's Sops the developer the soltion architect associate the only correct answer is to take all three of them at the same time because they really have a lot of overlap there's like 30 40% overlap um between each of them and really if you don't take all three of them you're just going to have incomplete knowledge uh so some people just think that oh well I want to become a devops engineer so let's skip the ones that are not necessary but I'm going to tell you the associate level you want all three of those general ones but anyway you can go take the soltion architect then the ssops then the developer or the developer in the sis Ops and from there you can go on and do the devops engineer if your goal is the Ops engineer the sis Ops administrator and the developer are a must absolute must because uh the combination of those two certifications is the devops but expanded in a much more uh deeper um curriculum okay and then if you want to uh specialize or uh uh do additive things you can go over and add the security certification or Advance networking I do want to point out that ADA certifications do not validate programming technical diagram code management and many other technical skills that are required for obtaining technical roles ads assumes that you are grabbing this information elsewhere um so you definitely need more than just these certifications to land your roles okay and I'm going to repeat that so you absolutely know that how long is it going to take to pass the certification very similar to the solution architect associate we're looking at 60 hours on begin or end and 20 hours on the experience end um so you know if you have your Cloud pred conditioner but maybe you haven't done your solution architect associate yet you're looking at 60 hours if you've done the um solution architect associate you can cut that down to 40 to 30 hours um for those who are very experienced already know ads very well and they're just trying to get the certification to prove they already have the knowledge you're looking at 20 hours or less just because there is a lot of um little things you have to learn doesn't mean you can't do the work but you need to prove that you know uh uh the knowledge of it of us fully and so you're going to have to dig into a lot of very specifics of services um so an average study time would be probably 24 hours 50% lectures in Labs 50% practice exams though I should really give more emphasis on Labs because my labs are getting more long longer these days uh recommended study is one to two hours for uh 24 days it really does take a month with each of these associate certifications if you're doing them individually if you do all three generals at the same time you can uh shave off a month so that's why I strongly recommend doing all the three Associates at the same time uh how how are we going to pass the certification while we're going to watch the lecture videos and memorize key information you're going to do HandsOn labs and that's the key difference between uh the S OA c01 course that I made and this one is that I've added so much more Labs that's what makes this course so darn long because I'm adding very very high quality labs for you to follow and I really want you to do them in your own account and I would strongly recommend getting some practice exams we have paid ones we also have a free one make sure you get your free one by signing up on exam amam pr.co uh even if it looks like you you have to pay do not worry just click through you will get to your free prac exam there and just follow through there and we have a bunch there for you okay in terms of the content outline uh it says four domains there's not four domains there's like six or eight so that must be wrong let me get my pen tool out here and we'll adjust the number here in a moment but as always the domains have their own waiting and this determines how many questions of a domain that will show up so we have domain one which is monitor during logging and Remediation domain two reliability and business uh continuity very hard word to say conty I think people know what that means but I can't say it then we have domain three deployment provisioning and automation now it says deployment here but honestly uh the deployment knowledge is more for the developer not so much as the ssop so it's extremely light um they're not talking about application workloads they're not talking about code Pipeline and things like that we have domain four so security and compliance domain five networking content delivery domain six networking network uh or sorry cost and performance optimization these things domain six domain 4 They Don't Really cover that much in the exam which is bizarre but you're supposed to have this knowledge and again that's why you need to take my cloud partitioner because that's where I shove all that knowledge and I'm not going to uh go over it more than once because you're already supposed to have that knowledge uh networking content delivery is a bit unusual because um that is something that is more really for the developer but uh we do cover um some of the uh uh Network like the content delivery stuff networking is very very heavy but content delivery not so much for this certification where do you take this exam well you're going to take this at either an inperson test center or online from the convenience of your own home itus delivers these exams via Pearson View and so they have an online Proctor system so you can do that from home or you can go to one of their many test centers um that they are networked with to do it in person if you have the option do it in person it's so much less stressful um because the environment is very controlled um and uh you know just it is a better experience but if if you can't then take it from home and that's what you're going to have to do they used to utilize PSI online um for whatever reason they've stopped using PSI but uh to be fair PSI hasn't been very good these years I think um uh GitHub GitHub uses PSI and it's just an awful experience so I think that's why itus has moved away from PSI and that's why they're using Pearson um you need to understand that these exams are proctored that means there's a supervisor or person who monitors students during an examination I'm not exactly sure how true that is anymore there's definitely someone that checks you in and stuff like that uh with the rise of AI I would imagine that they're probably leveraging uh some level of AI to automate it um as I found that when I'm taking my exam they've been a little bit more flexible with uh things that are happening not to say that I'm cheating but I just just mean to say it doesn't feel like a person's watching watching you the entire time but act and treat it as if there is because they do capture that information and if they decide that they don't like um uh your test uh your your your test your test taking they can revoke your exam and so sometimes I see on Reddit people people pass to get their exam and then two weeks later a month later they revoke it because they decided somewhere in that footage uh there was something questionable so make sure uh you present yourself as honest as possible uh during your examination okay uh for grading it's the same as the solution architect um and so that means you have to get about 72% to pass we say around because it's totally possible to fail with the 72% so aim to get higher than 72% on your practice exams get 85% get 10% over in a bit so that you have that wiggle room uh to make sure that you do pass for real response types here um is multiple choice and multiple select with 65 questions there are 50 scored 15 unscored so you can afford to at least get 15 scored uh questions wrong actually more than that um if you count the unscored one so you can get quite quite a few wrong there are no penalties for wrong questions so always answer them and again the format is multiple choice multiple answer these uncored questions the reason they have these on your exam uh is mostly because they want to uh introduce new types of questions another reason is that it can um help determine how to make the exam easier or harder because if people are passing with the very easy unscored question then they might consider adding them or adding more difficult questions and I believe that it aids also in detecting if people are cheating because um they might be administrating very specific questions to um specific areas and so it helps them narrow down where dumps are being stolen from I can narrow down who is doing the stealing so uh just understand that's the purpose of them and if you encounter a question you've never seen before don't stress out so remember you have 15 questions you can get wrong um and they may be an unscored question for the duration of this exam you have about two two hours uh two minutes per question so 130 minutes is your exam time your SE time is 160 minutes when we are talking about SE time this is the time you need to prepare uh for the whole exam meaning you're going to uh uh have time to review the instructions show uh online Proctor your workspace uh read and accept the NDA complete the exam provide feedback verify your identity um so yeah always be 30 minutes to an hour prior to your exam and uh factor in uh any problems that you could have okay because it's your money you don't want to uh waste that these exams are valid for 36 months and it's three years before you need to recertify most people don't recertify because once you have that base knowledge you don't really need to get recertified um they might have a free method for recertification but um you know it's up to you and up to up to your employer but if you're a firsttime uh person you obviously want to uh pass passy certification so you can show that you have that base knowledge at some point in time I want to have a bit of real talk with you because I just want to strongly strongly emphasize that if you pass this exam that doesn't make you a devops engineer okay devops engineer is a senior role that is given to those with years of experience and deep knowledge of implementing technical workloads so you could obtain something like a junior devops engineer role um but not again solely with this certification you have to consider there's like about 200 250 to 500 hours of additional work that is outside the scope of cloud that you need to have alongside with Cloud to obtain these uh these um these rules and I just want to point out that adus does not care about ad certifications for hiring so if you're trying to get a job at ads you say look at all these ads certifications I have they don't care they want you to also have those fundamental skills because they know if you do get H adabs they can then internally uh provide access and get you trained on certifications if that is required in your position um certifications serve a structured way of learning with a goalpost that's not to say that um they don't matter to uh in um to employees so some employees do care about them but I just want to make it very very clear that this only teaches you the cloud component and there is so much more that you need to know uh besides that but luckily for you I try to um put as much of that in my courses and that's why my labs are so long because I'm bringing those missing skills that they're just so you don't it's not required to pass but it it's required for you to do the job and you really want to have those skills so Cloud certifications expect you to have these foundational skills programming scripting SQL it networking Linux and windows servers project management developer Tools application development skills compi algorithms and more um to fill these technical gaps leveraging you can leverage the free Camp large catalog for General technical content content to get skill ready and job ready look at the examp Am pro supporter subscription because I am creating uh projects specifically to Cloud that brings all of these skills and I'm making isolate courses that also uh bring these skills specifically for cloud um so look at those two options and that's going to really help fill out those missing hours those those missing Gap skills okay I just want to talk about how we do our um our HandsOn Labs because it's very different from other providers or other content creators in our HandsOn instruction we do our best to try and fill the missing gaps we might spend considerable time before using a service developing these Gap skills Labs can be long because I want to show you everything and and labs are not heavily edited okay uh so some hands handon Labs might end in a failed implementation but are left uh uh left in to experience troubleshooting or giving an accurate reflection of what it is uh what it's like working with that service so some adaba services we have to learn but are not recommended for use and I'm very honest and open about what is a good service and what's a bad service and what's the likelihood that you would actually have to use it for real so if there's a service that I think that is just in the exam that we have to pass but it's not going to benefit you in your career we're not going to go heavy into it and I'm going to make it very clear uh in the materials uh we do uh we try to do our best to clean up costly infrastructure but you should always be proactive and check if resources are left running you are responsible for cost and spending your ad account we cover in the cloud practitioner thoroughly billing cost management things like that we're not doing that heavily in these courses it's assumed at this point that you took my cloud partitioner and you know how to responsibly monitor your spend um and you'll see me sometimes I'll I'll miss some resources they're minor resources but if you can't afford the pennies if there's a bucket or uh some alarms left open and you get charged a dollar or two um just understand that uh I'm not going to uh 100% give you a guarantee that all those things are spun down because it's your responsibility and you should know how to do that okay so always check your spend go into the cost Explorer and check check check check okay but uh there you go what we'll do next is just take a look at the exam guide so we can understand the contents of the exam specifically okay hey this is Andrew Brown we're going to take a look at the um exam guide for the CIS Ops administrator associate what's interesting here is they're talking about um in March 2023 like last year of the time right now is that they used to have um HandsOn Labs so they' have like one or two real Labs that would spit up en's account to make sure you knew what you were doing they got rid of those and they've gone back to multiple choice multi select which I think is a mistake I think that they were going in the right direction but maybe it was a cost or uh I don't know for whatever reason they've gone back to just multiple choice and multiple selects so that's what it is if you want to download the exam guide it's right here there's also sample questions which we can take a look at though I would just say take the uh sample questions with a grain of salt because they're not very reflective of the real exam as I've always found um so if you don't have confidence with this don't worry about it take the real take take a real practice exam like my free one and you'll have much more confidence or an idea of what the exam is going to be like but anyway let's go here and I have the exam guide open um and we'll go all the way to the top here so if we scroll on down we can see we have our um domains so we have domain 1 2 3 4 5 six and what's really interesting about this this one is that I read through it all I've taken the exam multiple times um and it's not as reflective as it should be with the actual exam it's not as bad as the suan architect associate which is wildly uh wildly inaccurate to the actual exam but this one is okay but in some areas I would say that it's uh not accurate so you don't have to stress out about some of these things so I'm going to get my face out of the way so we can see exactly what we're talking about here and I got to hide my face not the screen and let's zoom in here so we can see a little bit closer here and let's take a look at domain one so domain one is logging um logging and Remediation and I'm just going to get this tab out of the way here so Implement metrics alarms filters by using a monitoring logging Services identify collect analyze export log so in my cloud partitioner I teach you how to do logs so we're not going to or sorry not logs but I teach you how to do um alarms uh so we're not going to go really deep in alarms it's something that you should already know at this point same thing with cloudwatch dashboards that's something I would do in the cloud petitioner um but we what is very important is learning how to filter metrics working with the cloudwatch agent which I have a very long video on this one we spend most of the time building an app and then configuring this this one's a very high value uh lab that I gave you but we have cloudwatch logs uh Cloud watch insights cloud trail logs and we cover all those in laps then down below here we have configure notifications this is really straightforward there's not much to say about notifications other than when you create a um alarm you can tell to notify something it's very straightforward so there's not much to talk about that one we have remediate issues based on monitoring availability metrics so troubleshoot or take corrective actions based on notification alarms um they don't really need you to cover this in the exam content uh it's very straightforward uh like what it would be to troubleshoot or correct an action um I can't think of an example off the top of my head but I'm just telling you that uh they make it sound like you have to learn more than there actually is invoking an event Bridge rule uh so yeah we definitely need to know how to use event Bridge uh so we cover that one very very thoroughly um and if we don't have a lab in the event Bridge section we definitely cover event bridge in one of our other labs multiple times just because it's a core service that you're going to run into when building out uh serverless pipelines so that's definitely something we're good to touch we have uh adus systems manager automation runbooks I believe that we cover this more than once um so this is something that we will cover a few few times here it's saying take action based on adus config rules if they're talking about Abus config I never saw this whatsoever in the exam uh I it's like doesn't show up whatsoever so I I wouldn't really worry about it was config uh we have Implement scalability and elasticity so create and maintain a auto scaling plan so we have a lab on that very straightforward Implement caching so possibly we're talking about elastic cache and memory DB memory DB is not required but I put it in there because it's a newer service so I imagine at some point they will do that Implement RDS replicas and Amazon aora replicas um you can just learn the lecture content for that I think it's a little bit very involved to uh learn the uh how to set up replicas to be honest and in practicality most people are starting off with RDS and not really working with uh a so I think the theory knowledge here is better than the implementation knowledge Implement Loosely coupled architecture um just using cloud services we're already doing uh Loosely coupled architecture so there's not really much to say there it kind of feels like fluff text that they threw in here uh differentiate between horizontal scaling vertical scaling this is something we cover in the cloud partitioner it's not something and the solution architect associate it's not something that uh we need to explicitly call out in this certification um horizontal scaling is when you add more servers vertical scaling is when you make servers larger that's it so I'm not sure why they have that as a point on here Implement higher availability and resilient environments so configure elb with rough 53 health checks um again I think you just need to know about those I I don't think that it would require you to actually be able to do it we do cover R 53 and we have Labs on elb elb is more so covered in the developer than it is in the umis Ops so uh that's why I always say to study all three of them because it'll fill any gaps and you might studying one area for a different certification but you'll have the full knowledge of of these Services um and so that's again my recommendation is to do all the associates and the developer will give you really good elb skills ASG skills and things like that but here they're talking about differentiate between the use of single a and multi uh multi deploys um so that's pretty straightforward and lots of lecture slides cover that Implement fault tolerant workloads so we have uh EFS elastic IP um Implement Route 53 routing policies you don't really need to learn how to do this uh HandsOn but I did make a HandsOn lab just in case implement backup and restore strategies so we have automate snapshots backup uh based on use cases here they really want to uh put large focus on A's backup I added Amazon data life cycle manager but it's just a life cycle system for uh EBS so once you learn about like S3 buckets right U and their life cycle then you start to understand life cycle feature in every service restore datab so you should understand how point and time restore works you should try to back up um even if I don't have a lab on it I think I do but if I don't you should try to back up a database and then restore it and see how long it takes because it takes some time for it to restore uh Implement versioning and life cycle rule so that stuff's pretty straightforward uh cross region replication we covered that in the solution architect associate it's it's repeated in this content here for you perform data uh Disaster Recovery procedures um I mean I think it's more so about there's like a um there's like a a diagram that they they have that looks like a thermometer I'm forgetting what it's called off the top of my head but if you understand what that thing is that's what they mean by perform Disaster Recovery procedures provision to maintain Cloud resources so create and manage Amis so we definitely have a lab on ec2 image Builder create manage troubleshoot a cloud formation we cover that extremely thoroughly because it's useful for both actually all certifications you should thoroughly know CL inform the only course where we kind of left it uh thin was in the solution architect associate but we had lots of labs and Cloud information but we didn't go through the details of the specification of the language provision resource resources across multiple abis regions and accounts um so we have Ram stack sets stack sets don't come up a lot but if you conceptually understand what these are you'll be in good shape uh select deployment scenarios and services blue green rolling uh Canary deploy so yeah we cover deployments but again deployment is more so in the adist developer associate so if you do have any questions uh that is going to round it out a lot more and it's not even being very specific about what is being blue green like rolling Canary so this is kind of weird because usually youd say like in the context of you know is it code pipeline it like what is it and so they're just generically talking here this is where I keep saying that they're adding junk lines in here um identifying remediate deployment issues so for example service quota subnet size in CL information errors permissions again this is another junk line but service quotas is something we cover in the cloud pratitioner so we're not covering it in this course as it's already in that one subnet sizing this is not something that comes up a lot but I guess what they're trying to say here is that when you choose uh to build a network that you have to make sure that you have the right size of subnet um but honestly this is a problem for Enterprises and this is not this is not associate level content so and not going to show up me your exam um other than just understanding sizing of subnets but there's like like identifying and and deployment issues because of subnet sizing we have automate and manual or repeatable process so use dat services like System Manager Cloud information Implement automated patch management and they're specifically talking about Windows servers here schedule automated task bya Services um we have security and compliance a lot of this stuff again I cover in the cloud petitioner and and I don't know why they they did this so heavily here because um it's not like they ask you so many security questions so again I'm not exactly sure as to why they did this but um anyway we have the content and and we've brought a good chunk of it over into the um sis office administrator but if you need more do the cloud partitioner it won't take you that long uh but there's some key ones like KMS so any or ACM we definitely have uh dedicated videos for ACM same thing with parameter store at Secrets manager Secrets manager is something that will definitely show up on the exam so make sure you know that one um we have base videos for this we probably includeed in this course here networking and content delivery so it's more so about networking than the content delivery part as content delivery is going to be more covered in the developer um but we have VPC we just have a very thorough VPC section we include in all of our Associates so you're going to get that uh private connectivity um it's more so about knowing how to do it as opposed to implementing it because some of these things are expensive or difficult to implement so just understand we're limited for what we can do in labs for those we have a configur network Protection Services WAFF Shield so we can't really use the paid version of of Shield we can only talk about it um abos Waf is pretty straightforward and we do cover that there for ref 3 we have hosted zoner records routing policies resolver which is not easy to use we learn about what it is O AC's or origin access controls there's o AC's there's o AC's OAC versus ois I covered this in the course I just can't remember which one is the newer one um yeah so O's is is the new one and we we definitely walk through cloudfront example of O AC's we have static website hosting which is covered more so in the developer and the solution architect and we even cover in the cloud pratitioner so it's really bizarre that this is this is in the list but you should understand about it so if you haven't done it just understand the concept about it it's not going to be very thorough about checking you on that one troubleshoot networking connectivity issues so interpret VPC configuration so just understand how to configure vpcs collect and interpret log so we definitely uh do these uh in the in the course identify remediate cloudfront cashing issues so we we cover the most basic uh cloudfront issues it's not too difficult troubleshoot hybrid and private connectivity issues that is not a fair thing to put in this exam because again it's very hard to set up hybrid and private connectivity issues so you'll conceptually learn about those things but we're not going to do those uh from a lab perspective cost and performance optimization so all this stuff I'm not even going to touch here because again this is stuff we cover in all the other ones it's implicit why they have a dedicated section on this I have no idea it's an absolute mistake in this course let's go down the appendix um so there are services in here that I put in the exam in the exam and that's why my course is so darn long because even though they don't show up on the exam I never saw them and I don't even know why they're on here they're in the list and because they're in the list I just want to cover our bases so we have Open Source service event Bridge SNS sqs so this one I don't know why it's on here but it's here um these are should be known by every single certification so there's no reason to explicitly cover those but they have them here um yeah these are fine this is fine they don't list memory DB but I've added memory DB into here the adabs tools and SDK I thoroughly thoroughly do a job on the CLI and SDK tools more important to the developer but just as important as this Ops one understand that it is overkill but it's not for the certification it's for your own good so that you are good at using AWS cloud trail Cloud watch those we cover extremely thoroughly compute Optimizer um is not a very important service very simple um but it we've added it to the list ad config uh very straightforward Service as well more important in the um in the uh security certification but it's here control tower is not something we're going to implement um we should know about it we have a slide on it it is a pain to set up it is a pain to use customers don't like it um so that's why we don't thoroughly cover that one AIS Health dashboard and license manager those are CL predition level stuff and we brought them over in this course even though it's they're already covered in that one is Management console is the thing that you use so there's no point in listing that there organizations we uh cover but again mostly in the cloud partitioner and it's a pain to set these things up um so just understand that the labs might be limited for that one service catalog we conceptually talk about it um it's not a it's not a very good service it's more for the Enterprise level and something again we cover in the comp petitioner so carried the lecture content over for that systems manager we cover thoroughly though I noticed that when I went in there they did make a bunch of updates so understand that I might not be covering the most up todate uh UI but I'm covering all the conceptual things because those things have not changed and we thoroughly cover them like how to build documents and do run commands and automations so there might be a bit of discrepancy with um the UI because they're always changing it there but I wasn't going to go re re uh redo all those slides because they they changed superficially some some things there trust advisor is something that we cover in the cloud pratitioner it's no point in covering in any associate or anyone beyond that you should have that at the base level there uh data sync was not something that ever came on the exam for me or anybody else same thing with transfer family but we covered them and I actually have a very complex lab on transfer family that uh covers um I think it's transfer family no no I'm thinking of uh the FTP Service the transfer family is uh something else so so forget that well I don't know I guess it's in the course you'll find out when you take my course here uh cloudfront we cover elb Global accelerator we do that one I was surprised how easy it was I was I was uh last time I used it it wasn't so easy and so it's really a nice experience now ref 3 you'll have to buy a domain if you want to utilize it if you don't want to buy domain you can just watch me and that should be sufficient enough Transit Gateway which is not easy to use VPC uh VPN okay ACM we definitely do a lab on ACM detective which is cool to look at but not much to talk about directory service which we do a lab on and is very difficult to use so if you don't want to do it you can just watch it and learn firewall manager which is expensive so we don't really run it for real guard Duty which is very straightforward we already covered in Cloud petitioner so we're not going to thoroughly cover that one again IM we thoroughly cover IM uh so we're in good shape there access analyzer that's a very simple service inspector that's a very simple service but they made the offering a lot larger so I didn't cover the entire service something I would do in the security one but uh it used to just be for scanning um uh scanning uh a machine like an ec2 instance to see how hardened it is um but there's a lot more offerings there KMS which is very straightforward Secrets manager we cover thoroughly uh security Hub we check out uh Shield we can't use for real wa we we utilize to to some point um we cover it was backup that one was a pain to learn but I I learned it and I'm teaching it to you um personally I wouldn't find much use for it but it's more of an Enterprise thing because you can consolidate all your backups in one place EBS we thoroughly cover e EFS uh actually had some nice improvements it's even easier to use than last time so liking that service but very straightforward FSX um where you have your own file systems we do a lab on that and the lecture content it is hard but uh we do cover it um so just do your best to get that that theory down S3 which we absolutely cover in very thoroughly same thing with Glacier same thing with storage gateways and there's a lot of services that are out of scope so hopefully that gives you kind of an idea of what is going on but again I think that there's a lot of stuff in here that you're just never going to see on the exam so don't get too frustrated with the amount of content uh do your best to go through as much of my content that you have and then do the practice exams and if you're scoring good in the practice exams you should be pretty comfortable with it and again don't skip that cloud partitioner I will see you in the next one okay ciao all right so we're on to the introduction uh to cloudwatch here and so cloudwatch is a monitoring solution for your ad's resources and it's really an umbrella service meaning that it's a collection of monitoring tools and there's a lot under here starting with cloudwatch logs here so this is for any kind of custom log data that you want to centralize such as applications logs engine X logs any kind of logs you want then you have cloudwatch metrics these represent a Time ordered set of data points so think of it of a variable in time that you want to monitor maybe uh CPU usage memory usage Network in network out uh then you have cloudwatch events these triggers an event based on a condition so take a snapshot every hour the service has now been rebranded as Amazon event Bridge um but some people still refer to it as cloudwatch events then you have cloudwatch alarms these trigger notifications based on metrics which breach a defined threshold then you have dashboards these create visual visualizations based on metrics you have service lens this visualize and analyzes the health performance availability of your app in a single place and this one really pulls in uh all three levels of observability uh for you which is interesting we'll talk about that when we get to that section we have container insights so this collects Aggregates and summarizes metrics and logs from your containerized apps and microservices you have synthetics this test your web apps to see if they're broken and then we have contributor insights and this views the top contributors impacting the performance of your systems and applications in real time so you can see there's a lot of services but the ones you really really need to know are the top five that's logs metrics events alarms and dashboards these other ones service lens container insights synthetics and contributor insights are new uh they're not going to be so important unless you're going for the devop devop Pro or you're going for the CIS uh sis Ops administrator uh but it's good to know all of them anyway way and I just want to emphasize that all these cloudwatch services are built off of cloudwatch logs and I want to emphasize that here just to show you what I mean so down below we have logs so again logs is a place to store all your files and lo and Metric uh leverages logs to turn uh to turn them into monitorable variables and then the dashboard leverages metrics to turn them into graphs if you want to create alarms it it it uh builds off a metric if you want to use service lens it builds off a metric but service lens actually pulls in uh multiple services so that's not exactly clear but it's just the way I fit it into here then you have events these trigger actions based on event uh data you have contributor insights uh and then you have uh or sorry container insights then you have contributor insights which is all over it's by itself it doesn't use logs and synthetics which doesn't use logs but there you go so that is the introduction to cloudwatch so before we jump into Cloud watch let's take a look first at the pill of observability so we understand the utility of all these cloudwatch services so first let's define what is observability this is the ability to measure and understand how internal systems work in order to answer questions regarding performance tolerance Securities and faults within our system or application and to obtain observability we need to be looking at three things that's metrics logs and traces and they need to work together to give us observability we can't just use them all in isolate and expect that we have observability they to work together so let's look at those three now starting with metric so metric uh just think of it as a number that is measured over a period of time so if we measured the CPU usage and aggregated it over a period of time we could have average CPU uh then you have logs so these are just text files where each uh line in the actual text file contains data about something that has happened at a particular time uh then you have traces this is a history of requests that travel through multiple apps or Services where we can pinpoint performance or failure uh and there's one uh extra bonus one here especially regarding AWS which are alarms sometimes considered the fourth pillar of observability but when observability were defined alarms were not part of it uh so I'm just adding it there and I like to make the joke that it looks kind of like the Triforce so they should have called it the Triforce of observability so there you go all right so we're going to take a look here at cloudwatch logs and this is a service used to monitor store and access your log files but specifically it is a centralized log Management Service uh and it does a lot of things it has a lot of integration so let's talk about them right now the first is that you can export your logs to S3 so that you can perform custom analysis maybe use Athena for that or um uh some other service you can stream your logs to lassic search service so you can have full robust or uh robust full Tech search or use the elk stack uh you can stream cloudwatch events to cloudwatch logs and this allows you to to uh then uh uh uh analyze or leverage your cloudwatch or cloud trail events just just as if they're Cloud watch logs uh it's secure by default so all the log groups are encrypted using SSC you can use your own uh customer master key with adus KMS uh so you have a bit control over there uh you can uh do log filtering so it has a filtering syntax um and cloudwatch logs has a sub service called cloudwatch insights which we will cover uh in this uh section here uh for log r mention uh all logs are kept by default uh indefinitely and they never expire but you can adjust adjust the retention period for each log group keeping it indefinitely Or choosing between one day and 10 years um and most Services integrate with cloudwatch logs uh sometimes you have to turn some services on or require IM permissions to use cloudwatch logs but more or less AWS will do the heavy lifting for you to uh you know set those permissions and turn them on so there you go now let's take a look at cloudwatch logs long group so a long group is a collection of log streams and it's a common to name the log groups with the for SL syntax which we'll see here in a moment uh so here uh if you were to open up cloudwatch you'd have log groups and you can go ahead and create a log you would name it using that for/ syntax um and so it would then go appear here uh the reason you'd want to do that is then you could scope it based on maybe your production environment or whatever convention that you want to use um you can see that the retention is never expire uh but you can change that from uh never expired to 120 months here which we can see here which is 10 years and that's log groups so we were just looking at log groups and log groups contain log streams let's talk about what log streams are so log streams represent a sequence of events from an application or instance being monitored and you can create uh Lo log streams manually but generally you don't have to do this ads will do it for you um but anyway I just want to show you some examples of different log streams and again if if you have a log group you click in here and this is what you would see so for Lambda function you can see that the uh the log streams are uh named based on the time it ran and also it has uh like an ID on on the end of it on the end of it there uh if you're looking at a log group for an ec2 instance what you're going to see is that it's going to make the log stream name just the instance ID and if you were to use adz glue which is a service that uses um cloudwatch it's going to name it based on the glue job so the convention for log stream names is all over the place but you can see certain Services have certain conventions um but they can be whatever we want them to be okay hey this is Andrew brand this video we're going to take a look at cloudwatch uh so what I want to do is work with logs and I want to create a log and send data to the log and so I'm hoping that we can do this completely programmatically I actually haven't ever tried to send individual logs um using the uh uh the SDK or C so I'm very curious to see if that uh that could be accomplished worst case we could always just spin up an instance and uh stream data but let's find out what we can figure out here so um I'm utilizing our ad examples repo I'm opening this up in G pod you can use codes spaces or whatever you want to utilize just remember you need to configure your adus credentials uh for your account mine should be already preconfigured for this and I just set those as environment variables um but I'll wait for the terminal to open here um so we give that just a moment here you see I have a lot of folders and once terminal is open here I can go ahead and create myself a new directory we'll call This cloudwatch And since we might do more than one thing in cloudwatch I'm going to go ahead here and just CD into that Direct three and we'll just say mkd logs okay and then we'll go into logs here and again we might do quite a few things here I'm not sure maybe we might not do much but I'll just go basic for now Amazon Q has moved to its own extension I don't care I don't find Amazon q that useful um and I probably not going to use it anytime soon anyway so let's go down below I do find um I do like GitHub copilot and I do pay for that so I kind of want to bring that in here uh if you don't mind so just give me a second I'm going to get copilot installed I don't even see the option so I'm not really sure if we can actually uh do that it says this extension is deprecated because this runs on um uh the open vsx Library so it's like uh they'd have to publish to those ones and I'm not finding it here right now I guess it' be part of the GitHub extension but for whatever reason it says it's old so I guess I'm just not going to use any AI assistance I don't really need it um and uh if you don't need it don't bother with it as well it's up to what you want to do here but when we say copilot something about GitHub copilot let's go ahead to that um uh to here I'll just make a I'll just touch a new readme file here readme.md and on left hand side here we'll just expand this and I'm going to drag this into the basic folder and so the first thing we want to do is create ourselves a log so that should be pretty easy we'll go ahead and tip it CLI cloud watch logs and see what options we have and we have create log groups we've clearly done this before we could easily use cloud formation for this but I think for this one it's so easy we're just going to use uh the CLI here today okay so we have this and there's a pattern to the naming where you'll do a forward slash I probably cover those in the slides I'm actually just looking at my slides here if I do talk about it but um uh and like here's just like a slide here but you can see like the scope can be whatever you want so here this is like based on date this is log stream um so maybe log group here yeah so you know you can do the forward slash and have whatever you want it just depends on what you want to have so I'm going to go ahead and just call this um uh example uh log uh app okay so or we going to say example app I suppose there is no app per se but maybe we'll go basic here we'll say basic app and that'll be kind of the scope again you don't you could just name it one thing if you want to but I'm going to do that here today just because it has forward slashes in it I don't trust it so I'm going to give it double quotations we'll go ahead and copy before I do anything else I'm just going to make sure that uh I actually logged in here and which account I'm logged into it examples okay great and I believe that is the correct account so we'll go ahead and copy that good habit to do that once in a while and I think uh last time I was fiddling with here and I changed my font sizes so I'll just quick fix that you're just say font size terminal increase and what is it this HCK key this HCK key I'm trying to figure out what the hcky is here control shift p I'm pressing that I'm because I'm in a browser it might not be acting the way that I want it to work so I'm just going to have to do that a couple times like this there we go one more time and that feels comfortable to me anyway so let's go see if we created our log so I'll go over to Cloud watch and we'll see what we have I might have a bunch in here I don't know we'll go to cloudwatch log groups and so you can see I have a bunch but the one we're looking for is example basic app so if we click into this okay you can see we have no log stream so nothing has been sent over here all right but what might be interesting is just to take a look at what it looks like when we create a log so I'm just going to click this I'm not going to create one here but notice we have a retention setting so we can say when it's going to expire we have a log class whether we want to save money with the infrequent access um we can apply KMS key I would assume that I would think that they'd be encrypted by default but I'm not certain uh uh for certain but you could add that uh KMS key there if we go into it there might be more configuration because sometimes there's configuration that you set time of creation there's stuff that you can set afterwards so if we go here it looks like there's things we can create on top of it so we have a metric filter data protection policy um which is kind of interesting but for now we'll leave those Alone um and what I want to do is go ahead and create a stream but before we move on let's just go take a look at that retention option because I wonder if they even have it in here on the create it might be a separate action so it says it it will not expire so we actually would have to set it separately I think that's a good idea to set up a retention uh policy so I'm going to just click back here to the logs and we'll look for retention so that's just like one of those places where the um uh the console is doing two things but there's actually two separate commands so we go down below here and I'll change this um because logs can add up and cost you money so it's probably always good to set a retention period I'm going to set one day here and then I'm just going to grab this here and paste it in as such and we'll go ahead and copy this command let's just say create log and then set retention log all right and we'll go back over to here we'll go to our log groups we'll go down below and did not change now maybe maybe it's cuz we have to do a hard refresh here sometimes that happens where if you're clicking around it's not pulling the latest data we didn't run into an error so that's fine so go back and check again one day okay great so that's what I want so the next thing I want to do is create some stream data now again uh usually there's things you integrate like the agent and uh it will be the one that is sending the data but I'm just really curious if we can just just directly send data uh to it and probably probably something we'll want to do is create a stream because that is a component so there is create log stream so let's go ahead and do that next and we'll go down below to examples and I also just curious can we create that via the uh we can it's just name okay so there's nothing special going on here say create loog uh log stream this will actually be creating a log group so that is good there we'll grab the name and then we can give it whatever name we want so we could say like a name right now we might want to give it like the current date I'm just going to see how do we get that as um UTC or uh times so say uh I'm looking for date in uh Unix format Linux so I'm just looking for it here I want Unix format this website has a lot of ads so it's like really buckling buckling my uh memory here we'll go down below here I just want to know how to get this in Unix format okay I'm going to go ask chat GPT I don't have time to go on these uh adrid websites so we'll give this a moment here I'm not sure what this is up here oh it's just my logo here so let's just say how to uh get uh Unix timestamp in Linux as a command it's probably like some kind of formatting we have to apply on in so that's probably what we want so go ahead and copy this I'm going to paste it in here hit enter and that looks good to me so what I'll do is put it right here and we might have to wrap it like this I think that's what you'd have to do and so hopefully what that's going to do is create one right now so that's probably how we'd want to have a stream we'll go ahead and copy this and paste and hit enter and we'll take a look and see if it did what we wanted it to do so we'll go ahead and give this a refresh and there's our stream if we go into it nothing in here so the question is can we push data to this this is what I want to know and so we'll go back to logs here and I'm taking a look here I don't think deliver is that that might be like importing can you import a delivery is a connection between a logical delivery source and a logical destination okay so maybe there some kind of connections I'm not 100% certain what that is right now probably I have to go add that to my cloudwatch section here but I'm just going to carefully look through this here delete resource policy describe get delivery probably be like a put I would think put retention policy put delivery Source create or update to logical delivery source so I'm not really sure but let's find out can we use the adabs CLI or ads SDK to directly write to a stream because I think that'd be interesting to find out oh it says you can well what's the command put record ah there we go definitely going to update my um lecture content you'll probably already see me have shown it to you and then it'll seem like it's like why don't you know but we don't see put record here so where is it is it lying to me so we go over to here no no no no no no no no I I sorry I wrote it wrong here cloudwatch log stream not Kinesis I mean there has to be some way because there's an API right um but maybe it's just not possible and we have to just use the agent I'll give it a moment to think okay so here they're suggesting the the ads SDK for JavaScript has a method here and down below it's suggested there Cloud watch real user monitoring oh they have rum now oh yeah I remember there's rum now okay let's go take a look here and see what they say so it says create your log group that's what we did put log events so that's what it's called okay so I didn't see it there okay oh it is there okay great so that's what it was that's what we're looking for so let's go down to the examples and it looks like we can just take a file and send it over um so we'll go here and just say uh send logs to uh logstream all right and so now we can have a log here so we have log group name my log log stream name 20 2015 uh 601 Etc okay so and then we have our file so maybe you can send multiple files here I'm actually curious about that so we'll go back over to the API here and I want to check that specific parameter where it talks about um log events so we go ahead and we'll say log events um represents a log event which is a record of activity that was recorded by the app and so it's expecting a time stamp and a message the raw um event message no larger than 256 kilobytes so I guess the question is like if you have logs do you include the log in the message or do you separate that out um let's just look at some log formats com like common log formats or is that actually a type of log format called common log format for uh for log management common log format is a standardized text format when when generating server files uh for web analysis or web an analyzer if there's a standard that's great I'm not the best at remembering these but here it chose the IP address um the user identity the person and stuff like that so I would think that the challenge with this is that uh we would have to it seems like we'd place it in both I think that's what's happening here let's going an extended log format this is another standardized format that is used by web servers in comparison EF is is provides more information and flexibility okay so what I'll do is I go to chbt I'm just I just want some data so uh generate say generate genen genen generate out log data in elf format with 100 records uh from a web server make it look like real data so we'll generate that out and then and if you don't have chat GPT or something else I'll just copy it out just just don't do that with me here today well you know what actually let's do it let's let it do that I was being difficult I just wanted to give me a file but you know what let's actually make a file that's going to generate some fake fake data I think that's actually not a bad idea first I was mad now I'm not so we'll give it a moment okay personally I would have preferred Ruby but I did not know it was going to generate python I know folks like python so I guess we can utilize that here it looks like it was missing something here so generate a some something so I'm not sure if it just didn't finish but what we'll do is we'll go ahead and make the script I can use Python that's fine we'll just say log. piy or I just say I don't know if that's a reservoir so say my log. py and I'm going to go back over to here tat PT hello wake up come back white screen give me a second okay there we go it's back so so we have um it's back here and hopefully it sticks around so I not sure if it's missing some but we'll find out here in just a moment so I'm going to copy this content again you can just run these files but you can just see that you'll be part of the process of how we would actually go ahead and get uh create this but I'll go back down here below and this should generate the logs so I have a feeling that something's missing here I can't really tell just yet um but we have generate time stamp method path status path status method user agent IP uh no looks like it's all there um the only thing is that this generates all the logs but it doesn't write it to a file yeah it's not writing to a file so I'll just go ask it like okay um show me the code to write okay this should write the logs to a txt file with each record on a line oh never mind it's telling us how to run it okay but I kind of prefer if we just ran it and it it would create it but I guess we could um send the logs to an output like that okay we just try this out that'd be interesting because this would this would print out to um uh St out anyway so if that works that'd be very interesting so we'll go up here and just say uh create logging data and we'll assume that we can run it as such so I'm going to go ahead and we're in logs we're not in basic though do we need to do pip install here is there anything special we're using no it looks like all standard libraries so we'll go ahead and copy this paste this in below and it says it's complaining it's still generated out something here so I'm not sure what it's complaining about but it says can't open file generate logs. py oh because it's not called that it's it's called well you know what I'll just rename it because that actually is a better name and I like that it's using an underscore so I'll just rename that as such okay and at least it made the file which is weird there we go now we have some data which is pretty cool and so the thing is is that in order to bring this data in let's go back here it says timestamp log message string so the problem is even if we have this as a log and like even if we had it like this we'd have to take that file and then iterate through to do it or we'd have to modify our python script to um uh utilize this but what we will try to do here I'm going to try again use chbt so we'll say um say uh take the uh take so take a log file that is in elf format and let's iterate let's write a bash script that will send the logs to cloudwatch log stream and so what I'm thinking is what we'll have to do is it we'll have to uh parse it each line pull out the date and then Loop through it and send the data there okay so I think that's what it's going to do and let's just take a look and see if it actually makes sense okay so here it says get the sequence token for the log stream um why is it called sequence token okay we it didn't talk about sequence token when we looked at the CLI let's go back here and take a look here so sequence token the sequence token obtained from the response from the put log events does it describe it anywhere here each subsequent call uh requires the next sequence token provided by the previous call to be specified within the token okay that makes sense so it says the following command puts the events to the log stream and then we move on okay so let's go back here and take a look see if we can make sense of this so we start the stream or we start the put events I'm going to copy this so we can just take a look at it and if it doesn't work that's totally fine I'm not going to run it blindly I never run things blindly and so I just want to call this uh put um put logs and then we'll paste that on in here and I'm just going to chmod this so say chamod U plus X generate logs I not sure I think that's the right one I can go here and just say where is Bash it says it's user bin bash so I would think that it has to be user bin bash we usually have older scripts so I could just go check one of them I can find a um a one here I'm just trying to find any that might be a good idea here so look at like deploy script user been user bin EnV bash I mean I think these are the equivalent but I'm going to do this one because I just know that's more portable anyway so uh let's take a look here more closely so log group name stream name log file and we create the log group we've already done that so we don't need these to okay the sequence we are probably going to need so I'm just going to go ahead and bring these on down like this it's a little bit easier to work with and then we have function to send log events which is fine read log file and send logs to cloudwatch so it's saying it's making a for Loop and it's doing what I thought it's extracting out the Tim stamp because that format was asking for time stamp and message so it takes the whole line with the date in it and then it separates it out and then here it's calling that function um and honestly I don't really like writing functions by hand with bash so it's totally acceptable to use uh generated services for this um I don't like how long these are I think we could do this I'm going to just see if that works by the way if we do this I have to do this as well here where that's going to mess up let's just make that a little bit easier to work with again I like my stuff readable and easy to work with so I'm going to go ahead and do that this is probably why like in the past we never saw really good um thorough tutorials programmatically because there so much work to write these by hand and people didn't want to do that so it's really nice that we have generative AI for that stuff what's interesting is we could take this um sequence token and put it probably in a function as well but we will just stick with this code because I don't want to make things more complicated than it is and even me doing these backlashes probably might make a mistake but I want to carefully read this so we have send the log events so we have log events here and it says dollar sign one um so I think that's referring to the first input because when you have a um a b script use dollar sign one dollar sign 2 to bring in parameters right and so I'm assuming this is in the context of what was passed here which is over here and so we have this log event here and then it's saying if there's no sequence then start it from scratch otherwise let's go ahead and create it right and then it says go get the last sequence now this might actually return the last sequence so I'm not sure if this is inefficient so notice here like you put the log event it will actually return the last sequence so this additional call is probably not needed um next sequent event so I'm going to tweak this and I'm going to go here and do this and do this and we'll go bring this down here and we'll say um query and it should just be that and that should get the next one okay I can't remember if I need a period or not here I always kind of forget but I think that's all we need yeah I think that's all we need and so then we can do this and then called this sequence token like that and so that would be fine now the other thing is that this one probably returns one as well I think they all do so if we go to uh put logs here it's the same story next uh sequence token so we'll go here and do that that's why it's good to know what you're doing and read it not just take code blindly because you can get better results that way so I'll paste that in here as such and then we'll go here and paste that in his such and I think that's fine uh yeah this has the wrap around it it doesn't seem like this one does and this one's supposed to have this on it otherwise it's not going to select it correctly and that's supposed to have that there okay great so I think that this will do we want I'm going to go down below here and so we have this one here what's the point of getting it if we always get it somewhere because even the first one we don't get it so this seems really silly to me I'm going to get rid of this right and let's go back and take a look at these logs again yeah this one is the initial one yeah and so that's a lot cleaner that makes more sense I'm seeing this as red as if it's confused this is here I think this one's extra there we go because that that one matches to that one that one matches to that one that's fine okay so we'll go that back down below and and carefully read this so get the date use a to to get the value or sorry is the one that's out I'm hoping that this works correctly then we have message line which is fine whoops then we're constructing the Json which is fine that looks fine to me and we'll send it that way we could use uh JQ if we wanted to constructed as well but um a string is totally fine as well and then we have the log file as it's iterating through it right so I think that's the input that's coming in I don't know what ifs is unless it's a uh thing there there so I'm hoping that works so yeah I think my script is okay let's go ahead and we'll hold on before we move on we have web server logs. log and we called this what the problem is well it's not really a problem but we'll just have to grab it manually so you're going to have to change this for whatever yours is we could pass in parameters for the script but I don't care we're going to hardcode this today there's enough we're learning here right now and so I'm going to go ahead and paste that in here right so I'm hoping that this just works it'd be awesome if it does so I'm going to go ahead ahead and say put log no this is not autocomp completing so it's telling me this needs to be ched I thought we already CH moded it so say you plus X put logs okay and so now do forward slash like that put logs and right away the dates the date's wrong the dat's wrong okay so invalidate 30 May 2024 input is not a terminal um and so the thing is like when chat gbt is gener out it doesn't necessarily know that we're using Ubuntu or something else um so I think what would be useful is where is this messing up so what I'm going to do I going to comment this out I just wanted to Loop through it right and I want this to just um Echo the time stamp so we can see what it is because I don't know where this is failing this could be failing um when trying to send it or could just literally be failing on this line so that's what we're going to find out right here another thing we can do here is if it fails we can set set hyphen E I think it is and that will stop where like stop on the first issue um so right away saying date invalid it didn't even get to my echo hello we'll try this again and so that is no good um what I'm going to do here is just Echo out the line so we can see it first and so this is our line and so the idea is that we could take this line okay and we'll just say test lineals this there might be double quotations in there so that might be hard for me to assign okay so that's not going to exactly work but clearly it doesn't like something here um the challenge is that there's double quotations in here and so I have to escape them all to assign them I could put singles around it I might be able to do that let's try singles here so I'm going to go ahead and just say um copy this and say say test uh test what they call there line test line or I'll just say tline and then I'll put singles around it I think that might work we'll hit enter and so I'll just say Echo dollar sign T line okay so we have that and so now what I can do is I can copy this here and run it and we know it's going to fail but we now we definitively know that it's failing here tline okay and so that's fine so just give me a second okay all right so yeah we now have this part so we'd have to step through it individually but I think the thing is that uh it doesn't know that we're on Ubuntu so I'm going to go ahead I'm just going to say this doesn't work um I'm on Ubuntu because the tools like a and stuff like that they vary based on uh what you're utilizing but while that is thinking I'm going to go ahead here and just try to run some of these lines so if we do this here like this you can see that it's grabbing um the fourth part of it and then if we do this here like this so I'm just trying figure out what part of it not working here let me take that off there and so it's extracting out the date which is perfect so the issue is like this date can't handle this date so if I go ahead and do this like this and then we paste in this here it's saying date's invalid um so yeah it doesn't like something there so it's probably going to tell us something about the date command what the okay hold on here so here what is it doing it's changing the way it's extracting it out so extract the time stamp so this is another way that it could do it and then it doesn't look much different looks like it's doing the exact same thing so like if I go back over to here what is what is different so it grabs it puts it into a stamp convert time stamp to milliseconds but okay and the flag is now just D instead so now we have d right and plus percentage s percentage 3n it's the same it's the same thing so look it's saying date inv valid date let's see if it understands so I think it's just it's misinterpreting how the date supposed to work this is where we might have to look up how the date function works so now we take a look at here extract and convert a format to a date that it can part so now it's suggesting this so I'm going to copy just this part out and we'll type in clear here and I'm going to go ahead and paste in this line and so now what I want to do is try it with the tline that we have here T line and so all it's done is it's added the plus z0 so that might actually be enough for it to fix it so let's go ahead here and try that out so we'll go ahead and try this still invalidate so I'm going to make a new one here I don't like this conversation um and we'll try this so now it's saying that so we'll try this now okay okay let's read the man so let's go ahead and do this chat PT can't do it for us it's dumb as bricks display the time described by string not now okay that does not tell me much uh so I'm just carefully looking at this here here date date display time described by string and then um I guess the question is like do we need to parse it well it was it was converting to milliseconds right so let's go back to the API here let's take a look here and yeah it wants it in that millisecond format okay so look we'll say uh bash convert date string to Mill seconds because I mean this is all chat PT is doing and it's obviously doing it in a very bad way um so yeah they're kind of using it here let's use this as an example and see if it works I'll go here and just say date like that and then we'll go ahead and try this okay so that converts it over um and if we just try date here like this hold on here I want to go here what does it what does it do if we just do this it just prints back out the format okay so if we go here back to our format and I'm going just do double quotations around here I just want to see what format it's going to take right so if we do this it doesn't like that if we do this it doesn't like that if we do what do zero on that it still doesn't like that what's the format of the one above here this one um this one here I already kind of forgot what it was go over here huh okay so what date the question is like what date formats what day formats will will date hyphen D accept that's what I want to know while that is figuring that out I'm going to go back and check the man because this is the Crux to our problem and that's the formatting convert seconds since the epoch show the time zone sometimes with these tools what they'll do is they'll have a um like you'll have to specify what the format is but it really tells us next to nothing which is kind of annoying so here it's suggesting that it can handle these so let's just try some of these here but it says it's invalid and this format is an American style date so uh date hyen D does not work with American style date so a way that we could work around this problem is instead of doing that we could just change the format a bit so I would rather have it an ISO ISO because I feel like that one would be more reliable so we'll go ahead and try this here and we hit enter and that works fine so what we'll do is we will need to change we can parse the string right uh here so we'll go ahead and say um I'll go a line above here we'll just say uh time string and we will go and grab this one which is fine I don't think I need the doubles and then the thing we want to do is take this format here and say convert this string to ISO date format leave in the time and don't use date to convert so it might use SD or some kind of other tool I'm not really good at writing by hand and and so what I'm hoping is that it will no no no no no no no no no I want it with bash okay we'll go here and say use bash to convert so A and S is what I assumed it would do and so what this will do is extract out the parts and then reassemble it I'm not sure why we need the whole month here because the month will already be numbers but maybe the point is is that maybe it would be like if it's it says may that's why okay fair enough so what we'll do is um okay turn turn it into a bash function could probably figured that out ourselves but we'll do that anyway and so my question is if I had this with this work could this be parsed so we go ahead and try this so can parse that format which is it's not exactly what we uh we did earlier and so what we'll do is copy this function okay and we'll go here we'll paste that there and then we'll go back down below and we'll grab this here and so hopefully this is what we want to occur okay so I go ahead and paste this here and this will be um now in the format they want so we'll say ISO so uh date time and it we just say uh original date time okay this would be original date time like that and then here we will keep the date we'll take this part out like that and then we'll say ISO date time and so I'm hoping that that will get us what we want so I'll say Echo timestamp like that and so hopefully that will get us our result so I'll go ahead and run this again or I'll just type it manually put logs and you can see it is printing out the line so that's a little bit hard to see what's going on so just comment that out for now and I mean that looks good the question is are all the dates the same so if we go back to here and we look at our times 2407 2414 they're clearly not all the same so it's like it sets the first one and then it just keeps using the first one which is not what we want to happen so first what I'm going to do I just want to see if they're different here so I'm going to say um Echo original date here first we'll try that because if these are all different then we know where our problem is original date time sorry okay so these ones are all different which is good and so the next one I want to do is I want to convert to ISO and see if these are all the same so we say ISO daytime I guess I have to spell it right for it to work maybe that was our problem maybe we spelled it wrong and I mean that one doesn't really look like what we had there so I'm a bit confused why does it look cut off original date time try this again that is not what we were expecting right because when we looked at this they showed an example of this converting out to this will output that if we go back to our output it didn't make that so the time zone supposed to be on there so i' have to copy this and go back to here and say um and this is just Linux it's very finicky to figure this out so we'll say this turned into this using convert ISO fix the script okay what I'm thinking it's going to do is is fix the uh a here so we go back up to here time zone I'm going to go up to time zone here so this is time zone and I go down below to this one no I want the time zone I'm saying the old one did not work uh it did not output a time zone give me a break here watch it's going to give me like me the same script that looks the same to me so I'm G to go back over to here let's take a look says 1 2 3 4 five this one says oh no it has a six okay well we'll see right so we'll grab this again and I will go ahead and paste this in as such and then hopefully this time it will do what I want it's like it's not listening okay let me go figure this out I'll just fix this and uh I'll be back in a second okay you know what I think the problem is I'm looking at this date here and it doesn't have that plus z00 Z on the end so earlier I think we were doing this one here it had an alternate one and so that's probably what I'm not bringing in here so I think it's this this one that's missing I was like debugging it but I was just like staring it a little bit closer and I think this is our problem right here okay so let me try this just fingers cross that this works and by the way if you're trying to do an echo within a function you have to do Dev TTY so that you can uh see it we'll go ahead and we'll put those logs invalidate convert all right let me fiddle around okay you know what um I think what we need to do is just add in the plus and I think what that will do is set it to like green Wich time I'm almost certain that's what it is so just to make our life a lot easier I'm go ahead and do this okay because there is no time zone here doesn't have one I don't think the original string has that right but when we did it before it looked like it extracted it out so that's why I'm confused right I could have swore we had a VAR A variation here where it did it but we can go check the logs go look at the raw data it has it right here right so basically that's telling me that maybe it's not this function it's our it's the thing that actually extracts it out okay so I think this one should be fine we'll go back and put this back in here we could hardcode it in but I want to just get that regex that will do it and so our real problem is this thing here this thing is not doing what we want it to do okay that's our problem um so yeah it's just go back here and try one more time okay all right so I think I know where this has gone wrong so I had to read about o because I really didn't want to get into it but this hyphen f says to use a regular expression and wherever it encounters any of these characters it's going to uh create a break okay so it's going to um uh basically like if you had a string and you did a split on it's basically splitting on there and so the idea is that one will be 30 then it will break on the or split on the for Slash and then may split on the for slash 2024 split on the colon which we have here um and I mean we don't have any Square braces in here but it seems like maybe it's Square braces or that's part of that syntax I don't really know I don't really care um but what I know is that when we print this out to time Zone's 39 and 39 is really the the the seconds here and this is the milliseconds so I'm thinking what we need to do because the time and the time zone can be kind of together I suppose or like I guess it could be separate but for time I'm thinking maybe we can do print I'm not sure if we can do this print four five and six I'm not sure if this will work but I'm going to just try it here and we'll do this and so now the time is getting closer to what we want so I'm going to go back here and say four basically bringing that format back five and six okay and so we look at our time it does not like that so go back here um probably because do doubles here wonder if what would happen if doubles here like this it's not exactly printing how we wanted to print but the idea is that there should be some way to reassemble it right so that's what I'll go figure out here okay all right so I've gotone somewhere I had to look it up and this is how you do it it must be like a sub syntax of a I'm not sure um but anyway so we look here our our time looks right we're extracting our time zone I had to put the plus back in here and I don't think the these squares matter because we are not uh I mean I guess it's a range right so that actually does make sense if we're saying a range of things that's probably what it means as a regular expression so maybe I will leave those in there but technically we don't need them for all of them well we might because then the rest would get appended here but anyway so I think now we have a format uh that should be closed here so let's go run this again and um it's kind of freaking out but things look like they are in better shape so what I'm going to do is just comment this stuff out here because that's just our debug stuff and then we'll try this again so it's not airing out which is good so that indicates to me that it's Lo it's it's iterating through this and we're having a better success here okay so um what I'm going to do is now take this one take this one off take this one off and want to see if it actually produced the correct time stamp and it looks like it is okay great and what we'll do is go next here and I'm going to hope that this just works that'd be really nice if it does so it's sending what's interesting is that it's sending one log but the I guess the question is could it send multiple logs because it's showing that there's an array here right so if we go back to um and it also suggests that it's plural because it said log of events right represents a log event which is a record of activity that was recorded by the application and each has to 256 so it's not really stating whether you can do multiline but if it's doing squares here it makes me think that it's sending an array and it might be more efficient to send 10 at a time but I just want this to work okay so I'm going to try this out here and another thing you have to consider is that there could be rate limiting so right now we are going to just Spam the the API and see if it works um but you might want to put like a sleep in here or something like that because it might be too fast to send them all like that or we should be sending them batch I think ideally it would probably work in batch let's go ahead and see if this even works as we will find out here so I'm going to go ahead and I think everything's uncommented now right and so we'll go ahead and hit put logs input is not a terminal fd0 okay so um I think at least we're getting to send event logs right and so what I'm going to do is just say um Echo send log events I don't know if I need to do this here but I'm going to do it anyway I'll say Dev TTY okay so it tries to send the log which is fine I'm going to Echo the log here so we'll say dollar song log events I'm going to do Dev TTY and I don't even see anything there so I don't see any logs being passed to it interesting well here's a question if we go down to here can we print out this we'll say Echo log event so that will be this one up here I just want to make sure we don't get confused which one it is I'm going to do this here just put a few of those in the front of it and does that turn into something so that clearly is one that we have here so I'm just going to carefully look at it so we have the time stamp which makes sense um and then the message which appears to be escaped though I'm noticing here in the message it starts here but wouldn't it immediately end here if it's not properly escaped so this is what I'm thinking that this is not uh really correct I don't think that is doing what we wanted to do so I think that we should probably assemble our string using JQ so that's what I would rather do so let's say I want to assemble a uh a safe Json string using JQ I need to create the the message I need to create me uh a array of a single Json object with a field timestamp and a field message and I will supply those values with two separate n bars hopefully it understands what I'm asking for okay we'll give it a moment all right so here we have uh JQ and args timestamp message Etc and so hopefully this will cause us less issues using this now we'd have to have JQ installed and I think I have instructions installing JQ somewhere but let's go take a look do we have JQ already here we do but you might have to install just so you know and JQ could vary based on your machine but JQ is generally a more reliable way to generate out uh this here so I'm going to go here and do this just to make this a little bit more readable I want to bring this onto different lines here so we have this Arc here and then this Arc here um and then this one here okay great I like to bring that down like that let's just be ah we'll leave it up there it's fine okay great so the idea is that this should generate out our log event and so I feel like this would be more reliable I'm just going to comment that out here and this should be more reliable for that generation okay so it's generating out and it looks fine I don't really need this to be multiline so I'm not sure maybe it's just wrapping is it because it's wrapping yeah it's just wrapping it's not actually multiline um and so let's go take a look and see if it prefers that and I'm just going to go ahead and say log event here like this okay and we'll type in clear and we'll try this again and we still get this error now the other question is like we go back up to here so that other one might not have been the issue but at least we're logging here which is fine oh you know why this didn't CU that spelled right which is fine but I would have preferred to use JQ anyway I have more uh trust in that one but notice here that it is just printing out the single one here so did it even send this whole thing over is what I'm wondering so what I'm going to do here is just uh maybe do this here see if this helps it's called log event we go back to this one I'll type in clear so now it's showing correctly okay so it did have to be um in those double quotations and so now we are here and so that one is fine so now we get to this part here where it's we're trying to put a log and by the way let's go take look and see if we've actually put any logs here yet we'll give this refresh and we don't have any logs here yet so there is a way of printing out let's say print out all it um commands in bat and this way will allow us to actually just copy it there's a way to do this um I think there's like a here it is Trace and I'm not sure if we need to put the hyphen X there but we we'll give this a go and see if this will let us trace it and we'll run this like this and the reason I'm doing that is because I'm trying to see if it'll print out the a CLI command so that we can just manually paste it and try it so I'm going here and this is honestly a bit of a mess so that's not going to work but what we need to do is now solve this part here right so we know what the log event looks like I'm going to just type in clear here and try this again so put logs and I need to print out the log before we pass it in here so I just say uh Echo log event uh there might be an easier way to be honest if I go up to here to uh this command here I'm just going to copy this here if we said Echo here and then I did singles it might just print this out for us see what that does that it uh did did not we just make this one line because again it's like we don't want to have to like manually bu this in we can save us some trouble here so maybe this one we'll do it oh you know what probably should do uh Dev TTL TTY I'm just close these out so we can see this longer line here um and I want to put this up here so there's more of a chance that this will print out I'm going to put this as the first line oh you know it has to go after this one it's not going to know what it is unless it's after line 15 uh no such F directory I have to put a forward slash in the front there like that and I think TTY stand for it's like T like it's for the teletype machine so TT what what does TTY stand for yeah t t t type writer because the way computers used to work just so you know the way this is in my Vim course by the way if you took my Vim course but when you go and you um like back in 1969 when they had really big computers like the PDP they didn't have a monitor and the way they they would put input into the computer is they'd put it into a teletype machine uh and basically it looks like a typewriter you and you type and it would show it to you on your paper and then you hit enter and it would send it to the computer and the computer would print back the results and so that system's still there and so we're saying dump out to TTY um which is basically to print right and that's why the print print command is called print okay we'll go ahead and we will try this again and so now we can just copy this command the part that we want and then we can debug it CU maybe my quer is messed up like we don't know what's wrong here the only thing that kind of sucks is that it's not printing out the actual um uh the these these ones here uh which is kind of annoying so maybe those aren't accessible there and that's the problem so what if I go ahead and I because we only use these within the function so I'm going to go ahead and just grab these like this and print them out like this and then we could set them as local so I don't think we use it outside of that we don't okay but they probably would work in the function and and keep them at the top but I'm just trying to rule out possible problems that this is causing but I would assume that this would have printed out uh the string but maybe it's because we have it in single quotations it's not interpreting them um so I'd have to do does it have any doubles it has one doubles in it so it's not that bad so I'll go here I'll do this I'll just have to esape that right there okay and then I'll try this again log file ambiguous redirect line 95 oh yeah I don't care about that I wanted to see if it would print this out here and so for some reason this Echo is not printing out the dollar like why is it not printing those out it's kind of frustrating um it could be because we have uh this one here so I'm going to go ahead and just remove this one here just take that up like that now does it print it out oh you know what it's not even um it's not even proceeding to the next one so I don't think that's the problem line 95 95 sorry line 95 I'm just going to take this back up here like this maybe that other one wasn't the problem we'll go ahead and check this again and so maybe it was that dollar sign yeah I think that's what it was it was that so I'm going to take these back up I'm GNA move these back up here hold on we'll just undo a bit we'll move these back up sorry and that was all of them I think and so what I'm going to do here is I'm just going to take this part off because it's trying to interpret it okay so we'll try this again and so now we have our put log that we can test so we can see if this is the issue oh it didn't print them out it doesn't print them out it's driving me crazy okay we'll just manually assemble it just would have been nice because like if we're debugging it it goes a lot quicker um you know so yeah whatever whatever okay so I'm just going to go ahead and just copy these manually but I don't want to print that one out that's going to be pain there has to be like it print it out just a sec okay I'm going to pause I'll figure it out okay it's cuz I got rid of the double quotations we got to put those back in there so those matter those matter and so now I think that if I do this one like this like that it's printing that one out now great okay but why is it not printing out log stream um I don't think it's doing that one is it no no it is okay so they're all there and that was the only issue there okay so now let's go ahead and copy this this one to here and we'll paste it and it has a problem with something here so it thinks it's incomplete so I'm going to try to run this again okay and then I'll copy this I'll just make a new scratch Pad here just say new file here paste this in because it it can't be multiline right don't want any so maybe we get rid of the brakes is what I'm thinking here um I really would like this to be a single line so go back here and um JQ should not format uh should output the jcon all on a single line no line breaks okay okay let's see if we can tell to do that literally a single flank it says hyphen C okay let's see if that actually does that we go down here this is what it's like being a cloud engineer or or whatever devops it's like fiddling with these scripts till they work witha stuff okay so in here we'll do hyphen C okay and so what I'm hoping for is that this will print out a little bit nicer there we go that's looking a lot nicer and so that's going to make it a little bit easier when we copy and paste this because then we're dealing with line breaks and that's another thing we have to rule out so it still has a problem with something missing here so I'm going to copy this here this command and I'm going to carefully look for where the problem is we're going to pretend that our company does not give us an AI assistance tool and so I go to the beginning of this line here I'm going to tell this file to wrap so I can see a little bit clearer what's going on here edit selection wrap WP where's WP do we have a WP where's a wrap wrap WP WP come on word wrap here it is I use macf I don't use this every day all right so I'm going to carefully look and so I'm assuming that the problem is somewhere within here okay and so I'm thinking I know what the problem is immediately it's because we have double quotations and the interior has double quotations we can probably solve it with single quotation so if we go back to here here the problem is probably um uh the the doubles the doubles which is up in our function up here right and so it probably we just need to do singles and that will fix our issue okay we could also tell JQ maybe we say JQ can you uh could you not do that here so if we did this we'll just change it right now right to this but we'll interpret that if single quotations don't know well we'll try this but that might not solve our problem okay so I'm going to go ahead and do this again and so I don't think the singles fixed their problem I think it's that we need to escape Escape it so I'll go back here and say um please escape the uh double quotations for the string uh when outputting JQ and I maybe there's a function for that we'll find out in a second all right and so they're suggesting this one line here um I said to only escape the double quotations it's like escaping a lot more than that all right I don't know anyway we'll try this and see if that fixes our issue here because we we're going to have to stick with those um those ones there so if we do this if we paste this in here like such okay and then we go back and we place this here and then we do this doubles now I'm not fixing the ones below I I expect it uh to mess up that's totally fine what I want to see is that there and so it didn't print out anything oh because we have to do this here like that here we go we'll try that again and it looks more reasonable so this one is just escaping the interior ones but we don't have any doubles on the outside of it um probably because this is a little bit different so I'll go ahead and do that there we go just because we are uh echoing it out that's why we have to do it that way so I'll go ahead and copy this and pit enter it still says there's an unexpected one here so I'm going to go ahead and copy this here go back to here we'll look at it carefully it obviously is something to do with our our log events so I don't even care about the rest up here I'm just going to go to log events here log events end of word um delete to the start there we go so I can see this more clearly and so clearly there's something wrong here I'm seeing again here this is not escaped so this is escaped over here this is escaped over here that's escaped that's escaped why is this not escaped doesn't seem like it's escaping everything okay so let's go back over to here it takes the whole thing it's supposed to escape it right but it clearly is not doing that let me fiddle with this a bit more okay all right so I just want to show you I try just taking it and I'm trying to just format it till I can get it to work and so I'm trying the shorthand syntax it's still saying a comma is missing or something even though I've I've gone through it so I'm going to just try to put it in the Json format now all right so I got it to work it it worked in this format and so now that I have a structure to which I I know that's going to work then I'm going to change this so instead of passing a Jon object I'm just going to pass these two values and then place them in here so again I'm just trying to speed through here so I'll just get quicker to the solution here okay so you don't have to watch me do every little thing I just just don't want to skip this part here so this part should be pretty easy because now we have um uh we don't need to do JQ or anything like this I don't need to construct it here uh which is basically we're going back to how we was before but we're just kind of simplifying by doing it in the command as opposed to making a string and then passing the string um and so here we want to do timestamp and then message okay but I'm going to pause here and figure out the next part I just simplified it I just passed the line in because it was the message here okay so we're getting closer all right so I got it to work uh kind of um basically back top I was like Hey and you know what it did is like put it into JQ This and like we're bringing back all the stuff we had before which I thought that's what we'd have to do um but a few different things is that it told me I mean obviously for the sequence we should have done output text because um that would produce the thing we want but basically it said just tell it to ignore the error and I went really tell it to ignore the error that seems like dumb but for whatever reason so it says here we C both the output the war the message warning should be suppressed so it's suggesting that it's not a proper problem it's just the warning the command should execute correctly which is true because I go here and it's in the logs okay um so I guess it's just a warning but anyway now that I have these tweak the question is could I import the whole thing also I started with a sequence and I didn't continue on with the last one so I'm not sure if that will run into an issue I'm going to go ahead and try to run this uh I got to get rid of the the set e because it actually is aing out when it hits any kind of error so this might work is that working we'll go back over to here refresh this so I don't see depending so I'm going to stop this here uh let's see how long does it take for cloud watch logs to show up in cloud well you know what I think that it says 5 to 10 minutes but I don't think that this is uh correct I think this is actually when we got it working before remember that I tried it manually and it worked that's probably what this thing is so it's probably still not working I don't think we can ignore the error I think that's chat jpt being really really really really stupid here um so I'm going to ignore that and I'm going to assume that's not going to work I'm going to wait around here just to see if anything shows up so I'll be back in 5 to 10 minutes here but I'm going assume that it's wrong okay also um I'm not waiting I'm done waiting here but also didn't update the bottom one here as this one's still just doing this but again um I think like if it worked we would see another record right so I'm not convinced I guess what we could do is we just tear this down for a second and and make it again as I just want to make sure we are uh ruling this out here so we'll go ahead and delete this log group and I'm going to go back over to here I'm going to go back to our read me which which is down here I really thought this was going to be way easier but like everything with Cloud everything's much harder than it always has to be um so I'm going to go ahead and do this again call this app 2 just in case the Old One's still there I'm going to go ahead and copy paste and we'll set retention to one day here this is now two copy paste and I'll go down uh to this one here okay and so I'm going to go ahead and try running this again well before I do I got to change this to two and this is also going to generate out a new uh stream so I got to go back here and check again and we we could have coded this in Ruby or python or something else and maybe we would have had less issues um because we could have used the SDK but I think a lot of common use cases for devops would probably be utilizing A bash script for this so that's why I kind of focused on this even though we're having a lot of frustration with it um but we'll go ahead and try this again okay so it's running and I'm just going to ignore it I'm going to let it run a few of these and then I'm going to stop here and I'm going to be back in 10 minutes okay all right so I'm back and um I'm G to go check here and I don't think it worked we have no logs so that error that we were getting when I kept Googling it kept talking about Python and the reason why is that the a CLI is ridden using boto 3 and so I'm thinking okay obviously there's some kind of syntax issue in bash it's going to be really hard to figure out and honestly I've always had issues with that and I think everybody that works with bash files do and chbt and these tools cannot figure it out because they're just not nuanced enough to do that um and so we could write it using the SDK using python but since we already having that terminal I just want to avoid python in this case and I've actually uh have a ruby script here so what I've done is I just told Chad gbt give me a Ruby version of it and then I formatted it because they had it like this and I don't trust their code so I just turned it into a class and so the idea is that um I like to write classes as stateless classes meaning that um you have individual functions you know exactly their inputs or outputs and this makes it really easy for testing debugging so here we have a function called run and we create a new client all right sorry I had a phone call come in so that's why I'm a bit U confused um but anyway so the idea is that yeah we we do this we create a client to Cloud watch we uh we don't have to do it here but what this does is it will check it will attempt to create the uh the log group if it it already exists it will error out and we'll just say ignore the error we'll do that for the the stream as well so this way we'll have a guarantee that will create those those two here making our other CLI commands not needed then we have parf log file so we go down here and then we need to provide that log file path which apparently I did not supply here so we say log file path you don't have to obviously change it you'll just run the code AS you'll find it here in the repo and so looking here it looks like it's attempting to parse it grabs the time converts it to milliseconds gets the um this is message here oh it takes the line and strip strip just will remove uh forward or trailing um spes uh or um or even possibly uh new line carriers which is fine and then it'll turn it into uh a a ruby hash and then it'll push it onto this so this could actually be push if we wanted that instead I'm not sure why it does it that way but same syntax I think that one's a little bit more readable so we'll switch it over to this okay and so the idea is that we are uh we have an array here we're collecting that new formatted stuff and then we will have the log events here so we go back up to hear it and so now we're outputting it here right input output this makes reading code I do this in all the languages it's not just Ruby um but this just a method of having functional code that makes your life a lot easier so here if there are no events it'll say it'll raise an error and say hey there's a problem and what's interesting and I think we kind of saw that earlier when we when we had our um bash script um but um apparently what you can do supposedly until we execute this is that you can describe the log group and you can see if there are any streams and then you can go ahead and say okay get me the first upload sequence um and so what that will do is it will grab that upload sequence if there isn't it will air out and it's rescuing I'm not sure if exit one that'll actually exit it out so that would kind of suck but uh yeah so here we go because I think the thing is like you create a stream you have to upload to it and you can't start over from scratch right you have to uh have the stream complete right so anyway um if it notices that an upload sequence exists um and we have a sequence token it will assign us a sequence token which is great so I think what I I need to do here because I didn't do it up here I'm going to go here and just say nil actually I think what it's going to do is it's going to check if there's a sequence token and if there isn't then it's going to just pass an empty one so I was I was misspeaking there how that worked but we'll go ahead and say return so the idea is that it'll check if anything's been uploaded before if it has give us a sequence so if it's not then it'll be nail and then it will just start and that one will turn one we'll go down to our logs so we have our put log here and now we go down below to here to to put log here on the right hand side and so that will take in that sequence here and we'll put our log events and then it's passing them in bulk so I guess you probably can do them in bulk and that would be the more efficient way we had a bit of uncertainty there with the bash scripting because the way TPT did it out and the and the CLI did not tell us otherwise but if this is Ping from Ruby examples it's probably correct and so the log events I'm making sure they're coming from that and so then this will upload the only thing here is that it will return back a sequence but if we don't need to you know what I'm saying like we're not batching this right so how would we know if we we we were over the 250 limit that's something that this doesn't factor in in our script um but what I'm going to do is I'm going to attempt to run this and if it fails I will tweak it I assume this will fail first attempt but the thing that I'm missing here is the um run bundle in it we need the gem file we need to include a couple things so I'm going to put gem ox or noiri noou no Kiri which I just recently means saw in Japanese and you know why because I've been doing Japanese woodw we'll say here we'll put fry in here Ruby is also made by uh it was made by a Japanese person uh so it kind of explains like when you see Japanese words in here we have gem pry and then this is the inabus s k Cloud watch logs we'll do a bundle install and if this doesn't work I will pause the video and I'll get a fully working version come back here okay I promise this time and and so we'll do bundle exec bundle exec Ruby put logs okay no errors that's interesting but you know what I didn't do oh this doesn't actually work because we didn't run it so to run it we could do I'm G I'm going to use rake for this I don't always do that but I'm going to do rake here today rake me uh is similar to make but it's the r is for rake so if you know um make files you know what I'm talking about here and we type a rake file here it's just a way of executing stuff using rake so I'm going to go here and just say task put logs do and um or we'll just say log and then the idea is I can just call this function so we go ahead and do this here and I'll just say require relative and we'll say uh this is called put logs put logs RB and I just copy that so I could easily do this so now we can specify all these things here in fact we could just make it whatever we want um with the exception of the log file this has to be web server logs. log and so this could actually be something new so we could go here and say like example uh basic app three here and then this could be um time. now toi I don't know if we'll have to require time for that it should be uh built into Ruby but the idea is that will get us in milliseconds I think the um the value here so what I'm going to do here is um I'm just going to go ahead and because I want to make sure that we actually get data here and so I'm just going to go ahead and just say puts log events because I want to just see something thing okay I don't want it to get through here and then find out it doesn't Parts properly so I'll go ahead and we will go now instead of doing ble exact we're doing ble rake logs or log sorry did I do a bundle install after that by the way after I installed rake I don't know if I did that and what do we have here expected log stream name to be a string and got an integer instead oh that's because this uh this has to be that so we'll say 2s try that again and put logs the parsing is not working okay great so I'm going to go ahead and update our docs here Ruby Ruby SDK put SDK logs and so we have a bundle install and then bundle exec uh rake log just so you know what it is and I'm going to go solve this cuz I can definitely solve Ruby you know I'm good at Ruby I'll be back in just a moment all right so I ran into one little snag again I'm going to get this fully working but where I ran into an issue was uh this time parsing again the parsing is messing up um and it's just because Ruby time parse can't handle things uh in a standard way I just put a pry here to uh find out that that was the issue um but uh there is a library called chronic and I know it for many years because anytime you want to par something this thing is like a really good to par so I'm hoping that this works but yeah I'll just continue on here okay and I'm actually surprised it's the first time it can't parse something so I just wanted to show that I attempted to parse it but maybe if I provide its format that might um that might help but I guess I should have gone down here and checked and so oh it should have parsed it it does not look that format but you know what it's not hard for me to um manually parse this I'll be back in just a second okay all right so so chronic did not work and so I just had to convert it I had to tell it exactly the format and now I have in the format okay so I'll continue on here all right so uh all I did was fix that time time issue ran the script and it looks like it's working so let's go over here and see if it h happened to insert so I don't oh you know we're not on app 2 anymore right we um ours is now called app three so down below here and we are seeing logs what's interesting is that it's showing these as different times you know why it's because I ran this multiple times and every time I picked up the time right and so you can see here that these ones don't have any events in them so they they are failed attempts right um and so that's where you know you might want to make sure you have the logs before you create it so we could change our script so it's a bit better here but it instantly logs we didn't have did not have to wait and so there we go so I guess we'll just use our Ruby script and and and that is that um it's uh unfortunate we couldn't get the scrip working but those are the two points of contention is parsing times and dealing with escaping of characters so that's not an uncommon experience with for uh like if you're actually doing logging you probably again use the cloud watch agent and you would specify what logs you want to log and what format they're in um we probably should explore the types of format so I might go back and update the uh course course to have um different formats in there but um that solves our issue so I'm pretty happy with that let's go ahead and commit this just say uh Cloud watch logs basic and yeah that was a longer video but what what can we do here I want to um well we don't have to get rid of those logs they'll just delete on their own after a certain amount of time but if we want to just clean up here we might as well go ahead and do that so we're going to delete this log group we'll say delete and we'll go all the way down to the bottom here and get example here and delete and you can delete whatever you like and I'll see you in the next one okay ciao okay so we looked at log groups and log groups contain log streams and then inside of a l stream we have log events so let's look at log events so a log event represents a single event in a log file and a log event can be seen within a log stream so that makes sense so if you were to open up a log stream here are log events each of those uh uh each of those lines is a log event uh and so what you can do is you're able to filter um uh these log events with a simple pattern matching syntax so if I wanted to just put in D which stands for debug possibly it will then pull out all the debug lines uh or the uh log events that are for debugging okay but we'll look at some more richer uh options for filtering next with Cloud watch logs insights all right so let's take a look at cloudwatch log insights and this allows you to interactively search and analyze your cloudwatch log data uh it's more robust than what we saw on the last slide it's less burdensome than exporting your logs des3 and analyzing them via Athena and it supports all types of logs and so the way it works is you're going to log into the adus console and go to cloudwatch logs logs insights and that's what you're going to see you're going to have this little language you can put in there uh and you can run the query you can select the log groups that you're going to search across uh and so they do have this own little query syntax that they have it's not too hard to learn it's not exactly SQL but it's uh again it's not too difficult uh and they actually automate automated a lot for you so you can click a bunch of buttons and it'll just generate it out for you uh a single request can query up to 20 uh log groups uh queries time out after 15 minutes if they're not completed and query results are available for 7 days so let's just look at a little bit more stuff here uh so obviously that query language uh looks very complex but the great thing is adus is going to provide you a lot of great examples to get started so what I can do if I go to the the left right hand side and I click on queries they're going to have a bunch of sample queries and they pretty much meet all the use cases you'll need um I I rarely am writing uh queries by hand I just go to the sample queries and then tweak them from there so you click apply and so uh now you're able to then visualize your information it's cool you actually get a little a little visualization graph uh there as well and you can also save your queries um so if you do have a query and you feel like you're not going to be able to write again just save it and you'll have it there for later so we said that cloudwatch log Insight supports any kind of log file and the way it does this is through discovering Fields so when uh cloudwatch insights reads a log it's going to analyze the log events and try to structure the content by Jing fields that you can then use in your queries so cloudwatch log insights inserts an at sign symbol at the start of a field that it generates and it has five system fields that will always always be generated no matter what uh type of log it is reading from and so we have message this is the Raw on Parts log event we have timestamp this is the event timestamp contained in the log events timestamp field we have ingestion time this is the time when the log event was received by cloudwatch logs we have log stream this is the name of the log stream that the log event was added to we have log and this is a log group identifier in the form of account hyphen ID colon log group name uh and then we we had the systems Fields but let's look at the actual uh fields that it automatically discovers from different services that use cloud watch logs so if you're using Amazon VPC flow logs we're going to get uh these ones if we're using R3 we're going to get these ones if you use a Lambda we're going to get these ones if we're using AOS cloud trail uh we're going to get these ones but notice that we're going to have to look at the full list uh and I believe do I believe we do look at this um but uh cloud trail has a lot of information there so we'll have to see what it actually can pull from it then we have Jason logs so these are these the fields of a Jason log will just be turned into fields and for any other types uh uh that it cannot discover you can just parse the command to extract and create uh etherial fields that it uses in the query so there you go oh just one more thing here I just want to show you uh uh what it looks like uh so when you use a log if you click on fields you can see all the discovered Fields there and then you click on them it'll just add it into the query for you so there you go hey this is angre BR this video what I want want to do is take a look at um log events so the idea is that we create a log and we want to be able to easily search the logs for information uh so what we'll do is we'll go back to our repo and um what did I not change here I guess I forgot to push this commit you're probably going to want me to push that commit so I'm just going to open this up and push that commit but we're going to work in our it was examples repos as per usual so just give me a moment as I uh commit our last changes and we'll continue on from there all right so I'm back here and we're going to go to our cloudwatch directory and this one is going to be for log events so I'm going to just make a U CD into this directory Cloud watch logs and we'll say mkdr events I'm not sure uh if there's anything to do in this one like in terms of writing new code but in case we do I'll just make a new read me here because I don't know if we can filter um uh logs using the CLI that'd be really interesting to find out because we can filter like this right but maybe we can try to download our logs or see if there's some kind of filtration method I know there's like a way that we can monitor our logs using another tool but let's go ahead here and the first thing I want to do is generate out our log so in our basic because we already have that one working pretty well we'll go back to basic here and we'll do bundle uh bundle install because it doesn't automatically install the uh dependencies and if you remember this one from last time all the script does it's actually our p python script we want to run is that it's going to generate out um an elf formatted log so go ahead here and say um generate say python generate logs and so that's going to Output the logs which you actually already have right here so we didn't really have to make another one but I'm going to just clear that out and we'll do that again whoops just go ahead and delete this file here I forgot that it was committed so we didn't really actually have to generated again but I'll I'll do this again here and I'll give this a refresh did our script oh you know what it is that's not how it works the way it works is we have to Output it to a file like this okay we'll say allow and so now we have our log file here it's the same thing but at least we'll have updated time so you know if you're doing this you might want to just delete it and run it again and so the next thing I want to do is run my uh bundle exec rake before we do that I'm just going to go over to my rake command here and we can just adjust this appropriately so we just say like um Events app like this okay it'll do bundle exact rake log all right and so that should generate out some logs for us we go back to cloudwatch uh and we search for example we now have this one here okay um if we click into our log we have all of our data here it looks like they're all expanded so maybe it remembered from last time but normally you can tell it to yeah collapse all the rows and by the way you can uh copy the results here download the CSV if you want to I've never downloaded a CSV so I'm just kind of curious It's not usually how I work with logs but we'll go ahead and open this up in Excel for fun okay and so we downloaded it it's just the time stamp and message nothing super interesting there but what we want to do is we want to and by the way you could tail this so like if let's say we're pushing this and things were changing then we could see in real time if something's uh there just to be a external plugin for that let's go ahead and we'll type in something so let's say we wanted to filter all the login Pages we say for login and so that didn't exactly work how I thought it would let's go look at the filter patterns I was looking at the individual slide and it didn't show any filter patterns but clearly there is some uh patterns going on here so we have some regular expression stuff so me putting a forward slash probably messed it up a bit and what's interesting is like if you have we're doing we're working with unstructured log data right now so if you have unstructured log you just put a term in two will give you multiples um opt optionals or phrase with parentheses if you don't want it to count as separate on so that's interesting but I think what would be interesting is actually generating out some Json data and um filtering that because you get some Advanced options for that or improved options for that so anyway uh what we'll do is go back here so if we want login we're just going to have to put in login here so type in login and I was really expecting it to match this so why is that not doing that login or let's try HTTP get that out of there so say login contact it doesn't really look like it's filtering how we were expecting it to filter so just give me a second here to read um but it usually is just as simple as write it in give me two seconds all right so I have no idea why but I typed in home and now all of a sudden it's deciding to filter so I'm not sure why that was having an issue it could be something with the URL at the top here so noce that um I'm not sure again if it filters out all the information but we'll go back here to this log here I'm just going to go back in here give this a hard refresh and we'll try that again give it a moment here to reload so we'll go to the top here we'll type in home enter so now it's filtering for home why wouldn't it do login let's try login again now it's doing login and we could use a regular expression so if we wanted to say home or login maybe we could do this um that's usually how maybe we have to put squares around it uh nope not exactly no um I mean we really could just put two here right we could just say home and log if that's all we wanted so we have home in here log in please and so you can see this thing a little bit finicky and uh not very reliable but supposedly works sometimes I think really has to do with how it fil filters out this into the top here but um yeah so here's Define a character class that it be like a bunch of characters or the or and so that's what I was doing was I was giving an or but maybe we have to put percentages in front of it let's see what happens see do um percentage home pipe or this we'll try that so hit enter okay so that now giving us the behavior that we want all right so pretty straightforward uh but it's not the funnest figuring out that syntax but what I would like to look at is Json because Json uh filtering is is extremely useful so it says filter patterns to match terms with Json log events and then we have using filter patterns in match terms in space delimited log events so there's ones that are space delimitated which actually looks like um the one that we have here because uh this one says unstructured right so that means like any pattern above unstructured but um space eliminated is actually what we're utilizing so that's probably we should have been looking at this so here it says you can create filter patterns to match the space of limited events the following code snippet shows a space elated log that contains these fields characters between brackets and double quotations are considered single Fields okay but what's what what are we looking at here so let's create a filter pattern do this yeah so I'm guessing what it's suggesting here and this looks like it's really looking at like the ellf log so it's good that we did that before but if we go back to our logs here let's just expand this for a second yeah I'm not exactly sure on how how to work that but uh I'm more interested in doing the Json so let's go ahead and uh figure out Json so what we'll have to do let's go back here and we'll have to generate out some Json data so I'm going to go over to chat gbt to save us some time I guess we'll do it in Python because we've been doing everything in Python so uh using python write us a uh script that will create um mock web server logs the logs must be in Json format okay so we'll give it a second there to see what it comes up with all right let's see what it came up with it's apparently going to use Faker which which is totally fine I'm totally comfortable with that so I'm going to copy this and we'll go over to here and we will go into our Vents and we'll call this um generate Json logs. Pi I'll go ahead and paste that on in here and looks fine to me I guess I don't know and um we do need to install Faker so I'll go here and we'll say uh we got to go into the right directory I don't know that's going to add it to a pip file though oops I don't want fake I want Faker so I never remember how to do this in uh python but we'll go here and we'll just say uh Faker that way we'll know that it's included and so what I want to do here is attempt to run this what does it need they probably gave us instructions it's probably similar to the other one it doesn't show us how to run it this one in particular is telling us to save to a specific file up here so it doesn't work exactly the same way as our other one but I'm going to call this just uh uh web server logs Json I think it's totally fine we could even uh increase it to we'll leave it as 100 right now I'll type in clear and we'll say python generate logs hopefully that works and we now have logs we'll go over to here yeah looks like something so that's fine uh so what I'll want to do now is upload those logs and the thing is that we have our put logs here so we'll have to make a new file here we'll call this put logs json. RB and we'll bring over our rake file so we'll say rake file and we'll bring uh we'll have to bundle AIT here and it shouldn't be too hard to adjust our previous script so we'll go into our gem file here copy this we'll go down to our gem file here and replace it and our rake file we'll want something very similar so I'll copy this I'll paste that in here I'll call this put logs Json put logs Json and we will grab this one here and paste it here so what's going to be the difference here well first before we move on let's go back to our rate command here it's already says it's already called events here so that's perfect and but I'll say events Json app so something a little bit different there so we know what it is and um this one's a bit different because what we're doing is we're we're not parsing uh parsing Elf or parsing Json log file is that what it's called ef ef log file okay it is I keep saying LF it's just spells elf I haven't said elf once I'm not sure if that's frustrating anybody but whatever so the idea is that we're going to bring the file into here and what we want to do is we want to read the file log file path probably not the most efficient way to do it here they actually iterate through the lines but what I don't know is if the uh the log file here is actually um uh per line so what we'd have to do is we'd have to check the logs here not those ones but this one and probably what I'd actually want because if you got logs it would actually probably be like line per line right so I go back here and say uh the uh the logs should be adjacent object per line not in an array no commas between the items of the array please because that's actually how a log would look like the other question would be if it generates it out so it does the dumps right so I'm hoping that that will be in a better format it's like writing me the script like three times is it a dummy here today we'll give it a second all right I think chbd is having a hard time because it just kept cycling and cycling and cycling I think what's happening is it can't make it to the end and that's where it's having an issue so it keeps trying to complete it like why is it doing multiple versions of it I don't understand um but anyway what I'll do is I'll just drag this off screen so I can kind of figure it out and just uh change it here so I'm going to try to see what they change so the idea is that we get to generate logs and honestly I'd rather that this works like our last one I don't really want it to uh dump a Json file so I'm going to do is go back up to here and uh in here all it does is we have log entries and then it iterates through them which is fine so here it says logs D logs which looks probably similar to our other one and so what I'm thinking is that we can do this here okay and is this one still called generate log entry it is okay so that actually will work fine we don't get to specify our thing it's just 100 here so I'll just take this out of here and this out of here and this out of here get out of here and um so I'm just trying to look at this one this one here generates them all out and then it assembles a string okay so this is actually assembling a string whereas this generate logs this say generate log entries but that's just repeated here so I'll take that out of this this will actually return back a a log entry object here so that's fine but I don't know how this is going to print it out because I'm not sure what it would print out here so I'm just going to try it I'm not sure what's going to happen and we'll update our read me so it's more like our other one so this will be gener web server logs Jon actually no we just call this Json that would make more sense no yeah it's fine we'll just call Json it's not technically a Json file because it's not format correctly but I'm got to call it that anyway okay so just understand that I'm uh taking some Liberties there so we'll go ahead and try this and we do have an issue because I'm not very good at python um and this is saying cannot open file generate log. Pi that's because this is called Json here and we'll hit enter and so now we're getting the format that we want so this is what I really wanted and I don't want commas on the end and stuff like this like when you're working with big data this is generally the form format that Json will be presented and will be pared parsed upon which is single line and stuff like that the only thing that I'm seeing here is that we're seeing single quotations I'm not sure if it matters if it's single or double so just give me a second and so uh it's saying here it has to be doubles okay so it is printing these out but the problem is that this is not um uh the correct format so we can't just print it like that so I'll just say uh let's say print Json single line JS it's probably stringify but oh I got did I say python I don't want a pretty print I just want normal printing maybe it is pretty print and you can just format it different ways so okay let's take a look at dump here default it says indent none so let's go back to here maybe the other one had that on there and so we'll just go uh Jason dump entry and let's see if that does that works maybe Json like that nope it's just Json is it imported y it is okay missing one positional argument give me a break python dump single line Json come on why is this so darn hard this is why I don't like python just tell me without me having to read a thousand things you know give me a second okay chbt says it's this it better be this okay so go ahead and paste that as such I still think we'd have to print it right so we'll go print here and doesn't like that probably doesn't need parentheses around this let me thinking I'm in Ruby here we'll go ahead and enter again um this is the entry here and I'm going to go here I'm just going to delete this so I can make sure this results works we'll hit enter we'll go back boom okay so do I have the logs in the format that I want yes I do excellent we'll go back to uh Ruby I can't remember if we finished that or not but all we have to do here is um because I didn't want to use the read here I'd rather it for the file because that's actually a better way to do it and so in here the idea is that we need to get the time out of here so we look at our logs which is here we have a time stamp and so that format looks like ISO so it looks like this is something that I would think that Ruby could par so say time parse this but I don't trust it so I'd rather get the um St stpr time for this I'll just say I'll go back over to here and say t stbr Ruby for this date time format because that will save us some trouble here we'll hit enter I hate that it explains things I just want the information come on give there we go there we go that's all I want I just want this part here thank you thank you go away okay let's go back over to here I'm not sure what I'm more abusive to Alexa or chat GPT it's hard to say I'm going to go ahead and paste this in here there we go uh I don't think the other one had singles no they didn't so I'm going to go ahead and do that I think it's just getting doubled up by accident could be singles or doubles here it doesn't really matter and so that should get us the time stamp in the um proper format this is per line so I would say line or this is the same here but we want to take this here and say line timestamp so that will get us our time stamp and then we have our message and our message is the line okay so these are the two things things that we need to return and so I'll go down here delete out the rest yeah the other one does time samp and message so that's perfect and we actually want to yeah push this we'll say log events push like that these are now have to be colon because that that makes more sense and that looks good I'm going just put like do line on this so I don't forget what this is a file for each I always do in the end just I don't get mixed up and so that will get us that now the thing is this line well it's parsing each line but each line is Json so before we can do that we'd actually have to parse the Json because I think it would be treated as a string and so I have to say parse line and that will get us the Jason and then we do this and then we provide the line because the line's just going to to be raw or like a jifi uhj a stringified Json stringified Json that's what I think it's supposed to do okay and I think we're supposed to have a comma there in the end or it's going to error out so if that's changed I don't think we have to do anything different to the script it was just that parsing that we had to change so um I'm going to go type in clear here I'm going to go back to our rake file this one is using web server logs Json and we'll type in clear we'll do bundle exact rake log and it says it doesn't know what it is because we need to change the name here there we go we'll try this again and it has a problem with put log put logs Json put logs Json put logs Json it's the same put logs Json put logs Json oh here right so these are changes as well there we go um and then this one as well I don't think we can use self inside of self like that so that's why I do it maybe we can and I'm just like paranoid and I I over I'm over verbose here but that's how I just do it okay and so if you scroll on up to B integer got time class instead okay so what I'm going to do here I think I have pry in here I'm G to go and put a binding pry right here because maybe maybe it's already in a date time format maybe it was interpreted as a date already and so we go here and I type in I type in Json q and then Json and the Tim stamp check it here Tim stamp it will say class see what it is it's a string so it's definitely not it's definitely not a time stamp so what do is I'm going to grab this here credit and then I'll put it above you know what this is this thing's actually fine it just has to go to 2 I I think that's my problem that's our problem okay so I'm going to go and type exit exclamation mark down here exit exclamation mark clear and we'll try to run this again it says log events in a single put events requests must be in chronological order and I think the reason why this is I didn't know that that's an issue we have to have them in chronical logical order that's interesting but I think the thing is that as we iterate through it I'm trying to think of like would it make it go backwards if we start in the first one it was the first one it would push no no it would still end up in the the correct order let's go back over to our Json here and it's suggesting that's not chronological so I think that's an issue of Our Generation script here so yeah here it's saying random times let's go look at our old one and I think it's just that this script sucks is is basically what's happening here so we're going to go ahead and copy this and the other thing is that this one doesn't even use Faker and it does the exact same thing which is really annoying um so I really don't like the fact that this uses faker cuz then it's like we we have to be reliant on that I kind of prefer this one here and the only difference is that um is how it's generating the log entry so you know what I'm going to do I think we're going to have to just take this one and and bring this one over here sorry we'll go down like this I just want to be very clear which is new and which is old and I'm going to go ahead and paste this in below and we'll close this out close this out close this out close this out I'm going to split this over here and so I I'm going to work with the top one and the bottom one so the bottom one is the old one which is better or sorry like this is the one we want to keep and so all we want to change in here is the generate log entry so if we go here the key difference is that it's just returning instead of this it's going to return this okay so we we'll have um time stamp is time stamp I mean we can just go down the list here we have IP is IP that's how we do it or no you know what I think it's equals I mean that one's using colon okay sorry and then uh huh hold on here let me go back for a second I'll keep this around as a reference here this other one here okay so I'm going to try this again so we have IP just put two here so it doesn't I'm not sure why it's showing that error that's kind of annoying but um I'm going to drag this on over IP is IP user identifier is user identifier I realized I'm missing the commas I'll come back to that in a second user ID is user ID time stamp is timestamp method is Method path is path protocol is protocol protocol status is status bytes transferred is bytes transferred referer is referrer user agent is user agent okay so this has to have commas around it's going to complain like that this one can go away this one is now log entry um need comma on this one here okay so that's good the other thing we want to bring over here is um this other one so we scroll on down here very similar except this one is just doing this and the other one's wrapped in a main and like you should have a main for um whatever but I don't care it still works without it so whatever and then I think what we can do is we can get rid of this top one and now what's really nice is we can get rid of our requirements.txt because I don't want to have that there okay so I'm going to go ahead and delete this we're going to go back just type clear here we going to type in Python I'm going to go grab it from here the command and so hopefully this works without issue enter it doesn't of course it doesn't and it's saying uh Jason name Jason is not defined we go down below here we just have to require it so say require Json here or import Json and I'm going to go ahead and hit up and so I'll check my logs now and looks fine I'm not sure why our our user identifier and ID are blank and it refers blank is it because it doesn't actually generate those out okay so it's like basically saying like hey you don't have these right so they were first and that's fine I don't care um but now we have data that is more structured the way that we want I kind of remember oh because it wasn't doing it in the right time order right so now I'm assuming that this time stamp will uh go in the order that we want also the other thing is that now it's matching the format of the other one so now I have to go back to our Ruby file and then in here there was this uh formatting and we got to go back to our older one or puts log and we'll go down to this one here and we'll bring it back this thing in here and actually you know what I just noticed this is hardcoded it's not even bringing in the timestamp so our old script has been mucked up this entire time and I never even noticed which is bad um but maybe you notice that as you're working through it so I mean we'll fix it now which is not a big deal but uh this here yeah how would it how would have this one worked before time stamp time stamp yeah how did I not notice that before okay well it's lesss of a problem for this one but we'll have to fix both so let's just do this one really quickly this one uh all we had to do is get this format here so we'll grab this and then we'll go ahead and grab it like that and so that will fix our issue but the question is like did this log actually do the thing that I thought did because I thought maybe this one uh generates it in order the um time stamp random time now random minutes um so I will just say here like change this function so every subsequent call it will increase the time uh from the previous time because we don't want random times and I don't want to have to do a sort function here okay and so we have a function within a function cool whatever sure why not and so I'm going to go grab this one here and now what I'm going to do is go back and delete this one we're still going to fix our other script lucky for you if you're if you're in the uh if you're watching this we'll fix it so you won't even notice that I made that mistake generate Json logs python log um Json and we'll go up here objective type function is not a Json serial serializer all we changed was one function okay I'll undo that for one second here let's try that again no problem so it does not like our new function we'll paste it in here what's the difference from the last one face I mean it looks like the same thing try this again all right give me a second all right so now they're just having this external variable here so I guess we'll just adjust that here and we'll place this here and we'll go back we'll grab this one I mean I don't really like the idea of having a function inside of a function but we'll try this and I'll go down here we'll delete this one permanently we'll type in clear and I'm going to go ahead and generate this again and let's see if the time actually increments it's now incrementing correctly okay so that should be less of an issue I'm going to go over to cloudwatch because we're probably creating um maybe the same log but I'm going to go down here and just go for SL examples and see what we have okay so we have a couple here these two here I'll delete these log groups and uh we'll go and type in clear here and I'm going to see if our other one works now and it looks like it worked perfectly fine so we'll go over to here and we'll go into here we'll click on our stream refresh I don't see any data so it looks like it worked so I'm not sure what the problem is there so we'll give that a little wait but we should go back and uh cuz sometimes there can be a bit of a delay here so I'm not sure if that's what we're experiencing right now I'm going to go ahead and delete this one here delete loog group and I'm just going to rename it to like two or something so we'll go down to this I'll try that again and we'll go back over to here refresh no log data okay let me figure this out all right so I went in here and it's passing the log events and there's no error I added more error handling here and it's not showing any problems it's telling me that it's going to Cloud watch logs it's going to cloudwatch logs where is it then I don't see it here it's like it's lying to me um and so I'm not exactly sure what's going on one thing we could do is we could try to generate out fewer logs or just cut down our log file so let's go here and let's just cut it down to like 50 because we know there's a limit to how much we can send and maybe it's just too large okay but it should have raised an error told us something right so try this again and we know that this works almost instantaneously here it's nothing okay let's cut it down to like literally two lines and if you're thinking that it doesn't work I'm going to go ahead and show you this going do binding pry here um my other thought is here is like that's putting out the events which is not very useful it logs it goes down here it definitely calls this function and maybe I'll put a binding pry after here and just see what happens because maybe it is airing out but it's not throwing an error okay so I'm going to just show you I'm going to type in log events Q has like Vim Vim keys there so log events you can see we have message and time stamp those look correct to me we type exit to go to the next stage and we're going to write out the response uh I have to hit Q There R response so it seems like it should work but maybe um I don't know it's kind of weird oh we have more than one log oh you know what maybe we're looking at an old one no not that one so I'm getting kind of confused here I'm going to delete these here just so we have one okay I'm going to take all these bunny prize again maybe we had that one failed and we were just refreshing on it constantly give a refresh here we'll look into here what what is going on like what if we change this to be like we only have two logs in here so there's not a whole lot that could go wrong here um so but why does it not show anything clear all time you have to maybe you have to do like maybe it's outside of the reach that we're in absolute what if I go all the way back to here like all all I can think of is like maybe the dates messed up up and it can't see it apply let's try this how about this uh maybe I have to choose something that's actually today what's the day is the May 30th May 30th here to here nothing so this is confusing right like all right let me figure it out okay you know what um comparing our old one it seems like we uh this is still in seconds right so this might be the wrong format even though it's saying successful it's probably rejecting it because the time format's not right this is just kind of like a blind way of me trying to figure it out uh because this one here will do this and then it will do this and we still haven't fixed our old time here which is uh which sucks okay and so we'll go here um and then we'll put the time stamp in here but I'm surprised it doesn't like throw an uh throw an error or something because I again I really think that's our problem but we'll go ahead and try this again so I'm going to go ahead and type in x exit exit here whoops exit I'm going to go over to cloudwatch I'm going to go back a layer here I'm going to get rid of these two streams we'll delete them we'll go back over to our code here we'll type in clear and I'm going to try this again okay um I'm going to type in q log events and so now it's showing zero well that's no good for our time stamp that's not going to work so there's clearly a problem here so I'm going to go binding pry here and we we toi it twice we don't need to do it twice oh you know what it's not a sign like this that's the problem okay so I'm going to go ahead and type exit here clear we'll try this again and we'll go ahead and type in see I don't want to I don't have to debug this right now but I guess we could just write time stamp here and see what we have time stamp looks better it looks longer it looks like what it's supposed to be I don't think we need the binding pry down here right now so I'm going to just take that out and we'll type in clear oops clear and I'll go ahead and write that and so supposedly we've written our logs which is here or sorry over here over here this is what we're looking for and it's showing stuff in there so now it must be writing the data so there we go so that's record that we want excellent so now we can go back and regenerate out our proper log so I'm going to go delete this file first delete this and we'll go to our rake file here and I'll change this back to app we don't need it to be app 2 and I'm going to go ahead and clear this out and I'm going to generate this out again I'm going to go back over to here and I'm just going to delete this log group here okay and we'll type in clear and we want to rake our logs and then from here we'll give this a refresh and we'll go into here and so I don't know why we have two in here because we hadn't I don't think I ran it twice it doesn't want to delete it's kind of acting weird that's okay and we'll go to here and so now we have it in adjacent structure and the whole point of this was to show you how you could filter based on these let's go down here and take a look so if we go into this property equality operator string okay so that's what it's showing us um let looks like uh we I know I must have documented this in another area so um it must be covered in the the lecture cont CU remember doing this but this I thought it's a special object is the dollar sign but we can try it here I don't think that's what we're supposed to do but we'll try it here anyway and I'll go back over to here and if we have dollar sign time stamp or maybe like in the message if we were to drop this down and say IP equals okay I'm just guessing I'm not sure why it's doing weird things as I write 171 uh 250. 72 that and hit enter will that filter it no okay let me read this for a second okay all right so I just carefully read through this and it just says dollar sign the thing that you want equals and then whatever so we did do that we go back over to here I guess the question is did it actually interpret it as Json that's the first question the way we know is if we expand it it does this right if it expands like that then we know that it understands what we want I don't know if we need to put curlies here but I'm going just go ahead and say IP um grab this here say IP equals this does that work no does this work no does this work there we go okay so that's the format that it wants okay so if we wanted to get something like method put we could do like method here and then put okay and so now we're getting the filtration that we want so that's what I wanted to show for you so we're basically done here the only thing that I didn't look up was like can we filter a CLI uh commands uh like for the logs this way I also want to fix our old script because I don't want to leave you with a broken script so if we go here is there like a get describe filter log events List log events from a specific log group and then maybe you can filter them so they don't have an example here but it has filter expression so I'm going to ask jgpt for this so um AWS you really got to fill out your examples filter log event um it CLI example with a filter expression I don't think it's that hard to do but I just don't feel like writing it out all by hand here and grabbing all the stuff there we go okay great so what we'll do is grab this here and I'm going to go down here uh filter event data and so here we could say method we'll say put and then this log group is I guess it go across all the streams right because we're not specifying a particular stream in there maybe we can because I could have swore there was that option right stream names so let's not trust chat GPT to get this right but we'll do this for the uh start time we can probably figure it out by going to our log here and in our time stamps I kind of wish that we had done that uh the beginning part here here ooh we don't have what those time stamps look like that's kind of frustrating um but we can go over to here right no we can't because we have to take this time stab and convert this over so what I'll have to do is make a tiny tiny tiny tiny little script here like super tiny and this will just be we'll go into here and we'll just grab um these two lines okay so I'm going to just grab this here and I'm just going to say I don't even want to make a script for this I just want to I mean I do I just don't want to make a file for the script so go ahead and say copy and we'll say paste here I'll say task range do and then we'll just say file read or is it like read lines say read lines I think we can do that read lines and then we'll provide this here log file path this will read all the lines and then I want to say line first lines last require Json here and then we'll go down here and then we'll say json.parse so be Json first Json last and then we'll bring this here for a second this will be Json first and we'll say 2 I * 1000 so will be uh time stamp first and then we'll grab this this will be the last and then hopefully that just works it'll say puts time stamp first just want to do match this a bit here this is start time so let's say start time just so I don't get them mixed up end time and then hopefully that just works first go that'd be really nice so I'll go ahead and do task uh or sorry um bundle exact rake range and so we now we have those here so I'll just copy these here and go ahead and paste this in of course you will not have the same times as me so you have to run this for it to work okay can't just take my code and and hope that it's going to work perfectly and so we'll go here and then I'll just put this around here I just noticed this is called Rd not MD we'll fix that right now there we go this one doesn't need that on the end and we'll go ahead and copy this and we'll try to see if this works and there we go so we're getting uh filter data back it's not easy to work with because it's in Json format but at least we're seeing it we could uh probably filter this out so if we did and we said uh let's say query events Square braces and then we'll just say message and then um I mean this would be a little bit nicer so we'll go ahead and copy this like this like that uh so this query must not be correct oh it's lowercase events okay lower case events like that okay so we'll just try that again clear hit enter okay and so not the nicest looking thing I I'm not sure if text would look better so say text on this output text then we'll just paste that again copy paste it's not letting me paste I'll go to another tab I guess yeah so I guess I mean it is Json so that kind of makes sense but it's uh not the not the nicest display um but anyway uh I just want to go back and fix this script really quickly because uh we might use this again in the future or people that are encountering it I want to actually not run into problems so what this is supposed to do it's supposed to parse out like it's going through each line and it's supposed to then um extract the line from it which I'm really surprised that it didn't do that now we did use chat GPT to generate this out so let's see if we can find it and so in here it was using the dollar sign one because this regular expression is selecting it so what it's doing here see the square braces if we go over to here uh the this time is encapsulated in the Square so it's saying grab everything between the square and that's the date and then we just put dollar sign one here so that might just be a really easy fix but um this might not work if it's not in the correct order so I'm going to go back into uh the logs basic here well I refreshed my screen or something so it took me or I'm in another tab that's why so I'll just go this tab does not work so we'll just ignore it so we'll go back into cloudwatch Cloud watch this is up we'll give this a refresh here it's acting really funny when things act funny just give them a refresh and so we'll wait for this to open here there we go and we'll go into basic here or logs basic and I'm going to go ahead and run this rake file so this will be bundle exec rake log um to generate out the log so go ahead and delete this well this one's not the problem right the dates are fine here it's it's just this put logs so actually that's all we have to test we don't have to regenerate anything out so um I'm just going to go to my rake file this one's called uh let me just make sure this is basic again I'll do bundle exec because I'm expecting to say like it has to be in the correct order yeah so I think that yeah we have that issue so we'll just have to go back to this one here our other one and then just grab our um it's in the generation that's why we wanted to regenerate this out so this uh this time stamp method is no good so we'll go back to this one here and we'll go to here and we'll just copy this one here copy and then we'll go back to this one here that time stamp here okay so that should fix our issue here all right and um so we'll have to regenerate this one out and it's I'm missing one piece of code I we forgot to bring over this line here okay so that line is now generated and so the idea is that we need to get those logs uh up there so we'll go ahead and do our bundle ex R rake log and so now we have less of an issue we'll go over to this here and we click into here no no that's not it this one here this one and boom okay so now we actually have real working EX examples that are proper sometimes that happens you know you guys just got to watch out for stuff here I won't always remember to tell you to delete everything so just be thorough and make sure you not causing any spending your account um and they weren't even setting the expire on those we'd have to adjust our scripts to do that just say uh update uh or this will just be more Cloud watch log examples I didn't expect this video to be so long but you know this just the work that that we put in here but you know if we need to do more examples in cloudwatch this is going to be great I'll see you in the next one okay ciao all right so we're on to Cloud metric and this represents a Time ordered set of data points and it's a variable that is monitored over time so cloudwatch comes with many predefined metrics that are generally namespaced by adus service so uh specifically for an ec2 instance we already have some metrics like CPU utilization disk read Ops disk write Ops disk read bytes disk write bytes Network in network out Network packets in um and there could be even more but let's just take a look here and network packets out but let's just take a look at where this would go so if you were to look in your metrics you could choose based on the category and so that's where we went to ec2 we would choose Network in there and then we could see that information over time for network in on a particular resource so there you go hey this is Andrew Brown this video we're going to take a look at metric filters so metric filters allows us to create a filter um on top of our cloudwatch log and we saw that button earlier but we just never pressed it I don't even think I I I I called out to it so what we'll do is we'll open up our ad examples repo um so I'm going to open this up in GitHub as I normally do again get get pod sorry get pod use whatever you want code spaces whatever but understand that you'll have to configure your environment I'm going to get logged in here into AWS which apparently I'm not logged in so just give me a moment all right so I'm logged in here and let's make our way over to cloudwatch uh and so we'll have to generate out some logs um we we're lucky that we fixed our basic script from before uh if you're for that lab there um so we're going to go and do CD into uh cloudwatch um cloudwatch where is it cloudwatch logs basic here and we need to bring that data there so if we want the latest data we should generate out every single time so I'm just going to remove the old web log file here and we're going to run our python script that will generate out our elf logs so we'll go down below to our read me here I'm going to go ahead and copy this I'm going to go ahead and paste this in say allow and hit enter so now I have an uptodate uh web server logs and we'll go to our rake file that uh is the one that is responsible for pushing it that one looks fine so let's go ahead and uh run that so we'll say bundle exec we'll do bundle install first because um we'll have to do that for Ruby and then we'll do bundle bundle exact uh rake log and so that's going to run that task and so now if we make our way over to our logs in our log group we should see somewhere in here that so I'll type in for SL examples and um we'll go into this one and they have a button here that says I think we have to click into it and we don't see yet but if we were to filter something out here so we learned the other day that no it did just work when we did login but let's wanted to filter login and home I think it was like percentage login pipe home percentage enter and so now it's filtering for those two things and so we could create a metric filter and we'll just call it uh home or you know we could just do login to make it a bit easier we'll just do login here and notice that some are delete some are put some are get so maybe what we' want to do is just make sure it's consistent and oh we'll be fine we'll be for all of them it's totally fine we'll hit create metric because I was thinking our our data is random so it doesn't logically make any sense right there wouldn't be a delete put and get on login but that's fine so it just be login attempts as our filter filter name and then below we have metric name to identify this thing I'm going to see if I can name it the same thing metric value that is published to the metric name when the filter match occur so valid metric value are floating points numbers are stuff so what what do we want to here we'll have about one for each one that's there it we'll get a value one the default value will be zero um and then this is the unit and so we're going to go down with count because it's just counting stuff actually that's we just want regular count right so that should be sufficient oh we need a namespace and so our namespace here we'll call this um cloudwatch app okay we'll go ahead and create that and so now we have ourselves a metric okay so if we go back to all metrics here right I imagine that it would show up somewhere in here because that's what a metric is so where would our metric show up let me go find it okay all right so I still can't find it in the metrics I it could be that it might take time for data to come in and then for it of us to detect it but I was expecting when we go to metrics here that uh there's a lot of ways that we can look at our metrics but if we were to um this says cloudwatch metrics Insight so that's the data source here but I was expecting that if we will go here we should see a nam space uh for our own or be able to search and say login because that's what we called it and find it here so it maybe it doesn't have data yet there's many reasons as to why this could be happening um but at the very least what we can do is try to set up an alarm because if we go to um our metric here all right if we go over to here um we do have the filter here and we can apply an alarm so let's go ahead and create an alarm because maybe we can get it to trigger um and so that's what I want to find out so right now we'll leave it as static and we want a number here so we'll say when it goes over 20 right and we want to sum it up within a f minute period so I'm going to go ahead and hit next and alarm state so the metric or expression is outside the threshold then it's an alarm yep okay and we can cause a notification I don't really care I just want to see that it gets triggered so I got to ignore all these other options down below here oh it's making a select one here uh I thought we could just skip this here create a new topic sure fine um I don't know why I thought we like if we had another action we could just oh you know what maybe we can just remove this one there we go okay next yeah sorry I'm just used to doing it pro programmatically and I guess we could have done it programmatically but uh whatever so we'll call this login attempts exceed uh I can't remember we said 20 we'll say next and so here we have it we'll go ahead and create this alarm and so we should be able to monitor our alarms here it is right it says insufficient data which is totally fine but let's go ahead and import more data so what I'm going to do I'm going to modify our script because I want to have way more data than we have here and I'm going to change our script so that instead of 100 we can specify I think there's like 100 in here somewhere in here it says how much it's generating out oh sorry it's in our python file here so in here there's the number 100 yeah and I want to be able to say how much I want to generate out right so I'm going to go to the top here and I'm going just say uh number of entries and say 500 right and then I go down below here and we'll say five oh well number of entries while we're here I'm going to go update my other one for my events because it seems like a good idea to do that I would like to bring in bring this in as a parameter um I would really pre prefer that so we'll just look up um what is it called Nars python so I don't necessarily remember thank you free Camp fre Camp's always very helpful for this stuff and so that's probably something we're looking for here so go to here and this is going to be number of entries I don't know if we can do this in Python I'm going to try it anyway that's like a python uh or that's a ruby syntax going to say um python um or operator single line assignment variable it's a ruby thing I just I have a feeling that you can probably do it uh let's see here assign if blank if value not assigned python yeah I'm GNA ask chat GPT I just wanted to know sorry just I'm a programmer I like to think so I was like uh using Ruby I can do this hello equals uh bar like uh string value or hello world how can I do something similar in Python that's the great thing about like if you know Ruby really well you can translate it over to other languages okay so we're just using an or command all right so pretty close again I'm not sure if that's true they know I just faked it and didn't actually think about the language but let's try here or yep okay there we go um and so I'm going to set this back to a more realistic number like 100 and so the idea is that we'll want to supply that when we call this so what I'll do is go over to our readme file here and I'm going to adjust this so that it has uh this I'm going to say I'll call this like n entries so we can say like how many we want and then the idea is we'll just say you know like 200 or whatever I'm going to put 10 here like a l number here just for the moment and I'm going to go ahead and try this out so I'm going to go ahead and delete this delete permit copy this and paste this into enter oh we have to require it that's fine require or import OS we'll try this again and it still has a problem here and entries and entries I want to make sure that it's working so I'm going to go ahead and say just print this here I just want to make sure that it is pulling in that environment variable okay try that again and I'm not sure if it's working at all so print again hello world try this again here no it's a syntax here so it's just not going to run at all so we'll go down to here first it says number of entries I mean that should be fine right number of entries number of entries what's the problem string cannot be interpreted as an integer okay so this here is probably because it's thinking that's what it is so uh string to integer python because I don't know what it is int okay great so we'll go over to here and we'll wrap this in an INT I mean there's still issues that things could crop up here but this should be sufficient enough there we go and we will try this again and so I'm hoping that it does 10 I'm not sure what happen if this return zero because if this was blank it might still uh trigger so this code might still suck but I don't care so I have 10 here now my question is if I change this will it just overwrite the file every time I don't have to delete it yeah it does if I go ahead and try this again we'll do one okay that's good so what I'm thinking here now is that I want to um keep generating out data so we'll go back over to here this one is fine yeah I don't think we're going to be worried about that Jason one we'll leave it alone so what I'm going to do here is I'm going to go ahead and uh we'll do 100 and then after that what I want to do is I want to do um bundle exact rake log so we'll generate 100 logs and then we'll send them so I'm going to hit enter UND do that great I'm going to wait a few seconds here just hanging out hit do that again okay I'm G to wait a little bit do that again I'll wait a little bit and I'll do that again okay so what I'm trying to do is just populate enough data okay and we're doing that because we need enough data so that we trigger that but we might still have to wait a little bit so I'm going to go back over to here I'm going to go to our alarms oh it says we're alarms did it work this is Target tracking table low events uh this is for a Dynamo DB table do I have a Dynamo DB table I forgot to delete sometimes when you're alarms you might see this stuff and you go here no so sometimes Dynamo DB doesn't delete the alarms and it's really annoying so I'll just get rid of these because I don't want to pay for these but what I'm looking for we'll go back to all alarms is that we're waiting for this to become sufficient so we'll have to wait here for a bit so I'm going to wait here and we'll be back in a bit okay all right so it's been uh easily over five minutes I believe I'm just going to check my time here it's been so it's 2114 and this is 49 so about 15 minutes so it should have had enough time um for something to happen here and so what I'm think is happening here is that it's just going to take time for this to propagate because it's not showing up anywhere right and oh now it's here okay so clearly there's a bit of a delay right but we'll click into here and so now we have our metrics with no Dimensions all right and we can click into this and I don't uh I mean at least there's data we know that it's it's been detected but I'm not seeing anything in the graph and so this is the last three hours and I'm thinking that it just takes time for data to show up so I think what we should do is we should go back over to here and we should give this a bit more data so I'm going to go ahead and hit enter and keep doing this I'm just going to confirm that we are sending data over to it so we'll go back over to our logs here and I'm in systems manager I'm not sure why I must have opened it by accident and we'll go back over to our logs and we'll go to examples here and we'll open this and so I believe that we're creating more I'm going to go ahead and try this again okay I'm refresh this one two three four five six seven eight nine does it tell us how many we have selected no uh but we'll just keep doing this a little bit more because I think what's going to happen is that it is going to eventually work but I think there's just a delay at least with these user metrics because people are saying online you might have to wait days um and I suppose I could come back here uh to this video days later and see what happens but uh right now I'm just going to run this for a bit okay and I'm going to go back over to our metri and notice we still don't have anything in here uh one thing we can do to test this again it's just because I can't tell uh until the data comes in and so it's like am I doing it wrong or is the or is my data wrong and that's always that's always a concern but what we can try to do here is if we go over to I believe milter uh metric filter and we click into it we can do a test pattern here so I can go ahead and grab my log data here so we just grab there's a lot of login so I'm sure we're triggering this uh and then I'll just go ahead and paste this in as such and we test the pattern it shows that it's counting events so it clearly is is getting numbers and so that is not our issue here so I think we're just going to have to patiently wait and I have no idea how long it's going to take I personally am probably going to wait hours and check uh a few hours from now and I'm going to just go ahead and do other labs but we're going to assume that we know how to create a metric filter that we can use in a metrics if I don't make another video then maybe that just means that I never was able to see the data but I will try to come back here and then Stitch the videos together so that we can uh see the result okay so I'll see you uh if this works okay all right so it's been a few days uh and I want to take a look at our uh metric filters because I wanted to see if uh it will show up somewhere if we go back to our alarms let take a look here uh and we go to all of our alarms arms and right now I'm in North Virginia I think we were in C Center one last time so let's take a look here it's been multiple day so I'm a little bit confused as to where I was last and so it still says insufficient data so I'm not exactly sure what's going on with this uh this is three hours ago so we could go one week okay there we go so we were getting information and so it looks like uh this does actually work did we ever get close to our alarm no we did not but at least it did track that information which was really important let's go over to our metrics and let's go into here uh metric with no dimensions and we will checkbox this and we'll go back to one week and so we have those records so for whatever reason it takes time for that to show up when is a realistic time to uh see that information I don't really know um but at the very least uh we can see that information so I go here doesn't show it with the bars probably be better if we kind of uh zoomed in on that stuff a bit better but anyway obviously there was a uh data coming in and so we'll consider that as uh successful okay so um we can keep the alarm around or delete it I'll probably go ahead and delete the alarms because alarms do cost money if we keep in the round so we'll go ahead I'll just go and delete this okay and if you want you can clean up those log groups I don't really care in particular so I'm just going to keep them around but if you want you can delete the log groups and uh um or make sure that you set your data to expire so just remember that wherever that was I can't remember was basic app or not did we set it up in this one again it's been like a multiple days so I don't remember if we go to metric filter so we have it here yeah I guess I'll just clean it up might as well do that let's delete this one out um and I guess I can go ahead and delete these logs as well there we go so I guess I am cleaning up but I'll see you in the next one okay ciao all right so with cloudwatch uh for different Services it will emit data uh to Cloud watch and the availability of the data is going to vary based on the service okay and so just looking at an example here with ec2 and other services so for ec2 um if you spin up an ec2 instance it's going to uh it's going to uh make the data available to you every 5 minutes and if you put detailed monitor on it's going to be 1 minute but for all the other services uh it's going to be generally 1 minute but sometimes it can be three or five but I really want to point this out because it's really really important on the exams that you understand that ec2 is 5 minutes uh and then with detailed monitor it's one and all their services are one by default so it's just kind of one of those little um weird gchas so I just really wanted to point that out to you so we keep on talking about the cloudwatch agent but how do we actually install it onto our ec2 instances so to install it we can just use adus simples systems manager also just now called systems manager or abbreviated to SSM and in there they have a service called run command uh and this what you can do is you press a a button that says run command and then they have all these packages and the one you're looking for is called adus configure adus package so what does this package do it installs or uninstalls a distribut a distributor package so you can install the latest version default version etc etc packages provided by adus such as the cloud watch agent and some other things are there so this is the one that you use it's confusing because if you go into um systems manager run command there's actually one named cloudwatch agent but that's actually just for restarting uh the agent but this one's the one that is used for installing the actual agent okay and so what you'll do is you'll choose it you'll have some parameters so you'll have to put install and then in the name you'll have to put Amazon cloudwatch agent because this package could be used to install a variety of different AWS packages and you'll want to use the latest one and then you need to just choose what ec2 instance you want to install it on so you can manually choose it or if it has a tag or if it's in a resource Group uh and one other thing I have to point out is that you must attach the cloudwatch agent Service uh server Ro um as an IMR uh to your ec2 instance profile so that when the agent's running it can actually go and uh report back to cloudwatch okay so there you go so now let's take a look at cloudwatch agent and the host level metrics so uh some metrics you might think you are tracking by default for your E2 instance are not and requires you to install the cloud watch agent okay uh and this caught me off guard because when I spended up an E2 server uh and then I was running out of storage space I thought I would have that data but I did not have that available to me and I'll show you what I mean so with host level metrics and this is what you get by default these are uh if you don't install the agent at all so um cloudwatch uh cloudwatch metric you're going to get is CPU usage Network usage disk usage it it'll do status checks okay so the underlying hypervisor status and the underlying ec2 instance status so you know you see those two checks for an ec2 instance that's what we're talking about but agent level metrics this is when you actually install the cloudwatch agent you're now going to get memory utilization disk swap utilization disk space utilization page file utilization uh and log collection okay um and so the cloudwatch agent is able to collect a various amount of long so if you had applications logs and uh other things that you installed on your server you need the cloudwatch agent to do it but I just really want to emphasize uh on the agent levels the memory utilization and dis space so like if you're wondering if you're running out of memory or you're running out of disc space you need the cloudwatch agent okay so there you go so now let's take a look at custom metrics and high resolution metrics so you can publish your own custom metric using the a CLI or SDK so you do ads cloudwatch put hyphen metric hyphen data uh you provide the the metric name the name space the unit the value and some Dimensions which are basically variables you're passing along if you're doing the CIS Ops the devops you definitely want to know how to do this um for high resolution metrics uh you can only switch to high resolution metrics when you are publishing a custom metric uh so the standard resolution is 1 minute but if you want it to report even more frequently so under a minute uh down to a second you can use use high resolution with high resolution you can track in intervals of 1 second uh 5 Seconds 10 seconds 30 seconds or multiples of 60 seconds okay so there you go so we just said that the cloudwatch agent can collect logs and let's look at that in a bit more detail here so the cloudwatch agent can send logs running ec2 instance to a cloudwatch log group uh and to send logs what you're going to have to do is configure the agent um to include the logs that you want to send over the cloudwatch agent service needs to be restarted the agent's configuration file is located here so if you're looking for where it is uh that's where it's going to be located and what you'll do is you're going to go ahead and edit it so you can specify the location of the log file uh and the log group that you want it to be sent to and this is how you restart it so that's how you do log collection you you'll probably want to do that on your server hey this is Andre Brown in this video I want to take a look at installing the cloudwatch agent so the cloudwatch agent is is what is utilized in order to install uh or uh to collect logs on a server and it's the preferable way to do it so obviously we've been writing our own script um in previous labs to send data but that's not really what You' want to do if you have a web server that is outputting data you would rather send those logs uh there so um I guess that's what we're going to do here so I'm thinking that we probably have to build a very simple app that does logging and we'll want to deploy it so I'm trying to think of something that I know how to deploy that is very easy for me to do um and um I want to use rails and the reason I want to use Ruby on Rails is because it has a very distinct way of logging and that's going to be very useful for us so it will take a little bit of time not maybe not that much time but it will take us a little bit time to get set up here I'm going to make a new one folder here called agent and um we're going to start building something out here so we want to have an app so I'm just thinking about this so I want to build a rails app I don't want to build a big deployment pipeline I just want to build the app get it on the server and then um have the agent installed and then uh hit end points to the server okay so what we'll do is we'll CD into our agent directory here all right uh go up a layer here agent agent logs agent there we go and I'm going to generate a new rails app so again you have to have Ruby installed for this and if you're on uh um uh G pod or code spaces this makes it a lot easier if you're on your local machine you're going to have a hard time following I'm just going to be honest with you um but uh you can also just grab the code from here so if you can't code it here and you don't want to code it you could just grab the code and put it on the server but it shouldn't take us too long to to build an app here so to use Ruby we need to have rails installed so I'm going to make a new file here called readme.md and we're going to keep make the simplest app ever so I'm going to go ahead and just say uh install rails so we'll have to do gem install rails and there's a couple things I want to do I want to do rails API only um SQL light like like SQL light database or no database can we do it with no database here we'll do SQL light database we'll do SQL light database no we'll do no no database sorry no database again really picky here right um so here yeah we can skip active record so that's the first thing I want to do and I I want this to be API only so I'm just going to say rails new configuration Flags okay we're just going to tear out a lot of stuff in here so that we don't have to worry about configuration so there should be a bunch of options here I'm just trying to find them so like hyphen hyphen skip rails new flag command line here we go and so in this one it's going to tell us so if we go rails new this is what we're looking for and rail shouldn't scare you because it's the it's uh most Frameworks are based off of it but we have skip get and actually we do want to skip G in this case because we're just going to do this locally okay so I don't want to get uh a get repo here uh skip Docker file I mean I don't care if it creates a Docker file if it does that's kind of cool but um I I want to I don't want to install it in a container on the server because I want uh to it to leverage the cloudwatch agent which is on the virtual machine I want to do API only so there should be something for API in here API and so I'm just looking for it here skip I don't want jbuilder I don't want hot wire there's like a rail there's like an API only mode let me see rails API only here yeah it's weird that they don't show that flag in here when that's such an important flag and so that's going to leave out a bunch of the stuff like jbuilder and stuff like that because that's usually the pain uh the pain the butt Part to configure and so I think we just need those few Flags so we don't have a database which is fine we don't have git which is fine and we don't have a front end we just want to have it API and so that's should generate set out app we need to name our app um I'm trying to think of an okay app name to to do because I'm just trying to think of like the nature of this app and this one will be for I don't know uh Shogun sorry I been watching the TV show Shogun so I don't know I don't know what else to name this here okay so good show by the way it's a h FX Max show really good Nine episodes on four fourway in there if you got Disney plus you can watch it on there you got to change your parental controls it's not a show for kids but if you want a really good show I like it anyway so that will be what we do here and hopefully this works okay so that's going to build out a rail app and so we don't have to worry about a front and looks like there's a lot of stuff in here we do have a Docker file but we don't plan to use it but it's cool that they give us one off the bat um so I didn't know that they had that that's uh used to have to make those uh back in the day but that's really cool that they have that and I want to see what it's installing so even though we we're not using a database it's installing SQL light uh it has boot snap in here so the only thing I don't want in here is this one here because we're not going to use active record okay so I'm going to go ahead and CD into that directory so we'll say Shogun and it's trying to change the default Ruby version which is fineable to bundle install here okay and this one's very particular about what it wants I'm actually going to comment out the Ruby version because if we have the not exact version of Ruby it's going to ask us to install it and so I just want to get a version that might work on um ec2 and so ec2 Amazon Linux uh we'll probably have a Ruby version that is easy to install it's probably a version three and so as long as it's three compatible that should be sufficient okay so I'm doing a bundle install here I'm not sure why it's taking so darn long um because it seems like it did it again but I'm going to do bundle install again so that it knows in the um the gem lock file that we're not locking to a specific version of Ruby okay and so as long as we're on version three we should be fine so Puma is the server rails is this thing over here we have uh uh TZ info data boot snap and debug and this is fine um and so what I'm going to do is I'm going to go ahead and uh test our application so we have our app but we need to create an endpoint so if we go into controllers here we have this application controller and we could create under control other controllers here but I feel like we could probably achieve most of the things we want in here so I'm just trying to think about this for a second because I don't need anything complicated I'm just going to go ahead and say um I'm going to go ahead and let's create some end points so I'm trying to think I G to go back like Shogun characters I don't know I called it Shogun so I'm going to go ahead and ask gbt I am uh creating an example rails app I want to create end uh end points uh in the application controller based on uh uh character names from the TV show Shogun the fs FX FX show please give me a uh file okay let's see if it can do that for me because if we call them it will show that we made an htb call to that and so that should be sufficient for it so I'm going to give it a moment to generate out because I've asked something really bizarre and it's getting a bit confused I don't need that I just want the that is not what I told it to do okay you shut up shut up shut up shut up okay give me a list of characters from the TV show shun FX just a plain bull plain list okay it totally did not want it okay there we go so now we have some names I got to go back over to here I'm going to paste in our names here and I'm going to go down here and change these all to so these are now functions I'm going to lowercase all this because they have to be gu to go lowercase in Vim underscore underscore underscore underscore underscore there we go and so we have these end points all right so that is good and and I'm going to split this here and there's a routes file here and this is so you know where the routes are so we'll go config routes. RB here it is and I'm going to make some routes so I'm going to copy these again this deaf here is doing nothing and this indentation is wrong again we won't take very long to build this app we're almost done basically and I'm going to go ahead and just take out uh this because I need them all back as a single line so I should have copied them earlier but I didn't and the idea is that these are going to be um uh commands these are now end points that we can hit and what I want to do here is just say I want to point this to our application and then it'll be the name of it this just like the Shand how it works so I'm going to go ahead and copy this like this okay and uh this will be get this will be put this will be post this will be delete this will be well we won't do uh post there we'll just say post we'll say get get put post get get post I'm just giving it some variety here okay and then over here we have something we could have a response I suppose and I'm just going to make the response uh so we'll say render Json and I'm just going to give back its name so just say name this will be John Blackthorne okay this will be Lord toranaga really good show by the way really good show and this will be Maro this one will be father Alvito this one will be lady fujo this one will be Amy I actually don't know which character Amy is we have aido ishido ishido sorry ishido for those who know how to speak Japanese I apologize we'll have yabo bonaro and and Kiku okay so these are all responding back with Json okay and so I want to test this app here so this is enough for this app to work okay and to run it we'll do a rails you should do bundle EXA in front of everything so say bundle EXA rails s and that should start up the application I'm going to go back to our read me here our not the one in the rails app but the one that is um this one here okay so here it's like start app so we'll just CD into Shogun we'll say ra bundle exec rails s all right and so the idea is that if we hit those end points I'll make a new tab here this will uh call endpoints so this runs on Local Host so if we do curl Local Host uh 3000 for SL uh Maro uh something should happen here we got an error here saying SQL light adapter is missing a gem so it's complaining about that but I don't want to have any kind of adapter so I'm going to go here because I'm trying to do this without any dependencies so we don't have a really hard time here and I'm going to go into our database. yaml file this is what it's complaining about because this is configured for this so I'm going to see if I can just comment this out I'm not sure if you're allowed to do that but I'm going to try this anyway and I'm just going to say no database because we don't even have active records so there's no database connection anyway so I'm doing contrl C to kill the server I got to go back to this tab contrl C we're hitting up okay I'm going to go back here and hopefully this doesn't complain and it still complains Puma caught this error database is not configured development error so I'm going to ask this how can I configure this I don't want a database how can I exclude in rails CU then we have to figure out how to install es light it's not that hard to do but um just remove it maybe we just need to remove it okay so let's go ahead and see if we can get just get rid of this here delete it and I'll try this again I didn't think that that would happen but it's not the normal way to run rails with database but we just want again afford the outputs okay so yeah you're full of baloney okay so remove or comment out the database we did that modify the application RB and Skip active record it shouldn't be loading active record so if we go down to uh active record here or sorry application.rb which uh is under our config I believe and then here yeah so it doesn't know what it's talking about I mean it's we do have rails all so what it's saying is take out the rails all and then just put these in particulars fine I guess we'll just have SQL light it's not that big of a deal so I'll go back here and just say rails database default uh SQL light example because we deleted it out so now I don't have it right stupid stupid thing um yeah that's the default one so I'll go back here to I'll make the database yel again let say database. yaml and I guess that's back now even though I don't want it and we'll go back over to our rake file here sorry gem file and then we'll bring back SQL light I think that's like one thing I don't like about the framework is like that part there but that's not a big deal I don't think it's that hard to install light on Amazon 2023 so now we have it installed and so now it should not complain anymore so we'll go ahead and do this even though we're not using a database and we're going to go use Maro so notice right away it's still not found so it's saying that this endpoint doesn't exist if we go back to our routes uh marioo a post and that's why it's not working right so for that to work we'd have to do I think hyphen X post is that is that it yeah now it's giving us stuff back okay so that's working but what we really want what's is valuable is the logging right now this is not how we would log in production so let's go ahead and try to run this in production to do that we do rails EnV equals production and so it will complain because we probably didn't generate out something yeah and so it wants the um the production database so I go ahead and just do this and say production and then we'll say production here like that and then we'll try this again so this is starting up and so let's go try that that event again like this uh no specified URL does it start on 3,000 in this one yeah it still does oh it doesn't have the post in it hold on here okay and so if we go back the logging should look different so the logs will show up here in the production. log over here uh we go to logs and we should see a a production log so I'll go back here and try this again go back here and we're not seeing a production log so I'm not sure why it's not logging uh no production log showing for rails okay rails 3 is really old but we'll just take a look here I don't think we have to change that but we can try it the great thing about learning how to do this rails is it applies the lell a lot of Frameworks that kind of match it so in here there's like a log level uh info generic information about system operations that should not be what we want I'll go back here logging to standard out rails production log not working when so the thing is like normally we run it through Puma but I'm going to go back here for a second so I don't see I don't see uh ra uh production log for rails when running rails EnV production bundle exec rails s because when you log in production you don't log the same way in development and so it's going to look a little bit different um I don't think we have to do debug we could try touching the file to help it out here that's what I was thinking but I don't think that's our problem I have to CD into the correct directory so this is cloudwatch logs okay and then we go into agent and I'm G to go ahead and we'll try this again oh sorry Shogun and we'll touch this file and so we'll try to run this again because if we don't have logging that's not going to be very useful for us so I'm going to go back up to here and I'm just going to make sure we grab this command so I don't keep finding it here just say uh test endpoint okay and so if I go back to here oh notice we're not getting any data back so that's kind of interesting H all right let's stop this for a second let's go back to development well we'll just take this out to go back to development and we'll try this again so it's a local host and we get data back okay so we stop this we set it to production production mode it's starting on 0000 it's binding to zero we go back to here maybe it's because I'm not I'm not doing 00 Z okay so let's try this I would think Local Host would work for that we'll try it anyway and yeah it's not I don't think it's receiving anything so let's go back over to here that's not our problem so uh when running rails in production mode when I send a curl to 000000 3000 I get no response and the server does not log anything in production log so there's clearly something that we're missing here let give it a moment here yeah server is running and it's already binding to Port zero which is fine and we don't need to chod it it already has sufficient permissions yeah and so we could try to run it uh with another server like web brick or puma and that's probably a good idea so how to start rails with Puma so I'll take a look here I'll go back here and just ask chat to PT I don't want to be spending all day trying to figure it out so we'll go here and just say um how do I start up rails in production for Puma um you think I'd know because I I run rails apps but I I kind of forget I don't want to do all that stuff it's this command here this is what we want okay so I'm going to grab this command just give me this Comm oh my goodness what a pain in the butt there we go great okay so we'll go back over to here I'm just going to see if this resolves our issue here so like for uh rails you have Puma thin web brick the default one and so these are uh web servers and technically when this runs it actually is running on web brick or something but um we're going to go ahead and tell it to execute the Puma file there should already be a puma file configured so if we go into here there is right and so this has information about production and has concurrency and yada y y so that's kind of interesting but I'm going to go back over to here uh to this line and we'll go ahead and try to start it up using puma and the other one this might also be Puma I'm not sure it is booty and Puma okay so now we're just explicitly saying use puma in production which is probably the correct way to do it's telling us not to use SQL light which is fine and we'll try to curl again and it's still not freaking working so give me a second I'll figure it out okay all right so one thing I checked was I went to 31 and I used a get get one here and it works here so it's making me think that it's a security issue right and it's rejecting things that are not coming from uh the same address and so that's what I'm thinking that it is it's probably a security feature and so if I go into application.rb there's probably something in here maybe production that is going to prevent um local requests so that's probably the problem we're having so I'm going to see what I can figure out for it okay yeah it's our config do host but I was looking for it to see if it was there in particular so if we go back to here so I don't know if there's anything for hosts in here oh there is right here okay great and so we'll go ahead and we'll just grab config hosts and I guess we're have to allow Local Host um and let's see if that's actually the case here so we go back to chat gbt and maybe 000000 Z but first of all I'm just going to try Local Host and see what happens okay so let's go back over to here because I just want to I want to make a program that's going to spam our app for log and then the logging will get sent over to there so if we stop our app here okay I'm binding and doing all this other stuff I shouldn't have to do any of that and we try this again all right it still does not work so let's try it on uh 00000000000000 Z and how about uh 127 other one here yeah I want all of these let's just take all of these here and I'll just push it onto it so this is uh this is the same thing as doing push because it's probably an array so we'll do this I'm going to stop this I'm going to start it up the normal way so just nice and clean like that and then I'll go up here and I'll do this on Port 300 I'll try this on 0000 Z because it is binding to 000000 Z when it runs still nothing z0000 Z huh all right let me try a bit more okay all right so this kind of sucks but I I still think that we can make this work I just wanted to run a script on the server locally but we can publish the um it on a server have it at a a specific IP address and then write a program locally to hit that endpoint okay so in a sense we're going to basically be uh um I don't want to say dosing but we're going to basically be sending a lot of requests to to Benchmark our our rail server so what I'm going to do is I'm going to go back over to um our production log because I made those changes and these are not good things to have in here so I'm going to take those out but the point is is that this would do some kind of logging we'll have to observe that on the server so I'm going to go ahead and type in whoops um so we we have our rails app and so the next thing I want to do is launch up an instance so I'm going to make a new file here in our agent and this will be uh template yaml and I'm going to want to launch up a new E2 instance so we probably have one kicking around from one of the other um projects in here and I'd rather start from something as opposed to from scratch so what I'm going to do is search for in our files SSM roll because that that will be one that actually has an ec2 instance and maybe this one's good so yeah I'm going to grab this one from s from the app config feature flag so I'm going grab this here copy and we'll go into this template here and I'm going to go onto the left hand side here into uh app config and we'll go in here and grab looks like a simple simple rails app here we're going to go ahead and grab um the deploy file and we'll copy this we'll go back down to our logs into our agent I'm going to create a new file here this one is going to be called deploy so we can deploy our ec2 instance we'll go ahead and paste this in and so now we have uh this will be adus cloudwatch Agent app okay so this will be let's say CW so this gets a bit smaller here and um I'm in CA Central today so I'm doing CA Central 1 that's usually where I do most of my stuff my older videos I used to always do USC to one but now I do CA Central okay so we do not want all this user data here so I'm just take this out don't need any of that and we'll just start with a blank server this thing we'll start at T3 micro I'm going to have to swap out some of the these codes here because this is not going to work what's this button here application composer cool and I'm going to go over to adabs and we're going to go to ec2 and I'm in CA Central one which is perfect I want to grab an Ami so I'm going to say uh launch an instance and I'm going to grab the Ami code from here so we'll grab this one here we'll go back over to our template we'll place it in here of course you could pass these as parameters externally but I just for these demos I just do them in here um so I need my VPC and subnet because that's probably for us one and I need to swap this over for my correct one so we're going to make our way over to VPC okay and from here we will go into vpcs and I'm going to grab this name here and go back over to here and paste it on in there we go now I need a subnet I find that uh CA Central 1D is cursed so I always go with a different one like this once there's something weird about uh 1D there's something weird going on with that one uh there's certain things you can't do there it's frustrating but anyway so those are now updated and the security group has everything open let's see here for the internet so that's what we want that is fine we could even be more specific and say Port 80 but we'll just say everywhere because I don't care and we have an SSM rule which is going to allow us to log in um I don't need app config because that's not what we're doing here today okay so that is now configured and so what we can do is we can now go ahead and launch This Server so I'm going to go ahead and type in clear and I want to deploy This Server so we are going to go ahead and chamod in case we haven't I don't know if we copied it or not but if we didn't we're going to chamod that file so we're going to go back level + x to make it executable it's for the deploy file so now we can go ahead and deploy this okay and I'm going to go over to cloud formation the only I kind of wish I did was name the the server but that's fine and we'll go over here and we will say review and progress we'll click into it change sets and we'll execute the change set now it's good for us to also learn how to use systems manager commands because if you're doing the sis Ops or the develop uh the devops engineer or other ones uh that one will come up a lot so I think that we should use um that to install the cloudwatch agent and it is the preferred way to install it um and maybe we could even write a uh a run command to I don't know if that would work but we could try to run a run command to install it on the server so what I'm going to do here is I'm going to need to get the rails app zipped and then we want to deliver it to an S3 bucket so that then we can pull it down uh for that so I'm going to go over to this read me here okay and and already I'm thinking our ec2 instance doesn't have permissions for S3 and we're going to have a problem here but that's not a big deal so I'm going to go ahead and say here uh ZIP rails app script and we're going to go ahead and I know I've done this somewhere before in this project so in one of these things so I'm going to go see if I can find this here if you have always reuser code it saves you so much trouble finding files say zip and it seems like we're zipping stuff over here yeah so here's an example of zipping I thought maybe there was like a script I had for this pretty sure I had a bash script for this that's okay well we could also just check if it's there this is for app config again is app config going to give us a nice script no oh yeah it does right here so we we can grab this one very lucky here today we'll we'll call this upload as well and I'm going to paste this in here and this one's going to need a bucket so we're going to have to create a bucket first go back to read me here we'll go all the way down the bottom we'll say uh create bucket for S3 or for um rails artifact and this will be it us S3 make bucket S3 SL slash I'm going to call this this is for cloudwatch Agent app I'll put some numbers here you have to do something random you can't have the same as me or it's not going to work because someone else will created or I'll create it and you won't be able to make that bucket so I've now made this bucket which is good and so in my uh script here I'm going to have to update this so I want to zip is where we want to zip it but I'm going to say uh workspace cloudwatch logs agent for Slash and then the app will go here and so here I'm saying everything in this directory so this will be the Shogun directory it's going to make that as an app zip I'm going to copy this here want to make this a bit easier we just say bucket bucket up here why I didn't do that before I don't know but I'm doing it now okay so we'll do this and we'll say bucket and then this also will be a bucket and the other thing is I want the region to be uh C Central one so I'm just going to get refactor this a bit C Central one and this will be region dollar sign region and so that will um upload that in the appropriate area that we want it to be in so this should just work so let's go ahead and see if this works and the other thing is this will clean it up afterwards so it will remove it if it already exists at the start of it and I kind of wish it would do this twice I'm gonna do this actually twice there we go and let's go ahead and we'll chmod this upload script uh now that we have these two here I'll leave it fine usually I put them in a bin directory but it's fine we'll leave them where they are you plus X upload let's go ahead and upload this and so it's zipping it there's the zip excellent and now it's uploading and so now it's there and then it cleaned it up afterwards so that worked great um I'm going to assume that it is there we can check we'll say Aus S3 LS um and then grab this here and we can see that that is there so our ZIP is up there we can go take a look at our ec2 instance and obviously there's going to be a couple dependencies that we're going to need like sqlite and Ruby so we'll go ahead and we'll connect our instance here with sessions manager that's why we did the SSM rle there with the manage cord that allows to get access here this is the way I like to always access stuff so that's how I do it we'll do pseudo Su hyphen ec2 user so it's going to switch over to the ec2 user because we don't want to be the ssmr default one say who am I so we know who we are and I want to see if there's Ruby already here so it probably doesn't have it it doesn't so say pseudo yum install uh Ruby I believe that's how we have to install it and we have a version there I'm going to go ahead and try this again and just say hyphen y um normally what we want is we want the de the the developer version of it but I think that this will work with this one so I'm going to go and just do hyphen y here I thought it was Hyphen Y is it not hyphen y pseudo yum hyphen y I guess it's lowercase Y and I think that we've installed the version that we want because we're going to need to adjust this script with our user data I took it out I don't know why I did because we are we are going to have user data in here um so I'm going to go out I think it's just user data yeah it's just user data there's always like a little bit more than that so I'm going to go back into our app config into that one here where we actually have one I have scripts all over the place but this is one it's this line here I want I want to grab these two these three I should say okay and um so what we're going to do here is we'll and again this is something that we could actually do as a run command so I'm not sure why we're doing it here uh you know what I'm going to make it a run command let's we can put it in the user data there but I think I'd rather have a a run command for that so we'll run it manually sorry I changed my mind there I just want you get practice with sess uh systems manager for those that are doing those other specific certifications um so Ruby's installed so we'll just paste it here for now paste oh that's not what I want the other thing that we're going to need is SQL light that's like basically the only two requirements I think come on paste oh come on pseudo yum install Ruby hyphen hyphen y uh we want this beforehand so be like install these all right so we have this uh the other thing is SQL light let's see if we can just install that so we say pseudo yum install SQL light 3 this is why I did not want SQL light SQL light install Amazon Linux 2023 I believe we're using 2023 let's just try SQL light so there is a package there and it's already version three so that's what we want so we'll say hyphen y here while that is installing we'll go back over to here pseudo yum install SQL light hyphen uh light hyphen y okay in fact we could do this as one line here we say esal light like that and I still think we need Ruby Dev l or Dev because we need to compile binaries but let's find out if that's a problem first right so we have that part I do gem install rails let's see if that works it should just works so far it's working which is fine so we have gem install rails is the next thing we need to do and it says failed to build native extension so that's what I was worried about so normally when you install Ruby there's like a a devel version so we' say like devel okay so there it is and we don't need both Ruby and Ruby devel Ruby devil will bring us everything it's like developer tools developer something it's so we can uh compile native extensions because not everything Ruby is is Ruby it's somewhere written in C and so they have to compile SE code for it to work so we'll go ahead and try this again see if that resolves our issue and here it says failed to do this now we did do a rails app somewhere else before and I think we used a bunto or Amazon Linux so let me go search our repo it probably was in elastic bean stock that we did this but I'm going to go ahead and just say edit find in files we'll search for it and we do we have it here oh it's feature Flags again it's always feature Flags this one's been uh really useful for us and so this one actually has a bunch of stuff for rails okay but it well this one's actually not rails this one is for um Sinatra but the Sinatra is a smaller framework but doesn't have proper logging that's the only reason why we're using rails here today and so this is probably what we need so I'm going to go ahead and grab these two lines because this is probably going to resolve our issues okay the only difference here is that I'm also including SQL light SQL light and so we'll we'll fix this here and yeah we should install bundler as well we should have bundler before we have uh rails Gem and so we'll go ahead and try this again I didn't even do yum update so let's go ahead and run this in order like once we figure out what actually works on the server then then you automate it that's usually how it works or that's how I do it so we'll go ahead and do this we'll paste this in here enter uh the other one didn't have that it was fine and so I'll go grab this one here as next this one's fine we'll save paste and so that will install all the dependencies and I've had this for a very long time I think I grabbed this from um a very old one but like this is GCC just so to compile C code which is what the N extensions need to do with the development toolkit uh XML which we'll need to Parts XML and I don't know what STL is but I know that we need it uh we have red haap RPM config which might be if we need to download other things um and usually that is something that we probably have to specify for um uh Amazon 2023 at least it it should be anyway so now that that's installed let's go ahead and install bundler first that one shouldn't be a problem it's going to be rails we want to see if it works oh my goodness jum gem install bundler okay and so we'll say gem install rails and so if this installs then we should be good shape we can even generate other rails app here we're going to use the one that we created but I'm just going to make sure that we can run a very basic app on this okay so this is where it could take a bit of time it's funny because we're not using websocket but uh there's just a bunch of stuff in here we have to download we'll just give this a moment shouldn't take too long all right so that built uh it didn't take that long and so now what I want to do it doesn't really matter where we are right now but I'm going to just look where we are PWD I'm going to say rails new and I'm going to grab all the stuff that we had over here so we go back to our read me uh which is over here here we had this command which is fine we'll copy that we'll paste it we'll hit enter so this is just building out a basic app okay and I mean I see something's messing up it says permissions issue I'm not sure why that's an issue so it says could not find SQL light 3 a bunch of stuff an error curred while installing base 02 so this is where we kind of run into problem sometimes retrying download it's likely you need permissions to write for that directory okay so let's go ask TPT if there's a quick fix for this that's not what I want I mean we could just change the permission so that we could write for it and that would be like a quick fix I wonder if that's what it's going to be just like hey create the directory and then say it has permissions because we don't want to do a pseudo on this eh um that's exactly what it did which was stupid holy smokes what is all this junk it's giving me where did it go like oh I guess all the styling broke and yeah we can install the install the gems locally I suppose like that's normally what you do in production I don't want to do it for this example though so what I'm going to do is I'm going to copy this here and I'm going to go back over to here and so what I'm thinking is that we'll just create that directory ahead of time so we do mkd p and then we'll just go ahead and create it uh this ahead of time like this okay and I'll go back here and paste it as such make di m kdor p maybe it's that I forget okay uh make di can I just do it like this ah okay it already exists all right I wasn't expecting that let's do um LS hyphen La user share Ruby 2.3 gems and it's saying that it's root okay so Roots the the only one has permissions for it so what we'll do for that is I'm going to go here and I'm just going to say ruby gems and hopefully that will uh change the permissions for everybody and I want to change the owner so say CH own recursive I think we just to do that and that's recursive right and this will be um E2 user E2 user cuz that's who we are right now in production what you normally do is install the gems locally but again I don't want to do that we'll go ahead and take a look here and so now this one says it's all ec2 user what about the directory itself ec2 user okay so I'm thinking that might resolve our issue just in case we have to do for both I'm going to do for ruby gems as well I just don't want to have any problems here so let's do this one as well so we'll paste that in oh come on just paste you have to be really finicky here and do a right click copy right click paste I hate doing that and so we'll do L siphon La again and I'm just going to filter out for I'll just say grep Ruby so I can see what I'm looking for um and that's looking fine so I'm thinking that might resolve our issue let's go ahead and try this again so we were trying to do I think a uh like generated a new app let's say we have less problems now so we're still having problems this one's at a different directory okay uh user lib 64 okay did it copy it no it didn't user lib 64 four we'll go back over to here for/ gems Ruby 3.2 okay just remember again this is not for production this is just for us to get it to work and we'll try this again Watch there's G to be another directory we missed okay so at least like we know native extensions is working because date installed with it installing my native extensions is not specific to Ruby lots of programming languages will need to compile native C code underneath um and then and then have a wrap around it uh I'm not sure why it's taking so long to do big decimal native extensions install it normally doesn't take that long but it is now proceeding so we'll give it a moment okay yeah I'm just going to pause here and wait for this to finish it came pretty close to finishing though uh so what's the problem now you don't have permissions for the user bin directory okay um I guess what we'll do is we'll do local so I don't think we should overwrite the ec2 bin director that's kind of an important one so another thing that we can do is install uh gems locally I think we saw that a moment ago somewhere it might have been in here and it's this here so what I'm going to do is I'm going to copy these two here and I'm going to type in clear and we'll p paste this in here as such we'll hit enter and the idea is that but we don't have a project yet so that's where it's like a chicken and egg thing it's like we need to use rails to to install something so maybe this is where we should just download it because I was going to just make an app but and so uh to make this easy but it's clearly more complicated than I thought so what I'm going to do is I'm going to get access to that bucket for the repo uh for the ec2 instance so what we're going to do is is shut down this cc2 instance and launch it again so we'll go over to over to ec2 here or cloud formation right cloud formation and we'll go ahead and we'll tear this down delete that's why this one failed let's get rid of this one for before retry delete cluster am I running a cluster somewhere else where would I be running a cluster kubernetes I must have torn down the kubernetes cluster right I didn't leave a kubernetes cluster running oh thank goodness I thought maybe I had one still running maybe it's ECS you got to watch out for stuff like it catches me all the time I do my best to clean stuff up so yeah there's a um a cluster here it's empty it's not a big deal so when I was doing ECS for one of the courses I don't know but any anyway we are tearing down the uh that cloud formation stack so we'll go back over to here and so that is deleting which is great I'm going to go back over to our template file here and what I want to incl uh include in here is S3 permissions so I'm just trying to think about like get objects again I probably have this in another repo and just going to go ahead and type in um get object here we go and so here's one where we are giving permissions to a file so I'm going to go ahead and copy this policy document here I want to find one where it's actually the E2 instance going to see if I can find one for that here we go there's a good one we'll go ahead and copy this I just wanted to make sure that I could uh easily bring that over and so we'll go here and I guess it's right here in line with the rest here there we go get put object um we don't really need to put but I'm going to leave it in there because I want to so I'm going to do that here today and I'm going to go and grab a bucket and we'll put in that up here so we'll say bucket name type string and then I'll just put the default as our bucket again probably would pass these as parameters uh like as override parameters but I'm just again making this life easy for me okay so now I have that set up I'm going to go back over to here this now has permissions um for that and I'm going to go ahead and I want to what do I want to do I want to um run this again I want to deploy it again right so we'll go ahead and deploy and so this is now going to deploy our ec2 instance right and what we want to achieve is we want to now download the zi rails app and then we'll do a bundle we'll install U gem files locally into the vendor directory so that they're not trying to install into directories that they don't have permissions to install into we have to accept this review here and then when that's done and we actually have the app running then we'll go write run commands to automate the installation and then we'll use cloudwatch agent to install and then and then we need to build a simple app to hit the server with data and then we can then observe that that data is being being logged by the cloudwatch agent okay can't make it any easier than that but that's that's the involvement here that we need to do so we're going to wait for this to um deploy okay I mean as we're waiting we could go write the the thing that we want so what I'm going to do here is while that is deploying I'm going to go over to our app I'm going to use Ruby for this because I just like to use Ruby and and uh if we go to our Shogun app and we go into our config and into our routes grab these end points so I'm going to say um write me a ruby script that will send at random data to these end points okay and so the thing is we uh the script should have yeah yeah so we'll just ask you to do that first and see what it comes up with as a basis and then we'll we'll rework the script a bit okay and yeah we could use net htvp for that that that would work fine so yeah it's creating the end points that looks good and it says I don't need data I don't not sure why it's sending data so it's looking a little bit better it's not exactly what I want but it's it's pretty close we'll go ahead here and we'll call this um go the top level here into our agent and I'll collapse this we'll make a new file here and this will be called um data gen data generator. RB and I'm going to make a in this directory I'm going to say bundle in it bundle anit and we create a new gem file here and we'll say gem rake gem Ox it always wants ox or noiri and I'll put pry here so I can debug this we'll go ahead and say bundle install and I'm going to rework this script here so I I always like to make things as a class and so I'll go here and call this data generator and I always like to have a run in here make things classless or fun uh uh stateless functions stateless functions and so we'll need a new rake file here so I'm going to go ahead and just say file new rake file you got to spell it right or it won't work rake file we'll just say task do run or run this will uh require relative our data generator okay uh RB it will complain if we don't have RB on there so we'll say data generator run and this will be our parameter so the idea is that we want to have the the base URL here so that'll be wherever the IP address is right um and that's at least one parameter you want so we'll say base URL here we'll bring that into here okay base URL okay so we have end points that we can hit which is good and what we can do is just say like we'll make this a function say random endpoint all right so that's our end points and I think we can get an end point at random by doing um is it like Shuffle Shuffle and or you know we could just say sample I think sample is is what it is if I go down here to IRB I'll just test this really quickly so if we say IRB and I'm just going to make a quick array here so just say one 2 3 four five I do sample it's going to pick one at random okay so and we don't have to put a return because this will automatically return this but if you want you can do return like that but we don't need to do that okay so this is going to return a method and a path one of these um hashes from Ruby okay so here we can say data generator random endpoint we could probably do self here I'm not sure but I'd rather just put the full name because I know that works and so that will be our end point um and so that's that part here's talking about ging random data I don't I don't want to send any data we don't need that the same for post and put request we we don't need to actually send any real data for that um so we'll take this one here as a function and then we'll say send request like it's not random at this point because we' we've chosen one right so this one will say this will be uh send req for request doing the same thing as we did the sampling here but I'm going to say data generator send request and I'll pass in the end point here and I'll pass in the Bas URL here as well okay and so we'll go here and just say base URL this has to have a self in the front of it and this will be endpoint and this will be base URL here and that's the endpoint path so that's technically correct and we have the endpoint method so that is also correct these look fine to me they all should be application Json so I'm going to go ahead here and just tell this one it is that as well I don't need any bodies I'm just going to take that out for now and that should send the request yes and that should work okay great and then down below then we need to actually do some looping here so I'm going to go grab this here and the problem with this it's going to Loop forever right between one and 5 Seconds I guess that's not that bad um I'd rather it be like a range I'm going to make a range here so I'm going to say it can either be every I'm not sure what that would be um so like yeah I'm not sure what that would be one second here so how could we specify sleep for 100 milliseconds in Ruby it's going to be like 0 point something and I don't know what is 0.01 yeah that's what it would be because a th like a one would be a th and so this would be 100 milliseconds so this is 100 milliseconds this is 500 milliseconds we have 1 second uh we'll say 3 seconds maybe two sorry and then we'll do uh um four seconds so the idea is that it will randomize I don't know I'll keep it down to this um and so the idea is we'll just say sample and it we'll choose a random time and I'll make that a function as well so just say defa self a random sleep time okay again we don't have to write return here because it's already inferred the last thing in a function for Ruby does that so we can have that here and this would be they be like data generator so we can just do this we say data generator random sleep time like that and so that will sleep that a random and make a request so the thing is like I think this will just keep running until we tell it to stop running is basically what will happen here um so I'm going to take this out of here I'm going just say Local Host 3,000 because we can test this locally first before we do anything else and yeah it should just Loop until we kill it so let's go take a look at our our server first and so that is complete and I kind of would like to test this first so maybe we'll do that first okay all right so yeah I'd like to test this against our local one first so if we have the server running which we do this one is Local Host 3000 um and I'm in the correct directory I'm just going to do bundle install I don't know if we did that in the agent directory and then I'll do bundle exec uh rake rake run okay we have a slight syntax there here I think I just forgot the commas on this comma comma comma comma try this again it's just task it's not tasks try this again um inv valid your ey let's try this invalid your ey not sure what the problem is with the URI here is um Bas URL base URL that's because up here this supposed to be like that there we go I should fix that send random request did we put a self on that I don't think it's called send random request it's just send it's going to be data generator oh we have it right there it's right here right so we bring this up here and the loop we'll try this again and so now we're getting a 308 which is not particularly good I think this one might be running in production that's why so I'm just going to test this against development so we'll just say yeah there we go like that this stopped after a while okay and so that one is working it's it's showing um saying Omni and we're getting back uh John Blackthorne which is not exactly what we want to happen so I think there's something wrong with our app let's go take a look at our app here oh you know what it is it's probably our routes so if we go to our routes I bet we might have not configured that properly possibly yeah they're all going to Jo John Blackthorne that's why all right so we'll try this again we'll run this again well I we got to well do we have to restart our app I mean is it's working it's doing uh buaro but the problem is it's not randomizing between each one so it's because we have to also place this into the loop as well there we go um we could we could put this on another line too if we want we just say uh time or TS here for time it is time in seconds so that that makes sense and I don't think we have to restart this and so now this is working how we want and so you know it's kind of working here it's not going like super fast which is fine um I think I'll take off this one here and I'll put on yeah I think that's fine we'll try this instead let's see what happens here yeah and that feels more reasonable in terms of of data so the question will be will that work externally we don't know let's go over to our E2 instance and we'll go over to our instance here and we will log into it we'll connect and so now what I want to do is I want to run our upload script so let's go ahead and run that that's going to zip our repo or our app and then place it onto the server we'll go back over to our ec2 instance here we'll connect to it into I already did that with sessions manager didn't I we'll say pseudo Su hyphen ec2 user we'll go back over to here we'll go to our read me we'll do a pseudo yum update first say paste then we'll go ahead and do this line here and we'll go paste we will go and do gem install bundler after that is done we'll just wait for that to finish that is now done good so we'll go to our next uh line which is the gem install bundler paste and then after that we'll do gem rails paste so now what we need to do is download our um application so that's going to be from a S3 um copy S3 col slash we will just collapse this so we can see what we're doing here and I'll go to the upload as we need our bucket here as such and it'll be slapp doip and I'll just say app. zip here so grab this we'll go back we'll wait for this to finish installing all right so that's ran uh I don't remember what we ran last Gem Gem install rails okay so now what we want to do is download uh this from here so we should have permissions to do so hopefully that works enter and that downloads the zip now we need to unzip it um I always forget that command so I'm going to go ahead and look for it we'll say file edit finding files unzip so yeah unzip app. zip I keep thinking there's like more to it but there's not we might not have uh unzip installed or zip installed but we'll see what happens here oh it's there okay and we'll do LS and so now we have I don't like how it's like zipping the entire directory that's really annoying so uh ZIP don't include entire path it's like that's so stupid use the J option The Parent Directory so we'll see if that works um so what I'm going to do here I'm going to try zipping this again we we'll change our um our our our hers to say R hyphen J well what does The Hyphen R flag do what what does hyphen r n we'll just leave it alone uh so we'll try this okay and see if that works better saying that's invalid all right I'm G ask chat GPT so I want to zip a folder but I don't want to have the full path of its parent par folder folder path included let's see if it can tell me can just give me hyphen J using the eight no oh my goodness I didn't say to use Ruby let's copy this using the zip command for un or Linux okay I don't know if like it just feels like it's getting stupider every day Chach PT and then it like messes up its output we do the hyphen R command oh my goodness this is useless give me a second here here it's suggesting just to use J and not R so let's go ahead and change that out all right we'll just try this again cannot remove file directory I mean that's fine okay um let's just comment this out for a second here will this work nothing to do all right I got to figure this out someone's suggesting that we can uh do this apparently so we'll see if this fixes our problem I'm not really sure if it will but I going to go copy this because this uh this this problem is really annoying I don't like having this here so we'll go ahead and paste this in as such and I'm going to go ahead and copy this and so here we want to go to the path of where the actual hold on let's take a look here so this is saying this my path oh you know what it's cuz we include the entire path here oh okay so then what we could do is say Shogun like this and then uh what did it do here let's see here for a second here it's not using the J command it's using this one so we would do my path like this right and then we would say app. zip Shogun like that and then it's suggesting that we can do that so let's see if that fixes our problem that' be really nice if it does it looks like it's right because it's doing this so that looks good and uh I'm going to remove this app. Z because that's kind of useless to us and I'm going to try to download this again so we'll copy this did we copy that command into a read me file here let's go take a look uh we did and then now we'll try to unzip it we do LS there we go now we have what we want so I'll CD into Shogun and now let's do a bundle install but we're not going to bundle install we're going to do that local thing um to help it out here so I think chat GPT was the one that told us like I'm sure we can find that somewhere else let's just go get it from chat GPT even though it was kind of a jerk last time we used it so somewhere in here I had something for bundler it's this here so I'm going to go ahead and grab this here and we'll try this first and see if this works so the idea is we're saying uh set the install path to vendor bundle which is where uh you'd have it if you're packaging with it so we'll see if that works okay all right we came pretty close here but now it's complaining about psych so I'm not sure why I don't even know what psych is so let's figure this out psych native extensions fail to install but fix the Run by Jam Jam update lib yaml do we install that let's go over here we don't so let's see let's see if we did a pseudo yum install lib yaml is that a thing no lib yaml uh yum let's try this libyaml de de deel is that a thing it is okay great so we'll try that I'm not sure if that will fix our issue here we'll try the bundle install again because if it's dependent on that that could be the uh the reason why and so hopefully it will build now let's find out there we go that wasn't too bad hey we did it okay uh so I'll just put that one in there it was called lib yod Dev L I believe let's just double check that make sure that's correct Li devl so that's fine but does the app actually work and that's a different story so we'll do bundle exact rails s to start up the app I'm just doing a local host right now I just want to see if it even starts up and it does start up on on uh the server here it's starting up on Port 3000 which is fine um it's not binding to Port 00 so it's not going to be publicly available so what I'm going to do is just stop this here and we're going to launch this for production so I'm going to go over here the only thing is that we didn't update our config files but it's coming from an IP address so it should be less of a problem I think um so what I'm going to do is go over to our uh readme file in the agent directory and we had the command to start it up and it starts it in single single cluster mode meaning it's starting up a single one now when you run a rails app a T3 micro is kind of too small but we'll see what we can get away with here okay because we're not really doing anything we don't have a database running and so this is now starting it up and it's now binding on Port 000000 and so what we can do is take a look here and see getting the public IP address we'll put this here and I'll go to another tab here and we'll try 3,00 and that's okay if that happens that's fine and we'll try HTTP because we don't have htps and we need an endpoint in here so I'll go back over to our um script here to our config uh you know we reallya just grab it from our data generator here and we'll just do a post because that'll be the easiest one to get stuff for so John Blackthorne is a post and we'll see if that works it's trying to do htps again we'll we'll just tell it to switch over to http and so it doesn't seem to be working it keeps switching it it keeps forcing it on htbp so it might be that it's it's forcing SSL um if we go over to there's an Options under here for rails say config Shogun config environment production it's forcing it here so we really want this to be false okay because we're not going to use SSL here so I'm going to stop this I'm going to use VI to do this you could download it zip it and download it again I'm not going to do that I'm going to use VI to fix that so I'll type in VI or vimes is vim here it is I'll do control Q so Vim config uh production or sorry application environments environments production by the way I have a Vim course so if you don't know how to use Vim you should really watch it it's an essential course because I'm not going to explain all the keys for this or if this doesn't work you can open this up in Nano but you'll have to figure that out yourself so what I'm going to do is I'm going to make my way over to um the SSL so I'm going to go down I'm going to type in force I did for slforce and for next I'm going to move over here I'm using the L key to do that I did um uh change words C W and then I'm typing false escape to get an insert mode colon look down below left corner W then Q right and quit and so now I've changed the code so that is good and we'll go ahead and start the server again and so hopefully that will resolve our issue here and so it should not force this anymore it still is though which is bizarre okay so I'm going to stop this control C I'm going to go open this up again production and yeah it just keeps refor uh redirecting I don't know why this could be um maybe in the Puma configuration so I'm going to go take a look here at the Puma configuration uhhuh and I don't see anything in here that would specify that yeah that's interesting okay so let me figure this out one second one thing I'm noticing is that I started back up and it's saying HTTP so it's not like it's listening on Port 3000 so or I mean like it's not that it's expecting to be htps so I'm not sure why it keeps redirecting which is kind of annoying so I wonder what would happen I'm just to try this I'm going to go ahead and grab this here because this is going to have to work anyway if we want this to work and I want to go head and just do HTTP here like this and see if that does anything and that returns back so even though it's not working here I don't care what matters is that this is working okay and this is returning when we hit that that end point because now what I can do is I can grab this here and I can go over to our um script which is here our data generator in our R our r file here sorry rck file and then I can replace this with this and the idea is that I can now run the the data against there so the other thing I want to know is is it uh logging to the production file so I just stop that there I'm going to LS hyphen LA and there's a log directory here so I'm going to go tail hyphen n 100 lines and we're going to go and say log production log and I want to see if there's any logging data in there so I don't see anything so I'm not sure why it could be because Puma is intercepting it um so Puma production log not logging Puma okay so there could be a reason as to why that's happening there was an error in my Puma log I figured out that problem by copy and paste the Puma file after that you restart your server so it could be something because Puma is like a web server in front of it and it could be mucking with our our rail server not necessarily McKing but we need to configure it to uh log out because if we don't have that production log then it's not going to go anywhere right it might be going to STD out or somewhere else um but I want to go to production log because we want to be able to pick up that log file with cloudwatch the cloudwatch agent and utilize it with it so what I'm going to do here here is I'm going to go over to um what what am I going over to I want to go to my Puma file here and I want to go into Shogun config Puma I'll close out all these other ones so I can see what I'm doing here and I mean I want to run in production is this running production or development and actually that could be another reason that we're having an issue it's saying production here because when we run our line here we're telling it to run and production right um and so if we go back to this fellow's code I think it was this person here right or was it over here it was over here so we have rails the socket don't care about that SD out redirect so it's possible I mean there could be a puma sdl log let's go take a look at that so I'm going to do lsla log here again but there's no Puma log so what the heck is going on here right hm all right give me a second all right so no no help so far but I'm going to do LS hyen log here and what I want to do is just take a look here and see who has permissions it says e ec2 user has permissions here if I cat out the production or the log production here let's cat out the development First Development oh we have logs in here okay am I currently running this all right so it actually is logging I don't think anything new though did this thing stop it did um okay well what if I send one here connection refuse well the server is not running right now okay so let's go back to here and we'll start up the server again right and I'm going to run this a few times 1 2 3 4 we'll go back here notice for getting requests coming in okay so it's logging them and then I'm going to go ahead and check here okay that's fine but I want it to the production so like it's clearly getting requests I'm not sure why there's even anything in the in the development side here but this is a little bit frustrating because clearly it's returning stuff and clearly it's logging stuff and it's logging it to SD out right so H let's see well we'll ask chat GPT again even though I I find it extremely unreliable um see if I can copy this command I can't I can't copy anything from anything anytime okay so if we go to here into the read me and we copy this one here and we go back and so we'll say here uh how do I ensure logs right to a log file try that here oh so maybe it's telling us to log these this will set logging to this the paths okay so it won't necessarily be production but it will it will like usually we go production. log as we're used to to to go but maybe that's what it's saying that we need to do okay so what I'm going to do is go into that Puma file again locally first right and I'm just going to drop this anywhere here it doesn't matter and then I'm going to copy these lines here I'll go back over to here I'm going use Vim to get into it so we'll say config um just a second here all right I did not know I was muted but I want to tell you because I don't know where I messed up this video but I just want to tell you that um we got it working and all we had to do was go to our Puma file and inside of our Puma file I'll show it to you here I don't know where where I paused and I didn't pause but I just noticed it was paused it's not supposed to be paused but if we go into our puma. RB I added um this line here and now it's outputting to these directories Okay so so I opened it I I tailed this file and this is working fine I sent it here so now we have uh something that is working so this isn't good shape now we have a bunch of stuff here this looks fine to me I'm just making sure there's not too many files here it says 1,892 which is obviously too much um so what I'm going to do is I'm need I still need to get ignore here so get ignore rails and I'm gonna have to still copy this here otherwise it's going to make a big old mess I'm going to go ahead and place this in here dot get ignore and I'll paste this in here and we'll save it I'll go back here this should greatly reduce it there we go that's now manageable and so in theory we have part one done which is we have a working app um and so the next part is to uh install the cloudwatch agent and and then also automate some of our steps here so I'm can call this uh install rails app create data generator okay because I feel like this video has been quite a long video it's like 1 hour and 32 minutes I think we can divide those two parts up and so I will see you later in the next one I'm going to tear down our ec2 instance because we know that it works in its current state and we'll look at uh doing some automation next here okay so I'll see you in a bit okay ciao hey everyone it's Andrew Brown and we're continu on with the cloud watch agent so the first part of the video uh was us building up our application and and uh getting things uh through to that process and so now we are moving on to uh the next component so I'm going to go ahead and reopen my environment I'm using gitpod of course use whatever you like and if I seem that I am a bit unfamiliar with the app it's because it's been a few days and I literally forget things the next day uh so we'll just stumble here a little bit and get back into um what we were doing so opening this up and I believe we were under cloudwatch and in here we had the agent folder and we have a bunch of stuff all right there we go and so oh yeah I was watching the the show shun which by the way I finished very good show now I'm on to a new show called Tokyo Vice Tokyo Vice uh so hopefully this is a good show as well I guess we're going to find out it has a good rating here but you can see I'm on a trend of a certain type of show that takes uh takes place in Japan so I don't know just what I've been into as of late but anyway trying to remember where we were at so we have our template. yl file and yeah we have our SSM rule so we can log in and we decided not to set this up via our user data script uh and so I believe that we collected all of our steps over here and so these are all of our steps that we need to um run and install the application okay so we have sud install uh install bundler you know what I'm going to do I'm just going to watch the old video so I can just speed myself up and I'm not repeating things give me a second okay all right so I think I remember what happened last time and uh what I wanted to do is I wanted to take this here and start putting this into a run command as run commands is something that is covered in one of the courses but anytime we do Cloud agent likely uh run commands is something you'll want to know how to do I've done these quite a bit in the past um but I wouldn't remember how to do this uh from scratch so I'm going to go here because I think I have um an example of a repo here that is private I think it's in teacher seat and so I'm going to go ahead and uh grab that and see if we can use that to a provision I'm just trying to think maybe it's like web server uh could be this ah build Ami so this one is for building an an Amazon machine image and this would be a great alternative if you were doing something other than um E2 image Builders is the way you had to do it before you can see it's four years ago so uh this is obviously the way that we would have done it before but what I'm interested in here is the template for um uh run command so if I go into here we have the web server R we have instance profile and then we create an automation document okay so we have an automation document here and in this automation document it is going to have some steps so here it starts up an E2 instance it waits for the instance to run it's going to check its status and then it wants to do things so in here it actually installs or updates the SSM agent installs cloudwatch agent um and what I'm expecting here is that it's going to run a bunch of commands so I'm going to also go back over whoops all the way to the top here just maybe zoom out here I want this side back here because I want to see what's in this I kind of forget if these are all my commands and so these are all the individual commands and actually a lot of the stuff is still pretty relevant so this is what I kind of want to reconstruct so I'm going to use this as an example of course and um you know you'll just grab it from here just run the template so it's not so painful for you but what I'm going to do is go down to the ground and we're going to make room for something new so the question is do we need run commands and an automation document you don't know what I'm talking about let's go over to systems manager for a moment so we go over to systems manager we can also bake an Ami we can use an ec2 image Builder to bake an Ami but I just want to provision a new ec2 instance and then run these when I want to run these but the idea is that you have run commands and these are individual commands that you can run against um a uh an ec2 compute so the idea is that you'd have documents that you create and then you would then run uh the documents so a run command against and so they would be defined in here and then the idea is that automation which I believe is that automation it says under change management they say new it's not really new is that you create a runbook and a runbook is going to run a series of commands but what definitely is new here is this um UI so this was definitely not here before and this UI looks similar to something else I can't remember when we did this before but um maybe this is really similar to where have we seen this before this this looking thing I don't know but I've seen this UI the UI before and so the idea is that we could assemble it in here um so you could say like I want to create an Ami I want to run a command this is not how I would do it um I would just use cloud formation to do and that's how we're going to do it like this is okay I guess it simplifies things but again this is not how I want to do it so this this is totally new which is really interesting so I guess that's what they mean by new but that's not what I mean by new so anyway we're going to go back over to here and what I need to do is figure out do we need a run command because a run command or sorry an automation because an automation is to run against something and so if I look at this um this one's called an automation document type we have content we have outputs and so this one is going to start up an ec2 instance but I don't want to do that I just want to get an existing one okay so I think that we can work with this all right so what I'm going to do is I'm going to copy this part here and go over here okay I'm going to paste this in here say allow and I'm going to change this so this is uh you know configure rails configure app document and so it's not going to launch an ec2 and since what it's going to do is it's going to uh be used to run against an existing ec2 instance so that's one thing we can pass parameters into the content here which is great um I don't need an output for this because we are not launching E2 instance but under the content here I do want to supply an ec2 uh ID but let's go down to the main steps here so here we have our content we have our parameters and then we have our main steps so I'm going to go ahead and grab um just a couple steps here actually I'll grab a bunch I'm going to grab them all because this is very similar to what we're building so uh not as complicated but I'm going to go ahead and just grab all the main steps and I'm going to go back to here okay and I'm going to scroll up again that repo is private so you'll not be able to get this you just basically have to uh you know get the code from here or whatever cuz we're going to I'm going to pair this down anyway the idea is that we have the image ID the instance type the instance profile name let says run an instance so we don't want this step we don't care about this step so we'll take this part out okay and so the next one is wait for instance in the State running um we could do that but I'm not going to do that so I'm going to take that one out as well we'll take that out wait until instance status is okay we could do that as well but I don't want that here today um so that is fine so this is where we get into normal territory okay so the idea is this is probably the first thing we would do is we'd update yum so here we're saying let's update yum and then we have our document all right then we're saying which instance uh do we want this to be and then what are our outputs so what we want to do is we do want to pass over an instance ID and I think that uh I mean this one's referring to to another one I think so if I go back over to this one and we go to here this step is called launch ec2 instance but I just want to reference an instance ID up here in the parameters and it looks like we just use double quotations for that so that's what I'm going to do I'm just going to grab anyone um these are this obviously isn't the instance ID but I'm going to go up here uh and I'm going to say right here um parameters param meters I can call this instance ID and there's no default here and the idea is that we'll need to provide that and so now we'll just say instance ID as such and hopefully that will work at as is it might expect like an array of instances so I'm not sure if that would cause an issue let's go back here and see if anything indicates that in the code so if we are outputting this this is instance IDs which is plural we go down here to run2 instance I'm seeing if there's like any output on this I don't see that exactly so yeah I'm not sure about that but we're going to have this because we're going to pass it in here and maybe we might run into an error here that's totally fine uh the other thing is that um there's documents here and so we can make this as a separate one so I'm going to make a new template here I'm just going to call this um and are we in the same level here I think we need to be in the agent level here sorry agent and this new file is going to be commands RB or sorry yaml do yaml okay and so in this one it's going to be kind of similar uh to the template but the idea is that we want to bring in all of our commands so if I go back into our cloud formation here and we go into nested and Ami run commands you can make a folder called nested I'm not going to do that here today and so the idea is that I want um some of this and this actually contains all of our commands so I'm going to go ahead here and just copy all of this so I have a lot of good stuff in here and of course for you to use this you're going to have to go into this repo and uh pull what I have here and if you can't pull you can just watch you'll have to stop the video and write it out um you'll have to do whatever you have to do to do that but the idea here is is that um we will need to reference the stack so here um if we go down back to our automation here which is here notice that it is saying get at Ami run command stack so I'm going to call this um I mean that's fine run command stack but I I need to figure out how to pass this into that into this one here like how are we going to cross reference those so if I go back to uh this one here and I go to our template okay and we go into here somewhere in here I would imagine we need to reference that so here we have the I'm just checking the top here so we have artifact S3 bucket artifact X3 path Ami name um so that doesn't help but here we're referencing here so this is we need to get bring in this resource this resource is going to allow us um to reference the stack so we'll go uh above here like this and I'm not going to call this Ami run command stack I'm just going to call this a run command stack because it's not specific to Ami and this one's going to be commands Okay so now we are referencing that other stack here and we're specifying its template uh relative to where we are here so now I'm can go down below here and I can I can say this and say paste say run commands output document yum update so if we go to here you can see I have uh this document here and if I go all the way down to the ground the bottom I'm not sure why I can't see the uh my right hand bar here but these are all getting exported for their name so we have outputs document yum and it's referencing that document here and you can see that it's getting referenced here we're passing our instance ID um and then these are inputs to this command so the idea is that we're saying cloudwatch output config and so if we go back to the top of this document here all the way to the top okay uh we should have um some kind of specif specification I'm just carefully looking at this here Cloud watch output config why is this here because this is inputs cloudwatch log group name Cloud watch log group enabl is that on all of them okay so this must just be a way for us to um have logging right so that's probably what this is if we go up and look at this we could probably find it here I'm just going to go find uh that document usually that's what I do is I'll I'll look them up first I'm not sure why it has a problem with my uh my computer here today but it's kind of frustrating doesn't like my IP anymore I guess and so I'm going to go ahead and do this okay and in this we could probably take a look here so we go into content then we have the SSM document schema features which is talking about and then within here I can search for where's it here the this this one here because I think that all that's doing is logging each each of these run commands so parameters action there must be like a sub page here for this if we don't know I can just put this in here and I bet we could find it this way here it is so uh configuration option for sending out command output to cloudwatch log so this would be really useful for us us to see what's happening for each one um this is going to log based on the stack name and then it's saying it's enabled so to log to log these and see what happens and that that's a good idea so I think that's a something we will definitely do just going to bring this up to our first command okay and so what we have here is we have run command sack outputs document so update yum if we go over to uh the one on the right here all right so we create a a document we say the document type is a command um I don't know what other types it could be um I pretty sure I covered that in lecture content but right now I I cannot remember here we have uh Tags I'm not sure how important tags are in this I can't remember why I would have tagged this a long time ago but it's tagged for whatever reason and so in the content area it looks very similar to the automation document but it's not exactly the same but the idea is that we are going to specify the action so here this is saying we want to run a command and this one this one underneath is saying we want to run a shell script and so for this uh we can say what it is that we want to do and this is actually um I'm sorry I meant to be on this one here I'm looking at the next command by accident because this one mat this update yum matches this one but the idea is we have pseudo Su hyphen ec2 user hyphen C pseudo yum hyphen Y update and I remember when I wrote this originally that it was really hard to get run commands to work and you always had to make sure that you force it into the context of ec2 user and that's what I'm doing here so I think all these commands uh I do this and um I think actually when I originally wrote like wrote my provisioning I had to do that in in this so I would go and I would you know how we did all this on the server you'd have to do this but also do this and make sure all your commands work because any of these could fail if something's wrong or it doesn't like something but in this one it's installing python which is not a bad idea but um I cuz the reason we want python is because of um cloud watch agent also say need for something else but I'm not sure if we need this right now so what I'm going to do is just comment out this one um this one installed the a you know what I keep meaning I keep meaning to be on this first command I'm so sorry so run commands for uh setting up apps Sorry so this one's pretty straightforward so we're happy with the yum this one is good okay so that's our first step right this one is good so then we're going to move on to the next one the next one is set update SSM agent and this one document is called adab us up up update SSM agents so what we're doing here is we're referencing uh this agent which is one that already exists in system manager because the way that you would um uh install SSM is you'd use the the predefined Run commands that itus has so if I go into here and search for this I should be able to find it it click document name and then do this so there should be one here if I don't see it here we can go to documents down below and this is another way we can find it and so I'm trying to search for this document so here it is okay so it definitely exists all right and if actually we clicked into it we could probably run it from here so you could go and run the Run command for here sometimes it's easier to do that but the idea is that if we wanted to use the Run command from here uh we would specify um the targets so we say we want to run this version of SSM agent and we want to Target these specific ec2 instances and if there's outputs we want to uh send that output to cloudwatch loog so this thing here that that that option here is that thing here okay um but anyway so we are saying run band The managed one here and again we're going to pass in our ec2 instance ID and this one's fine okay we go to the next one install install the cloud watch agent this is another one that one has called ads configs package this one is not just for a cloudwatch agent this is for installing a bunch of different packages so if you go down below it says install the Amazon cloudwatch agent so we'll go back over to here and we'll go back to documents it probably tells us about this document here and if we search for it like that should show up there it is we click into it it should tell us what this is install or uninstall distributor package you can install the latest version default version uh packages provided byus such as Amazon cloudwatch agent iTab us Ena Network driver iTab us PV driver are also supported so this thing installs more than one thing but all we want to install is the cloudwatch agent and this is the recommended way to install the agent there are manual ways to do it but I wouldn't do it that way so that's that um so that one's fine let's take a look at our next steps this one's system dependencies so I'm saying like what are we dependent to install here again we will change this to be uh uh instance ID because I've changed that I'm going to bring this on over here and so if I go down here I'm looking for document system depend so this what this document might not be in orders I'm just going to search for it here and so this one is going to install what a bunch of default stuff right so what I'm going to do is I'm going to cut this one here okay and I'm just going to bring this above here and um yeah so we'll have that above we don't need thei installed on this I'm not sure why we're installing it so I'm going to just take that out here okay so the is not in installing here but we go I know this document is a bit hard to follow but we have yum update SSM agent cloudwatch agent system dependency so here we'll take a look at our system dependencies I want this to wrap so this is a bit easier to work with here view wrap okay and I'm going to go back over to our readme file and so this is where we want to install um all of this stuff okay so I'm going to go ahead and grab this here I say copy and then I'm going to go ahead and replace just this part because when I made this other one this was all for Amazon Linux 2 so that's why I can't just use it it's using Amazon Linux 2023 now and so this should work I just pasted it into that area I'm going to get rid of that on the end and so that should install that now we are installing Ruby in a separate step so go back here um I'm not going to install JQ here today we don't need it so I'm going to look for wherever JQ is in here JQ I'm going to go ahead and remove that here okay going to go back up to the top of my document and the next thing is Ruby um we don't need Ruby installed we need our gems installed so this one is installing using rvm which is not a bad idea but I don't want it to be super complicated so I'm going to go ahead and remove this we are installing Ruby just not this way okay so we'll go ahead and take this one out all right next one is enable additionals so what is additionals in here this is enable Amazon Linux package additionals this is if we had to um enable anything so in Amazon look 2 this is something that you'd have to do a lot you'd have to enable them but Amazon 2023 comes with a lot of packages so this is a nonissue now you don't have to do this anymore so I'm going to go ahead and just delete this out and take out enable uh uh initials we have update CLI we don't have the CLI installed so I'm going to take this out and this one actually is a command that we made this is called document update CI so I don't want this one so I'm going to look for to say uh update CLI it might not be in this document anymore it might have already deleted it update CLI it's already gone excellent then we have install mpm um that's node package manager I don't need that here today so I'm going to take that out I'm going to go over here and search for it node say mpm so I don't need this so I'm going to take that out of here we have install install uh code deploy agent which is not a bad idea but we're not using uh code deploy here today so I'm going to take that out when we do code deploy we might come back and revisit this okay so we'll take that out we have install postgress which would have been great if we had utilized that we're not doing that here today download the documents here what's interesting is I don't think we uh had to install the C when we went into that instance I think we already had access to um that say uh Amazon Linux 2023 preinstalled with a CLI I think it was right because I don't recall installing it the current release has the version two of Theus so yeah it's already preinstalled so that's something I don't have to do anymore um so that's really nice but we do want to download our code here and so this one is copying from a particular location so that's something we definitely do want to do but that's not what I want to do just yet because what I'm really interested in is um installing gems so I do have a installed gem one here so so I'm going to go ahead this one's really big as you can see we'll go ahead and grab it here and I'm going to go all the way to the top here so we have the order here doesn't matter I'm just doing this so that I can uh follow what we're doing here so this would be our uh before our download document database and I'm going to go back over to here and uh we don't need any private GitHub access I'm going to take this out of here we might have to create some temporary directories but I'm going to take this and move this up here above the download code base and I'm going to say instance ID okay and we'll take a look at what the instructions are so we have document install gems all right and so this is a command we have some parameters here for specific versions um I don't need a specific version of postest Cu I'm not doing that I don't need a specific version of Ruby cuz we're going to just use whatever Ruby we have and so I'm going to take out the parameters and we don't have any parameters here so we have install bundler install PG gem a bundle config bundle install Okay so let's go back over to a readme and in here we want to install bundler so we definitely want to do that so carefully looking here I uh don't want to use rvm I'm just going to do gem install bundle and then the next thing I want to do is install rails so I'm going to go down here and say install rails and I'm going to clear this out to the end change the end and then this one here is going to be gem install rails okay then down below here we have some stuff going on here so we have bundle config um I don't want to do that here today so I'm going take that out we have something specific for GitHub I think so I'll take that out I'll take that out great so now we're at the stage where we want to download our repo right so we could pass a parameter here and that's probably not a bad idea so in here we have artifact S3 bucket artifact S3 path artifact file name okay so I'm going to put some defaults in here I think we can put defaults and so this one's going to be the CW agent obviously if yours is different change it to what you need to make yours we have um the artifact path which is defaulting to this it's not going anywhere and I see the bucket has a for SL there so I'm actually going to make this empty cuz there's nothing to set there and then the artifact name is going to be app. zip and so now I can just keep all this in place I don't have to change any of that which is nice so we'll go back over to here and so our next step is to download stuff which is perfect this is passing in um these values here so I mean it might make more sense to have set those in the policy here did I delete it out of the document it looks like I did so I didn't bring those back but I can probably grab them from here I think they're the same thing so I'm go ahead and just grab them like this right click copy and then I'm going to go below this here and we'll say paste and then we can take all these defaults here because they're going to get passed through anyway so that's not going to matter so uh we'll go back down to here so we had uh SSM agent cloudwatch agent system dependencies install the gems download the code base and do we need to create a temporary directory because when we zip stuff and download it I don't think that's necessary so here because it's a rails app it's creating some uh initial ones when you're in production you need these I don't think this is going to be a problem for us so I'm going to take this out but this has been a problem in the past with rail apps in particular so I'm going to take that out and so we don't need separate directories now this is installing Puma to run as a um a service so we do use Puma but we're not going to use it in this way we're going to keep it the most simplest way possible you see there's a lot going on there with Puma remember spending a lot of time doing that so we'll take that out and so that is now out of here we'll go here and get rid of Puma so now we have configure Cloud watch which is actually not what we want to do as of yet I'm going to get rid of some other things I want to I don't want to create an image I don't want to terminate an instance and so now this is where we kind of have to add our own steps in here so we do want to configure cloudwatch agent after it's it starts running the idea is that you want to uh configure this to U monitor particular files um but we're going to go back over to here and so in here we have ads S3 CP yeah so we actually do download this but we don't unzip it so that's something that we're going to want to do so I'm going to go here to download code base I'm going to carefully look at here because it looks like we have more than one step in here so what we have is we have um pseudo Su Hy E2 user so do this as the ec2 user okay we're going to copy this and it's actually going to place it in a temporary directory which is fine we could do that I don't mind that but we will have to move it next so then the next thing we do is we say make a directory so we're saying we're going to make this in some particular directory this is in the home you user app which is actually where it would end up any anyway we just put it in the roote one but because we're doing this as the pseudo user through the eect user I you we don't have a guarantee that it's going to place it in the home directory so that's why I put an attempt directory first and then we make a directory where it goes then we're going to unzip it which is actually perfect this is exactly what we want and then we're going to delete the original artifact to clean it up which is awesome so that's good uh so now what we need is we need um these steps here right so we need these to uh be be set up um okay the only thing that we don't have here is it doesn't start the server did we write how to start that yeah we have it up here uh I think it's just that's just this so what I'm going to do here is now we want to set up we'll say we call this document uh bundle install okay and so this is going to be a document type the property is going to be this I don't know if we need tags but because I've been doing this the entire time I'm going to do it I don't remember why we'll call this bundle install there must have been a reason why I tagged it we'll grab the schema here I don't think we need any parameters but we might want to provide the full path here so and that's fine we can just hardcode that we'll go down here we want to run a shell script so I'm going to grab these two lines here and this here this is an older document we don't want document private GitHub access I'm just going to take this out is there any other documents we don't want in here no I think that was the uh what's this big one here document config and start Cloud agent yeah we'll leave that one for a moment um so we have let me go down here we have our main step in our action so if I go up to here for a moment we need a name we need an input we need a run command I'm going to copy any any old one here I understand if it doesn't complete in 60 seconds it will mess things up so for this one in particular I'm actually going to increase it to something like um 600 because it can take some time to install stuff and I I don't want this to run into an issue and so we want our Command to go inside of here so I'm going to copy this so I don't make a mistake and for this I want to give it an absolute path because I don't want any mistakes whatsoever so I'm going to go here and say um home you user apps I'll grab this and we want to place it in the vendor directory of the app okay now I just want to double check this this looks a little bit more verbos bundle config set local path bundle yeah I think it was repeated twice here just a copy mistake problem here so go do this um like that and that looks good okay so that's that and then I need another run command because I need to do bundle install so we need another step here this one is not named download code base this one is uh config bundle and then I want to have another action here oops and this one here is going to be um install bundle so this will just be bundle install there's a lot of stuff that can go wrong here but again you know it's worth our time to do this um even though all we're trying to do is configure cloudwatch agent you know sometimes we have to go a little bit farther with these Labs so that puts that into place now the next question is like if we have the cloudwatch agent does it need to do we need to have our rails app started before we start collecting stuff this is something I'm not sure about what I'm going to do is drag this on over here and let's take a look at our document very carefully so we're going to configure cloudwatch agent to start we specifi bucket a path for um something and then we go down below and it's going to configure a few things so it says collect for uh collect D okay and then we have copy config so here it's saying pseudo suc2 user and it looks like I have a configuration file here called Amazon cloudwatch agent Json and then I'm placing into the correct location so that might be that we want so let me go go grab that I'm just going to pause because it's in a private repo all right so I'm just in the exam Pro repo I'm not obviously showing you everything because it's our main repo but it's the app that the this the the actual um platform runs on but in here I have a cloudwatch agent so we can grab this and this is what we're going to utilize to configure there's a lot of stuff going on here but that we can configure our cloudwatch agent so I'm going to copy that I'm going to go over to here and what I want to do is I want to go to our Shogun app and I'll make a similar directory called ads I like to put stuff in databus directory like that and then this one needs to be called what it is called Amazon cloudwatch agent um Amazon cloudwatch agent Json so I'm going to make a new file here and call it Amazon cloudwatch agent. Json and I'll go ahead and paste this in here so now we have ourselves an agent this one in particular is trying to copy the produ ction log so if we look at this carefully it's saying we have an agent collect at the 60 interval run as user E2 user for context what logs do you want to collect and we're saying we want to collect the production log and we want to collect the shuroken log which we don't have in this case if there's an Aus SDK log which we have in our app our main app but we only want to collect One log here but I don't think it's called production log I don't think that's what we configured it as if we go over to our Puma file I'd like it to be the production log and we could probably do that if we just tweaked our Puma file so here we could just call this um like we say redirected and just say production. log we could probably just write production. log and production airor log but I'm going to leave this alone I'm going to keep it with the Puma one because I just want this to work and I don't want to fiddle with this too much and we could collect both logs but I'm just going to collect one here I just want the one that works right now and so here it's saying the file path and then this one's going to be log puma. St out. log and I'm just going to name it the same puma. St out. log here we have our log group name so where is it going to log this uh I think we're calling this what is it examples cloud watch or maybe uh Agent app I don't know that's the path I think that will still work um so I don't think that's an issue we might have to create our log group before we uh go ahead and do that so that might be something that's very important for us to do so in our actual template yaml file that's something we'll probably want to do is go back to our template if I can find it here where is our template our template is not the Shogun directory it's here in the template and so we're going to probably want a log group and a log stream so I'm going to go here I'm just going to ask chat PT to do this because it will just save me some time okay uh so and it's also a very small thing and today I'm getting lots of cloud flare things it doesn't like my IP I must have done something it doesn't like yes I'm a human let me in please so say here uh create a cloudwatch agent sorry in CFN create a cloudwatch log group I don't think we have to create the stream we just have to create the log group okay so this is very simple yeah there you go it's as simple as that so go ahead and copy this I like that we can put the retention in so going to paste this in I'm going to leave it for uh 7 days just in case and we'll just say log group okay this one is going to be examples Agent app we could totally pass this reference the slog group name into the other one I'm not going to do that it's just too much work it's not our Focus here today and I'll take that part out that part out that part out fix the indentation and so it will create a log group and here it will collect the stuff I think in particular we're telling to use um nothing interesting there we can also specify metrics so if we want to collect additional metrics this is the time that we would do that um usually that we do that in a separate video but like in my older courses but I'm just to you here um so the thing is like uh these things aren't usually collected by default by cloudwatch logs and so if you want or metrics and so if you want these additional metrics like CPUs CPU usage and memory usage you would specify it in here so I'm going to go down below here and just take a look use percentage iodes free that looks fine so here's collecting memory swap information so dis information CPU information so here you can see that it's collecting more additional information that's stuff that I would probably recommend configuring that doesn't come out come by default with uh uh metric logging so anyway we have that and this instance ID will get inserted by the cloudwatch agent so we don't have to worry about that so our configuration looks good so I'm going to close this out I say looks good because I don't know if it'll actually work but let's carefully take a look at what we're looking at here so we have make a directory called collect what's collect D again is that just a format yeah it's a way of I I'm sure I have content on this sumwhere I don't remember everything that I do small Damon which collects a system information periodically and provides information so it's just probably the thing that collects information then passes along and so we're saying we're going to use collect D here today which um I think that that's no issue that should work and then we have a run script to copy the config and then we start the agent up so the only problem here is that there is no log file so I wonder if it will collect it when the file is created even if it's not there so that's something we don't know but we'll figure that out as we go there's a bunch of references here for things that don't exist anymore so we don't have Ruby we don't do additionals we don't do AI we're not installing npm we're not doing the Cloud watch deploy agent this one we are downloading code we're not not creating temporary directories we are installing gems we're not installing Puma we're not doing private access we're not doing this but we do need to have one for document um bundle install the bundle install one it is where did we put it maybe we never created the step forward here so when does it happen it's going to be download then do that okay great so uh for this one here we need to reference it so I'm going to copy this block here and we'll go down below this one will be uh install what we call the other one I like to match them to make my life easy this one is called yeah bundle install right and this one be run command while I'm here I'll just fix any other ones if I mess them up Ami so it's run command run command run command that's all of them great also this one shouldn't be here the launch ec2 instance again just going to clean this up quickly before I forget so we will take this out here on that one we'll go to the next one here clean that up of course if you're just grabbing my file it's a lot easier than that we'll go ahead and do this we'll go ahead and take this one out there we go and so coming down to here we have our bundle install uh we don't have any parameters for this it requires no parameters whatsoever I'm noticing that this one is um title case I wonder if that's going to matter in our other location here and so this one this particular document here is referencing the document bundle install and so the only thing that we're not doing here is we're not starting up the um the server which could do but I think I would leave that for us to do manually and even though this one fails we could restart the agent if that's an issue on the server okay so just scrolling up here and taking a look um I'm just seeing if I messed up those other parameters earlier no looks fine everything looks fine here so that's everything so what could go wrong here um maybe temporary directories need to exist the rails app is not uh started for it to collect information the configuration is incorrect maybe collect D Damon isn't on the Amazon L 2023 but I believe that it is um uh maybe this is supposed to be an array of IDs even though it's only one so we have a few parameters here that we do need to figure out um so I think we are in good shape here and again this this won't run until we trigger it so we're not telling it to automatically trigger it has to be manually ran uh in automation so what I'm going to do is type in Ls I'm going to CD into cloudwatch from here I'm going to do CD into agent whoops into agent oh it is logs and then it's agent um and then from here we will go ahead and deploy so we'll go ahead and attempt to deploy here I'm going to go back over to here we're going to go over to uh cloud formation and I'll give this a refresh here it it was cloudwatch Agent app did we not tear down the old one oh maybe I Never Tear Down the old one have we I've been running a server for multiple days I have been running a server for multiple days of course I have so we'll go ahead and delete it I'm sure you'd be more worried about it if you're running stuff for multiple days I'm going to just tear this down and then I'm going to set it back up okay give me it just a moment and it looks like our uh our deploy failed here so let's go take a look here what is our issue um oh no it oh sorry I was looking at this old one here so what the heck is this one load balancer resources that is a bunch of stuff I don't know what that's from I'm going to go ahead and delete it go get out of here oh it's the cluster okay I'll go ahead and delete that uh but that's an old one so I'm not worried about that anyway we'll go ahead and hit up we'll go ahead and say deploys and now we are going to attempt to deploy this again it was trying to update in place which we could have done but I honestly would rather just tear down and start from scratch um in this case so go ahead and execute this and we'll see if we get an error the chances of us getting error is very very high but I guess it well we'll see if there's a syntax here but uh yep there we go all so that was fast and we'll go back and take a look here it says the failed to create SSM roll run command stack SG log group roll back requested temp URL must be a supported URL so the thing is is that we are um what we need to do is I think we need to upload the other one and then reference the other stack that's probably what's going on here so just give me a moment here our Cloud foration section we cover this I don't have to do nested Stacks too often so I kind of forget half the time but if we go back to our main template yaml that's where we're running into an issue here and we look for wherever that thing is here run command stack right run command stack this is where we're running into our issue template URL so how did I do it locally though so I have it locally so I got go check back the uh the reper we got this from just a moment all right so this one's a little bit different what it's doing is it's first packaging it deploy uh deploying it so I'm surprised I didn't covered that before but um that is definitely a step that we will have to factor in here so yeah we need to package it and then deploy it okay so what I'm going to do is grab my package command bring this on over and we'll go back to our deploy script I'm just going to paste it in here above I'm not using a profile I'm just using this local machine so we'll take that out profile is when you have multiple profiles uh this one is going to specify the root template which I'm going to keep it simple I'm just going to say template yaml okay we need a cloud formation bucket so I'll go up here because we do need to put the artifact somewhere so say CF bucket and we have CFN file name what did I do for this one I get them as parameters okay that's fair enough um and then this one here is the output file file directory so here it's outputting um the package template we can just place it in place like this okay so we can just do that but the idea is that um the other one references the other templat so the idea is it's going to pull in that code and then package it as one template I think that's what's happening here and so we have our our bucket and our prefix um I'm just going to this for a second I want to go down to this one and so I think the only difference is that this deploy has to reference the packaged one right all right so we go down to here yeah it's just referencing the package template so we'll go back over to here and this one will be our packaged template and then um we need a bucket and a name so I'm going to need a new type of S3 bucket so I'll have to make one here just give me a moment and go back to our read me here and we created a bucket for the rails artifact I suppose we could just use the same bucket there's no reason why we can't and that would just also just save us some trouble here CU I don't want to have multiple things here so I'll call this s SW agent uh whatever whatever memb you have to use your own it can't be the same and then the file name here is what um well hold on here well why do we need that at all why do we need to set it there at all I don't know why but uh let let go take a look here let's take a look at this command maybe like I CU like it's saying send it over to to S3 but like the the it's packaging immediately and it's using immediately locally so why are we specifying that there and that's where my confusion is I just don't particularly remember and so if we go to S3 bucket the name of the S3 bucket with the command uploads the AR effect that are referenc in the template a prefix name command so I don't if these are actually necessary I'm going to take this out here because if we don't have to do it I'd rather not um but we might have to as a requirement even if it's going to take the package template from locally so let's go ahead and see if this works I'm going to go ahead and try this again I'm going to go back over to here and delete this one because there's no way that was going to work and we'll go ahead and try this out so we'll hit enter here and see what happens it doesn't like this line so we'll hit enter see what it wants so it does want those so regardless if we're going going to use it locally it's going to want those so we'll go back and put those in there okay uh I don't know package template yaml seems fine to me right it depends like if it's an artifact is it going to be a zip I can't remember so successfully package artifact and wrote to the template yaml execute the following command to deploy it and it's telling us you know that's what we would do and see how now it's specifying um no it's telling us to local to use the the thing so that's that makes sense um I'm going to go over to the S3 bucket I'm just curious I mean while we're waiting we can also uh execute this and before we do let's just take a look at the template um so we can't see it right now I'll have to execute it and then we can um take a look at the template but I'm going to go over to S3 because I want to see was that a zip or was that actually a yamal file that it uh created I don't care which which one it does I just want to know as I've forgotten and so it's actually a folder and it has the template inside of it so we don't need uh because it's a prefix right so we could have just said instead of being package template well we should have called this is um I would just call this CFN right that's probably a better name for it but anyway we we have it there now that's not a big deal but the template's there but let's go back over to cloud formation here and it's rolled back so it has an issue what is its problem it says doc doent uh install JQ so I'm I forgot to remove something here we'll go back over to here I'm going to search for document JQ JQ it's not this one so it must been missed in the outputs in the commands file here type in JQ it's this one here okay so I'll take that out and um I might as well get rid of this bucket because it's just kind of a weird bucket name I don't like that as a bucket name so I'll delete that and we'll say permanently delete okay so that's now deleted and um I'm going to go ahead and delete this stack and we'll go back over to here wait for the stack to delete shouldn't take too long should be like instantaneous and the stack is gone so we go back over to here and we will attempt the deploy again and so we'll go back over to here to clation we'll refresh we'll go into here we'll go into change sets we'll go ahead and click into this we'll execute the change set we'll execute the change set and so now we are waiting to see if this works and I have a feeling it's going to go a lot better so security grips created I kind of wish it created The Run command first because that feels like that's what's going to fail I'm going to wait here and be back in a bit okay all right so looks like we have another failure no surprise here um what is our problem so we have here document download code base not not found in stack document download code base okay um fair enough so we have oh this says npm install here this is supposed to be this there we go I might have just like cut and pasted wrong in here and that's our problem here so that might have been just the issue so go ahead back to our cloud formation stack it's not fun when this thing messes up we'll go ahead and tear that down and we'll wait for that to tear down now all right so our deploy is complete and I'm going to go over to my server it's been a little while for me here so I did step away for lunch so this should be already with status checks pass for me so if I go over to here let's go ahead and establish uh our connection here oh where's our instance what over to resources profile instance yeah there's an instance oh we weren't creating we were deleting yeah sorry I thought we we had a successful create here I guess not there was something we made a mistake with I'm assuming I fixed it already let's go ahead and deploy again oh what a shame and we'll go ahead and deploy again as this is always the tedious part promise the other labs will not be as big as this one we're going to go ahead and deploy and be back in just a moment okay and it's failed bu again this time it says document bundle install not found in the stack for the output I'm going to go back over to here maybe that's what we were trying to adjust here um and it would be in the yeah the commands which is the issue right so in here I guess we never created that one so we go ahead and do this and then hopefully we can just reference it as the same here I'm just going to make sure that it is called this I think we match the names I usually do that to make my life really easy uh sorry document bundle installed yeah there we go and so again I'll tear this down one one more time and we want to do uh this one here not the Nesta stack but looks like the Nesta Stacks supp ploying without issue so it's really just this top level one and deleting that should tear down both of them so we'll just wait for that to tear down all right so let's go ahead and attempt another deploy I'm hoping that this time we're going to be in better shape so we'll give that a moment and apparently there is a mistake with uh the syntax it's saying document bundle install where were we doing this template here uh that's line 161 so we'll go back all the way down to the ground here and we can see that it's a very obvious here thankfully it did not did it actually create that change set I think it's still tried to create it anyway but that's okay we can just tell it to ignore it so I'm going to go ahead and delete this one which will take seconds to delete because we've not done anything yet um I'll give this a nice refresh here and we will go back and we will attempt this okay we'll go and deploy this and that is deploying and I'm going to give this a refresh here we'll go here and change sets execute change sets we will give that a moment there to deploy and hopefully this time it works okay and it still didn't work let's go take a look here and check our outputs so go over to events uh resource handle request and valid request parameter P gem version not installed because we're not doing that anymore so I'm going to go ahead and search for this this is the issue with our gems as it is trying to pass in parameters but for this we don't need this okay so um I don't need any parameters to be passed into here okay the other thing is I'm just going to go take a look here and take a look at this document making sure that document install gems doesn't require that as well and it doesn't there's nothing to pass along so that was just an issue of parameters I think so we go back to here it's saying not declared and that's makes sense because we don't actually declare any so I'm going to go back over to here delete our stack and I'll be back here when it's deleted we'll try again okay all right our stack is deleted let's go ahead and attempt a deploy again and eventually our deploys will work here so we'll go back and this is just our us ual T tedious tediousness that we have to work through so I'm hoping that this time it works let's go ahead and execute the change set and we'll find out all right there we go and it actually finally uh deployed without issue so we are now in good shape let's make our way over to ec2 and I'm waiting for it to pass those two stat checks which it has we'll go ahead and connect to it now the thing is is that uh we do want to use systems manager but before we do let's just make sure that um I guess actually there wouldn't be anything on the server because wouldn't ran the Run command yet but we'll go over to systems command Okay systems manager systems manager actually we already have it open up here in a tab no no that was sessions manager so we'll say systems manager and we go over to here on the left hand side if we go to automation because we've created our own here and we can go ahead and execute an automation so there should be one that we created we say owned by me and there it is configure app document I'll click into it and so the idea is that I should be able to execute it what's cool is that it shows this I don't remember this being here before so that's really nice to have that visualization we'll go ahead and execute uh this here and we have SIMPLE execution that's exactly what I want I mean technically it is a manual execution in a sense but um no not really because I I would assume that means yeah step by step you'd have to run it uh which not a bad idea but here here we want the artifact path and surprise oh no it's blank it's supposed to be blank file name is fine the bucket is fine here uh we need to choose an instance so here is the instance that we are choosing so it's nice that we can apply this way the only thing that I'm surprised about we go back to here I uh had a field here for the ec2 instance find all instances we selected it oh okay cool like it auto detected that and that wasn't intentional that's just what happened so that's really cool um that it selected that and made that really easy I wonder if it actually pass it as an array but even though it's said as a string so I'm not really sure about that um I don't care about an alarm but you can see we have options for that we're apparently we could execute this as a CI command so it looks like we could run use this link to trigger it but I'm going to go ahead and hit execute I'd rather do this via the console here and the idea is that we're going to see what happens and already it's failed that was fast so I'm thinking that it has to do something with our input so let's go take a look here and see what's happening steps failed when it's validating and resolving the step input failed to resolve instance ID to string list instance ID is found to be a type string please refer to the automation so that is where we're running into an issue right away we're going to go back over to here and I don't know if we can change this easily but I'm going to try and so under our document we have parameters for our template and I'm going to see if I can just change it to string list okay so if we go to here I'm going change it to string list because it says that's a type and so I'm hoping that it will automatically make our lives super easy so we'll go back here I'm going to try to deploy in place and see if that works go ahead and uh hit up and deploy I'm going to go back over to cloud formation just make a new tab here and we'll say cloud information and we will click into here we'll go to change sets we will accept the change set to execut it and hopefully this rolls up without error so we'll give it a moment all right so that update has now applied we're going to make our way back over to systems manager I have a sessions manager open here not that we're doing anything with it as of yet close this tab out and we'll go back into uh this here and we'll see if we can now uh select them so since this one doesn't exist we'll click back as it might have generated a new name this might have changed a little bit these ones are owned by me so these are all documents but I want an automation document so we'll go over to Automation and we'll go um so that one failed that's totally fine so we'll say execute automation own by me we'll select this one we'll go ahead and hit execute automation we will have simple execution and we will select this one and so now it should be a string instance all of these are fine we'll leave them alone if you need to change them change them we'll go ahead and execute it and hopefully this time we'll get past the first step because the first step should be really easy and so that is now running the idea is that this should be logging out to cloudwatch Let's whoops I did not mean to close that tab which is totally fine we'll go over to cloudwatch and this should have created a new um uh a new log Group which is based off the name CW agent or whatever we call the stack which is C ad of us CW Agent app so I'm going to copy this so I can easily find it as I believe the name will be based off of that we'll put that in there and so I don't necessarily see one here so is it logging so I might delete out a few of these other ones so I can just see what's going on here as I do have a lot of logs as I've been doing stuff in here so I'm just clearing out one so I can make sense of what I am looking at just deleting some here uh kesis Lambda again trying to find this one that I'm logging right now store Gateway so that one was created but it's not the one that we're looking for so I don't see it logging but maybe it will eventually log okay I don't think there' be anything in here no okay so I'm not sure why we don't see any logging but that's totally fine we go back here as it is proceeding without issue so it's saying installed the dependencies we click on these steps and take a look at what's happening so it is showing us the output of what is happening so if there was an issue we could go investigate here which is a lot nicer than what we were doing there before but you can see how you could use this or ec2 image Builder if you've done the ec2 image Builder section U because I know how to use this I'd probably just stick with this um you see image Builder is nice as well so I'll click into this one and I knew this one would take a bit of time so I'm thinking that we increase the timeout for this one if we didn't we're going to run into an issue here so if we go and take a look here I think it was bundle install where I think I told it to uh take longer so maybe maybe that's where it is let's go back to here so it shouldn't take long to install gems that's usually pretty fast but anyway we'll just wait and see what happens okay so I thought we were going to fail the gem but where we actually failed was on the download code base so you know I don't know if it's because that pre uh prefix was the issue but let's go take a look and see what our issue is so we'll go into here steps failed when it is verifying the command is completed one thing is does the server have permission to download it must because we we gave it to that before uh returns unexpected invocation results status failed it's not explaining why steps fail when it's verifying the command has is completed okay so that's a little bit less information than uh I was expecting let's click into here does this provide any more information so I'm just clicking through and trying to take a look and seeing if there's somewhere else we get more information so that is the name of that that and that so the thing is is that if there was a problem we could tweak this now here's a question do we have any logging I know that we didn't see logging before I'm going to check again as I just think that it might magically appear for whatever reason I think we already have cloudwatch open here so I'll go back to uh my log groups and see if there's any additional logging as logging is turned on but I'm not seeing any logging so I'm not sure if this is like a permissions issue thing that I don't have um automation permission to or the Run commands to send logs there but for whatever reason they're not sending logs there I'm not sure if that would even provide more additional information as you can see we have the input parameters for cloud watch uh config up here maybe it's just indented at the wrong level and that's why it's not working it's considering as an input parameter as opposed to um a configuration so that could be an issue but we don't know why this is not working H and we saw before when it executed a shell command it would show us more information let's go take a look at what that looks like so we'll go back to our commands here and we have this command here it's running a shell script right and if we go over to here so I'm trying to check this if this could be the problem in any way I don't think it is and then after that it has to create the app directory and then unzip it and then remove the original artifact so nothing is standing standing here telling me that there's something wrong but I'm going to just take a moment to debug I'm going to pause here and figure it out okay so yeah no information but um one thing that we could do is we could go into that run command I'm just going to close a couple tabs out here so we're not too confused with what's going on but but um here it failed I'm going to click into this command and I'm going to see if we can just tweak it here right so if we go into this particular run command uh if we go here is there any way we can adjust it or do we have to always copy it it might be easier to go down to our documents our shared resources here we'll go owned by me and there should be a document here for download download codebase here it is um if we go to its content we can see the content of it so here it is it's in Json which is not very nice it's converting it from Json to yaml and what I'm thinking is that maybe it's the artifact name that it's an issue with that unless it's the a CLI but we know the CLI should be installed I'm going to go confirm I know we had sessions manager open here just a moment ago but I'm going to check again and see if the a is installed just to make sure to rule that out so I'm not wasting my time here so we have pseudo Su hyphen ec2 user and while we're in here I'm going to go ahead and type in what UHS and is a CLIA there it is there okay so the CLI is there so that should not be an issue of the CLI uh one thing I'd like is actual logging so I don't understand why things are not logging so what I'm going to go do is go look at my template I want to take a look here where we're placing this option here says inputs it would definitely error out if it was somewhere else so it must be fine to be here not logging let's see if there's an option here I'm getting really tired of those complaints okay let me read about this for a second here it's suggesting if I don't have this policy that it's not going to be able to uh send log so I'm going to go take a look here into our Ro that we have here our SSM Ro for our server and we have this managed policy but I don't have the other one so let's go take a look at what that policy does that might be um what we need so I'm going to go ahead and go over to IM policy okay and from here I'm going to go over and search for policies so we have this one this one allows you to log create log groups and stuff like that so maybe it's this policy that it wants in order to do the logging but it would have been really useful for us to have logs if there's something more verbose so I'm going to go ahead and add this policy but for this to take effect we'll have to pull this down one thing that I'd like to do because I just don't trust um uh that script is I'm going to just try to hardcode the value so we have less of a problem here again I want it to work I don't want to fiddle around with this all day as we uh I think we've learned enough about um uh run commands but in here if we go down to wh the option is here uh no I want this one here this one here and we're looking for this one I want to rule out this path by just taking the hardcoded one so what I'm going to do is go in the front here comment this out I'm going to copy this one and I'm just going to hardcode the value in for now I might not even come back to this and fix it I don't think I care enough to do that but we will see what happens so I'm going to go and pull this completely out of here we'll say app. zip this one's going to be app. zip and this one's going to be app dotzip and this one's going to be app dotzip and then there's no question that that isn't uh that's wrong we need to bring in the bucket name so I believe in my readme I might have specified it my template yaml my deploy yeah uh right here okay so I'm going to grab this place this here and so now I'll tear this down because I want the ec2 instance to be basically reset I guess we could just restart the server I don't want to do that well that wouldn't even work anyway it would still retain it so we'll go ahead to CLA formation we'll tear it down again but we're making extremely good progress here um but yeah we'll rule that out and then hopefully we'll have logging when we come back here so I'll be back here when this is teared down okay all right so that was pulled down I'm going to go back over to here we're going to do our deployment again and know this time supposedly we have better permissions uh supposedly then we get logging uh supposedly we rule out that download issue uh and we're in better shape here and just before I do that I just want to check uh one more time our our template here and yeah we do have access to that bucket so it's not an issue of bucket access close this tab out because that doesn't matter anymore uh close this one out this one out and we'll go into here and execute this change set I'll be back in just a moment okay all right so that is now back up the idea here is that we want to go and run our run our automation against um that there so I'm going back to systems manager here we're going to to go back to automation ones that I own which is execute an automation owned by me this one here here and even though we can enter in um I'm just going to execute this even though we can enter in these options they don't actually matter because we've hardcoded the values in here we're going to select the instance we're going to execute and then hopefully this time we're going to have better luck or at least we'll get some information out in cloudwatch so we have an idea what is going on with this fails okay all right so uh better success this time it downloaded the code so so obviously I think that it was probably that prefix which was the issue because it was blank and maybe it considered it a null character or something um so we can go back and tweak that I don't care bundle install failed I think it probably because it timed out so obviously it's not able to do the next step unless it is trying to do the next step we don't know I'm click into this one see what information we have not a whole lot it's weird because last time it failed I guess I succeeded it gives it a lot of input but when it fails we don't get a whole lot of information let's go over to our um let's go over to uh cloudwatch cloudwatch Al it's already open somewhere here and so maybe it has now successfully logged as it now should have sufficient permissions to log if that was our issue before and assuming we configured it correctly and it looks like that we do have logs now which is good uh so we'll click into here we have a bunch of stuff and so the one that was failing was install bundle so it says could not locate gem file okay well that's clear and that's really nice while we're here we might as well take a look at some of these other ones so yeah we get different kinds of information but you can see that it is showing that information uh as it's going there so it does not know where the gem file is okay so I'm going to go back to our commands here and take a look here well we can just tweak it in place because I don't feel like we need to we'll tweak it in place get it to work and then we'll go back fix our template and then hopefully cloudwatch uh the cloudwatch agent works but anyway so what we want to do is we want to go back to systens manager documents get there however you want to get there we'll go to Own by me and in here we have the bundle install and I'm going to try to edit this in place so if I I see the content here it looks fine we'll go ahead and say create a new version um not sure how we need to touch that going leave that alone and so here it's saying huh so I could have swore that we gave this the full path I'm not sure why we don't see that there so that's one thing but I can understand why this is having an issue so we'll go back over to here and vendor bundle vendor bundle well it's definitely not that so I think probably what happen is I tried to do a copy and paste I wasn't paying attention and it probably copy and pasted the wrong thing so I'm going to go right click copy so finicky here right click copy right click paste so that's one issue okay so that should be corrected so we'll go ahead and copy this first and I'll go back over to uh here and we'll swap this one out because this one is definitely wrong okay the other one is we need to do a bundle install but we're not in the correct directory so how can we do this as one command that's always a bit tricky um I'm not sure if we had any other examples here it'd be really nice if we did here we're showing that we have multiple commands the problem is we need to be in the directory at the same time um I'm not sure if this will work but I'll try it anyway I'm going to uh tell it to CD into that directory okay I'm going to say CD slome SL2 user slapp and I say and do this so hopefully that will work we'll create this new version I want that as the default version by the way not sure if it's doing that or not but I'm going to go ahead and try to execute this document so we'll go back to systems manager here and instead of going to automation we're going to go down to run command and so for run command we'll hit run command I'm going to go name of the document this one is going to be what is it bundle bundle saw sometimes it's easier to go down to documents go to the document and then launch it from there as I find that very frustrating any other way to uh figure it out so we'll go here we'll say this we'll run the command we will choose our version which is two we'll choose our instance which is this one here there are no other parameters to apply to it I'm not going to enable the S3 bucket I don't care about that um I do want the cloud watch logs here I want this to go to the correct place I'm going to go ahead and grab uh that path which is in our logs if we go here I know we hardcoded it so it should go no actually it would go based on the name of the stack so I'm going to go over to stack here for a second confirmation stack maybe better just go to Cloud watch and take a look here this is what we're actually looking for this this right here I'll go back over to here I'll put that there so it logs to the correct place I'll go run and we'll see if this runs and you know my expectation is that I need to increase the timeout for it but maybe this will uh fix our issue here by having that it failed still which is unfortunate let's see what the issue is here um output you're replacing the current local value of path which is currently vendor bundle vendor okay I don't see that as a problem could not locate the gem file still a problem huh okay so when you have an issue like this the best thing to do is to go into um the actual instance so we'll log into the instance here and and we're going to go over to here and we'll connect to this instance we'll go to sessions manager here okay we'll give it a moment we'll say uh pseudo Su pseudo ec2 pseudo Su hyphen ec2 user actually you know what I'm going to exit out of that I want to be in this user because I want to test that command that's running right so if we go back over to commands and we're looking for the gem install no no it's the uh config here the idea is that we're going to copy this because this is what it's doing it's going here and it's logged in as a user probably whatever this default user is we go back here say who am I I'm the SSM user right and I paste this in and so this is what this thing is actually doing so this is the command that has to work so we hit enter here and it's not completing the reason why is it's missing the double quotation here so just I hit control C to do that and so it looks like it ran it didn't complain about anything that doesn't mean it necessarily worked so I'm just going to go ahead and just cat this out or just say um not cat we'll say LS this directory here as this should be uh accessible to stupid paste so I'll do LS home ec2 user and then from here we should have app okay so let's go here do we not even have that oh I don't have permission I have permission to do whatever I want I'll say pseudo LS don't tell me I don't have permission I have permission okay so we have app here oh oh okay so when it unzipped it it's actually appen and Shogun so that's why I can't find anything because even if we seated it into the correct directly for the bundle it's going to run into that issue here okay so that's actually not a problem with that that's a problem with our um unzipping here so we unzip it it's um it contains the folder hm so that's probably more of a problem with our upload script because here I don't want the folder name I just want the contents of the folder and that might be fixed I think it's a little bit different because when we did this we unzip it and it is Shogun I think and that's fine but I think what we need to do is we need to go into the directory zip its contents I wonder if we can change this so what I'm going to do is I'm going to take this off here we have my path and then we're going to put uh shun here is that going to work cuz now we're inside the directory which is fine I think that will work I I think that will work so I'll change this out like this and I'm going to try to do this upload again and see if it works okay and so what I think has happened here if I've done this right is now if I unzip it that will be the contents of it the only way to know is to actually download it and find out so I'm going to go back to my bucket give this a refresh and I have my new zip here I'm going to download this one and however you have to on your computer open up that zip okay okay and so I've opened up the zip and now it just contains that and so that means that when we unzip it it's going to unzip to the correct location so unfortunately because of this pain we will have to tear down the server one more time we're so close for this to work the only thing I don't know is if that CD part will work correctly but I'm hoping that it will uh we'll still have to fiddle with that a bit so I'm going to tear this down I'll see you back in a bit and we'll deploy again okay all right so that is now tear down now we will go ahead and do our deploy again and this time I'm hoping that we are going to be in good shape okay so I'm going to go here and we'll accept this we will wait for this to deploy and then we will run our automation okay all right so this is deployed again we'll make our way over to ec2 making sure our instance is running with uh passes checked let's close a few of these tabs so that we don't have too much of a mess as we do have a lot of stuff here um and I'm going to go back to automation we're going to go into execute automation owned by me choose this configuration you're starting to get the pattern here execute automation we'll go down below choose your instance leave those things blank exe execute and hopefully uh this proceeds B past the bundle install or we get something different here okay so we have failed on the bundle install again no surprise there it's the last one we have to get working before we get to our configuration and so it's not telling us what's wrong but we click into it maybe we'll get more information uh any more information not here but let's go back to Cloud watch if we still have it open I'm going to go ahead and type in cloudwatch and I'm thinking this time it will actually tell us what the issue is probably going to say JUMP file not found is per usual as it does not seem to know where that thing is but we go into here here and we check our log streams this is probably yeah the last one we have STD out so that's error mean something didn't work could not locate gem file okay so one thing I want to know at the very least is did it configure the set to I guess it wouldn't have done the config bundle set we'd have to run it to find out so I thinking did the first step before it not work but I we won't know that until we actually run but install so it's that trick of getting that command correctly last time we checked here we didn't see it unzip correctly so I'm going to check that first so that was home ec2 ec2 user uh we'll put pseudo in front of here and I know this is app and so I'm sitting out there and now we're getting something that looks respect respectful I can't say that word today I don't know why so we'll go over to commands and our Command is oh did we update it in our bundle install like here maybe we just didn't an update and it actually does work so in our yeah I didn't update it that's why so it might have been working this entire time so we'll say CD slash home SL2 user slapp and then I'll do this and maybe this actually does work and it's just all my fault because I did not update this so we'll copy this I think this one looks correct here and we'll go back over to here we'll paste this in and we'll hit enter and we'll see if it works okay so that was clearly our problem question is how long does this take I'm just watching here it say uh I'm just checking my time here just want to know if it takes longer than a minute I'm just going to pause here as it's installing and just keep track of the time okay so I took about a couple minutes according to my clock or a minute and a half so the time out here says 6 600 so we do have a lot of time this one doesn't need 600 this one's 60 seconds so I'm thinking that um 600 is sufficient because it's about five minutes or more I don't know it's it's a sufficient amount of time so I think that that is fine um and so this should solve that problem now the next one we'd have to run it to find out I'm just going to double check here make sure everything's okay so this one looks fine to me so in order for us to do this unfortunately we're going to have to you guessed it tear it down and set it back up so uh at least we're getting somewhere here uh so I'll close out that tab there learning cloud is fun but this is why this video is so darn long just to show you all the steps here unfortunately uh no easier way to do this anyway so I'm going to tear that down be back in just a moment okay that is teared down let's go back and set it up we'll go back over to our Stacks here we'll give this a refresh we'll go over here we'll change the set we'll go ahead and execute the change set and we'll wait for that to complete our deployment is complete we will go back to automation we will attempt this again so we'll go back to systems manager automation here close some of these other tabs here and we'll go back to executing our automation book owned by me into here um we will will execute this automation we have values hardcoded so it's let's have an issue we just select our instance we go execute we're going to go ahead and wait for this to work and so hopefully this one doesn't a out I just hope that one works now weather configures it is another story um do we tell it to start here I think we do so in here we tell it to um configure it but then we have to start it I believe because we using the Amazon cloudwatch agent CTL and we're providing it a file we're telling it an N2 instance and it's we're telling to fetch so I believe that configures it to work here and so yeah it says Run start the agent so that's starting the agent okay so we will hold out here and hopefully this one works okay well well well well looks like we made it all the way to the end which is really good and so that indicates that the cloudwatch agent probably is running uh now I'm not sure how we collect anything because we don't have any logs per se let's go ahead and take a look at our running application we'll connect to it uh we'll go back to instances is that was probably just an old tab we'll go ahead and try this again and connect we'll go to sessions manager here but there is no application running right we didn't tell it to start up an application as we're just going to run that manually we could have made that part of our script but I think our script is doing enough as it is so I want to do app in here and and I want to do Ls I'm just want to LS the vendor directory to make sure stuff is in there I see bundle that is good I want to do LS hyphen log here and notice there are no logs which is totally fine we're going to go back over to here and I believe if I can remember we need to copy this command to start up our production I think we started in production I can't remember to be honest anymore uh but I'll assemble through it it's not a big deal we'll go ahead enter and so this is going to start it up in production mode and we have the secret key base missing which I was really surprised we never saw this sooner um this is usually something you need to set when you have this what I'm going to do I'm just trying to decide here whether I want to run this in production or not because we ran in production last time we weren't getting information but use of the secret ke key base is something you're supposed to have in a production environment and it's saying that we haven't set that so I'm going to do I'm just going to try to run it without the environment and see what happens and now it's complaining about no such file or directory temp bids server ID so it's getting kind of annoying where that's why I had like those temporary directories there before where it's like when you when we manually installed it we didn't have any of these issues right now it's complaining about things that like saying we didn't do these things and whatever um as to why I don't know but that's fine I'm going to go ahead and say um make directory um temp we have here it's saying oh there's no pids directory fine we'll make your pids directory there you go probably wants a sockets directory too let's make it sockets directory I had those in my automation script before but it's weird like how things just happen and they don't happen other times it's going to keep complaining about that key the way we could fix that is by editing it so we'll just say generate secret key base rails there might be a rate command for that there's like a rate command uh I don't think it matters what we set it as but I'm going go back over to here we'll try this I'll try in production first we'll say um this one is what it wants bin wants us to go bin rails credentials edit uh what's the problem adding master key to the storage encryption if you lose the key I don't care I thought we can edit this try this again no visual editor marked okay so we have to mark a visual editor so here I'm going to just say um editor like where is vim is vim installed here it is so I'm going to say uh export editor Vim because it wants an editor you don't if you don't know Vim you don't have to set it can set to something easier something like Nano but you have to learn and look that up if you don't know Vim look at my Vim course as that is the way that we do that editing it couldn't decrypt the credentials yaml per perhaps you per pass the wrong key I didn't set any key the key is right there if you lose a key no one is including you can access anything encrypted with it ignoring the config master key so you don't end up in your commit history um I'm not sure what it's talking about the key should be right here we go here master. key that's what it would use to open it up not sure what else it wants and so this one's really annoying it's like anyway we'll try to launch it up in development here again it doesn't matter if it's in production we just want it to log right well actually we do want in production because otherwise it makes really busy logs so I guess we do want it to start up like that let's see if it'll start up now uh we're having less of an issue you're running SQL light in production this is generally not recommended you can disable the warning by going here that's crazy like we didn't get all these things before so I'm going to go ahead and copy this command here uh unable to load application active support message invalid uhhuh what the heck is going on you're running scho light yeah I don't care about that unable to load application active support in Ballad message this is crazy we'll go here we'll copy this see if I can copy that doing control C here nope can't copy that try that again rightclick copy right click paste okay and this is the discrepancy between development and launching ec2 instance configuring manually and having automated scripts do it uh running rails app and haven't found files and master key credentials and also so it seems like it's coming back to that issue I'll figure this out be back in just a moment okay all right so when it says it sets up a new project it creates that Master dock key now I never noticed that here if we go to our config do we have a master key in here if we go to this one config yeah there's no master key I guess because you don't need it in development but we have uh this here like there's this encrypted file here um so what they're suggesting is delete it and then then try it again so I guess I'll do that I'll say remove config uh what is it called credentials yamel encryption because we have a new master key here right and so maybe it created before that we'll try this again there we go and so it wants a secret key base and it already generated one out so we have one here so I'm going to go hit Escape colon WQ and so it has everything that it should need to run now so go back and we will attempt to start the production server again and I'm hoping that we don't have an issue now and it doesn't look like we're having an issue okay so um I know that's confusing but hopefully that was uh not too hard to follow along what I'm going to do here is I'm going to go back over to here because we wrote an app if I can still remember what it was back in the uh the old days when we did it at the start of this tutorial or or lab or whatever you want to call it uh we needed our instance ID and we had a script here I believe it was a rake script here and here I could substitute out this no I don't want the instance ID but if I go back over to um get this public IP address the idea is that this should start logging or collecting the logs and sending them somewhere so how do we know where it's going to send the logs well well we set that up if you if you remember in this adus folder in this configuration here and so it should grab this and put it into examples Agent app Puma right and we already created that log let's make sure we have that log before we confirm anything else all right and we'll go to our log groups and so in here here we're expecting it to log out to here now we do see it here which is interesting so I guess it must be working but that's weird because this one's set to one week um and that's the one we created but then it created one here so I thought I was going to call the file there I guess that didn't really work out as expected I guess the cloudwatch agent has the ability to create it ah yes yes okay so I see I see my mistake here so I named name the log group name examples Agent app Puma STL log uh well that's not bad I suppose so I'm just thinking here like if we had multiple files it had instance ID that would be the stream name we wouldn't know what it is so I guess it's fine um but basically in our Cloud information template us creting that log was completely useless because it's not being utilized here or we'd have to name it exactly as the one that is here anyway let's go back over to here because it seems like it might already be logging we go here yeah it started it up so um cloudwatch agent is working it's configured correctly that is really good uh what I'm going to do is now go back over to here as we have pasted in our IP address I believe that is the one that we grabbed here 15 175 uh 157 621 176 it is let's go ahead and run this we'll say bundle exec rake I'm in the agent directory make sure you're in the agent directory I just point the screen as if you could see me I'm going to say run and so this should run the generator oh I got to do bundle install right and this should run the generator against that if you remember our generator from a long time ago it's good that we have a rails app and some configuration and a generator because we can definitely use this for other things for devops and sis Ops and things like that so it doesn't hurt to have the stuff around here I'm going to go ahead and say bundle exact rake run and it is lo loing stuff so it is running so that's great I'm going to let that run for a little bit I'm going to go over to here and we should be able to tail this if we say start tailing if some it's going over to the live tail option over here and if logs start happening we'll see stuff and so I think it's tailing right now now I don't have to make a separate video to talk about tailing right so you can see in real time we're getting more stuff okay so I'm going to let it log for a little bit because it wouldn't hurt to have some data here um that we can use in another video but I might not end up using it I might end up exporting this data here okay and yeah I'm going to stop this now but you can see cloudwatch agent is working we could talk more about like like if something went wrong and how we started and stop it but I mean we achieved what we wanted to do here and this is all you need to know in practicality I do want to export these logs in case we want to use them for something so I'm going to say download as um it's not going to be very useful I was thinking like I could download the logs and then reuse them later but it's only going to give me Us in this format so this is not very useful I was hoping In The Raw format but no it's not that that's not what I want so I guess we'll call this done export data T3 oh that's not a bad idea try this I'm going to go to the bucket that we created I do not need a prefix I will export it out please check cloudwatch has granted permissions to oh boy oh I just want to export the data out uh so let's go over to S3 bucket cuz I just want to download and then place it into our repo so that we have it for later in case I want to pull it for other cloudwatch if you want to analyze it um so I'm going to go into the CW agent here I'm going to go into I think uh bucket permissions and permissions permissions here and we will set a bucket policy and what we want to do is allow um access to uh reading so here is really the principle that we're interested in so um allow Cloud watch logs to export to S3 I just don't know what the principle is for it Jason permission bucket policy I got to go ask chat GPT I don't feel like figuring this out all day but what I'm looking for here is there's some kind of principle that we need to supply to it and it's some probably some generic name like cloudwatch log for yeah service there it is that's what I wanted um and I'm going to just keep it very broad here I'm not going to tell exactly what log group here but if you wanted to you could do that if you want um and we do have to specify the resource here so this is what I'm looking for here I'm going to go back over to our bucket I'm going to replace um some of this here like this as such I'm going to indent and I'm going to go ahead and do this I'm going to grab this here I'm going to put our is it the full thing that's the full Aron yeah so going to have to grab it like that like that and like that all right and that's should give us permissions we'll go ahead and save these logs we'll go back over to cloudwatch we will attempt this again and so it should have worked view the export task I'm not sure how fast oh very fast okay we'll go over to our objects we'll take a look here give this a refresh whoops give this a refresh our log is exported logs I just want to make sure that it is a file it is a gzipped file not exactly what I wanted but I'll go ahead download this and I'm going to drag this into um into here okay so I'll go ahead and drag this into agent can I open this up this gz file no is there something that for like I can view gz gz gzip um uncompress Gip file gunzip so this is how we could decompress it I again I don't know the contents of it I'm hoping that it's a single file we might end up making a big mess here it's not a big deal um and so we'll go 0 did it uncompress it probably easier to go here and see um um oh right here and oh that's junk it gave us the first line that's not what I wanted let's go back over to here let's try this again did I not export it correctly let's go back here let's refresh the logs there's just the one log uhhuh Why didn't it give me all this data let's try this again one more time so go here I'll checkbox it uh actions export it to S3 we'll choose the bucket we've been using for this I already forgot what it's called CW agent try this again oh you know what it is hold on um yeah like what is the last log I don't know can I can I be more generous here and just be like from a time that doesn't exist to a time so today is the fourth so if I select if I go to the fourth to the six how about ads you make it so I can easily just selected here so relative time start time should be either earlier than the art so how does this one work so this is relative time I don't know a week can I do a week out custom there we go did developer experience databus developer experience so I'm hoping that this one now exports I don't know what that one's called it didn't give it a name we'll go back to here is it just replacing the existing one exported logs um how do I know which one it is how do I know I don't know they're all named all the damn same um we'll go back over to here to whever cloudwatch logs export was does it tell us the name what if I click View and three that might bring me right to it hey at least that kind of worked we'll go into this here 6.8 KOB sounds better than the last one we'll go ahead and download this um so this got I'm going to delete the old one here and fix the naming I'm just doing this off screen you cannot see me doing this and I'm going to drag in the gz the the gunzip file back into here so we'll go and I'm going delete this one here delete I'm going to break bring on over this I'm going to drop it right into agent make sure I drop it in the right directory here agent did I drop the last one in Shogun maybe I deleted it which is what I wanted to do um so we'll do that and so now it's oh you know it it just gets rid of the old one ah there we go okay great so now we have it so let's just be like rename exported adus logs okay text file so we have that text file there and the idea is that in case we want to use it for something else like insights or something else we have it um but yeah I was surprised that we were super successful there I mean yes a lot of waste of time and Cloud information but nothing's wasted because troubleshooting and all these other tools you'll end up having to utilize them and they're used across these things here um we'll go ahead and delete that and and that will tear down the only thing we could have looked at while we're here was the metric we could redo that in another video maybe not make it super comp complex but um I'm going to tear this down and just confirm that this is teared down while that is going I'm going to go ahead and just save my changes so uh Cloud watch agent complete there we go and um what we'll do here is go back over to here refresh we'll wait for that to delete it'll be back in just a moment okay well actually before I'm sorry I'm let that delete still but what I'm going to do because I know it's not going to delete this over in cloudwatch so the logs that we did was created by the server so it's not going to delete um uh the the SD out one so I'll just go delete that manually it will get it doesn't ever expire which is not good we'll go ahead and delete that log group here so that we don't have to worry about that this one here was created by um automation so it's not going to get tear down either so I'll tear that tear that down manually as well all right and so again just going to wait for this to complete and I'll see you back in just a moment there we go that's teared down and I will see you in the next one okay ciao all right so we're taking a look here at Amazon event Bridge and just before we jump into it let's describe what an event bus is so an event bus receives events from a source and routes events to a Target based on rules so you have events going into the event bus the event bus holds those events then you have uh rules and those rules are going to uh select certain events to send to targets uh so eventbridge is a Serv event bus service that is used for application integration by streaming realtime data to your applications and this service eventbridge was formerly called Amazon cloudwatch event events you'll still see cloudwatch events under the adus cloudwatch console um but you can use either or name all right so now let's take a look at the core components of eventbridge so here I have a diagram that represents how it works uh and the first thing I want to draw attention to is the event bus itself so it holds event data and you define uh rules on the event bus to react to events and you'll notice there's actually three types of event bus the first is the default event bus you don't have to do anything to create this one uh if you have an ads account you have one by default I don't even think you can turn it off um but um anyway uh you'll always have at least one then you have a custom event bus uh and you'd want to make this one if you wanted to consume uh events from another Aus account and I believe that you can actually uh have it uh bring in events from multiple AOS accounts or maybe an organizational wide events then you have the ass s event bus and this is scoped to uh to be for thirdparty SAS providers then you have your producers these are just adus services that emit events so um uh whether you want them to emit them into the bus or not they are there for you to consume a lot of a Services just emit events um then you have the events themselves and so uh these are just Json objects and they travel through the event bus which is called streaming uh then you have partner sources these are thirdparty apps these are producers as well but they're just 30 third party services so you can see we have data dog signal FX off zero segment there's a a bunch of them there's more than just those four uh then we have rules these determine uh what events to capture and pass to targets you can have 100 rules per bus then you have the targets C and this is where the um events will uh uh eventually go to and targets are just ad services that consume the events and you can have five targets per uh rule so there you go so now let's look at the anatomy of an event for Amazon event bridge and all the top level Fields listed here will always appear in a single event and the contents of the fields appearing under just the detail section will vary based on the cloudwatch service that emits the event and when you're looking at cloud trail there's definitely a lot in there let's start at the top here so the version by default is always going to be set to zero I assume that this is just for uh name spacing maybe ad us in the future will'll make it one uh a you have an ID that's a unique value generated from every event you have the detail type this identifies fields and values that appear in the detail field uh the source this identifies the service that the source uh that's that Source the event you have the 12digit number identifying the ad's account you have the event time stamp you have the region uh you have uh the resource so this this could be uh ajacent array containing ARS that identify resources that are involved in the event and then you have detail this is adjacent object uh containing uh data provided by the service and can contain 50 fields nested several levels deep okay so there you go so now let's take a look at schedule expressions for Amazon event Bridge so you can create an event Bridge rule that's triggered on a schedule and you can think of it as a serverless Cron job so all schedule advents use UTC time zone the minimum Precision for schedules is 1 minute and event Bridge supports cron expressions and rate Expressions so let's look at those right now so cron Chon Expressions uh allow you to have very fine grain control uh but they use that weird Kon syntax uh so you'd have to go look up cron tab KRON job or or Etc um and I'm not going to really explain it here because I don't think it's necessary for the exams uh but I just want to show you that you have that kind of syntax there and you can specify it to run at 1215 Monday to Friday or every 15 minutes it's very very robust on the other side you have rate Expressions these are easy to set set but they're not as fine grained uh and as you can see you just pick uh a number and uh you have a drop down between hours minutes uh and days okay so there you go so let's take a look at event Bridge rules so this is the whole thing that we want to achieve right you create rules that react to events in AWS so you can specify up to five Targets in a single rule uh very commonly used Services would be Lambda function sqs SNS fire hose and ECS you can see there's a very long list so event Bridge integrates with a lot of stuff uh you may have some additional fields to select the target so if you do choose a Lambda function there uh you might have some additional Fields you can fill fill in like Lambda function Alias inversion whatever Target you choose you're going to get different fields you can specify what gets passed along by changing the configure input this acts as sort of a filter because when you have a a rule and it collects event you might not want to pass all the event data along and so we have a bunch of options here so for match events it's going to match the uh the entire event pattern just pass it along uh so basically this is just act as normal but we're going to look at all the configure inputs here in detail next so now let's take a look at configure input and again this is uh what allows us to filter out data that uh our rules match before we pass it along so match events is going to match everything part of a match match event is going to only part uh match part of the event text that you specify so if you put in dollar sign. detail it's only going to match it's only going to pass along detail um if you use constant this is basically mocked data so you can just provide any kind of Json object and that's what it's going to pass along um you have input Transformer this one's pretty interesting this is where you can kind of transform the text either into a string or into a Json object arranged differently so for this one here you can see that we have a Json object and then we print them out as very variables down below so it's a string and here um even though it's a very simple mapping um we could we could rename these or however we want okay to change that Json object so you map fields from event datas to variables then you use those variables in a string or Json object and that's what you get passed along uh there are some variable names you cannot use so you can't use Aus event rules AR Aus event rules AR or a events events okay so but but again you know you see dollar sign. detail instance you're mapping it to a variable called instance right so the the the one on the right hand side dollar sign is like selecting the base of the Json object and going. detail. instance okay so there you go all right so now let's take a look at schema registry for Amazon event Bridge so schema registry allows you to create discover and manage open API schemas for event bridge and before we jump into it let's just Define what a schema is a schema is an outline diagram or model and schemas are often used to describe the structure of different types of data so here um we have an event and and and and the structure is what we're storing okay so event bridge will monitor these events and they're going to put the structure into uh which is the schema into their catalog so there it is so you can actually click into that schema and now you can see that schema and uh you can view that schema and download the code bindings which we'll talk about here in a minute but why would you want to use a schema of events in your event uh event Bridge event bus so you can see if the structure has changed over time so you can see that uh the version is changed so maybe you expect event data to be a very particular way but then it's changed um this makes it easier for developers so if the developer needed to um look at different versions but another cool thing that you can do is you can actually export these schemas out so that you can use it with your code uh so that's what code bindings are so you can download code bindings for various languages uh and this makes it easier for developers to work with okay and just to describe what a code binding is a code binding is when the schema is wrapped in a programming object that is standardized uh that standardizes how to work with the event data in in the code leading to fewer bugs uh and easier discovery of data okay uh and just to explain how we get those code bindings well adus has this thing called the adus toolkit for VSS code if you don't know vs code it's extremely popular um uh uh code editor I think like 80% of the market now uses it I still use Vim but that's me and you can install extensions into vs code and that's how you'd install the itus toolkit so I'd go into vs code install itus toolkit it is toolip does a lot of stuff but it can do code bind code bindings for event uh for event bridge and so what you do is you type in AWS and you connect to AWS account and vs code then you're going to get this nice little icon here uh in your sidebar and then when you click in there you can you can drop down schemas and there they are and you can view the schemas uh right in there okay uh but you can also just download the code binding so there you go so Amazon eventbridge integrates directly with a lot of adus services but not with all of them because not all services emit a cloudwatch event so for the ones that don't what we can do is we can use cloud trail so over here uh this is um uh cloud trail all the data that You' see um and so turning on cloud trail allows event bridge to track changes to a Services made by API calls or by AWS users at the top there you can see we have a detail type Cloud we'll call it AWS API call Via cloud trail and adus API calls events that are larger than 256 kilobytes in size are not supported okay um so yeah it's just that's how you get uh integration with other services you just use it through uh cloud trail events so now let's look at event patterns for Amazon event Bridge so event patterns are used to filter what events should be used to pass along to a Target so you can filter events by providing the same fields and values found in the original events so let's say we have an event here and this event uh it's only here to react to an ec2 instance if it's terminated so look at that uh that that script there see the detail type says ec2 instance State change notification uh and then under the detail says State terminated okay so what we can do is we can create our own little Json object and we're going to map the same uh field so see how it has source and we have Source on the top there and we have detail type and we have a detailed State terminated so we're only pulling the ones that we actually want to filter out from the top there and what we can do in event bridge is we can choose event pattern and we can go custom pattern and just paste that in there and that's going to filter out um uh vents based on that okay and there's actually a lot more we can do with uh event patterns because there's a lot of operators that we have so we can do prefix matching so match on the prefix of a value in the Event Source we can do anything but matching matches anything except what the what's provided in the rule uh we have numeric matching so match against numeric operators we have IP address matching so match against available uh available for um matching against the IP address available for both ipv4 and IPv6 addresses exists matching so matching works on the presence of or absence of a field in the Json empty value matching for for Strings you can use uh uh uh just two double quotation uh uh double quotations there and for other values you can just use null and uh complex examples with multiple matching so you can combine a variety of these things there to do more complex event pattern matching so there you go so let's take a look at partner event sources for event Bridge so there's a list of thirdparty service providers that we can uh integrate into our into event bridge and these events will be emitted from the service provider into your event bus so here we have a list of service providers just to make it a bit more visual we have pager Duty data dog signal effect a segment not listed in those images there is off zero mongodb so the idea is that we're able to accept events from thirdparty services and that's just like a way of uh integrating with those uh those Services uh and that's a really cool thing to do so there you go all right so now let's take a look at cloudwatch Alarms so cloudwatch alarms monitors a cloud watch metric based on a defined threshold okay so there we have an alarm uh and so the idea here is that when the alarm is breached it's going outside the defined threshold and then it changes state okay uh and so for a metric alarm State we have okay we have alarm and we have insufficient data okay so there's those three different alarms so uh when it changes state then we can define what action it should trigger and so uh what we can do here is choose what we want it to do so we can send out a notification via SNS it can uh trigger an autoscaling group action or it can trigger an ec2 action such as shutting down a server okay all right so now let's look at the anatomy of an alarm so here is the diagram for the alarm and this is going to make sense here in a moment so the threshold condition it defines uh uh when a data point is breached okay so what you're doing is you're defining this line and whatever is above it is breached or below it those rules can change based on some conditions you have your metric this is the actual data we are measuring um and a metric could be something such as Network in okay so that's that you have your data point this represents the the metric measurement at a given period Then you have the period This Is How often it checks to evaluate the alarm which apparently is not true it doesn't mean the data becomes available it's actually always 5 minutes I believe um but anyway that's the period of data points that will be collected then you have the evaluation period so this is the number of previous periods and then you have data points to alarm so this is uh for one data point is breached an evaluation period is is going to go back four rules so this is what actually triggers the alarm that one you want to play uh pay close attention to there so there you go all right so let's take a look at the type of conditions we can set on a cloudwatch alarm so uh here is the form you you can see we have static and anomaly detection we're going to focus on static first so static is the most common type that you're going to probably use uh and then you can Define the condition of the alarm so greater greater equals uh lower equals or lower most people go with greater and then you can Define the threshold so whatever the value you want it to be it's going to be relative to the type of alarm you set based on the metric you're working with so an example is you could create a a cloudwatch alarm because you want to avoid unexpected charges uh and so in that case you'd use the estimated charge metrics you'd set the threshold type to static you'd set the alarm condition to Greater and you'd set it to $50 so there you go that's the static side now let's go look at anomaly detection which is a bit more complicated but very very cool so let's say uh you have reoccurring data points that breach a static threshold but this would not be considered unusual behavior so imagine every morning all your company employees log in so you have a spike of traffic for 30 minutes so using a static threshold type uh uh it would trigger an alarm uh but the thing is is that the state uh would technically be a false positive what do I mean by that well imagine if you just have this line right and so you have these these dips okay so the thing is is that it's not unusual traffic it's just that you're going to have those spikes every period of time so this is where anomaly detection comes into play uh so you what you can do instead of defining a single static line you can define a band that wraps around uh the actual uh data points there so the idea is that instead of it um being a single line the idea is that it works with that band and so unless something radically changes it's not going to trigger and that's a lot more flexible so there you go so now let's take a look at composite alarms so composite alarms are alarms that watch other alarms and using composite alarms you can reduce the alarm noise that's the whole point behind them so imagine you have two alarms and you configure them to have no action so you could set it on CPU and network in so the idea here is you're going to create those two alarms and when you go to configure the actions you just choose nothing you don't choose notifications autoc scaling groups or any ec2 actions they don't do anything uh so they'll show up like this where you'll see network in on the end there the actions say no actions and same thing with CPU utilization so you select both alarms and then what you can do is create a composite alarm and then you can set an alarm condition so there's a bit of logic there that you can use to do that and then once you do that you can go ahead and set when it should trigger uh but the thing is is that it can only trigger an SNS topic so it can only do a notification uh but that's a way for you to reduce alarm noise and work with uh multiple alarms so there you go all right so now let's take a look at cloudwatch dashboards and dashboards allows you to visualize your Cloud metrics in the form of visual uh various graphs and what you can do is create these widgets and configure metrics uh and add them to your dashboard so here's a variety of different uh dashboard widgets and you're just going to go ahead there and save it and so now you have a dashboard so there you go now let's take a look at cloudwatch service lens so service lens gives you observability for distributed applications by consolidating metrics traces logs and alarms into a unified dashboard so what is a distributed application it's also known as a distributed system but it's when Network isolated services or applications that have to communicate over a network together make up a larger system or application all right so applications that could be defined as a distributed system generally utilize microservices containers various cloud services such as compute and databases tied together using application integration Services uh and so service L integrates uh with cloudwatch with xray to provide an endtoend view of your applications to help you uh efficiently pinpoint performance bottlenecks and identify impacted users because when you look at Cloud Ser or sorry um Xray and service lens they seem like they're the same thing but um uh xray is just really traces where service lens brings all that stuff together metric traces logs and alarms let's just look at some of the things that we can do in service lens so what we can do is have a service map which displays your service endpoints as nodes and highlights the traffic Laten and errors of each nodes in the connections so if you look at this graphic here you can see there's a bunch of lines those are the connections and you have a bunch of nodes you click on that nodes you can see uh detailed information so service lens integrates with cloudwatch synthetics which we'll talk about in the next section here and service lens supports uh log correlation with Lambda functions API Gateway uh Java based apps for ec2 ECS eks and kubernetes with container insights and so to install and use service lens you need to deploy an xray uh instrument to your services you need to deploy the cloudwatch agent and xray Damon because it really does rely on xray quite a bit um and just a few other screenshots here I just want to show you so you we had that map view but if you want to look at in a list view I find that a lot easier to do but we obviously have the map view which we just saw in the list view it's easier to look at all the nodes and uh and see what services are being used um and just a few other screenshots here when we click actually into a node we get a service dashboard with a lot more information so you click into there you get a bunch of charts you click uh you click into the uh uh further you get more charts and then you can even look at the actual instances that attached so that's what they mean by observability you get all that kind of information uh so service lend lets us quickly Trace information to open up xray analytics so that's basically what we're trying to get to so here we have a bunch of filters uh and you can see um we have a bunch of information so we could look at Trace status and stuff like that and just open that we get graphs and then we can see all of our traces so again service lens is really tied together with xray but it makes it easier to explore and ties in all that other stuff so there you go all right so now let's take a look at cloudwatch synthetics so synthetics is used to test web applications by creating canaries to uh to test broken or dead links stepbystep task completion page load errors log latency of assets complex wizard flows checkout flows uh but exactly what is a canary so a canary is not what uh we refer to when we're doing deployment it's totally something else so where the AOS would reuse this term in another service but they do but canaries are configurable scripts that run on a schedule to monitor your endpoints uh and apis canaries mimic steps a real user would take so you can continuously verify the customer experience there shouldn't be an R only in there but that's what it is Canary's run on adus Lambda using the node.js library uh which in turn uses Puppeteer and if you're not sure what Puppeteer is it's a headless Chrome browser and an automating testing framework you can use Puppeteer to open a web browser and click and enter information into a website and headless means that there's no visible window so you don't see the browser it just kind of works so that's really what synthetics is it's puppet here and it has a layer on top of it but let's look at some of the things that we can do uh with synthetics here so when you're creating a canary you can use a blueprint use an inline editor import from S3 so here we have that example where you can choose between a blueprint uh editing yourself or importing your script there are four blueprints that are here and we're actually going to go take a look at all four of them here so you can kind of have an idea what you can do with synthetics so the first um blueprint we can use is called heartbeat monitoring this will be used to check a single page so what we'll get here on the right hand side you always get a script editor but generally you'll have fields on the top there so I can supply a single URL uh and then uh it will wait uh wait a while and it'll take a screenshot and then until the page is loaded and it's called a heartbeat monitoring uh monitoring because it checks continuously to see if the page is still alive and here you can set an option this options basically on all of them but this one it's more important where you can say run every 5 minutes uh and make sure that the website or the single page is still running let's take a look at API canaries so you can use this to check out an API endpoint so here uh what we can do is supply the get or post we can give the uh headers the the payload we can specify the actual endpoint we want to test and then uh you can see that again there's a script editor where we can change things if the endpoint is successful you get 200 if not um you got to code it in yourself but it's basically like the last uh one like the heartbeat uh one but this is for API endpoints then you have a broken link Checker this one's kind of cool so you supply a link and then it looks for the links on the page and follows them to see if those links are broken so it does a bit of crawling around really great for marketing websites not so much for a web application but uh so what you can do is you can supply uh the website and then you can say the max amount of links it will follow follow meaning click through uh there and so you can see that you can actually Supply multiple URL so even though it shows in the top there you're only supplying one website you can actually Supply apply multiple ones in there which is kind of interesting and it we'll log all the pages uh um so that you can go look at them later on to see uh the results of it through uh cloudwatch blogs um so there you go and the last one here is the gooey work workload or workflow Builder so test a sequence of steps that make up a workflow so here what you do is you set up a bunch of actions and you have selectors and text so it's a way of like selecting stuff and so here are the options that you can select um and that's your stepbystep actions is synthetics really that great no not really but um if you're looking for a hosted solution for puppeteer and doing the stuff it's not too bad um I would probably just use a custom script for this but that is synthetic so there you go all right so now let's take a look at container insights so container insights collects Aggregates and summarizes information about your containers for metrics and logs so uh here uh we just have a bunch of summary of a bunch of different information um about are containers so container sites works with elastic uh container service ECS fargate uh kubernetes uh or E eks KU is running on ec2 uh instance okay so metrics that the container insights collect are available in the cloud watch automatic dashboards you can analyze and troubleshoot container performance and log data uh for cloudwatch log insights operational data is collected as performance log events these are entries that use a structured Json schema that enables High uh cardinality data to be ingested stored at scale and container insights can be filtered by cluster node pod task or service level so you can see here all the filtration options you have just looking at some graphing information um here contrib uh contributor insights allows you to view top contributors impacting the performance of your systems uh and application in real time so that is that real time information so you're seeing time series data uh and this is based on Insight rules and here is a sample rule so you can go here and drop it down and say how to check for that kind of stuff so hopefully that gives you an idea but what you want to remember is that um contributor insights is for uh figuring out the or viewing the top contributors in packing performance so there you go hey this is Angie Brown from exam Pro and we are looking at cloud trail which is used for logging API calls between aw services and the way I like to think about this service it's when you need to know who to blame okay so as I said earlier cloud trail is used to monitor API calls and actions made on an AWS account and whenever you see these keywords governance compliance operational auditing or risk auditing it's a good indicator they're probably talking about adus cloud trail now I have a record over here to give you an example of the kind of things that cloud trail tracks to help you know how you can blame uh someone when some something's gone wrong and so we have the WHERE when who and what so the where so we have the account ID what uh like which account did it happen in and uh the IP address of the person who created that request the when so the time it actually happened the who so we have the user agent which is you know you could say it could tell you the the operating system the language the method of making uh this API call the user itself so here we can see Warf uh made this call and and what so to what service and you know it'll say what region and what service so this service it's using uh I am here and the action so it's creating a user so there you go that is cloud trail in a nutshell So within your adus account you actually already have cloud trail logging things by default and it will collect things for the last 90 days under the event history here and we get a nice little uh interface here and we can filter out um these events now if you need uh logging Beyond 90 days and that is a very common use case which you definitely want to create your own trail you'd have to create a custom Trail the only downside when you create a custom Trail is that it doesn't have a gooey like here such as event history so there is some manual labor involved to um visualize um that information and a very common method is to use Amazon Athena so if you see cloud trail Amazon Athena being uh mentioned in unison there's that reason for that okay so there's a bunch of Trail options I want to highlight and you need to know these they're very important for cloud trail uh so the first thing you need to know is that a trail can be set to log in all regions so we have the ability here to say yes and now no region is missed if you are using an organization you'll have multiple abis accounts and you want to have coverage across all those so in a single Trail you can checkbox on apply uh to my entire organization you can encrypt your uh cloud trail logs which you definitely want to do using serers side encryption via Key Management Service which abbreviated is SS KMS um and you want to enable um log file validation because this is going to tell whether someone's actually tampered with your log so it's not going to prevent someone from being able to Tamper from your logs but it's going to at least let you know how much you can trust your logs so I do want to emphasize that cloudt trail can deliver its events to cloudwatch so there's an option after you create the trail where you can hit configure and then it will send uh your events to Cloud watch logs all right I know cloud trail and Cloud watch are confusing because they seem like they have uh overlapping of responsibilities um and there are a lot of adaa services that are like that um but you know just know that you can send cloud trail events to cloudwatch not the other way around um and there is that ability to connect the two there are different types of events in cloud trail we have management events and data events and generally you're always looking at management events because that's what's turned on by default um and there's a lot of those events so I can't really list them all out for you here but I can give you general idea what those events are so um here are for categories so you could be configuring security so you have attach rule policy um you'd be registering devices uh it would be configuring rules for routing data it' be setting up logging okay so 90% of events in cloud trail are management events and then you have data events and data events are actually only for two Services currently so if you uh were creating your Trail you'd see tabs and I assume is when they have other services that can leverage data events we'll see more tabs here but really it's just S3 and Lambda and they're turned off by default for good reason because these events are high volume uh they occur very frequently okay and so this is tracking more in detail S3 um events such as get object delete object put object if it's a Lambda it' be uh every time it gets uh invoked so those are just higher there and so those are turned off by default okay hey this is angre brown in this video I want to take a look at cloud trail so cloud trail is a great way for us to um debug our applications but also uh keep audit Trails of all API actions that are occurring so I'm going over to cloud trail I just want to show you by default you're always going to have some event history here and so I've been working in this account doing a bunch of things I was recently working on Dynamo DB and I was actually provisioning these via cloud formation so it's very clear what is happening here we can click into this and we can get some information like uh the user that uh uh triggered this API call what was the event name um the access key not the secret necessarily but we go down here and then we have this structure so this is really useful when you run into issues um because sometimes with cloud formation if you run into an issue it doesn't always tell you all the information but if you go into here you're more likely to find it so I'm not sure if I can find an example of one where it failed um there should be like uh let's see delete stack might be it no no so I know like a few times I had some issues here like maybe this one but anyway the point is is that I just want you to always remember that cloud trail is something you can go take a look at to uh find issues there uh cloud trail Lake was not here before so this is definitely something new which might be worth for us to check out cloud trail Lake lets you run finetune SQL queries on your events that is really convenient because it'd be really nice to be able to uh search across stuff but right now what I want to do is I want to actually uh set up a um a trail in this A's account I'm not in my root account but if I was I would actually be able to um enable this so that it would track for all of my accounts what I'm going to do is go ahead and go to my repo here we'll programmatically turn on cloud trail so I'll just go type in cloud trail CLI as I'm looking for a CLI command I'm opening up a git pod which has um my adus credentials ready to go and I want to go ahead and create myself a new Trail so we'll go down here and we'll we go down to examples and we'll grab one here and I'm going to go ahead and make a new folder and we'll just say cloud trail okay and we'll make a new file here I'm just going to call it readme.md as we're just going to manually set this up and we'll paste it in just do it in the docs here today and I'm just waiting for Vim commands to show up because it's really mucking up my typing here and so we can set this to be um a multirail my trail say inabus S3 uh make make bucket make bucket S3 col SL my cloud trail AB 1212 here so I can first create a bucket create a bucket for cloud trail log so that'll be step one there we go and then I'll just apply this here your name will have to obviously be a bit different because these names are unique we could set is multi regional um I'm working just mostly in CA Central so I'm not really worried about it I'm just going to explicitly set this here but if you wanted to you could collect everything from everywhere which is not a bad idea uh there could be some other options here if we go up to the top so let's this topic include global Service events is multi regional enable log file validation so whether you don't want it to be tampered so that could be a very good thing whether you want to be Global so that' be an organizational Trail that's how you'd set it there uh we might want to configure it to stream to cloudwatch logs which is not a bad idea but right now I just want to set up a very basic Trail so let say create Trail my trail my cloud trail AB C Central 1 and I'm going to go ahead and uh copy this I'm going to just hope that this executes without issue incorrect S3 bucket policy um detected for okay what do you mean what we don't have any we don't have anything for it so let's go ahead here and I'm so used to just clicking through the click offs here that I'm not used to it but use an existing bucket uh okay did I name it wrong no so just go ahead look at this do we have to attach a policy or something here didn't put the dependency on the bucket policy when this was done so we need a bucket policy for do we need a bucket policy for cloud trail must have a policy that allows CL Trail to put logs into it okay well fair enough um so I guess we'll have to create a bucket policy we look up it CLI S3 bucket policy so I'm not going to remember how to do that we'll I guess it's just the put command here we'll go down to our examples and we'll grab this and we'll bring this over here as such I'm going to go ahead and make a new file just say bucket policy and by the way we'll just go ahead and set this up as basic for now and this is the bucket policy I want to have so I'm going move that into here same thing with the read me move that into here I'm going to just rename this to be. Json okay and um for our bucket policy let's go take a look at what they're suggesting so I'll just copy this one because this one seems like a specific for cloud trail I think it looks pretty big so let's just take a look at what we have here so we have ACL um I don't think we need ACL access I think we just need the standard bucket pulse yeah I mean I guess we give it access to both uh do we need to have both or just one yeah I okay I'm not sure if it needs both but I suppose that we can just set for both which is totally fine no no we'll take out ACL cuz we don't even have ACL enabled so I'll go ahead and do this and um we need to replace this with our actual uh bucket iron my bucket oh where we want this to to go e okay so we'll go over here and we'll grab our bucket name here and we'll replace that there and then we need to change the optional prefix I don't want to have an optional prefix I'll just take that out and then down below here we need our region so I'm going to go say Central one because that's where I'm doing everything I'm going to need my account ID we'll go ahead and grab it up there we'll paste it in there uh then we need the trail name so this one is called well we haven't made it yet but it's going to be called my trail assuming it doesn't mind that name so we'll go back over to here good and so this looks okay to me we'll go back to our read me so we'll say uh create Trail here create bucket policy to allow cloud trail to put to bucket and we'll grab our bucket here and this policy is uh local here but I'm just going to go ahead and say bucket policy we'll just make sure we CD into that directory as I'm basically nowhere right now cloud trail basic okay and so I'm going to just go ahead and copy this and hopefully that puts the bucket policy without issue this is not copy and pasting right now I'm going to just refresh this this is a g pod specific issue that I keep having and I can't and I've talked to get pod and they don't help me so I question my life okay so we'll go ahead and paste this in here hit enter and so let's go take a look and see if that bucket policy has been applied we'll go ahead to S3 here and we'll look for our new bucket which is right here and we'll go into uh permissions possibly and so we have our bucket policy doesn't seem to be complaining about about it so okay I don't think we have to turn on block public access to allow so S3 block new buckets and access points grants access to objects I keep wondering if that this means like it won't work for anything but anyway we'll find out here in a second so we'll go ahead and grab our Trail command and we'll give this a go hit enter incorrect S3 bucket policy is detected for the bucket uh okay well we'll go back over here and'll take a look let me just read this carefully I mean it didn't say anything in particular again we did not provide um this one so I mean we could try to give it both and see if that resolves the issue so we'll go ahead and edit this okay but we don't have ACL turned on so kind of seems silly to have that there but we'll go here and just copy over some of the stuff so CA Central 1 is going to be something so CA Central one and then we'll grab our ID here and then we'll grab the trail name the nice thing is that if there is an issue it would tell us down here below but uh we're not having any issues we'll go ahead and save that uh pacia has invalid resource great where you got to tell me where you can't just say that it's it's invalid um so I mean I guess this is the resource right oh my count ID this is not specified here so we'll go ahead and Supply that oh maybe that was our problem okay save changes uh okay why would it be a account put object resource policy has invalid resource oh well this problem up here as well first off let's just take this one out because maybe we don't need the ACL there we go now let's try this again so we'll go back over here we'll hit up still doesn't like it all right we'll go back over to our policy we'll edit it and I'm just going to carefully look here so R adus S3 my CR Trail AB uh 122 8 of us logs our actual count ID which is up here that's what they told us to put there um and again we're not specifying the um the ACL so I'll go back and put this in it's just confusing because again it's like ACLS are old and I would think that they would not require it so here I guess we're only providing access to the bucket and then this Aron is going to be the same so I'm going to go ahead and just copy this take out the trailing trailing there we'll save that we'll try this again we'll hit up there we go so I guess it does need the ACL interesting so our uh cloud trail has been created we'll go back over to here and we'll go into it's our unusual activity not worried about that here um but we'll go over to our Trails here we can see that we've created our Trail just notice that trail logging is off so I guess we'd actually have to turn it on for it to work um I'm surprised that that's just not part of the command but we'll go back here and maybe there is a separate command to do that I'm going to look for enable uh update update Trail maybe it was part of the flag when we had it at the beginning here and I just didn't notice it so I'm going to go look for enable enable log oh that's not what I want I wanted to enable logging so go back over to our trail trail logging is turned off okay let's search for logging there must be a separate command for this that could be a little explanation here so just going to carefully look at this and see where that command could be start logging there we go okay great so we'll go down to examples it's as simple as that that's nice we'll go ahead and turn that on so we'll say start logging all right and so that would uh start the logging we'll go ahead and do that enter excellent uh we'll go back over to um here and we will take a look at our bucket so we haven't done anything yet but we should probably trigger some actions so what I'm going to do is go over to Dynamo DB and create a table insert some records we'll just use click Ops for that we'll create a table just say my table we'll just say uh ID here we'll leave the sort key out of here we'll just go with the default we'll create that and we'll give it a moment apparently I must have not teared down a previous Dynamo DB table so I'm going to go ahead and do that I'm just trying to create some kind of activity here so that uh it will log okay so I can just delete this older older one that I forgot forgot to do in my last video um we'll go back over to our new Dynamo DB table we'll wait for it to provision here it is we'll go ahead and explore the table I'm going to scan scan scan scan scan scan can I'm going to try to insert some records in here so we'll go into here and explore the table and we'll create an item uh we'll say hello We'll add another tribute here string fruit apple create We'll add another item we'll say goodbye We'll add a new tribute Here Fruit orange we'll create that that looks really good um could we use the party que editor for fun we'll go ahead here and query the table we'll query it we get no results that is totally fine I do not care I'm just trying to do anything to trigger API actions we'll go ahead and delete this table or we can update it update the settings and we'll change something superficial this is the update page that does not feel like an edit capacity let's do that and I'm going to change the capacity down to five units here even sure why it's set to 10 we'll save that change we'll edit it again again just trying to make some changes here we'll go ahead and delete the table we'll say confirm okay and so we should have some activity now the question is how long will it take to log so cloud trail how long to log into S3 about every 5 minutes so we're going to have to hang around here for a bit and wait for some logs it looks like we actually already nope not yet so we'll just wait till some logs appear okay so I'll see you back here in a bit okay so I just uh went inside and had some crackers and now I'm back in uh back in here and I'm just clicking through my folders and you can see we actually have uh some data in here so that's kind of interesting notice their gzip files or we'll go ahead and do is download one of them or attempt to open them so I'm going to download that and then I'm going to go over to uh G pod and just drag one in and see if we can um see its contents so I'm not sure I think for some reason gitpod will let us just see the contents of it y there we go and so we can see uh that information this is pretty much structured the same as uh what we saw in the event history when we were looking at that there so if we go back over to cloud trail and we go into event history right we click into something like this it's basically this information okay so you know we have those records there this is not the easiest to um explore and so there are obviously other ways that we can work with this I'm going to leave our Trail around for now as I would like to uh do a few different things such as streaming into cloudwatch logs or uh if we want to query our data though I do find it interesting that we have um cloud trail Lake as uh previously the way that you would analyze your data is you would send it over to Athena and you could use utilize it that way U but this definitely seems if it seems to offer the same thing but more convenient uh Pace here so that might be interesting so we'll have to consider that um there is other things to look at for cloud trail so uh we weren't doing this but if you go over uh to here and we go down below there are different types of events so right now we're collecting all management type events so those are your common read and write actions that uh we've been um expecting to collect but there's also uh data events and so data events are um for things like S3 lamb to Dynamo DB where you'll have a high level of a transactional data or just lots of objects so like if you track every API call on an object that would fill your bucket extremely quickly and so you actually have to turn this on to uh get access to it because it gets really expensive very quickly so um that is not turned on by default but we could easily turn that on I'm not going to do that here today but I just wanted to show you that that those options are there inside events has wasn't there before so I don't remember this but I guess this is for collecting Insight data so that we can uh get some kind of hey there's something wrong with our uh our account kind of uh setting here so that's kind of interesting um but yeah I I think that is sufficient uh for now but we'll continue on on uh we'll keep that trail around and configure something different in the next video okay ciao so now it's time to take a quick tour of cloud trail and create our very own trail which is something you definitely want to do in your account but before we jump into doing that let's go over to event history and see what we have here so AWS by default will uh track events for the last 90 days and this is a great Safeguard if you have yet to create your own Trail um and so we have some event history here and if we were just to expand any of them doesn't matter which one and click view event we get to and we get to see what the raw data looks like here for a uh specific event um and we do have this nice interface where we can uh search via time ranges and some additional information but if you need uh data Beyond 90 days you're going to have to create a trail and also just to analyze this because we're not going to have this interface we're going to have to use Athena to really U make sense of any Cloud TR information but um now that we have learned that we do have um event history available to us let's move on to creating our own trail let's go ahead and create our first Trail and I'm just going to name my trail here exam Pro Trail I do want you to notice that you can apply Trail to all regions and you definitely want to do that then we have management events where we can decide whether we want to have read only or write only events we're going to want all of them uh then you have data events now these can get expensive because um S3 and Lambda the events that they're tracking are high frequency events so you can imagine how often someone might access something from an S3 bucket such as a get or a put so they uh definitely do not include these and you have to uh uh check them on here to uh have the inclusion of them so if you do want to uh track data events we would just say for all RS3 buckets or specify them and lambdas are also high frequency because we would track the invocations of lambdas and you could be in the thousands upon Millions there so these are uh uh sely not included by default um now down below we need to choose our storage location we're going to let it create a new S3 bucket for us that seems like a good choice we're going to drop down Advanced here uh because it hides some really good tidbits here so we can turn on uh uh encryption which is definitely something we want to do with KMS and so I apparently have a key already here um and so I'm just going to add that I don't know if that's the default key I don't know if you get a default key with cloud trail usually you'd have one in there but I'm just going to uh select that one there uh then we have enable log file validation so we definitely want to have this to yes it's going to check whether someone's everever tampered with our logs and whether we should not be able to trust their logs and then we could send a notification about log file delivery this is kind of annoying so I don't want to do that um and then we uh should be able to create our Trail uh as soon as we um name our bucket here so we will go ahead and just name it we'll say exam Pro Trail assuming I don't have one in another account okay and so it doesn't like that one that's fine so I'm just going to create a new KMS key here C keys do cost a buck per so if you want to skip the step you can totally do so I'm just going to create one for this uh here called exam Pro Trails okay great and so now it has created that trail uh and we'll just uh use the uh the site here and then maybe we'll uh take a peak here in that S3 bucket when we do have some data all right um I do want to point out one more thing is that you couldn't set the uh the the cloudwatch event to uh track across all organizations I didn't see that option there it's probably because I'm in a sub account so if I was in my if you have an ABS organization right uh and this was the uh root account I bet I could probably turn it on to uh work across all accounts um so we didn't have that option there but just be aware that it is there and you can turn um a trail to be AC cross all organizations so I just had to switch into my uh uh root organization account because I definitely wanted to show you that this option does exist here so when you create a trail uh we have apply to all regions but we also can apply to all organizations uh which means all the accounts within an organization okay so you know just be aware of that option so now that our Trail is created I just want you to click into it and be aware that there's an additional feature that wasn't available to us when we were creating the trail and that is the ability to send our uh cloud trail events to cloudwatch log so if you wanted to go ahead and do that you could configure that and create an IM rooll uh and send it to a log uh or cloudwatch log group um there are additional uh fees applied here and it's not that important to go through uh the Motions of this but just be aware that that is a capability uh that you can uh do with cloud trail okay he everyone it's Andrew Brown and I'm back here with more cloud trail so I've just have everything open up here I'm going to make a new folder in here this one will be called cloudwatch because a common uh strategy is to send your cloud trail logs to Cloud watch and so we already have an existing uh Trail and so I just want to use that one that's why I did not delete it in the last one so hopefully you did not delete it either I told you not to delete it um and so we'll go over to cloud trail here and there are some configurations that we can do for this so I think we just have to update our Trail for this so we'll go over to update Trail not sure what channnel is we keep seeing Channel okay we'll go over here and update um update our Trail and what I want to do here is just configure it so that it has uh Cloud watch logs so we'll see that we have uh these two parameters here so that looks like all there is for us to do so I'll go down and just grab this first paste this in here and we'll need our Trail name which I believe is is my trail I don't believe we need the bucket as we're not going to change the bucket so we'll go all the way back to the top here and I want to grab uh these two because this seems to be what we need to uh change our Trail here so we have Cloud watch logs uh group AR so that will be the first one and then we'll need the actual R AR to let us write to it so it seems like uh that's kind of important so we're going to go ahead and type in cloud trail Cloud watch logs roll permissions and maybe ad us has something for us uh permission policy required for cloud trail Ro to send logs to cloudwatch logs that sounds really good to me and so if we look at this following example is for permissions uh so uh that can log if you're creating a policy that might be used for organization Trails as well we're not right now we're keeping it nice and simple so we'll go ahead and grab this this will uh be our new file this will be our uh Cloud watch uh policy Json and we'll go ahead and paste that in here and so here we need to provide um what so we need to have well count ID that's that's easy we'll grab that first place that in here and here uh the lock group name I'm not sure if it's asking us to replace that I guess so because if this is log group that' be the log group name cloud trail log stream name prefix H okay I wonder if this is the prefix they were talking about before when we uh wanted to have one in the um whatever let's see here so the bucket name the service principle I'm looking for something here it's not super useful what it's saying here but uh let's go back to this did I I copied the right one right yep okay so I guess we should create the log first so I'm going to go back we'll come back to this uh configuration here but we'll say uh update Trail for cloudwatch logs we'll have to create cloudwatch log so we'll go ahead it was Cloud watch logs we'll look that up here it create log group I'm in my office and I have a space here it just keeps turning off and it's so cold it's winter anyway I'll go ahead and grab this sorry if you keep hearing in the background I just uh not sure if it's picking up on the microphone and if it is I apologize so I'll just say here uh cloud trail let's make a cloud trail so let's say my cloud trail and we'll go ahead and create that there typ it clear and I need to refresh this this is a complaining so we'll go ahead and do this okay I'm going to go ahead and copy this and I'll paste this in again we'll hit enter there we go so that log should be created I assume we have to create uh log and stream because I think uh we have to specify the uh stream here right let's go over to our Cloud watch logs cloudwatch logs okay and then we can find it here Cloud watch cloud trail my cloud trail and uh we don't have any log streams in here so we'll go back over to here and I want to look at this policy I just want to see yeah so we're giving you access to a specific Stream So I think that'll be important for us to name that so we'll go back here and create our stream next I guess stream create log stream go down to examples we'll grab this one here and we'll go back to our read me we'll paste this in here so oh this is not the right file this is the basic we'll go back to this one paste this in again paste and so we have the log here my cloud trail I'm just going to call this one main I don't know what to call it really we'll hit enter and so now we should have a stream in here called main there we go then there's nothing in it but that's where we expect our data to go uh let's go back to our policy now that we have those two values it should be a little bit easier for us to fill in the rest here so we have this as our log group name here boom and boom and then we have our log stream so here this will be Main and Main great now the other part of it is so we need the log group AR so we'll have to go grab that that is will provide it to us here yep it's right here that's the iron I'm not sure if we need the aster on the end there but I guess we'll just Supply it until it complains and says hey we don't want it like that um we're going to need to make a rule so I'm just going to go make this manually because uh not everything has to be painful through the uh CI if we got a bit better in kind of organized stuff it wouldn't be so bad but uh we'll just go ahead and say cloud uh this is for uh cloud trail okay well first we got to actually create the policy sorry one second all these years and they don't make this process easier so we'll just say uh Cloud oh no we'll just go here we'll paste this in here as such excellent we'll go and look at any errors it has an issue here um that doesn't look right that does not look like our code sorry we're in the wrong folder it's over here go ahead and copy this we'll go back we'll paste this in and there's no problems here next so let say my cloud trail cloud watch policy okay pretty long name but uh whatever as long as it gets the job done I'll go ahead and do this this will be for cloud trail because that's who we're giving permission to do stuff with we'll create this role cloud trail my cloud trail roll we'll give this a refresh as it doesn't seem to be aware of the new role we just created oh come on where's my role did we not create it there it is right here and I'm going to go ahead and um no this is not what I want we'll go ahead and the heck happened here this is not what I wanted at all we'll go back here and try this again so we'll just go my cloud trail I'll just delete that cloud trail delete this here this is this is a service rle um okay maybe you never actually created because that's not what I called it so we'll type this again cloud trail next why is this the only policy I can choose all right give me a second apparently there's like a default imal policy I'm actually just kind of well that's for uh Ohio region that's not going to help me but uh yeah I'm a little bit confused I thought that uh we could do that but it looks like that's the only thing we can do there so let me just see what we can do I don't know maybe we'll go ahead and we'll just create this anyway we'll just try this one more time says this name already exists okay uh we'll go back and take a look here because this is a uh this is a service role TR relationship Cloud watch Trail logs huh okay let's go back over to here Cloud watch logs AR group cloudwatch logs R AR so maybe we're providing maybe it's for cloud watch logs okay well whatever we'll just try it can't hurt right Cloud watch logs I don't know I'm just confused on this one cloudwatch logs Cloud watch watch what what do we do here uh okay I guess I'll read more all right so that's chat 2bt and I'm not crazy because it's telling me to go create a cloud trail role and so I mean it should be this allow inous services to perform actions in this account cloud trail choose the use case allows cloud trail to access ad us resources on your behalf yes next why is this the only thing I can select uhuh yeah so even chat gbt is confused because it's going say like create a rle select it a service as the type of trusted entity cloud trail as a service for this role I mean the other thing that we could do like we could just ignore it and just say custom trust policy and then this way we could just specify what we want to have here and so um let's go take a look at another rule here for cloud trail and maybe we can just make a custom one and work our way around it because that's all I can think of that we would do yeah so this is what I'm thinking like we'll go here okay and we'll paste this in here right and then we go next aha there we go create put logs okay great we don't really need to create the stream because we've already made it but uh that's totally fine we'll go ahead and hit next we'll say my cloud trail to Cloud watch rule whatever it allows us to have the number in there cloud trail Paul Ro not attached cannot create a service Ro policy to a customer role so did it not make it what oh well no it's right there okay but I'm not trying to create a service link rule right yeah yeah so that's not what I'm trying to do so we'll go here and we want L cloud trail in order to do this so I'm ignoring this complaint here I'm going to assume that I know what I'm doing I'm just going to ignore 's uh suggestions here and we're going to go back over to our code and we'll Supply this and so I'm hoping that this works we'll go ahead and hit enter when we'll go up you must specify a log group and an and a did I not put the oh I didn't put the Hy in here or the backslab we'll try this one more time copy paste enter verify and I am that the RO has adequate permissions when calling the update Trail operation actess denied all right let me just figure this out all right so I just looking up the air and looks like someone did the same approach they said okay I'll make a custom trust policy and then they'll attach this note that for a delegated admin to at cloudwatch loging CLA must uh be used and it cannot be done using the console which is totally fine I don't care um I mean we're not doing delegated admin here we're just uh updating the trail and then they just update it so it looks pretty much the same as what we're doing here let's go back and take a look at our code so we have cloudwatch log R iron roll my cloud trail to cloudwatch log roll um our log group is called my cloud trail which is fine um specifically this maybe we'll just take this off of here maybe it doesn't like that and we'll try this again cannot validate the specified log group AR so I guess still it's this I mean we only really want to go to main so I'm going to go ahead and do this instead let's see if it'll take this instead all right still doesn't like it so maybe it has to be like this access denied verify in that the RO has adequate permissions so suggesting that there's not enough permissions here uh we'll go into our permissions here and we'll take a look at our policy and oh no this is not the right one let's go back cloud trail I got to go back to our rule I just want to make sure I'm following this through and making sure everything is right so we click on this one and we have my cloudt trail Cloud watch policy I'm going to click through to it and I mean it looks like we should have access we'll look at the Json we'll edit it so here we have log creates log stream that looks fine to me uc2 Us East uh Us East oh well first of all it's not USC's 2 it's CA Central CA Central 1 so that's going to cus me some issues I'm going go back over to here I guess I never noticed that see Central one and then we'll say ca Central one I still wonder if like this stream has to be a very particular name um I I mean I wrote Main in there but I would assume that it must follow some kind of convention based on what what the stream is called so maybe that's also my problem but we'll find out here in a moment because the permission still doesn't work that's probably what it is so we go here and there are no problems we'll go ahead and hit next save the changes uh we learned before that this stuff does not propagate instantaneously so just do do like a twominute break and we'll come back here in a second all right so let's give this another go and we still have invalid permission so I'm thinking that there's some very particular naming for um that there so I'm just going back to this and uh Cloud TR log policy no that's the bucket policy so that's not what we want cloud trail log stream name prefix um I don't know what it wants for this oh there's an aster on here okay I think I know what we're supposed to do here so let's go back and I think that whole making that main thing was completely useless now that I think about it uh so what we'll do is go back to our policy if we can find it here we'll edit this policy and basically we'll take out the word main like that and so that will then give the access that I need it was that if we had that prefix which we don't actually have because we didn't actually Add a prefix and we'll make sure this is the current policy version and so I think that that is closer to what we're supposed to have go back here and just update this okay and uh if we go back to our read me then this this thing's totally useless the uh this one here we don't have to make our stream okay uh fingers crossed that this works there we go great so now it's creating um we'll go back over to cloudwatch logs over here and we'll just type in cloud trail again and so I'm just going to get rid of this one because this one is useless I don't know if there's actually any logs in here yet so nothing as of yet so we're going to have to go create some activity as per usual I'm going to go over to Dynamo DB and we'll go ahead and just create a table did we not delete this table apparently we did not okay whatever we'll delete this table and while that's going I'm going to create a new table say my new table uh ID here we'll go ahead and create the table we'll let the table create we'll give it a moment all right so we've created our new table I'm going to go ahead and just uh again just perform any kind of operations on here to trigger uh some API action so that we can go observe that in our um cloudwatch log so I'm going to go ahead and change this to three okay um and those uh item items that we put in there they wouldn't show up because that would be for data streams which we're not doing or data events um I'll just change this a few more times we'll change this to On Demand we'll do that that seems fine we'll go ahead and delete the table now confirm there we go oh it's being updated that's why I didn't delete the last time I guess we'll just wait for that to update all right it doesn't take that long to update a table we'll go give this a refresh and go ahead and delete this table we'll say confirm and so the question is do we have any data over in uh cloud trail or sorry Cloud watch log so we'll go back over to Cloud watch we'll go to our laog groups we'll go take a look here cloud trail and we have some data streaming here excellent so let's go take a look and see what we can see so here we have you know some operations coming in here so that's interesting very easy to read it from here so yeah that's pretty straightforward um so that's pretty much all I wanted to achieve here there's very little in this log I don't want to delete this stuff I might want to query this later but for the time being I want to go back to cloud trail and I want to just disable uh cloudwatch logs uh because I don't really actually want to do this because it will add up after a while um but for that small sample we could maybe use that for something else or turn this back on later on so we go ahead here and we will um check for that configuration for cloudwatch logs and I'm just going to disable this for the time being and we'll save those changes would have just showed us the policy document the entire time here I don't know maybe it's the one that's attached well whatever I guess it might have been in the UI that entire time I don't know but anyway I will see you in the next one we'll keep our cloud trail log around or cloud trail yeah log around here or Trail for something okay hey this is angre brown and in this video I want to take a look at uh uh cloud trail Lake because that is a new um a new service at least for me it seems pretty straightforward in terms of how it would work but it appears that what we can do is apply queries um against your data so the dashboard helps you visualize the data in your event data store by using queries you can choose the event data Store and type of dashboard you want to create so it seems like we have to create an event data store before we can do that um so let's see here configure one or more event data storage with the data and management events that you want okay let's go give that a go and let's see here so generally recommended pricing option wow a lot of options here let me just carefully read this all right so we'll just call this my data store and we'll stick with the one year um I'm just trying to I just was confused whether we're getting locked into anything but it seems like we'll be okay here and we're not going to injust too much here uh it says Lake query Federation lets you run esql quers against your event data using Amazon Athena so it sounds like it's already using Amazon Athena it's just kind of like a wrap around it so we'll let it create a new rle just say my Athena Lake query I mean I didn't have much trouble using Athena before then we have the event types that we want to uh see here so we have idless events idless cloud trail events I would say that's the case is this not from a specific Trail looks like we can also do this for other stuff here cloud trail events let's go over to configuration items okay so what I keep expecting here is some way to um spe specify the trail and I'm not seeing that option but that's totally fine we want read and write so that's not too bad we'll go ahead and hit create uh event data store okay and so now we can go into here I really would have thought that we would choose the specific Trail but I guess not so I'm it seems like what it is is that we're creating like a managed S3 bucket that has the data in it and that is integrated with query so Athena so you don't have to do all that stuff yeah this looks like Athena almost okay so we'll go here and I mean to make sense of this we actually have to know what we're looking at so I'm going to go back over to uh Cloud watch logs for a second because we were able to visualize that pretty easily it's interesting because we can search for Athena but we can also kind of search with um uh login sites as well so you know seems like there's more than one way to analyze stuff but uh we'll go into cloud trail and then from here we'll go into here and I'm just trying to see something that we can search on say create log stream no list Stacks no no looking for something interesting I mean doesn't really matter that much but I'm going to just keep looking around a bit here so I mean that might be interesting or Event Source like maybe the Event Source from Dynamo DB so what we'll do is we'll go back over to our query and in here there's probably Event Source there it is uhhuh oh it doesn't it just shows us that that it's there okay not very helpful uh sample queries I guess this is a terrible UI just horrendous okay let me give give me a second here let me just take a look here this query language is not fun looking okay so I'll copy this one here and we'll go back I feel like this would be easier just to use uh Athena directly we'll go here and I'm just trying to expand this to see what I'm looking at so elet run the query do we get anything back failed query is not valid how could it not be valid I copied and pasted it from what ad of us provided me you serious all right well I just clicked one there we go Cur results nothing so I mean at least we can click through here on specific ones but it doesn't really help us for uh creating our own queries so it's not the best thing to look at here I just really want to um see if we can write our own query so what I'll do is I'm just going to open this up in another tab so I can see what's going on here so we can't write our own queries what's the point okay so I would say that I want to select I select everything first then there's this I'm not exactly sure what that's supposed to mean Event Source here would be Dynamo DB and I don't care uh the time range run will this give me anything back I mean it worked so that's good how do I see the results result history uh uh select all so we can click here maybe if we go into here because the way Athena works is that it would deliver the results uh to S3 so that we could uh see it but notice here I'm not seeing any of the results so it's not uh very useful we'll run this again okay nothing no results all right let's save this to S3 um I'm going to go browse I guess it's just creating a new bucket that's fine we can go ahead and have it do that we'll just run this again and we'll view this in S3 there's no results that's fine I just I just can't make sense of what it's saying so I'm going to go ahead and download the CSV and then because it's a a gz file um for whatever reason in vs code or G pod it just seems to show it right away so I'm going to go um I'm just looking for it here just give me a second actually the file seems to be a results. CSV it's not even gzip so I don't even need to bring it into vs code but I'm going to anyway because it's just easier for me to do and so here we have the query save that's cool what about the data uh let's go back to here maybe this is the data can I just open this up open because it's just Json all right okay so not useful so far oh the event data stores this value so that's what we're quering against every time um okay well maybe it's because the at the time we created this event data store there is no data for it to query and we have to wait a little while for something to occur so I'm going to go ahead and open up Dynamo DB in another tab we're going to do the same thing we're just going to kind of goof around and try to get some data to appear and then wait a little while and then requery it okay so we'll query it and say uh my new new table and we'll say ID here and I'll go down and create this table and we'll wait for that table to be created all right so I'm going to go ahead and just change the settings here so we'll just change this to edit the capacity we'll just set this to five and five and then we'll save those changes and then I'm going to edit the capacity again and just change it to be on demand and uh yeah we'll just have to wait a little while here I guess this to update okay all right so that table's still updating but I feel like maybe there's enough data that might have propagated for us to do our query so we'll go back over to uh cloud trail if we can find it and we'll go ahead and try to run our query again and see if we actually get some data back so now we're getting data back okay so I think it was just a matter of us having to wait here and so now we can see our data um these queries are going to uh S3 so I think if we go here it' be interesting to see if it also dump the data there as well uh I mean it might have can we just open this one here open it' be nice if we could just open it without having nope we got to drag it in here so we go back over to here and we'll just take a look at the results again it doesn't show us the data but we definitely get results back Le that's nice um what about the other file here nope well at least we know that query works but uh yeah that's one way of doing this so I would say that satisfies our ability to use event data store it seems like this can be used a lot more than just cloud trail but what I'm going to do is I want to get rid of this now so let's go see if we can delete this Lake Federation enabled so we got to stop ingesting first okay I guess it's a really easy way to have basically a data Lake where you're just getting data ingested into one place and so it's stopped can I delete it or do I have to hold on to that retention period 366 days okay so I guess we can't delete it for 366 days that sucks and that's what I was worried about I like it seemed like we could delete it any time but I guess we can't um so that's kind of stupid just give me a second all right well we've adjusted very little here so I think it's less of an issue um you want to choose a retention period of one year on your event data store you have two options you have a one terabyte of cloud trail management data events ingested into cloud trail in a given month in your account you want to choose a retention period of one year for your EV evaded D store okay so what am I paying so yeah I guess uh I guess we're locked in for the year but uh we did really ingest in a whole a whole lot of information so I think it's not much of an issue so it says cloud trail management data events 075 and so we're definitely going to be under that but it's just frustrating because at least seems like we're going to be like 75 cents or less um per month but we only have a onee we have this onee extendable retention period okay let's go back here I'm assuming we can't delete this let's go back to uh where was this over here yeah yeah so I I can't delete this I guess oh wait no that's termination protection on okay hold on change uh disabled okay refresh nope okay so I guess we're stuck for it for the year um maybe we have to disable it first I'm just going to disable Lake quer Federation here okay now can I delete it okay there we go all right I tell you it's kind of hard to make sense of this stuff half the time okay we'll delete that great we're in great shape um still says retention period 366 days but I'm hoping that that's fine but uh yeah anyway that is clra Lake um I don't find this much advantageous over using Athena maybe we should show you how to use Athena but uh I'm going to go over to Dynamo DB as we do have this table here I just want to go ahead and delete it okay and I'll see you in the next one okay ciao so I said earlier that this uh will um collect Beyond 90 days but you're not going to have that nice interface that you have in event history here so how would you go about analyzing that login I said you could use Amazon Athena so luckily they have this link here that's going to save you a bunch of setup to do that so if you were to click this here and choose the S3 bucket which is this one here it's going to uh create that um table for you in Athena we used to have to do this manually it was quite the pain so it's very nice that they uh they've added this one link here and I can just hit create table uh and so what that's going to do it's going to create um that table in Athena 4 us and we can jump over to Athena okay and um yeah it should be created here uh just give it a little refresh here I guess we'll just click get started I'm not sure why it's not showing up here we're getting the splash screen um but we'll go in here and our table is there so we get this little uh goofy tutorial I don't want to go through it but um that table has now been created and we have a bunch of stuff here um there is a way of running a sample query I think you could go here and was preview table and that will create us um a query and then we it will just run the query and so we can start getting data so the cool Advantage here is that if we want to uh query our data just like using SQL uh uh you can uh do so here in Athena I'm not doing this on a daytoday basis so I can't say I'm the best at it but you know if we gave this a try here and um tried to query something maybe based on event type I wonder if we could just like group by event type here so that is definitely a option so we say distinct okay and I want to be distinct on maybe uh event type here okay it doesn't like that what if I just take that out there great so uh there we go so that was just like a way so I could see all the uh unique event types I just take the limit off there the quer will take longer and so we do have uh that one there um but anyway the point is is that you have this way of uh using SQL to query your logs obviously we don't have much in our logs but just important for you to know know that you can do that and there's that one uh button press there to create that table and then start creating your log data so one of the most important Concepts to learn for cloud networking is the OSI model and that stands for the open systems interconnection model and this is a a definition of standards of communications for Telecom and computer systems so the OSI model was invented in 1970s because organizations and governments were creating proprietary Network Technology and they needed to agree upon a conceptual standard on how these Technologies would communicate with each other at abstract level and that's how we got the OSI model so it describes how information moves from uh software on a computer through a network to another computer and the OSI model is made up of multiple layers which are numbered through one uh as seven as follows and we're going to work our way down uh and up it doesn't really matter the order we go but we just got to get through them all so we have physical data link Network transport session presentation and application now to just really understand when we talked about it moving through those layers here is an uh an example of two computers a source computer and a destination computer and data moving between them so when data leaves one computer it has to pass through each of the seven layers and then over the network uh or the internet and then pass again through the seven layers but going in the reverse order and as it moves from layer to layer the data is transformed into pdus and we'll talk about pdus uh in this section here and each layer has its own pdu format so uh there is the fundamentals of OSI and we will be going through all the layers because it's important to know the differences but there you go so now let's take a look at tcpip which is super important to Cloud networking and to really understand what it is we need to First understand what is a protocol and a protocol is a technical implementation defining a strict standard to which communication between Technologies must adhere and so an example of a protocol is the Internet Protocol version for ipv4 which we definitely need to know and this is a protocol that is used to pass information around on the internet uh and the way ipv4 passes information around is that it organizes it into packets which uh leads us to our next question what exactly is a packet and it is a protocol data unit I told you we'd be talking about these things also known as a pdu and it is a single unit information transmitted among peer entities of a computer network uh and there's different pdus for different uh parts of the OSI layer uh so for the transport layer we have segments for the network layer we have packets which we just mentioned there a moment ago for data link layer we have uh frame and for the physical layer we have bits so and some of these might spill over into other layers but just to look at what a packet is there's an example of a packet and so a packet is represented by that data structure there and all of them are different so now what is the Internet Protocol Suite also known as tcpip so tcpip are Suites uh is a suite of different protocols used for the internet so you might have heard the terms HTTP SSL TCP IP Mac DSL those are all part of the tcpip protocols and the conceptual model and set of communication protocols used in the internet and similar computer networks it is commonly known as tcpip because the foundational protocols in the suite are the transmission control protocol TCP and the Internet Protocol IP so those were the two foundational protocols uh and even though it makes up a lot more than just those two that's what we call it and that's what everyone remembers it as so there you go so now we're going to take a look at all the different um layers of the OSI model starting with the physical layer uh and the physical layer is responsible for transmitting raw bits as a physical signal to the destination Network so the raw bits are sent as a bitstream and a bitstream is a sequence of bits okay and the data signal could be either electrical or Optical so an easy way to remember the physical layer is just think of lasers so uh physical ways of trans transporting data and the hardware or concepts that is related to the physical layer would be things such as voltage so electrical pressure between two points pin layout so that's actually the pins on the Silicon board cabling so it's actually the cabling that runs between your computers very common with is coaxial cable might be spelled wrong there not sure uh radio frequencies um repeaters um and there you go so that is the physical layer so now let's take a look at the data link layer also known as Layer Two and it's responsible for packaging data into frames to transfer to network nodes on the same layer so Network nodes could be a computer printer medum switch Hub Bridge or server and the list goes on and the data link layers handle things such as package frames send frames to uh Network nodes error detection and correction and identify Network nodes based on the Mac address and I want to give the MAC address a bit of emphasis here because it's good to know what that is and so the MAC address which stands for media Access Control address is a unique identifier assigned to a network interface controller a Nick for use of a network address if you're wondering what a niit card is if you if you ever had an old computer uh where you have to actually plug the internet into you're plugging it into your Nick that's how internet gets into a computer and so a MAC address are hardcoded in a network card so that's when we say neck we're talking about network card and it cannot be changed so if you are on a Apple computer and you were to open up your network tab under Hardware you'd see that you have a MAC address and that Mac address is completely unique um so just to summary uh summarize here for the DAT link protocols we have ethernet and this is a family of networking Technologies commonly used in land so uh ethernet is uh you've heard the term but there's no definitive term of what that is uh then there's PPP uh we have switch and we have Bridge so there you go so now we're going to take a look at the networking layer also known as layer three and this is one of the important layers you need to remember layer three layer four and layer seven okay so the network layer is responsible for routing or forwarding IP addresses and the network network layer handles logical addressing so uh it can give you addresses for ipv4 IPv6 I ipx uh it does switching so it can do routing package to uh specific devices it can do route Discovery and selection so determine the best route to send packets um and it's pdu for this layer are packets and a packet is a formatted unit that consists of control information and user data uh and user data uh generally could be referred as to the payload so this is the data that you're sending over the Internet uh and what you really want to know about is ipv4 and IPv6 which are routing IP packet protocols um then you have IPC which is a term you'll hear uh quite frequently in uh Cloud networking and it is for the Internet Protocol security so it authenticates and encrypts packets for secure routing so you want to send packets over the Internet uh securely I IPC is the way to go uh and then you have icmp which is a error messaging protocol but there you go that is the network layer so now we're taking a look at the transport layer also known as layer 4 and again layer three layer four and layer seven are the ones you need to know so uh pay close attention to this one uh so the transport layer is responsible for endtoend connections and reliability the transport layer is a conceptual division of the OSI upper layers so the host layers and the uh the lower layers so the media layers and just to make that more clear on the bottom we have physical data link Network transport so 1 to four and the media layers are responsible for data that arrives at the expected destination and the upper layers layer five six and seven so session uh presentation application are the host layers and they're responsible for accurate data between hosts so the transport layer handles connection oriented communication so connections are established between useful data is transferred then you have reliability so check if the data is corrupted or has not been lost or requested again you have flow control so the rate at which data flows congestion control so the ability to avoid the congestion of a data flow multiplexing uh and this gathers multiple chunks of data from multiple sockets and packages them as segments so now let's take a look at the protocols for the transport layer and we have two kinds we have the data protocols and the control protocol starting with data we have TCP so transmission control protocol and so packets require acknowledgement of receival it guarantees retrieval of packets used commonly in web applications because you need a guarantee that the packets you're sending are being received then you have UDP so user data uh datagram protocol and these are packets that are sent and require no acknowledgement packets can be lost but this protocol is much faster and commonly used in video games because you need a lad latency and it's just better to send the same repeat packets over again instead of waiting for that acknowledgement uh then you have control protocols so here we have icmp so this is the internet control message protocol and it sends error messages or uh operational information indicating success fail uh when communicating with another IP address and it's used by networking devices such as routers should say routers they're not routes um so the pdu for this transport layer are packet segments so a segment is a packet that has been divided into smaller units of uh uh units for transmission over a network uh so there you go that is the transport layer so now let's take a look at sessions also known as the session layer and let's talk about what is a session so a session identifies who is doing what on a computer or device an example of a session could be a user session so when you log into a website or a session has a state so uh logged in could be the example of a state and the state has to be stored in some kind of history sessions are temporary uh which is something important to remember about them and they can expire when a web browser closes or it's based on expiry time so the session layer is responsible for creating maintaining and destroying sessions and the session uh layer protocols we have here are API so application program interface which could require a token to persist a session websockets which establishes a continuous session for streaming data in real time to web apps net bios also known uh it's fullterm Network basic input output system allow apps on different computers to communicate within a local network NFS so Network file system allow multiple users to access uh shared files uh a file system on a on a hard drive RPC so remote protocol uh or remote procedure call a protocol that can be used to log into Windows desktop servers so there you go that is the sessions layer so now let's take a look at the presentation layer also known as layer six and the presentation layer formats and delivers information to the application layer it's also known as the syntax layer and it can handle things such as data encryption decryption character code translation uh data compression and so why do we need a presentation layer just another way to put it but the presentation layer ensures that data at one end of a connection is interpreted in the same way that it reaches the other end of the connection and so some presentation layers protocols here would actually be things that you're very familiar with such as JPEG GIF PNG uh maybe aski or EG for fil so there's a lot of protocols that are are processed at this layer but there you go so now let's take a look at the application layer also known as layer 7 and again layer three layer four and layer seven are the most important layers that I want you to remember uh so let's jump into it here so the application layer is the closest to the end user the protocols of the application application layer are used by software applications such as email web applications chat applications shell Terminals and the application layer internet protocols being used uh here would be htps uh SSH DNS uh DHCP uh ldap SSL FTP IRC BPG RTP t LS mqtt telet uh and there's a lot here there's just tons and tons here but the ones that I want you to remember and know and we do cover them through this course are these ones so HTTP SSH DNS dhtp dap and SSL okay so there you go so now let's take a quick look at uh the protocol data units uh and we talked about these before but let's talk about them again so pdus are a single unit of information transmitted among peer entities of a computer network and pdus of the oide layers look different uh at those different layers I just wanted to show you like in a in a snapshot how they are different okay so the application layer you're usually just dealing with the data itself so that could be HTTP request uh and then when it goes to the transport layer notice that layers five and six aren't there because they don't really have pdus um it's kind of hard to explain but what I want you to know is just these different data structures so we have segments and a segment is a chunk of data prepared for transmission so that's where data has been uh segmented into parts and it has a transport header so that's just information to tell where it is going uh then you have packets so a it's a chunk of data prepared by software uh and here it now has additional information uh which is the network header uh to know how to transverse the network then you have frames these are a chunk of data prepared by the hardware uh and so we have a frame header and a frame trailer it's also used in layer one uh and then layer one is bitstream so a sequence of bits prepared for physical Transportation so literally broken down into machine code so just an idea of like how like data uh has things added to it and then how it gets cut up and then sent out at those different layers okay so there you go so now let's take a look at network interfaces so what is a network interface well it's software or Hardware interface between two pieces of equipment or protocol layers in a computer network and so this is where we have Network inter uh interface controllers also known as NYX and this is a computer hardware component that connects a computer to a computer network also known as a network interface card network adapter LAN adapter physical internet interface so you see it has a few different terms that it goes by and N uh communicate using the Internet Protocol so IP and you've probably seen one of these back in the 1990s if you needed a Nick or a way of connecting to the internet you'd put that in the back of your computer you can see there's like a little plug for RJ45 Jack or maybe the internet uh you still have them in your computers they just do wireless now um but uh yeah that is a Nick card and so the nick uh devices operate on the data link layer and the physical layer which makes sense because that's when you're dealing with Hardware um and in the context of AWS uh adus has a service called Amazon VPC elastic network interface or Enis and this is a virtual network interface that is attached to an ec2 instance without an eni an ec2 instance would not have a way to communicate out to the internet or to other ec2 instances or any way in general and an ec2 instance can have multiple Nicks or e or Enis attached to them so it's weird because like you don't really think about nxs or or Enis on AWS too much but they are there um and so I just wanted to make you aware of them so there you go a very important topic for networking is knowing what an IP address is so the Internet Protocol address is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication an IP address serves two main functions it it's either the host or network interface identification so it's saying who is this and then there's location addressing so where do we live in the network and here is a very simple example of a private Network and you can see we have two computers uh or sorry three computers one that is actually literally a server and the other two are personal computers and they have IP addresses underneath specifically ip4 addresses which we'll talk about soon enough and there are two versions of IP addresses currently in use ipv4 which is invented in 1981 where we have 4 billion that are in the address space that we can use and IPv6 was which was invented in 1995 we invented it because we R we were worried about running out of addresses with ipv 4 and this one has 340 un undes unilan uh addresses and in November 2019 the last remaining ipv4 address was claimed that doesn't mean that we still can't use it just just that we have to like share them uh and we still use ipv4 addresses and it's been a slow migration to IPv6 for a few reasons because of old technology Legacy software outdated attitudes but we're getting there slowly so there you go so now let's take a look at the ipv4 so Internet Protocol version 4 uh so ipv4 addresses has a size of 32 bits which limits the address bace to 4 billion addresses so ipv4 uses dot dotted decimal notation a series of four numbers ranging from 0 to 55 so an example could be 192 168 0.1 which is a very common number uh used to identify yourself uh and so here's this example of that dotted decimal notation and so you see we have eight bits and and the reason it's eight bits is you can see that there are uh a combination of zeros and ones there of eight of them okay uh and so that makes up in total 32 bits which are four bytes uh but there you go that is ipv4 so now it's time to look at IPv6 so Internet Protocol version 6 and IPv6 address has a size of 128 bits which limits the address space to three 340 unilan uh addresses don't know if I'm saying that right but that's how I'm going to keep on saying it and so this is what it looks like so you have a global uh routing prefix a subnet and interface identifier um so the inas identifier identifies the computer the subnet identifies the sub Network and the global routing prefix is the router that is whatever uh and so I mean it's not so important to remember um like exactly how this works but just understand that this is what it looks like and it has a large address space uh and that's that so now let's take a look at the concept of binary math uh and the reason why is that if you understand it you can really understand IP addresses which will make your life a lot easier especially when we get to cider notation so uh binary is a base two numeral system of Z or one and it's called binary because we only use two numbers the zero or the one and so B binary is the lowest level language a computer uses to communicate commonly known as a machine language and uh some terminologies here so a bit is a basic unit uh of information that represents either a zero or one so if you have a z one that's a bit and a bite is a basic unit of information that represents consecutive bits so if you had 1 0 1 0 1 0 0 that could be a bite but the most common type of bite is eight consecutive byes known as an octet uh and binary math is when you use octets to represent a number so uh the way binary math works and we'll get to this is you count from right to left and each consecutive bite is to the two is two to the power of that bite position and you add up the numbers to get the final number and I want to point out that when you look at a range of an octet so if you have between 1 and 256 um it's it's going to actually be between 0 and 255 because we start counting at zero and if that doesn't make any sense it will shortly as we work our way through this here so what I have here is um we have an octet and so what we're going to do is we're going to add up the numbers and again so an octet is eight numbers but the highest number it can be is 256 so if those were all one across the board it would be 256 or actually 255 but um let's work our way at at the beginning here so we're looking at that zero and so that 0 is 0 * 1 that's two to the power of zero and so that's going to give us a zero and the bite total is zero the next number is one so we do 1 * 2 so that's 2 to the power of 1 and so that's going to give us two and that means our total is now two then you have 1 to the power or 1 * 4 so that's 2 to the^ of two uh so that gives us a four we add it up we get six we do that with eight we get six because there's nothing to be added there we do it with 1 * 16 2 the^ 4 we get 16 we get to 22 and you get the idea and so when we make it all the way to the end here our total number is 214 but again remember we started counting uh we started counting at zero so really our number is 213 uh even though it comes up to 214 uh but again if all those numbers were one across the board it' be 256 so there you go that's binary math so now let's take a look at classful addressing and this type of addressing architecture is where the size of the network was predefined based on classes and classful addressing was used between 1981 and 1983 uh and was deprecated uh with the introduction of uh cider which is classless addressing even though we don't use it anymore um it's still good to learn because uh there's some things that still kind of show up for legacy reasons or it helps to contextualize things uh so that's why we're going to go through this okay so an IP address with classful addressing would be divided into two parts the network ID and the uh subnet so the net ID or network ID is how many host addresses were available and the host ID is how many host addresses were available so here we have the class and we'll just go through here and look at some of the information here so we have class A so if we had class A the sub the subnet and we'll explain or the the subnet Mas and we'll explain that in another video is 2 2550000 the the size of the network is 128 addresses so that's how many network uh addresses you could have and that' be 2 to the^ of 7 and then for the number of host per networks we have 2 the^ 24 so that looks like we have uh 16 million uh hosts and the IP address range there is 0 to 127 and The Cider notation if you wanted to equate it to something would be for sl8 and so then for class B notice it the subnet mask is gone from 255 and the next block is now 255 at 0 zero and the the amount of networks has uh increased but now the number of hosts have decreased okay and then going down to class C you can see that um the number of networks are larger and the number of hosts are down to 256 so very very small there uh and so the first four leading bits are designated to the identity of the class um so that's why the number of networks we have 2 the^ 7 2 the^ 14 2 the^ 21 instead of um 2 the^ 8 Etc um and that's not really that important but if you want to dig into it or if you're looking at that and you see in the future slides why they don't match up that's the reason why and there are classes d and e uh but for cloud networking we only really need to understand A and C because that's all we really uh we see okay and what we really see are those IP address starts but we'll talk about that in another slide so there you go so now let's take a look at networking terms okay so I just want to show you um kind of like a a diagram of what the networking would look like and the IP addresses associated with it so the first is an i or what is a network so an IP network are interconnected devices that are using TCP IP suite for communications so notice down below uh we have I believe that's a router and around that uh in that uh Network we have computers but notice that the network has an IP address and look it's 192.168.0.0 okay then we have what is a subnet so a subnet is a logical subdivision of an IP network it is a subn network okay so that's where we we're we are cutting up our Network into smaller parts so notice at the subnet we have 192 168 0.0 on the Le hand side and then we have 192.168.1.0 on the right hand side uh then the last thing here is what is a host so a host is a computer device that communicates with other hosts on the network and notice that now we have uh addresses for our our computers or laptops or servers down below so notice that in the sub networks 1926 168 z.0 we have an address on the end there with 01 and 02 and then the subnet that ends in 10 notice that we have a computer that has or server that has one one and one two so there you go so now let's take a look at leading bits and the net and host ID uh leading bits we only really deal with in classful addressing but I just kind of wanted to point that out because when we get to subnetting or uh subnet masks it might make a bit of sense to you if it doesn't I just put this in here uh because I thought it was great information but let's say we have an IP address that's 17122 1 and 12 and we want to determine uh What uh uh uh class size it is whether it's a b or c how would we figure that out remember that the first two uh numbers is the network ID so this is a specific subnet on the private Network and then uh the latter here is a specific host with a specific subnet so that identifies a computer on the network and the amount of network and hosts available in classful addressing is determined by the one to four leading bits okay and leading bits is sometimes called significant bit or high order bits um but what did they mean by one to four leading bits well when you look at that address at the top 171 22 112 it has a binary representation and this is what it looks like so notice that like you have an octet and there's a period and octet and a period and octet and a period That's mapping up to the numbers above right so 171 is that first octet there and so when they say the one to four leading bits they're talking about the the bits the the the numbers at the front of that address and so uh depending uh and it again it doesn't have to look at all four the front but it'll look at it'll look at the first uh first few and determine where it goes so if we were to look at the first couple of bits uh and you you were to put them into binary and turn them back into decimal that would equal 128 and when we look at our class Network um our class network uh would be the size of B because it looks at the first bits so see where it says one and zero that's what the leading bits would be for class B if if the F if the first leading bits was 1 1 Z that's how we know it's Class C if the first bit was just zero then we know it's Class A and that's going to determine the size all right and because we are using up those those bits so see on Class A where we're using zero um notice in the number of networks it says 2 the^ 7 because we have less uh bits to work with so uh I just wanted to point that out but when we get to classful addressing we don't really calculate the size of the network in class or or sorry classless addressing like cider but I just kind of wanted to point that out because it really bothered me uh wondering where those bits had had gone uh but there you go let's take a look at private address spaces so when you create your virtual private clouds like VPC an AWS or any other um cloud service provider you need to choose a private address space and so just an example here I'm creating a VPC in AWS I give it a name and then I give it an ipv4 cider block and the the private address space is that number in the front there the 10 and the CER range is on the end there 4/16 uh so why did I use that specific IP address and so there's three you're always going to see quite commonly so you got 10.0.0.0 uh 17260 192 1680 and the reason why you see these numbers is because of this document uh called the RFC 1918 uh which was produced by the uh internet assigned numbers Authority so the IIA and a and so this is a document of standards and they reserve the following three blocks of IP addresses for private internet private internets meaning your own uh your own network okay and so here we see Class A Class B Class C so you you thought that classful networking was deprecated well we still use it at least the terminology still uh and so 10.0.0 the first one is Class A and the next one's to class B and the next one's Class C so do you have to use these IP addresses well no they're just a recommendation you can use whatever you want um but what you'll find is that people will use the especially people that have a a very strong networking background because if you do use them it kind of tells people at a glance what's what's the size of the network Engineers can expect them to be so some people will consciously choose these to signal to you but you could use them for whatever you want uh so I just wanted to explain where those come from so you're not wondering like why are we using those let's take a look at subnet masks so a subnet mask is a 32bit number that looks like an IP address but it's not and so it could be 255.0.0.0 uh and when we were looking at classful addressing I was showing you or I was talking about leading bits and the way it determines where to split the net and the host ID is based on those leading bits in classless addressing which we're coming close to talking about here we don't use leading bits we have to use subnet masks okay uh and that's what a subnet mask does it divides the IP address into the net ID so the actual identity of the network and the host ID so all the possible hosts that can be on that Network which could be that subnet so here we have a network address so 17122 1.12 okay so what part of that IP address is identifying the subnet and what part of it's identifying the the host or possible hosts that can be on that uh that Network okay and so let's say we drew the line here we could draw the line anywhere we want because it's just going to change uh what is here so the net ID is 17122 okay and then and the host ID is 1.12 so if we want to draw our line there uh that's that's how we use our subnet so if we wanted it to be there uh we fill all the all the bits in the front with ones and then uh everything zero is the other side of it right so the ones are masking or covering uh that so they can't be changed and then the the ones on the end the zeros can be flip to determine our hosts okay so if we have zero and then one and two and three and four that's going to identify all those hosts okay um uh so if we did that we just flipped all those numbers we have the the possibility of 248 addresses the way we figured that out was binary math remember remember working right to left so notice that we have that octet those eight uh uh bits on the end and then we have those three extra bits if you kept on counting up you have uh 2 uh 248 possible computers that you can put on this network and those one in the fronts are important as well even though they can't change and they identify the actual subnet uh see where we have sl21 that's CER notation we're going to talk about that shortly but I just wanted to show you that the for sl21 it uh is there because there's literally 21 ones in a row that's that's what CER notation is it's those subnet masks where you see that ones that go to 21 okay so subnet mask can also refer to a net MK but 99% of the time uh they mean the same thing Okay the reason why is when we are defining our virtual Network cider range we'll call it a net mask and when we're defining a subnet cider range will uh say a subnet mask so if you hear those two terms don't get too confused about it they usually mean the same thing uh it's just what it is okay so on AWS um it has reserved addresses so when you specify site AR range it reserves the first five IP addresses so if if you created a cider block and I I know we haven't explained cider ranges but we will uh shortly if you had 10.0.0.0 sl28 that would result in a possibility of 16 uh possible addresses and so eight of us would reserve the first four and the last one and you would be left with 11 host addresses okay so the first uh one which is 10.0.0 is reserved for adus Network address then the next one is reserved for the adus VPC router the next one is reserved by AWS for whatever reason the same thing with the next one and the last one is reserved for the broadcast address so a broadcast address is always reserved in the last IP in The Cider block range and so if it were to be uh a larger address range like 424 it would Reserve 10.0.0 255 okay so there you go so let's take a look at classless domain routing also known as cider so what is this cider thing it is a method of allocating IP addresses for IP routing it was introduced to replace classful networking since uh the primary goal was to slow down the rapid exhaustion of ipv4 addresses a CER all allows you to choose the size of network versus host so you can use exactly what you need uh and we've been working our way uh through this section just to get to this because this is the most important thing you need to learn so hopefully makes sense but uh we need to understand what a cider block range is so sometimes referred to just cider block The Cider block range defines the size of the network versus host and the range of IP addresses so when you see uh this IP or this private address space of 10.0.0.0 and you see 416 that is the cider block range that those two things together is that's what it is and so that 4/6 is called Cider notation and this defines the subnet Mas the number the number represents the uh the the amount of leading or actually I guess yeah it's leading ones so here if we had 4/ 16 it's the first 16 uh bits are ones that means our subnet is going to be 25 25 0 0 and if we were to do the math on the zeros on the end there uh uh using binary math we would get a a total of 65,000 uh addresses roughly okay uh and so if we go up to here and we were to use a CER range of 4/32 this is the same thing as saying a single IP address because it only allocates a single um host uh ID so there you go so let's take a look at Dynamic host configuration protocol also known as DHCP and this is a network management protocol used on IP networks whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on a network so they can communicate with other IP networks I know it sounds really complicated but it'll make sense here in a moment so why do we need DHCP well DHCP is needed to simplify the management of IP addresses on a network and so no Two Hosts can have the same IP address in order for you to make sure they had different ones you'd have to manually assign them uh but this could be air prone depending on how large your network is if you have hundreds and thousands of machines so automating this process using DHCP is going to make it a lot easier for users and administrators uh and so the reason we're talking about DHCP is because it's a it's a option for uh edus VPC uh and the idea here is that with this option you're able to change the following the domain name the domain servers the ntp the net bios name servers and node types okay so you go there and you be able to choose it uh in that drop down there with all those settings so uh a use case that you'd actually want to change the like so there is DHCP but why would you want to change the DHCP settings to a different DNS server well let's say you wanted to use quad 9 DNS which is another DNS provider uh and the idea is that if you were to route your traffic through that you could be adding another layer security uh where this other provider will block known hosts but generally you're not going to want to change your DHCP uh but it is an option that you can do so now we're on to the trials of subnetting and this is a thing that I I came up with to really help you understand subnetting subnetting is when we chop our virtual private Network into uh sub networks and the reason we want to do this is because a lot of people say uh the sis Ops or the devops is difficult because of the cloud networking and understanding subnetting so all the prior knowledge we're going to leverage uh through a bunch of challenges here to make sure that you master it here and so when you take set your exam it's going to feel a lot easier uh than it would if you had not done this so what we're going to do is you're going to attempt the challenges on your own uh then uh then you can watch me uh do the challenge myself because I will actually go ahead and go through it uh then we'll talk about why it did or didn't work because some of these challenges are designed to fail to help you think about subnetting uh and so let's begin the trials of subnet subnetting and to prepare ourselves for this what we're going to have to do is go to an unused adus region delete the default VPC and after each challenge we'll delete uh the created network resources for that challenge okay uh so let's get to it all right so we're on to the first challenge here which is uh VPC sizing and what we're going to do in the first one is create a VPC that allocates 256 IP addresses create a VPC that allocates three host addresses create a VPC that allocates 65,000 host addresses and create a VPC allocates uh 256 Network addresses so let's jump into it okay so we're here uh in the adus console and what I want you to do is we're going to set ourselves up by going to an unused region I'm going to go to Canada Central I rarely ever use um uh this region here and we're going to make our way over to VPC so just type in VPC here and once we get there what I want you to do is go to the leth hand side go to your VP VCS and we're going to go ahead and delete the default VPC um it's not a really big deal if we delete it especially in a region we're not going to use um so we'll go ahead and do that just make sure it's in a region you never plan to use and so now we have no VPC so we're going to go ahead and start the challenge here uh in the next video all right so we're on to the first Challenge and this is create a VPC that allocates 256 IP addresses okay so not necessarily uh uh how many hosts will be available like how many uh ec2 instances we could spin up in this VPC network but how many IP addresses would be generally available uh so what we're going to do is go ahead and create a new VPC here I'm going to call this challenge uh 1.1 and I'm just going to give it um or a private address of 10.0.0.0 doesn't really matter we just want to put something in there so what we need to figure out is this CER notation the 4/24 how are we going to get 25 six well uh we're going to use cider doxyz it's just a visual visual tool to see cider ranges and so over here on the right hand side it says we have 16 count when we have 428 right um so remember that um this this number here represents the amount of leading uh bits that will be marked as one so it says 28 here we have 28 bits that are even though they say zero and one on the inside the the mask is putting ones all over these okay so uh we know that we want our our our count to be 256 uh and we know that an octet is eight okay so if we had all of these as zeros right so this was Zero over here and these were all 256 across the board so that these would all be ones uh how do we how do we come to this number okay well we know this is 8 16 and 24 so if I put in 24 these are all now zeros and that is 256 because the maximum number that could be in an octad is 256 so there you go that's what it is we'll go over here we're going to put in 24 and so that's going to uh give us an IP address range or IP address um of 256 now it doesn't say it here anywhere we won't see that until we make a subnet but that is the answer uh for this one and now we'll just go look at a visual diagram to understand it more clearly all right so let's review the answer here so it's a little bit more clear uh so um a net mask in a CER notation of for sl24 will allocate 256 IP addresses and here's the way we calculate it so for sl24 uh remember that that indicates that the first 24 bits are going to be flipped to one so see how it's one all the way across the board and then if we were to do this in decimal notation uh this is what the subnet Mass would look like 255 255 25 Z so the zero bit uh bits uh in that that longer that that that uh one where it has all the ones and zeros indicates the amount of IP addresses that will be available and there are eight bits so octets that we can use to define the IP address so we extract out the zeros and if we were to flip them to ones in binary math so take the zeros on the end there turn them into ones and if you do the binary math there remember it's 2 the^ 0 to^ 1 to the^ two Etc that's going to give us out a value of uh 255 which is really 256 because remember we count the zero so there is still 256 all right and that's how that works uh it's easier just to remember that the uh that two24 equals 256 um I'm not doing the math all the time I just remember the number and that's how I know uh but we'll come back to the most common net masks at the end of challenge one okay all right so we're on to challenge 1.2 and just I'm going to clean up here I didn't delete the previous one here uh so challenge 1.2 is create a VPC that allocates three host addresses so it's not saying how many IP addresses we need to allocate but how many uh hosts so how many addresses we can actually sign uh to ec2 instances if we were to launch in this VPC now remember that databus is going to reserve five IP addresses so we have to do a bit of math to figure that out all right uh so we'll go to create our VPC we're going to call this challenge uh 1.2 and we're going to put in a private address space of 10.0.0 again doesn't matter and what we're trying to figure out is what is going to be the forward slash to get exactly three host addresses so we come back to cider doxyz um so the idea is that everything goes up in uh those numbers there so if we were go to 32 that would give us one if we were to go to uh 31 that would give us two if we were to go to uh 30 that would give us four now remember we need three host address and eight of us takes uh five reserves so if you have 5 plus three that would be eight right so now you're just trying to get eight allocated here so if I just keep on going up to 29 that would give us eight okay so that seems like that would make sense so we're going to go over here and I'm going to type in 429 and I get an error and it says block sizes must be be between for sl6 and for sl28 so yes technically this makes sense because if we if we were able to create a a um a uh VPC with 429 We would be left with three host addresses but with AWS it has to start at for6 you can't make it smaller than that uh so we're just not going to be able to proceed forward here um but that's the answer so now let's take a look at answer 1.2 uh so create a VPC that allocates three host addresses so when you create a VPC in AWS uh and you specify that that CER range uh you're only allowed to make one between for6 and 42828 being 16 addresses so you can't uh you can't make a VPC with less than 16 addresses in it it's just not possible um so but let's just pretend that you're able to um because we want to arrive at that three host addresses so if you did 4/29 that would produce eight IP addresses if did sl28 that'd be four IP addresses but neither of them is three and what I'm trying to tell you is that IP addresses and host addresses aren't the same thing okay that's what we're trying to figure out how to get three host addresses not three IP addresses so the host address are the available IP addresses after we deduct the five addresses that adus reserves okay and those uh those available IP addresses are what we can actually assign to E2 instances so if we were to be able to uh do 4/29 and we were to minus the ad us reserved addresses we would be left with three host addresses so there you go but again you can't have uh you can't have less or you can't have 429 all right so we're on to uh Challenge number uh 1.3 which is create a VPC that allocates let's go ahead and make our next one so this is challenge 1.3 um and we're just going to give it a private address of 10.0.0.0 and we need to figure out the for slash uh address here to figure out how to allocate 65,000 531 host addressed okay so we come here and it was easy to figure out um 256 because that's what this number would be but how will we figure out 65,000 that one's really tricky um if it was me I would just I would just literally type it in so I do 24 I'd keep going up until I hit the number that I wanted um so uh we could do 24 and how about we do 16 here and there we go so we get 65,536 addresses it says that we need to uh um calculate 6553134 recommended number for AWS if you don't know what you're doing just always put 4/6 you get a lot of addresses and we'll go ahead and hit create all right uh and uh there we go so that's all there is to it uh that one's not too hard um we'll go ahead here and just delete this VPC and go take a look at the answer all right so we're looking at the answer for 1.3 create a VPC that allocates 6553134 addresses remember host and IP addresses not exactly the same host addresses are what you're left with after you've reserved so when creating a VPC ad of us generally recommends for 16 which is 65,536 addresses and that gives us a lot of uh growth for the future um so if we were to look at this uh in uh uh decimal notation and in binary math uh for6 would look like that so see how it's perfectly split and the subnet mask is 255 25500 uh again remember that host addresses are the available IP addresses after we deduct the five addresses from the reserved a US addresses so if we did that math that's how we get to 6553134 6 Network addresses so if I just go here I'm going to type in challenge 1.4 and we're going to give it a a private address bace of 10.0.0 SL and this is what we need to figure out okay and so the challenge is create a VPC that allocates 256 Network addresses so it's not talking about host addresses it's talking about Network addresses now remember uh the subnet mask it's splitting the host hosts on the right and the net uh on the net ID on the left right so this is going to determine how many available Network addresses there are uh but the question is how do you get to that number and uh because here it's just showing you the right side what you can get uh as the host ID so the amount of host networks after you minus your reserved adus addresses uh and my point is is that there is no easy way to calculate that and when you're doing the Cloud you don't really try to calculate Network addresses um so uh there's nothing to do because there's no way to figure it out easily um so there you go so now we're taking a look at the answer for 1.4 create a VPC that allocates 256 Network addresses so when you create a VPC you must specify a CER range between for6 and 428 but notice the emphasis that that CER notation tells you how many available uh host addresses are uh but it's not really telling us Network addresses and the reason why is that unless you're a really large Enterprise you rarely need to plan for how many subnets so rarely will you ever have to think about how many net IDs you're going to have to have uh now if we look at classful net networking if we use Class A Class B Class C you can see we have that math there that uh will tell us exactly how many um uh uh networks were able to support with this uh this way of addressing okay um but I just want to point out that even if let's say we were we were able to figure out how many net IDs we' be able to calculate so we know how many subnets we have iTab us has a service limit of 200 subnets per VPC so even if you could allocate more uh Network addresses you generally cannot fully utilize uh the AL Network addresses anyway so it doesn't really matter but I just wanted to point that out so you're thinking about that um it's not something that's going to show up in the exam but just helps you think about subnetting all right so we're looking at challenge two which is ipv4 subnet shopping all right and we got a lot of challenges here because this is something you really need to know for the exam it's going to really help you uh with those harder questions so create a VPC of 10.0.0.0 for s 24 and chop it into one subnet that fills 100% of the host addresses then we're going to do the same thing but we're going to do two subnets that fill 50% of the host address addresses we're going to CH change our private address space to 192.168.0.0 just because and we it's going to be 4/24 but we're going to chop it into four subnets uh the first Subnet will have 64 IP addresses the second will have 32 the the third will have 16 and the fourth will have eight then we're going to do uh uh a different private address 17216 0.020 and we're going to chop it into two subnets that are both sl24 in size then we're going to do create a a same same address space but this time 423 we're going to chop into two subnets that are 25 and 24 in size then we're going to do we're going to go back to our original private address space and we're going to do 416 and chop it into three subnets first one's going to be 2506 next one's going to be 512 the last one's going to be 1024 and the reason we're just doing different private address space it's just to get you more familiar with them again we could use whatever we want but I just want you to uh see them and type them in so you get exposure to them so there you go let's get started all right so we're on to challenge 2.1 uh if you're lost here just make sure you go to vpcs delete any vpcs that are in here I'm doing it in Canada Central and so this challenge is create a VPC with 10.0.0.0 sl24 and then chop it into one subnet that fills 100% of the host addresses so the first part's pretty easy we're just going to go challenge uh 2.1 and we're going to do 10.0.0.0 sl24 we're going to go ahead and create that so that's pretty darn easy now here comes the hard part we need to create a subnet that fills 100% of the host addresses so this is24 we're going to go subnets we're going to create a new subnet we're going to say challenge 2.1 subnet one and we're going to choose our VPC and uh it doesn't matter the availability Zone but now we got to fill in this cider block so what is it going to be well when you create subnets you have to follow the same private address space so we're going to do 10.0.0.0 and what we need to figure out is what do we put on the end here to fill 100% of the host addresses right well um if we want to fill all of them we're just going to match the numbers so we're going to put for sl24 and that should create the same amount so we'll go ahead and hit create that's going to create our subnet we're going to take a look at it here and it should tell us down below uh that this this is the available IP addresses so we have 251 uh and so that's pretty much all of them right because remember uh for SL 24 is 256 addresses right if we go over here we type in 24 that's 256 and the subnet is 250 uh uh 251 that's how many it's it's allocating eight of us took five because they're reserved go back to VPC I'm not sure if it would tell us how many are remaining be really nice if it did I thought it used to do that but maybe it doesn't but uh there you go so that's all it t uh takes to fill up the first one so I'll go ahead here and delete this subnet and we're going to go back to our VPC here and delete this one here and that's the end of this part and we'll go look at the answer in more detail so we're looking at the answer for .1 and what we did is we created a private address bace of 10.0.0.0 sl24 for our VPC and then we're going to chop it up into one subnet so it has to fill 100% of the host addresses so if we do 424 uh what we're going to be left with is um a mass that looks like this and that means we're going to have 256 addresses remember 424 equals 256 so if we want to fill up all the host addresses that just means all the addresses we need to make a subnet that is exactly the same size as our vbc so we just do 4/24 and then the uh the subnet address was 10.0.0 uh which was the start of our our subnet and where it ends is on 10.0.0 255 so from0 to 255 that's how many addresses 256 addresses we were allocating and so that's what we did there uh and just remember that if you if you use up all the addresses then you're not going to be able to make another um subnet it would actually complain uh because we would have ran out of space so there you go all right so we're on to 2.2 um and so this is where we create a VPC of 10.0.0 for SL uh uh 24 so the same thing okay and if you remember for sl24 is going to give us 256 addresses to work with Okay and actually that needs to be there and we're going to call this challenge uh 2.2 go ahead and create that and so now what we need to do is create our uh subnet so we're going to have to make two and they need to be 50% of the address so how are we going to get it so that um we have one that's 128 and another one that's 128 so if we look at here you see that these are zeros right you might think that you just split this in half okay but that's not the case because remember that this always doubles up so if you have 24 or 24 is 256 if you go 25 now it's 128 all right so what we need is two subnets that are uh 25 which will'll add up to 256 okay so we'll go here I'm going to call this uh um challenge 2.2 subnet one okay we're going to choose the VPC we need to match the private address space so 10.0.0.0 SL uh what we say 25 is 128 addresses okay and we'll go ahead and create that we'll go look at that subnet and so here it says there's 123 addresses that totally makes sense because um because eight of us uh takes away five okay from 128 let's go ahead and create our second subnet so it's going to be challenge 2.2 subnet 2 we'll drop this down here and uh we'll choose the next one so what should the next address be well remember that it's taken up um 128 addresses right so so that means that the next number is going to start at the end of that so it's going to be 10.0.0 um uh 128 sl25 okay so the reason it's 128 is because the first takes up 20 128 addresses and that's where it ends okay so we'll go ahead and create this and so now we have our two subnets so that's that challenge there uh so we're all done here you can see we have uh this and they're all in the same one and so we'll go ahead head and we'll just go delete these subnets we'll go back to our VPC and we will delete our VPC and we'll look at the answer in more detail uh now here okay all right so we're looking at the answer for 2.2 so create um a VPC with 10.0.0 sl24 chop it into two subnets that fill 50% of the host addresses so if we do 424 that gets gives us uh 256 avail IP addresses meaning that we need to uh uh make two subnets that are that have 128 IP addresses respectively okay so if we do 425 and 25 that makes up a 424 so we have zero I know it looks like a c but it's actually zero I don't know why it's cut off but 0 to 128 and 128 to 256 notice the addresses it starts at zero and then it ends at 127 because 128 addresses if you start at zero is 128 addresses the next one starts at 1228 and ends at 255 okay so hopefully that's clear why we started the next subnet at 128 because that was the next available address after the the remaining one from the first Subnet so we create two subnets that are 425 that fill all the available host addresses you can create a subnet exactly the same size as your vbc but you won't have room to create another subnet um so just be aware that we filled it up so we would be able to make another one what I really want to point out out is that uh 225s make up a 24 so uh the previous number is uh is always half of the last one okay all right so we're on to uh 2.3 this one's a little bit more complicated so we're going to create a VPC um and we said that we wanted the private address space we'll just name this challenge 2.3 we wanted the private address space to be 192.168.0.0 again we're just naming it whatever we feel like uh we' like we want it to be it's going to be24 remember 424 is going to equal 256 available addresses right and so the other part to this challenge is we need to create four subnets we're going to create one that's 64 IP addresses one that's 32 uh one that's 16 and one that's eight okay so um what we'll do is we'll make our way over to uh subnets here and we'll go ahead and make the first one so we'll create a subet um and we'll say challenge um 2.3 subnet 1 okay we're going to choose our vbc we need to match the private space we're going to start it at uh the start here which is 0.0 and this is where we need to figure out how are we going to get 64 IP addresses so we'll make our way back over to our cider range calculator remember this is 256 if we work our way right they always double up right so 32 is 1 31 is um uh two okay 30 is 4 um 29 is 8 28 is 16 again we're trying to get to 64 we're almost there so 27 is 32 and then 26 is 64 so that's the one we need which is 64 okay so we'll make our way back here and we're going to type in um what we say it was 26 to get 64 addresses for our first one and no problem there first one's is always pretty easy so what we'll do is we'll go to the next one we'll say challenge uh 2.3 subnet 2 we're going to drop that down there we're going to copy this in and the next one needs to be half the size 32 so if our last one was what did we say 26 remember we said that the next one uh the the next number up is always going to be half the mount so if we do 27 that's how we get 32 pretty easy okay so if we do for sl27 notice we get an error because it's overlapping it we have to find the number that it ends on right so the last one had 64 addresses so if we do 64 here now we have no problem because that's where the the next available address is so we'll go ahead and create that there no problem so far uh what we'll do is we need to make our third subnet our third subnet is going to be um uh half the size of our last one so 32 is 27 how are we going to get 16 16 is half of that it's going to be uh we go up it's 28 so that's how we get 16 so we'll go ahead create the next one and we'll say challenge uh 2.3 subnet 3 we're going to drop it down here we want to use the exact same IP address space 192 uh 168 do. z.0 but we got to figure out what the next available address uh is so um our last one was 64 and um uh that's where we started our last one but we added 32 so 64 plus 32 um is um well shouldn't be too hard 32 plus 48 oh that doesn't make no no 92 92 right it's 92 and um or sorry 96 I don't know why I'm having a hard time with math but if you add 64 plus plus 32 you get 96 so that's where our next address is and uh we want this to be 16 IP address right so it's just going to be 28 right so we'll put in 28 there and that's how we get that one there and then we need to make one more which is we need to create a subnet uh subnet 4 with only eight IP addresses okay so we go ahead here and I'm going to go and type in challenge 2.3 uh subnet 4 and I'm going to write in uh challenge 2.3 and we're going to uh look at what we need the next number to be so remember uh it's 192.168 and our last number started at 96 and we add 16 so it's now going to be 112 is where our next number is going to start and we need this to be um eight IP addresses so if we go up here 16 is 28 we're going to switch this to 27 that is oops wrong way 29 it's going to be that so we're going to go ahead and put in 29 but look we get an an issue here okay so um when we do for SL 29 it's just not possible for us to do that um because you're not allowed to make subnets smaller than 428 so this last subnet is just simply not possible okay so that's the end of the challenge so we'll go here uh we'll go ahead and delete these subnets all right we're going to go and delete this VPC and we'll talk about why that last one's not possible and we'll just get a a bit of a visual because I know that's quite the mess there that we just went through all right so now we are looking at answer 2.3 and this is create a um a vbc with 192 168 0.0 and chop it up into four subnets so our first Subnet is 64 IP address um the second is 32 the third is 6 16 and the last is eight notice the numbers are halfing every single time um so we set our vbc is going to be 424 that gives us 256 IP addresses and down below this is what we were doing and so the first Subnet was 192 168 0.0 it started here again this is not a c it's a zero and so uh it went from 0 to 64 and so the and address is 63 and that's why for subnet 2 it starts at 64 and we needed to make this half the size so notice that it's 26 it's going up 27 27 is half the size of 26 so 64 to to 95 and then uh our next subnet uh is only need to be 16 IP addresses and it's going to be for sl28 because again it halfs as the numbers go up um and so it starts at it starts at 96 and ends at 112 and then we attempted subnet 4 which was just not possible because you cannot create a subnet that is uh less or greater than the number uh 28 right so you can't make something with fewer than eight um IP addresses uh so I just want to point that out that when you create subnets you just simply cannot make them smaller than um that they have to be between 16 and 28 okay and the reason why is that a VPC you can't create a VPC that's smaller than 28 so why would you be able to create a subnet uh that is uh smaller just wouldn't make any sense uh so uh there you go all right so we're on to 2.4 create a subnet with 172 6.0.0 sl20 and chop it up into two subnets that are both 24 inside so we're going to go to cider here I just want to put it in 20 here to see the size here and that's going to give us an address range of 4,000 uh uh 496 addresses we'll go and create our vbc this is challenge this is challenge 2. 4 and we're going to say 172 uh 160.0 again we're just doing that for fun because we can make them whatever we want and we're going to do it for20 we'll go ahead and create that there then we'll go down to our subnets and create our first one and we're only making two subnets here and they have to both be24 so it's very specific what we need to make them if we go to 24 uh that is 256 right so we'll say um challenge do 2.4 subnet 1 we're going to select our vbc again it has to be the same private IP address space 17216 0.0 right and we said they have to be24 which is going to give us 256 addresses okay we'll create that subnet we'll create the next one so we'll say challenge uh 2.4 um subnet 2 we'll drop this down again it has to start with the private address space uh and it's going to be24 but where does this one end right so um remember that the first one is 256 um 256 addresses right and so if it's going to roll into the next one it's not it's not going to start at 250 it can't start at like 256 that doesn't make any sense right so it actually rolls over because 255 is the upper limit so once it rolls on to 256 it's actually one now and this is zero so that's where the next address starts okay so go ahead there and create that and that's all you needed to know was the fact that when it hits 255 it rolls over to the next number which is one so we'll go ahead and delete that and we'll go back to our VPC and delete that and now we'll look at the answer in more detail okay so let's take a look at answer 2.5 for where we created a VPC with 172 160.0 sl20 and we're going to chop it up into two subnets that are both 24 in size so if we do for size 20 um it's going to give us 4,096 IP addresses which is a lot uh but it was said that we had to make them 424 which again equals 256 IP addresses uh and so the trick to this question is that the first one starts at zero and ends at 255 but when you need the next available address you can't write in 256 it's going to roll over to one okay so that's what I want you to know is that if you if you uh if if you fill up that entire first block it's going to go and be one next all right all right so let's do challenge 2.5 create a VPC with 172 160.0 423 and then chop it into two subnets one the size of 25 and one the size of 24 so we'll go here we'll say challenge uh 2 uh or uh challenge 2.5 and then we're going to go here and make it 172 172 1680 do0 sl23 so what is 23 remember 24 is 256 and 23 is one number less so that's going to make it 512 remember if you remember uh that number there it makes things a lot easier so see again 24 right 256 the next number down is going to be uh 512 okay so we'll go ahead here and just uh create that now we need to make two subnets one that is 25 and one that is 24 so if 24 is 56 and we go 25 it's going to be half of that it's going to be 128 right see it's 128 so we go here and say challenge 1 point or sorry uh 2.5 subnet one we're going to drop this down we're going to choose the same private address space we're going to put for25 again that's 128 addresses we're going to go ahead there and hit create we're going to now make our second subnet so challenge 2.5 um subnet 2 we'll drop this down and uh remember the last one is 128 so that's how many IP addresses so we'd think that the next address would start at 0.128 and we need to just do a24 uh and 24 is 256 addresses and we'll hit create it doesn't work okay and the reason why is that you can't start a number in one octet and have it end in another okay so if the next uh if 424 adds 256 addresses right this 128 is going to then go into one whatever it is 1. 127 that the end will be 127 and that just doesn't work so what we need to do is just put at the next available octet which is going to be one uh and I don't know why I keep on doing that but uh I did this video before and I made the same mistake but it has to be 172 16 uh 172 not 127 but I just want to show you that if you did do it this way 128 it's not going to work okay it's got to be the next available octet because our IP can't bleed into another octet just won't let us do that so we'll get go ahead there and create that no problem there we'll go ahead and delete both of these and then then we'll go and delete our VPC and that is the end of challenge 2.5 and we'll look at it in more detail uh here in a moment all right so we're looking at answer 2.5 create a VPC with 172 160.0 423 and create uh two subnets one that's 425 and one that is 424 so 23 is going to give us 512 IP addresses and So Below we're going to 425 424 we clearly have room left over and so when we created our first Subnet it was started at 0.0 since it's 128 addresses it ends at 127 because we count the zero as a number the next one you'd expect to start at 128 but we ran into an error and the reason why is that um it's where our address ended so see how it started in zero but it didn't end in zero it it ended in one and that's the problem they can't cross over like that and so what we had to do is push it at one so technically we have uh addresses not being used between 128 and uh 0 255 okay uh so you just need to be aware of that so subnet cannot start in the middle of of an octet and end in another a subnet can uh start in the middle of an octet and end within the same octet a subnet can uh start at the start of an octet and end anywhere in the same or another octet so that's all the use or the Ed cases there that you need to know so there you go oh and just one more thing uh just remember that the octet is between the range of 0o and 25 and the last number in the dot decimal is 255 so uh that's just good to know so there you go so for Challenge three we have IPv6 subnet chopping this is just to get you a little bit of experience with IPv6 so we're going to create a VPC which is 10.0.0.0 4/24 and we're going to create two subnets so one and they're both going to be size 25 and then just we're going to assign IPv6 ciders to them okay so there's not much to learn here just just to go through the motions here to so you can say yes I've touched IPv6 all right so we're going to start uh challenge uh 3.1 which is the only challenge in part three which is for IPv6 so I'm going to make a new challenge here or uh a subnet or vbc we'll call it 3.1 we said that we're going to make it 10 0.0.0 sl24 24 being 256 IP addresses we're going to enable IPv6 going to go ahead here and hit create we're going to make our way over to subnets we're going to have to create two here okay because the whole point of this is just to show you how to assign um subnets to IPv6 is okay so we'll say oh0 point1 here we'll say subnet one we're going to choose this uh we're going to make it half the IP address range so that's pretty easy we'll do uh if it's 24 we want half of it then we'll chose 25 CU remember if we have 25 it's half of half of 24 okay we're not going to assign the IPv6 I want to show you how to do it manually we'll go ahead and make our other subnet here challenge uh 3.1 subnet 2 we'll drop this down here we're going to do 10.0.0 uh 0 sl25 uh the next one starts at 128 cuz remember it's half the size there and so now the way we need to uh to assign our our IPv6 notice there is no IPv6 we're going to go up here we're going to go and hit um edit the cers here we're going to hit add and now we have this ability to put in a number here so I'll just put in one um oh I don't think I can start at one uh what does it start at let me just think here for a second it starts at uh oh maybe it's 01 I think that's what it has to be okay I always I always forget that and we'll go to the next one and we'll hit IPv6 and so we'll add another one we'll say 02 okay and that's how we assign IPv6 addresses um there's no point in getting into any kind of IPv6 math it's not like ipv4 where it's very easy to make sense of this is super super complicated but I just wanted you to experience IPv6 uh that's all there is to it we'll just go ahead there and delete this and we'll go ahead and delete that and we'll just go look at the answer now all right so let's look at the answer here for 3.1 and so we created a subnet or sorry VPC 10 0000 uh 24 we created two subnets for25 so that's half the size of our uh VPC each and then we wanted to assign IP IPv6 cers so when you enable IPv6 you don't need to choose a net mask or subnet mask whatever you want to call it adus will set it to 56 so you get many addresses it doesn't matter uh you don't have to have fine to control of this so uh yeah I didn't point that out in the in the video but if you had noticed uh if you look back you'll notice that when we created our our vbc it was it was hardcoded as for sl56 so what you do is you say Amazon provided IPv6 cider block and it puts a for 56 cider notation on the end there um you add IPv6 at the time of creating the vbc or after its creation so you can do it either or uh same thing with subnet so when you apply an IPv6 to your subnets all you do is choose two digits the IPv6 subnet are hardcoded at for mk64 okay so all we do is put in 01 02 Etc and there you go just a quick introduction here to Ita service catalog it enables organizations to create and manage cataloges of products that are approved for use on adus to achieve consistent governance and meet compliance requirements adus service catalog is an alternative to granting direct access to adus resources of VI databus console and the advantages of using it is you get standardization selfservice Discovery and launch fine grade access control and extensibility and Version Control all right so let's take a quick look at the anatomy of the ad service catalog and so I have a uh one side for the administrative user who manages the catalog and then the end user who uses the catalog and we have these nice um uh architectural diagrams describing all the components and we'll just walk through them quickly here so on the administrative user you create portfolios and portfolios are just containers for products permissions and constraints and then we're going to look at those three now so uh you have the product itself and this basically is just a cloud formation template which could have Associated service actions we'll talk about those in a moment then you have your permissions about who can uh View and launch products and this is you assigning groups roles and users that are approved to the portfolio which grants you access uh for those products then you have constraints and these apply rules on provision uh products uh there's five different kind of rules and just kind of limits on how these products can be used then on the end user side uh you have the catalog and it's a userfriendly console to view and launch products then you have the products themselves and they're just visible in the catalog to launch uh when a product is uh launched it's considered a provision product and again these are just cloud formation Stacks so that's all it is underneath and then there's these things called service actions service actions allow you to uh give end users some additional controls around these launch resources and they are just really SSM documents that to perform tasks on the stack so there you go talk about the types of users in the ad service catalog which we saw in the anatomy there a moment ago and there's two types of service catalog users we have catalog administrators and end users so catalog administrators manage a catalog of products organizing them into portfolios and granting access to the end users an administrator uh is uh uh technically responsible for preparing the cloud information templates configuring constraints managing IM roles assigned to products and the end user uses the Management console to launch products that admins have granted the uh access to the end user so there you go so let's take a look at the administrator side and then we'll look at the end user side starting with products so a product is just a cloud information template that defines the resources that will be launched and so here I have an example of a cloud information template which launches an ec2 uh T2 micro and when you create products you can actually associate um an adus budget with them or create one on them this is going to help you forecast or um uh keep uh keep track of cost uh per product once a product is created uh it can only be edited you can't delete it um and also if you want to delete a product it has to be removed from the portfolio and not provisioned by any users in order to delete it and in order to for products to be made visible to the users you're going to have to add it to Port uh to a portfolio and then associate some permissions which we'll look at in the administrator section okay so let's take a look at portfol olos uh in service catalog here on the administrator side so uh here is a portfolio and it has a collection of products and you can set constraints on how those products are used uh and you can also associate uh different types of identity such as groups roles or users that's going to determine who's able to see uh the actual products and launch them okay so um what I do want to uh emphasize though is that all products in the portfolio will be shared to the added identities and you cannot limit some products to some users in a portfolio so you can't just say and those constraints don't help either it's not like you can say okay only these two products for these two people and these two products for those two people you just have to make separate portfolios uh in those use cases but I just want quickly want to show you you can add a group you can add a role or you can add a user and that's going to that's going to determine who has access now let's take a quick look at constraints for portfolios so you can create constraints for specific products in your portfolio uh and so this is what it looks like on the left hand hand side but let's walk through all those constraint types so the first type is launch and you what you can do here is you can specify an It Roll instead of the end user credentials that would be used in launching that resource the reason you'd want to do this is so that you don't have to Grant end user permissions to a service directly and this will be less permissive and more secure then you have notifications and these send product notifications to a stack then you have a template this limits the options that are provided to the end user when launching a product it also sets the restrictions on the underlying cloud information parameter inputs so you could just say only allow launching a T2 micro that one's extremely useful then you have stack set this allows you to configure product deployments across accounts regions using a cloud formation stack sets I actually don't really understand this one but it's not a big deal it's not going to show up the test but I just wanted to point that one out and then you have tag updates so you can choose to allow or disallow your end users to update tags on resources associated with a provisioned product okay all right so now we're going to take a look at the end user side when they're using a product uh so an end user has a userfriendly interface via the adus console when opening up the adus service catalog they also have um an API so if you really wanted to give your own skin you didn't want to use the one that's part of the console you can totally do so but there are two things you're going to see you're going to see products and provision products so you open up the catalog there's a product you're going to go ahead and launch that product and then you can uh name it and choose the product version because you products have versions and then you're going to fill in the parameters and parameters are just basically cloud formation template parameters um but you know that's just what it is uh and then uh what we can do is just before we launch the product you can choose to create a PR provision product plan and a plan includes a list of resources that will be created or modified uh before the product is launched this might sound really familiar if you know cloud formation uh templates which we'll talk about in a moment here but you create a plan and then it says that it's been added and so this is basically CLA formation chain sets why ads calls it something else I don't know but that's what they do uh and then once you have your product provision there it is and then you can get more details about it uh and uh that's the information about the product and then in the top right corner we have some actions uh and this is where we can actually uh add our own userdefined actions which we'll look at next called service actions so now we're taking a look at service actions which appear on on the end user side and so service actions are SSM documents that are associated with a product to allow the end user to perform maintenance so what you can do is you can choose SSM documents and the administrator can just associate them to a product and then as the end user when you have a provision product you you have that drop down and you have those options so that's your way of extending functionality uh that you have governance over around a product so there you go hey this is Andrew Brown and we are looking at Route 53 so Route 53 is a domain name service DNS and you can think of it as something like GoDaddy or name cheap but with integration specifically with ads to make it um a lot more powerful than these two so what can you do with Revue 3 well you can register and manage domains create various record sets on a domain Implement complex traffic flows uh continuous monitoring uh records with uh health checks resolve epcs outside of ads so let's say you need to um uh have some resolution with your on premise stuff this is all stuff that you can do I want to point out one distinct thing that I noticed that um Revue 3 does that other cloud service providers do not do which is the ability to register domain so if you were to go use Microsoft you can't do that there and then uh Google used to have Google domains but they gave it away to Squarespace so it's basically just ads that is allowing you to steal um register domains uh directly so that is really interesting what could be a use case for using R3 well um just when you want to Route things so the idea is that imagine you have incoming traffic into uh into your vbc through R 53 uh or anywhere uh it doesn't necessarily to be a vbc but generally that's where it's going to be going and so you could Route traffic over to let's say an ALB or you could Route traffic to an instance um uh as that or you can Route traffic to an API Gateway or to a cloudfront or an elastic IP so there's a lot of places you can send uh your routing data but we'll cover that in the upcoming slides here okay so here I have a use case and this is actually how we use it at exam Pro um is that we have our domain name uh you can purchase it or you can uh have R 53 manage the uh the name servers which allow you to then uh set your record sets within row 53 and so here we have a bunch of different record sets uh for subdomains and we want those subdomains to point to different resources on AWS so for our app our app runs behind elastic load balancer if we uh need to work on an Ami image we could launch a single E2 instance and point that subdomain there for our API if it was powered by API Gateway we could use that subdomain for that for our static website hosting we would probably want to point to cloudfront so the ww. points to a cloudfront distribution and for fun uh and for learning uh we might run a Minecraft server on a very specific IP probably would be elastic IP because we wouldn't want it to change and that could be Minecraft exampro doco so there's a basic example but we're going to jump into all the different uh uh complex rules that we can do in ref 3 uh here hey this is angrew brown and we are taking a look at hosted zones for R3 so a hosted Zone is a container for record sets scope to Route traffic for a specific domain or subdomain and there are two types of zones we got public hosted zones and private hosted zones the key difference is that public hosted zone is for uh traffic inbound from the internet where as private hosted zones is traffic within an Amazon VPC um so to configure the difference here if you are creating a public one it would look like this uh when you use the API you have to provide a color reference this is just a unique uh value so that uh when you send the send this API call um that it won't create a duplicate record because that's something it really doesn't want to do so here we're using the date we could be whatever you want uh we don't have to supply a comment there but that's just a comment to help identify what this hosted zone is for if we are creating um a hosted Zone that is private we would be specifying a specific VPC and its region and we' be setting the hosted Zone configuration uh to be um true for private Zone uh so one another thing that I want to specify here because I said up there it's for domains and subdomains but um one thing that you might do this is a more complex use case and it's something that we actually did uh at exam Pro uh for our application but let's say you wanted to build a SAS product where each customer had their own subdomain on the apps subdomain so the idea here is that you have this subdomain called app and then everyone's going to get their own one so you know whoever it is that they can choose whatever the name they want to be it's going to um when it routes to that it will the Apple know to be scoped uh for that particular customer uh do customer's domain or tenant uh so here is some uh cloud formation code and uh we'll just walk through this really quickly so the first thing is you'd have to create a hosted zone so here we have a hosted Zone on the uh primary domain and then what we' need is we need to create a record Set uh to the subdomain pointing to the name server of the sub sub doain so what we're talking about here is uh This Record set here and notice that this record set belongs to the um main hosted Zone and it's using App here and it's asking for name servers and then it's point it's referencing the name servers from the other subdomain uh hosted Zone okay so then we have this other host host Zone that we created and this one in particular is for app as as opposed to the one up here that does not have it uh and then what we'll do is we'll create a record set within uh the subdomain hosted Zone and we'll put a wild card on it here with an a uh an a record and then all it's going to do is anything that's captured in this wild card is going to go back uh to this domain but the idea is that we will know um the original uh address so that we can uh determine what tenant or uh specific customer were referencing and so this is a real real example and I think this is a very common example if you are uh building SAS applications but just to kind of distinguish that when you have a hosted Zone it can be for a domain or a subdomain okay we are taking a look here at record sets which are collection of Records which determine where to send traffic the way it works with the adus API is you'll actually send it to batch changes if you're using the adus Management console it looks like you're individually creating adding deleting but I want you to know that underneath does this kind of back thing if you've ever used any other kind of domain name uh services or servers you'll notice that they kind of have a batch approach to it so that's why I like showing the API here uh there are the following record types that are supported so we have our uh Alias record the foras which I believe is for iv6 CAA C name DS MX napor NS pointer s SOA uh SPF SRV txt um and that's all of them there if you know what these are you should have done our domain name uh server primer where I talk about all the main ones here I'm not going to talk about them here but you should learn some of them as we work through this I just want to point out here um uh what this is doing over here so here we are calling the change record uh record sets we're not creating or updating it's just change and then we provide our changes and we give it an action and in here we are setting a c name and we're saying take this domain and point it to that domain simp it's pretty simple uh this API call has three types of actions we you have the create the delete and the upsert create creates delete deletes upsert will create a record um if it doesn't already exist and if it already exists it's going to update it there's a special record type called record Alias or record uh type and the idea here is that if you set um this to be a then you can provide Alias Target and you can specify DNS name to a very specific adabas resource and the reason why you'd want to do this and it's preferred to do this is because adab us will continuously monitor that uh the s endpoint and it will um it will make it so that you don't end up with dead uh dead routes so if you're pointing to a DNS record like it'll always map this thing will always map to the latest IP address if it changes and so this is is definitely preferable but when you utilize it Inus Management console you just like click through and it connects to it but if you're using cross management accounts like in another account you can't just click and select it you have to know what the DNS name is and put it in there so it's good to get some exposure to looking at what that looks like um these Alias targets can um point to cloudfront beanock elb S3 web side endpoint resource record set a VPC endpoint and API Gateway end points for regional apis so there you go uh so R 53 has a tool called traffic flow and it is a visual editor that lets you create sophisticated routing configuration for your resources using routing types uh so you'll see a lot of of our routing types that we are aware of here it supports versioning so you can roll out uh or roll back updates and it costs I think about $50 per policy still um so yeah it's a visual tool it is very expensive so you have to really decide whether you want to use it or not um but uh it can do a lot more complex things because you basically can take a lot of these policies and merge them together okay so there are uh seven different types of routing policies available in R 53 and so I want to go over them here and then we will dive into them individually so the first is simple routing this is the default routing policy where you have multiple addresses uh address results that uh are randomly selected when being returned you have weighted routing so this routes traffic based on a weighted value to split the traffic latency B uh based routing Route traffic to your region resources with the lowest latency failover routing so Route traffic if the primary endpoint is unhealthy to the secondary endpoint geolocation routing so Route traffic based on the location of your users Geo proximity routing which sounds similar but Route traffic based on the location of your resources and optionally shift traffic from resources in one location to resources in another multivalue answer routing so response to DNS queries with up to eight healthy records selected at random sounds very similar to simple routing but uh uh we'll go over the differences when we look at that okay simple routing policies are the most basic routing policy rev3 and these are going to be the default policy when you're creating them via the inabus Management console the idea here is you have one record and provide multiple IP addresses when multiple values are specified for a record R 53 will return all values back to the user in a random order so basically um you know if you have a domain you're saying www.ex pro. it's going to pick one at random and here is what the implementation would look like I'm going get my pen tool out here so we can just look closer so notice we have our a record type here which is you can use this with other things but we're just using it with an a record and uh notice that uh here we have our resource records and uh we just provide them so there's no other additional option that's just the default option so we're not saying like this is simple it's just that if we use the resource records here it's going to to be a simple policy okay weighted routing policies let you split up traffic based on different weights assigned uh this allows you to send a certain percentage of overall traffic to one server and have other traffic to be directed to a completely different server so for example if you had an ALB running experimental features you could test against a small amount of traffic at a at random to minimize the impact of the effect so coming back over here with our pen tool notes that says here weighted routing policy weighted records and then we're providing the weights here so hopefully that is clear there you go failover routing policies allow you to create active passive setups in situations where you want a primary site in one location and a secondary data recovery site in another so R3 automatically monitors health check from your primary site to determine the health of the endpoints if an endpoint is determined to be in a failed State all traffic is automatically directed to the secondary location so for example we have a primary and secondary web app backed by ALB and ruffed 3 determines our primary is unhealthy and fails over to the secondary ALB I'm going to get my pen tool out here so we can have a closer look and notice that we are setting the fail over routing policy notice that it has a set identifier just like our previous example I'm calling this one primary and secondary and we have to have different identifiers between our uh multiple resources so that we can use the same name and type and then we are providing the record resources uh uh resource records but it's this failover routing policy that helps us determine based on the identifier where it's going to go so there you go geolocation routing policies allow you to direct traffic based on the geographic location of where the requests originated from so for example this would let you route all traffic coming from North America servers locate North America regions where queries from other regions could be directed to servers hosted in that region so let's take a look at our code example I'm going to get my pen tool out here notice it uses a set identifier to uh distinguish between these records as they both have the same name and type now we set the geolocation routing policy uh each with their country code and then we have our resource records down below so hopefully you're starting to see a pattern in terms of how these things are configured but it's not too difficult Geo proximity routing policies allow you to direct traffic based on the geographic location of your users and your adus resources you can route more or less traffic to a specific resource by specifying a bi value Bice values expand or shrink the size of the geographic region from which traffic is readed or to you must use a r 53 traffic flow to use Geo proximity Runing policies which is why you're not going to see me uh showing you C commands API commands here so how does it work well this is the standard bias that you would see in the traffic flow and you can adjust it so if you said plus 25 in Us East then it will grow or if you reduce it then it will shrink uh in R3 traffic flow you can select any region and visualize the the biased so you select one and you can see there's a lot to choose from and here's an example if you had all the regions selected because you select which regions you want to have biased with and here is if it was custom so we are giving a very particular coordinate like I think I think I want to say Hawaii but I'm not 100% certain if that's Hawaii but I think I was trying to go for Hawaii there but there you go latency based routing allows you to direct traffic based on the lowest Network latency possible for your end user based on region so here is a more complex looking example as you can see it's a lot more code than the other ones but it requires a latency resource record to be set for uh something like ec2 or elb that hosts your application in each region um so the idea is that we would have to have um a redundant resource of course and so you know an example could be here would be you have two copies of your web app backed by ALB one in one region one in another and it's going to choose where the lowest latency is but getting my pen tool out here let's take a look at this because these records look very similar so we have uh this on example.com and this one on example.com they're both a but the key difference is that you have to provide a set identifier because when you have two with the same name and the same type then and you'll need this identifier and so we're just naming them based off the region that they're in to distinguish them and notice that we set latency routing policies based on its region and then we have Alias Target to say where it is going it doesn't have to be an alias Target but that is what we're utilizing here so hopefully that is clear and we will see you in the next one multivalue answer policies let you configure ref3 to return multiple values such as an i address for your web servers in response to DNS queries multiple values can be specified for almost any record prty 3 automatically performs health checks on resources and only returns values that uh of ones that are deemed healthy so it's very similar to simple routing however with an added health check for your record set resources so it's not just going to continually select ones that are incorrect noce looks very similar to simple U uh simple routing where we have the resource records but now we also have this multivalue answer uh uh routing policy and so it says EV value a Target Target health checks but this actually should be turned to true and not false otherwise what's the point right that it's not really working so just notice that this this should have said true I'm not sure why I have false in here um but yeah there you go another feature R3 is health checks uh health checks are useful when you want to to um check if something's healthy and then tell it to go somewhere else and some particular services or routing rules you'll have to use them so it checks Health every 30 seconds by default and can be reduced down to 10 seconds a health check can initiate a failover if the status would return unhealthy a cloudwatch alarm can be created to alert your St uh you of a status of a unhealthy health check a health check can monitor other health checks to create a chain of reactions it can create up to 50 health checks for an a end point within or link to the same account um that's an example of a health check at app. exampro docl the the UI for whatever reason in health checks is extremely old I don't know why but it's like we're two versions behind so just understand that I'm not showing you an old slide they just have not updated whatsoever the UI for I don't know six six seven years uh there is a difference in pricing depending on whether you're using a end point or a nonous Endo so just understand that it's going to be more expensive for nonous endpoints when you're using health checks okay Amazon rev3 resolver is a DNS server that allows you to resolve DS queries between your on premise Network and your VPC so rev3 resolver was originally known as point2 resolver or Amazon DNS server I don't remember the reason why it was called Point 2 there is a story behind that which is now forgotten probably one of my older videos what I used to remember but if you want to have a DNS server that's going to work um uh with your VPC then this is what you're going to utilize so just look at the diagram here uh and there's two things that you can possibly configure which is are inbound and outbound endpoints this really just depends on the direction of the traffic you need things to go so if you have an inbound resolver this allows DNS queries to your VPC from your on premise Network and then outbound is uh DNS queries from your VPC to your on premise Network notice that you create resolver rules and these are going to work with private hosted zones public domains and VPC local domain names I'm greatly simplifying this service um but because it is quite difficult to utilize if you don't have an on premise uh environment but uh we do cover somewhere else maybe not in the rev3 section but we do cover about rev3 resolvers DNS firewall which is uh one of the uh big advantages of using R3 resolver but there you go domain name system security extensions DNS SEC are Suite of extension specifications by the internet engineering task force for securing data Exchange in domain name systems in in Internet Protocol networks so dnsx signing lets DNS resolvers validate that a DNS response came from Amazon rev 3 and has not been tampered with uh this is a very simplified version but the idea is that you will create a KS ksk signing key uh and then you will enable DNS SEC um I say this is a a very very simple example because there is a lot involved here and the timing of it and uh it's very tricky to do this but generally if you have a domain name you should be using DNS SEC because this guarantees that people cannot impersonate your domain so if you're using like something like SCS uh withs they force you to actually have to use it um or if you are found not to be using um uh the stuff then maybe your uh domain will not be received by um recipients and things like that so this is something you definitely want to turn on but it is a a a very complex process um than what I'm showing here okay zonal shift is a capability in Amazon refid 3 application recovery controller also known as R 53 Arc uh it shifts a load balancer resource away from an impaired a to a healthy a with a single action so imagine you have a problem in AA it's going to make everything go over to ASB Z shifts are only supported on albs and nlbs with cross Zone load balancing turned off zonal shift isn't supported when you use albs as an accelerator endpoint in ads Global accelerator you can start a zonal shift for a specific load balancer only for a single a you can't start a zonal shift for multiple A's I was very tempted to put this in the elb section but it's over here in the rough 53 section because it is a rough 53 feature um but uh very straight forward what zonal shifts are but there you go hey this is Andrew Brown and this fallong we're going to take a look at R 53 so I already have some domains registered I'm pretty sure we register domain and probably Cloud petitioner I don't know what account it's in but I do have like one called like the Frankie Alliance and other domains that I've used in the past um but I can't find them so what I'm going to do is just use one of the existing ones I have uh maybe cloudborn causing me issues but what I'm going to do here is just click into cloud.oracle.com um you can see when this was registered when it's expiring the name servers when you create a domain um these name servers will get set to whatever they are so just understand that there will be um variation in terms of the name servers that are set but once you have a domain what you really want is you want to have a hosted zone so I'm going to go over here and by default it should create a hosted zone for you and if I go down here I have Cloud borgor I actually have two I don't know why I have two of them um notice that Su are created by ads so this was created by ads this is the original one and then I have one in here so right now club.org is not doing anything so I'm going to click into this one we'll take a look at what we have here um a bunch of nothing so what I'm going to do go back I'll click in the other one here what's this one a bunch of nothing so I'm going to click back oh Cindy was using it for something not exactly sure what let's just delete that record there and I'm going to go down below here I'm going to go ahead and just delete this one out okay and yeah if we wanted to we can create our own hosted zone so if we go here you have the option for public and private we are not going to create one because we should just use the one that comes by default so if you want to follow along go by domain and once you have that domain come back over to here and uh once it's confirmed it's created we'll go ahead and play around with these records so I'll give you a moment to do that so go out and do that and then come back here so now assuming that you have your domain notice that we have have uh two records here we have NS for name server and then SOA I always forget what SOA is it's a standard one that's always there um SOA uh record just go look that up here quickly start of authority storage important information about a domain or Zone such as the email address of the administrator so yeah those are always pretty standard that you're going to have um but now let's say we wanted to uh point this to something like a web server uh so what I'm going to do is I'm going to go over to ec2 and I want to launch up an ec2 instance so I know in here we have an example um example code for a dc2 instance I'm going to launch this in G pod you can launch it in whatever you want but what I want to do is launch up an ec2 instense and I want to um have an IP address attached to and then we'll point to that IP address okay so I'll just give this a moment to spin up and I'm going to find where I might have a file so I I know I'm just ending up with a lot of folders here but I'm going to do is make a new folder once this is responsive we'll call it R 53 and I'm going to search for this because I want to find where I already have an ec2 template so I'm go here and say edit finding files E2 colon colon maybe instance here and we'll give it a moment to search everything is not working very intell so I'm going to go here and say ec2 instance uh clir so I'm just looking for that particular name and now my internet's not working okay well I'll be back here when my internet's back so just give me a moment okay all right so it's back uh yeah so I was just looking for this as a means to search as it might be a bit faster oh kind of worked I think it was just my internet was going slow and so one of these templates will have um a very simple ec2 server so that's why I'm just looking for here um this one looks really simple so I'm going to go ahead and grab this template here that was under uh app config so I'm going to go into this one and I'm going to grab our template and our deploy I'm going to go ahead and copy these and I'll go down to yeah's just say allow I'm going to go down to R3 folder and I'll make a new folder here called basic just in case I end up doing something more complex and I'll paste these files in here so in my template file I do not want to actually do app config so I'm going to take out this app config policy and I do want an SSM roll because I it would be nice to be able to log into the app um in here it's installing a bunch of stuff I don't actually want what I would like is a simple Apachi server so I will search for that um this will be under the edit finding files and and maybe under EBS basic you might have that yeah here we go so here is a simple simple Apache server we'll go ahead and copy this and we'll go back and we'll replace this it looks pretty good and this is open up to the Internet so that is great the only thing this doesn't have is a static IP address which is totally fine um as I think we don't need one as of yet but I'm going to just change this so that this says server one and I might want to have more than one uh server here because when we're doing these routing rules we're going to probably need more than one uh so I think this is fine or it would be really nice if I could um put the custom name that I want in here for the server so I'm just trying to think here how could I do this I'm going to go ahead and ask chat GPT because this is kind of a pain figuring out the cloud formation for it so I just say uh we'll put this in here and just say I want to dynamically change the server one text with a parameter input for cloud formation and let's see if it actually gets it right is a little bit tricky but we'll see what if it does it so that's kind of what I was hoping for um I yeah I think we can do that that looks right and so I'll go back here and we'll paste that in as such and hopefully it's not upset I guess we'll find out um but the idea is that we have this here and so we'll go up here we'll say type string and then the idea is that I want to uh Supply that so what I'll do so I'll go over a deploy script and then we'll want to have our overrides here so I'm going to go ahead and just open this tab and there should be like parameter overrides yeah this one here so go down to here and I'll grab this and I'll put parameter overrides and we'll look at what syntax it wants so this should be somewhere here key value key one so it's something like that and so we said server name server one I'm not sure if it'll let us do that but hopefully it will and so the idea is that this will be for rout3 servers as our example okay so we have server one that looks good and this will be in Us East one um and I'm going to go ahead and do that so we'll go ahead and say R3 bin nope uh R3 uh CD basic LS oh there is no bin it's just deploy okay so we'll do deploy and so that is going to go ahead and hopefully deploy that server so we'll go over to here I'll open a new tab and we'll go over to cloud formation and we'll give this a refresh and it's not working so there's probably a problem we go back here it said it created it but this created it in uh the wrong account because I have this set up go check this here adus SCS get Coler ID ident identity this is in a different adus account so I'm using an account that actually has a u domains here so what I'm going have to do is just uh set a um my credentials here you see me do it many times over if you don't there's a I am section for that but I'll give me a moment I'm just going to set I'm going to make new user and Set uh it was credentials here okay all right so I've swapped out my key here I I cleared my history here just so I don't have to worry about anyone seeing it but uh what we'll do is go ahead and try to deploy this again and so we're going to go ahead and deploy over here uh This Server here we go and I'll go over to our change sets here and we'll go ahead and execute this change set and so I'll wait for that server to be provisioned okay apparently have aoll back in Pro uh progress I imagine that it probably has to do with our sub there as I had a feeling there could be something wrong but you know what I'm thinking about it is that I'm in USC one and I'm providing a subnet VPC that might not exist so I'm going to do is go back over here I'm not even going to read this I already know that that's the problem and so I'm going to go over to VPC I'm going to go ahead and update that stuff that's something you just have to be aware of when you're switching accounts or regions that um you are deploying somewhere that I can actually deploy so I'm going to go here and select the default vbc which is this one here here so I'll grab this one and I'll go back over to our template here and I'll replace out this VPC here as such um I'll have to swap out the subnet as well so if go to my resource map I'll grab that one there here uh which would be right there okay good didn't really do what I want we'll try that again paste to bring it up on a line there like that and so now I just need the Ami as that could be different as well since I don't know where that last one is coming from and we will go ahead and launch a new instance I will grab the amid wherever it is here is one I'll grab that one and we'll go back over to here and we will paste it into here there we go and we will try this again and so hopefully this time where is it hello deploy it let's try this again maybe the old one wasn't just deleted just yet there I think it's creating it now yep okay great uh we'll go back over to here give this a refresh and we'll go to change sets and we'll execute the change set and we'll hopefully this will create this time okay all right it didn't work let's go take a look as to why as we are having a bit of a hard time getting this to work VPC does not exist VPC does not exist um okay well I'm not sure what to tell you because I just I just grabbed that VPC so you're full baloney but let's go take a look here and look at our values sometimes this happens so we're going to this one and I grabbed this VP C ID right and it doesn't look like it so I I guess it didn't really copy paste you got to be careful there with the copy paste I guess it doesn't always work properly so delete that again we'll wait for that to delete it is possibly deleted now I'm going to go ahead and Trigger this again we'll go back over to cloud foration and we'll go over to our change sets and we will execute our change set here we go and so I think this time it might create okay all right so um that easy2 instance is up and so the idea is that we want to make sure it's working by first going and taking a look at our instances here in North Virginia so I should have one here over here there we go and I'm going to go ahead and grab its IP address and we'll place it in here and we'll see if our server resolves we might have to tell it to not do htps but the server is there server one so that is good um and so now in to create a route that is going to or record that's going to go there so we can just do it on the www and in here we have an a record and the idea is that we want to bring in this IP address okay and we'll just place it into here okay like that and notice it has simple routing on so this is where we could change our options and we'll create that record and so now this record is here and the idea is that wwww uh this stuff is that this stuff doesn't always happen instantly because things have to go out to the internet and propagate but notice that it worked instantly so that is really good as there was probably nothing set on that before and so now www.cloud.com up again and then we'll go take a look and see what happens all right so that is now stopped I'm going to go ahead and start it back up but you should know that if you stop a server and restart it it changes the IP address so what's going to happen here if we refresh this it is going to hang because it cannot resolve because the IP address has changed and so if we were to take a look at our um new IP addresses is 54 210 whatever so we'd have to keep going in and swapping that out and that is not something that you want to do uh so what should happen is that we should go ahead and create an elastic IP address um for our server and that will resolve our issue so what I'm going to do is look up elastic IP uh CFN so we can go ahead and grab that example and I'll go down to our examples down below and I'll grab this and we'll go back and we'll paste this in here so let's just take a moment here and then the idea is that we need to reference our instance myc2 instance like this and so that will give us an elastic IP so I'm going to go ahead and hit deploy we'll make our way over to cloud formation we'll give this a refresh and we'll go to change sets because it's not going to know anything and I just want to see if it would replace the server now it's just going to create the lasic IP address so we'll go ahead and execute that and we'll wait for that to finish okay all right so that's updated so let's go take a look at our ec2 instance here so if I give it a refresh um and we'll scroll on down here got a lot of stops servers here I could get rid of but if we go into um uh here we're looking for the IP address this says 50711 so I want to see if that actually is the uh static IP address that we're using 5017 10187 so it is so let's go back over to here uh into our record and we will use the CI to update this at some point but right now I want to go and edit this record and I want to see if we can like obviously we can put the IP address the static IP address in here uh which is great but let's go take a look and see what we have for uh these options here and so you'll notice we have API Gateway app Runner app syn things like that so notice that there's nothing here for uh IP addresses because these are all pointing to uh uh DNS DNS records right so um for this in this particular case we can't use our Alias yet we're just going to have to put a static IP address in so what I'm going to do instead of updating from here let's go update it from the CLI as we should get some practice here using CLI with this um and so I'm going to go ahead and type in it CLI uh change record set um here and we'll go over to the docs I want version two not version one we'll click over to here and in here we should have change uh resource record set we'll go down to examples and so we have uh kind of an example here we'll go back over to here I'll past this in we'll just say uh change record record set all right and we'll need the hosted Zone ID that'll be the first thing and then we'll need the change batch file but I'm not going to make it Json file I'm just going to do it in line here we're going to use single quotations I don't think it uses back ticks for that and so we'll go back over to here and I need the hosted Zone uh details here so in here I want the hosted Zone ID so we'll go back over to here we'll paste this in and so now the next thing we need is our changes um so we'll go back and grab this link so if you are trying to follow along you don't have to look for it um but I'll go ahead and paste this link in here okay let's references references and over here we will grab um sure we'll just grab this and I'll go back and I'll paste it in and we'll just change what we want to change here so so this is optional so we take that out this is going to be an upsert because we are not um deleting or creating we're just going to replace and in here this is going to be the domain name so this is wwww do Cloud borgor for whatever you're doing you're obviously going to change it to what you need to change it to this is a c name record I believe no it's an a record because of what we're doing the default here is 300 so we'll leave that alone and then we have our resource records and then we'll put our value into here so what do we want to put in here this will be our new IP address also get rid of this as that's not supposed to be there I'll get rid of that um and so we need our new IP address which is this one here let's test to make sure it works before we do anything else I'm going to go ahead and hit enter here and notice that it's resolving so that is good and I'm going to go back over to here and we will change out not exactly what we want but pretty close we'll change up that value and so hopefully this will just work all right so what I'm going to do also going to specify the region so let's just say region Us East one I think that anytime you're using um revity 3 it's going to be us East one anyway so regardless of what you want to change it's always going to be us East one I believe so we go ahead and hit enter and it doesn't like something we'll go ahead and see what's wrong it doesn't say what's wrong here so I'm going to go ahead and just maybe simplify this because it can be a little bit tricky to figure out what's wrong with it like if there's something missing or something so that bracket's there that bracket's there that bracket's there it looks like we have all the brackets yeah this looks fine to me so I'm going to go ahead and copy this again hit enter and it's not it's not liking something so it says invalid expecting a comma on line 10 so line 10 from where it starts so 1 2 3 4 5 6 7 8 9 10 so there's a comma missing I can see it the problem is it's right here we're missing that comma so we'll go ahead and copy this again and we'll go ahead and enter and so now it's changing it says it's pending but it probably has changed since then we'll give a refresh here and we will check and I must have made a mistake because this looks different this one has four W's that is not what it wanted and we'll go back here to uh this and we'll take out our extra W and we'll try this again we'll hit enter there we go and we'll go back to here we'll refresh and so now we have our new one so let's go up to here and see if it resolves now it should and there we go so that is um using a simple record so hopefully that is really clear the next thing we should try to learn is to use the Alias but I think that for a simple record we have kind of satisfied this video here so I'm going to go over to cloud formation tear that down and I will see you in the next one let's go over to cloud formation here and yeah if we tear that down it should take everything with it uh that we're concerned about I don't really care about that route that route is now a dead route you can delete it if you want out of there um I suppose we should do that so I'll go ahead and just do that all right and I'll see you in the next one okay CIA hey this is Andrew Brown this video I want to take a look at uh utilizing ref3 and and using some of their interesting uh rules so I'm going to go over to here uh to here and let's see if we can um uh utilize some of this stuff so to get this going I'm going to go to our GitHub account as per usual I'm going to go over to ad's examples I'm going to open up um uh this here in git pod you can use whatever you like but today I am using git pod so I'm going to go ahead and open that up here just give it a moment great and so git pod is now open and I'm not sure if I have a yep I do we did R 503 probably with aliases and and we had a basic one um and this one here looks like it launched an ec2 instance with an Apache server and that could be useful if we were trying to test out some of these routing policies which is something that I'd like to do um so we're going to have to use some kind of domain and of course you'd have to purchase your domain ahead of time I have a bunch of domains that I can work with today I want to work with cloud one and the idea is that we want to explore some of these different record sets um or or routing policies as just say so we have SIMPLE routing weighted U we won't do all of them we'll do some of them here and uh you know maybe the first might be waited but in order to test this we're going to need a uh template so I'm going to make a new folder here so's going to be called weighted and um I need to have a template that's going to launch more than one server I might just change this to be uh rules or policies or routing what are these things called routing policies so I'm just going to rename this to routing policies because I'm not going to make a folder for every single one I just want to launch it up a template with more than one server okay and so this one here um I think it's for this account but let's go double check obviously you'd have to uh change this for your VPC I'm going to make my way over to vpcs here and on the left hand side we are going to go to your vpcs and I have more than one which is kind of annoying but I'm going to go over to here oh so it's this one right here so this is the one that um is I believe our default VPC yes and that's what we're going to be launching uh launching these servers in here today so that's set up so I'm going assume this Ami is correct if you need to go look up in your region Like Us East one Ami and go get the Amazon 2023 one and paste it in here we've shown that so many times and you'll have to grab a subnet so this is one of the subnets and of course you can grab this by go here to uh just select this one here it should show you in the resource map one of these subnets so this is probably us 1A I'm not really sure it doesn't really matter to me um the idea is that I need to have two servers so what I'm going to do is I'm going to uh copy these two here I'm going to paste y we'll allow for copy paste and we'll paste this into here and um I want to make a couple changes so the first thing I want to do is I want to change this resource so that we have uh one okay and then we'll have two here says server name say server name one server name two and we'll just say default serve one serve two and in fact we might want to have more than just uh two servers we might want to have three just to make our lives easier there's probably a way we could iterate over this and do this but uh I'm not not going to teach that here today but um we have uh two one oops and three if you want to do them in order you can do that it's up to you but I'm just going to get this to work here so say three and this one is two and this one is one okay so that is good and so now I have um potentially three servers the SSM R will be the same for all them so we can SSM if we need to um they'll have an instance profile this one in particular is using a static IP address um we probably actually will need this for all of them so I'm going to go ahead and just make three because they're not behind a load balcer so have to say ip1 ip2 ip3 and this will be one 2 and three and so I think this is all ready to go and and I'm going to go over to here tooy I'm just going to call this ry3 yeah it's servers but for routing I suppose just we'll just say R just so we know what that is for I don't need the overrides in here today I'll just take that out um and so that should be enough to get started so I'm going to CD into our ref 3 directory into our routing policy directory and what I want to do is I want to go ahead and deploy this so we say oh um uh deploy and we'll give this a go and so we'll say rating for changes excellent um so what we'll do is go over to cloud formation and we'll go oh this is pinpoint that's not what I wanted um oh where is it did it fail it probably failed oh you know what it is it's because this went to my um my other account I'm in a different account so here I have to just temporarily set my credentials so if I just type in ums STS get caller ID identity yeah right now I'm not in the 's examples account so just give me a moment to um put in these credentials here okay all right so I swapped it out and if this looks the same it's not it says a different account number so I'm going to go ahead and uh attempt to deploy this again and we'll go back over to cloud formation in us one which is where I'm deploying today I like to do c Central one I can but today I'm doing here and so what we'll do is we'll go ahead and click into this we'll go over to change sets and we will execute this change set and so now that is going down to deploying so we're going to wait for those um instances to come up hopefully there are no issues here um yeah we'll just wait to see if our three serers spin up okay all right so it looks like our servers are ready let's make our way over to ec2 and see if that's the case so we go here and we go to instances and we have three running it's a a shame we don't have names for them but we know that uh they should probably work if we go here and we just open their address in a new tab it should tell us we're expecting 1 two three give it a moment it says that it's not working but it should work it's probably because it's trying to do HPS there we go that's server three I'll just name this serve three so we know whoops serve three then here we have another other one so we'll open this one here this one is serve one so we go ahead and change this to serve one and then this one is obviously serve to so we're not going to worry about that right now and so now that we have those the idea is that we want to have some routing so and I just showed no I didn't necessarily show it but uh not great to have that page open there so I'll go over here to 53 and we'll go back to hosted zones and I'm going to use the domain cloud borgor of course you have to buy your own domain and set that stuff up so I believe we do that in the cloud partitioner but uh we have a video on it somewhere um and you should know it at this point already so let's go ahead and create a new record and this will be the www since I don't think we have one right now so say www.cloud.com it should be here thep unless oh maybe not elastic IP address but um because i' have to be behind a load bouncer I think but we can go ahead and grab a specific IP address so if I go here and well we just say Ser server one right and grab its public IP we could go here and paste it in here and create the record and so now this uh www. cloud.oracle.com an issue with your Chrome browser so sometimes the better way to check here is actually to go to um here and we just say ping this and you see it says 23 20 22 23 60 36 and so that should be uh 23 20 23 23 60 36 is the same thing 23 23 60 36 so this will be a more reliable way to check than than here because um sometimes your browser caches things that also things take time to propagate let's go ahead and change this to something more complex one thing that we should have paid attention to is the time to live uh when we have these records because that can uh greatly affec things so if I go here and add the record here it's 300 seconds so 300 seconds is what uh say 300 divided 60 that's about 5 minutes so just understand that we change this it might not take effect right away as it's telling you everything else to time out after a certain amount of time we could lower this you're not really supposed to it's not a great idea to do that we're going to do that here anyway and so now the next one we have that we could test here may be weighted so the idea is that we can tell it to sometimes go to one over the other um and I'm just trying to think here so we have our weighted policy and the ideas that we want to place in our add here the weight of each record that that has the same name and type determines the proportion of the queries okay so we have this one here and it says between 0 and 255 if you zero then R3 stops responding to the DNS record um so I'm just trying to think of like a waiting example here I already forgot I know it's in our slides we'll say weighted it was routing just because it's between 055 and not a single value so here it says for example if you want to send a tiny portion uh you must specify the weight so one of 26 gets the traffic so I'm just trying to think here we could do this as one as the first one so that will be that so we'll create that record uh is required so Us West load balancer we're not even in US West why do we need that give me a second okay oh sorry it's not that it wants a usw record balancer we're just saying like weight one sorry so like weight a all right there we go so we have weight a I was a little bit confused by that wording then we'll create another record this one will also be wwww because we're we're doing uh this weighted routing right we'll go here to weighted this will be um weighted two and this one will be two and so we'll go over to our server two and copy its public IP address and we will put this to 60 just make it nice and low and we'll enter that in and so now we have our second weight let's go ahead and make a third one we could have of course done this all in one go I'm not sure why I didn't do that but whatever and we'll grab our third one which is here we'll copy the uh this this address here we'll paste this in this will be 60 seconds as well we'll say waited we'll say waited three and three and so the idea is this one will have the is waiting now I think for this weight records let you specify what portion of the traffic but the question is like what requirements does it have and I believe as long as it's they all have www then it should work okay so now we have one two and three as our differentiator and this is obviously the waiting and so in theory this one will be weigh the most server three okay um because we did in that order we matched it that even though we didn't call it exactly the same thing friendly names over here record what do they call that can't really see it hide some of these other ones here like health check and uh still doesn't show us the name I want to see what the name is here record ID okay all right so let's go over to cloud.oracle.com it's constantly defaulting to this one so it makes me think it hasn't T taken effect yet let's go over here and we'll just um we could curl and so we have server one server two server three server 3 server two server two server two server 3 server 3 server two server two server 3 this makes sense because one has the lowest waiting so server 3 is going to show up the most and server 2 is going to show up the next uh if we wanted to be really smart about that we could have um made a script I don't want to make this into a big long lab we could make a script print out all the values and then determine how the frequency and then that eventually it would average out to that but notice when we're mostly getting three two and then one sometimes and so that is working exactly as expected so our weighted is working so that is good I'm going to go ahead and just delete these records so weighted is uh done and we'll delete those but obviously this one was wasn't giving us the best results as I don't trust the browser right away eventually it will eventually work let's take a look at what else we can do so we did simple we have weighted we have geolocation which is a bit more comp comp complicated we have latency um so there's that failover multivalue answer IP based Geo proximity so if we do multivalue answer that is one as well that we could do all right so let's do that one I believe we just Supply a bunch here so I think what we do for this one is we just go ahead and we grab our IP address here we'll just paste it in here we'll grab the second one here we'll paste it in here on a new line I believe and then we put in the third one and the idea is that multivalue will just choose from one based on whatever it decides what it is I know the lecture slides we explain what it is uh multivalue you configure ref to return multiple values such as an IP addresses of your web server in response to DS queries I mean that's not exactly how I know it I remember it chooses at random so I think that's what it does before we run this I want to make sure that we're not getting anything back so we go here notice it doesn't resolve anything so that's perfect again I'm going to keep this nice and low as we uh I don't want this to propagate forever as we were just doing this for tests we'll go ahead in a real scenario you wouldn't do that I'm going go ahead and do wwwww cloud.oracle.com if it's multivalue answer records let you configure rep3 to return multiple values huh I thought you could just put them all in one go maybe we can't let's go take a look here to return multi values such as IP address for your web servers and respons queries you can specify multiple values for almost any record uh routing lets you check the health of each uh resource okay why does this seem different than what I recall it being I thought you just like slam them all in there so just give me a second okay all right so I'm not 100% sure what the result is here I apparently don't remember maybe it was simple that you could do this we we'll take a look after this and see if we can do that with simple um but I could have swore you could just dump a bunch of ones in it would randomly choose one which is totally fine I'm going to go ahead and add a record here and add a record here doing this for years just can't remember anything uh so grab this one and paste that in and grab this one grab this one I'll be www www www they want a name here I'm going to call this uh I think I wanted a name record name here it is so let's just say A1 A2 I don't know which is which for these IP addresses I'm just giving it oh do we just give it one name record ID oh it's just one name do we even need one then oh you know what it is I didn't set this to waited or or sorry to multivalue that's why so we go here and set this also to multivalue answer there we go I want to make sure these are all 60 it's changing everything on us so we just have to be very careful and we'll try this again so let's just say A1 the record two will be A2 we could have done this programmatically but honestly when you're working with records it's just easier to do this unless you're automating it but usually don't automate this stuff so now we have three records for multivalue so I'm really curious what's going to happen here I wonder if I have to update my slides and I got it wrong or I'm just reading it wrong so we do this I get serve two serve one no no it does exactly what I thought it does okay some of them are not resolving which is not great I'm not sure why that's happening just give me a second you know what it sounds like it sounds like we're supposed to have um maybe like a main domain if it's not responsive and health check it goes to these other ones cuz it's saying here use a multivalue answer uh create one or more record of the same name when a client makes a DNS request with multivalue routing R3 response the DNS queries with up to eight healthy records selected at random from particular domain name these records can be attached to ref3 health checks okay which help prevent client from receiving a DNS response that is not reachable which we actually got multiple value answer to distributes becomes unavailable after stuff so I'm not 100% certain again I'm not I don't claim to be really good at uh um very good at networking but I'm not sure why we're getting this because it seems like we should not be getting that unless there is like a record that still exists and it's confused also are we unless I entered in one of the IP addresses wrong that I mean that's a total possibility as well so could just be my mistake there we're getting server 2 3 and one but yeah sometimes that happens so clearly there's something I don't understand about multi value answer so I guess I'm not the best uh to to show that information there but honestly I rarely ever use that type like in practice and that's why um I'm honestly confused about it let's go take a look at the simple routing all right so looking at my slides I mean it looks like I knew what it was when I when I made this content again because I really use it so we can figure R through to return values such as IP addresses some simple routing however with an added health check for your uh record set resources so it seems like all three should be working nothing should be ending up unresolved so I'm not exactly sure why that is happening but uh it is so I don't know I think I'm using it correctly but now I have some uncertainty kind of wish that we had programmatically done this um updating the routes but I'm not going to do that now we'll just stretch things out I'm sure you know how to do programming by now with me anyway so we did that one so I was thinking simple routing allows you to do it and yes you can have multiple records here so the question is like if we go down to this one they're all collected here and then this one up here they're all uh the same again it's because I would probably use the CLI I'm I mean you don't have do but I think normally I used to see a line that's why I'm a bit confused here today so we'll go ahead and uh do this and delete these records let's say if we can do a single value a simple one so let say www dot and I want to just see if I can drop all the values here okay so I'm going to go ahead and grab server one it in here I'm going to go to server two I wonder if the other the reason why that happened was that when I created the multi value answer I wonder if I had um more than one IP address record because remember in the first line I had like an enter and maybe I entered in an empty line so that might be the that could explain the the reason for it I'm not going to go back and find out if you want to you can do that but generally you can put multiple values in a thing so I'm going to put this to 60 and so this is our new one before we do that let's just make sure that things are unresponsive C okay good so that that is good we'll go ahead and create this record right so now we have those three and so the idea here is that uh we should now go back to here and curl it there we go one 3 two 2 1 3 3 2 could not resolve you know another reason why it might not be resolving is that it's just taking time to propagate and because like every time you send a request here the way it goes and gets the information is going to be different and so it's possible it's just taking a path where that server thinks that there's still no server there and so I bet our multivalue worked perfectly fine um and we're just kind of mistaken uh from before but that's where you kind of have uncertainty when you were working with uh uh records and stuff and that's why I'm trying to keep this really simple because we could do a more complex scenario but i' have I'd lack the confidence in if something went wrong just because of how networking works but that's our simple routing one which I don't think we showed that earlier like a multivalue one there so that's interesting um if we want to do something like geolocation or latency based we're going to have to um move one of our servers to somewhere else so what I'm going to do is we're going to have to go here back to our script and go into our template here and change it so just understand I'll just add a fourth one so that uh if you run this you're not going to run into problems we're going to have server for like server other this will be like other so it's like farther away and I'll copy this one here this will be server other um this will be server name other and uh I'll need a fourth IP address so say other other and the idea is that we need to deploy this somewhere else and uh to do that we will need probably a separate Security Group because those are usually yeah the region specific um so I'll go ahead and copy this one here and this one will be other and we'll have VPC other and I'll need all of these here again but this will be other all right other other other so these are all somewhere else right and uh if we go over to here we'll type in VPC and I want to go to somewhere else that's not Us East one so let's go somewhere far whatever you can do but I'm going to go to Tokyo because that seems like a really fun place to go to so let's go ahead and try Tokyo and there should be significant latency because it's on the other side of the world literally 12 hours away from me um the opposite time so I'm going to go here and see if I have any vpcs I do have one so I'm going to go ahead and grab this one here and I'm going to go ahead and paste it in here paste it in and this will be uh I just want to indicate like this is Tokyo to key Tokyo I'm not sure why I'm having such a hard time spelling today why do I want to put I in there it's not that it's to Tokyo toy y Toco yo okay too yo that's right right oh that's my problem yeah too there we go and you know what if my keyboard was in Japanese I know how to type it in in um uh hugana okay or not hereon in uh in kanji so just for those if anyone's Japanese watching this I'm sorry I just forgot that it's Kio okay uh and those blend together um so anyway we have our VPC which is the first thing uh the next thing we need is a subnet so I'm going to go back over to here we'll go into our resource map I'm going to grab the first one and we'll copy this um so we'll paste that in we have our image ID so I'm going to go over to well actually I'm already here so we'll go over to ec2 in the Tokyo region we'll give it a moment and we will launch ourselves an instance we's say launch an instance and we will go ahead and grab this Ami ID uh which will be here so I think those are replaced which is good the other thing that we need to replace here is the security group so here it's going to be SG other this one will be subnet other other this this type is fine other assuming that there's no issue with that and we'll see what happens when we uh go ahead and update this so go ahead and do a deploy hopefully that works um unresolve dependencies ec2 instance profile other oh well you know this would be the same it would be ec2 instance profile so this one's fine subnet other should be working subnet ID other if I don't spell it right it's not going to work so we'll try that again and we will go ahead and deploy this also if I sound a little bit stuffed up that's cuz I'm sick it's not AI Andrew some like I sometimes in some of my videos not for ad's videos I'll have a um a synthetic voice when I'm uh my voice isn't working but I'm just telling you I actually am literally sick and I'm recording here so I apologize for this specific video other videos all sound nice and crisp and we'll go back over to cloud formation um our stack is is this going to work because our sack is in one region I don't think we can do that in another region but let's do it for fun anyway I think it's going to fail but I think we might actually have to have a separate cloudformation template for this because um the resource is somewhere else right so uh let's try it anyway I don't think it's going to work okay but we'll give it a moment see what happens okay and just as I thought yeah it doesn't work because we are in the wrong region so what I'll do is I'll make another template here and we'll call this uh toio yaml and I'll copy the contents of this one here and we'll paste it in here we'll have to go back to this template and rip out all the stuff that we just put in here because it's useless so we'll take out uh this here and this here and we'll take out my instance ec2 other and we'll just look for other here won't take that long take that one out next that one out so now this templates back to what it was before which is totally fine if we go to this one here now we just need to take out all the other stuff so we'll do this and we don't have to call it other now we can just call it this it kind of makes things easier too if we do that um and we will keep our the First Security Group we don't need the other Security Group because we're going to just rename that to bpc anyway and we oh we'll just take one actually here and go up all the way to other here and we'll just change this back to normal like that and instead of being server name one it be server name and I don't need the other I don't need 3 two one I do need one of them so do this and say serve too um we do want this one so we keep this one here we want this one here cuz it's the correct one this one here and then we want to get rid of uh these ones here okay so in theory this should work um so I'll try this again deploy um I don't think the is the region hardcoded here it is hardcoded here so we need another deploy script so I'll just copy it might be easier if we just do this and paste it and I'll rename this deploy toe Kio and I need the region I don't know what their Region's called I rarely deploy in Tokyo so I wouldn't know it is AP Northeast one AP Northeast one Asia Pacific North East one there we go has the same name as the other one but it's in another region so it shouldn't matter so deploy Tokyo it's kind of fun and we'll see what happens so we'll go to back over to cloud formation we'll switch over to Northeast one here and do we have something to deploy we do excellent we'll go here to change sets and we'll execute that change set and hopefully this will work and then we can do some finer routing all right so we got a failure on our hands so what's going on um VPC does not exist okay well fair enough I could have the wrong one in here though I'm pretty sure oh you know what it is we have to choose the template name too that's our problem it has to be Tokyo over here otherwise it's not going to work so we'll go back here delete the stack came pretty close to it we'll go ahead and deploy this again now we have the new one here and I think this one definitely is going to work okay all right there we go so it is now uh deployed and so the idea is well we'll make sure first we'll go to ec2 but if it does work we'll go back to our other region and then we'll see if we can um have some routing differences okay so if we go to our instances here uh we have one here this is our server Tokyo so what I'm going to do is grab its IP address I don't want a bunch I just want a couple but I'm go ahead and grab this one and we're going to go over to um ref3 in our original Zone and what I'm going to do is I'm going to just do a simple routing because first I just want to make sure I can route between the two of them and and it's working and then we'll do uh um oops then we'll do uh Geographic or latency based routing and then see what happens okay so we go to R 53 and we go to our hosted Zone and in here we go to I'm going to create a new record and we have our Japanese one first or Tokyo One first www um and then I need one from I don't need a period on there that's going to mess that up I'm going to need another one from the current ec2 uh location so I'm going to open this up in a new tab so I don't have to keep getting this other one here we'll go to ec2 and I just need any server that is running here I have uh oh gu switch this over here one second we'll grab serve one serve one seems fine to me we'll go ahead and grab that I'm going to go back over to here I'm going to paste this in and we have SIMPLE rounding so between this it'll think it'll switch between the Tokyo One and server one and I'm going to set this to 60 I want this to be nice and low we'll create that so now this is set up um not sure if it'll work instantly but we'll give it a go here so we got Tokyo uh serve one there we go and so noce like again depending on the path it takes over the Internet that's why it's doing that and it's very easy to have a lack of confidence because of that so just understand that networking's hard but anyway so it's clearly working but uh we want something more interesting than this so I'm going to go back into here we will delete this record and what I'm going to do is create a new record and this one will be geolocation um choose the location that the DNS queries originate from so if I go to here um and basically it's just this one's saying like if we're from here then go to there right so if I'm in Canada and I am in Canada and that's what my IP address should show right now we call this Canada then we want this to go to let's say serve two okay and that's pretty straightforward if we were somewhere else would say w w www again here um what's interesting is I don't know where this place is so I have no idea the location of This Server I didn't even think about that how would we know where the server is I have no idea um so this is where I might have to use my local computer to actually th really test this um but anyway so we have this one here so if I'm in Canada this will be the Canada record so we'll add that one here that's geolocation we'll might as well do both the records while we're here and then we have Tokyo so we go ahead and grab this IP address here I'll paste this in and we'll do GE geolocation not Geo proximity but geolocation and if I'm in Japan you should go to the Japanese server so say Japan here and again set these nice and low just for purpose of these videos you would never change it to 60 usually this will be wwww do so we have these two here and the idea is that they should respectively route to uh their areas so for this to work I'm actually going have to use my local machine it's not going to really work properly if I uh do it any other way so I'm going to open up um Visual Studio code you use whatever you want to use um but you need to have a way out to the internet in Linux would be the easiest way to do it okay all right so I just opened up visual studio code and what I'm going to do here is just do a curl to www. Cloud Borg .org and uh getting a lot more information than I normally do I'm not sure why I guess we're special but we get serve two back which is what I want oh it thinks I'm in Powershell okay well maybe I could have just done this with uh windows on Powershell locally but uh we go over to command prompt because I don't need I mean it's actually kind of nice having all the information so actually I might stay in here but uh you could do this in bash as well I guess I'll just do it in power shell here today but now I need some way to change to uh to look like I'm in Japan so there are different types of um vpns you can use I have one called private internet access I'm just turning that on here so I just went and I'm opening that up here give that a moment to there so I'll click on that uh if you don't know uh vpns are service like this one cost me like a dollar a month or something I don't know it's like really low but like there's nordvpn a bunch I use private internet access just cuz I've using it forever but I can go here and I can say I'm now somewhere else and so I want to be in Japan so I'm going to drop this down hopefully it has Japan it's going to be really annoying where are you Japan J for Japan right h j k do they not have Japan oh they do okay uh and I'm GNA go to Tokyo so let's go ahead and connect and so now my connection is is going to first go there and then that's where it thinks I am so if we go back to here and try this again hopefully it will go to the Tokyo server we'll try that again and I'm not exactly doing what I want huh okay let's go back here again this kind of stuff is very tricky so I'm not really surprised it's not working but this address here would be the Tokyo one so if we go here and take a look yeah server Tokyo okay um but supposively I am in connected in Tokyo right is said I was yeah yeah so oh there it is server Tokyo okay maybe it just took a moment for it to get working so clearly is working which is really cool um so that gives you geolocation there are other ones that we didn't do like latency um you know we could test that one as well um but I think we've done enough here you get the idea it's not that hard to set up these records and I think that is sufficient so we'll call this done we'll clean up so go ahead and delete these records out of here and then I need to tear down my claw formation so we have one in Japan region Tokyo Japan Northeast region there so I'll first go over to that one well it doesn't matter I'll tear this one down cuz I'm over over here delete shirt and then we'll go to Tokyo and we will go ahead and tear this one down as well my bar out here so we don't have to look at that anymore and there we go and we'll call that sufficient uh practice of R3 there's other stuff of course but I think that is good enough here so to say ru3 uh router R policies and there you go ciao hey this is Andrew Brown and we are back for more 53 I'm just picking up where I last left off I just tore down a cloud formation uh server but in our uh it examples repo I'm going to make a new folder and this one will be called Alias as what I want to do in this one is to create an alias and in order to do that we're going to need a resource um that actually has a DNS record probably the easiest thing we could do is is uh a load balancer so I'm going to go take a look at our ALB section and take a look at this template and um this one looks pretty good so I think this is what we're going to want to copy here today so I'm going to go ahead and copy these two I'll go down below into our Alias here I'll paste this in I'm going to change a couple things just be for uh R3 Alias uh this will be R3 Alias uh I'm going to change this to be us East one all right um the other thing is uh some of these things will have to change so we just did this in the other one and so for this account I'm going to go ahead and just copy these over the bpc ID the image ID and the subnet ID here and it looks like we need two we need a and b so I'll go ahead and grab the B1 so we'll go back here I'm going to go and type in VPC and I just need a second subnet as we are launching a load balancer it's going to at least want to so go to the resource map here I'm just going to grab the second value right here and we'll go back to our code and we'll replace B and so now this should set up a load balcer this should set up a load balcer with all our interesting stuff here okay so I'm GNA go in CD into that directory Alias I'm going to deploy that and I'm G to make my way over to cloud formation and so this should deploy an ALB with a workload a comp like a web server and so we'll wait for this to deploy or fix things if things go wrong all right so I think our load balancer is ready uh let's make our way over to to uh the ec2 section here and we'll check our load balancer to see if we can find that uh DNS record so if we go to load balancers here and we select this one here we should have a DNS name down below okay so we'll go ahead and paste this in we'll make sure that this resolves to something it might want to oh there we go so it works so the question is okay how will we set up aliases so if we go over to here and we create a record I'm going to just say wwww Club .org and you select Alias then now you can basically just select that's going to an application load balancer and then select the region uh so Us East one and then you choose your load balcer so would be this one now I want to point out that if you are working across an account you can't select it this way and so what you'd have to do is you just have to go into here copy your route and you would for more or less place it in here the only key difference that you're seeing here is is that this one has dual stack on the front of it so if you noce I'm just copying pasting back and forth this one has dual stack so you might want to adjust that but I'm just saying you can put whatever you want in here okay you don't you're not forced to whatever is automatically selecting so now the next question is we go to www.cloud.com Bare Bones routing that you need to know one thing I'd like to show you is maybe just the traffic flow policy while I'm here just because this is a visual tool that uh still really old the UI it's the only way to see Geo proximity here so um this cost a lot of money I don't really want to run it so maybe I won't create it but my point is that this is just a visual tool that you can click and and do stuff with um the exam's not going to ask you about it you don't really need to learn it so it's not that big of a deal but um yeah I think that satisfies uh us learning about R3 so I'm just going to tear that down I'm just save my changes for later here so just say save3 code 53 code and I will see you in the next one okay ciao oh well sorry uh yeah I deleted it okay great yeah okay bye let us take a look here at ec2 which stands for your last compute cloud and it is a highly configurable virtual server that allows you to resize it to meet your compute capacity and it takes minutes to launch new instances and I like to call it the backbone of a Deus because basically every kind of computer pretty much runs on ec2 whether you can uh directly see the ec2 instances or not the uh everything is going to be running basically on uh some kind of machine and usually it is going to be ec2 um so you know the procedure in ter terms of what you're going to do when you go ahead and launch an ec2 instance you're going to first choose your OS via an Amazon machine image and this could be uh a variety of different operating system distributions which adus has access to quite a few if not the most out of all uh cloud service providers then you'll choose your instance type which is going to determine uh the actual resources that you'll have underneath so that would determine um you know the type of Hardware that you're going to be using and also U the of vcpus memory uh Network throughput and things like that then you'll want to add your storage such as EBS or EFS um there's a variety of different types of storage mediums that you can attach to your ec2 instances and then there's a lot of things that we can configure so security groups for our firewalls key pairs to SSH into user data to run a script on Startup um IM roll so that we can or instance profiles that so that we can um provide the cct2 instance access to to other inas resources placement groups and more um but that really scratches the service as about ec2 but we will dive deeply into it here starting now so Cloud init is the industry standard multidistribution method for crossplatform cloud instance initialization um and that's what cloud and it stands for uh Cloud initialization or Cloud instance initialization and it is ported across all major public Cloud providers provisioning systems for private Cloud infrastructure and bare metal instances so what do we mean when we say cloud instance initialization this is the process of preparing an instance with configuration data for the operating system and runtime and when you have a cloud instance that is being initialized from a disk or an image or an instance data uh there are three components that matter the metadata the user data and the vendor data I want to point out the metadata here has nothing to do with ec2 metata or at least I don't think it does um so just don't get confused about that but user data I hadl in red because that is a very important one and is the reason why I'm priming you with this cloudinit information but anyway here's a diagram with the idea of we have those images and we have the instance and then we have user data metadata um shows user data twice I didn't make the diagram but uh vendor data would have made sense in here I just noticed how that looks a little bit funny to me but anyway the user data is a script that you want want to run when an instance first booots up like installing an a Apachi web server and the reason we're talking about this is because Amazon or ads ec2 supports cloudinit across their Linux distributions so we can absolutely use user data and I just want you to know it's coming from here because if you don't understand how the user data works if you look more into cloud in it and their standards and the stuff around it you'll understand everything about how your uh instan is being initialized but you know again we're not covering all of that we're just talking about where where user data comes from and it comes from cloud ornit hey this is Andie Brown let's take a look at cloudinit so uh there's a website Cloud hit. where you can read a bit more about it um I believe that is managed by canonical who are the same uh folks that produce Ubuntu something I did not know was that originally this project was developed specifically for um Ubuntu distributions on ec2 but you know now it's accessible by all Linux Unix I mean all or most Linux or Unix distributions on every major Cloud platform which is kind of interesting um you know we can go here to the GitHub page and get more information about it and then the docs are very interesting the only thing that I really care about showing you here is showing you where the um the logs are stored on AWS because I feel that sometimes you need to go in there uh and take a look at that kind of stuff but you know if you want to read more about cloud in it it is very interesting um but uh you know once you are using major Cloud providers you kind of lose that knowledge unless you are intimately configuring this stuff but what we'll do is go over to uc2 and I'm going to go over to our UHS repo as per usual and um I'm going to go ahead and launch a new instance and in here I know this is a future video but what we'll do is go into this user data here and grab this yaml file which is using Cloud config which is a special special type of configuration and we'll use that to launch our web server so I just say uh my web server cloud unit we don't care if anything works we just want to have some kind of example to launch up and get access to so we'll choose Amazon L 2023 T2 micro is totally fine we'll say proceed without key pair I don't care about um Security Group rules so much so I'll let it go to the default here in fact I'm just going to choose the default Security Group as I feel that will make my life a lot easier we'll go to advaned Det details we'll choose our ec2 SSM rle uh we set this up in a different video um in the VPC section because we do launch ec2 instances there this is very straightforward it's just a um a role that has the SSM core manage policy attached so that's something you should be able to figure out we'll go ahead and paste this in here and so this should install in aachi server but I'm not really interested in that install I'm more interested in getting access to this V A sessions manager and taking a look at where that default log file is so if we type in um system logs for ec2 I think it tells us the path of it and I'm not sure if it'll show up here but um I'm expecting to find that cloud nit log so there's a few different files but we're going to take a look and see if there's anything interesting inside of that file okay and actually this one says CFN in it so that's actually cloud formation but I'm interested in Cloud hyphen a knit it's interesting that it says Amazon one and and and two um so maybe it's stored somewhere else but um while that is spinning up and getting into those two status checks I'm actually going to read up on that okay all right so you know again I was reading a bit more about how Amazon 2023 has a customized version of cloud in it um and so that mean showing where the configuration file is it's not indicating where the log is which I would like to really know um so my question is is that if I log into this is it going to be where we need to see it or am I going to have to launch an Amazon links 2 to see what we'd normally expect to see and this is the just the nature of cloud where they're changing things or they're trying to modify um the base level ones I really wish they us uh stuck with the open source one but uh you know I can understand why things get changed so let's go ahead and take a look and see if um it actually is located in our VAR log directory so we'll go ahead and take a look and see what's in there VAR log and we'll do LS LS hyphen LA and we still have cloud in it so I was thinking that it wasn't going to be there but apparently it is so maybe the docs are just out of date as it was indicating that it was only for these two but clearly is in here um well I guess maybe this thing's all about whether it will actually send it over to cloudwatch log agents so this one's actually saying what will be streamed over the cloudwatch logs it doesn't necessarily mean that these files would not be on the instant so maybe that was just my misunderstanding of that page there but let's go ahead and take a look and just um we'll cat out the file we'll go Cloud andit log and uh there's a lot of stuff going on in here so let's just scroll up and see if we can make sense of any of it so what I'm mostly interested in is seeing where it might do that Apache install so we're here at the top and here it says restoring SC Linux that's secured Linux going through here and just carefully looking at there's anything interesting here notice it's going out to uh the metadata service it's grabbing a token okay and just again I'm just slowly scrolling through here and taking a look at what's going on so here we say writing to Cloud IDs I'm not sure what that is and then notice it's talking about reading from this Cloud config directory which was being talked about over here so it says uses the cloud unit actions in this directory and this one you can create your own initial actions in this file so it might be interesting to go open those files and take a look at what they do and again just scrolling through to see if there's anything interesting worth pointing out and now I'm seeing stuff that looks kind of like um Apachi so it looks like here it's it's running our installation script and the reason I can tell is that we can see here it's writing index HTML I'm just carefully looking to see where the maybe the install occurs and we're just going down a little bit more here notice we're seeing update to the authorization keys for SSH and I'm seeing yum so I'm going to assume around here I might be doing the Apache install yeah uh you know the idea is that you can kind of sift through it and get an idea of what's going on um let's go take a look at these other uh other things here like this one Cloud config and D I think they were saying that it was in this one so I'm going to go ahead and just cat that out that's a directory so let's just CD into it then and we have a few files here we have ec2 config so I wonder if that's what that's probably what it was running when it starts up that's what clader it is uh executing I think and I just wanted to CD out that content I probably have to put the zero in front of it it's my fault that kind of looks like this intax for are there one there for a configuration but we'll take a look at that one that's for the ec2 instance so configuration disabled the data source is the metadata URL um we'll just keep looking at let's take a look at logging so that's something yeah I'm not saying I know what all this stuff is I'm just thinking that it makes uh makes good sense to look at this this is how you learn you just kind of open stuff up and you poke around uh yeah more metadata stuff that's something for secure Linux I'm going to go back one and I'm just curious what would happen if we took a look at the other one I think the other one is actually a file and the other one is a a folder so this one I think is a file place that in maybe it's like the main startup there we go more interesting stuff in here um so we start up the top power State oh we C it here again sorry so here we see users's default disable the root as true so we're not actually having true access to the root preserve the host name so if you want to preserve it you'd have to change that we actually talk about that in the host name section of ec2 um here we're sping a data source out to meta URLs I'm not sure why it says bl.com that's kind of interesting so maybe that isn't going to metadata but is that actually a domain it's not going anywhere so maybe that's like a default setting the modules that run inside the init so here we have a bunch of modules more module stuff yeah so there's something but anyway there will be time and time in place where you might have to open up this file and just know about it so there's that while we're here we might as well just take a look at some of the other um logs there's any other interesting ones we want to take a look at while we're here so just uh CD into bar log lsph La I mean there's this one here that's also related to cloudit oh cool so it shows you what's installed so here so Network device information we can see a key pair is being generated it's showing aachi being installed cool all right so there you go pretty straightforward um so I'm going to go ahead and just terminate this instance and just remember that get to tear down your security groups uh it's a bit hard to um well we were using the default one but I use this another one but if you do end up creating security groups you can always just clean those up it'll save you some trouble there but uh we'll see you in the next one okay ciao so you can provide a script to your ec2 user data to have cloudin it automatically run on first boot so the idea is that you have an ec2 instance you want to have some stuff installed the first time it launches up um and so this is where you can inject your script so there is a box that you can provide or you can provide it programmatically we're going to focus on the programmatic way because I think that's more value to you um but that script is going to either look like a bash script and this example installs an Apachi web server which is my goto for showcasing user data and we use it a lot in the course or you can use a cloud config yaml file so this is a yaml file for a very specialized syntax that's defined by cloud in it so if you want to know more about that you can look it up you'll notice that it's a little bit more uh modular and more flexible so if you have something that's a bit larger uh you might find that very useful of course you could use ansible in place of user data but this user data script is great for um smaller amounts of things um but uh yeah anyway these scripts that you utilize they must be B 64 when directly using the API but we're always going to be using the CLI in the console so we don't have to wor worry about uh converting these scripts into base 64 encoding um I I think somewhere in this course we talking about Bas 6 before but the idea is that it is um uh transforming it into something that is not necessarily human readable but is more compressed when passing it along to these programs um so I just want to show you that in the a CLI we could pass a file and so here I'm specifying A bash script uh or you can use it in line so apparently there is this multiline option I don't even think I've tested that but um chat gbt told me that you could do multiline which I was surprised I'm used to doing it as a single line but anyway those are our two options and we'll definitely uh test uh test these things out for Real uh the one other thing I want to point out is that if you ever wonder what um your user data script is you can use ec2 metadata using WG or curl and hit the endpoint to find out exactly what you're using but usually you don't have to do that but that is a method that you could use okay for hey this is Angie Brown and this fall along we're going to take a look at launching uh three different instances to utilize user data in three uh similar but different ways in my itous examples repositor I'm going to hit period to open it up in here at our ec2 user data because I have one for uh PS1 so Powershell a Bas script and then Cloud config which is a yaml file so I think knowing that there is a lot of options is really good when I made the lecture content I was not aware that Powershell could be utilized um and so um I decided to put this in here I really tried to make this work it does the exact same thing as the other ones but for whatever reason I cannot access the website um so I mean it might work but we'll we'll see okay um I did run this lab just a moment ago and it completely failed because I did not realize that I did not have an internet gateway attached to my VPC so now I'm hoping that this time around I'm going to have a much better luck but before we get started let's just make sure uh since I'm paranoid here um look at your default VPC just make sure that you have an outbound connection if you checkbox this and go to resource map you should see it mapped already here if we go to our route table and go to our routes we should have a route out to the internet because we're going to need that let's go back over to ec2 and I'm going to launch our first instance so here we'll go ahead and uh launch our Windows Server let's just say windows Apache to uh Powershell because that is what we're doing we'll choose Windows I'm going to go with a t uh a T2 medium even though they say it can launch it with a T2 micro I cannot imagine it working if you are afraid of any kind of spend do not launch this and just skip this part of it and do the other to um but I'm going to go ahead and launch that cuz I'm not worried about spend um make sure you're in your default vbc for this one I'm going to create a new Security Group I'm going to call this one Apachi SG this will be for RDP and I'll also add in um uh SSH for the Linux servers and I also want to open this up to htttp I we don't need htps but I'm going to add it anyway just cuz I'm a bit paranoid here that that might break this lab if I don't add it so just looking for https there we go and then we'll go down below to advance details I'm going to select the ec2 SSM roll we set this up so many times throughout this course so um if you do the VPC section will definitely do that um but it's simply we're adding a a policy to a role and that one is for the SSM managed core uh uh policer role so down below here we're getting ready for our Windows servers let's go over here and grab this script um and so it turns out that if you want to put Powershell you just have to have uh these tags in front of it and let's just talk about some of the stuff here so the first thing that's very important is check what version of Powershell you're using turns out that uh the that Windows base server the latest one that it was has is running Powershell 5.1 desktop Edition the latest version of power shell is like poell 7 and power shell changes quite a bit per version and so uh per version so you have to um do some tweaks to make it work uh we're downloading a binary from the Apache lounge and so this is version 2.4 then we are specifying where the zip path is going to be when we download it normally you use invoke web requests to download files uh the issue was that we're getting a 308 on the redirect because that's the error throws if it has been redirected but even though I added this additional flag to follow the redirection and refus to do it so boo my cofounder who's really good at windows and Powershell pulled up this older way of doing it where you use the web client or alternate way of doing it we're use the web client to download it then we extract it into our C directory which is going to actually place it into a folder called Apachi 24 which will contain everything that uh everything like the configuration the logs and everything so if you're wondering where to look for stuff it's going to be in that folder so we extract it we delete the original zip um this is not something that matters so I'm just going to comment this out but this is if we wanted to do additional monitoring down below so I'll just comment that out um then we specify the path for for the binary and then we are starting the process and providing arguments to start it uh here I had it if I wanted to change the port from 80 to 8080 because sometimes Windows servers have things running on Port 80 and so uh you'll have a very hard time stopping those other ones again I do not expect this one to work but I just wanted to install and make sure it's running to um checkbox that we were able to use Powershell then we start the service okay so we'll go ahead and again copy this and we'll paste it in again because I did take out one thing there and I'm going to go ahead and launch this instance oh here I'm just going to choose from anywhere if you're really security concerned you can all uh just choose your own IP address but I'm going to go ahead and do that um and so we'll launch that instance I'm going to go back over to here and you can see I have ones from prior that failed but this one is starting up we'll go ahead and launch our Linux one so we'll do Linux Apache um bash and we'll let it choose choose Amazon 20 Linux 2023 T2 micro we do not need a key pair this time I'm going to select the existing Security Group and and checkbox Apachi SG we'll go down below here we will choose our IM roll to be uh this one or instance profile to be the SSM roll one and then down below we'll go over to our bash script and we'll copy this and you should B scripting pretty well in my courses because I do a lot of it uh so we're going to update we could do an upgrade and an update but we update our packages so we have the latest then we install Apachi which is called httpd there is like sometimes Apachi too for different um Linux OS distributions but for uh Amazon 2023 it uses httpd as the package name then we're going to use system CTL to start it enable it allows it so that if we reboot the system it will automatically start up the um service for apachi we'll create our custom HTML file so that we have something a little bit different than other than the default one we don't have to that would serve up a default page but it's nice to change that out and then we restart it which actually is not necessary but it's in the script anyway we'll go ahead and copy this and I'm going to go ahead and I guess we already did that and we'll paste it in and we'll go ahead and just double check make sure there is no major mistakes when copying pasting we'll go ahead and launch that instance I'm going to go over to my instances I'm going to launch another one this one's going to be Linux Linux Apache um Cloud config because we're going to use the cloud config which is for cloud and knit using yamel to configure the startup here so I'm going to go and proceed without a key pair we'll select an existing one we go all the way down the bottom here go to Advanced details and we'll select SSM ec2 roll and then this time we're going to use this script so let's talk about this script here for a moment M um it is a little bit more convenient than using the bash script but that's only if you understand what's going on here first we're updating our package manager we're upgrading packages and then we are going to install the hcpd package we'll write this file here make sure that it is accessible with the proper permissions and then here we are starting enabling restarting again the restart is not necessary but it's part of that script so we'll go ahead and launch this instance and we'll wait for these all to pass their status checks give this a refresh you'll have to hit refresh to see what's going on here but this will take a little bit of time okay all right so our status checks are done let's take a look here at uh the Linux Apachi bash script that we have and we will go ahead and open that up uh we'll take a look also at the other Apache Linux one here and one thing that we'll have to note is just make sure you swap out the htps for HTTP because it's running on Port 80 and so those both work if you're running into issues with these you can open this up in um these things in sessions manager here and you can run these line by line and figure out what the issue is at least for the Bas script not no not so much for cloud config if you wanted to debug this I suppose that you'd have to um look at the cloudinit directory and see how that installation information works there are logs for that but I'm not covering that here today but anyway these two work so we're in great shape the real question is did the Poh one work or sorry yeah the power shell one so I'm going to open this one up if it works I'll be really happy but I I really don't expect it to work as I found this one to be the most difficult to figure out and does it resolve I don't think it's going to resolve but what we can do is just go connect to it because I just want to show you like you can connect and if you wanted to you could step through it and debug it so you know for this one in particular and you have to know Powershell a little bit to know what's going on but one thing we want want to know is like is it running so we could type in get service and that will list all of our services this resolve no it did not nope um and so I could go here and say get service Apachi 2.4 it says that it's running so the other thing that we could do is we could go into the C directory here and then into the pchi 2 point uh two uh 2.4 it's 24 but it's 2.4 that's what it means and you can see we have HT HT docs other things like that so we could go into the logs and then we could cat out if there's any errors and none of this indicates that there are any errors these all say um warnings notices so these aren't errors then we have access logs okay and so access logs would be what we'd expect to see if if this was being hit so you know there could be things like changing the port over to port 8080 which um I could try that right now but I don't think it's going to fix it right so if I do this and hit enter um and then I go back a directory and then we CD into the conf directory because just look what it's doing saying open this file replace 80 with 880 right and so I'm just going to cat out that file and I have to scroll all the way back up to see where that changes and if I go here it now says 8080 right so I could try restarting this um I'd have to open up uh Port 880 on the um the security group but let's just see what happens I'm can just C copy this here we'll paste it in and I'm just going to say stop service and we'll go back here and then I will start service and then we'll go over to our security group here and then I'm going to add an inbound rule on TCP for 8080 from everywhere again not expecting this to fix but just showing you how you would actually go around debugging these issues and if we go back here and hit enter I'd have to actually put 8080 on the end here let's see if that resolves the issue so notice it's not resolving so you know if you wanted something to do to test your skills you could try to fix that but uh again I just wanted to show that the power script uh script was actually working as we just want to know that we can execute those so I'm going to go ahead and just close these out this is done here did I make a change probably so minor change for future people minor change and I'm going to go ahead and spin this stuff down so we'll go over to our instances here and I'll just checkbox all three of these and we'll say stop when they're all stopped you can go ahead and delete your security groups it doesn't matter if your security groups kick around but um you can't delete it while they are stopping you have to wait till they're 100% stopped but I tend to go in here and try to clean up my security groups as much as I can because they just get overwhelming after a while but I'll see you in the next one okay ciao so from within your ec2 instance you can access the ec2 metadata information from the metadata service via a special endpoint and uh you know I've seen the nalization MDS but there's also IM imds which would be the instance metadata so just understand I'm using a very iation there but there are two versions of the metadata service we have instance metadata service version one which is a simple request and response method and version two which is basically the same thing except you're passing along a token so it has an additional step why are there two versions well at one point uh there was a very large um breach to a very famous account I think it was Capital One I think that's what it was and it exploited um the fact that instance metadata uh was not protected and so by introducing a a a token uh it could protect against open firewalls reverse proxies and ssrf vulnerabilities and specifically that Capital One um which is a big Bank in the states not here in Canada um the exploit I think was open firewall open firewall rules where you had a previous adus employee uh that worked for the S3 team who had left and knew how to uh make use of that exploit so this version two closes that off and generally when you're launching up a new ec2 instance even though you can use either or version it's going to force you for version two unless you explicitly change your settings but there are two end points that you can use we have the ipv4 one and the IPv6 one um in Prior exams they would actually ask you to remember the ipv4 address I didn't see it on the new exam I don't know if they're still doing that maybe they' decided against it because now there's the IPv6 one but um you might want to memorize that one because I remember it as an exam question and IPv6 obviously has the square braces because that's what you have to do but I don't think you have to memorize it uh let's take a look at um how we would actually go about retrieving data so if we're using imds version one we could use a curl and you'll notice we're just passing along to http totally works fine and then you have idms version two and all it's doing is doing a separate curl I know this looks complicated but it's not um it's doing a separate curl over here to get the API token and it's saying the lifetime of that token and then it's passing that token along uh to the second request here as a header um to the same end point okay now if your machine does not have curl you can also use W get they serve the same purpose the syntax is a little bit different as you have to provide some flags and some formatting differences but these Services curl and W get work the same way you should know how to use both both of them because one is not always available it's usually curl it's not around um um so instance metadata is grouped into categories and there are there are over 60 plus categories I could not fit them all on here and there's no reason to really put them all on here but the point is is that categories is basically just uh addition like is specific end points and you'd have to use specific end points anyway to get anything useful because if you hit the main one all it's going to do is give you the list of of categories but the idea is that you're going to append that to the end of your URL and and in this case if I was using the default Security Group it would just return back that value so the idea is that you can get information about your ec2 instance that you can then pragmatically use for whatever you would need to do notice here that we broken this up into two steps because you don't really have to join them in a separate step even the last one I probably would have used xargs U we talked about that in the ad CLI tutorial how you can use xargs to pass along a single line but uh yeah that is a more concrete one this one I find that I get more reliable results when utilizing it there are configur configuration options you can configure around metadata so you can enforce the use of tokens you can turn off the endpoint altogether you can specify the amount of network hops allowed it says a load but there is supposed to be a w here so I apologize and so if you do this in the Management console it's displayed differently as opposed to here um it just looks like you pick version one to two but really what you're doing is you're saying enforce the token or do not have a token if you're enforcing it then you're using version two if you receive a 401 when attempting to use um I said use the token but I meant to say like if you're using version IMD IMD version two or just using curl then it's likely that tokens are required you must use IMD version two so just understand that I wrote two here it's supposed to be one but just I don't want to record this little video just because it's one mistake but the point is is that if you get a 401 you're probably using the plain curl and you need to use um the one with the token okay but there you go hey this is Andrew Brown in this video we're going to take a look at the metadata service which is um a way of getting U metadata information about our ec2 instance while we are on the interior of it so I'm going to launch up first um and an ec2 instance that just will give us access to version one and then we'll do version two so we'll go ahead and say U metadata metadata ec2 or metadata version one maybe and we're going to choose Amazon 2023 we're going to stick with T2 micro I do not want to use key pairs uh for this we don't need any security groups so that's not going to matter in this case so I'm going to go with the default one as there's nothing special that we need here today I'm going to go to Advanced details I'm going to choose our E2 SSM roll if you don't know what that is go make yourself an IM am profile it's super easy um I don't say in every video what you need to do but the idea is what you're going to do is go here and chose ec2 okay go next and you're looking for SSM core and that's going to be your your policy okay but I'm just going to go back over to here uh and I la want to launch up this instance there's nothing super fancy going on here that I need to do um we don't even need to have a run script but one thing I do want to check is the metadata information so if we go over to metadata we can choose whether it's accessible or not um and then we have meditated version so whether it's version two we going to be version one and two um which is interesting that I can't just say one but you can say with or without the token and so this is going to give us the option of doing one or the other so actually I guess we can do everything in one instance we'll go ahead and launch this instance so that is now launching we'll go over here and we will wait for this um this to get running okay all right so I think this is running and I mean really still not initialized there we go really it's version one and version two right but we'll go ahead here and uh create a connection and we'll go establish that connection so now we are here I'm going to go ahead and type in pseudo Su hyphen ec2 user so that we are logged into the default user uh which is what we want to be uh and so this IP is 169 254 169 5254 I kind of feel like we need a bit of a scratch Pad so uh what I'm going to do is just go over to my GitHub repo you can use whatever you want I'm just going to use this as the scratch Pad because it's just so quick and easy for me to open I'm hitting period with 169 254 and I don't know if they do it anymore in the exam but they it's still might appear on the exam so you might want to um to have that there like know know about that right um so I think that that is the default URL that we want to hit let me just double check my slides it's metadata so if we want to get all the the metad DAT we're going to go ahead and utilize this so we're going to go ahead and copy this and we're going to go back over to here I'm going to type in curl I'm going to paste this in and hit enter and notice it's showing us the categories that we can then query I want to show you that you can use this with W get W get's going to be a little bit different so we'll type in this hyphen qo hyphen um sorry zero I put zero in there it's supposed to be o I think um and you have to actually type w get correctly and you don't want to have the word uh curl in there and you're supposed to have a space after it and you're supposed to have the protocol lots of mistakes but there you go same same process right but I'm going to use the curl because it's a little bit nicer to write and so now that we have our uh curl let's go look at all the categories that are possible so we'll just type in categories categories uh metadata ec2 and you'll see why I didn't include in the slide because there's just way way too many there's just tons and tons so if we go here we can try to find something um so let's take a look here and see something fun that we could pull up I mean not super fun but we can get the public ipv4 address so I'm going to go back over to here into our scratch pad and the idea is that we should be able to append this on the end so we're going to go ahead and do that and I believe it's just this to get it so hopefully that works we'll go back over here sessions manager and we'll hit enter and so notice we're getting back the ipv4 address so we know how to do it with version one let's do it with version two so for version two we actually have to get a token and I think what we should do is we should force it to only use uh version two first so that's what I would like to do and so let's uh go ahead and use a CLI command to do that um so this one is the modify instance Metadate options I think it's called so we'll say e client modify instance metadata options I make it sound like I have this all this stuff memorized but I don't I have slides over here to uh to appear a little bit smarter than I am but uh definitely can get stuff done here so we go down here below and we can see our example here uh I'm going to go ahead and copy this and I'm going to go back over to our scratch pad and so um this is enforcing that this is required so that's all I really want to put on here I don't think we need to change this because it already is enabled so that is totally fine we probably will have to specify the region so we'll go ahead it here and type in CA Central 1 and the other part is we're going to need to go get this instance ID now this might not take effect until we restart the machine I'm not 100% sure but we're going to find out here in just a moment okay so we'll go ahead and we'll copy this command i' actually want to do this from from CL Cloud shell so we'll open up cloudshell here and we'll just give it a moment to load it is now loaded we'll go ahead and paste in our link and we'll hit enter and so that should enable yep version two CU we're expecting that token so if we go up this should not work and so that instantly works so now we have to use uh the token method in order to grab it I find it easier to um set the token up separately here so that's what I'm going to do we're going to go down here for a moment I'm going type in token dollar sign and I'm going to go ahead and type in my curl so this be curl hyphen X because we need to specify it needs to be a put method we're going to have double quotations for our string we're going to have something very similar here so I'm going to grab this one up here here but instead of going to the metadata we're going to go and get an API token and then I'm just going to put a for slash on the end here and the next thing we need to do is we need to define the header because we're going to need a header this is ADS ec2 metadata token TTL seconds because we're going to say how long we want this to live for um I'm not sure how long 216000 uh seconds is but that sounds like a lot of seconds we could shorten it if you really want to I'm not going to do that here today and so that will get us our token um the only other thing I want to do is I want to put a silent on our curl here just going to double check that looks good to me so I'm going to go ahead and grab that and we'll paste this in here and hit enter and I'm just going to do an echo on my token to see if it's actually set and it says 403 Forbidden so that's not necessarily a good thing as we do want this to work out properly so let me just double check and see what I'm doing wrong okay now as far as I can tell this looks fine to me but um it's just not working so what I'm going to do because we did make a major change I'm going to go ahead and uh reboot this instance uh so we'll go ahead and reboot this and so that should stop this here right so that's going to terminate that session we're going to go ahead and reconnect when it is ready to let us reconnect so we'll have to wait a little bit okay all right let's give this a refresh and see if we're able to reconnect to sessions manager here there we go we'll go ahead and do that and we'll just log in here again Pudo Su hyphen ec2 user and so I'm hoping that uh now what we'll be able to do is use that token I'm going to go back over to our scratch Pad grab this line that we wrote and hopefully it's just going to work if it doesn't we can probably go to the docs um and try to get the correct one still not working okay um let's just go over to the docs so we'll say metadata ec2 docs frustrating because uh this worked before and we want to take a look at actually how to utilize it so if we go here should be something like use it retrieve instance metadata so this is two this is like another way of doing it we'll go ahead and copy this just copy the line as such I'll go back here paste it in hit enter and it's working okay so for whatever reason I'm not sure why it wouldn't let me set the token there but now is working we're using version two and there you go so those are the two methods that you can utilize you can see you can turn it on and off you have to restart the server possibly um but not super complicated um but yeah we'll just wrap this up here by terminating this instance here we go and I will see you in the next one ciao let us talk about instance types and specifically the naming convention when you see something like this what does it mean because there is a logic behind it uh how important is it to memorize well uh for associates not so much for pros it can really help you understand what kind of instance you were launching for your scenario so in some of the pros like the solution architect it does kind of matter more but let's break it down the first is the instance family this is the intended workflow the instance type is designed to meet and that will always be the first thing it's a basically a letter the next thing is going to be the instance generation it's the version of the family offering it affects the version of the hardware so uh newer means better right so if you're on version C10 then it's going to be using the latest processors or underlying Hardware or the hardware can just absolutely change between versions um but the idea is that usually the higher the number the better it is the next is the processor family so this is a specific type of processor um this is not always there I think G is for graviton um uh but the idea is that if it's not there then it's probably using Intel usually that's the default one but um sometimes you have let's say C7 and you have c7g right then there is uh additional uh capability so this is indicating what extra functionality this inst instance type might have and so that could be something like extra storage I'm not saying that that end represents extra storage I'm just saying that a letter there could represent some additional functionality and then you have your instance size and this is going to determine the amount of virtual resources available to you such as the vcpus and memory um so that one is a a big deal that you'll want to remember um I just want to point out here that some parts of the instance type may be emitted so like you see T3 micro it does not show the processor family it does not show the additional capabilities um because it doesn't need to it doesn't have additional capabilities and uh we can assume that it's using Intel underneath um but yeah there you go what are instance families well they are different combinations of CPU memory storage and networking capacity uh and the idea is you allows you to choose the appropriate combination of capacity to meet your applications your new requirements so different instance families are different because of the varying Hardware used to give them their unique properties so I don't think think this is the whole list but it's most of the families that I want you to know and then we'll look at the full list after this in a different perspective the first is general purpose so here we have a bunch we have uh A1 the tline the M line um I want to point out that A1 is actually um a previous generation one but I put it in there anyway it used to run arm but uh you know there can be um uh families that are uh previous generation or or um that you wouldn't necessarily want to select but they're around for Legacy reasons and so that's an example of uh a previous generation uh instance type so the idea of the general purpose balance of compute memory networking resources but even between the t's and the M's there is a uh a key difference which we'll talk about in the next Slide the use case here is web servers and code repos then you have compute optimize so that was going to be your SE line and these are ideal for compute bound applications that benefit from high performance processors this could be for scientific modeling dedicated gaming servers or ad server engines or even um a cheaper form of uh machine learning training or inference when you can't afford the gpus we have memory optimized so that's going to be our rline or xline and the zline this this is a fast performance for workloads that process large data sets in memory uh this is for inmemory caches inmemory databases realtime big data analytics we have the accelerated optimize so this is your Pine your gine your uh uh INF uh that's for inference one and VT this is for Hardware accelerators or coprocessors so this is for machine learning compute Finance seismic analysis speech recognition so imagine you have um gpus attached to it I think um so we have storage optimize so you have your iine your dline and H1 this is high sequential read and write access to a very large data set on local storage so the example here could be nosql inmemory transactional databases data warehousing I wouldn't get so fixated on the numbers or the the family versions as some of these are could be a bit dated now but what's important to remember is the individual letters as they are going to really help you indicate this stuff commonly instances families families are called instant types but an instance type is a combination of size and family so just understand that that term gets mixed up and here's the other view so this is actually the full list as of today so you can see we have compute dense fpga Graphics intensive high performance Computing storage optimize storage optimize with a different ratio for vpcu memory for 1 to four 1 to six uh infer netia I don't even know how to say it but it's for inference for machine learning that's a special chip that dat us has produced general purpose Mac which you can run Mac OS which is pretty cool GPU accelerated memory optimize burstable performance uh train tanium which um is for training machine learning because it has special hardware for that hper high memory video encoding memory intensive but there are two that I really want to point out and this is from the general purpose families we have the tline uh which offers or sorry we did the M first but the M offers an even balance of compute memory and networking and then the T family is the cheapest family that you're going to be familiar with because it's in the free tier and it's great for variable workloads and basically the burstable ones allow you to um burst for a period of time Beyond uh your usual uh compute but uh if you want more stable memory or sorry stable uh capacity what you're utilizing then the mine's going to be great but you're going to use it at a certain point uh when your apps become stable and you need a certain level of usage because they're just not as cheap as the tline so there you go hey this is angrew brown in this video I just want to take a look at instance types so that we fully understand um where we can find them in the AWS Management console and how we could understand the pricing so going to go ahead and pretend that I'm going to launch an instance and what we'll do is just scroll on down here and we'll see that we see instance type I don't remember this get advice button being here which is kind of interesting where you could kind of say hey what am I looking to do um I'm not sure if I would really take its advice but let's say we want to do content delivery Network and it's kind of weird because it's like use case and then workload so we'll make a mix of this I'm just curious what it would do what it would do and here we would recommend where do we see that over here oh okay so it's ping actually to Amazon q and then Amazon Q is going to answer it and so here it's suggesting the c7i c6i uh tritanium one series I would not take Amazon Q's advice whatsoever because it's not very intelligent so you know make sure you do your due diligence and read and understand what it is as these um AI tools are not actually intelligent um especially when choosing instance types but anyway what we can do is go ahead and look at compare instance types and here we get a nice big list of instance types it's not indicating price here which is unfortunate because with um with Azure it actually shows you in line whichever really like but if we wanted to find instance types prices we go over here and we just have to look uh based on its cost um based on what you're looking for okay but we're going to go back over to here and you can see we have T2 Nano T2 micro and it's pretty straightforward as you go up you can see uh things get larger um we can go here and filter based on a lot of different things so if you're looking for something with gpus you could say something that has two gpus and so here's something that has two gpus you'd have to obviously read up about it to find out exactly what is going on with these instance so here we'd actually be looking up to read about the G3 family so G3 family ec2 instances okay I guess this is the page and so you just have to carefully read about what it says so here it supports opengl 4.5 Cuda 8.0 it'll it'll tell you like what kind of gpus it's utilizing underneath somewhere here probably under here we have 2.7 Intel Zeon processors here it is four Nvidia Tesla M60 gpus um and that's pretty much it like it's not super complicated uh the other thing is that you might not be able to launch particular instances and the way we know is if we go to our service limits so somewhere here we have our um service limits quota increase and there should be one for ec2 instances just type in ec2 and uh you'll notice here like there is particular limits so the other day I was trying to launch something up in um uh what do you call it uh Sage maker because I wanted to do some um training or or something and I can't remember what it was maybe it was like a p instance I don't remember but I made a request uh to increase it but yeah that's where you could find out where you might need to increase it because some some you just don't have any access to you actually have to ask before you can use it because they're so darn expensive but that's all I wanted to show you so there you go I think it's worth spending some time learning about the different kinds of processors that are available at least on AWS um so ads underlying instances can have access to a variety of different processors to meet specific Cloud workflow needs this is going to be really determined based on the instance family and its version but let's just make sure we're familiar with these Brands the first is Intel Zeon processors these are similar to your Intel Desktop CPUs or your laptops but they have advanced capabilities that make them much more performant for doing nondesktop things which is basically what we're generally doing in the cloud um there is another line called Intel Zeon scalable which is um a a better performing version of the Intel Zeon processor uh but you know I'm not going to distinguish that there then we have amd's epyc I have no idea what epyc stands for I just know that it's an alternative to Intel's uh Intel based instances and the idea here is that um you will see instances where they will indicate that there is a family or sorry a um a processor version of let's say I don't know like maybe there's one for t4s or t3s and they would have one for AMD and the idea is that it's basically the same thing as using the Intel one but it's going to be more cost effective because amd's whole thing is to try to undercut Intel uh then you have Nvidia gpus these are grass graphic intensive workloads often used for machine learning so um at one point you could attach gpus um I think they got rid of that service or um I forget what it was called but anyway it was something that they had and they got kind of rid of so but anyway the idea is that you can spin up uh instance types that will have a GPU permanently attached to it but not necessarily attach a bunch of them um we have adus graviton processors this is a custom built uh processor by adus specifically for the arm architecture and arm is really good and any case that you can run x86 you can pretty much run arm and your software should generally work with uh no issues um and it's just going to be more efficient um and it usually is more cost effective so when you have that opportunity use the arm architecture when you can then you have Intel Habana Gotti processor so um Intel acquired Habana which made this very specialized processor for machine learning called GTI there's like a bunch bunch of versions of them just like graphon has version 1 2 and three G is like I think on version four or something now um and it is a very powerful processor for machine learning you have Intel fpga um so fpga stands for fuel programmable programmable gate arrays uh for workloads that benefit from Custom Hardware acceleration I don't know much about the space but it sounds like a uh programmable um processor which sounds very powerful um then you have uh I don't know how to pronounce it but I'm going to try zil zil zilink zilink and basically this is just like Intel fpga but it's amd's version of it I think AMD acquired this company so um there's that then there are a couple of specialized um processors made by AWS specifically for machine learning uh the infer entia which is specifically for ML inference and then you have TR uh tranium which is specifically for uh training models so there's specialized chips specifically for inference and machine learning um but yeah there you go let's spend a little bit of time here talking about burstable instances this is the family type that starts with t because it's going to be the first thing you're going to be launching so you might might as well know a little bit about it so burstable instances allow workloads to handle bursts of higher CPU utilization for very short durations this allows adous customers to save money overall since they do not need to upgrade their instance base on the highest peak usage the idea is imagine you have compute um and for the most part it's always going to be near that Baseline that average CPU and that you have some spikes and you don't really want have to pay for uh pay to provision instance that will meet that larger one or have uh more complex scaling operations to meet it and so this is where the tline comes into play there's a few different T lines you should know at this time we have the t4g which is based on the Gravitron arm that's why there's a g there we have the AMD epyc we have the Intel xon scalable one for the T3 and then we have the Intel zon one so I just want to show how uh this works so the idea is that T2 is the free tier one and it's because it's using previous generation uh processors right so if you want something that performs a bit better you're going to use the T3 and really the T3 is the overall best one um but the T T2 is what is offered in the free tier though I do believe uh T3 is also free tier as well um but I'd have to double check that but anyway you'll see me often switching over to T3 then if you want something that is cheaper and you're outside of the free tier then you can H put that a on there and that's going to indicate they're using the AMD epyc and we said in our processor section that epy uh processors by AMD are basically the same kind of compute but cheaper then you have arm so arm is a different architecture whereas uhep AMD and Intel processors they're basically using x86 they can do arm but the point is is that um this Gravitron one is specifically optimized for arm and arm is just a more efficient architecture and so the t4g is the cheapest one possible um but you have to go out of your way to select it so you'll never see me in um videos go and select the t4g even though it's great but that is the latest one as of this video in terms of burstable instances there are two modes we have standard mode so this is the default mode it provides a baseline level of CPU performance with the ability to burst above the Baseline using accumulated CPU credits suitable for workloads with variable CPUs then you have a limited mode this allows an instance to sustain High CPU performance for any period whenever required exceeding the Baseline and accumulate CPU credits with additional charges applied for the extra CPU usage beyond the accumulate credits so there is this whole credit system that goes in with burstable instances u in this slide I'm not explaining it but uh you know the professional level um or other Sears you might actually need to understand how that credit system works but right now for this slide we do not need to know but just understand that Bible allows you to um deal with spikes of increased CPU without up having to upgrade your instance or scale up horizontally other instances so there you go the ec2 source and destination check when enabled restricts an instance to only send and receive traffic where it is the source or the destination and why would you want to uh disable this feature or even have it well the idea is that there could be use cases where let's say you want to create your own natat which we do in this course um you would needed to pass along someone else's address uh because it would have to do Network address translation and so this is a case where you wouldd want to turn off source and destination checks source and destination checks are great to have on because uh they they protect against malicious behavior or you know using your machine to do nefarious things but uh again there are legitimate cases for turning off source and destination checks there you go in the ec2 Management console it allows you to observe the system logs for the ec2 instance this is really great when you are trying to uh troubleshoot your instance because you see some like it's not working as expected but let's say you don't necessarily uh want SSH into it or you simply can't um you know there might be some information here uh when it runs cloud in it you can even see it in there it's actually showing some things that cloud unit is doing uh some Marketplace and Community Amis will write the default username and password for software in the system log for you to initially log in to whatever uh web technology is there and then you obviously change it after the fact um so you know you can come across system log for a few reasons logs can be delivered to cloudwatch logs by installing the cloudwatch unified agent and can be accessed through a cloudwatch log group of the same name of the instance so um this is something that you have to install um I always think that it's preinstalled but it's not necessarily installed but there is a way to use systems manager to easily install it on systems like Amazon L 2023 if you do get it installed then you will be able to see all these logs collected and put into cloudwatch log which is extremely useful if you're using Amazon link 2023 you might not have the same amount of logs the log structures might be a bit different I highlight uh those ones in red because often I find that sometimes when I'm troubleshooting stuff I actually have to go look at cloudinit to see how it was um executing that stuff there uh but yeah there you go placements groups lets you choose the logical placement of your instance to optimize for communication performance durability and another nice thing to know is placement groups are free I've never had to fill with this but I'm sure some folks who have more impressive workloads have had to do that let's talk about the three different types of placement groups the first being cluster this packs instances close uh close together inside an availability Zone it's low latency Network performance for tightly coupled no to no communication it's well suited for HBC apps and clusters cannot be multiaz you have partition this spreads instances across logical partitions each partition does not share the underlying Hardware with each other so it's rack per partition it's well suited for large distributed replicated workloads such as Hadoop Cassandra and Kafka then we have spread this is each instance is placed on a different rack when critical instances should be kept separate from each other you can spread uh a Max of seven instances and spread can be a multiaz so there you go all right so there is a lot of different ways that you can connect to your ec2 instance U so I just want to in plain old text go over the different uh ways we can connect and then in the labs we'll go actually go ahead and do that so the first is SS client this is the most standard way the most common way that people know how to connect to Virtual machines and it's pretty straightforward the idea is that you have um a public and private key uh and you use use that to use ssh in order to get into your remote virtual machine how it's different on ads is that you actually generate the public and private key on ads for the default user you download the public key and then you utilize that to get into your instance uh it's all obviously done on Port 202 you have to make sure your security group has Port 22 open in order to use SSH um you can after the fact go in and add other users uh and add keys but generally you want to use that managed uh key pair that it provides you then you have E2 instance connect this is for shortlived SSH Keys controlled by IM policies uh it works only with Linux and not with all instances I don't know where exact limitations are I generally rarely ever use this because I like sessions managers better but it is a good in between between sessions manager and SSH client because you get SSH Keys they're shortlived so you have uh a better security than SSH clients and you have better uh control over access because you have IM policies um you know but you don't have to do everything within the sessions manager uh window which is not as fun then you have sessions manager this is a connection to Linux or Windows machine via a reverse connection or they call it it's like an outof bound connection not a networking person I don't understand I just know that it works and the idea is that you do not have to open up a port um in order to connect to it you do not have to open up um security groups for this uh you'll have you can use this with Windows if you log into a Windows Server it's going to log you into a terminal like EXP exp with Powershell if it's Linux it's going to be your standard B shell again you don't have to open up the ports can uh access can be controlled via IM supports audit Trails of logins you do have to attach the proper permissions in order to talk to sessions manager but other than that pretty easy we have fleet manager remote desktop um I only noticed this because I was trying to connect to a Windows machine and I was having a hard time downloading the RDP um file and I actually didn't list it in here but technically you can download the RDP r DP file and log in the usual way but um the way that inabus provides you is the remote desktop via fleet manager and basically it's RDP but in your browser super easy super simple you have ec2 serial console this establishes a serial connection giving you direct access for troubleshooting the underlying Hardware I don't really understand what to do with this because I'm not uh somebody that worked in data centers but I can understand that if you really know your Hardware very well and you logged in via the Seri console you'd have access to more stuff um whereas the other methods are at the software level this is when you want to deal with hardware issues and there you go hey this is Angie Brown in this video we're going to take a look at utilizing ec2 instance connect which allows us to use shortlived SSH Keys controlled by IM policies uh so what we're going to do is go ahead and launch a new ec2 instance that we're going to want to connect to so I'm going to go ahead and just launch a new one so we'll name this ec2 instance connect and I'm going to specify um uh Amazon 2023 we're going to do T2 micro um and should we use a key pair um I'm thinking about it no I'm not going to use a key pair here because um we are going to generate out an SS key that we're going to temporarily use we do need to have uh SSH traffic open so I do need to create a new uh Security Group I'm going to call this one uh ec2 instance connect SG and so that is sufficient enough for us to establish our connection let's go ahead and launch this instance okay and we'll just wait until it is ready okay all right let's take a look here and check on its status so it looks like it is up let's go ahead and connect and this this time what I'm going to do is go to ec2 instance connect so it says Connect using the ec2 instance connect and by the way that is a piece of software that has to be installed on the ec2 instance if you're using the Amazon L 2023 it's already preinstalled if we using something else uh we might have to do a bit of work to get that installed but notice that we have this or we can use a um a connect endpoint so there are uh a couple different ways that we can do this let's go take a look at this one and so simply we're going to do is go hit connect and I believe that should just connect okay but I need to make clear that it is using ssh in order to establish that connection so even though you don't see it it is downloading an SSH key and utilizing it um somewhere but uh you know we didn't have to do anything fancy but we are here let's go Ty type who am I and notice that we are already immediately the uh the um ec2 user so that is uh one way to establish that connection um I'm going to go ahead here and just disconnect we'll just type in exit to log out of that connection we'll go back over to here um and I'm not very familiar with this one let me just take a look here so as far as I understood is that if you wanted to connect you didn't don't need an endpoint per se but um only endpoints that have completed the creation process can be uh selected so I guess this is if you want to utilize a VPC endpoint from a private ipv4 address we're not going to do that today um what I want to do is um Connect using SSH and so way I'm going to do that is I'm going to go ahead and go over to GitHub and I'm going to need a kind of environment to work in so I'm going to use git pod if it decides to work today for me sometimes it is a bit problematic but I'm go ahead and establish a connection here with GitHub or git pod I'm going to spin up a new instance you could use anything you want your local machine GitHub code spaces wherever you have uh control over SSH but what we're going to need is an established connection to um this ec2 instance so I'm going to go ahead here and type in ads STS um get caller identity because I don't know who I am right now in this account and I might have already established some adus credentials here and it shows adus example as a user so what I've have done here is I've established a connection by uh uh creating an IM an IM uh user here let's go over to that user say I am user and I'm just going to set those uh the username and password if in case you don't remember we do that a lot in some videos but in the ECT section not so much so just in case we haven't done it yet I just want to go ahead and do that so under our users I have an in examples um and I have some credentials I'm going to go down here below I'm just going to delete deactivate and delete the existing ones okay delete and we'll do that and I'm going to go ahead and generate uh new credentials not for code commit I want this to be for um access keys there we go and we'll just say command line interface and yes I I will not abuse my own stuff go ahead and hit create and so we need to set those I'm going to look up IUS um access Keys uh itos access keys and bars I never remember what they are um and so over here in and get pod I'm just going to make a scratch Pad really quickly and I'm going to grab these three here paste them in say allow and I want to export but I also want to uh do gpv which is specific for git pod different environments have different things I show you in different videos so if you want to know how you'll have to look uh look that look that up for those other ones so I'm going to go ahead here and just clear these ones out say Central one I'm going to Expos my keys here but I am going to uh rotate them out at the end of this video so it's not going to matter I'm going to go ahead and paste this one in here and paste this one in here and I'm going to double quotation this because just with axis keys I always find that I have trouble with them if I don't do that there could be uh breaking characters in the um hear like for or backslash and stuff like that so you really do want to wrap these so go ahead and copy and paste and um that looks fine to me I'm not sure why it's highlighting wrong but I don't think it matters so I'm going to go ahead and grab the first bunch and do copy and paste and hit enter and then grab the second Bunch copy paste and enter and I'm going type in clear I'm going to do ads STS get caller identity it should still be the same user I typed identity wrong I always type identity wrong get color identity there we go and so I'm established this one this user has admin access so they have full permissions to do everything so I imagine they're not going to have much trouble connecting in order to establish connection we need a way of pushing an SSH key so what what I'm going to do is go to ads um we'll say ads ec2 connect uh instance connect I have a feeling the blog probably tells us how to do this does this one do it uh this one is not the one I'm looking for maybe this one there we go yeah so here we need to generate a new key so I'm going to grab this command very common command to uh have to know and just by the way I like I closed my scratch pad I really don't want to commit anything here by accident so just make sure you do that I'm going to go here and just say uh um E2 connect I'm going to hit enter I'm going to enter again I'm going to hit enter again and so now I have this key notice that it is here we really do not want to commit this so just be careful that we uh remove these at the end here um so the next thing is yeah we need to use this command called send SSH public key so we go over here to the docs going to go over to the new ones here we go down to examples what this is going to do is it's going to temporarily push our SSH key to the instance and that way we're going to be able to um uh then you'll be able to actually log into that instance so here look at this is the following sends an SSH public key example sends the specified SSH public key to the specified instance okay but how would it know about the private key oh because we would have the private key that's right all right that makes sense so um let me just think about that for a moment who has the private key the server should have the private key right and then we send the public key as a confirmation so how's this going to work if it doesn't have the private key give me just a moment okay all right maybe we are specifying the um the private key in the SSH connection the public key gets sent there and I'm just getting mixed up here but uh whatever uh let's just go ahead and follow along and this should uh technically work right so we'll go ahead and just grab this command here as we're going to be working with it might as well get the one that is here um I mean it doesn't really matter I suppose we could grab this one it is a little bit more uh tidy so go ahead and paste this in here and let's just take a look at what we have so we need the instance ID uh we'll go here and just make sure we grab the one that is running good and we'll replace this we are easy to user for this one that is good um this is in CA Central one but the question is what exact uh a is it in so go to networking here 1 a central so go back over to here and say 1 a and then we have uh this pub file so we'll just say e to connect and I'm going to line up the next line uh the next command that we have to run here so we'll go back to this one and we'll grab this we'll paste it in below and um we could actually use the IP address if we like instead of the DNS host name it depends on what you want to do I am going to grab the public IP address cuz it's shorter we'll paste that in here and then here we're just specifying ec2 connect um I feel like we'd have to traod that file so usually you have to do that and say um 400 is what they usually tell you to do so if we go ec2 ec2 connect I think we have to do for that file our our private key I think and we'll go ahead well actually let's go back and take a look here it would tell us right here yeah it is right yeah yeah okay and we do pass we do we do use the private key okay I I was getting it mixed up sometimes I get things backwards um so I'm going to go back over to here and we'll go ahead and copy this command and hopefully it just works figures crossed there we go and they said like 60 seconds so we got to go really quick here we'll go ahead and establish connection we'll say yes to the fingerprint and we're in so there you go so that is how you can use ec2 instance connect let's close this stuff out I do not want to save any of this environment so I'm going just I'm just going to say stop this workspace and let's go ahead and tear down this instance we'll terminate that and while I'm here I might as well rotate these out very quickly and I will see you in the next one ciao hey this is Angie Brown in this video I want to connect to a Windows instance using RDP so what we'll do is go ahead and launch an ec2 instance this shouldn't be uh too difficult and we'll go ahead and launch ourselves an instance and so I'll just say my web server and we'll go ahead and choose uh windows and here I want to CH something um a little bit larger than a T2 I'm going to go to a um a T2 medium or even yeah T2 medium seems fine and I do not want actually no I do need a key pair so I've actually made one here before I'm going to go ahead and delete it so we can see the whole process here so just take just a moment to get rid of the old one which is under key pairs here so let's go ahead and delete that delete but what I'm going to need to do is generate a new one so so we'll go create a new key pair I'm going to call this Windows RDP and we'll download the RSA and we'll download the Pam and we'll go ahead and do that that's going to download that file here which we are going to need to utilize here shortly after I'm just going to put this in the default um Security Group so I'm not making a mess of things we'll drop this down and we'll need to uh set our uh E2 SSM roll which I've shown you so many times um and then we'll go all the way down to the bottom just double check things everything seems fine we'll go ahead and launch this instance so now we will need to wait for this instance to launch and uh I'll be back here in just a moment okay all right let's take a look here and and see if the instance is ready so our our checks are pass we'll go ahead and connect and you'll notice we have a sessions manager which we can establish a connection but it's not that interesting for uh Windows servers I might as well just show you here our real goal is RDP but if you notice it will connect us into uh into this instance and it's going to start up Powershell okay uh if you're wondering what version of Powershell I think we say get um version or sorry it's like PS version table and so we can see that this is running version 5 5.1 I'm going to terminate that that is not our real goal here our real goal is to uh connect via RDP so um if we go to RDP client notice we have two options we have Connect using the RDP client or Connect using the fleet manager U so to this one you can download the remote desktop file this is an RDP file a lot of people with Windows uh work with Windows machines is very familiar with this and you have to have the remote desktop client installed if you're on uh Linux or if you're on a Mac you're going to have to install that so you just type in uh Windows desktop um RDP uh Mac OS it's a program you download Apple so Microsoft Remote Desktop it's totally free um on Windows it's already preinstalled I don't have to do anything extra the idea here is that we have that file and so if I open that file in my downloads and I double click that file if I can find it here wherever it is um here it is if I double click it you're going to notice it's going to say do you want to connect I'm going to say yes it's going to prompt me for a username password and so just trying to uh establish that connection now we use the default um the default sub uh Security Group and that's where we're running into problems because we have to have that uh Port open so I'll just say windows RDP SG and we'll go ahead and just choose RDP here we'll say from anywhere so just say windows RDP twice we'll create that Security Group and I'm going to go back to my instance and I can just attach it as an additional Security Group so go here to our security change security groups I'm going to go ahead and add the other one we'll go add it and save it and so now if I go back and I try to click that file it should now establish a connection because the RDP port is open and if it's not then uh yeah it complained again what I'm going to do is just reboot the server okay so we'll go ahead and reboot that and I'll just wait for that to reboot I'll just wait a a couple minutes okay all right so let's go ahead and see if we can uh connect via RDP now well I already have the uh desktop client file downloaded however since I've rebooted it I'm not sure if that uh address has changed so I'm going to download it again and just double click it and so hopefully will connect this time it sure is trying hard here today still no luck I'm going to go ahead and try it again usually it's not this problematic to establish a connection not sure why it's so hard here today and it still doesn't work I mean the only thing that we didn't do that was different was the fact that when I I did this I did not let it choose whatever the default um uh the default Security Group rules are so we can go down here and take a look here at the security group so here it would just open rtb traffic that's all it's doing and we've absolutely done that for sure because if we go back here let's just double check our instance and make sure that it is applied so we go here we'll go to security and for inbound rules I see all let's go to this one okay uh no inbound rules so maybe we didn't save it a little bit odd but okay we'll go to RDP here I'll save from anywhere and we will save this did we maybe add it as an outbound rule there that's our problem it was on the outbound so I mean it's automatically going to allow outbounds so we don't need that I'm going to go back uh back and double click that RDP file and we'll go connect and so now we have a connection notice that it's asking us to enter a password in um so I'm going to go over here the net bios is whatever my local computer is is I don't want to fiddle with that because I just want this to work the idea is that we're going to say get password and then it wants us to upload the private key or put in the private contents I'm just going to open that up here in vs code and then copy over the contents so just give me just a moment to do that should just take just a second here and uh I'll just open it up here we are so here it is in the file I'm going to go ahead and copy this whole thing and I'm going to paste this in there we go I'm going to go ahead and say decrypt and so now I have this password I'm going to copy it and I'm going to make my way back over to our popup here and I'm going to go ahead and paste that in and hit okay and we'll say yes and so now we should establish a connection via RDP there we go we'll see Windows spinning up here shortly there it is so that is RDP now that is one way to connect now imagine you don't want to enter all the stuff in you can just go connect to fleet manager and click on fleet manager remote desktop and this is a much more seamless experience um here we do have to provide uh the key pair still so we'll go here and I'm just going to say paste in the contents I'm surprised it doesn't just let you select it from well I guess it would have um you would have downloaded it but uh surprised we can't just select it from um ec2 but maybe or sorry maybe ec2 does not store the um the the the public key we only download it but anyway we're going to go ahead here and just give it a moment and so now we are within that ec2 instance so that is two ways that you can use RDP on adabs so very straightforward and we are done so let's go ahead and tear our stuff down so um I'm going to just disassociate the security group so that we can delete it we'll just go ahead and I'm going to move both of them it wants the default we'll leave the default on there okay and uh we'll go ahead and just tear this down and while that is tearing down I'm going to go ahead and delete the uh Security Group I created here and I still have another one from before I just want to get rid of we'll go ahead and delete those and I'll see you in the next one okay ciao hey this is Angie Brown in this video we're going to take a look at ec2 serial console which is one of the ways that we can connect to our ec2 instance so let's go ahead and launch ourselves a new instance um so this one is going to be launch instance and I'm not actually sure if we have to do anything um interesting to connect to it because I rarely ever use ec2 serial console uh because it's for debugging hardware issues and I'm not the best at doing that but we'll go ahead and just type in ec2 serial console example because my thought is that um you don't need to have any kind of connection because it's going to directly connect to the hardware it's my assumption we don't have to give it any fancy rle so we'll go ahead and I'm going to proceed without a key pair I'm going to use the default security security group we are launching your standard Amazon Linux 2023 instance and we're not going to be even providing an instance profile so we'll go ahead and just launch the instance and then we'll see if we're able to uh connect to it here in just a moment okay all right let's take a look here if we can connect to the ec2 serial console so we'll go over here it says to connect to this instance using ec2 console the account must be authorized in the ec2 account setting okay didn't even know that was a thing that we had to do so we'll go ahead and allow it and so now it's enabled here under the ec2 settings so that is great we're going to go back over to our instances we're going to go here and go back to connect and that says to connect to the serial Port of an in using easy to zero console the instance must use an instance type that is built with adus Nitro system you can change the instance type to a supported virtualized instance type or a bare metal one so I guess Nitro system is going to be very important so which instances have that that's a good question um T2 is pretty old so maybe something like T3 or T4 will be utilizing it that's the a great question um so let's go over here and take a look so so I really want to know which ones have Nitro so I guess I'll have to go look that up be back in just a moment so I'm not exactly sure but I have a feeling that it's going to be T3 t3s that have it um it could be t4s but I'm almost certain that it's going to be T t3s so I'm going to go ahead and terminate this one we're going to go ahead and launch a new one we'll say EC to um serial console example and then we'll go down here below I'm going to switch out to T3 micro T3 there we go and we don't need a a key pair we'll use the default uh Security Group here and we'll go ahead and launch this instance and so I'll see you back here in just a moment okay all right let's go take a look here and see if we are able to connect to this instance so we'll go ahead and we can see what port we are connecting on and we'll go ahead and do that and we'll just give it a moment to guess connect okay and you know I keep expecting to see something here but I'm not oh there we go if we hit enter now we can go ahead and log in so I think the one thing that we'll have to do is log in as root but I didn't set a password so I don't even know what this would be um so I guess the question is what we do here logging is a rout I etc etc um okay so I guess we're going to have to figure out what the root password is or we have to set one is there a way to do that let me go ahead and take a look here that's a really good question let me go figure that out all right so apparently what we're going to need to actually do is connect to this instance first with sessions manager and then set a password for root and then once we do then we will actually be able to connect here so this is not going to be very useful for us right now um we already have a a rule that we keep using so I'm going to go ahead and attach that IM profile just going to go ahead and look for that so we'll say modify I roll I'll drop this down choose the ec2 SSM roll we'll go ahead and do that I'm going to go ahead and do a soft reboot here and I'm going to hope that soft booot um allows us to establish a connection to sessions manager so I'll just give that a couple of uh minutes because sometimes it takes some time and then I'll connect with sessions manager okay all right so let's go ahead and see if we can connect here with sessions manager um and sometimes sessions manager is a bit funny there we go and so we'll go ahead and and connect and in here I think we're going to have to switch over to our other user here actually we don't probably don't have to do that we proba just type in pseudo in here so we'll do pseudo password and I'm not spelling wrong that's how you have to spell it and you type in root and so now we can enter a root password I'm just going to put testing all lowercase and it doesn't like it so I'm going to do testing one two three okay so testing one two 3 testing one two three so the password is testing 1 two3 I'm going to go ahead and terminate this and now what I'm going to do is go ahead and establish a um a Serial console connection okay and uh yeah so no is just kind of fiddling around there so we'll try this testing or sorry root testing one two three and so now we are in and uh supposedly we can get access to things that we normally wouldn't get access to and you know this only makes sense if you are good with Hardware or network engineer I'm not so let me just read a little bit and see if we can see something so here they're suggesting there is a um a configuration for um the network interface that we might be able to look at now this says e zero but I just want you to know if you type in if config I think it's if config it's not called en0 it's en5 for Amazon link 2023 it's just um how they do it for whatever reason so we'll go ahead and see if we can find this I'm going to type in cat because I think it's just a file here and we'll go ahead and type in ETC and we'll type in CIS config and then the next thing here appears to be Network scripts so type in network scripts and then we have if config so if and it looks like it is showing us that one there so actually surprised that it showed that I thought it' be the other one no I guess that's what it is so maybe it maps to it or something I'm not 100% sure but we are seeing stuff here do I know what I'm looking at absolutely not not a networking person it's all foreign to me but I guess my thought is like that that we're in here is this stuff that we wouldn't necessarily see if we were logged in the other way so like if I was to go ahead and just connect the normal way with um sessions manager um would I get access to the same file I don't know so let's go take a look and see if there's any difference if you are a networking person and you're looking at me going Andrew oh boy you don't know what you're doing it is super true so this one was over here I'm going to go ahead and just see if the same file is accessible I just want to see what the difference is and that same file is over there so that is very unexciting um I mean here they're suggesting that there are two things that we can utilize depending on the situation if I go down here maybe it's not here we'll say ec2 serial console docs have it go over to here say troubleshooting they talk about troubleshooting with grub or cist requirements assist requirements looks like what they're system request is what it looks like they were using in the other one there so I'm going to go over here just take a look um to use S system request the serial console sender break control Z to issue a system request command so let's go ahead and try that I'm going to type in control zero nope that's not it command zero so I just did control C there and I actually got a bunch of options which is very interesting but here it says to send a break press press control plus Z okay and then we get this um I want to get out of this I'm just hit contrl C again type in clear so if I do control C I'm not getting it okay so that control 0 must have work I'm going to do control Z and now control C and notice when I hit control C it's it's basically saying hey we don't know what that input is so I think it actually is working when we do control Z and so H for help I guess and that's what it's bringing out so we'll do control Z Z and then hit H and so it's printing out uh help reboot crash terminate full memory Etc show registers sync again all stuff that is interesting but I have no idea what it is I guess let's show all registers as an example so we'll go here and try this again and do this and the letter was what P there you go so yeah if you're a networking person you probably understand this I don't this is all I wanted to show you so that you knew how to connect to it uh let's go ahead and just tear down this instance and we'll call this done okay and I'll see you in the next one okay ciao let's take a look at Amazon Linux which is ad's managed Linux distribution that is based off of Centos and Fedora which in turn is based off of red hat uh Linux so let's get into it there are multiple versions of Amazon Linux we have Amazon Linux one Amazon Linux 2 and the current version of this this time of this video Amazon Linux 2023 so I would just say that you're going to use one you should be using 2023 as there's not many reasons to use the older ones unless there's you're just having a hard time getting your software to run on the latest and greatest and you need something that is more uh with what your app works with um but anyway adus hardens and maintains the Amazon Linux Amazon Linux often has direct support or the only choice for specific adus Services as the choice as the underlying OS inabus provides technical support via the inabus support um so when you open up case cases uh to get help from technical support they're going to have a better understanding of the Amazon Linux and if you bring them something that they aren't usually looking at they just might not help you as well so when you can use Amazon Linux because it's pretty easy to use um it uses the Yum package manager which technically there is an improved version of yum called dnf that's available on Amazon 20 Linux 2023 but if you're using one or two you're going to be using the Yum package manager on June 30th 2024 Centos Linux 7 will reach its end of life uh Amazon Linux 2 is strongly tied to Centos so migrate to Amazon L 2023 when you can because a lot of the the packages and and stuff are is tied to that um so you know I'm sure we'll see use after that for Amazon 2 because Amazon is one is still being utilized even though I don't really recommend it for use um but yeah there's a thing called Amazon Linux extras which is specific for Amazon Linux 2 and this is a way for you to install additional software packages that are not included with Amazon Linux 2 um and this is really useful because U you know prior to Amazon 2 there was a al1 I remember Amazon Linux because it had tons and tons of packages but then when they uh moved to Amazon Linux 2 all those packages weren't there which is a good thing because less packages generally means um a more hardened uh server because there's less things around for a malicious actor to utilize but um for whatever reason uh when they moved to Al 2023 I think it's because they're using a different Upstream from Centos whatever the version is eight or n they just had more available packages and less of an issue and I don't think people like Amazon L extras are having to install stuff to get stuff and so um basically Al 2023 kind of works like al1 where it just has a lot of packages so this is not going to work for Amazon link 2023 specifically for Amazon Linux 2 still talking about Amazon Linux 2 there is a thing called extra package for Enterprise Linux called epel um and it's a fedora project and the idea was to add additional packages and basically this is what L extras is but this is more generic and Taps into even more packages so if there's something that Amazon L extras cannot do you can install EP epel via Amazon Linux extras and then you'll have access uh to a larger variety of packages um so while Amazon Linux extras can be used to install packages if you need a broader range of packages you can use the EPA L7 um repo or Library whatever you want to call it so Amazon L 2 has high compatibility with centos7 and can support E7 packages I've seen even eight being used um but uh you can't install through Amazon links extras and I probably wouldn't recommend it I think it's worth comparing al2 to Al 2023 because this is going to be the tossup no one's touching al1 uh these days here but al2 it has limited packages but can be extended with the Amazon extra and R p7 Amazon 2023 contains thousands of packages for use you cannot use uh uh rpal with Amazon 2023 I didn't write that in here but I'm just telling you you can't so if you don't have a package you want to use you're going to be having to install it from source to be honest uh al2 comes packaged with 2.7 where the default for Amazon 2023 is Python 3 al2 uses yum where Al 2023 uses dnf could you install dnf on al2 probably al2 has SC Linux so security enhanced Linux disabled by default where L 2023 has it enabled by default al20 al2 is using an older version open SSL where Al 2023 is using the latest L2 uses the centos7 stream where l223 uses the Centos 9 stream uh al2 is going to use the gp2 volumes by default whereas Al 2023 is going to use the gp3 volumes by default al2 has crony install which gives you the cron tab Behavior by default if you want cron tab Behavior Uh you're going to have to install crony yourself al2 has open jdk because at the time I don't think Amazon cetto existed now Amazon coretto exists if you don't know what that is it's uh A's Java uh runtime Library um so you know it's for whatever reason they made their own everyone seems to want to make their own runtime runtime for Java and AD us has made their own as well but there you go let us talk about instant sizes so each instant type includes one or more instance sizes that you can choose from that allows you to scale your resources based on your target uh Target workloads capacity requirements so ec2 instances size generally double in price and key attributes this is not always the case but for the most part if you're going to go to the next size like from a small to a medium or whatever you should see double the VPC cus in Ram uh but there is more to the story as um uh the other things like network throughput is not necessarily going to be double and and uh these jumps are not always double but anyway when we see small medium large extra large just make a note of the numbers so 112 224 236 so not exactly double here this is the same and this is increased but now over here to 54 so you know for the most part it doubles that kind of holds true and in terms of cost and again this is just the t2 line but you can see $16 roughly double here roughly double here roughly double here so you know understand that uh you know things are generally doubling as they go up okay ec2 instance profile is a reference to an IM roll that will be passed and assumed by the ec2 instance when it starts up so the idea is that the instance profile is really a reference to I am roll and that instance profile is a associated with the ec2 instance the whole purpose is to avoid passing Long Live credentials so you don't have to hard code or somehow pass your access key in secret into your um into your OS environment um and the way it's really doing this is using STS assume so if you don't understand that we cover that in the CLI section but anyway ec2 instance profile can be Associated at the time of launch or on a running ec2 instance so if you forget to do this you can do it after the fact however if you never had an E2 instance profile attach a hard boot re reboot is required for the role to be assumed because otherwise it will just not work and you will waste a lot of time this is something we definitely demonstrate in a lab because uh it should be known only a single IM rule can be associated with an instance profile changing roles is not instantaneous due to eventual consistency which is why we need a hard reboot in the first case but if you need the role immediately you need to disassociate and reassociate the profile or do a hard reboot of the instance uh when you select the IM rule when launching an ec2 instance ads will automatically create an instance profile for you so you don't really notice this in the console but when you definitely use the a CLI you are creating an instance profile um and you can't see instance profiles anywhere in the Management console the only way you can uh see a list of them is via the API which is why we're looking at the CLI commands here so we have creating the instance profile adding a single IM roll to instance profile because you can only have a single roll attached an instance profile associate instance profile um to the ec2 instance I'm not sure why I didn't put the word ec2 on the end there but so say to the ec2 ec2 there we go you can list the IM am uh I say I am rolls but it's I am profiles really mucking up here today uh and then you can get information about a specific instance profile again wrote roll but we're saying instance profile here but there you go let us talk about the life cycle of an instance so we have this diagram here on the right hand side which is representing most of uh the actions and states of the instance life cycle I think it actually has all almost all of the states but not necessarily all the actions but let's talk about actions that we can perform which would move something into a particular state so we have launch so this creates and starts an ec2 instance based on an Ami we have stop this will turn off but not delete the current ec2 instance we have start which will turn back on a previously stopped ec2 instance we have terminate so delete the ec2 instance reboot performs a soft reboot at the OS level we have retire um this notifies when an instance is scheduled for retirement due to Hardway failure or end of life and it must be replaced or migrated I don't know where this would be set maybe this is more for dedicated servers or something else I've never seen any C2 instance but it does exist we have recover so automatically recovers a failed instance on a new on new Hardware if enabled keeping the instance ID and other configurations um I don't think I've ever observed that and that automatically happens I think that has to do with the status checks so you know when a status check fails it's saying there's something wrong with the hardware it's going to try to recover it um now we have the states which are you know what state the instance can be in so we have pending this is the instance that's preparing to enter the running State an instance enters the pending State when it is launched or when it is started after being stop in the stop state we have running the instance is running and ready for use we have stopping the instance is prepared to be stopped I meant to bold it I just didn't we have it being stopped as you can imagine what that means stopped uh does not mean it is terminated it just means that you want to use it for later on we have shutting down this is when you have an instance that is being terminated so it's going to be deleted then it's terminated this is where it's completely deleted sometimes you see this for a while afterwards uh it eventually ADV vanishes but the idea is that you can't recover a terminated instance you can just see that you had one uh previously will the exam ask you any of this no should you know this absolutely so that's why we covered it here um there is a little bit uh other stuff that I want to cover with instance life cycles I didn't I forgot to animate the slide so I'll just get my pen out here but there are things that you can um uh configure which can affect the state and all of this has dropdowns like easy drop downs but I wanted to pull up um the CLI commands because they almost all touch the modifi instance attribute so the first is change termination protection so you can prevent the instance from being terminated you can prevent the instance for being stopped you can change what would happen if it ever shut down so should it actually stop or terminate um which is a bit confusing because the word shutdown at least associated in ads always almost indicates terminate but the idea is that if it's turning off for any reason you can tell it where to default and then you have that auto recovery Behavior which we talked about which is enabled by default but you can turn it off for whatever reason if you don't want it to recover but there you go instance console screenshot uh will take a screenshot of the current state of your instance very straight forward why would you want this um this is useful if you're trying to troubleshoot booting issues when you cannot access uh the instance via SSH and RDP this is not the same thing as um the system log is that showing you log information where this is literally showing you what it would be like if you were to ssh in or or trying to gain access and so sometimes what you see here is not a login screen and something else that's more useful um or the information at the top could be uh useful this works for both Linux and windows so you might see what the Windows screen looks like you can use the CLI to grab the screenshot if you prefer and again this is if something went wrong you might see something other than a login screen uh but there you go a host name is a unique name in your network to identify a machine via DNS I want to point out that host names is not a concept specific to ec2 it's just a general thing that you should know um but we are going to talk about in the context of adabs so the host name format will vary based on what compute is being used or specifically what region it's being used uh we'll talk about the ec2 host name format here in a moment um but if you want to ensure your um if you want to change uh change your your host name you need to ensure that you you preserve the host name changes and then you can go ahead and change it to something else because you could have your own domain that you'd rather utilize to identify specific instances for whatever reason uh but you can do that on ads and you can reboot it can do that basically with any virtual machine but I just wanted to make that very clear changing the host name could be necessary in specific use cases where software is expecting a very specific name so I found that when I was working with kubernetes um and I was sending have service messages this is where I had to start fiddling with host names but for the most part you probably don't have to touch it especially if you're a developer uh on the networking side they know they know what's best and and they might fiddle with these things a lot more uh there are two host name types for the guest OS um host name uh for ec2 in particular or we should say AWS and it's going to be dependent on whether you're in Us East one or other regions why it's different in this matter I have no idea why but it is and the first is IP name so this is the Legacy name ging scheme uh scheme based on the private IP address it says Legacy but I don't think it's that bad to use and most people are using ipv4 so if you launch an instance with only ipv4 it's going to be using this format so if you're new a one uh you'll notice that it will have the IP address going to get the pen out here so it has the uh private IP address then2 and internal okay other the regions for some weird reason the region is now included which is not that weird but it's the fact that they Chang this word to compute why I have no idea um the other one is resource name so this is the other type of host name format and this is if if you're going to be using IPv6 um and you can use dual stack if sorry if you if you choose to have an instance that has both ipv4 IPv6 then you can choose between which one you want IP name and resource names but if you only have IPv6 you're going to be using resource names if you only have ipv4 you're going to be using IP names and so here it's using the instance uh ID as the identifier and it's following the same format where it has um here you can see the uh region and then here it says compute so there you go the default username of an OS managed by ads will vary based on its OS distribution why do we care about the default username well often you'll have to use sessions man manager to get into your instance and if you were to check what user you are you're always the SSM user but you have to switch out um to the default user that's where you're supposed to do stuff um and so you need to know what that is if you want to use SSH you absolutely need to know what the default user is so let's take a look at what it is and I'm going to tell you if you just remember ec2 User it's going to make your life easy in most cases but for all the Amazon Linux distributions 1 2 and 23 they're all2 user for the OS the sentos OS Ami which is managed bys it's going to be E2 user other ones might be Cent OS if you're using Debian it's going to be admin if you're using Fedora it's ec2 user if you're using Red Hat Linux it could be E2 user or root I feel like if it's managed by a it's always going to be E2 user and if it's uh Marketplace probably the the normal default which is the admin or root or or the name of the distribution for Susi we have ec2 user or root for Ubuntu we have Ubuntu for Oracle uh we have ec2 user for bitnami uh and those aren't necessarily managed bys but bitnami always uses bitnami so you know like the ones I remember is always Telly You2 user if you're using iunu it's auntu if you're using bitnami it's bami and that's that's as simple as it gets okay but otherwise check with the am provider because there can be variation hey this is Andrew Brown from exampro and we are looking at Amazon machine images Amis which is a template to configure new instances so an Ami provides information required to launch an instance so you can turn your ec2 instances into Amis so that in turn you can create copies of your servers okay and so an Ami holds the following it's going to have a template for the root volume for the instance so it's either going to be an EBS snapshot or an instance store template uh and that is going to contain your operating system your application server your applications everything that actually makes up what you want your AI Ami to be uh then you have launch permissions the controls which uh AWS accounts can use uh for the Ami to launch instances then you have block device mapping uh that specifies volumes to attach to the instances when it's launched all right and so now I just have that physical representation over here so you have your EBS snapshot which is registered to an Ami and then you can launch that Ami or make a copy of an Ami to make another Ami okay and Amis are are region specific and we're going to get into that shortly here so I just wanted to talk about the use cases of Ami and this is how I utilize it so Amis help you keep incremental changes to your OS application code and system packages all right so let's say you have a web application and or web server and you create an Ami on it and it's going to have some uh things you've already installed on it but let's say you had to come back and install reddis because you wanted to run something like sidekick or now you need to install image magic uh for image processing or you need the cloud watch agent because you wanted to stream logs from your ec2 instance to cloudwatch and that's where you're going to be creating those revisions okay and that's just going to be based on the names um Ami is generally uh utilized with uh systems manager automation so this is a service which will uh routinely patch your Amis with security updates and then you can bake those Ami so then you quickly launch them uh which ties into launch configurations so when you're dealing with auto scaling groups those use launch configurations and launch configurations have to have an Ami so when you attach an Ami to launch configuration and you update the long configuration in your autoscaling group it's going to roll out those updates to all those multiple instances so just to give you like a a bigger picture of how Ami tie into the um AWS ecosystem so I just quickly wanted to show you the ABS Marketplace and so again the marketplace lets you purchase subscriptions to vendor maintain Amis there can also be free ones in here as well but generally they are for paid and they come in additional cost on top of your uc2 instance so here if you wanted to use Microsoft's uh deep learning Ami you could and you'd have to pay whatever that is uh per hour um but generally people are purchasing from the marketplace security hardened Amis they're very popular so let's say you had to run um Amazon Linux and you wanted it to meet the requirements of level one CIS well there it is in the marketplace and it only costs you 02 I guess that's 2 cents per hour or $130 uh $130 per year uh yeah and so just wanted to highlight that so we're going to look at how we would go about choosing our Ami and so AWS has hundreds of Amis you can search and select from and so they have something called the community Ami which are free Amis maintained by the community and then we also have the adus marketplace which are free or PID Amis maintained by vendors uh and so here in front of me um this is where you would actually go select an Ami but I wanted to show you something that's interesting because you can have an Ami say Amazon alytic 2 and if you were to look at it in North Virginia and compare to it in another region such as Canada Central you're going to notice that there's going to be some variation there and that's because Amis even though they are the same they they are different to meet the needs of that region and so so um you know you can see here for Amazon 2 North Virginia we can launch it in x86 or arm but only in Canada Central is it 64 bit all right so uh the way we can tell that these Amis are unique is that they have Ami IDs so they're not one one uh and so Amis are region specific and so they will have different Ami IDs per region uh and you're not going to just be able to take it uh an ID from another region and launch it from another region there is some some things you have to do to uh get an Ami to another region and we will talk about that but the most important thing here I just want to show you is that we do have hundreds of Amis to choose from and uh there is some variation between regions so with choosing Ami we do have a lot of options open to us to filter down what it is that we're looking for and so you can see that we could choose based on our OS whether the root device type is EBS or instance store whether it's for all regions or current regions or maybe the architecture so we do have a bunch of filtration options available to us Amis are categorized as either backed by EBS or backed by instant store this is a very important uh option here and you're going to notice that um in the bottom left corner on there I just wanted to highlight because it is something that's very important so when you're creating Ami you can create an Ami from an existing ec2 instance it's either running or stopped okay and all you have to do to create an Ami is drop down your a action go to image and create image and that's all there is to it so you can also make copies of your Ami and this feature is also really important when we're talking about Amis because they are region specific so the only way you can get an Ami from one region to another is you have to use the copy command so you do copy Ami and then you'd have the ability to choose to send that Ami to another region so there we are hey this is Andre Brown and we are taking a look at Amazon machine image also known as Ami and that is how we are going to say it throughout the rest of this course because that's what everybody calls it um so Ami provides the information required to launch your instance and you can turn those ec2 instances into Ami so you can create copies of your server so here I have a nice little diagram let's talk about what is being held within this uh Ami so we have a template for the root volume volume of the instance this could be uh EBS snapshot or the instance store template uh we're going to have things like operating system application server or applications itself in the diagram here you can see here on the left hand side we're calling out that EBS uh volume but understand that uh that stuff uh might be contained within that volume launch missions that control which adus accounts can use the Ami to launch instances a block device mapping that specifies the volumes to attach to the instance when it's launched um and the most important thing I want you to know is that Amis are region specific this is super important to remember because if you if you copy an Ami ID uh from a different region and try to launch it as an instance it's just not going to work let's talk about some use cases so Amis help you keep incremental changes to your OS application code and system packages obviously makes U launching um ec2 instances virtual Machines of the same kind of uh template uh very easy so here I have an example have a bunch of Amis and they're just Nam uh increment incrementally uh so they're counting up you can use systems manager automation so you can routinely patch your Amis with security updates and bake those Amis baking is a uh a process uh when you automate the creation of Amis when you or or just just images images of virtual machines when you are making changes Amis are used within launch configurations and launch templates to manage Ami revisions launch configurations is the um old way and launch templates is the new way uh I tend to use both when they're available but they're very similar but they uh have a slight difference there something that's really tightly coupled with Amis is the Abus Marketplace because most of the things in the Abus Marketplace especially for uc2 are Ami images and so you could go there and you could get a subscription to something like Nvidia GPU optimized Ami directly from Nvidia you could go there and get a security hardened Ami uh from the center of Internet Security so a lot of cool uh Amis that you can leverage right away some are free some are paid uh then you need to choose your Ami so it's important to know a couple things when about your Amis and the first thing I want to show you is um the Ami is different per region I already mentioned this but look at this one here so this is the same Amazon Linux 2023 Ami just in two different regions so in the top we have for us East one and for the bottom we have CA Central 1 I'm going to bring out my pen tool cuz I want to make is very clear uh look at the end numbers 29d E5 C they're not the same some Amis will have multiple architecture options with alternate Ami IDs so if you wanted to use Arm instead of x86 you could do so but notice that it has a completely different Mi ID uh you can search based on a bunch of uh uh parameters for your Amis there's region of course you'll switch to the the region you need the OS the architecture the launch permissions uh the state of or like what kind of root device volume that you're utilizing and you can see those options there I say root device volume say root device type means the same thing here but we have some options here to quickly filter and find what we're looking for let's talk about boot modes Ami has two different boot modes the first is the Legacy BIOS basic input output system this is the traditional firmware interface for computers which has been been used for decades and initiates Hardware during the boot up process and providing a runtime service for OS and programs if you've ever had a Windows machine and you had to press like F2 to get into some boot screen to make some changes that's what we're talking about right here not that I've ever had access to this on ads but we're just talking about what kind of boot boot systems there are on these uh ec2 instances and if you have the Legacy BIOS this would not have support for secure boot um which you really want secure be that's when you have a secure Hardware chip to make sure there's no tampering um uh with with the uh the machine that it may require in some cases the reason you might want to use Legacy BIOS is that um you might be using a legacy operating system or Legacy software for the most part you're going to be wanting to use the unified extension extensible extension extensible firmware interface UE Fi not used to saying the full initialism uh usually just UEFI this is a modern firmware interface for computers designed to replace the older bias firm firmware interface that supports secure but faster startup times supports drives larger than two terabytes preboot environments with graphical UI with some capabil capabilities the only reason you use Legacy BIOS is for legacy reasons but for the most part you want to utilize UEI UEFI and when you are launching your Amis you can see which boot system it is using there is a third option which I don't display here which says UEFI preferred so when it can it will use ufi but generally most are going to be UEFI you'd be uh hard pressed to find the BIOS one when you're launching your Amis you'd have to search for it to be honest to find it there's a thing called elastic network adapter this is part of the enhanced networking which there's a lot to talk about there but I just want to talk about this particular enhanced networking thing so the elastic Network adap supports Network speeds of up to 100 GB per second or gigabits per second for supported instance types and you can see that some are enabled there so the Amazon 2023 Mii is enn enabled most of them seem to be en enn enabled these days all instances based on the Nitro system used Ena for enhanced networking so the Nitro system has um a bunch of things one of it is uh the Nitro hypervisor and Nitro cards but there are specific instance types that you can launch like T3 micro that utilizes the Nitro system uh and so those ones are going to be able to take full advantage of Ena support so just because you have Ena turned on it really still depends on what instance type you choose underneath you have the root device type or the volume and we have one that is backed by EBS or one that is backed by instance store the key difference here is that one that is backed by EBS um it will actually have uh independent storage for from the actual instance and if it's independent from it and you terminate or stop this instance the storage can remain and so you will be able to persist your storage beyond your instance you could even attach it to a new instance or or whatever you want to do with it make copies of it um and so most of these are EBS backed there are cases where there are instance stored backed and this is where you are sharing um the uh the storage whatever the actual Hardware storage with the actual instance is underneath and the thing is is that if you uh stop or terminate your instance you're going to lose that data so pretty straightforward um there are some things we should know how to do with AMI so the first is create an Ami you can do this on either a running or stopped instance you can even reboot it before you do it so that it's just um in better shape so a lot of options here you can copy an Ami and often you have to copy Amis because it's the only way you can get them to another region and that could be exam question by the way uh so you know that is something to consider also if you have a Ami that is not encrypted the way that you encrypted is make a copy of it and then put in that encrypted option okay so that is another thing to know so besides just copying image there's store and restore which seems very similar to copy but has a different use case so this is where you're storing the Ami in the S3 bucket and you can restore it from an S3 bucket this is when you want to copy Amis from One ads partition to another so we have the store command and then the restore command uh I've never had to do this but the reason why you would do this over copy is that it you could have potential savings uh with cheaper longterm storage uh life cycle management of Amis Disaster Recovery uh auditing compliance but to be honest like adab us manages thei so the chance of you losing those things is very unlikely but if you just need to have that for compliance reasons you can put it into an S3 bucket wherever you want somewhere else then there's a couple other things we can do we can deprecate Amis the idea here is that if you deprecate it um or sorry deregister first I'm reading at the top here but if we are deregistering it basically you are saying you're done with this um Mi it's kind of like when you release an elastic IP it's basically the way of deleting Ami ons just don't call it delete because you're not actually deleting it you're deregistering it and so the idea is that when you try to launch for future instances they will not launch however if you deregister which is essentially the delete for Amis it doesn't delete snapshots so if you need to do that and get rid of those snapshots you're going have to delete those manually you can deprecate Amis this is when you have an Ami and you want people to stop using it after a particular date um very important if you are let's say sharing that Ami externally or selling in the marketplace you can disable Amis so this just prevents people from uh launching Ami you could obviously reenable them but it's kind of like a soft delete um if you will that you can always undo later on you can share Amis and there are three different uh settings for sharing public explicit implicit implicit is the default that's basically you have the Ami explicit is when you want to share with a very specific it was account organization or OU organizational unit public is basically anyone can share it that's where you're putting it into the community uh there are some options you have to configure before you can do that but those are the three uh and that's where we're basically changing the launch permissions So when you say launch permissions group all you're basically saying this is public now this would be a bit different for the the ones where you specify a specific Aus account or otherwise Amis can be sold in Marketplace so um that is something that is cool I'm not covering that in this course but uh you know it's something you could do as a business model model virtualization types matter because when you have an Ami it's going to either be HMV or hvm sorry or PV so Hardware virtual instance or par virtualization it's all in the name what it does virt uh so for hvm this is a full virtualization with Hardware assistance pair of virtualization is software assisted virtualization requiring OS modification uh the hardware assisted part here is that hvm does utilize Hardware assistant technology so it's it's utilizing the actual Hardware whereas par virtualization is simulating the hardware uh for performance you're going to get better performance with hvm and with PV not so much there used to be a case where PV could be better but now uh hm is always more performant you have more support for uh for os's because you can utilize the hardware whereas par virtual par virtualization you were limited the boot method here is it can be it can boot from an EBS volume or store and Par virtualization can only boot from an instant store for usage uh it's recommended for modern OS applications requiring specific Hardware features and Par virtualization is historically used for certain workloads and older instance types less commonly used for new deployments you're basically never going to see uh virtualization types because you basically never ever ever ever need to uh choose PV because ads is basically moving everything over at H hbm but in the case that you do see it I wanted you to know about it and that's all I need you to know about Amis there you go hey this is Andrew Brown and there's a few things I want to cover here um the first thing with Amis is that I want to show you how to take an Ami that is not encrypted and encrypt it the other one is copying an Ami to another region as those are the two major things that adus likes you to know um for the exams basically uh and that's usually the two edge cases for AMI so what we'll do is make our way over to ec2 I'm going to go ahead and launch a new instance because we're going to create an Ami from an existing uh ec2 instance I actually go over to our um GitHub repo and I figure we could just launch this with an Apachi server so I'm going to go make my over to into vpcs and knackles that one always seems to be a really good good one for us here and down below we just have this A bash template which is what I want so I'm just going to say Apachi Ser Apachi to server and I'm going to go down here to T2 micro which is fine actually I'm going to choose a T3 micro I don't know I just feel like I I want to utilize that today and um let's actually take a look at the Ami so if we go here and browse we can see all of our Amis and notice here that we have our free tier ones here on the left hand side make note of the Ami IDs here because often you have to grab these programmatically and uh that's something that we will need to do at some point here but we have this one here which is UEFI preferred meaning that if it can it will always use UEFI but there's a chance that it could use um it could end up using uh the Legacy BIOS I think that's really dependent on what instance type you choose then over here notice we have arm so if we just switch over here it's going to end up using this one here in fact let's use arm today because arm is very efficient and great notice that it shows us the root device type this is going to be uh EBS that means it's going to have elastic Block store storage attached to it this is using hvm it's very hard to see something that's not using hvm these days and it Hasa enabled we could just go over here for a second and go to the Abus Marketplace there's a lot of cool things here I think I showed in the slides about security and one that's uh very popular is CIS so if you go here and type in CIS we can see we have um the CSI Benchmark there's two different levels I think level two is um more secure but if we type in 2023 we could find that one there but notice this costs money so if we select this it'll tell us the product details how we use it and stuff like that sometimes these things are free and then you pay a subscription that is you bring in a key so you're not necessarily paying through Amazon you might pay through the product another way but generally uh they'll have a cost there and you can factor that in but it'll show you um should show you the monthly cost here at least the used to pricing not the best but uh it might be on the market page so these also have an external page that you can do that and basically that is something you can do there then there are Community Amis and this is basically if you make a an Ami public anyone can utilize it so we have a bunch of architectures here and root device types and stuff so if we want to find something with instant story we could checkbox that on and we we could find them if you wanted something with genu you could do that as well uh definitely not going to be with instant store but we can uncheck that and so that is our options there but I'm going to go back and cancel this because I want to stick with the Amazon Linux 2023 Ami image um so we have this is a T3 micro so this should be fine and we'll go down here and the thing is is that normally your EBS volumes I'm not sure if they're encrypted by default now so at one point they were not encrypted by default let me just take a look here EBS optimize image let me just find it here all right sorry here it is and the reason I'm confused is because this UI used to look way different and I generally do everything programmatically but um anyway over here if we go to advance it might show us somewhere here the encryption option here is here it is instead so basically it's we can encrypt this but I'm going to leave it as not encrypted because I want to uh show that use Case by copying Ami so what we'll do is go down below here and we'll just grab our script we don't really need a script but I'm going to put it in here anyway and I just want to bring this all back to the wall I wish there was a I was trying to De indent it but it wasn't working I was trying to do a tab space or shift tab to to do that but anyway so if we go up here that looks good great we'll go ahead and launch this instance um I'm not going to launch it with a keypad today I do not care and we'll launch that instance and I'll be back here in a moment when it's finished launching okay all right so our Apache server is ready I just want to show you how easy it is to uh create an instance or sorry an Ami and it's somewhere here image and template so notice we can create an image right here and if I click it we'll have some options so we can name our image give it a description whether we want to reboot it or not um the instance volumes whether we want to encrypt it after the fact um not sure why we can't uh checkbox that there but um I that's fine but anyway what we're going to do is we're going to go and utilize the actual command because I think that's a better way of uh learning how to utilize this and you know one thing I want to do is I want to uh create our em Ami first so we'll say ads I guess you you'd encrypt it when you copy it right you couldn't create and encrypt at the same time so that kind of Mak sense we'll go ahead and type in ads ec2 create image instance ID we'll go ahead and grab the instance ID here and we'll paste that in there we'll type in name we'll say my am I 000000 just so we know which one it is I'm go ahead and hit enter and that's going to go ahead and create that Ami while we're waiting let's go take a look at Amis here on the left hand side I gu I have a catalog now I assume that's just okay it's just the marketplace and so we're waiting for this to be complete so we'll just wait a little while they're okay all right so after a little wait here we'll give a refresh here and we can see that it's now available what I'm going to do is go ahead and um I mean look at the look at the Ami because we should know a little bit about it before we proceed here so we have image type as machine I'm not sure what else it would be but I guess it could be something else you can see the boot mode here is ufi preferred because we understand what the underlying thing in is here we could edit our permissions here if we really wanted to to give access to Shared accounts or other OU or other idless accounts uh for the storage we can see that it's not encrypted so that's something that we're going to want to resolve so what I want to do is I want to copy this Ami uh into another region I'm right right now in North Virginia and so I want to bring this over into CA Central 1 so what I'll do here is I'll go ahead and type in a ec2 copy image and I'm going to go ahead and type in source region so this is from us East one and we'll type in the source image ID which is this ID here okay and then we'll type in the name we'll say my copied Ami and I'm going to switch the region to ca Central 1 and the other thing I'm going to do is I'm also going to encrypt EBS in crypted we'll hit enter hopefully I typed all that right there we go and so now what I'll do is make my way over to ca Central 1 and that should now be creating we'll just wait a moment okay all right so let's take a look here give this a refresh and so this is now available and so we should have successfully copy this to another region if we check our storage it is now encrypted so that's pretty much all I really wanted to show you um there is more to do with Amis when we're talking about launch types or launch templates and launch configurations which I'm not seeing it here anymore is launch configurations gone now launch configurations is it gone the reason why I'm not like I'm not sure is because like I like I I've been using launch templates for such a long time at one point I only used launch configurations and so I'm looking at this and going where the heck did they go or they just simply gone now did eight of us get rid of launch configurations because there's not really much reason to use them anymore starting 2023 launch configurations do not support any new Amazon dc2 instance type so I guess they're gone well that's all right but uh yeah I I had even noticed that they were gone but anyway let's go ahead and uh get rid of this Ami so we'll deregister it there we go um and just remember that uh those snapshots might not vanish so we'll have to double check here in a moment snapshots do eat up space and they will cost you money so make sure you delete them uh we're going to make our way over to Ami over here by going to ec2 and we'll go to Ami and we'll go ahead and well we got to switch regions I guess and we'll deregister this one as well and I'm going to make my way over to snapshots here as I do want to um get rid of those but I need to wait for that Ami to deregister so oh deregister that fast well that's great and so you can see I have a bunch of snapshots here this is all junk I'm going go ahead and just select all of these if I can get rid of them recycle bin what's that protect your EBS snapshots at Amazon Amis from accidental deletion okay pretty straightforward didn't know they had that now but whatever we'll go ahead and delete it there's always something new we'll go ahead and delete that as well there we go and I will see you in the next one okay ciao E2 image Builder is a fully managed service to automate continuous builds of containers or virtual machine images so here on the left hand side we have a representation of the image pipeline uh which is what you need to create when you create uh image Builder and inside of this we have to Define particular components the first is the image recipe or the container recipe this is the base image or comp uh and components and custom scripts to configure the image so we talk about the base image we're saying something like the docker base image just think of like when you create a Docker file you're saying pull pull from this base um like in the from command or when we're talking about uh Amis we're using a base Ami components are these reusable U modular code that you can Define in yaml and then you have your custom scripts or your Docker file depends on if you're building an image a virtual machine image or a container you have your image workflow this defines either the build or test uh steps written as a yaml file I say build steps because it's on the build server but it's build steps or test steps okay we have infrastructure config this defines the underlying compute that will build the image so what size of instance type are we going to use whether it will be launched in a um like uh which particular uh VPC and subnet and Security Group things like that uh we have the distribution settings so this defines where to publish the outputed image um so whether it'll go to ECR or if it will go to um a particular um Ami in a particular region and account you have a scheduler so you define if like when it will trigger on it on a schedule so you have a Chron expression the schedule Builder which is just a simplified version of a Chron expression or manual you could also trigger it via event bridge I didn't write that in there but you should know you can programmatically trigger it that way you can manage the life cycle of your images using E2 image Builder image life cycle um so you know you can say after a particular criteria that it should delete these images because you don't want uh uh accumulate too many images E2 image Builder is free to use the underly compute to build the image and the images stored have their own cost okay an image workflow defines the sequence now we're talking about image workflow the sequence of steps that the ec2 image Builder performs during the build and test stages of the image creation process so here's an example of that workflow okay and so if you notice on the left hand side I'm just going to get the pen tool out here so we can look closer notice it says launch the build instance uh it's waiting for the SSM agent it's going to apply the build components then it's going to uh do inventory collection and then run run a sanitized script so you know the default ones that come are really good you don't really need to write your own but if you have a more complex setup you can absolutely do that you have either a build stage or a test stage this one is for specifically um uh the build stage this is build image up here so the name indicates that it's for for that there the build stage uh makes changes to the ec2 build instance that are uh that's running your base image and then the test stage runs the test Suite on the E2 build instance against your base image so hopefully that makes sense there we have infrastructure configuration so this defines you know the actual compute that will run the base image such as the instance type the instance profile en roll the VPC subnets security groups logging key pair for SSH termination uh termination protection they supposed to say protection not project so we're talking about this over here SS SNS topic for notifications the default instance type when not set is an M5 large and in the video I was confused because I did the labs before I did the slides here cuz I wanted to make sure that um I I had it the information accurate but when ec2 image Builder first came out you couldn't change the instance type all there was was an M5 large and so when I was using the uh the service um I couldn't find an option to change it like not right away but as I search the docs and you see me do this it wouldn't tell me like where I could change it but obviously you can change it and I figured that out eventually so I understand that it runs an M5 large bike by default and if you're worried about that you need to change the infrastructure configuration because I don't do this in the lab immediately I do it in like the second lab in the container lab so if you're worried about that cost of the M5 large you can switch it down uh to something smaller or you can just watch the video okay um but yeah you likely want to change that value let's talk about distribution settings this is where uh it will um if we're talking about Ami in particular it's where to Output the Ami image such as account and region whether it has sharing permissions license in configuration uh creating a launch template or faster launch configuration uh applying Ami tags on the image which I see that is spelled wrong I apologize for that um then if it's for building out a container image I don't know why I didn't write the word container image on here container image okay uh this is where we'll go in ECR uh apply container tags to The Container image let's talk about components so components allow us to create modular actions that will be performed during the build and verify or test uh or the test phase of our build and so here are all the module module actions that you have I have a sneaking suspicion these are the exact same ones that SSM run command allows you to use and this might be the exact same syntax as SSM run command so I didn't check but I just kind of remember doing that and when you before you see to image Builder the way you'd actually have to build images or containers is using SSM run commands and automating that so I imagine that's what's happening here but here's an example of downloading a file we actually use this in the lab for the container one uh because uh we need to do that so I didn't make a separate video on on components I just did it in the second containers I just want to warn you about the labs that the second one where we do containers we don't actually succeed uh completely in our goal to deploy the app uh but we do a lot of debugging there and I wanted to keep that video there even though it was a failure uh technical failure in the end we do end up building a container but not the one that I wanted to but we do a lot of debugging I think that gets you really deep knowledge and so I wanted to leave it in as is uh so I just wanted to warn you there uh but yeah uh there you go hey this is Angie Brown in this video what I want to do is take a look at ec2 image Builder so I have used it in the past and I compared it to other uh other tools like um Packer which is hashy corpse uh implementation and some other uh uh things that can build up images um and I can't remember why I did not like using this it maybe it was like really expensive or something uh like it had a default ec2 instance but we'll take a look here and see what we have um it can build out images and containers um and it has a lot of functionality here but let's go ahead and see if we can just do a basic example um so I'm just going here blind and we'll just pick it up as we go here so let's type our pipeline let say my basic Pipeline and we'll just say my basic pipeline it looks like they're both required here uh do we want enhanced metadata collection for AMI only we'll leave that alone uh we'll just leave it checkbox which is totally fine uh we'll go down below and we have a schedule Builder a Chron expression or manual so this is going to determine how often we want to rebuild our images um the idea here is that if you have uh an image and you're rebuilding it it's going to run the commands to build it every single time to install packages and things like that and so if there is updated packages you're going to always have the latest patched um or updated uh packages and that's why you'd build uh daily I'm going to go to manual because I don't want to build on a schedule here today and we have a few options so we can use an existing recipe or create a new one I'm going to create a new one and so we have Ami or Docker image I think for this round let's do Ami I just call this my basic Ami and we'll choose 1.0.0 to uh add our versioning before this I used to have to create Amis and then have to remember to name them and you'd have to set up a pipeline through uh systems manager to automate that so this is a nice tool but again you'd save more money if you probably utilize this because I feel like this has an additional cost or a fixed cost that we have to deal with here select image so here we can choose our base image um Amazon link seems fine so we'll choose that and then we have the quick start Amazon managed image so choose the image to configure from a list of previously created uh pipeline images so that seems fine to me we have a bunch here I would like to go with with um 2023 arm 64 use latest available OS that sounds like a good idea to me so that idea is every time it deploys it will pull from whatever uh the latest one is for ads ECU image Builder have the SSM agent so remove the S agent after the pipeline execution um no I I want it because I like to log in with SSM and then we have our user data so when you provide it this is what it will uh install into the base image so that's a great place where we could install something uh we just deployed an ec2 instance in it was ASG and it had some basic template actually if we go down to our ec2 folder here we have some user data and so I can just go ahead and grab uh this script and so this thing will install Apachi so I'm going to go ahead and grab this and paste it in here and the user data is already base 64 that's not true we'll leave that alone and we'll go down below here it has a working directory which is fine and then we have some components so this is really cool we can get really set up with a bunch of stuff so let's say we want on cetto to do some Java um 11 seems fine to me but there's 17 so there's even a newer version so 11's really old well let's go with 17 I'm just trying to take a look here which one do we want headless oh there's even newer oh it's just not ordered in a way that is alphabetical that's that's really interesting so 21 seems like the latest I'm just trying to think of something that we can add that' be interesting um JRE jdk what's the difference uh JRE versus jdk versus headless so I honestly don't know um which is display which will display the device keyboard mouse is lacking sound unexpected uh okay so then what's jdk versus J JDR so jdk versus JRE people that know Java are like come on Andrew you don't know this jdk is used for develop Java where JRE is used to run the Java applications so I'm going to go with JRE I suppose or maybe jdk no I'm go jdk not that it really matters but I'm just trying to think of things that we can install they probably have more tabs here so there's probably a lot of interesting stuff that we could uh Place into here we have like a hello world Linux so we could have uh deployed that if we want to we could install go if we want to uh it's really cool that they have all these packages I don't remember these being here before wonder if they have one for Ruby let's take take look here is there a Ruby one no but they have python come on AWS so we'll install Java and python onto here and then we have other additional components uh select test to verify the uh output of the Ami so there is a few tests here test whether the system can reboot successfully that sounds like a good idea so I like that um so that is fine we have our our storage we'll go ahead and hit next and then we have default workflows custom workflows let's take a look here see we have for custom workflows um so I imagine that this is where we could add additional build steps choose a build workflow say image and let's go take a look here see I'm not 100% sure what this is right now I simply cannot remember but I'm going to go back to the default workflows and say I'm just not 100% confident with that so we'll go ahead and hit next and then we have Define infrastructure configuration so create infrastructure configuration using service defaults uh so we can change some things down here I'm going to leave this back to default okay so that seems fine we'll go ahead and hit next and this seems fine so we have a bunch of options here that's is next we'll go ahead and hit review so what I was looking for through that that whole process was where would I um actually say hey I want to install these things or run these things because usually when you have an image Builder you're specifying that path there and I just don't seem to remember that but we'll go over here we'll take a look and I guess what we could do is we go ahead and run this Pipeline and so the idea is that this should now build out our our image um and the other thing is that this is going to use some kind of of Base a comput let's go take a look at pricing ac2 uh image Builder pricing and if we go down below it'll probably tell us something about cost somewhere here there is offered at no cost other than the cost of underlying resources I feel like that's not true because it uses some level of Base image that it has to do to run it so it's not telling us the full truth there and I'm not exactly sure what the cost is I'm not really concerned about it because it should build pretty quickly but it is a little bit annoying I'm going to dig a little bit deeper just give me a moment okay so yeah I still don't know and when the service first launched you couldn't change its compute and so what I'm thinking is that somewhere in here it probably would tell us in our image pipeline we click into this the compute that we use distribution settings infrastructure configuration maybe that's what it's talking about here because we're not specifying instance type um but maybe this is actually for what it will actually use for the for all these other settings because when you have the distribution settings you can say I wanted this to be a launch template and I would say it should have an instance type right so yeah I'm not 100% certain on that point but for the time being let's go take a look and see if our Ami built and so here we have our Ami did it already build oh it's still building okay so we're going to have to wait for that Ami to build but again I'm looking for the cost of the underlying compute I just kind of feel like there's a hidden cost so we'll let that build and I'll just do some research okay all right so I'm Blown Away with this because it really seems like there's no cost to build images and I I don't understand why because it definitely had a cost before so if it's free that is great um I'm still skeptical but that's fine because I used to have to debate whether I wanted to use ec2 image Builder and SSM to build images so we go over here for just a moment we go over to SSM and um in here what you could do is you could create a bunch of run commands and then you could automate uh the deployment of that on a schedule so there's like a scheder somewhere in here um and so that's how I would normally do it or or every time you push changes to the repo it would rebuild and then go through your run commands and I I have like a big repo where I've done that but if this takes away a lot of that work that seems really uh good to me um so that's really interesting okay and then here we can see the components what I'd be interested is like can we create our own components so if we go here oh we can all right is this just using uh run command language uh component is defined by a gamble document phases steps so this seems like this is probably run commands uses the adabs so action modules what is that inabus task orchestrator and execut executor so I don't know why but I have a strong feeling this has some strong relationship with the with what we would do in systems manager and we would create run commands over here cuz I see steps and phases and I I kind of recall that being similar um so if you go down below here again I don't know this is we'll have to find this out later on but there's documents I believe I'm just trying to find them yeah and so you have documents here and the idea here is that if you go into them they'll have things like steps and stuff like this or it might be its own language I don't know at this point but it'd be interesting to create our own components um because I think that would be really cool if we could Define some there so I might do that in a separate video and find out a little bit more about that but let's go back to our images and wait for that to build um so yeah we'll just wait for that okay all right so it's still building it's going really slow but maybe that has to do with why it's free now because I remember before again it utilized a compute behind it that was expensive but uh if we go over here we can actually see the steps that it's following so you'll see that it launches the the build image applies the build components inventories the collections run sanitize script create the Mi output and so um it obviously skipped that one so now it's just creating the uh the output Ami and we're going to have to wait here now we did tell it to launch and restart so there is a testing process here and so we'd have to wait that entire time for that ec2 instance to spin up um so maybe if we got rid of that test it would be a lot faster um but you know it is going so we'll just wait here and keep refreshing and uh see when it's done okay all right imagine this must be done even though uh it doesn't show it's done I'm going to give this a hard refresh and we'll take a look here and it is now available so our image is here let's go take a look and see if we can launch it so I'm going to go to our image uh can I launch it uh no not from here but we'll go over to ec2 and we'll go to Amis or the Ami catalog am am I sorry and we do have an image here is this the image we created though and that's what I'm not 100% certain on this is saying this says Amazon 2 so it's not exactly what I want I'm going to go over here for a moment and down below here we have the amid says 0 ae9 so if we go back over to this one 003 so I'm going to just deselect that one and give this a refresh now we're seeing all of our Amis and I actually have a bunch in here um but it's actually showing all of them so these are all the public images and I just want owned by me or private images so this is the only one I have in here and it's definitely not the one that I was thinking that I wanted to utilize so what's going on here that's what I want to know so if I click through here now I get my Ami but why wouldn't it show up over here oh it does okay the name's right here all right so I'm just not paying attention says my basic Ami my basic Ami and you're probably looking at Andrew like hey why aren't you reading it's right there in the Ami name so there it is we'll go ahead and launch it I just want to confirm it actually does what we think it does so just say my uh ec2 Builder Ami image and we'll go down below and so that looks good t4g Nano I mean this does have Java on it so I'm going to go a little bit larger if you don't want to you totally don't have to I'm going to go down below to Advanced details and I want to add in uh our SSM R we create that somewhere else so it's just the basic SSM rle to get access if you don't know what it is go look it up we do in our ec2 section so many times um over here I want to have a allow HTP and htps traffic so that we can see our website um and that seems good so we'll go ahead and launch that instance and I'll proceed without a key pair we don't need one because we have the SSM Rule and we'll wait for that to launch all right so be back here when this is ready okay all right so our instance is uh ready let's go ahead and see if we can connect to it and the other part of it is I want to go and see if we have a public IP address and if we do do we have our Apachi server that we configured so I'm not seeing anything there which is uh not necessarily a bad thing but we'll go ahead and type in pseudo Su ec2 user and there are a couple of things we installed like python so is python here uh python I don't see it so that's not the best indicator that this was successful um do we have Java at least uh seems like that's running Java so that is there and that is the version we installed maybe if we installed Python 3 so Python 3 is there excellent um but what about our web server why didn't that work um so we'll go back over to our script cuz I could swore we told it to utilize that on the configuration step to install it so we'll go back into our Ami here whoops not over here sorry I want to go over into our image recipes and click into here as we did specify this user data script right so instance configuration um so unless there's something wrong with this uh that could be our indicator so we do that we can use pseudo system STL to find out so we'll say pseudo system uh CTL sorry it's not STL and if I go over to here we have start so I'm going to do status on this status httpd and it is running okay so there is an Apachi server there um let's do curl Local Host see what we get okay so it is installed and so that means that it's probably something with our security group then so if we go back over to here and we go to security and we look at our ports Port 80 is open so probably what it is is that we just have to take the S out of there there we go so our image works and that is great um and I'm just going to go ahead and terminate this instance I'm going to keep around our image because I want to do more things with it uh so that is something that we will do um I'm going to keep the Ami around if you're worried about cost you delete the Ami but everything else is not a big deal like you could just build the pipeline again and have the that Ami there again but uh again I just want to keep utilizing ec2 image Builder because it seems to have a lot more capabilities since the last time I used it and I think that that seems like a very useful service to learn more about okay um so yeah I deleted that instance and I'll see you in the next one okay ciao hey this is Andrew Brown in this video I want to go ahead and build a container image uh for uh Docker so what we're going to do here is go type in um Docker container image pipeline whatever you want to call it I'm just copy and paste that twice and we'll go down below I'm going to go to manual because I don't want to run this on a schedule we're just learning we're going to create a new recipe we're going to choose Docker here I'm just call this my Docker image we'll call this version 1.0.0 we'll go down below and it's asking for us to select a base image um so that's interesting I'm not exactly sure why we need to choose the OS let me just read about that for a moment okay that makes sense and the idea is that uh here we can choose a managed images from AWS we can choose our own ECR image but I would imagine that we have to make sure that whatever uh we choose there is compatible so I can't imagine we can just use whatever um ECR image we want so I'm going to go back to selected managed images here I'm going to go and um choose the latest so 2023 latest and we have a quick start here we have use latest OS version which is fine we'll go down below I kind of want to use a buntu because it's always easier to use a buntu so you know what I'm changing my mind I'm using a buntu here today I just want my life to be easy and so we'll do quick images here and I want to choose something that is the latest so we'll do jammie because jamm is probably the latest right now so x86 Jammy use the latest OS version sounds good it's going to produce our Ami ID um can I rename this to be like ubun to oh doesn't like it so we'll leave it alone I suppose and we'll I guess we'll not muddle with it um but we'll go down below we'll scroll on down and so we have some things we can install so we can install the Amazon cloudwatch agent which is uh not a bad idea uh maybe we want to install python because often um we are doing python we actually do have a python app so if there was python in here that wouldn't be a bad idea do we even have that as an option if we don't it's not a big deal I don't think it's that hard to install python we have go here which is fine um we go next here so nothing super exciting we'll go down below and so we have that one component installed and now it says select uh and verify stuff I don't think I want to do that this time and so now we're here and we have our example so the idea is that it's going to populate this information into our template and then the idea is that we can Define stuff afterwards here so what I want to do is I want to go over to our code base and we have stuff in here I'm going to make a new folder here we just call mkd ec2 Image Builder and I'm going to CD into that I'm going to make a new directory here called uh container or Docker and in there I'm just going to CD into that and I'm going to create a new file called Docker file and what we'll do is we'll copy over um some of our stuff because in ECR we uh we created some stuff so if we go over to ECR here we have this Docker file and we can copy its contents but the one thing I don't know is how would it grab all the other stuff so that's something I'm not 100% confident about but I guess we'll find out here in a moment let's go look for this folder here this ec2 image Builder folder it's getting a little bit unwieldy to try to find things here but it should be all alphabetized ec2 images Builder oh I put it in basic ASG by accident so what I'll do I'll just uh move this up I didn't realize I was in a folder but we'll do CD do dot CD do dot I'll just say move ec2 image Builders and I'll just go Um back to directories and so now it should be here good I'll just fix that spelling mistake here or I'll never hear the end of it okay and so in here we have our Docker file we don't have our Docker folder which is kind of frustrating I'm not sure where that went to oh I guess we we just moved that all right and um I'll make a new folder here again I'll just call Docker no no it's right here never mind I just couldn't see it and we probably want to name this file uh correctly to Docker file and so I want to go back over again to ECR wherever that is here it is I'm going to grab this code it'd be really nice if we could um compile this app I'm not sure if we're going to have some trouble with this but we'll go ahead and paste this in and so if we go back over to here we can grab these lines okay we're just using this as a working environment you could launch this in um sure Ubuntu Jammy does it come preinstalled with python because it might says is it preinstalled by default so I don't think we actually have to install we just have to provide all of our other stuff here it might already have curl and get installed and this was this was specifically for this video but my question is how's it going to pull in these files right because if we go over to here how would it know and so I think what would what would have to happen is we'd have to pull these from S3 or something right because otherwise it's not going to know um so that is something that I'm kind of wondering here okay so let me go figure that out so there's obviously managed policies in some way for easy to image Builder to work with it so I'm going to assume that the way we're going to get these files is by placing them in S3 so what I'm going to do is I'm going to go ahead and create myself a new bucket also just going to see into the correct directory here so we know what we're doing so say ec2 whoops ec2 image Builder and I'm going to go to this uh Docker one here whoops Docker because I really want to grab the contents of these app files so what I'm going to do is go ahead and copy this and I'm going to go back into the E2 image Builder directory wherever that is um here yeah okay and I want to paste this there we go and the only thing I'm going to do is remove this Docker file because we don't really need it in here or what we could do is we could just take the contents of it no no I'm going to delete it out of here because I just want to download or sorry I want to um not upload that to a ss3 so we'll go ahead and make a new file here we're going to call this one readme do readme.md and in here I want to do a S3 sync and I want to sync um the app directory uh to an S3 bucket but we'll figure that out here in just a moment because the first thing I want to do is create a bucket so create a bucket to host our app files and so we'll go ahead and do this we can make it a zip and then unzip it I don't want to do that I just want to download the files individually quickly so it'll say a S3 uh sorry MB to create a bucket S3 col slash and this one's going to be in region CA Central because that's where I've been operating this entire time it should create it there but I'm going to do that anyway explicitly we'll just say uc2 image Builder uh app and I'll just put some numbers here to create that I'm going to go ahead and do that if you are creating this you might have to use different numbers of course uh I forgot the S3 in front of it so it's not going to work without that so we'll go ahead and cancel that out we'll try this again and we'll hit enter okay so now we have that bucket I always forget the sync command so we'll go look it up it S3 sync and we'll go over here to this and I'll go to version two we'll go down to examples and it's just sync period then to the bucket okay so that's pretty straightforward so I do want to sync the contents of the app directory I'm not sure if I can do this and I want to say S3 ec2 image Builder I wonder if I can do this like app I wonder if that would work so we'll go ahead and try this out doesn't like something here maybe this would work better oh you know we have to have S3 in the front of here otherwise it's not going to work we'll try this again and it looks like it uploaded it to the app directory so that's good okay so that is good um and then we need to modify this Docker file because we need to pull from this location so I'm going to want to run those commands I'm going to go say just run and this is not going to be efficient whatsoever I'm just going to go ahead and paste it in here and say S3 copy I mean we could just sync it as well that' probably be more efficient we'll just do sync in the other direction and we'll just say um app here and actually I'll just do period there so it downloads that and so the idea is that I want to sync the contents of that folder I'm not sure if that would work correctly I'm going to do a quick test here just going to say uh mkdr test and CD into the test directory here and then I just want to see what happens if I was to do this okay and so if we go into this test directory LS it doesn't seem like uh that worked and also I'm not sure why it's not showing me that test folder here there it is yeah so that didn't work um what if we try this and do it LS here there we go so this is the command I actually want so that is good I'm going to go ahead and just delete this folder it's going to save us some time just by doing these tests and so we CD in that folder we create it and the requirements.txt is there uh we don't need to copy that so we'll just take that out and so this should in theory create our container so I'm going to copy this contents I'm going to go I'm going to set copy here I'm going to go all the way back over to here and paste it into our example file there we go has one error what's its problem end of document or a copy separated a document separator is expected so it's expecting something um not exactly sure why we'll try this again it might be because just like wrap this like this run is it run or CMD I think in this case it'd be run right just give me a second okay actually I think run is fine because run only happens during the build and so we want to copy those files during the build not when it starts up the application so that is totally fine um I'm not sure why it has a problem here maybe maybe it's that forward slash it doesn't like so I'm going to go ahead and you got to be kidding me what what's going on here it's really messing up here okay there we go okay so I'm not exactly sure what I saying that give me a moment to figure it out all right you know what I'm going to just ignore this and see what happens because this is parsing yaml right and I know what yaml looks like this looks fine to me and maybe this is messing it up so I'm going to ignore that for the time being I'm going to close this tab it's kind of in my way I'm going to just see if I can proceed even though I have an error here um Target repo so this is the target where the output container is stored so we'll actually have to create a new ECR container um I'm just going to quickly do that by going over to ECR and doing that manually so we go ahead and just say uh ec2 uh app or ec2 builder app okay and this one can be private and we'll leave it as uh mutable for this one there we go and so I'll go back over to here we'll give this a refresh I'm so happy they gave us a refresh button here we'll go down we'll see if we can proceed next yeah I didn't care so I just don't trust it I think that it probably will work um a docum is defined for the components we'll go ahead and hit next and just checking here this seems fine we'll go ahead and hit next next and this is fine we'll hit next and we'll create this pipeline what I understand is how is it going to pull from S3 because somewhere here it has to be configured for that right so I'm going to go in here and just take a look and I guess I'll just do a bit of reading just give me a moment okay all right so it seems to be suggesting there's an instance profile so maybe there is a actual piece of compute that is happening here and so I'm just going to look a little bit around we'll look at the infrastructure we'll see we have image workflow we'll ignore that um instance type IM IM roll so it says ec2 instance profile for image Builder and so this is what I'm really interested in let's go take a look and see what that looks like okay because I I just don't know what's in this one here so we'll go here and we have SSM access okay we have this one which allows us to get components all right and then we have being able to pull images from containers and stuff like that so that's good but there's nothing here that says it can have access to S3 so this is where we're going to have to modify this um we could create a new one but I'm really lazy here I'm just going to give it S3 access because I feel like that's okay I'm going go inline policy here I'm just going to choose S3 okay and I want it to be able to read from S3 I say get objects and that should be sufficient just going to search for get in here oh they're all gets yeah so I'm going to do that and I don't care if it's all um is this a good idea yeah I'm doing it get objects from S3 you know you have to decide on your own risk but I just want that to work and so I think that will um uh give us the access that we need could we also add a bucket policy I don't know um I guess we would give that roll so if we wanted to we could take that roll and then add it that way there's two different ways that we can do it but I'm going to just do it this way because I think this is the easier way to do it so let's go back over to our Pipeline and I think that we have all the access that we need in order to do this let's go ahead and run this Pipeline and hopefully that it will work okay so somewhere in here we can watch it running um maybe it's under images here it is we'll go over to Docker and we'll click into this and we'll go over to our workflow and so we'll have to watch and see how this goes so this is really nice I mean obviously we can use code build to build containers and build virtual machines but I mean this service is getting uh really really nice so I like it so far but anyway we'll just wait here and see what happens okay aha okay so so while this image is building I went over tc2 and it's launching an M5 large I knew it I knew it was doing something so in the other video I was saying it was kept saying it was free it was free I'm like okay yeah but what's the real cost what's the real cost and here it is so it's actually launching up um an ec2 image and so it's not free but the question is can you change it away from an M5 large and that's something I don't know and so it might be the case that when we actually launch up this infrastructure here uh because we don't choose the instance type maybe it's defaulting to that M5 uh so yeah that's what I was wondering so yeah um now we know but anyway we're going to watch this build and see what happens and hopefully it works the first time I would love it to work the first time but uh you know that's not always the case but anyway we'll just uh sit tight here okay all right so our container build has failed uh no surprise there as I expect it to fail on the S3 part but let's go in here and take a look and see what's happening so uh here we're getting and defile um so I'm not exactly sure what I'm looking at okay that's the input output but what happened so expectation not met error here let me carefully look here and see where the problem is okay also I just want to point out it looks like it's running SSM commands underneath so we talked about run commands how we would traditionally do this so that's kind of interesting but still trying to figure out what that error is here okay here they're talking about something about changing the working directory so maybe um I've affected it somehow with the working directory but we'll have to take a look and read a bit more about the docs here so what I would say is this is very cryptic and um it's not very useful so I think the only way I'm going to figure this out is by editing our image or our image recipe or whatever you want to call or Pipeline and uh maybe from there uh we can fix our issue so I'm just looking for where we might have configured the actual file I'm going to go to edit pipeline maybe it's just here no but I need to find out where we defined the docker file which I have no idea where H give me a moment okay I went over here to container recipes maybe sent our container recipe create a new version and yeah so this is where it changes so what I'm going to do is take out the cloudwatch agent Linux okay and we don't have any other components there I'm going to go down here what I'm going to do is take out all of this all of this okay and I need some kind of like CMD hello world for do uh for Docker image I have a feeling we just use the echo command CMD Docker Echo hello world I just want to see someone else has done it before I do it I'm just going to do it anyway and so the question will be will this work right so we'll go ahead and paste that in here whoops that's not what I wanted I'll just write it by hand Echo hello world I wonder if that will work okay so hopefully that will work but I'm just thinking by simplifying this significantly then maybe we'll run into less issues we'll create this recipe we'll call this one 1.0.1 can I create this without a description probably won't let me uh it says at least one build component must be specified maybe it's because we have this in here I'm going to take out the components create the recipe nope it wants us to specify component so we don't have any way around this does it say what kind of components at least one build component all right I guess I'm putting the agent back in I'm surprised we actually have to put one in at all I don't know why we have to okay so we'll go ahead and create that and so hopefully that will rule out a lot of our issues here uh I do want to update our pipeline so go back to our image pipeline here and I'm not sure if this is actually configured to use the latest so we'll go into here and somewhere it should tell us the version container recipe so that's using version 1.0 I'm going to go and edit our pipeline I'm going to see where we can update this to our latest so here it is 1.0.1 we'll go all the way down to the ground and something I'm really curious about is what would happen if we change our infrastructure here so I think this one was defaulting to an M5 so M5 large I guess we go back to instances and take a look here M5 large and so um you don't have to do this but I'm just going to do this because I just want to test if it's using something else and so I'm just going to switch this to an x large okay or even a medium do we have a medium here no we don't can we choose T3 medium okay CU I just want to see what happens if we do that and my new infa here we'll just choose whatever the default one is easy2 instance profile uh for image Builder there we go and that's the only thing I'm changing so we'll go back over to our Pipeline and and I want to go here and I just want to confirm has that changed I mean it doesn't look like it's using the new one can we edit this can I edit my pipeline we'll go down below here I'm going to choose this one instead so now it's using T3 what happens if I go to this one this one is a same except this one has a T3 medium okay great so we'll go ahead and save that and what we'll do is we'll go ahead and run this pipeline again okay so what I'm hoping is that on this run we'll get something else that happens and while we're waiting here we might as well go over to our instances and take a look and so now it's launching T3 medium so we do have control over uh what we can launch there originally uc2 image Builder you could not change it away from an M5 so so uh it's really nice that we can do that as that can help us be cost effective and now we are this I would say would be a better solution than having to use systems manager to configure this all by default or all by hand um but anyway we're going to uh hold tight here and see what happens because this is going to take a little bit of time okay all right so our image build was successful so that is um good and if we want to go see our images I guess we'd go over to ECR and take a look over there so just in another tab I'm just going to open up R I would really like to build our actual image because that's what we really want to do but if we go over to here we can see we have our image and it is built but the question is what is wrong with our template so this was clearly not well hold on this is not our whole template all we did was hello world and we changed a few things so I think in order to test this we would have to um add more stuff so yeah that's the question do we want to go through through this process and do this I don't know let me think about it okay all right I guess I'm just going to continue on and see and see if I can fix this and so where it could be failing is possibly on this line here or it could be the working directory so I think what we should do is uh go ahead and edit this now you can just watch and then wait until I fix it and then do the final implementation because you don't want to wait every time and uh based on how long these builds are I'm going to go over to our container recipe I'm going to go and make a new version of this one and what we'll do is go here I thought I would prepopulate it let's go back for a second and create new version yeah there we go this will be 1.0.2 1.0.2 see if work app messes up template I mean it did show us in particular there was a problem we placed this one line in here really didn't like it for some reason so we'll go all the way down to the ground here and if we place that in here I'm not getting that error now okay what if I grab the next line then what I still don't have that problem what if I grab the next line no problem what if I grab the next line again I'm waiting for this to complain no problem so far what about the copy and now I'm thinking that maybe what the problem was is this line here because it has the curly braces right so we go here no problem still I'm gonna hard code this to 4567 just so that uh we can rule that out I'm going to try to set this as well still no problems and I'll grab this last line here okay so we're not seeing any problems this time around but um maybe that's an indicator that this will work correctly so I'm just going to try this okay and then we'll see if we actually get an error so we'll do that we'll create and then we'll go over to our uh image pipelines and we'll go back to our Docker container Pipeline and we'll go to our container recipe I'm going to go ahead and just edit this pipeline here we will I don't know why it doesn't show the uh pipeline name when it does that just kind of throws me off making me think that it's not going to work correctly going to edit that again yeah it just seems like it's a bit glitchy we just got to be careful there and so I'm going to drop this down choose 1.0 to and I'm going to find out if this uh actually works so we'll go ahead and run the pipeline and we'll see if it builds okay so we'll just hang tight here and we'll go to images here and so it should be building a new one is it running there we go okay and then we will just have to observe what happens okay all right looks like our latest one failed it might be the same reason why but at least we weren't getting that yaml uh error there so we'll go over to workflow we'll scroll on down we'll take a look it's failing on the last step um and again you know I find this part to be very cryptic but we'll do our best to make sense of it uh oh do we get application logs well that would actually make it a lot easier to make sense of as we were looking at the step output I don't think we actually ever went to application logs so maybe that's why it was so hard to make sense of this um it is failing on the same steps so let's go take a look at the application logs which I just never noticed here before and we'll see if this tells us anything different command failed waiting for command to complete attempt number three finish execution of all documents so we have step F cloudwatch agent execute back finished execution so it's not exactly clear what is happening but obviously something is not working and to me it seems like it's probably this command here it's probably the adabs S3 sync that is failing um so I would assume that this would have preinstalled onto onto it the ad Li however I do believe we CH chose as our base image the Amazon link uh or sorry the Ubuntu image and so Ubuntu actually might not come preinstalled with it and so that might be our actual problem so what I'm going to do is go over to container recipes and I'm going to try this again and this is just a total guess that I'm thinking that's our issue and we'll go here and our base image actually says Amazon Linux no down here it says uh Ubuntu 2023 so we're using ubun 2023 I guess this doesn't really matter because this is obviously lying and again it's just a glitchy UI I'm not sure why it's so inconsistent but we'll go ahead and try this again and we'll ignore the fact that that's selected because clearly it's selecting a buntu down here and maybe what we need to do is install um the CLI and so maybe in here there is one for CLI there we go and so this might resolve our issue so all I'll go all the way to the top and we'll call this one 1.0.3 we'll say include the AWS CLI I actually don't really care about the cloudwatch Asian but it is something that is nice to have installed we'll go all the way down the ground we're going to go ahead and hit create the recipe and I'm going to go ahead and try to deploy this again so go over to um image pipelines we will go click into here we will edit our Pipeline and notice that it's not populating so I do not trust it we'll go back okay you got to pay attention cuz sometimes it us stuff just does doesn't work properly for whatever reason and we'll go down here to version three we'll go ahead and save those changes and that should be save we'll confirm that 03 we'll go ahead and run the pipeline so it should be running the pipeline we'll go to images it takes a moment for this to show up so we'll just wait a moment for this to show up there it is it's now showing up so that's excellent and we'll click into here we'll go into here again we'll go to workflow and we'll see if it passes are fails okay all right so we have another failure that's okay um but at least we identified the fact that we didn't have the ad CLI because that definitely would be something that would mess things up so we'll go over to here we will take a look at our application logs we'll see if we have any better indicator as to what is going on so I'm going to scroll down here I just what I want to see is whether installed the a CLI so it says a CLI is running the desired version so it is definitely there um if we go up here we see python um so that's actually it doing the install right so it's going ahead and it's installing those components then it's going down here and it says it is Success successfully completed waiting for the command to complete attempted three times so it seems again like it is failing on this step um so what we could do I suppose is we could go ahead and I'm going to go back to our uh recipe container recipe and I'm going to go ahead and make a new version and I go down below here I'm going to copy our template back over and I'm just going to paste in this version so I have this version in here and so I really think it's on this one so I wouldn't mind doing some Echoes or or logging here so just give me a moment and so I just couldn't remember what it is but I think what we can do is run an echo command so what I want to do here just to make things more clear and I don't know if it'll actually do this we'll just say uh ads S3 sync and then we'll say copy Rex and it'll say run pip install all right and so that should give us a little bit what's going on here right um because I don't think any of these things ran but I think it's failing literally on this one here if it's the working directory I'm just going to go go up here and just say uh working directory and I'm just just in case like this messes it up I'm going to do this as I just don't trust it for whatever reason so I really want to know what's going on here and another thing that I'm going to do is I'm just going to be very um explicit and I'm going to go to the bucket policy and I'm going to add in the um instance profile so this profile can actually pull so I think this is okay I'm going to go ahead and copy all this we're going to go back over to here we're going to paste this in doesn't show any errors so that is good um I'm going to go all the way to the top here and we'll say 1.0.4 add Echoes to see if it logs in the application logs because that's what we need to see we go all the way down to the ground we'll go ahead and create this recipe okay so that recipe is now there we're going to go back to our image pipeline I'm going to go into here I'm going to go ahead and hit edit pipeline notice that it's not uh displaying properly so I'm going to try this again there we go going to drop this down we're looking for version 0.4 everything else is fine that T3 medium is totally fine it's more cost effective I like that um and so what I'm going to do is go over to the S3 bucket okay and what we're going to do is change the bucket policy what I'm going to try to do is tell it to allow the roll um from uh from this one so it is able to so we'll go into uh permissions here and we'll edit our bucket policy and what I want to do here is look for some examples that's a put example and this is kind of what I want here so I'm going to go ahead and copy this one it's kind of what I want and I'll go back here and I'll paste this in I don't want this uh condition here so I'm going to take this condition out I want to utilize this bucket okay and I need to add in um a specific Ro so this says ads uh I'm just going to look for uh adabs principle roll in S3 bucket just because I'm not sure if it h it is for ads and I'm just going to type in roll it principal I am out it account user roll so I think that's what it is I think that's how we're going to do it is is to provide the role there okay so that's what I'm going to try to do um so I'm going to go back over to our container and go to our image pipeline click into this one I'm going to go into our infrastructure configuration and in here we have a roll so it should be somewhere in here it's this one here so I'm going to go over to IM specifically rolles that's what I want to pull up is rules and we'll go ahead here and search and I want to grab this Arn here that's what I want and so I'm going to go back over uh to our bucket and I'm going to go ahead and paste in this okay and so in theory this should give us access we could give it full access to everything but I think this is simple enough and we'll go ahead and save those changes all right um the other thing that I'm thinking about here is I'm just thinking about this for a moment I mean that's fine I just want to double check I mean that looks right so that that name looks correct to me so let's go ahead and pull the trigger this time okay and so I'm hoping that we'll see some kind of difference whether it's the logging or something else here I just got too many tabs I'm just going to close a few of these out here so this is a little bit more manageable and we'll go ahead back to image Builder and we'll run that pipeline again all right so container image we'll go ahead and run that container I'm going to go over to images we're going to give this a refresh and I'm waiting for that 1.04 to show up there it is we'll click into it and so yeah I'm hoping that uh this time we'll have better success or better logging at least okay so I'll see you back in a bit all right so let's go ahead and give this a nice refresh here we'll take another look here so this one's failed uh which not a big deal um it's not fun but it's what happens and so the question really be did it time out again did we have an issue with our Echo we go to our step output um and so we see our run and so what I'm looking for is to see if we have any information about our Echo um I don't see anything in the SD out so I'm assuming didn't write there we'll check over here and go all the way down to the bottom and it looks like see here carefully checking here so we have SD out for a c okay and also I didn't know but we can probably expand these I just could go ahead and maybe there's a way to uh not sure if there's an easy way to expand these but what I'm looking for is to see if it ran any of our commands and if it outputed the information what was it called uh this one was called ads S3 or working directory S3 sync so I want to to search this here we say working directory I'm just going to enlarge this that was useless um maybe I'll expand this I didn't know we could expand this before I really wish we knew what it was doing because here I just cannot tell what is even going on yeah this is incredibly uh difficult to debug um well anyway I think what's happening is that it's probably failing I think on the sync because I don't think it's the working directory issue I really think that it's the adus S3 sync action so maybe it doesn't have access to the internet but that makes no sense because how could it pull stuff but I guess it could be pulling stuff uh internally so let's go take a look at our infrastructure configuration for this one and we don't specify VPC we don't specify subnet we don't specify Security Group okay so none of those are set if those aren't set then how would it communicate out to the internet let's go back to 1.44 here and what I want to check is the infrastructure configuration so we can see this stuff another thing we could do is maybe if we go to ec2 maybe we can still see uh the configuration and I just want to go here and find the last one that ran I guess it doesn't really matter because they're all going to be doing the same thing and I'm not sure if it show any networking information after the fact so what I want to do is I I know this is going to fail but I want to run it again um and I want to observe where it's launching because I want to know is it in a private subnet public subnet um and does it have a security group out to the internet you know things like that okay so what I'm going to do here is go ahead and launch this again even though that seems crazy and you know hopefully you've just been watching along here and not necessarily running this every time and waiting for me to succeed here we'll go ahead and run this pipeline again and so this time I'm going to go over to our ec2 instances and I want to wait for this thing to spin up there it is okay and what I'm looking for here don't want both of them just the one I want to what networking settings it's getting so it's launching the default VPC I know that's my default VPC and then the other part of it is what security group does it have we'll go to security well first of all does it have a public IP address address it does and it's launching with the default one okay so the default one I don't think communicates out to the internet all traffic all and so if we edit that rule yeah so this is another use uh thing that I think is happening I think what's happening is that it wants to talk out to the internet it can't um because in order to talk to S3 has to go over the Internet because we don't have a VPC endpoint I don't even know if we can configure a VPC endpoint for this particular service I don't even want to try so what I'm going to do is create a new security group and call this one uh my ec2 image Builder uh eget or Internet so this will be out to the internet we only have the default subnets I'm not worried about placing in the wrong place I'm going to go zero I'm going to say all TCP traffic from anywhere ipv4 okay and just in case it wants it I'm going to give it IPv6 as well okay so we'll go ahead and create that and so now we have this Security Group I'm going to go back over to our image pipeline I know that one's running I don't care I know it's going to fail and I'm going to go to infrastructure configuration I'm going to edit this one if it's possible possible okay and we're going to go down below here and I'm going to set the VPC explicitly I'm going to select uh the subnet explicitly I'm going to select this uh this Security Group and so what I'm hoping for is that's going to um help us uh here okay one thing I'm noticing is that it's not populating say creating infrastructure config I don't want to make a new one I want to edit the current one so again I think this is just an issue with ad Us's UI there we go just be very mindful of that because their stuff messes up all the time they're going to change the UI it's not going to get any better so we'll go ahead and try this again there we go and so I'm going to go down here I'm going to save this configuration and so now it's set to that VPC and that's SEC group I'm going to go ahead and just update my image recipe because I don't want those um a container recipe because I don't want all those echoes in there and then I also just want to know what version I'm on so I can keep track of it so I'm going to create a new version we'll give this a moment and it did populate there so this will be 1.0.5 I'm not going to give this a description I don't care um try again I'm going to go all the way down to the ground and I'm just going to take out the Echoes out of this so I just want to get rid of uh this Echo and this Echo and this Echo and this Echo okay so those are gone we'll create the recipe I'm going to go back over to our image pipeline I'm going to go to The Container one and I'm going to click into it I'm going to go to actions and edit this pipeline notice it's not filled in because it's just glitchy so I'll go back and try this again we'll click into it we'll say edit pipeline we'll scroll on down we'll make sure that we're using 1.0.5 we'll look at the infrastructure details and I'm trying to see in the infrastructure details here it says the VPC subnet and our security group excellent we'll save those changes great and um I'm not sure if that image failed I don't care I'm just going to run it again and we'll go ahead and run this pipeline we're going to go over to images I'm going to refresh here and I'm waiting for I believe 1.05 was the one that we want to uh run here it is yeah so go into here and so I'm hoping that that is our cause okay but yeah we will hold off here and see what happens all right so um we're still having the same issue it's still failing um and I'm just trying to think of this for a moment here because the idea is that we have this container and it's trying to pull from the internet does the container have access to the internet so that's like my next question um so let me just figure this out okay all right so another thing I was thinking about was the fact that um maybe we just shouldn't be trying to download directly from S3 and what we should do is actually download the files onto the build system and then copy them over to the container to me that makes more sense um so I think fundamentally what I'm trying to do here is just wrong the whole uh sync is wrong here um and so what I'm going to do I'm just going to try to get rid of these other ones so I can take a look and and see what my code looks like here instead running this I really want to actually just copy the files over um also this file doesn't make any sense because we're not actually copying from anywhere so probably we don't want that anyway uh what I'm going to do is go back over um to here and I believe in components we actually have some commands that we can utilize to uh copy stuff so there might be an example here and I believe that they have like a download S3 so what I'm going to do is go over to here and say download S3 ec2 image Builder and uh there are I wanted to make the separate video but it seems like we'll have to do components here but there should be something for downloading and uploading stuff so if we go to download I think it's in here download S3 where S3 download oh here it is right here so uh with S3 download action module you can download an S3 object or set of objects to a local fer folder that you specify with the destination path okay so here's an example and so this is something that we could do um for our component right so essentially I think we have to create a component to download our source is what I'm getting at here um so what I'm going to do is go back over to ec2 image Builder and we are trying to create a create one here a component and we do want this for Linux and this is for ubun 2 or whatever it doesn't really matter it's compatible with everything here this going to be like download um S3 code S3 app code download S3 app code from our datab S3 repo so say 1.0.0 okay okay um and then we need to Define what it is that we want to do so here it has a bunch of steps and build and stuff like that so I need to figure out how to get this in the right location just give me a moment so I think what we can do is we can just take out these two other steps I don't know if we actually need them we'll I guess we'll find out when uh we do something here and I'm going to go over to here and you can see that uh it is copying multiple files I really actually want to sync files but that's fine we can just manually specify each one here um and I might just take this out of here and just cut this in here and just say I'll make a new file here new file we'll just say component yam just so we have an easier time working with this okay and I believe what we need to do is put this here and then indent indent and indent and then in indent one more time and so this will be our build step we do have a few things set here so we'll have to specify the source which we have down below so I want this file here okay I want this file here I want this file here so we have the three files the question is where does it go because it needs to go go in a directory that it can copy from and I don't know where the default directory is so give me a moment to find that out so what I'm thinking is that um maybe what we can do we'll just take these well we'll leave the override on and uh it's kind of like glitching out on me here whoops uh may I have caps key on sorry about that and so there's that override option um I'm going to just put it on for all three of them and what I'm going to do cuz this is a Linux system so we're going to just do for sltm app app.py and so this is what I'm going to just try to do I'm just try to put it in here I don't want the read me file that one is total junk actually I only need two files I just need this and the requirements so we'll go ahead and we'll say requirements.txt okay so we have have those two set up and I'm hoping that it'll just create this folder for us uh we could also just put in the temp because the temp is definitely going to exist um so that's part one of it the next thing is we need to go back to our um Docker file here because this Docker file no longer makes sense and instead of doings S3 sync I just want to copy files over so I'm going to go here and just say temp app requirements.txt and then the other one is going to be temp app app.py to app.py so we're basically copying over those two files and so I'm hoping that that's going to get us the result that we want okay and so let's go ahead and copy this over paste this back in here whoops and we'll go ahead and create that component hopefully it doesn't need those other two steps we'll refresh here where did my component go I just made it where is it there we go there's a bit of a delay it's very unusual um and so we need to update our Docker file which is now this so we'll copy this again and we'll go back over to here we will go into our container recipes we'll go here I'm just going to give it a hard refresh because I just want it to populate properly and we'll edit this again create a new version sorry there we go and I'm going to go all the way down to the ground well before we do I just update this to 1.0.6 and we'll go all the way down to the ground and we will paste this in here okay the other thing is that we need to um select our other container so in here we have selected components and I want to go ones that are owned by me and so there it is I'm going to checkbox that and so now that one's there I could even change in what order it happens I'm going to put it um after the AI not not that we necessarily need it but just in case we do I'll put that in there and I'm going to go ahead and create this recipe and so now we have version 1.06 the other thing is well we have the container in there so we can go to our image Pipeline and we'll click and do it I'm going to give it a hard refresh just so that it doesn't uh not populate these values here which that didn't even help anyway so I guess I should have just uh refreshed anyway so go ahead and hit edit and we'll scroll on down and we'll make sure we're using 106 so that is our new one and hopefully hopefully this time it works so I'm going to run it again and I'm going to go down to images here I'm going to give this a refresh I'm going to click into here and we'll see if that makes any difference okay it failed again okay but there could be a different reason why I mean we're doing a completely different build this time so I'm expecting some kind of different information and this is uh ending a lot more abruptly so doesn't really help me here let's go to the application logs and we'll take a look carefully so I have the agent installed here okay the CLI is installing good and now it's showing the S3 download so it definitely is downloading files so here it says from here to there attempting to download the file and it creates the path good it creates creates that there and then it says checking for overwrite property so it's doing that it's copying the other one which is excellent so we're getting something different which is great and um then we have this here but then we have this waiting for command to complete waiting for a command to complete that is driving me crazy what command you have to tell us or we don't know okay let me look into this okay maybe there's something else going on here and what I'm thinking about is that these actions are defined in the image workflow right for the build oh oh are we building an image or a container oh oh oh okay okay okay hold on hold on hold on let's go back to our pipeline I just want to confirm that this is the the type of workflow we're using image workflows no it says default process okay so we're not explicitly choosing the image workflow the image workflow is the default so that makes me think that the image workflow we're using is this build container option because we're not saying anything other than what it is and so it's going through these steps these are the three steps and it's failing on this one but this is all it shows so we have execute components and that's all there is on this one okay uh bootstrap instance for container execute components but it is failing on this one so let's go back here and we'll carefully read that again and we'll go all the way down to the ground waiting for a command to complete action failed for step build components all right I'll investigate all right so uh you know working through my uh issues here I just kept going through here and making a correction so in this version you'll see I start removing each piece and so once I got to here okay it started working so to me it seems like it's probably failing on uh this line here like these two lines um so that's what I'm have to figure out next so let me uh see what the issue is here okay all right so I've went through multiple iterations of this and I'm just not getting anywhere uh with it and so one thought one thing I thought that might have been my issue is the fact that uh when you do that those copy commands you need to copy from where you are uh so if we go back over to here the idea is that I don't think that the copy command will work outside of where the build is being executed if that makes sense so you know hopefully that makes sense but where whever the build is is where that these files have to be and so I've been trying to figure out where these files are um or the build command is occurring so that I can place these files and download them in the uh in the component to the correct location so we go over here to version two I keep updating this and so I thought maybe it might have been here right um another indicator that might have told me where it is if I go here and we go over to our uh uh into the Container here and we check uh the workflow and we go to step three you know this should indicate to me some information if I go all the way down to the ground here um we have this directory called temp image service here and so I can see that it's referencing a Docker file here and so I thought maybe you know this is where it's executing but maybe that's wrong because this is referencing a fire a file from a Docker file from this location but it's not necessarily where the stuff is but anyway I tried to place them in here and so I'm just going to tell you there is no tutorial online there is no documentation or anything that tells you where the heck you have to put these files to copy them onto the container um if we were using um uh code build which this could be utilizing underneath but I don't think so then it'd be a lot easier for us to uh figure out what to do but but I'm just going to tell you this is really hard and it's just a bizarre uh case where I don't know where this is building and if no one else has solved it online and I can't find it uh and I have to actually go to aable support just to figure this out then maybe this is something that's not worth us figuring out um we could check the other steps here I don't think there'd be anything interesting in them which one is launching the instance and the other one is here but let's just take a look and see if there's anything interesting so it launches the instance okay yeah so nothing interesting so what I'm what I'm going to say here is that we did the best that we could here uh we could build out a simple image and we were able to do that but we weren't able to build um our application and run it which is what I really wanted to do so at this point I would just say like the service is a pain and we kind of was able to do everything we needed to do for the exams but not necessarily what I wanted to do from a practical standpoint but there you go um so we'll just send it there and you know whatever you want to clean up you can clean up um all this stuff is free so I'm not really worried about it but if you're worried about the containers you can go over to ECR and delete uh those containers you can delete all the pipelines I'm not going to clean up just in case I come back to this okay we'll see you in the next one ciao all right so we're taking a look at systems manager also known as simple systems manager OS has dropped the simple word but when we look at the initialism it's SSM so there's still that simple in there and it is an umbrella service used to automate the management of virtual machine so we can patch our VMS Supply configuration to our VMS run Linux Windows commands on our VMS monitor our VMS securely connect to our VMS maintain configured states of our VMS so there's a lot of stuff we can do there and then uh like System Manager it does a lot of different kinds of management and there's different uh System Manager services for that so we can do operations management using Explorer op Center uh cloudwatch dashboard trusted advisor personal health dashboard we can do application management using resource groups app config parameter store we can do action and change so automation uh change calendar maintenance window we can uh manage instance and nodes so we can do compliance inventory of them uh manage those instance do hybrid activation uh sessions manager run command State manager patch manager distributor we can do shared resources and that's the SSM documents so you can see there's a lot of stuff in here uh I want to note that uh this is all traditional architecture this is all the stuff we need to maintain VMS and stuff like that but if we were to use service architecture the majority of these operations would vanish that is the hidden cost of doing it the old way but um if you want to do it the old way abos has the tools to help you there now if we want to use SSM with our VMS there's going to there's going to be two things we're going to need to install we're going to need the SSM agent installed on our ec2 instance and we're going to have to have the SSM adus manage policy attached to our ec2 IM roll for instance profile so that it can talk to SSM so there you go so one of two things you're going to need if you want to work with systems manager is the SSM agent so the SSM agent is a software package that you need to install on your virtual machines operating system in order to interact with SSM okay so uh there's SSM there's your ec2 and there's the package inside of ec2 to suggest that it's been installed so uh luckily some instances come preinstalled with it and if you have to manually install we'll talk about that in a second here so if you are using the following Amazon machine images such as Amazon elux 2 Amazon elux uh one Ubuntu Server 18 Ubuntu Server 16 Windows Server 2008 to 2012 R2 after 2016 uh 2016 or 2019 Windows servers uh or an Amazon ECS um optimized uh Ami then you're going to be able you're not going to have to do anything it's just going to be installed there for you if you have to do a manual installation what you're going to have to do is uh download from an S3 URL so you're going to have to download um uh the package there so it's going to vary based on your operating system so you're going to just see that there's a variety of different ones there and then you're going to install it with your package manager and then start the agent service okay so now let's look at the other component that we need which is SSM ads manage policy so in order for your ec2 instance have permission to use SSM we're going to need that manage policy uh and so we're going to have to have it in our ec2 IM R so we canuse either ec2 rooll for SSM I'm uh notorious for always typing this one in because it's out of habit now but it's soon to be uh deprecated so we should probably shouldn't use it because we have a better one now called Amazon SSM manager instance core uh and so just to really give a visual here there's our E2 instance you see we have the SSM agent we have um the uh the role or the role or the policy applied to our role which is part of our instance profile and that's what we're going to need to talk to SSM okay and when you want to go create that manage policy ads makes it really easy you just go to ads for ec2 uh you choose uh your common use case which is E2 there and then you can just type in core and you'll be able to find that policy there okay all right so let's look at our first uh systems manager service Explorer so Explorer is customizable operations dashboard that reports information about your adus resources uh so what we're going to get inside of here is Ops data so this displays an aggre aggregated view of operations data then we have Ops items we're going to talk about in detail quite a bit here in the SSM section this is metadata about your ec2 instance patch compliance details and uh operational work items then you have widgets with an Explorer so these are individual report boxes that show data graphs or charts uh you have filter information in Explorer to focus on items that are relevant to you that at or or or can trigger an action uh you can have high priority issues identified and explore and use it within SSM op Center to resolve issues you can explore data across multiple accounts and across multiple regions via different modes so we have the single account single region mode which is by default and then you got the single account multi region mode and then you got the multi account multi region mode which just covers everything a really good mode to have on uh you also have reporting in here so you can export reports to a CSV stored in an S3 bucket Explorer is free but the underlying resources and API calls are not so uh you are charged on the number of op Ops items per month and the number of AP API calls per month to uh these following API calls okay so that is Explorer and we'll move on to the next section so now let's take a look at op Center so before we uh just talk about what it is let's define what an operational work item is so this is some form of work that needs to be performed on an ec2 instance and so op centers is a consolidation of your operational work items which are called ops items in SSM and it's for your it team to either view investigate or resolve that Ops item okay and Ops items can be manually created or they're automatically created uh via adus services such as adus config cloudwatch events which is also known as event bridge now cloudwatch uh application insights uh for net and SQL uh servers okay um so if we want to make one manually you could just use the uh ads API so here I'm using the CLI and we just do create Ops items give it a title description priority Source the operational data we want to pass along how we're going to get notified we can even tag it there and just some examples of Ops items that you might be creating your Ops Center maybe your ec2 instance disc is full maybe your Rd instance instance is not responding to a ping the ec2 instance needs to be terminated the ec2 instance has stopped the Autos scal instance is uh uh failed to launch the EBS snapshot copy has failed so you can see that um uh that it isn't just virtual machines but it's services around those virtual machines like Auto scaling groups uh databases and storage as well okay uh and let's just take a look at what that looks like so I just pulled out a screenshot here to show you here is a list of um Ops items where we need to perform some work on uh but we'll talk about Ops items next so now let's take a look at Ops items in a greater detail the settings that we can set on one so with Ops items you can set the following statuses open which means it's not being worked on but it's active in progress meaning you are working on it and resolved which means means that it's no longer active but you can search it in your past history uh you can set a priority on your Ops items between one and five these don't really mean anything it's you're just going to have to Define them what they mean within your company okay uh then we have related resources uh that you can have on Ops items so these are Aus resources that can be associated with an Ops items to create deep link uh information about that resource so there you can go ahead and add a resource you could add a a whole host of things like I can add an ec2 subnet which is odd item to add but you can totally do it um you can add operational data so operational data is data is just custom data that can be applied to an Ops items in the form of a key value pair if you're paying attention in our Ops Center uh slides there I was showing uh showing it with a manual thing where we were using the CLI and we're passing operational data but if you wanted to add it via this the console you totally can so you just hit that add operational data button there and then you just give it a key value okay uh with with Ops items they can also reduce duplication so um if Ops centers uh determines that there are duplicate Ops items it's going to dup them for you um you can also have related Ops items and you can associate up to 10 related Ops items which is very useful and then uh there's similar Ops items so ad of us will automatically recommend similar Ops items for you uh it serves the same purpose as of related Ops items it's just that it's just using AI ml to do that for you so there you go so the whole purpose of Ops items is that we can remediate them so let's talk about what remediation is so remediation means to make right to fix so when we say we are remediating Ops items we are putting them into action the act of fixing the reported issue on our infrastructure okay then we also have uh the concept of runbooks so a runbook is a document that contains a series of instructional steps to perform an operation they can be described uh to do regular maintenance uh remediation due to misconfiguration or procedure resulting in an unexpected incident rum books aren't like adus specific they're just good for devops or ssops in general to have these run books here but we can uh manage our run books using SSM automation so this is a service that allows you to Define documents which are run books that execute a sequence of commands carrying out those runbook actions we're definitely going to look at uh SSM Automation in greater detail let's just take a quick peek here so with SSM automation documents they can be associated with Ops items so when you have an Ops items appear in your SSM explor dashboard or your Ops Center Ops items list that uh uh list requires action you can just press a but button to uh to carry out that run book and that was really hard to say because there's a lot of different SSM services but you can see that they're all heavily interconnected so we're still talking about op items specifically related resources so op Center automatically creates a deep link to the original Resource page when you specify an AR uh it for a related resource to an Ops item so this enables you to view detailed information about your impacted related resource without having to leave the Ops Center so if you're looking at a particular item here you can um drop down related resources and you can see the You2 instance you could click it to get more information about it and so some supported resource types in include Cloud front distributions cloud cloud formation Stacks Cloud watch alarms code build projects codb tables ec2 instances uh uh elastic beanock applications IM users and groups Lambda functions RDS instances and even though it's not listed there that you can even associate networking objects like like subnets so there you go so before we jump into talking about resource groups let's talk about what tags are so tags are words or phrases that act as metadata for organiz your adus resources and so any kind of resource you launch up like an ec2 instance you can apply tags to it and the reason you want to do that is so you can use them within resource groups which are a collection of resources that share one or more tags okay so resource groups appear in the global console header or under the systems manager and so it just used to be in the header and then they moved it to systems manager if you've seen my CCP course I show you how to make a resource Group that way but we can also do it in uh syst as a manager and I don't know why but I love using this ballerina icon thing to represent resource groups so I never get rid of it it always shows up in in this slide when we're talking about resource groups but resource groups help you organize and consolidate information based on your project and the resources that you use resource groups can uh display details about a group of resources based on metrics alarms and configuration settings okay and any time you can modify the settings of your resource groups to change what resources appear all right all right so let's talk about app config for SSM so it is used to create manage and quickly deploy application configuration and we can use it to roll out changes to our application configuration files or when we need to avoid errors such as typos that could break our production environment so it supports controlled deployments to apps of any size includes builtin validation checks and monitoring we can use it to monitor our apps hosted on ec2 or lambdas or containers or our mobile apps or iot devices so that's what it is in a nutshell all right all right so we're looking at the anatomy of app config these are all the components that make up app config so the first thing we have is the environment this is a group of adus resources intended for deployment then you have deployments this is the act of deploying application configuration changes and for deployments you're going to need a configuration profile this is where the configuration is located and how to validate it so to locate it we need configuration Source this is the location of the new application configuration changes could be stored in S3 it could be in SSM document it could be an SSM parameter uh from parameter store then you need your validator so this will check if the configuration is valid for deployment either on Lambda or or via adjon file then you have deployment strategies such as deployment type either linear or exponential or deployment time so during uh the duration of this deployment or bake time the amount of time to wait before completing the deployment so there you go all right so let's take a look at SSM automation which simplifies common maintenance and deployment tasks of ec2 instances and other adus resources so here is a screenshot here of Automation and these are all you can just think of them as run books or workflows um but let's just talk about some of the points here so you can build automation workflows to configure and manage instances and Abus resources you can create a custom workflow or use a predefined workflow uh maintained by AWS so let's say you wanted to back something up and you wanted to back up a Dynamo DB database they got a lot of cool in here you want to go check out but you can make your own if you want to you can receive notifications about automation tasks and workflows by using cloudwatch events which is now known as event Bridge uh you can monitor automation progress and execution details by using ec2 or systems Management console so there you go so let's talk about a use case for automation uh and so the example I'm going to use is uh this one for stop ec2 instance with approval so it's an automation document where uh you can request that one or more IM user approves an instance action to stop so you're going to launch the automation task then you have an SS SNS topic that sends out an email to your IM users they're going to say yes it's okay to stop it and then the ec2 instance is going to uh change states to stop so this is a use case of automation that you can use with um automation so there you go all right so we're looking at the anatomy of automation starting with automation documents and these are known as either runbooks or operational playbooks they're defined as either Json or yaml most people like to use yaml nowadays and they uh they Define a series of actions Abus has predefined documents we saw that in the prior slide and you can create your own documents and then we have automation actions these are individual steps in the automation document and they make up the uh workflow and so actions determine the following uh at each step you're going to have to Define an input an output uh a behavior and steps are defined in the main steps section of your uh automation document and automation supports 20 distinct action types which I think we're going to look at uh shortly so then you have the automation CU and this is a queue that holds automations being currently executed adus accounts can run 25 automations simultaneously a maximum of 75 uh child automations and additional automations will be in a pending State the queue can hold uh can hold a thousand automation executions and I believe this is the queue so if you see here you can see what's being executed what has been executed so there you go all right so we're taking a look at an automation document I do want to point out that command documents and auto automation documents are more or less the same format so we're not going to review uh command documents later on because this is pretty much the lesson for both of them right here but let's just quickly go through and see what uh we can do with this document so the first thing is we set a schema version It's set to point 0.3 is there a difference between 0.2 and .1 probably but we don't need to know that we just need to know to set it to 0.3 then you have assume roll this is the IM R the automation document will assume you have parameters these are variables that uh you can use within the automation document then you have your main steps this is where all the action happens it's where you define all your actions and then you have an action and so there we are actually defining an action type called adabs execute adabs API and the action type you choose is going to determine what kind of inputs and outputs you're going to have to H have to um uh use and then you have the next step it says where to go next and so now we'll actually look at the actual uh types or actions that we can use so let's take a look at automation document actions these are the action types that we can Define on our steps and there are 20 of them so this will take a little bit of time here but I think it's worth going through here just to see what we can do so we can do adus approve this will pause an execution for manual approval we can do assert adus Resource Property this asserts an adus resource state or event State you can do Branch so this runs conditional automation steps you can do change instance state so change or assert an instance State you can do copy image so copy or encrypt an Ami you can create an image so create an Ami you can create a stack so that creates a cloud formation stack you can create tags you can delete an Ami you can delete a cloud formation stack you can execute um another automation document you can execute an ads API call you can execute um a script so that would probably be a bash script you can execute a state machine with a steps functions you can invoke a Lambda you can pause the automation execution you can uh trigger a run command document you can uh Trigger or you can um launch an ec2 instance you can uh tell the automation execution just to sleep for a period of time and you can wait for other ABS resource properties uh to become available and so uh these shared properties are always going to be defined when you have an action so you're always going to be able to Define Max attempts timeout seconds on failure the inputs and the name so there you go all right so let's take a look at change calendar and change calendars allow you to Define when SSM automations are allowed or not allowed to be executed by your team we have two types of calendars we have open by default so actions only run during a scheduled event and closed by default so actions are blocked from running during a scheduled event and so here I have a calendar and on it I have demo day and it says open by default so that means that it's actually backwards it should be closed by default but let's say you had a demo day and you didn't want anything to be ran on demo day then you'd want your calendar type to be closed by default not open by default but uh there you go all right so now we have maintenance Windows uh this lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system updating drivers installing or patching software this sounds really similar to change calendar but that's what it does uh and so what you can do is you can define a schedule uh you know when these actions can be performed you can register the targets and you can register the tasks so there you go so let's take a look at compliance for SSM so SSM compliance or configuration compliance scans your Fleet of managed instances for patch compliance and configuration inconsistencies so you're going to choose how you want to filter your resources you're going to see a summary based on compliancy then you can drill down and view the specific resources selected in the resource summary to see if they're compliant or not so there you go now we're going to take a look at SSM inventory an inventory provides visibility into your ec2 and on premise Computing environments you can use inventory to collect made metadata from your managed instances you can store metadata in S3 and then use builtin tools to query the data and quickly determine which instances are running the software and configuration required by your software policy and which instances need to be updated and so here you can see uh a bunch of Gras for inventory uh inventory is very useful just to make sure that you know exactly what you're looking at okay so there you go all right so let's take a look at hybrid activations so SSM activations allow you to register external Resources by managed AWS systems manager for your on premise servers or VMS your nonus servers and other devices with adus systems manager so what you're going to do is you're going to create an activation and this is going to issue you a code and ID that acts like your E2 access ID and secret and this is going to allow you to manage uh a compute that's just outside of AWS which is really cool uh so once the instance is registered it will show up under uh an activation and it will set an expiry date so as far as 30 days in the future so you do have to keep on renewing this but it's a way for you to use systems manager all the tools System Manager for your on premise stuff all right so we're on to systems manager sessions manager but before we jump into it let's just Define what a session is so sessions are based on a secure bidirectional uh Communication channel between the client you and the remote manage instance that streams in UTS and outputs for commands and the traffic is secured uh via the client and the managed instant using TLS encryption uh or and they're signed using sigv4 uh and with AWS you can use KMS to encrypt uh the data beyond the TLs encryption if you need uh better encryption for your connections there and the way sessions work is it's just two twoway communication via an interactive bash or poers shell to access the instance so systems manager lets you manage E2 instances uh uh or on premise instances or VMS through an interactive oneclick browser based shell or through the adus CLI and so uh the advantages of using sessions manager is that it centralizes Access Control to instances using IM policies there's no open inbound ports there's no need to manage Bastion hosts or SSH keys that is the the really really good reason why you want to use sess manager because SSH keys are a pain oneclick access to instances from the console and uh port forwarding it it does crossplatform support for both windows and Linux and it's going to do logging auditing of your session activity via cloudt Trail S3 cloudwatch logs cloudwatch events and SNS okay and the it's really simple to use there's a big button that says connect in the E2 console you're going to choose sessions manager and then you're going to be in your instance that's how easy it is uh way easier than managing SSH keys so there you go all right so let's take a look at systems manager run commands so run commands let you uh you remotely and securely manage the configuration of your managed instances so let's talk about what a managed instance is a managed instance is an ec2 instance or on premise machine in your hybrid environment that has been configured for systems manager and so we can automate the common administrative tasks and perform ad hoc configuration changes at scale you can change adus command line interface adus tools for uh Powershell or the SDK you can perform the following types of tasks for the managed instances you can install or bootstrap applications build a deployment pipeline capture log files when an instance is terminated from an Autos scaling group join instances to a window domain and you can do a variety of tasks so it's just a way of uh to uh a way of triggering a script on your server that's what run command is Okay so with run command you can install Windows machines you can run Chef recipes or anible uh playbooks you can configure adus packages that's a very common use case and actually I show you how to to uh use run command to install the cloudwatch agent in our cloudwatch section you can install Windows updates you can execute A bash or power power shell command you can run a Docker action and there's a variety of things that you can do that they have built in as actions and I just want to show you a couple of those actions so here uh they have one to apply a chef recipe that's a run command and you have a bunch of parameters so you just put in the parameters and then it will run you can also uh do uh a Docker action and then you put in commands the most common thing I use it for is for bash commands and you can run scripts with it or you can even Define your own run command documents you're going to use on a regular basis uh that's easier for your team so there you go all right so let's talk about State manager and state manager is a secure and scalable configuration Management Service that automates the process of keeping your ec2 in a hybrid infrastructure in a state that you define so you can bootstrap instances you can download and update agents with it you can configure your network set you can join uh Windows domain you can use it for patching you can run uh script for either windows or Linux uh you can and it also integrates with cloud trail okay so it sounds uh very cool because it sounds like it somehow manages State uh but when I really investigate the service it actually isn't as um sophisticated as you might think but we'll just continue on here for a moment so determine the state that you want to apply to your managed instance determine if a preconfigured SSM document can help you create the state manager association create the association monitor an update so let's demystify State manager here and really explain what it does so the state manager is really just a a service to run either a command or automation document on a schedule so when any of us says they monitor they just mean that they keep on running the same command okay and when they say determine the state it's up to the command or automation document code to determine that there's nothing that state manager is doing to determine State it's just literally pushing the same document on a schedule to force a particular State okay so I just want to show you some screenshots here so with State manager you're going to choose an automation document or command document then you're going to associate it to your Target and you're going to run it on schedule so that's all it is it's just command uh command documents you run on a schedule but anyway uh that's it all right so we're taking a look at patch manager and Patch manager automates the process of patching managed instances with both security related and other types of update so you can use patch manager to apply patches for both operating systems and applications Windows servers uh it will support it uh but it's limited to to updates for Microsoft applications you can install service packs on Windows instances and perform minor version upgrades on Linux instances you can patch a fleet of ec2 instances on your on premise servers and VMS by operating system type and the supported versions are Windows Server Amazon Linux Amazon Linux 2 sentos Debian Oracle Linux and red hat okay oh and Seuss we almost forgot about Seuss and auntu so patch manager uses patch baselines which includes rules for auto approving patches within days of the release as well as a list of approved and rejected patches you can install patches on a regular basis by scheduling patching to run as a systens manager maintenance window task and you can also install patches individually or to large groups of instances by using Amazon ec2 in uh ec2 tags and you can add tags to your patch baselines themselves when you create or update them so there you go that's patch manager so now let's take a look at distributor for SSM so distributor lets you package your own software or install adus provided agent software packages such as the Amazon cloudwatch agent to install on your systems manager managed instances so what you can do is you can just upload your software there and or you can just use one that is already provided by AWS uh and so some really good ones there like the Amazon cloudwatch agent I've actually never used it I always just use the Run command but that might be an easier way to actually install it I'm going to give it a try next time but what you would do if you wanted to upload you could just upload an MSI Deb or npm file uh and if you're not sure what a software package is it's an archive of files that contain source code and an application configuration specific to your operating system and just looking at those three types there MSI is actually a Windows installer pack package so it's Windows updates or third party software you got Deb which is a Debian package it's a Unix archive that contains two tars one that holds control information and other installable data and you got mpm which isn't really for an operating system it's just with node but it's an archive file containing JavaScript files for node.js so software packages can be installed or uninstalled uh one time or they can be installed and uninstalled on a schedule just to show you some screenshots let's say we choose Powershell Ubuntu uh and we can just look at that package details and then we can install it on a schedule um and that's just using the SSM State manager underneath we can look at the versions for it and you can also share these packages to other AAS accounts so there you go all right so let's look at a distributor manifest file so you have the scheme of version It's always set as two uh then you have the version that you define uh for your actual um file uh then you have the publisher so whatever the name you want it to be then you um specify the types of packages because you include multiple packages in here for different platforms then you have the platform version and architecture um that you're going to have there and which uh um file that you'll want to install I'm just going to show you an example if you actually configured it there because in the middle it's generic but you'd actually have to replace it with certain words like Amazon and the version 201609 and um the architecture X 8664 there's a big list of them I think we're going to look at that in the next slide uh and then you actually have your files here and this is just to do check sums to make sure that they don't fail during installation so there you go when you're creating your own distribution a manifest we saw that you can include uh multiple packages I just kind of wanted to show you the operating systems and architectures that are available to you so I'm just going to get up my little pen here and I'm just going to Mark through it so the first thing is we can have Windows so in our manifest file we just put in Windows here and then you choose your architecture then you got Debian Ubuntu Red Hat sentos Amazon Seuss op Opus leap I have no idea what that is an oracle and and just notice that only Ubuntu and red hat have arm and Amazon Linux uh uh 2 has arm which is kind of nice if you ever want to use those but there you go all right so let's take a look at SSM documents and before we jump into it let's just Define what is management as code this is the process of handling code changes for a fleet of remotely managed instances or images to ensure resources meet the desired state so SSM documents is uh is a variety of code management documents relating to different SSM services and these documents are either yaml or Json files with parameters and a series of steps and documents uh console consolidates all your doc documents into one place so here's all your documents you can see we have some automation documents and command documents and we saw in detail in this SSM section the automation document in detail we didn't go through all the different kinds I didn't show you all the examples because they're very similar or it's just it's not worth going into all of them but let's just list them all out here so we can see all the types of management documents there are uh so the first is command document so run command uh uses command documents to run or apply and run commands on target you can also use it with State manager to apply config configuration on targets but what we really know is just running commands on a schedule Oh with maintenance Windows as well you can use command documents to apply configurations on a schedule then you have automation documents very similar to command documents you can use automation documents to perform mainous tasks based on a resource uh life cycle you can use it with State manager you can use it with the maintenance window just like command documents because they're so darn similar then you have package document we did look at this one in detail with distributor on how you could uh make your own uh distributor package uh then you have sessions document uh this is used with uh this is used with sessions manager I've actually never looked at it before but apparently it exists then you have a policy document this is used with State manager specifically for a uh gather software inventory policy document to collect inventory data from a manag instance I don't think you actually ever create it I think it's just there and then you have change calendar documents so this is uh a document used with change calendar to uh set the dates that you're allowed to use or not use um but I never had to manually make one because you just do it through the console but that's all the type of documents that we can work with it with in with SSM but we're usually dealing with either command or automation documents hey this is Andrew Brown from exam Pro and we are looking at systems manager parameter store which is Secure hle Storage for configuration data management and secrets management so with parameter store uh you can store data such as passwords database strings license code and as parameter values and store configuration data and secure strings in hierarchies and track versions if that doesn't make sense it will as we work through this here and you can encrypt these parameters using KMS so you can uh optionally uh apply encryption though that doesn't necessarily mean things are encrypted in parameter store so to really understand parameter store let's go ahead and look at what it takes to create a parameter so the first thing is the uh name and this is the way you group parameters together based on a naming convention so by using the forward slashes you're creating hierarchies and this allows you to fetch uh parameters at different levels so if I created a bunch of U parameters under the for slpr then when I uh use the API to do examp pro applications slpr it'll give me all those parameters so it's a interesting way to organize um your parameters into groups and then you get to choose your tier and we'll talk about that a little bit more shortly here and then you choose the uh type so you can have a string that's just a string you can have a string that is or string list which is uh a comma separated string you can uh encrypt your string using KMS and then you just provide the value so talking about those tiers um there are two tiers we have standard and advanced so uh generally you're using the standard one and this is scope based on regions so if you never exceed 10,000 parameters uh this uh parameter story is going to be be free but once you over go over 10,000 You're Now using advanc parameters um or if you need to use um a parameter that has a value higher than 4 kilobytes you're going to have to use Advanced parameter and if you want to apply parameter policies you're going to have to use an advanced parameter now a parameter can be applied per or sorry the advanced tier can be applied per parameter so you can mix and match these two here um so that is something that's interesting to know but one thing you do need to know about these Advanced parameters is that um you can convert a standard parameter to an advanced parameter at any time but you can't revert an advanced parameter to a standard parameter so it's a oneway process and the reason for that is because if you were to revert an advanced parameter you would uh end up losing data because um you have an advanced parameter that has 8 kilobytes and it just can't go back to 4 kilobytes it would end up truncating the data so that is the reason for that so let's talk about parameter policies which is a feature of only Advanced parameters so the idea here is it's going to help uh uh uh force you to update or delete your passwords it's going to do this by using asynchronous periodic scans so after you create these policies you don't have to do anything uh parameter store will take care of the rest and you can apply multiple policies to a single parameter so now that we have a bit of that let's look at what policies we have available to us there's only three at this moment so the first one is expiration so the idea with this policy is you say I want this uh this parameter to expire after this date and time and then it will just Auto delete the next one is expiration notice and this one will tell you uh X days before or hours or minutes before an expiration going to happen so if for whatever reason you need to take action um with that uh stored data this is going to give you a heads up the last one is no change notification so let's say You're supposed to have a parameter that's supposed to be U modif ifed manually by a developer um so they're supposed to update it themselves after X amount of days or minutes or hours this parameter will tell you hey nothing has changed so maybe you should go ahead and investigate so that is parameter policies so to understand how the hierarchy um works with a parameter store I want to show you using the CLI uh so the first thing we want to do is we want to up create some parameters so so using the put parameter command we can just Supply uh the name and that's going to be how we Define our hierarchy here I'm going to provide some values we're going to store them as strings and so when you run each of these commands it's actually going to uh uh tell you what version it is so if you keep on uh doing a put on the same thing that version count is going to go up and this allows you to access older versions um because everything in parameter store is automatically versioned so we saved we uh put three par parameters here on Vulcan so how how would we actually get all the parameters in one go and that's where we would use get parameters by path so here we can just specify planets Vulcan and all the parameters underneath will be returned to us so here they are so you can see uh we have all of them there so that's all it takes to uh get your parameters and that's how you generally uh use this within your application so we are using the CLI here so you'd have to translate this over to the SDK but this is how you would get parameters in your application hey this is Angie Brown from exampro and we are looking at Secrets manager which is used to protect Secrets needed to access your applications and services so easily rotate manage and retrieve database credentials API keys and other Secrets through their life cycle so for Secrets manager you're going to generally want to use it to automatically store and rotate uh database credentials uh they say they do API keys and other things like that but really this is where uh Secrets manager shines um so you know the database is available to us is RDS red shift document DB then we have other databases which we'll look at closely here in a second and then you have key value which they say is for apis uh so what you do is you go ahead and select the um secret type that you want to do uh for RDS red shift and document DB it's very straightforward for other database and uh other types of Secrets it's a little a little bit different so let's look at those in Greater detail here so selecting for credentials for other database you can see you can select a specific type of uh relational uh engine but here you're providing the server address the database name and the port so for the other uh three managed ones you wouldn't do that you just provide the username password and select the resource within your adus uh account uh and then for the other types of Secrets this is just a key value if you go over to plain text uh that's not that doesn't mean you can encrypt a plain text file it just another representation of that uh key and value so you can just work with adjacent object um but yeah those are all the types there just a few other things to highlight about Secrets manager and that is uh when you go ahead and create any credential uh in uh encryption is enforced so with um parameter store it doesn't necessarily have to be encrypted but with Secrets manager it has to be encrypted at rest uh and you can just use the default encryption key or use a different cmk if you want to go make one uh the pricing uh is pretty simple so it's 40 cents USD per secret per month um so some people don't like Secrets manager because it costs that and you can pretty much use parameter store uh for free but um you know you have to decide what makes sense for you and it's up a half a cent per 10,000 API calls there and then one thing to note is that if you want to monitor credentials access in case you need to audit them or just investigate uh if you have a cloud trail uh uh created then it will monitor these secrets for you so you can investigate in the future probably a good idea to turn on cloud trail if you haven't so the huge value with Secrets manager is automatic rotation so you can set up automatic rotation for any database credentials so any of the manage services and even the other databases there there's no automatic rotation for uh the key value uh secret type so it's just the database stuff um so so you know once you go through the wizard uh for any of those uh steps you're going to come to this automatic rotation so you're going to go ahead and enable it you're going to choose your rotation interval uh which can be up to 365 days so up to one year so just to show you here if you expand it you have 30 6090 and you have custom if you go to custom you can set it up to one year uh and the way Secrets manager works is that it just creates a Lambda function for you uh and so some would argue that if you wanted to use um parameter store and just make your own Lambda you wouldn't need to use Secrets manager and that is 100% true um but you know you have to know how to do that so you know decide whether you want to put up extra effort so you don't have to pay for Secrets manager um but yeah it will go ahead and do that and there's one option down below here and this allows you to select what um password you're going to rotate out because you might not want to rotate out this password you might want to rotate out a developer's password this connected to the database um so you know that's another option for you there um so for the developer associate it's good to know the CLI command so let's just look at a couple for Secrets manager the first one uh being ad Secrets manager describes secret um and this is going to describe information about a particular secret the reason you'd want to do this is so that you could find out what version IDs uh are are there because you might want to specify a specific version of a secret uh and then you get some access information such as the last time was changed or accessed so that might be one precursor step before you actually try to go get the secret so the other uh CLI command we need to know is get secret value and this actually gets the secret so you can see you supply the secret ID and then the version stage and if you don't provide virgin stage it would just default to Adis current which is the current version but if you use the prior step there you could use a different uh version stage there and so using uh this CLI command you can see that we have this secret strting string and that is that is what's storing our credential information so in this case it's just a key value um uh credential or or secret and so that's what we're looking at here so to really help make sense of Secrets manager because it's not always clear you know how do you access the secrets versus how does the database access it versus how does the app access it um you know I made an architectural diagram here for you so you know the first thing is that You' use Secrets manager and uh we'd set up rotation with it so every 90 days it's going to rotate out that password that is stored in RDS so the password is actually in RDS um and then secret manager can store a version of it as well but what it's doing is uh the Lambda probably set up with a cloudwatch event and every 90 days it's going to say okay run this Lambda swap out that password so that's how the password gets swapped in RDS but how does the application access it well you'd have your web app running on ec2 and You' use the SDK which is the icon there in the actual ect2 instance here and you would uh make an SDK call to Secrets manager and get the database credentials and then uh you would use your web server with that new uh password and username and form a connection URL which is just a it looks like an HTML URL but it's used to connect to um postest databases uh not all uh relational database types have a connection URL so you might just have to provide the username and password but the point is you are making a connection to um RDS now from the developer side the way they would probably use it um because they're in development they just want to gain access to the um the database that's in the cloud they're going to use the CLI to obtain the database credentials and we just went through the CLI command so you have an idea what they would run and then the way they connect to the database would be using a database manager so you could use tables plus uh if you didn't have a a database manager you could just go ahead and use a terminal and and and connect that way there so hopefully that makes it very clear in terms of um how you can use Secrets manager uh for all these use cases hey this is Andrew Brown in this video we're going to take a look at Secrets manager so Secrets manager is a service that's similar to uh system parameter store um I kind of feel like it came about because parameter store was very popular and it was went oh why didn't we charge for this and so they made this additional service which is easier to use um and is nice but you could use parameter store more or less for it um though adabs has made it in certain areas where you do have to use Secrets manager and I again I think it's just to kind of force you to utilize the service which costs money I noticed that the price came down so used to be a dollar per secret now it's down to 40 cents per secret so that is really great there's obviously API calls that we have to be consider concerned about nice that we get the 30day free trial let's go ahead and um set this up so we'll go ahead and store a new secret um I'm going to go over to my examples repo and we're going to set up something programmatically to utilize secrets I'm going to open this up in git pod here today you use whatever you want to use Cloud9 GitHub code spaces your local development environment but understand you'll have to configure your adus credentials and install the adus CLI and all those things like that so let's take a look at what options we have so we have credentials for RDS database credentials for document DB database credentials for red shift data warehouse um credentials for data other databases and other types of secrets so when the service first came out it wasn't all of these it was just RDS or it was this and this was the thing that uh was basically the competitor to um parameter store the other service so let's start with this thing and then we'll look at uh setting up credentials I'm pretty sure we do this in one of our uh database sections but we can do it again here in a slightly different way and see how that goes um so the idea is that we can do ke Value store or plain text here and then down below we obviously have an encryption type so that is really nice there I think that we should programmatically do this so in our um UHS repo here what we'll do is go ahead and make a new directory I'm going to call the secrets manager and we will CD into that and I'll just make a new one here this one will be for basic and we'll CD into that and then I will just touch a read me file and on the left hand side here I'm looking for Secrets manager we will expand that go nor basic and we'll go over to the a CLI I don't know if you can create anything with Secrets manager via the um cloud formation we might actually be able to do that we could do that too instead of using the CLI um I know for parameter I would definitely utilize that so maybe that's what we'll do here today because then if we use Secrets manager here then maybe we can bring in a database and and connect it that way so I think that's my decision here um I'm going to need uh a couple of files that already exist I'm going to go to EBS basic and just copy these ones okay and then we'll go down below so if we're going to do a database no I'll leave it alone we'll just copy this for now and I'll paste it into here whoops and paste it into this directory allow did it actually copy I don't think it actually copy we'll try this again right click copy copy right click right click paste there we go okay so I change this over to uh Secrets manager I'll just make it SM for Secrets manager not to be confused with SSM simple systems manager will be Secrets manager stuff all right and then we'll go down below here and we don't need most of this stuff so what we'll do we'll just clear it down to the ground and I'll get rid of these other things because we don't need them and for now I'll just comment out parameters and we'll go ahead and grab this yaml code it says it as an example so let's go down to the example and grab this instead and I'm not sure if this is going to be a secret for a database or a secret for a generic one it looks like this is a generic one so that is great so just go my secret and what could you utilize for this well um if you had application configuration and you wanted to pull that down that data you could utilize that there is another service called app config which is more about a deployment pipeline around uh rolling out changes and having feature Flags it might integrate with Secrets I don't remember seeing that as we were utilizing it but um I think Secrets is fine if you want to uh again utilize this for application configuration what I'm going to do is take out the tag and we don't need that and I don't know if we need a name so let's go take a look here up to properties and the name is not required so let's get rid of that do we need a description the description is not required but I'll just say I'll take it out as well because think of like if you were somebody trying to get into somebody's account if you told them exactly what it was then you know maybe that wouldn't be good if you had a malicious actor in there so maybe that's a good idea that we don't even specify it there I'm going to go ahead and copy this and paste this in here so that we can uh bring in this uh type here right underneath you know I do that all the time um so we have generate secret string let's go take a look at what our options are so we have secret string and generate secret string so I think secet generate secret string is probably going to be for databases and the other one's not going to be that notice that it doesn't say like database anywhere here uh so just understand that the UI is um obscuring what's going on here as it's being utilized and actually might be doing an additional step because notice that it's showing a database here so I think what it's doing is it's generating out a secret and then establishing the connection so this is obviously doing more than uh what the API is doing but let's read these two so the text to encrypt the store in the secret we recommend you use Json structure key value pair to generate a random password use generate secret string and the other thing I'm wondering is that maybe when it's utilizing um this it's providing a very specific Jon structure that's probably what it's doing I I think that's what it is but anyway we'll keep reading this so the the text encrypt um to generate a random password use generate uh secret string instead if you emit both generate secret string and uh uh secret string you can keep an empty secret when you make a change this property etc etc so it looks like there are it's one or the other um and so we can use this one first I guess and let's take a look at how this works so uh secret string template generate string key so let's go into this one and read a about it so we'll go here I'll grab this link here and yeah so we're here so what we have we have exclude characters so it's basically saying like what kind of rules do you want to make to make your password and then we have our password length specifies whether to include at least one upper no no a template that generate uh generate that the generate string must match okay uh when you make changes to this property a new secret version is created a template that it that the generated string must match how's that a template that's really confusing uh generate string key let's take a look at this the Json key name for the key value pair so what I'm thinking is maybe what it's doing is it's going to put password here is password password um so let's see what happens if we do that so I'm going to go ahead and go ahead and deploy this and we'll make our way over to cloud formation all right and from here we'll go into this and execute or change that and we'll give it a moment to finish and uh I'm just kind of wondering what's going on here also why is it called us R 53 servers did we forget to rename the stack let's go back over to here no this is the stack name I deployed it from here uh what what's going on here and 24055 okay so that's oh you know what it is I'm in the wrong region well then what did did I just create did I just accept the WR did I just accept a a chain set from a long time ago well that's fine I'm going to go ahead here and just uh accept um where is it itus EBS basic itus okay what is this one called hold hold on here oh uh is this right did I rename the wrong one you know what it is I rename my wrong one here hold on here I'll just close this one out and I'll go ahead and paste this in here and I'll go back over to here that's why you got to really pay attention to what you're doing unlike me just clicking away here um but we'll go back here it'll just take a second to fix let's refresh this yeah that's fine okay so we'll try this again so now we have this and it shouldn't really take that long so come on let me execute it execute change set and so we'll just wait for this to finish okay all right so that created very quickly of course because it should and let's go take a look in our secrets and see what we have so I'm going to give this a refresh and make sure you are in CA Central or whichever region you want to deploy your stuff in and I have uh this one from earlier so this is an old one I'm going just get rid of that so I don't have to look at both of them uh seven apparently we can't delete them right away um and so this is the secret that uh we created let's take a look at its value so we'll retrieve the value notice we have the username and password so it inserted the key password based on what we called it and then gave it a random value now if you wanted to use this you'd have to Pro programmatically integrate it and I'm actually surprised that they actually have the code right here so maybe what we'll do is we'll go ahead and make sure that we can utilize that and of course I like Ruby so we'll use Ruby here today so I'm going to make um I'm Type in a bundle of nit down here because if you're going to use it you should know how to programmatically uh work with it otherwise what's the point um so we'll say gem probably ads SD Secrets manager imagine that's what it is let's go take a look at the code example first time I've seen them provide a code example it's really nice um we'll go ahead and just say main. RB here and I'll paste this in here I was just going to grab this part so that I can go back here and make sure this is in place all right and so we'll do gem ox uh gem pry just in case we want to do pry here I'll do a bundle install of course if you are not using this environment then you might have to install Ruby uh or with any other language if you are working with this stuff here so let's go back over to main. RB and this one's already for C Central one which is perfect and it even populated the secret ID in here so it gets the value and so I'm going to put a binding pry here so we can see what it returns and I'll just require pry which will put a break point in here and then will'll do bundle exact Ruby main. RB and hit enter um and I mean it ran the code but it didn't stop where I thought it was going to stop so I'm going to do um oh you know what it's not getting the secret okay so it's weird that they don't give you all the code which is fine but now it's stopped and so it's stopped right here so we can type the word secret down below here secret uh secret I had toit CU there so now this is what we get back so it's not 100% complete what we'd have to do is json. pars on this and then we get our data here and then data would be with the key password and then we could put that and then we could get the username okay so I'm going to go ahead and stop that there we'll type in clear we'll try this again so now we get our username and password so that's how you would Pro programmatically work with it but we just generated out of secret let's say we wanted to do it the other way which is to just provide our own thing that we want to encrypt so I'm going to go back to our template file here and I'll just say my secret generated and then my secret and just have the regular kind provided and we will go back into here and so for this one we want to go to secret string it's as simple as providing the secret string so we will just do this and I'll just say my super secret oh we'll do what we always do which is c t testing 1 2 3 4 5 6 exclamation mark so either that or we don't have the 456 on there but anyway we'll go ahead and deploy this now deploy and we'll make our way back over to Cloud foration we'll give this a refresh and a refresh and we'll go over to our change sets we'll go ahead and execute that and so now we are just waiting for that to execute so so it was pretty quick last time I almost feel like I don't have to uh do anything here but uh there we go now it's done so let's go back over to Secrets manager and now we have two we have the provided and the generated one so this is our provided one and notice that we have it here so that is pretty straightforward so now the next question will be how do we integrate this with um a database and so we'll do that in the next video and again we'll do at the lower level here so we can exactly see what is going on I'm just making I did not rename anything else here so let's just say uh basic um SM here and just to clean this up we'll go ahead back over to cloud formation tear down our stack and I'll see you in the next one as we continue on uh working with a database okay ciao hey this is Andrew Brown and we're continuing on learning about how to use Secrets manager and one thing I want to point out is that um uh we passed in we hardcoded in our our secret here and I just want to point out that that's not something that you'd want to do um even if you pass in parameters in here I suppose what we could do is we could mask it um obviously you should use the generated one as you can but let's imagine that we had to pass something in here uh how could we do that well um let's go take a look at parameters and see if they have one that masks information so we'll go ahead over here I guess this is kind of an in between video instead of us just doing the the database one but we'll go parameters and look up Cloud information here and there should be an option to MK the data at least I remember uh that being there it's like mask or something no Echo and so all no Echo will do is so whether to mass the parameter value to prevent it from being displayed in the console command line or API uh and then Cloud information returns a parameter value masked as asteris okay okay so let's go take a look and see what happens if we do that so we'll go over to here and I'll just say my password and I'll put no Echo true here and this will be type string and then we'll in whoops we'll indent these here I'll go down below and we'll just reference the string here and just say my password um I'm just doing this in the basic still by the way so you know what I can do hold on here because I I I'll just separate this here and we'll just go ahead and make a new one here we'll just call it mask and I'll just copy uh this stuff over to here copy right click want the deploy and this here copy this again and we'll paste this in here I'm not sure what this is here it's like a junk file I'm just going to get rid of that not sure how that got there but um I'm going to go back over to here and uh I think that this is probably the original one no that's copied okay great so I'll go back to this one and I'm just going to undo what was going on there we go and I just don't want to muck with our old one and I'll just call this one uh mask mask and we'll go down below here so yeah we'll say my password string uh true we're referencing it now and the key thing is that we're going to have to put an uh parameter overrides in here so parameter parameter overrides and this will be my password and we'll just put um we'll do it backwards we'll do exclamation mark uh 654 321 g i n i in testing Sorry whoa that was quite the cough um so I think it'd be n i doesn't matter if we really do that backwards but I think that this should work for um the parameter overrides assuming I spelled this correctly if it's not it'll tell us so we'll CD out of this and I'm having some unresponsiveness I'm just going to give this a Reload sometimes this happens with get bod unless it's my internet sometimes my internet goes on me here and it looks like it's my internet so just give me a moment to reconnect okay all right so my internet is back and I can't remember what I was doing I think I was just trying to do a deploy yep we'll go ahe and try this and we'll go back over to cloud formation here we'll go to our stacks and make sure we're in the correct location CA Central this one is saying Basics I'm not sure if this is the correct one as I Chang this to mask so I am in the wrong spot okay this time I'm on top of my mistakes and we'll go ahead here uh and deploy again well and we will go change sets execute all right and so that is going to go ahead and create it's usually pretty darn quick and we can make our way over to Secrets now and we'll give this a refresh here and we should only see one secret why are we seeing two because this is in The Mask file here we're in The Mask directory and we go into here oh do I still have two in here okay well I'll just take this other one out I just don't want to get confused so my secret uh passed not a big deal we'll just deploy this again and we'll go to change sets here and execute the change set and we'll just give it a moment okay all right so it looks like that uh this one is now deployed we'll go back what we want to see is the echo thing right so we want to make sure that there's no chance that we can see that secret um so go to parameters here and notice that it is obscured so we cannot see what the values and so that was the key indicator let's go over to uh Secrets manager and take a look so we have our secret and then we can see our actual secret here um and I guess the other thing is that it could show up other places but you know if you can then try not to pass uh sensitive information even if it's mask it has no Echo into Cloud information if you can um but yeah that's all I wanted to show in this video and then I guess in the next video we will um tackle uh looking at uh doing a database connection but before we move on I just want to go ahead and tear down this stack all right and I'll see you in the next one okay ciao hey this is Andrew Brown and we're uh back onto Secrets manager and in this one what I want to do is utilize it for an actual uh database so I believe the way that it works is that it's going to need to have a very particular structure in order to work with it so obviously if you utilize it this way it's a little bit easier um also notice that we have this optional configuration rotation option here but I think that we go and look at cloud formation I don't think it's going to show all that stuff so notice that we have whoops notice that we have two things here we have a secret a secret Target attachment and then the secret itself oh there it is rotation schedule and resource policy so I guess they have these as separate resources um so what we're going to do to get this going is we're going to go and make a new folder and call This One DB and I'm going to go copy these two files down for the mask and we'll paste them in here and just so I'm not getting confused with old files because it does happen I'm just going to open these up new tabs and this one here is going to be um uh I want the deploy here this one's going to be smdb smdb um I'm not sure if we want to pass in the password this way but I I actually kind of prefer to autogenerate out the password oh what's going on here it's just really dry in my office so my uh throat is throwing me for a loop but uh what we'll do is go over to the basic template and I want to just grab this generated one out that is more preferable and we'll go back to our one down here below I go ahead and for now just get rid of parameters so we have our secret that's step one um and we don't know the structure yet but I know it's going to be something like this and then the next thing I want to do is I want to go back over to cloud formation and then we'll obviously want to have a Secrets attachment so we'll go ahead and grab this maybe there's an example down below so here this one's specifically for a red shift cluster I mean if this works that would be great I'd rather just do that um and so yeah maybe if this just works let's go ahead and just do a red shift I don't know red shift can be kind of heavy to deploy so maybe not maybe we'll do a database instead but at least we can kind of see how it's being utilized but you can see um it's just passing them in as the username and password here um and then we have our attachment so I'm going to go down here below and we'll just paste this in as such and we'll just say um DB attachment and then we have our secret ID so this is going to be my secret and then we have our Target but we need to figure out what the target type is so we'll go back over to here I'm just going to go and grab this link so we have it and I want to see the target type I imagine there's probably one for a database so that would be something for post cross your might well that's more appropriate to what we would like to utilize there we go so now we need a database so let's go over to RDS yeah we have that in this section here and so we do set up a database and there are quite a few steps here but I'm going to go ahead and grab um I basically want this whole template really so I'm going to go ahead and just grab this whole template I guess and we'll just have to merge them together let's go ahead and copy this and I'll go down down below there we go um and I'll just close this out and close this out so I need to merge these together so we'll take these whoops these two parts here what is going on oh I lost my database I am not this clumsy it's just working in here not in Vim uh causes me these issues that is the price we pay when we want convenience of cloud Services okay so um I'm going to take off or take this part here and hit X which is a Vim command sorry and then I'm looking for resources I'm going to go ahead and paste this above there we go and then I'll just take these out of here I'll just say oh this says RDS instance with Secrets manager so we were using it before but we weren't explicitly attaching it so the idea here is that maybe what we might want to do do I'm going to undo this I'm sorry but maybe what we should do is we could h no we could do in one go I think I think it's fine I was thinking maybe we'd have to create the RDS database first as a separate cloudformation template and then join them together but I mean they didn't do that for red shift so seems fine but clearly there is a convenience here uh where we can use Secrets manager for our database uh somewhere here Secrets oh you know what it is we just passed it in so we actually didn't generate it out we we just passed the AR in there so then where does this get used secret AR and it's used here to get it there so I guess we'll take that out and we're basically just generating the thing out now um so I'll close out this tab so we can see what we're doing I'll just split it across so that we have a bit more room to work with uh so yeah this one says resolve Secrets manager RDS secret AR stuff but we don't need to do it that way because we directly have it so if we go back over to our example down below here this one references it directly oh no it still uses it but it's just passing in the value right here okay so what we can do I don't think it's the target it is the Yeah the secret itself so we can go ahead and where is it here resources where the heck is our secret oh no did it get rid of it undo undo undo undo undo there we go what a pain this is we'll cut this what the heck this is driving me crazy okay um um what I'm going to do is I'm going to take this part out and I'm going to go over to here and grab this whole template copy it here and then I'm going to paste it in the top and then I'll go and grab these outputs and then I'll put them down below here there we go finicky finicky finicky um so we want to reference our secret here and so this will just be my secret but I'm not exactly sure what that looks like so I'm going to go ahead and grab this one here so it's a little bit easier to look at and so this one says function sub that other one has a join we could probably just use this one and then just put in my secret my secret and this will be password like this and I think we could also do that for for the um the username as well so what we could do here take this out and put this here and then this one would be username I think that would be fine incorrect type expected string I'm not sure what's wrong with this it seems like this should work no problem because we copied it from here so because I copied it from here I'm just going to ignore it and assume that I am doing it correctly as I do not trust what it is saying um yeah so the other thing is that our deploy script might we might need more stuff in here so I'm going to just open this up and I'm go back to the RDS one here and yeah this one has a bunch of overrides so I'm going to go ahead and grab all these overrides these ones are specific for CA Central 1 so you might have to go into V uh your you I mean you can't use mine but you'll have to go into your VPC and just copy out those values um so the only thing that's not in here is the RDS secret AR so we'll just take that out and I mean we have username here but we're not going to set that there so really all we're setting here is the subnets and the VPC and the web server group ID which I don't even think I really need this so I'll go back over to here here for web server group ID because I'm just going to open it up to everything here web server group ID and instead of this I'll put something else um I think that we do this in elb uh we have an example there so I'm going to go over to elb here and in this template I believe yeah we just have the cider so I'll just open this up everywhere which you know not the best practice but that's fine so we'll do that and just thinking what else so we don't need the AR we're definitely not using that anymore and then we have the secret being generated out here and it has a user it's just test user which is not the best name but that's what it's going to be called I'm going to take the hyphen out there just in case it doesn't like that and yeah that seems fine now there are other things in here so we have rotation schedule sets the rotation schedule of the Lambda rotation function so this one is if you wanted to hook up a Lambda I don't personally want to do this as this is a lot of work but uh the idea is that you could have your Lambda and then your Lambda would trigger it and uh it would transform it or do something with it okay and then I guess there's a resource policy attach resource policy uh policy to a secret this is optional if the secret is already a PO a resource policy attached you must first remove it etc etc so I don't plan on doing that here today but I will go here and just make sure that the this information is up to date so I'm going to go over to my VPC here and I'm going to go to my vpcs so I have one here I'm going to grab this here this is 08f and I believe it's the same so I think mine is already ready to go yep but you might again have to swap yours out here in your subnets so I think that is fine I'm going to CD into um the DB directory here and we're going to go ahead and deploy this considering all the changes we make I'm expecting something to go wrong which is totally fine so go up to our template here and I'll get rid of username here and we'll try this again and now it says my red shift cluster because we have something that we copied that shouldn't be there so red shift um the target here would be the RDS instance I'll go down below here and swap that out place it in as such also we can use the shorter hand version here all right we'll try this again unresolved resource dependency username come on I thought we got rid of that username it's being referenced here I'll just take it out of here there we go does this work come on you can do it and it's created the chain set excellent so we'll go over to here and hopefully we don't run into any problems refresh this DB change sets execute and now we will take ourselves a break while we wait for this to deploy all right so looks like it is deployed now let's take a look here and see what we have for our secrets so if we go in here we have my secret and we can retrieve the secret and we can see we have a bunch of stuff set here so we have password database engine Port uh DB instance identifier host and username and it's interesting because we did not set all those things we just set the username um and the password but you can see that based on uh different types of configurations you'll need different things and so I'm not sure if the attachment is the one that generated that but all I'm saying is that I know that this stuff might vary based on uh databases so if you want to man to configure this you might have to look it up um but the attachment is the one that is doing there can we go test to see if our username password Works probably I don't really care to do that because I know for certain that it is being set correctly um otherwise it wouldn't have been able to create the cloud formation stack but hopefully that gives you an idea of how this works uh rotation is disabled so we could enable it here you know but we still have to provide it um a Lambda function so I'm not sure if they create those now but before used to always have to yeah see here use a rotation function from your own account or create a new rotation function so I'm not sure if it just sets it up and you'd have to do more with this but we're not going to do this here today because that's just a little bit too much work um but we'll go ahead and tear this down so we'll say delete and we'll consider this one complete so I'll see you the next one okay ciao P Brown and we are taking a look at elastic load balancer but let's first talk about what is a load balancer so it is a physical Hardware or virtual software that accepts incoming traffic and then distributes the traffic to multiple targets they can balance the load via different rules these rules vary based on the types of load balancers so elb is a suite of load balancers used to balance or distribute traffic to multiple ec2s ecss fargate e eks instances um I don't have the text here but generally they are uh bound to a specific VPC so it's not like these are Regional load balancers but we have the application load balancer also known as ALB and you really want to remember these initialisms because they're used a lot and people just say ALB NLB stuff like that okay but anyway ALB it operates on the OSI layer 7 uh application layer so this is HTTP uh capable of routing based on HTTP information can leverage W for additional uh security we have NLB so this operates on the OSI layer three of four so that's t CP UDP designed for large throughput of lowlevel traffic then we have the Gateway load balancer this is a a newer one also know as gwb routes traffic to Virtual appliances before traffic reaches its destination useful as a security layer for traffic in transit and just to describe it a bit more the idea is that you are sending it to um uh virtual machines that are going to then send it to the Final Destination so it's just a layer of stuff we have the classic load bouncer CB operates on the OSI layer 7 and 34 doesn't use Target groups directly attached targets previous generation of load balancer that is only used in Legacy cases where clients have uh not moved I wrote no moved but this should say not moved sorry about that over to ALB or NLB so there you go let's talk about the rules of traffic for elbs and just understand the components of how traffic flows the first are listeners so this these are for incoming traffic that is evaluated against listeners listeners evaluate any traffic that matches the listener's port commonly they listen on Port 443 or Port 80 so whether it is htps or HTTP for classic load balancers ec2 instances are directly registered to the load balancer uh we have rules this is only available for uh application load balancers not to say that there aren't rules for NLB it's just that the rules you don't change any of them like There Are Rules just a very simple rule and you can't you there's no customization listeners will uh uh then invoke rules to decide what to do with the traffic generally the next step is to forward traffic to a Target group uh but for albs we have advanced rules around HP request information which is only for ALB since it operates on the OS layer 7 I'm not sure why the seven the os7 layer let me just write that back in there there we go Target groups not available for classic load balancers are a logical grouping of possible targets such as a specific ec2 instance IP addresses clb just directly associate targets to the load balcer so just here again a diagram to show clb there is no target groups they just register them directly but there you go okay all right let's take a look here at ALB and by the way if you're seeing Graphics where some times the icon is purple or sometimes it's orange just know that ad us changed the color on me and I'm not going to go and just fix every diagram every time ad us decides to change the color so there will be color discrepancies throughout the section but anyway ALB is designed to balance HTTP HPS traffic it operates on the layer seven of the OSI model it has features called request routing those are those Advanced routes we talked about which allows you to add routing rules to your listeners based on HPS protocols supports web sockets HB 2 for Real Time bidirectional Communication apps it can handle authorization authentication of HTP requests usually will happen via a custom Lambda uh can only be accessed via a host name it if you need a static IP forward it to you can forward it to an NLB to then ALB um so that that's key thing you don't get a St IP with it uh we have uh waffs you can place in front of them which gives you the O oos protection OAS is a um a a type a suite of OAS is an open source project for security and they have um OAS top 10 which is what this is covering gu which covers against common uh threats via HPS you can use ACM to attach a certificate which would give you a custom domain and SSL for that custom domain you can use Global accelerator in front of it to improve Global availability you can put cloudfront in front front of it to improve Global caching for common HP requests usually read only requests uh you can use Amazon Cognito to uh use authentication for HTP requests probably through Lambda use cases here are microservices and containerized apps eCommerce retail websites corporate websites and web apps SAS apps very good load balancer you'll definitely be utilizing this one okay hey this is angrew brown in this video I want to take a look at utilizing application load balancer not a very particularly difficult service to use um as per usual will be doing this programmatically because I think that is probably the best way to learn this we're going to open this up um our it was examples repo again you can use G pod you can use whatever you want but you'll have to configure those uh base configurations which you know like AB CLI or whatever you'll have to do that on your own um but this already is loaded up and it's all because of this nice dogit pod gamble file um so what I'm going to do is make a new folder here I'm going to call this one um elb because we might end up doing more than one type of look balcer but for now we're going to focus on application load balcer I'm going to make a new file here we'll call it template. yaml um and we'll focus on making a CL formation template so we'll look up ALB um f CFN and so that will bring us over to load balancers before uh version two there's version one which is the classic load balancer when version two came out we had NLB and ALB so application load balancer Network load balancer um but these aren't too complicated to configure so what we'll do is go ahead and I don't think we have an example but let's just go and copy this currently and go here and just type in resources and we'll paste this in below say allow so that is good there just going to split this here and I think I have um some uh templates lying around here I just need to grab these two lines here so just get those in however you need to get them into your template and I'm just going to call this a Al and ALB serving up a web server all right so let's go walk through this and see what all the properties are so the first is enforce Security Group inbound rules on private link traffic indicates to whether inbound Security Group rules for traffic to an NLB we are not doing that so we will just take that out then we have IP address type the possible values for the IP address um it's not required so we're going to to um leave that out um or we could just explicitly say ipv4 uh we could it could also be dual stack doesn't really matter but I'm I'm going to leave it as ipv4 as I don't really care about dual Stacks since every time I use IPv6 it doesn't work as I would hope it would guess question what is the default you can specify dual stack for a load balancer with a UDP TCP UDP listener okay um so then we have our load balancer tributes I feel like there's going to be a lot here so we'll do that next um I'm if the name is generated automatically we'll do that so we'll just comment that out and let it generate out a name for us we have a scheme so the nodes of the interfacing load balancer have public IP addresses um I'm going to set this to be internet facing because that is what we want here today okay security groups so we'll have to Define our security groups here in a moment subnet mappings so the IDS of the subnets we plan to use let's go over to our VPC I'm in CA Central 1 and I'm going to go ahead and grab a few generally you'll want to have three um three uh to utilize in your VPC so we'll go to subnets and there are there all there is is three so we'll go ahead and grab this one and we'll grab the next one and we'll grab the next one so that is good I'm not sure if we need both so we has the IDS of the subnets you can specify only one subnet per a you must specify either subnets or subnet mappings not both um you must specify subnets of at least two azs which we've done you cannot specify an EIP that is totally fine uh what is the difference between these um well I'm going to stick with subnet mappings as I'm not sure what the difference is between subnet mappings and this one here so well this one's an array of subnet subnet mapping so this might have a particular structure oh it does okay well let's just go with subnets because that looks like it's just an array of strings which is a lot simpler subnets there we go so not going to worry about tagging right now then we have our type this probably determine if it's an NLB or ALB so or a Gateway apparently so go ahead and do that place that in here and I'll bring this up to the top um what we really need to configure are these two values I'm just bring these down here property scheme is not allowed in the AML schema yes it is it's part of the docks here I think it's just complaining about um something else we need to uh Define the name up here first though we'll go ALB and do that that resolved a lot of those issues um let's go into our uh next part here we do need a security group before we do I'm just going to copy this link and place it here so we can quickly gain access to this just going to look up Security Group uh CFN I'm sure we already have something lying around for this but I'm going to go ahead and just grab whatever I can here this one looks okay okay we'll go ahead and grab that one uh we'll paste it in below this will be ALB SG Security Group okay um and I'm going to go ahead and grab this link as well here so I'm just going to get out of here so we can get that this will be allow instances to connect allow SG of or just be like ALB SD we'll just put that there um for Ingress rules we want allow Port 80 that is totally true and Port 80 on the way out um we also want this for 443 I mean we might not necessarily be doing that right now but just in case we'll go ahead and do this and we'll just say 443 I believe that is htps Port I always get mixed up I was think it's going to be 433 but it is 443 and I always get it right but I always think that I get it wrong um and so we'll have it for that uh we'll do this as well so we're covering our bases so that looks pretty good to me so this will allow traffic in and out uh no problem there and this needs a comment in front of it that's why it's complaining we're also going to need a security group specifically for why that's still complaining we're going to need Security Group specifically for our um our instances that we're going to launch up but we'll get to that in a moment okay so yeah again not sure why this is complaining we'll just take that out for a second yeah again not sure why let's look into this uh load balancer tribute let's go here I'm just going to grab this link as well and just saving our time to find docs later on if we have to click through and here it says specifies and attribute for the application load balancer um deletion protection load balancing cross Zone access logs I mean access logs would be probably good to set I'm going to leave that alone for now it's kind of a more advanced topic http2 we to utilize it nothing super interesting popping out at me it'd be nice to attach an SSL certificate here um SSL or TLS or ACM not exactly seeing that they're going to go back one step here ACM where is the ACM if you're wondering what I'm talking about this is the ability to um attach a security group or sorry a um a certificate so it's it must so it allows us to have a secure connection that's probably going to Bean the listeners I don't exactly remember but uh over here yeah see this listener certificate so that's probably what it is we're also going to now need um so there wasn't really anything to configure here so I'll just take this out um and also we should probably set our security group here so that is something else that we'll need to do I'm just going to open up this L I'm holding control so it will open and click it's not working here today I'm not sure why we still have it open here here we're going to have to go down to our return values we're going to have to check what this wants an ID of the security groups and if we get the reference that's going to return back the ID so we'll go here and we'll just say ref ALB SG there we go um so we have our ALB we have our security grou for ALB we're going to need um uh listeners and Target groups so the next thing I'm going to do is to find a listener so we go here we'll have a listener and I will go down to examples and we'll grab one here we'll paste it in and that's what we need is an HP listener so that looks fine to me um let's see what it's doing so it's saying the default action is to redirect to htps which actually is a good idea it wants to reference our load balancer here which is our ALB so that is fine um so I'm fine with this one actually this seems good to me we are going to need an HPS one though I wonder if they provide that here no let's go ahead and just right click here and see if we can get that quickly create an htps load balancer I was just hoping we get the cloud information for it no but uh that's fine what we'll do is we'll just go ahead and say htps listener here this is going to be very similar but we'll have to look into the actual actions here so we'll go and back to here did we paste that link in here for ALB no we did not we'll just click back onto here we'll copy this and we'll just paste that on in here and let's go take a look at our default action which is just an action by the way so we have a few options uh there was the redirect config that's not what we want to do want to send this over to a Target group um I don't know if we'd have to do that under forward let's go take a look here information for creating an action that distributes the requests among one or more Target groups for nlbs you can specify a single Target group specify only the type is forward if you specify the Ford config and Target group Arn you can specify only one so I'll open this up and we'll take a look maybe this is a very simple configuration so this has Target groups then why do we have this one here um the target groups only when the target group Target type when the type is forward and you want to route a single Target group I mean that's what I want to do it's just a single one I want to do here so I'd rather keep it simple and we'll first I guess set the type here since we definitely need to do that that's going to be forward and then I'll go ahead and grab this target group Arn and we're going to just assume that we already have it we don't but we'll just say TG TG for Target group I'll just write the whole name in here Target group and I assume that's what it is we'll have to uh Circle back if that is wrong but let's just take a look here so we have order not required for config not necessarily required yeah simple as that so now we need our Target group so here is our Target group and we'll go down here and go to examples um I'm looking one for something that will refer HP like for what we want um these are for lambdas but we can tweak this it's not a big deal I'll go ahead and just paste this in here so Target group uh we do want a health check that's important and let's go grab the link for Target group so I think this is the page for it here yep I'm just going to go ahead and grab this and then paste this link here and we'll go up to properties so we definitely want a health check I suppose we can leave the defaults it's totally fine I don't mind those defaults and so yeah health check health check health check that's fine ipv address we'll leave that alone matcher we'll leave that alone as well name it's not required excellent Port the port which the target receives traffic on unless you specify Port override when registering the target if the Lambda is ETC okay um we'll go to protocol so I'm just going to pull up an example of one I already have so I can just kind of speed us through this quicker okay so this is not a a public repo but um it is just uh I have a lot of code laying around for over the years and um this is a very standard setup and it's going to help us just verify so we can just save some time here so we have this HTTP listener we're go back to ours and um it says a default action the redirect config is oh it's up here sorry so just want to confirm these here are correct it looks like those are set there so that is good and then we'll have a another listener which is for htps so this one we're going to need a default actions which we are filling in here um also hold on just a second if we go up to this one this one has a hyphen on it oh that's our HPS listener okay great so again just looking at our listener again um I think maybe we're supposed to do this here because I think that takes an array even though we only have one action we go up here yeah so this one has an array so that's going to save us some trouble here 443 did we specify the port we did not specify the port here that's probably important 443 and the protocol so we'll go next here to protocol HPS so that looks good here and then we'll end up adding our certificates so we'll come back to that in a moment uh right now we're focused on the target group and you know again this is just to save us a lot of time because this stuff can be really finicky to um configure so I mean I do do the name like this but we're just going to ignore that so we have health check enabled we do we'll take this out here um health check path will be whatever we wanted to set it to so I'm just going to explicitly set this and it's just going to be that we have health check intervals sometimes you want to fiddle with these and uh because if the health checks take forever it can be really annoying we're going to go ahead and grab the next part which is Timeout seconds we'll grab the next one here which is threshold count and we'll do this so we have 10522 so this is 105 two two we'll just read this quickly so how often uh to check your health checks how many seconds till timeout how many times healthy how many times unhealthy okay pretty straightforward Port is 80 so that's what I was I always kind of forget um but yeah it be Port 80 because the app on our machine would be running on Port 80 as opposed to um HPS so termination is going to be uh SSL termination is happening at the load balcer level uh then we need our protocol so that's going to be HTP protocol Pro to call HTP and then we have to reference our VPC bpc ID is that what it says it does okay and I'm just going to go ahead and hardcode that in you could pass it as parameters you know again not too worried about that right now but normally you'd pass these as as parameters and not hardcode them in so we have that set up um what about the target type Target group okay so go back over to here and I guess we're not specifying any Targets explicitly um it really depends on how we want to do this so like my goal here is to set up ALB I'm not really worried about Auto scaling groups this one actually utilizes Auto scaling groups and so the auto scaling group is probably um specifying that there so this is where we're going to deviate from this one and I actually want to explicitly set a Target so we have Target group attributes and then Target uh targets and Target types I'm going to go down to Target type the type of the target you must specify when registering the target so we have instance and I think that's what I want here today is instance so we'll say Target type instance register the instance by the instance ID so that's totally fine and then we have our targets I assume this is an array of this target description and so we'll go here and specify a Target to add to the Target group okay so the ID of the target the target type is Target group instance and specify the instance ID okay so we have um a ID and Port so we definitely need the ID and that's going to be whatever our instance ID is so we'll just say um web server and I'm going to assume this is what's gets returned back so we'll have to do that so we'll have to say web server certificate AR um there's something else that we had to create can't remember that's fine uh let's see if we need anything else in our targets here so we'll go down to A's um not required so we'll just ignore that then we have our Port the port to which the target is listening if ALB the target application must have at least one listener whose Port matches the target group Port this parameter is not used that the target is Lambda so I'm going to assume that we're just giving it Port 80 go up here Target group targets hold on here let's read that one more time the port on which the target is listening on which the target is listening so I think that's Port 80 here well the target is the one okay so like the thing is like here this yeah okay well we'll see I I just wonder if this supposed to be 443 or not that's I was get mixed up with that but anyway we'll see how that goes um so I think that our uh Target group is properly configured now I'm just uh interest in Target group attributes nothing that I'm interested in here so we'll go back and so now what we need is an ec2 instance um because that is something we're going to want to have here so I'll go ahead and just say ec2 instance um CFN and I almost feel like we have done this before so I'm going to go dig into our ec2 section here and maybe we already have a template no we don't have one here at least I know we've done it somewhere but trying to find that's going to be a little bit tricky but I can just type in web server here and we'll make our way over to E2 instance and we'll go to examples and we'll go ahead and grab one um I mean I want one to automatically assign a public IP address so I'm going grab this one instead okay we'll paste this in here so we have our ec2 instance we'll have to get our image ID um I just again going to provide one manually so we'll go over here to our uh ec2 we'll go to instances here uh we'll launch an instance sorry and I just need this Ami ID so grab that we have key name I don't want a key name I don't care about that uh network interface associate public IP address we absolutely do want to do that the group set the subnet ID we absolutely done this before I remember us doing this unless it was actually via the CLI um I could be mistaken but let me go look around and see if I can find it okay yeah we definitely have them over here in um in VPC we did this a bunch cuz I remember doing this so I think it'd be really useful if we just copy copy this into play so um what do I want to bring over here I definitely want basically all of this almost so yeah that's what I'm going to do so I'm going to just get this out of here and I'm going to go ahead and grab all of this we'll probably just swap some of these out the parameters here copy this go all the way to the top here we'll paste this in um and we actually have three subnets but we'll get back to that in just a moment so go all the way down to the bottom and I want to go back to our other template here we will grab the SSM roll because we definitely want that we want an ec2 instance profile we absolutely want that we want the ec2 instance and we also want the security group so we'll basically grab all this stuff and we'll go down and we'll paste it in a few things that are going to be different this is going to be our web server so I'm going to say web SG and then this is going to be web SG we'll leave that as my E2 instance because no we'll change it over to web server say web server okay so that's now in place so that is good and this one's referencing that one so that is fine as well um so we have those set up the security group here is only supposed to accept traffic from the ALB so this is where we would need a bit of a tweak I have a feeling that I probably have it in one of these templates here so I'm just going to go take a look here just say SG for Security Group um let go and see if I can find it in here or was this created in another template I'm not finding it so that's fine it's somewhere in there I just don't want to waste your time here by digging through my code so we'll just go and take a look at our Ingress rules and we can just specify that's it's coming from another Security Group so if we go down to source security group so the ID of the security group the default vbc the name of the of the source security group you must specify either the the security group ID or the the security group name you cannot specify both for security groups in a nondefault vbc you must specify the group ID okay well anyway I'm just going to specify the source security group ID and then when it throws problems then we'll go resolve it there and so here what I want to do is specify the this here and this will be on Port 80 so we don't want this from everywhere um this will be for everywhere that's totally fine and actually I'm going to go and just modify this one quickly because this web server one really should be from anywhere for the egress so I'm going to go ahead and just do that really quickly um if I can find our other Security Group wherever it is here it is and for the egress I just want to change it so it's like this all right so we go back down to our web Security Group and I want to specify the other Security Group here so I'm just going to split this so I can see the other one and if we go up that one's called Al BSG so we'll say ref ALB SG and so we have from port to port and this is where I think we'd leave out the cider yeah and then I'm thinking that this would be good we can low both protocols it doesn't really matter TCP UDP we'll let both go through but really we're only utilizing TCP here today um we did end up adding parameters here so it probably be a good idea for us to update these and make sure they're correct so we did grab an Ami somewhere in here so I'm just type in Ami whoops Ami there it uh no that's that's one we already have this one here so I'm just going to go ahead and just make sure it is the same okay great so I'm just going to reference it then so just say ref oh we're I keep thinking that I have it and I don't you know what it's because um we we pasted in the old ec2 uh code and we deleted it so that's why I'm getting confused uh but we'll have to update our VPC ID for sure um I'm not sure if we specified the VPC ID anywhere here type in VPC here other than this one it is down here so we'll just reference well first of all we'll copy this one up here okay we'll just say default and then I'll go back down to here and we'll say ref VPC ID so that is in good shape uh we only have a single subnet ID here um I kind of want to have three of them so in the web server we don't specify them here but it does get specified there we can pass in all three subnets uh in one go but I'm just going to say subnet a and subnet B and just make our lives super easy here um and then yeah we'll just do two subnets we don't have to do three it's fine and then we'll just go here and just say ref subnet a ref subnet B because it's going to complain if we don't at least have two and then down for down here for our uh web server we'll just change this over to subnet Ida um and then we'll have to go grab these two values as these are clearly probably not correct um so we'll go back over to our VPC and we'll go to your vpcs we'll click into here and we'll copy the VPC ID we'll go back over to here and we will no not the VPC ID the subnets I'm not thinking here straight today so we'll grab the first one here well before we do I just want to make sure it's not D because for some reason um the D one always messes up this is B so that's fine so I'll go ahead and grab that and I'll put actually in B so that it actually coordinates to the thing that it is I'll go to this next one here this one is D I don't want D I have so much troubles with d we'll go ahead and grab a so we'll do that so I believe that is good let's go all the way down to the ground um not sure why this is complaining here properties missing load balancer Aron in the HP listener oh yeah okay yeah we're missing that I mean should be happy now why is it why is it so complaining expected array type for default actions I mean that's why we gave it a a hyphen so I'm not I'm not sure what it's complaining about here let's go here uhhuh I mean it looks like a type in Array to me I'm just going to fix indentation here and see if that helps it so sometimes that helps again not exactly sure why it's complaining default actions incorrect expected array type it is an array um but we'll come back to that and and Fiddle with that okay because it it just doesn't trust me here for whatever reason um why is this a problem get ATT unresolved tag I mean I don't see get ATT anywhere here so that clearly is not a real problem here so we'll ignore that um maybe what we'll do is just put this in double quotations that might be messing it up no okay so our web server is hooked up it runs on aachi server we can log into it if we need to debug it the only thing we don't have here is the certificate AR um so I'm going to go back to my template because this kind of saves us a lot of time trying to figure that out and in here we will want to have the certificate AR so I'm just copy this here you'll have to write it out of course sorry or just grab it from my code base because this is all accessible to you and uh so this is the certificate AR and it seems like we are generating it out beforehand which is actually a good idea because it is a pain to deal with certificate ARS I'm just going to go ahead and type in certificate AR I'm actually thinking about this we don't really need a certificate AR because the the load balancer the whole point of a certificate AR is if we have a custom domain and we're actually not going to be setting up a cust domain so we don't actually need this here today uh and I think the low balance here by default the route that ID provides actually has HPS so I don't think that uh we have to worry about that um unless it actually asks us to attach it here and it complains because we're supposed to have the certificate AR here so anyway I guess we'll find out but I think we have everything that we need in place okay so we'll go ahead and create ourselves a new file this will be uh deploy and remember we were deploying things recently for something maybe Kinesis we'll go into here and yeah we have a deploy script we can go ahead and copy that and we'll go back over to here into our ALB um this is messing up I'm just going to give this a refresh sometimes this messes up reload for get pod specific to G pod it's not your problem and I'm G to give this a nice refresh here try making that deploy file again there we go I'll paste in this contents here so deploy ALB with ec2 say ALB basic uh our template is that CS Central one this is all fine that looks good to me I'm going to go to mod this file I have to CD into the directory first clear here chamod + x deploy okay and then we'll go ahead and deploy this here assuming there isn't something wrong with our template and there is it says it's not well formatted yaml on line 93 so we go to line 93 here and the ref is supposed to have a uppercase character there we'll try this again it says change set unresolved resource dependency subnet a subnet B I mean they're not called subnet a subnet B they're subnet ID a there we go we'll try this now unresolved dependency my VPC we don't have anything called my VPC so I'm going to go ahead and look for that here this would be vpci ID and we'll try this again working looks like it's created it we'll go back over to here we'll make our way over to cloud formation and we'll go to change settings and we'll go ahead and click into here see what it's creating a bunch of stuff we'll execute this change set all right so this is now going to start creating so we'll wait here and see what errors come by because this there's a lot of moving Parts here I do not expect this to work the first time all right so we've ran into a problem totally fine um a certificate must be specified for htps so that is where we're not going to be able to utilize htps here today because we would have to then create a certificate and we'd have to have a domain name and I really don't want to go down that route here today so what we'll do is we'll go back and tweak our code um and this is fine we can use this later on when we actually need to use a certificate and I'm sure we'll do that at some point um and I don't really want to redirect so I'm just going to go comment this out and what I'm going to do is just uh copy this one here and we'll go down below here and we'll just uncomment it and uh I'm going to change this to Port 80 HTTP and so that should be enough to resolve our issue we're going to go back over to cloud formation I'm going to delete this stack and wait for it to delete does take a little bit of time because some things require some effort to tear down so just wait for this to delete all right so that's been deleted and now we can go ahead and redeploy and hopefully this time we'll run into uh less issues but you know it takes a few times that is always the norm for uh developing IC this is a review in process so we'll go and check our change set and we can see we are creating a bunch of stuff so this looks pretty good we'll go ahead and execute this change set the only thing I'm thinking about is that uh no it's all fine yeah it's all fine the only thing I was thinking was like we changed the resource name or I mean we have this one here and uh did it have to reference anything and I don't think so but we'll be back here in a moment in case something goes wrong okay all right and it looks like our load balancer is created we can take a look at our resources that we have and we have a little bit quite a bit there let's make our way over to um ec2 which is where everything is all right and so we should have an instance running okay and if we go here it's a T2 micro it's still initializing but it might be depending on its state it might already have um that web server up and we can take the IP address directly and see it but it seems like it's still working uh to complete what's interesting with cloud formation is that if you want um if you want it to uh wait you could actually tell it to wait for the stat checks to pass there's some CL information code for that we're not doing that here today um I want to go over to our load balancer and take a look at what we have so we have our load balancer here good we see it's in it's application we have our listener we have our single listener it's saying what Target it is forwarding to and if we click into our Target we should be able to see um things are there it's saying it's healthy that means it's setting uh it's sending an HTTP um ping to the server so this says send to this path and we if we get back at 200 a status 200 from the application load balancer uh then it should be working and it says that it is if we go over here I don't see anything yet but you know why this is not working is because we told the security group to only allow access from the ALB security group right so if we go over to our security groups here and we look at the web one notice that it says only allowing traffic from the other one okay so the only way we're going to be able to access this is through our load balcer so if we click on our load balcer here um we have this DNS record and if we go and grab it or DNS name we should be able to hopefully get to our web server no is trying to do htps we don't have https so oh but it's still resolved so that's good even though well actually that's okay because the idea is that the um if it is it would just go and move over to that but uh yeah it's it's interesting we we did HPS but it just dropped to http but yeah that 100% Works um usually when you use application load balancer you use it in combination with an autoscaling group and so that would be uh something that we can do uh in a separate video is to kind of roll them up uh into their but that is a very standard uh setup for web applications that run on Virtual machines is to use an ALB and ASG and then you'd have your certificate but anyway uh this is going to be really easy clean up because all we got to do is tear down the cloudformation template and we're in good shape so I'm going to go ahead here and delete the stack I'm go commit my code so other folks can benefit so we'll just say ALB example um you know the key difference between the ALB and the NLB NLB is for uh lower level traffic whereas uh ALB is about having um it's about having h2p and I actually I guess we didn't show it in here but the routes you can actually set up a bunch of different routing rules um so that might be interesting as a separate video but for the scope of this one all I wanted to do was get it set up and I will see you in the next one okay um ciao let us talk about NLB so it's designed to balance TCP and udb traffic at operates on the layer four of the OSI model it can handle millions of requests per second while still maintaining extremely low latency it can uh have Global accelery in front of it just like ALB to improve uh Global availability it pre preserves the client Source IP uh when a static IP address is needed for a load balancer you can utilize it with this the use cases here is high performance Computing Big Data applications real time and multiplayer gaming platforms Financial trading platforms iot and smart device ecosystems telecommunications networks the reason I know NLB is because I have a friend that worked for um companies that built Roblox games you know for kids I think like one of the ones he worked on was like it pretended that you're like in a high school and so this thing could just could handle the throughput for it uh so when you need a lot of request and a static IP address uh then this is the one you're going to use okay let us talk about classic load bouncer so this is ad's first load bouncer it's a legacy one can balance HTP HTTP HPS TCP but not at the same time it can use a layer seven specific features of the O OSI model such as sticky sessions it can also use uh strict uh layer four for balancing purely TCP applications not recommended use for anymore because we have specialized ones it's really easy to configure and use so I mean it's not bad but you know it just says not to use it anymore but we still got to talk about it because it still is used doesn't show up in the exams not so much but it's good to just get some of this knowledge here okay let's talk about sticky sessions also known as session Affinity is an advanced load balancing method that allows you to bind a user session to specific compute targets so the idea is that by having cookies it knows how to go back to the same machine this is important when you have sessions so ensures all requests from the session are sent to the same instance typically utilized with a classic load balancer uh can be enabled with ALB so basically you're going to be using with ALB these days can only be set on a Target group not individual ec2 instances if you wanted to do an e ec2 in individual ec2 instances you use classic load balancer cookies are used to remember which ec2 instance for returning traffic ensures that all requests from the user during the session are sent to the same Target useful when specific information is stored uh locally on a single instance to use sticky sessions the client must support cookies sessions sticky sessions are not supported if cross Zone load balancing is disabled for applicationbased cookie names have to be specified per Target group application based stickies stickiness does not work with weighted Target groups for duration based cookies it says ad ALB is the only name used across all target groups for websocket connections uh they're inherently Sicky so you don't have to utilize this here lb uses the expires attribute in the cookie header instead of the max age attribute lbs do not support cookie values that are URL encoded if you are enabling duration based stickiness this is how you would do it if you're enabling application based stickiness um this is how you do it but with application based stickiness you cannot use very particular names okay so you have to set a name but um hopefully that is clear I'm just going to get my tool out here because I just want to look here so somewhere in here I think you set the name I'm trying to find the pen yeah here the the my cookie name right so there's a bunch of ones that are reserved that you can't use this one you don't set a name because it's always using the same fixed name which we talked about it was called adabs ALB I think so there you go let us talk about cross Zone load balancing and yeah there's a bunch of little numbers at the bottom I just couldn't be bothered to like select them all with so much work so they'll make sense in a second okay um but cross Zone load balancing is when traffic is evenly distributed to all Targets which improves traffic flow uh fault tolerance and cost efficiency so ALB it's turned on by default it cannot be turned off for NLB it's turned on by default uh but can be turned on or off for clb it's turned off by default but can be turned on and off all right so when cross Zone load balancing is enabled requests are distributed evenly across the instance in all enabled availability zones so look at the number it shows it it's the same number for every single instance across A's okay for cross Zone load balancing disabled requests are distributed evenly across the instance in only its its availability zone so notice there that you have it split and then it's split again okay so you can turn on Cross Zone balancing with utilizing ad CLI you can turn it off pretty straightforward there you go if you need the ipv4 address of a user check the X Ford 4 header okay the X Ford 4 also known as xff header is a common or sorry common a command method for identifying the or originating IP address of a client connecting to the web server through an HTP proxy of a load balcer so the idea is that we have um I'll get my pen out here we have a IP address of the client it's going through to the load balcer which has its own local IP address and so if you didn't if you triy to look at it you'd think that this is its IP address but if you look at the X for 4 then it's actually going to be this okay and yeah the icon's purple now but I didn't swap it out okay but there you go checks so instances that are monitored by elb report back health checks as in service or out of service health checks communicate directly with the instance to determines the states elb does not terminate kill unhealthy instances it just will redirect traffic to healthy instances so here's an example so here it finds that an instance is unhealthy it's going to redirect it not kill it but redirect it to a healthy instance it's ASG which is a a companion service you can use with lb that would actually terminate and replace it the uh for ALB and lb the health checks are found on the target group so you can see we have the parameters there that is an old screenshot the UI is a bit updated but I prefer the look of that because it it displays it a lot better so that's why I use the old one but there you go okay let's talk about request routing so these are the rules that allow you to forward or redirect traffic based on the HP request it's only for ALB I have the old icon in there the new one's purple it's the same thing it's just purple now instead of orange but the idea is that um imagine that you want to send different traffic to different places based on the request so maybe the subdomain I'm going to get my pen so we can specifically highlight this so maybe it's the subdomain maybe you want to redirect based on the uh the URL maybe you want to do it based on the query string or based on the uh a header that is set or the uh the method and so this is something you can do with request routing and so we're going to do that by utilizing rules and stuff like that okay hey this is Andrew Brown we are taking a look at ALB rules so here's some Json to get an idea of what it looks like a rule is compos the following parts rule conditions so determines the conditions that route should be considered we have rule action types determines the action that should be taken if the condition is met rule priority determines the priority uh uh a rule should take over another this is eval evaluated from lowest to highest we have the rule limits so 100 total rules per ALB five condition values per rule five wild cards per rule five weighted targets per rule I'm going to get my pen tool out here so we can draw some lines so you can see the conditions here you can see uh the rule action type uh here down here where the priority is over here okay so pretty straightforward all right so this is where all the magic happens it's with the rule action types so ALB supportting the following rule action types we have authenticate Cognito so use Cognito to authenticate requests we have authenticate oidc so use an identity provider that that is compliant with oidc to authenticate requests we have fixed response so return a custom HTP response forward forward requests to a specified Target group redirect so redirect requests from one your all to another uh if the protocol version is grpc as opposed to http2 the only supported actions are are Ford actions because you can set ALB to be grpc we didn't describe that previously but I I remember seeing it when I was working with it but I couldn't find it again the UI but the docs say that you can use grpc so I just wanted to put that in there what's important to look at is basically the fix response forward and redirect we'll look at those in more detail authenticate Cognito and authenticate oidc are nice but uh not necessarily going to show up in your exam okay all right let's take a look at fixed response which is a action type that ALB can do um and so here we have both the CLI command and the actual response like the uh not response but the the actual rule that we'll want to place in there so a fixed response will drop requests and return fixed data for either 200 400 or 500 status codes the X's means like if it's 404 400 whatever you want it to be uh the rule is very simple as you can see you just put the type is fixed response and then you say what status code you want I'm going to get my pen tool so you can see what I'm talking about here uh yeah status status code content type and then the message body uh when a fixed response action type is taken the action and the URL of the redirect Target are recorded in Access logs the count is successful fixed response actions is reported in the htb fixed response counts metric so you can definitely find that I believe that's Cloud watch there that they're talking about okay all right now we're taking a look here at redirect actions another um rule action type for Al so redirect actions let you redirect client requests to another URL you can specify either whether it's temporary or permanent you can retain information about the request me passing it along uh by using this uh interpolation interpolation is when you inject something into something else so we have protocol the host the port the path the the query uh so those are all the components there so the idea is that by specifying host that it's just going to pass that along okay same thing with path query you could hard code them whatever you want them to be but there you go we're taking a look here at the forward action which is another type of rule action type for ALB uh so Ford allows you to follow a single or multiple targets if you specify multiple targets you need to supply a weight the target weight can be between 0 to 999 weighted Target groups does not guarantee that the sticky sessions are honored usually the waiting that people use are increments of 10 because it's out of a thousand but you can see here we are saying for and then these are the two target groups and we're weighing them a bit differently there so there you go all right let's take a look at rule condition types the forward action has the following condition types host header so route based on the host name of the each request HTTP header route based on the HTP header of the request HTTP request method route based on the HTTP request method for each request path pattern so route based on path patterns and the request URLs query string so route based on the key value pairs of values in the query string Source IP route based on the source IP address of each request and the condition types are are uh available for ALB as they perform conditions based on HP requests you cannot do this with nlbs this is all it'll be stuff we're going to go through each example here because you should know them because this is the power of albs right here okay let's take a look at our first rule condition type which is host header this is based on the host name of each request so examples could be www.example.com or as. example.com you can match using a wild card so that's zero more characters or question mark to match exactly one character look over here we have host header config so host header values we're providing the values notice it's an array so you can provide more than one there you go okay our next one here is uh HTTP header route based on HTP headers for each request so examples of headers could be accept accept encoding accept language cache control connection cookie content type uh content length host which we have a host header for so we don't really need that one origin refer user agent authorization the way it works okay we say HP header config right here we provide the name of the header and then the possible values and you can see we're using wild cards around that one so there you go all right so the next one here is our HP request method route based on the HP request method for each request methods could be post yes put patch delete or custom one if you're doing something different wild cards work a question mark works pretty straight forward I'll get the pen out here so we can see exactly where in the code so we say HTP request method here and then we provide the values that we accept okay let us take a look here at our next one which is path pattern route based on path patterns in the request URL examples could be this route or that route you could use even do grpc routes as well um you can match based on wild card or question mark as per usual so we'll get our pen out we say path pattern config we provide our values again this is after the address so imagine there is the full URL address prior to this and then it's on the end of your url okay all right our next rule condition type is query string route based on key value pairs of the values in the query string can match on wild card on question mark a query string is the additional information that is provided on the end of a base URL so just understand that you can have a path right so it's URL and then whatever your path is in between here which we don't have anything and then it's the query string right um and so this is a great way especially when you have like um analytics data that you want to capture from links but not hard code in the URL you can do that so we have the query string config over here we provide the value we say the key is version the value is version one so that so here's the key hello and then we have world that makes sense you can have multiples uh usually that's the Ampersand so here you'd have an um another value okay so there you go okay let's take a look here at source IP so route based on the source IP address of each request the IP address must specify a CER format you can use both ipv4 or IPv6 wild cards are not supported for this one you cannot specify uh 255 255 255 255 for 32 for the source IP rule condition why I don't know I'm not great at networking but somebody that knows networking probably goes Andrew it's obvious anyway we specify source ip config here we have our values you can see it's in our cider block notation and there you go so when you have a Target group it has to register targets right and so there are different things we can register the first are instances so send traffic to a specific ec2 instance based on the instance ID this is great because if your IP address changes it's always going to point to the same instance you have IP addresses you can point to a specific IP address often microservices you're going to have to point to a specific IP address just the way it works so for example if you use fargate ALB can only target compute via IP addresses for fargate okay so that's just the way it works because fargate will get a Nick that'll have an IP address um and you can't just directly Target the compute the same way like an E2 instance um IP addresses do support ipv4 and IPv6 or ipv4 to IPv6 natting uh ipv4 has one of the following cider block formats to work so you have to I didn't know this but apparently it has to be one of these formats I guess I always just kind of stick with them um so we have 10 008 100 6400 17260 192 16800 and I believe what that is is those are specific classes of um IP IP ranges or or CER blocks um I covered this in my primer for cloud networking but uh that's what they look like to me so even though everything is classless they want you to use these classes still um we got Lambda functions we can Target so the target group can only route to a specific Lambda function Lambda does not have to be deployed within your VPC I really thought it had to be but apparently not and only ALB Target groups can Target Lambda functions you can also send uh a Target group to another ALB so send traffic to ALB from an A NLB or ALB you this could be used to send an internet facing elb to an internal facing elb okay let's talk about Target D registration so before we do we should talk about connection draining because this is something that's going to happen when you're trying to deregister a Target so connection draining also known as deregistration delay is when you deregister a Target the load bouncer Waits until inflate requests have completed the status of a Target is draining while connection draining is in progress here we can modify it uh the deregistration delay for ALB and clb um it could be 1 minute or sorry 1 second to uh one uh up to 1 hour but the default is 5 minutes lb does not have configuration uh configur able deregistration delay you get what you get um when you deregister a Target that was registered by an IP address you must wait for the deregistration delay to complete before you can register the same IP address again so that's just an issue can run with IP addresses um but the whole point of deregistration delay connection draining is that you know if you have a connection someone's doing something important you don't want them you don't want that person to get disrupted so you want to set this based on how long you think a request would be okay hey this is Andrew Brown and we are taking a look at autoscaling groups so an ASG contains a collection of ec2 instances that are treated as a group for the purpose of automatic scaling and management specifically horizontal scaling as it is just adding and removing instances automatic scaling can occur in a few different ways when you're using asgs you have capacity settings where you set the expected range of capacity where you could and this would be called manual scale in we have health check Replacements where it will determine if the Inus is unhealthy is going to replace it so we have ec2 or elb health checks that we can utilize with asgs and we have scaling policies these set complex rules to determine when to scale up or down and we have SIMPLE scaling step scaling Target tracking scaling predictive scaling um and I think that's all of them there I do want to point out that asgs are used to scale ec2 instances so if you're using ECA s and the underlying compute is ec2 you can use asgs if you are utilizing eks which is kubernetes and it's utilizing ec2 instances underneath those can utilize asgs as well fargate does not utilize asgs um it might internally so adab us might be utilizing it but from a customer perspective fargate is a fully a fully managed serverless containers and so it just automatically scales so hopefully that makes sense what would be a use case for asgs we hear here is a diagram um and let's just walk through it so imagine you have a burst of traffic that that is coming into our domain I'm just going to get our pen out here so I can just kind of follow along here so up here okay uh Ry 3 will point that to our load balancer over here uh our load balancer passes the traffic to our Target groups which is over here uh the target group is associated with our ASG and sends the traffic to the instance registered with our ASG okay over here the ASG scaling policy will check if our instances are near capacity and then the scaling policy determines we need another instance and it launches a new ec2 instance with the associated launch configuration to our ASG now this is scaling policy so that is the um that's one way scaling occurs but we will make sense of all this as we go through it okay all right taking a look here at capacity settings this is our way that we do manual scaling capacity settings is made up of Min size Max size and desired capacity so the Min size is how many ec2 instances should uh at least be running at a given time the max is how many are allowed to run at Max the desired capacity is how many ec2 instances you want to ideally run and so the way we would set these uh values for the CLI is we just update that auto scaling group and you can see we're setting the Min and Max here would work as well for uh that for desired capacity we would have a separate action here for set the desired capacity okay um ASU will always launch instances to meet the minimum size capacity so the idea is that if you have a scenario and you just want one instance to always be running uh you can set one and it will always make sure there is one uh running uh but the point is is that there can be a point of unavailability because there's that point where the new one is spinning up changing the min max and desired capacity is considered manual scaling that you have to manually change these numbers to change capacity so there you go another way we can scale is based off of health check Replacements so here we have um an example of a health check replacement this is when the ASG replaces an instance uh if it's considered unhealthy and there are two types of health checks asgs can perform the first is ec2 health check if the ec2 instance fails uh either of its Health uh ec2 St stat checks then it's going to get replaced um this one is not very useful in practicality but if you're not using an elb you can't take advantage of the E elb health checks but I would say that often when you're using SG you're going to have uh an application load balancer or something attached to it so for the elb health checks ASU will perform a health check based on the elb health check and elb will ping an HP endpoint it is specific path port and status code so look at the example below as you can see I'm I'm going to get my pen out here so we can just kind of narrow it in here but the idea is that we say to use the health check elb um and then we are specifying the subnets and the subnets have to be the same subnets as the uh the load balancer okay as the ASG so pretty straightforward and of course you configure the health check on the elb not on the ASG um if that makes sense okay hey this is angre brown and in this video I want to take a look at autoscaling groups for ec2 uh so we're just going to use them in isolet not with uh application load balancer as we can do that in a separate one but the idea is that autoscaling groups allow you to quickly provision more resour uh more uh servers based on uh the specified capacity so that's what we're going to be doing here today um so let's go ahead and get to it I'm going to be writing all of this in cloud formation here because that's what you should know how to do um and it shouldn't be too difficult here and the idea is that we can reuse those templates when we want to bring them together with ALB and other things like that so we'll let this get started up here there we go I'm going to make a new folder here called ASG new folder here called ASG I'm going to call this folder basic because this will be our basic example of using ASG I'm going to make a new file here called template. yaml so I was just working on albs so I'm going to go and grab some code to easily get uh started here and I'm looking for that ALB oh I I think I called it elb because it was ALB for elb um and so there's a couple things I want I want these two lines here so that will be good we'll paste that in there um and I'd like the same deploy script I'm going to copy that and we'll say new file deploy we'll paste that in place so uh oops deploy ASG just be ASG there we go and we'll make our way back to our template and so we'll need to have some resources here this is a little bit different because in order for us to have an uh E2 instance we have to create a launch template um so launch templates allows you to create a template that can be utilized uh for Autos scaling groups to to launch stuff there used to be one called launch configurations um it might still exist via the API but from the console view they don't appear to uh be making those anymore which is fine because launch launch configurations are just really out of date and have simply no purpose anymore since they serve the same purpose as launch templates the launch configuration being useless let's go look up launch template CFN here and see what we can find so we'll go over here we'll go down to examples and I'm going to go ahead and grab one uh right off the bat looks like we they actually gave us like the full script I like actually when they do that they don't always do that we go ahead and paste this in here I'm going to go to the top and we'll just delete out um the redundant uh lines here so this one is setting up a launch template and we can name it I'd rather it be named by AWS if we can we don't have to worry about conflicts I'm just going to go ahead and grab our link as we usually do we'll just place it in here and we'll go and take a look at the name so we do not have to specify names I'm just going to take that out of there um disable API termination I don't think I'd want to do that what is the default for it is that under oh that's under the launch template data okay so before we look at that is there anything else no it looks like basically it's all about this launch template so we'll go into this here grab this link and if you set this parameters here you can't ter the instance using the adabs console so um I'm going to take that out of there because I don't want that we probably would want to have an instance profile so that's a good idea we might go grab uh the one from the other one here in fact that's what I'm going to do I'm going to go over to our ALB example and in here we have um an SSM roll and instance profile so I'm going to go ahead and copy those because that seems like a a good idea for for stuff I just change this over to launch template I'm not going to have it my launch template and so then what we can do is is just reference um the instance profile here apparently it has the exact same names oh no this one's easy to instance profile okay and so then we want to get rid of this one this one is no use to us and we can bring this onto a the same line as actually no we probably we probably do have to reference it the way they're doing it because it looks like it's you have to have instance profile then Arn really no you know what I'm going to try it I think this is fine this really has to do with how yaml translates over and understanding underneath if it doesn't make sense to you that's totally fine I'm going to go up here and grab our image ID and bring that on over here it did not copy I'm going to try this again right click copy right click paste we'll put up here parameters okay then I go down here and we'll just replace this image ID I'll bring over the type as well these days I'm kind of preferring T3 micro T2 micro is still the free tier uh T3 micro might be under free tier if you're not confident with it you can launch T2 micro there's basically not a huge difference but I'm going to go with a little newer one there's even newer than that but that one's definitely not free tier t4s um so we have image ID ref ref uh we don't need a key pair because we are going going to launch this with you know our usual stuff um I'm going to grab this I'm actually go over to knle and grab the one from knle now I think about it we'll go over to VPC knle here and we'll go grab our example here good Cloud Engineers leverage their old code as much as they can so we can steal as much as we want if we wrot it we'll go ahead and paste this in and for here I mean like we want this open on Port 80 and 443 but I'm going to leave all the ports open because this is a low lowrisk app that we're building here um so this needs to be here yeah SG and I believe it Returns the ID automatically with the reference so that is fine so that looks good I feel like there's more that we need to configure like how would we know that it would have a a public IP address assigned to it that's something that I would care about so let's just carefully look here we have block device mapping CPU options credit specifications I'm looking for um things about the eni I mean this is for elastic interface accelerators which I don't know if exists anymore instance type specified that somewhere in here network interfaces that's what we want so let's go to our network interfaces here and it looks like it's just an array of network interface I have a feeling that we can probably grab this well this is for the launch template so it might be slightly different but if we go over to this one we do have um our other template I should say our uh we'll do the Knack that one's fine but in here we have a network interface and so this is probably pretty close to being the same so go ahead and just paste this in below and we'll just have to check that it is more or less the same so here I think I believe it's network interfaces interfaces and then it's an array of them so we'll do this okay we'll go back over to here launch template network interface is a property of the launch template okay yeah great um so what I'm looking for here is the device index again just making sure this all matches up so that is there good and subnet ID is that something I have to specify here it is good and then we have associate public IP address that is good and then we have uh group sets I see groups not group sets so that's going to be pretty similar let's go take a look here an array of string so that's going to probably be correct there delete on termination do we have that as an option so just be aware that just because something looks similar does not mean it's one to one because anyone like it of us could be copying and pasting from another team and then tweaking it for whatever reason so decided they didn't like the word group set so they Chang it to groups I'm looking for delete on termination is that in here yes it is okay so this is good there is the uh subnet being referenced here so we have that subnet ID um we'll go ahead and we'll grab our two subnets in our other template so we go back over to our ALB all the way top here we'll grab subnet a and subnet B whether we'll use both as a different story but uh we'll paste these two in here and so I'm going to go all the way down to the ground and I'm going to reference subnet ID a but I the thing is like I I wonder if that is optional so let's see here it's not required so let's leave it out because the idea is that we want the Autos scaling group to launch it in wherever uh subnets it's allowed to launch in and uh if we specify the subnet then it's going to want to launch it every time or it's going to get ignored it's hard to say but we have our launch template ready ready to go so let's go get our Auto scaling group now ASG ec2 uh or sorry um CFN for cloud formation we'll go over here we'll grab our link here and we'll get writing our autoscaling group so I'm just going to go indent and we'll say ASG hopefully they have a nice example for us they have something I have a full example actually that's kind of nice that they did the full example because it didn't lay off everybody AWS okay so we have our we have our template here uh launch template latest version number Min size Max size is one Min size is one and then V uh VPC identifier so subnets and so I said in like the ALB video that we could have a reference to multiple subnet IDs so we'll go over here and and um they do this is this is what we want so we'll grab this and we'll change this over to the more interesting format which is this and um now we need to provide that so I just say default here and I think that because it's a list it probably just be like this okay if we want to make comments to say a subnet a subnet b c uh C CA Central it's nice to know where things are coming from here so we'll take these out because we don't really need them and so I'm thinking that this is what we need um looks pretty good I think we have everything be nice if we have everything that'd be nice I'm going to chod whoops this is is uh is this unresponsive it is I'm going to go ahead and just reload this is a little hack I have to do for get pod so just reload this here and we'll CD into our uh ASG directory here into basic and we'll say chamad + x deploy I'm going to go ahead and deploy this and it doesn't like something in our script so I'm going to go take a look at our script something is missing so I cut something off by accident so I'm going to go back to our elb script over here and it looks like the stack name is missing it was there before I probably just accidentally removed it contrl C to kill that we'll hit enter template every default must be a string oh yeah so then I'm going to look up this this here exact format for the default so I'm not exactly sure what to provide here does it say here default I'm going to assume that it's just um common eliminated we're going here it's not showing but I'm going to have to take a guess obviously these are subnets and um we'll go ahead and do this there we go we'll go ahead and try this again template format error unresolve resource dependency VPC ID I mean we didn't bring one over so we'll go back to our other temp plate our other one here for the elb and I'll just bring that over so something must be referencing it I don't know what where it is here VPC ID that's our security group that needs it okay and we'll go ahead and try this again looks like we are in better shape this time around I'm going to just open up new tab here so we can keep one leg in ec2 and the other one in claw formation so it's created our change set we'll go over here we'll look at what's being created looks all good to me we'll go ahead and execute that change set all right so we're just going to wait and see if this works out be back here in just a moment okay all right so we have a problem let's go take a look here and see what the issue is um validation failed resource launch template with name network interface extraneous keys I remember us having this a lot for one of our other videos um externing this Keys usually meant like I was copying pasting something in the wrong area and that's probably what the issue is and here is our problem this is uh just supposed to be the network interfaces so we'll just take that out here and do this and so that is going to probably resolve our issue there um so I'll go ahead and delete the stack really struggled on that one on another video we'll wait for this to delete and then we'll launch it again all right so that's deleted we're going to go back over here give this another deploy and I'm going to uh go back and make sure we accept that change set and confirm yeah that looks good we'll execute that and hopefully this time it will work um sometimes this stuff can get caught in a loop so if it's not able to um pass it health check or whatever this could infinitely go here so just watch it and make sure that it is getting to a state that you want it to shouldn't take like hours but you know just pay attention okay all right something's wrong with our codes let's go take a look here and check what's happening here um launch template when a network interface is provided the security group must be part of it it's not part of it I thought we specified it let's go take a look at our security group it's definitely there let's click through and look at the specification for our security group or sorry launch template and we have Security Group ID and then security groups it's not really clear which is which so you can specify an idas of existing security groups and reference them we're non default bpcs we must use scre group IDs instead okay so we'll just Swap this out then not very clear and uh we'll go ahead and delete this we'll let it delete and once it's deleted we will go go ahead and try this again okay oh looks like that was fast so we'll go ahead and try it again execute change set there we go see you back in just a moment okay all right let's take a look here and it's uh failed again which is totally fine says invalid launch template when network interface is provided the security group must be part of it okay so I think what it's saying here is that for the network interface oh we have it here we also have it here so we can't have it both so I think that's probably what our problem is so I'm going to go ahead and just comment the out remember when I mean I don't know what video it was but it was in VPC where we're trying to launch an ec2 instance and we were setting up this network interface and we found out we we couldn't have it here could be one or the other not both so I'm going to just take it out there and and hopefully that will resolve it like uh it did for us when we set up an ec2 instance in our VPC earlier we're going to go back over to here and I'm going to go ahead and attempt to deploy this again we're waiting for a change set okay give this a refresh here we'll go into here we'll go to our change set and we'll execute that change set so we'll just wait for this and see if we run to more problems all right so looks like creation is complete let's make our way over to ec2 uh here we have our launch template and just so you know that launch templates are version so here it says version one and the idea is that every time we update it if we make changes to it it'll have version two and then we can tell our Auto scaling group what it can launch with so we go down here to autoscaling groups I mean we should see one but let's go back over to here and look at our resources and we should have an ASG yeah okay so why don't I see it here um it seems like it's a little bit confused so what I'm going to do do is just click through to it there we go and so yeah I'm not sure why the UI was being really silly there but here is our autoscaling group I'm going to expand it up here and notice we have a desired capacity of one a minimum capacity and a maximum capacity so basically um this is just going to make it so that an instance is always going to remain running and if it gets terminated it'll spin back up we can see our launch template that we're using so this is a T2 micro we wanted to we could edit it and change the version that we are using um but right now it is aled to one and it is one so that is great we're going to click back and let's go take a look at the instance management here I can see that it is healthy if it wasn't healthy it would terminate it and try to spin it up again um so I'm glad it's not doing that we're going to go ahead to the instance here and we'll go take a look and see that it actually is working so we have our ipv4 address we're clicking through to it and even though it doesn't resolve it's probably because we need to change it to http we'll see if this works do we get a website okay so we don't see the website does this have a public IP address it sure does okay another thing I want to check here is the health check status it seems fine so we'll refresh this again so this seems okay it's not like it's failing but why are we not seeing the website maybe our instance is not done yet let's go take a look here and it has initialized and the the stat checks have passed so I'm actually a little bit surprised that this is not resolving let's go take a look at the instance and log into it as it's supposed to be running that Apachi server so we have that Ro attached so that we're able to get into it via sessions manager and we'll just use PS Ox uh Ox GP um HTTP to see if the server is running HTTP or we'll try apachi and so it doesn't seem like the web server is uh running we'll go back to template and make sure that we actually specified a launch template here so I'm looking for our oh you know what we didn't because this is a launch template uh we didn't actually provide an area to start up web server um so that's really interesting now the health check is not going to check to see if a website is running that's something that ALB would do and that's why you'd pair an ALB uh with an ASG but let's go back and update our launch template and create a second version it's great opportunity for that and so there's an area for user data and so that's what we're going to do is bring the user data in here so I'm just going to uh close out this tab since this server is going to uh get spun down here in just a moment and we'll go and split this and we'll go back over to our ALB template which should have our user data script in here so here it is so I'm just going to go and copy this all right and then we'll go to our launch template here and we'll just paste it in we might need to fix our indentation here I'm not 100% sure whoops just paste that back in there and yeah we just got to make sure our indentation is correct I'm not sure why it's complaining I'm going to take this and put it on the bottom so that it's less complaining still is complaining incorrect type string for yaml format user data the user data is to make available the instance you must provide a Bas 64 encoded string that's what we're doing here oh this was also aing out so I think that's fine um what we'll do is we'll go ahead and deploy this and the idea is that it should increment this to the next version so we're not deleting the old one we're going to try to deploy this in place and see what happens okay so create the change set we're going to go here and refresh we'll go over to change sets we can see that there is a new one no that it's going to modify the ASG it's not replacing it it should create a second version at least that's what it should but we'll see what actually happens s um and we'll go ahead and execute this change set so we'll just wait here for that to occur shouldn't take too long all right so our uh update is complete let's take a look and see what it has changed so if we refresh this and we click into it um we're looking launch template it says version two that is great that's what we want to see we're going to go over to our actual um um launch templates here and you can see that we have version two here so we can see there is a bit of different difference in configuration I'm not sure if it shows everything so I'm not seeing the user data per se which I would assume that would be here which I'm a bit surprised that it's not there oh because that's version one and we even though I'm on uh oh sorry that's another launch template so b or somebody made a launch template there it's just kind of in the way yeah there's our user data script so clearly that is in good shape if we go over to our Autos scaling group question is did it automatically tear down the other one and spin the other one back up because it may not have done that so if we go here um we go to instance management what we could do is just click through and see when it was last launched so this one here it was created when where would that be somewhere in here created at some point in time created at okay status no monitoring no networking security no launch under security why why is it under the details it used to be just like right up here but now nope it's under security apparently knowing when things launch is a security component uh and so my time is like right now it's 2 and this says 1353 uh um so I'm assuming this is the old one and the way we can test this so we just open up ipv4 if it doesn't resolve yeah it probably is the old one and it looks like it's the same IP address so yeah it's the old one so it's it doesn't know to uh replace that and basically it's going to be the new ones of that launch that will have it and so what we can do is we can just go ahead and terminate this instance all right and since the auto scaling group is set to a minimum capacity of one it's going to notice is that that instance is gone it's going to become unhealthy and it's going to spin up a new one so I'm just refreshing this till it figures it out I'll just pause here until I capture that for you okay all right so now it says it's unhealthy and we can tell that because it's been terminated and so the idea here is that it should uh after a little bit of time determine hey this thing's not coming back up and spin up a new one so that is what we're trying to observe so now it says it's healthy it's pending meaning that it is launching a new one now of course as it's launching it's going to give it some time to uh to get ready but uh that's autoscaling group in action so that's pretty cool we'll wait for this to spin up okay all right so let's see if this is now initialized so it is uh let's go ahead and take a look here and we'll open up our IP address here our ipv4 and what I'm expecting to see is the website it should launch with the latest version so here it is it's launching with the default Apache uh website uh which is great I'm not sure why it didn't serve our custom HTML page maybe uh that got clipped out when we uh pasted this in here so maybe that last command here got uh yeah it did see has a bunch of KS on the end but anyway the point is it's still serving up Apachi so it is working and so you know we want to just adjust this that's the ALB so this one's fine it's this one so yeah have some extra ones on the end here okay all right so anyway uh yeah that's working and basically you know it's pretty straightforward how autoscaling groups work you can change uh these parameters and if there is more compute or some other metric that you want to have trigger autoscaling you can do that so we have these policies I'm not going to do this for this video this is that's for a separate video going through all these different types of um policies but the idea is that you can go in here and say yeah when CPU utilization reaches over particular amount then trigger it to scale up um but yeah just setting up the ASG and having our our base template so we can play around with it later is a great great thing I'm going to go ahead and tear down tear down this um stack all right and we'll see you the next one okay ciao let's take a look at at elb integration this is the idea of taking elastic load Bouncer and attaching it to your auto scaling group here's an example of us using the CI command to attach one uh notice that it's asking for the Target group IRS because the way you will associate a modern load balancer is not by associating the load balancer directly but it's Target groups and it it will infert from there uh back in the day all we had was classic load bouncers so you would just associate directly um but now it is always through the target group for ALB NLB and the Gateway load bouncer are associated indirectly via Target groups an attach lb means that the ASG can use the load balancers health checks instead of ec2 and so that is the main advantage there all right so Dynamic scaling policies are how much SG should change capacity and policies are triggered by Cloud WI alarms and if you only use this via the ads Management console they abstract a bunch of this away so you wouldn't realize uh it's so easy to configure these things and understand them when you're working with them via the CLI um but anyway there are three types of dynamic scaling policies we have SIMPLE scaling policies step scaling policies and Target tracking scaling policies and the idea here is that um we'll have these adjustment types that will determine how capacity should change so we have one called changing capacity so change capacity based on the scaling adjustment exact capacity change capacity to an exact number percent changeing capacity change capacity by a percentage and I need to point out that adjustment types are for simple and step scaling Target tracking is doing it a little bit differently but this will make sense as we go into uh the three types of dynamic scaling okay so simple scaling policies simply change capacity in either direction by a certain amount when a cloudwatch alarm is triggered so here is the example of creating a scale in and the scale out policy which you'd have to create both uh for each Direction and the idea is that we are looking at how this is working so notice that we have the adjustment type and we talked about that earlier where you're saying I want the capacity to change based on the percentage and then that it's saying change by uh change when uh change by 30% then below here we have a change of capacity okay and we're saying uh adjust it by negative one so remove one instance when you're scaling out okay so hopefully that is pretty clear um the idea is that these need to have uh cloudwatch alarms so the idea is we would create a cloud watch alarm we're showing an example for the scale in and so notice here that we are passing to the alarm actions the scale up policy if you were to use the itus Management console and you were creating cloudwatch alarm you could um tell the alarm as an action to uh go to your scaling policy I want to just point point out that we're not showing the scaleout cloudwatch alarm as there's not a lot of room here and it will look very similar but the idea with this cloudwatch alarm is saying when the threshold meet 70 it's greater than greater than 70 then trigger this and the idea is it's then going to here add 30% more resources in the scaleout policy here in the top okay so when using a simple scaling policy it's recommended to set a coold down period which you see over here uh it's recommended to not use Simple scaling policy and cool down and instead use the step or Target tracking policies and so we'll look at the next policy here so step scaling policies change capacity in either direction by a certain amount at a different threshold which we call steps when a cloud watch alarm is repeatedly triggered so the idea here is that we have um something that looks very similar but um I'm just going to get my pen tool out here now we have a set as a step scaling policy okay this adjustment type here says percentage and change capacity right but the idea here is that we're going to set um uh steps and so each of these is a step so we're saying we're setting a lower bound and an upper bound and then we say how much to adjust the scaling so here between 0 and 15 it's going to adjust the scaling by 10 from 15 to 25 is going to adjust it by 20 and then if it goes it says lower bound 25 and I guess anything above that is going to be 30 and then it tells you the uh the minimum magnitude of the adjustment so hopefully that is pretty clear um again we've seen what it looks like to create a cloudwatch alarm so we don't have to show that here on the screen so it's just not being shown there but pretty similar to simple scaling but with steps Target tracking scaling policy automatically scales the capacity of your ASG based on a Target metric value so here we um are defining a Target tracking policy let me get my pen tool out here so see it says Target tracking scaling and then we're providing a configuration file and we're saying that we want to Scale based on the predefined metric of ASG average CPU utilization so when your CPU utilization goes over a Target amount 50% then it's going to trigger a scaling action what kind of metrics could we use here well we just saw ASG average CP U utilization we have average Network in average Network out request count per Target um but if you want you can use a custom metric how you do that I'm not sure but uh the point is is that they say you can Target tracking scaling policy will create two cloudwatch alarms whereas simple and step scaling policy you have to create the cloudwatch alarms yourself unless you're using databas console which can Will indirectly create them for you so um it is nicer to set up uh track tracking Target tracking scaling policies but uh yeah does a different thing okay let us take a look here at predictive scaling policies this is when you trigger scaling by analyzing historical load data to detect daily or weekly patterns and traffic flows you need a forecast of 24hour cloudwatch Data before you can create a predictive scaling policy predictive scaling policy will continuously use the last 14 days of data to tweak the policy it will produce hourly forecast for capacity requirements for the next 48 hours it will update every 6 hours using the latest cloudwatch data um so here's an example if you creating one notice the policy type is predictive scaling we're providing it a configuration file um and it's going to just vary when we're either doing a forecast or if we are predicting so if we we first have to do a forecast and so here we're saying uh we're going to uh use the uh autoscaling group CPU utilization at the Target group of 40 okay and it's set to forecast mode or sorry forecast only mode and then when we actually want to utilize it then we're going to set it as forecast and scale so very similar um obviously this is a little bit harder to do a lab on because you'd have to wait a period of time but uh there you go ASG termination policies decide the order of terminating instances and so here is an example of a termination policy and notice there that we see oldest launch configurations closest to next instance hour so there are predefined policies here we have default we have allocation strategy oldest launch template oldest launch configuration closest to next instance hour next instance um oldest instance and so we have a bunch there if you want to know about them in more detail read the um documentation because I got big blocks of text on that uh I didn't feel like it was worth our time to summarize because it's pretty straightforward what what most of these do uh you can create a custom termination policy by invoking a Lambda function so that is also an option if you need something outside the predefined policies but there you go so ASG instance refresh allows you to replace multiple instances when you have new configuration changes you're required to roll out so here is example of us setting the start instance refresh um and we'll talk about uh some of the things in here so instance warm up is how long you let the instance warms up in seconds if it's just not clear I'm going to get my pen tool out here we're talking about this over here the preferences so how long to let the instance warm up in seconds we have the Min and Max so here we're showing the men uh percentages this is the percentage of healthy uh healthy uh instances during roll out we have checkpoints this pauses the instance refresh after a certain percentage of replacement to verify the progress we're not showing that in the configuration on the right hand side skip matching so compare old instances to the new configuration and only replace those that don't match multiple instance types apply a new or updated mixed instance policy as part of the desired configuration Auto roll out if it fails it uh if it fails it will automatically roll back to previous versions cloudwatch alarm so notify cloudwatch so there you go so instance scale in protection prevents instances from being terminated on an autoscaling group so this feature is turned off by default you have to turn it on uh and it's very simple you just say um for new instances protect from scalein okay you can enable or disable scalein uh protect at any time uh like the time of creation or anytime after so it's not something like you turn on once and you can't turn it off but the whole point is to prevent from being terminated okay it's just termination production a launch template is an E2 instance configuration information used to launch an instance into an autoscaling grip so here's an example of us uh saying we want to create a launch template um and we are specifying our version so you have to give a description of the version and then you're going to provide configuration information the only parameter that is actually required ired is the Ami for a launch template but often you are going to be providing a lot of information so launch template includes the Ami the instance type the key pair the security group the user data the network interfaces the block device mapping and any other parameters commonly seen when configuring the launch of an ec2 instance a launch template is required to make an ASG work so you cannot uh not have a launch template when working with ASG you can use SSM parameter so uh simple systems manager parameters instead of the Ami for the launch template so you don't have to hardcode the the uh image ID so you know it says the Ami is required but we can change this out so it's more Dynamic when you create your launch template with the same name it will push a new version an ASG can be set to launch a specific launch uh template version on the latest uh version okay and a launch configuration uh if uh you'll see this come up but a launch configuration is the Legacy system that that are no longer used to launch instances with ASG the docs still have a lot on them because um some users still have them kicking around but you'll not find it in the a Management console anymore probably still does exist VI the a CLI not going to show up your exams anymore launch template is the way to go the key difference between them was launch templates had um versions that were grouped together whereas launch configurations were like youd have to make a new launch configuration every time you made a um a version and they just weren't grouped together there you go life cycle hooks enable you to perform custom actions when an instance is pending or terminating you tell the ASD to wait uh until uh it either times out because it'll have a default timeout or an external program says it can transition the idea here is that we will have uh a scale out that's adding instances it'll go into a pending state from there there is pending waight and pending proceed um and so we have a Le hook we can inject in here for uh when it goes to inservice now it's running let's say we want to go ahead and terminate it so we were scaling in and then for the terminating there is a terminating wait and terminating proceed and then it'll go to terminated so where are the hooks they're in between those States so for terminating we have one called easy to instance terminating that's what it's called underneath and we connect to the instance download logs do whatever we need to do before we terminate it and then there's obviously one for launching this is where you can install configure software do whatever you need to do before it's ready to receive traffic um let's just take a look at this in more detail to fully understand how it works so we just have an example of scaling out so adding an instance and we have our um our hook our Hook is going to go out to something like a vent Bridge SNS or sqs and then from there they're going to call some type of compute it could even be the same uh the same compute that we're using to launch the instance but you probably would want to keep them separate if you can um this compute is going to do something programmatically and then it will use the SDK to make a call to ASG to say that the life cycle is complete so we're telling it the life cycle is done here and it's going to go back and now it's going to go to proceeding uh pending proceed and then it will go to inservice here is what it looks like to set up a life cycle hook notice that we have this notification Target AR and that's how we're specifying either SNS or sqs if you are targeting event Bridge you actually don't explicitly Define in here you're going to implicitly set up a rule uh trigger and event there is the heartbeat this is how long long it will remain in that state so this is your timeout and the default is 1 hour but you can set it to whatever you want to be let's just talk about the three scenarios about how we would programmatically work with it so create a notification Target and I am roll the target can either be an sqsq or an SNS topic the role allows ASG to publish life cycle notifications to the Target create a Lambda function and a rule that allows event bridge to invoke your Lambda function when an instance is put into a weight State due to a life cycle hook or create a launch temp plate with a user data script that runs while an instance is in a weight State due to a Lifey hook so you can perform the stuff on the instance or an external compute so there you go a warm pool is a pool of preinitialized ec2 instances that sit alongside an autoscaling group so a warm pool decreases latency for your app uh that have long boost times an ASG can pull from the warm pool to meet its desired capacity when an instance leaves the warm pool it's called a warm start uh the warm pool size is determined by the difference of Max size Max prepared capacity and desired capacity you can keep instances in the warm pool in one of three states stopped running or or hibernated life cycle hooks are used to put an instance into a weight State instances Reus policy and can return instances back to the pool instead of being terminated here's an example of us um setting up a uh warm pool apparently it's not as hard as I thought it would be uh you set a Min size the max group uh prepared capacity and you set set the state of the pool state to be running so yeah uh clearly uh this could be very useful if you are worried about the latency of instances spinning up but of course you're going to be paying for these instances that are on standby that are going to then uh take over but there you go hey this is Andrew Brown we're taking a look at detaching and attaching instances for asgs you can detach instances from your asgs and that instance will become independent and can either be managed on uh on its own or attached to a different autoscaling group let's talk about attaching first so if you had an instance that you wanted to attach you provide the instance IDs the auto auto group name it's as simple as that for the criteria of attaching the instance needs to be in a running State the Ami has to still exist uh it should not be a member of another ASG it should be in the same AZ because you can't bring an instance in a different uh in an AZ that the ASG cannot service for detaching uh the instance has to be in service uh State okay if you detach an instance in a standby State then this could cause issues and it might terminate other instances so make sure that it's in the correct State you can also decrement or reduce the desired capacity at the time of the Detachment so I'm just going to get my pen tool out to make this really clear but notice here very similar just attach and the detach okay and then down below here if we need to we can uh uh uh decrement the desired capacity why would you want to do that well the idea is that um let's say you want to get rid of one but you don't want one to replace it right away and so you'd want it to reduce by one but there you go so we know we can attach and detach an instance but another useful way of debugging instance would be to Temporary removing the instance and so what we can do is we can take an instance that's in service and move it into a standby state to update troubleshoot that instance and return it back uh uh to be in service now uh when we looked at the life cycle hooks we didn't visualize this so just imagine that there is a a line going off the box of inservice to standby because that is part of the life cycle of um asgs if you want to enter standby pretty straightforward we are saying enter standby uh we're saying the instance ID the Autos scaling group and it says it should decrement looks very similar to attach and attach but um apparently this is just a separate State um very similar for the exit but instances that are on standby are still part of the Autos scale group but they do not actively handle load balancer traffic and there's the exit pretty straightforward okay suspending processes can be useful when you need to investigate or troubleshoot an issue without inference from scaling policies or scheduled actions here's an example of us suspending a specific process so we're we're suspending health check and replace unhealthy um or we can suspend all of all of the processes and we're talking about processes we're talking about autoscaling processes not the ec2 instance um so the options we have here is launch terminate add to load balcer alarm notification AZ rebalance health check instance refresh replace unhealthy uh scheduled action so if we need these to stop for a while as we are trying to troubleshoot things and we don't want it to um the Autos scaling group to do certain things this is what we can do okay there are plenty of reasons why an autoscaling group will run into issues but let's take a look at uh what could be the reasons for it and how we can get more information about the exact thing that has happened so imagine we want to launch an ec2 instance these are things that will cause it to fail the ASG is not found the a is no longer supported uh invalid block device the instance type is not supported in the a uh this happens with in CA Central one uh D if you try to launch a T2 micro they do not like it you it has to be a T3 uh key pair does not exist launch configuration not supported play basement group not allowed to be used in instance types Security Group does not exist if we're having Ami issues it could be the amid does not exist the Ami is pending cannot be run uh the virtual name is invalid the architecture is mismatched for load bouncer issues if we have a load balcer attach attached to ASD it can't find it the instances are not in the VPC there's no active load balcer the security token is invalid if we're talking about capacity limits maybe it's the maximum number of instances are already running youve reached Your Capacity allowed for the a so how can we get more information about it well there's a CLI command called describe scaling activities and I never see this in the adus Management console if it's somewhere there I've never seen it I've always used the CI command for this and it's super useful so remember this when you run into an issue and see if you can get more information so there you go hey this is angre Brown let's take a look at EBS but before before we do let's define some terms up front so we know what they are the first is iops iops stands for input output per second and it is the speed at which noncontinuous reads and wrs can be performed on a storage medium high iio equals lots of small fast reads and wrs what is throughput the data transfer rate to and from the storage medium in the megabytes per second unless it's megabits but uh also what is bandwidth this is the measurement of total possible speed of data movement along the network think of bandwidth equaling pipes and throughput equaling water so let's talk about EBS EBS elastic Block store is a highly available and durable solution for attaching persistent persistent block storage volumes to an ec2 instance volumes are automatically replicated within their a to protect from component failure the types of volumes you can deploy we have general purpose we have general purpose uh version three we have provisioned IO so io1 we have uh provisioned io2 we have IO block Express we have cold hard drives we have throughput optimized hard drives we have magnetic so there's a lot of stuff going on here and I mean this is a general summary the idea is that usually when you have a newer version it's going to be cheaper or faster and that was the case for gp3 um I2 does not exist anymore basically you should always use I2 and we'll talk about that right now so all I2 volumes created after November 21st 2023 are io2 block volumes I2 volumes created before 2023 can be converted over to an io2 block Express volumes we're going to go over in detail all these types of volumes you have to know them all uh not so much io2 but I2 block Express um so we'll compare that next here okay all right uh let's take a look here at volume type usage first we'll compare the solid state drives and then the hard disk drives uh so the first thing is we have volume types gp2 gp3 io1 io2 block Express we're going to pretend like io2 does not exist because you should not use it anymore let's look at the use case for general purpose most General workloads transactional workloads virtual desktops medium size single instance databases low latency interactive apps boot volumes and development and test environment when you don't know what to choose choose the general purpose volume such as gp2 or gp3 uh sometimes people just use gp2 because the free tier is on gp2 as opposed to gp3 but if you're going to run something I'd run it on gp3 because you're not the free tier 2p3 is going to be more cost effective anyway we have io1 so workloads that requires sustained iops performance of more than 160 iops through iio intensive database workloads for io2 block Express these are submillisecond latency substained iops performance more than 64,000 iops or 1,000 uh uh megabits per second of throughput so just remember the 16,000 the 64,000 that's going to help you choose the volume because exams might ask you to choose the correct volume for durability we have 98 99.8% to 99.9% for io2 it's the same and for IO block express it just happens to be 99.999% the volume size is between 1 to 16 terabytes for gpt2 and gpt3 for io2 it's four G uh uh G gigabytes between 16 well I say it's gigabits when you see the ey it's gigabits so it's four gigabits between uh 16 terabits and the last one is four gigabits between 64 uh terabits so a lot of storage for io2 Block Express the max iops here is 16,000 this one's 64,000 this one's 256,000 remember those values that is important Max throughput is 250 megabits per second for gpt2 1,000 megabits per second for gpt3 for io1 we have 1,000 megabits for io2 Block Express we have 4,000 megabits we can not attach EBS uh multiat for general purpose but we can for iops provisioned SSD uh there is no NV NVM reservation so you can't reserve nvms uh but for I2 block Express you absolutely can that doesn't mean you can't use MVM with other ones it's just in particular it's about reservation for boot volume they can all be used for boot volume let's take a look at the Hard dis drives we have throughput optimized cold and magnetic S1 SC1 and standard Big Data data warehouses log processing for ST1 for cold it's going to be throughput oriented storage for data that is in frequently accessed where low storage cost is important and then magnetic is workloads where data is uh accessed infrequently Magneta is going to be um the cheapest at volume right for durability we have 99.8 to 99.9% uh we don't talk about durability here because it's it's not represented in the same way but if you want to think about durability these magnetic tapes are good for 30 years so there's not going to be an issue here um it says infrequently access but really like the last one is more like you never access unless you really like really infrequent because it's super cold storage right volume size here 120 uh 125 gigabits between 16 uh terabits here it's between 1 gigabit and one terabit we have 500 um 500 for the the max iops here it's 250 then 40 to 200 Max throughput is 500 megabits 250 megabits and then 4090 megabits you cannot do EBS multiat here not available for Magneta because it doesn't make any sense uh for boot volume nope uh for magnetic it is supported okay so there you go all right let's just make sure we are familiar with hard disk drive so there's an example of one you if you're old enough you probably remember these things um a hard dis Drive is a magnetic storage that uses rotating platters an actuator arm and a magnetic head similar to a record player uh they're very good at writing at continuous amounts of data they're not great for writing small reads and wrs so think of the arm of the record playing a player having to lift up and down and move around um they're better for throughput they're they have physical moving parts so they have more wear and tear uh so you know they're more likely to have issues in terms of reliability uh there's this idea of RPM because there is a thing that's spinning physically so we should know what these are this is revolutions per minute the speed at which the drive platters spin the platter is like the record faster RPMs mean faster access time slower RPMs mean better cost savings and there's actually specific RPMs that we might care about versus 72,000 RPMs these are standard for desktop high performance external drives offering a good balance of performance cost power consumption we have 54,000 RPMs used in laptops external drives apps where lower power consumption and Heat our priorities over top performance and then we have 10,000 so I like how I went I should have put like 54 above and then 72 I don't know I did in that order but mostly found in Enterprise environments or highend workstations where performance is critical though these are less common today due to the rise of solid state drives so yeah there we go all right so when we're talking about hard dis drives we need to also understand raid which is the redundant array of independent discs it is a data storage Virtual Technology for magnetic discs not magnetic tapes but diss to improve fault tolerance rate combines multiple physical volumes into a logical group storing redundant data ac across diss since hard drives H have mechanical parts and will result in where hard drives are more prone to failure than solid state drives if you have very important data you're definitely going to want to have raid configuration and so raid has like a number uh like raate zero raid one whatever and this is going to determine how it operates so raate zero which is called striping has no redundancy no redundancy the data is split across disc for high performance so increases the speed and capacity but offers no fault tolerance so it's just like when you want to treat multiple discs as one dis minimum of two discs required we have raid one called mirroring this is where data is duplicated on two or more discs offering High redundancy if one dis fails the data is still accessible from another requires it at least two discs so this is where we're copying or mirroring stuff to another place striping with uh parity parity which is raid five combined striping and parody for both speeds and data protection can withstand the failure of one drive without the data loss requires at least three discs then we have raid six striping with double par so similar to raid five but with double parity allowing it to survive the failure of two discs requires at least four discs then we have RAID 10 which is actually 1 plus o where is two three and four I don't know don't ask me I have no idea uh but these are the most common ones you should know a combination of raid one and raid zero offering redundancy and increased performance requires a minimum of four discs it's good to know raid zero and raid one as those are the most common that you'll come across but there you go hey this is Andre Brown we are taking a look at solid state drives so this is an example of a solid state drive where the interior is open so solate drives use integrated circuits ic's assemblies as memory to store data persistently typically using flash memory ssts are more res uh res uh resistant to physical shock uh run silently and have quicker access time and low latency an example of where that data is stored here is it's using nand flash memory so nand is end is like a logical operator that's why that's there's and there's I think there's like nor as well for that kind of stuff um there very good frequently for frequent reads and writes no physical moving Parts as you can see there's no platters there's no actuators inside of those uh there's a variety of types of um SSD and it really depends on the connection type so we have SATA ssds widely used uh compatible with most computers offering good performance but slower slower than mvme due to SATA interface limitations we didn't talk about SATA or interfaces with the hard disk drives but there are SATA hard disk drives of course and uh older formats I forget what the older one is ID or something I can't remember it's been it's been too long we have nvmes the reason I want to cover this was just to talk about MBM really but MBM use the pcie interface for higher performance ideal for intensive data task gaming available in the m.2 uh or the pcii card factors we have m.2 suit compact this is suitable for laptops compact PCS use SATA or mvme interfaces they install directly on the motherboard we have two uh U2 u.2 SSD similar to the performance of the n.2 nvme ssds but designed for 2.5 in Drive Bays mainly used in Enterprise servers environments portable ssds external drives for uh easy portability um then we have PCI ssds these are addon cards that provide high performance older systems you're going to see the word mvme a lot in in cloud service providers so I figured getting a little bit of knowledge about where the stuff's coming from is might help cement that know later on let's talk about SATA so the uh SATA which stands for serial advanced technology attachments these are the most common and are compatible with Serial ATA interfaces okay used by most desktop laptop computers they're relatively easy to install and offer good performance though they are typically slower than nvme counterparts due to their limitations of San interfaces so somebody's not using mvme it's probably using SATA this is what the SATA port looks like this is a SATA cable as you can see it goes from Big to small here is a SATA SSD where it has a data and power source as you can see there we have PCI SSD uh these are addon cards that fit to into the pcie slots on the motherboard they can offer High very high performance especially in configurations that support mvme they're a good option for users looking to upgrade older systems or specialized high performance tasks the PCC slot on the motherboard comes in different sizes referred to as Lanes obviously the bigger the lane the better the better the performance but there's also Generations besides just the sizes we're going to be looking at per Lane in 16 slots but as we went through time and notice 2003 2005 every year it's basically doubling it doubled you can see here on the right hand side so you can see the bandwidth is increased again and again and again and again and again the latest being 2022 for pcie 6.0 um we have nonvolatile Memory Express this is what we really wanted to talk about about so uh uh nvme ssds use the pcie interface that's why we're talking about the interface offering significantly higher performance compared to SATA SSD which is why we talked about SATA SSD so you knew what it was they are designed to take full advantage of highp speed high speeds of flash based storage Technologies mvme drives are found in the form of m.2 but can also be added to a computer using a PCI expanded slot uh so we have m 2 ssds again these are nvmes or sorry these are just form factors basically compact suitable for laptops and compact PCS can use SATA or mbme interfaces they're installed directly on the motherboard they look like this then you have um a u.2 SSD similar to Performance to mmes but designed for 2.5 Dy Bays mainly used in Enterprise and server environments they generally connect to the U the .2 Port they can connect to other types of ports but they usually have their own port but I just want to show it to you because you probably never seen a uh if you again haven't worked with in our data centers or other places you may have never seen a point uh .2 SSD but you saw a graphic of the SATA one which looked like a normal drive then you have the m.2 which looks like a stick and you got this uh uh U2 which is a bit more of a square so there you go okay tape so a large reel of magnetic tape a tape drive is used to write data to tape medium and large siiz data centers deploy both tape and disc formats they normally come in the form of a cassette magnetic is very cheap to produce and can store considerable amounts of data if you remember like supercomputers from the 60s or 70s they are using Magnetic Tape durable for decades good for at least 30 years cheap to produce they use a tape drive to read the modern cons setes this is an example of a modern conset where the tape is on the interior um we did Cover more magnetic stuff in um uh another section so you'll get the knowledge across both of those but there you go okay all right let's talk about moving volumes around so EBS uh volumes in order to move them to other places you basically are creating a snapshot if you want to move from one a to another you're going to have to take a snapshot of the volume create an Ami from the snapshot launch a new e instance in the desired a if it's going from one region to another you're going to take a snapshot of the volume create an Ami from the snapshot copy the Ami to another region and launch an ec2 instance from the copied Ami I hadn't seen on recent exams I'm really you know testing like how would you do this but it has been exam questions in the past and it is a a bit of an A different kind of way or you expect this to be a bit easier but these are a little bit involved so you should know these to make sure you understand how uh the restrictions around moving volumes okay all right so an ec2 instance will be backed by a root device which will be either EBS volumes or instance store volume we need to understand the difference between these two so for EBS volumes it's a durable Block Level storage device that you can attach to a single ec2 instance where an instant storage volume is a temporary storage type located on discs that are physically attached to the host machine for EBS volumes you can they can be created from an EBS snapshot for instance store volumes is Created from a template stored on the host machine for EVS volumes you can start and stop instances data will persist if you reboot the system for instance serve volumes you cannot stop instances it can only be terminated you will lose the data when that thing is terminated or if the um instance fails this is EBS volumes is ideal for when you want data to persist in most cases you're going to be wanting to use an EBS black volume but uh instance store volumes are ideal for temporary backup or storing apps cach logs or other random data so there you go let's talk about partitioning scheme so we have the master boot record and and uh we have guey partition table these are two different methods for storing partitions information on a dis they are fundamentally different in how they manage this information their capabilities you'll see these options when utilizing um block storage okay so let's compare the two so for compatibility MBR is great for older operating systems and is for bios for GPT this is modern operating systems and UEFI firmware if you ever booted up a computer and went into the Bios mode or the boot mode that's the thing you're looking at bios or the UEFI if you have a modern computer you're going to be using ufi most cases the partition limit for MBR is four partitions primary partitions three or three primaries with one extended multiple logical partitions GPT can have up to 128 partitions there's no need to extend or have logical partitions the dis size limit for MBR is 2 terabytes and then the limit for uh GPT is up to two ter starts at 2 terabytes up to 9.4 exabytes so they get pretty darn large for MBR the partition table uh is stored in MBR itself for GPT they use a separate partition for the partition table known as the EFI system partition or ESP the boot process you'll see bios that looks for boot loader in the first sector of the dis for ufi the it reads the EFI system partition to start the boot loader there uh uh no unique identifiers are supported for MBR uh GPT supports unique identifiers so uh which are guids for partitions there you go so logical address blocking is an important concept as we are working with block storage we got to talk about blocks so logical block addressing LBA is a common scheme used for specifying the location of blocks of data stored on storage volumes blocks are located by an integer index with the first block being LBA Z the second being lba1 and so on uh I mean the the diagram here is showing like uh one of the The Platters or or discs uh in a hard disk drive but the concept is similar for solid state drives uh different block sizes for logical addressing in file systems and storage devices are used to balance efficiency performance and management of storage space block size can be specified during the formatting of a storage volume block size has a direct impact on on volume size the the industry default size for logical data blocks is currently 4,096 bytes or four kilobits um block size and max volume sizes that you would normally see which is the four kilobits to uh which would result in a Max storage of 16 terabits that's industry size so the one is the size of the block and the other one is the max volume size right so uh 16 terabits is what you'll get for 8 kilobits you get 32 for 16 you get 64 4 for 32 you get 128 for 64 you get 250 258 so hopefully this gives you an idea like what block storage is and that there's some efficiencies around it but there you go hey this is Andrew Brown in this video uh we are going to learn about uh creating an EBS volume and attaching it so I'm going to show a few things VI the UI I'm going to assume in the future the UI is going to change here so I really want to Pro focus on the pragmatic way of doing it but I'm just going to click into um here so you have a general idea so when we create an instance uh we have an option down here to um attach volumes um so if we go down below here storage we'll have our our def our root volume here and then we can add an additional volumes if we go to advance we should have uh more options here to configure our volume but uh what I want to do is I want to create a volume uh this way so uh independently and then attach it uh to a volume but anytime we launch up uh an E2 instance we do have to have some kind of volume there so I'm going to go over to our ad examples and I've opened this up in git pod as per usual and what I want to do here is make a new folder see I have a lot of folders at this at this time of the video but I'm going to go ahead and say make directory EBS um and we'll just say make directory basic oh sorry we'll CD into that not EB I want to go to EBS basic and I'll CD into that okay and so what I want to do is just get a ec2 instance set up here so I should have an ec2 um uh cloud formation script somewhere so let me just go find that quickly I think I recall having one here up in the ad ASG and so here is one where we have a very simple one so I'm going to go ahead and just uh copy these two files out if it's the future you already have this uh in the correct folder so you just understand I'm doing what I'm doing here but uh we're going to go to our new EBS directory I'm going to go ahead and paste these files in here and we'll take a look at what we have so I'm just going to rename these so that it's a little bit nicer so deploy um E2 instance let's just say um EBS basic okay and I'm going to go over here and we're going to take a look here so this is for T3 micro and I have an Ami set up here and so if you need to fill these in just go over to ec2 it's not too hard just go over to ec2 here go to instances launch an instance and you can go grab the Ami for your particular uh region over here so here is one uh that is there I already know this is properly configured so I don't have an issue same thing with your vpcs and subnets go over to um go over to uh VPC I just knocked my mouse over VPC and grab those values that you need and I'm again just using the default vpcs in this case so I know that these are accurate because I've been using them quite a bit here uh and then I'm using the T3 micro T2 still exists at this time but um some Services they don't allow you to launch t2s anymore so I'm just being Progressive here using t3s there are t4s but um t3s is what's most likely going to be in the free tier so let's scroll on down here we have um a a poy server that's starting up here and the security groups that are open um so that is pretty good I think the only thing also I'd like to add here is um I would probably like to add into this um an SSM roll so just give me a moment to find that okay all right so I think I found one right over here in uh my autoscaling group and so yeah I have one right here so I'm going to grab these two parts okay copy this I'm going to go back over to here uh to our last template which was an EBS and I'm going to go ahead and just uh go down here and paste that in and so what we have here is an SSM Rule and it's just going to allow for this Amazon SSM manage instance core so that we can use sessions manager to get in I do need to attach the ec2 instance profile um so that is something that we'll want to do um so just give me a moment to pull up the docs here so we'll go over to our ec2 instance resources and there should be a profile Somewhere In Here I Am instance profile yep I'm going to go ahead down below to our ec2 instance and I'll place this in and I'm going to assume that it's ra it might not be ra but I'm going to try anyway okay and so we have our I am profile here now notice we Define our network interfaces in here we are not defining any block storage now we could do that if we want to so if we go up to here somewhere here it should have something for our block device mappings yeah and so the block device mappings entries that Define block devices to attach the instance so we go into here I'm curious if it if this is for creating them on the fly or if you can associate an existing one so specifies a block device mapping for an instance you must specify exactly one of the following properties virtual name EBS or no device um so we go down below parameters used to automatically set up an EBS volume when the instance launches we'll go into here and notice that it's not referencing an existing one it actually creates a new one uh so we can configure this if we want but I'm going to leave it alone because it's going to default in set something up for us and I'm that's not what I'm interested in I'm interested in creating an isolate EBS volume and then attaching it uh to this here so I think this is okay this seems like this is fine I'm going to go over to here and we are going to take a look at our deploy script everything looks fine the names look fine and I'm going to go and just in case I have I'm going to chmod that file okay and we'll go ahead and deploy this this is going into CA Central 1 um and so it has one little little error here with the I am profile which was the only thing we really added here I am instance profile unresolve dependency did I not copy that name correctly easy to instance profile copy this again go back up to here and I'll type in clear and we'll try to deploy this again my G pod is uh messing up so I'm just going to give it a nice refresh here Cloud developer environments are always kind of finicky and we're going to go ahead clear this out and I'm going to go and deploy this and I'm going to make my way over to cloud formation and we'll go over to change sets here and we'll execute our change set while this is deploying let's go take a look at EBS under ec2 so we can kind of see the options that we have so it's under here under volumes and we can create a volume and notice that we have our different types for this we can set the gigabyte size the iops the throughput for Mega megabits per second the availability Zone we probably want to launch this in the same a as our server so that's something we have to consider um notice we have the encryption option uh so that's all the stuff that we have here okay so what I want to do is I want to uh wait for that volume to spin up and while that is happening I'm going to go ahead and uh we're going to add some additional code here okay uh because I want to attach uh more volumes and so if we go here on the left hand side there should be something for EBS we have this volume attachment which I I think that we'll be using later on I'm going to go ahead and just paste that in here and if we go to examples here yeah so we can go ahead and create our volume also looks like it defines the mount point which is interesting volume attachment oh this is oh sorry they're calling it Mount point all right so what we'll do is we'll copy this oh yeah this is the Mount point and the volume this is actually perfect we need both of these so I'll go ahead here I'll paste this in and this one is really for uh this one here the volume uh attachment and I also want the volume so we'll go ahead and grab this as well okay and our E2 instance is called my ec2 instance so we'll have to go update that down below here this is my ec2 instance and then here we have my E2 instance and notice that it's launching the same availability zones and it's actually a good idea if we add a tag here because it's going to get really annoying if we can't tell our new volume so I'm just going to give it the name tag so that that will show up um I'm just going to make this 20 because I don't want the biggest volume in the world let's take a look at some of the properties okay get this out of the way so we have Auto enable IO so it indicates whether the volume is auto enabled for iio operations EBS disables IO volumes from attached to St instances when it determines the volume data is potentially inconsistent so that's one thing gra avilability Zone encrypted or iops multi attach enabled um size okay so that all that stuff seems pretty straightforward so whoops uh what I want to do is go take a look and see if our if our uc2 instance is ready and we have an instance all right and I want to go ahead and connect to this and we're going to use sessions manager so I want to take a look at the actual disc volumes and see what we have here so we'll type in pseudo Su hyphen C2 user okay we'll type in clear and if we type in like dhf I can't remember the is it DF or DH uh we'll say man DF oh this it this is it so DF reports a file system and H is for human readable so we'll do DF space hyphen H it's going to tell us what we have here and so what I'm looking uh for is to see um our our volume of data so let's go take a look at our e instance and we go to storage wherever that is it says that we have a volume of 8 gbits and it says the device name is Dev xvda okay so we go back over here and we see uh Dev but I don't see our volume so what happens if we do this I was kind of expecting to see the volume here so just give me a moment okay all right so I think our volume is here it's just that it's partitioned and so I'm assuming that you see this Dev directory I'm assuming that uh we have this uh more in one partition so we have um our actual volume that we're on right now and then we have over here uh the boot volume so I think that is what is occurring here so um you know again I'm not the best at storage but you know I've done enough of it to kind of have an idea what's going on so that is I think what is happening here um so now what I want to do is go back over to here and we have our new volume and let's go ahead and see if we can uh deploy that it says unresolved dependencies my easy to instance so something is spelled wrong here I'm going to go up uh here copy that and I'm just going to make sure I spelled that exactly the same I think uh it's the casing is a little bit different and we'll go ahead and deploy that I'm going to go back over to here to cloud formation and we'll go into E2 basic we'll go to our change sets and we'll go ahead and execute this and so we'll just give a moment for that volume to create and we'll be back here when this is uh successfully done all right so that update is complete so I'm going to go back assistance manager um as I believe we are still connected and we'll do dfh oh we'll just clear this here DF DFS and what I'm looking for is our additional volume and uh I can't remember what we said it would be maybe 20 uh 20 Gigabytes so but let's go take a look at uh EBS uh EBS ec2 I wish they make that searchable I don't know why they don't do that it's been years come on people we go over here we see our new volume oh it's gpt2 I it's fine but I want a GP uh three I guess that's okay and so we're looking for uh that additional volume so notice that I do not see it um let's go over to our ec2 instance and so this is our current one over here and we'll go over to storage and so we can see that the device name is here so we're going to have to do a little bit more work to get this to show up actually this one's 202 so let's go back and take a look here um yeah so we're going have to do a little bit more work so just give me a moment here to get the next step all right so I've gone over here and there's another command we can use called lsblk and so the one that we're using DF which is a Linux command shows you um uh dis usage but this one actually shows you uh block storage so I think if we run this we're going to get some more interesting information here okay and so now we can kind of see that we have um uh this volume here the 8 gbes and how it is being partitioned into our three partitions so that makes more sense definitely easier than this up here and then down below we can see that we have this other volume here but it's not partitioned so I guess I guess what we'll have to do is partition it so that we can start using it but let me just see what the next step is okay all right so it seems like we should check to see if it actually has a file system on it so it's telling us to use the pseudo command uh is it xvda 1 I don't know we'll take a look here it is sdh so that is totally fine we'll go ahead and just paste this in for a second and I'm going to change this to S is a DH is what we said here go back over to here sdh um well what we could do is just auto complete and see what's here uh yeah we'll just do that and maybe what it is it's xvda 1 so we go ahead and try this xvda 1 and this one has a symbolic Li link to this so I'm guessing it's not that I'm looking for the s's here there's an s in here okay SD oh okay so it is I maybe I just wasn't paying attention I thought we had an error but it is so this is the symbolic link to this and so this is our dis um it didn't show us anything interesting here so if we go back here notice it says what file system it is using so I guess the next thing we need to do is apply a file system all right so we need to create a mount point so we'll go ahead and create that wherever we want uh they're suggesting data I'm going to go back over to here we'll just say pseudo uh make directory I'm going to say new volume so now we have this as our uh new place to mount our volume okay and so then we can just mount it that way but again there's no file system so I I keep feeling like there's a step that is missing here again not the best with volumes B is the one that's uh a lot better at this but uh we'll we'll stumble our way through here I don't have to go get boo on this we'll say pseudo Mount and we want to do Dev SD and then we'll say new volume wrong FS type bad super block okay so I imagine what we have to do is get a file system on here so just give me a moment okay all right so um I found what we need to find out so we can type in pseudo MF uh M mkfs so make a file system and then we can specify one that we want to utilize we can do xfs or something like uh ext4 I'm not the best at this I'm just going to choose F FF and I hope that it works here um I'm going to go ahead and we're going to go to our Dev directory and this one was s DH okay so I'm just going to hope that that works and it looks like that it probably worked um so now we can probably go Mount the volume but before we do let's just go back up to our Command and see if it tells us what it is so if we go here it just says there's a symbolic link there um I mean it looks like it did something here so I'm I'm assuming that it worked but let's go back up to our Mount and we'll attempt to mount this and now mounted okay great so now we have our mountable drive I'm go back to our lbk Command wherever that was lsblk and so we can see our new volume and its Mount Point let's go back to DF hyphen H there we go and there is our new volume so we are in great shape just so that you don't have to remember all these commands what I'm going to do is go over here and just quickly document them so I'll go ahead and say new file readme.md and just give me a moment to type it in all right there we go so I uh wrote those all in here so that saves us some time we can go to our new volume now and if we want to we can just make a file so say hello.txt and we have that so we'll type in pseudo and this just has to do with the permissions of the volume so we do LS hyphen LA on our new volume um I think we made it at root right yeah it's here and so only root can do that and that's why we're having to type pseudo Su uh we can change those permissions if we want to um I think we say own and say ec2 user ec2 user for the new volume I'll see if that helps out here and we'll put suit in front of that so we can do that and so now if we go into our new volume we just say touch goodbye txt there we go and so then we can work with that so that's how we would uh work with that additional volume I think that is sufficient um uh for for this part of the video so I think we are good and we are done here so we'll go ahead and clean up so I'm going to go over to ec2 here and we'll go to instances or sorry cloud formation I'm getting a little bit mixed up here today cloud formation and we will tear down our stack and that should take down everything all right so we'll just give this a refresh and I'll see you in the next one okay ciaoo hey this is angre brown in this video we are going to look at uh extending uh the size of a a volume after we've launched a ec2 instance so I've actually done one run with this and I failed so I already have some of the set up here but in the resize folder I have a template for us to deploy it's the same as our basic one but I just removed that additional volume and so we're going to go ahead and deploy that so go into that folder deploy that make your way over to cloud formation okay and we'll give it a moment to think here I'm not sure why it's uh having an issue could be my internet it might be my internet yep it's my internet just give me a moment here okay all right my internet is back I'm going to go over to here I'm going to give this a hard refresh we're going to make our way over to our cloud formation Stacks I'm going to go ahead and execute this change set so what I'm waiting for is this to deploy once this is done we'll go and take a look at that ec2 instance okay all right so it looks like our instance is ready we're going to make our way over to ec2 uh and we'll go over to instances here and we'll go ahead and connect to this instance uh well it has to be the one that's running so and just remember that we launch this with a T3 micro meaning that we are using the Nitro system if it's T2 it's going to be on the uh Zen uh uh Zen system so that's really important because when we want to go ahead and modify our volume um and do things it uh it's going to change the requirements but anyway we're in here and we want to type that command we know what is it lbk lbk there we nope I cannot remember it to save my life uh the command is lsblk Okay so LS BLK so list blocks and so here we have it and it's at 8 gigabytes and the idea is that I want to expand that or resize it so what we're going to do is go over here and I have the command we can grab this from the mod modified volume it's basically the exact same thing and uh you need to grab your uh EBS uh volume so go to your volumes and we'll go to here and we'll grab this value and we'll replace it as such and I'm just going to change it to 12 gbt okay so we'll go ahead and copy this and we'll paste it and hit enter okay I have my in credential set up of course you have to have your profile set up like all of our videos and so over here if we go back and check again notice that it says 12 but our partition here only has eight so we want to make all of it available here and so we're going to have to follow some steps for extending the file system so they have instructions here um it's going to vary whether it's Zen or Nitro it's more less the same it doesn't look a whole lot different um but anyway uh I kind of uh just saved us some time here and I've grouped them together and so we need to grow part and do then do xfs grow grow FS so grow part is used to expand the partition to the whole dis and f f f xfs is the grow FS to use and resize and apply the changes so I believe that this is what that is we'll just double check again to make sure this is the case but this one says nvme0 N1 nvme0 N1 so that seems fine so go ahead and copy this and now we'll run this here on our server okay it says it's new size we'll go take a look here and notice it's 12 but I believe that we still have to run this so we'll go ahead and run this okay and so now we are done it is resized we'll go ahead and type this in again okay and so that's how you would uh resize this just understand you can't just modify volume and hope everything is good now could you restart the volume and would it know about the resize I don't think so um I don't feel like testing that if you want to go find out you can do that yourself but uh yeah I think that satisfies um resizing our volume so we'll go ahead over to cloud formation and we will tear this down and I will see you in the next one okay ciao hey this is Andrew Brown in this video I want to take a look at EBS uh encryption so we'll go ahead and make a new folder we'll just call this one encrypt encrypt and so I'm going to copy the uh basic one in here actually just all of them probably and we'll go ahead and paste this in here I'm just going to clear out the REM me since we don't need that right now and so we'll go intoy our deploy I'm just going to change this to encrypt assuming I spelled that wrong uh right if it's wrong you can fix the spelling I'm dyslexic I always spell things wrong forever and ever and so in this one we have um a volume that will get deployed with a new volume that will be attached and the thing is is that we do not have this encrypted by default so if we wanted to we can configure this for encryption so see where we have this encrypted option and here it says indicates where the volume should be encrypted so ebx volumes must be attached to the instance that support EBS encryption and it's just a Boolean so if we go ahead right and put this flag on here and say true then this should launch an encrypted volume okay okay we could probably also set that for our um base one if we want to encryp the base one we'd have to define the the block mappings as we saw before we going to CD into this other directory and we'll go ahead and deploy this the reason we don't have to chmod is because we copied it so it has the executable commands over there as per usual make sure you have your credentials so you can go ahead and do this and here it is I'm going to go ahead and change set accept it we're going to launch this and then we're going to confirm uh if it has taken effect the actual um encryption but that's only one scenario the other scenario is what happens if you have a volume that is not encrypted and so that is something that we will uh tackle after we confirm this is encrypted okay all right so our volume is uh ready let's go take a look here we'll go over to ec2 and we'll go to instances and we'll take a look at our running instance okay and so I want to look at the storage and we're seeing that it is encrypted so there you go this one is encrypted if we want to uh encrypt an existing volume what we're going to have to do is we're going to have to uh create an encrypted snapshot or sorry create a copy and then create a snapshot and that's the way it's going to work so let's go ahead and see if we can do that so I'm going to click through this volume here and I'm going to just checkbox on this okay and we're going to go ahead and we're going to say create a snapshot and we'll just say my example uh snapshot okay we'll go ahead and create that and I believe that it is this one because it's eight uh or no maybe it's not there yet oh sorry snapshots snapshots I'm getting mixed up here so notice that it's pending okay and so we are waiting for this snapshot to create okay all right so now it's complete and so in here we have our example snapshot and we need to go ahead and create a copy of the snapshot and when we do this now we're able to checkbox on encrypted okay and so from there we would have that and then we would create an an Ami an Amazon machine image uh that um utilizes this uh volume okay so I'm not going to go through all that because it's just going to take up time and then we'll have to just delete all the stuff down and all you had to see was that there was a check boox there but I'm going to go ahead to back to here and we're just going to clean up here so this was our snapshot I'm going to go ahead and delete the snapshot and we're going to go back over to cloud formation and we will tear this down and I will see you in the next one okay ciao hey this is Andrew Brown in this video we're going to take a look at U multi attach so I'm going to go ahead and make a new folder called multi attach and in here we're going to copy some stuff over here I'm just trying to think which is a good one probably our basic which uh has um uh some good stuff in there we'll go ahead and paste that in and the reason I want to use that one is because it will deploy a volume as well as a dc2 instance I'm go ahead and just call this one multi attach multi seems like it's very similar to EFS except it's a regular EBS volume that allows you to attach to multiple uh ec2 instances that are in the same availability zone so what I'm going to do here is I'm going to uh create uh two ec2 instances I'm going to get rid of this user data we don't need both of them and uh the network interface is fine but we don't need that as well so I'm going to go ahead and just remove this and it will just launch with the default so I'll take that out of here um and we'll have my ec2 instance one and then we'll have my E2 instance two one and two the only thing I want to do is make sure we know exactly where they're launching so I'm going to go ahead and type an ec2 instance um cloud formation because we need to really specify the availability Zone and so in here we have an Avail availability Zone option I go back over to here and we'll paste that in and we'll just say ca Central 1 a for this one and for this one so hopefully that will be sufficient uh we do want to create a new volume but one thing we need to do is we need to change it so that it could use multi attach so here we have multi attach and we'll go switch this over to multi attach enabled and I believe there are some other requirements because you can't just multi any kind of volume it has to be a particular kind so let me go take a look here so I think this is the instructions here for multi attach I'm going to go grab this and put it in our read me here I think we have to use a um an iops one here so we go ahead and go over here and let's take a look at the requirements so io2 volumes can have multi attach so if you're not comfortable launching up an io2 then don't do this you can just watch me do it but that's what we're going to need to do for this to work so I'm going to go over here and take a look at our config ation options and I'm looking for io2 so this will be iops and we'll go here we say io2 I feel like there's some other things we'll have to configure here let's go back to this for a moment let's close out some tabs I got too much going on here I can't see what what I'm doing close this one out close this one out and let me just read this quickly it looks like i1 can do it but like it's talking about after creation okay but again I'm just going to keep reading here for specific requirements all right the other thing is that this is only available in particular regions so we're actually going to have to change this to be uh Us East one it's not going to work in other places um I'm going to go here and change this to be us East 1A and that one to be us East 1A let me read a bit more okay uh the other thing is I just don't know what is the default or the minimum value for um the the volume for an io2 so I'm going to go over to ec2 I'm just using this as a way to take a quick look if I go over to volumes and I create a volume um I want to switch this over to io2 and the low is four so that's what I'm going to do is change this to four just change that to four okay and I think that's okay I think that's what it's supposed to be but it keeps uh tripping me up here making me think that this is incorrect um so the parameters required 42 the default gpt3 is 300 the parameter is not supported for uh these okay oh but isn't it specifying how much the iops and not necessarily the type of driv is so I think that one's wrong and I'm actually looking for the volume type that's what I want to change I thought that looked a little bit funny so I'm going to switch this out to volume type instead and hopefully this will work um but yeah the idea is that we have a volume here and we actually want to attach it to two volume so we have my2 instance one and then we have my uh uh two so we'll just say one and two okay so in theory this should work so I'm going to go ahead and deploy this I'm going to go over to console make it over to cloud formation and we'll go to change sets and it looks like it's going to do all this stuff we'll go ahead and execute and we'll see if this succeeds or we have a failure here okay all right so it looks like it worked first time that is great I love it when we don't have to uh debug things like a maniac we're going to go over to ec2 and what I want to do is I want to see if the volume is there so uh we should have two instances running I see one so right away that is not a good indicator oh you know what I think one is in C sensor one and one is in Us East one is it though how would that be you know what hold on here I think this is an old one so I'm going to go over to cloud formation one second here cuz this should not be launching in censor 1 this should be in Us East one right because we said that we need to run it here okay so what I'm going to do I thought I was smart I thought I had it all and apparently it's not deployed in the right place at all okay so we're going to go back here because I do not trust this and you know what it is I'm in the wrong folder if you're not in the right folder you can't get things working properly so we'll go ahead and deploy this and I bet it won't work there you go it doesn't work undefined resource my ec2 instance okay uh so we'll go back over to here and we have our new volume I'm just going to hardcode this we'll just say Us East 1A okay we'll go ahead and deploy this and I'm going to go over to us East One have no idea what all this junk is we got old junk here and I'm going to go into here and then hopefully this works first time I'm so full of myself I thought I got it first time okay see you in a bit okay and it failed it failed we'll go to events here we'll take a look here uh unsupported type I24 volume so there's something wrong with what I typed in here io2 thought that's what it was I guess we'll go to take a look here io2 oh you know what I think I wrote a zero yeah see the slash through it there we go that'll fix it we'll go delete this all right so we'll just delete that and we will make our second attempt here and hopefully this time it works with little issue we'll go ahead and execute that chain set and I'll see you back here in a bit okay so we got roll back in progress let's take a look here and see what's happening um the parameter IIs must be specified for io2 so we can't just deploy this without specifying the ion so there you go um so the question is what is the lowest value we can set for this and it looks like 100 okay I'm just going to copy this again just in case it doesn't like the way I typed it so we'll go ahead here type 100 because I don't need a lot of iops and we will go and tear this down learning from our mistakes we will go ahead and try this again we'll give this a refresh we will execute this and we will hang tight all right now what does it want it's just never uh never ending here we'll go take a look here again the image ID does not exist um oh you know what it is because we switched from CA Central to us East one that totally makes sense we'll go ahead and tear this down we'll go over to ec2 this stuff never just works the first time does it and we will go and pretend we're launching an instance so we can get that Ami ID the other thing it's going to complain about and I already know this is the VPC uh so I'll go back over to here and into our multi attach template and I'm going to replace the Ami This Is Us East one and now I got to replace the VPC okay we'll grab this as well and I need a subnet and this one in particular I want us East 1A so we go to subnets here and we got a billion subnets I just want us East 1A Us East one C no Us East one a yes is this public or private public because I see ipv4 so I'm going to grab this subnet and we will go back over to here paste that in here and so hopefully this time we got it so let's go back over to cloud formation if I have yet to tear down the old one uh it is still deleting I'm going to wait for this to completely delete before I launch the next one okay all right so so uh that has now been uh teared down so I'm going to go back over to here we're going to go ahead and deploy this again and then hopefully this time we have not made any more mistakes okay be back here in just a moment all right we finally have some success here let's take a look at the resources and it looks like it is provisioning both of our ec2 instances we'll make it our way over to ec2 I'm going to log into one we're going to write something to the disc or the um uh the storage and then we'll go over to the other one okay um so these are initializing so we'll have to wait a little bit here um so actually I guess we don't really have to wait but uh there's one and there's two I wish we name these um so we'll have to keep track of this this one's 107 224 so we'll connect to this one first okay and we'll say pseudo SU ec2 user we have the lsblk to see our volumes and um so we have our our main one here and then we have this one here notice this one is not mounted to anything so we're going to have to mount it uh so I'm going to make my way back over to here as we do have mounting instructions uh somewhere here I think it's on this one okay so go over here and I'll just go to our multi attach and what we're going to want to do here is just run couple lines I just want these two okay so I'm going to copy this one and paste it in and I'm not sure if it's the exact same name so I'm going to double check and make sure that's the case we go to storage it is okay good and so we'll go ahead and copy this and we'll paste this in enter um oh it has to be formatted for the first time so that is something that we will have to do so I'll go back over to our basic and we have this line here so we go back over to this one and this will be format and mount and then just format or sorry just Mount because in the second one we don't need to format it twice right otherwise it's going to wipe the data out and I'm just going to adjust this here there we go and we'll bring this down below here so go ahead and just copy this paste it in hit enter so that should now be formatted we should now be able to mount it we can I'm going to do an LS I'm going to say touch hello.txt LS so we have a file there I'm going to exit out of this we're going to go over to our other one we're going to connect going to type in pseudo suyen ec2 user we're going to type in LS PWD we'll say LS this no that it's not there whatsoever we're going to go ahead and create this new volume and we'll go and see the into it did I even see the end of that directory when I made that volume now I'm I'm not confident about that now no we didn't so I'll do that now say hello.txt we're kind of doing this backwards I'm gonna go type in pseudo here LS typ in LA and um I'm going to I'm going to actually have to get out of this I'm going to have to go back into the original one because I got it back words I apologize we'll go ahead and connect to the E21 or whatever it is the other one and now we will CD into the new volume and there it is okay so there is our multi attach that worked out great we're going to go over to cloud formation and just wrap this thing up by uh deleting this stack there we go I'll just Commit This for later multi attach we have to CD into that directory so we'll just say new volume and touch hello.txt for people in the future there you go and we will see you in the next one okay ciao hey this is Andrew Brown and we are taking a look at Amazon data life cycle manager so this is a feature of EBS that allows you to automate the creation retention deletion of EBS snapshots and EBS back Amis so the policies uh you utilize here are are here to Define backup creation retention requirements and you have two options here we have the default policy and we have the custom policy so there's not a whole lot to talk about here because it's it's pretty straightforward uh in terms of uh what it does but uh just carefully looking at this here when we have policies it what you want need to remember is that we have EBS snapshots EBS back Ami policies and there's the cross account copy event policy so I think the custom policies are more interest in the default ones but anyway pretty straightforward so there you go let's take a look here at storage Gateway which is a collection of services so it storage Gateway connects on premise software apps with cloudbased storage the first we have is file Gateway this allows you to run a Gateway within your on premise environment so you can interact through SMB or an NFS file system protocol underneath we have file Gateway way to Amazon S3 so that's where you are storing data in S3 and we have file gateway to F FSX for Windows file server so storing data in a Windows file server then we have volume Gateway this allows you to mount S3 as a local Drive using the isaz Z protocol we have cache volumes so we have primary data stored on S3 and frequently access files stored locally then we have stored volumes or noncached volumes primary data stored locally and entire data back up to S3 we got tape storage that stores files on virtual library tapes also known as vtls for backing up your files uh on a very costeffective longterm storage I always get these offerings mixed up no matter how many years it's been just always forget how they work so anyway think of file Gateway as a shared folder and think of volume Gateway as a remote drive and that will make it easier to distinguish what these services do and obviously understand difference between cach volumes and stored volumes as those two can get easily mixed up okay Amazon S3 file Gateway allows your files to be stored as objects inside your S3 buckets access your files through the network file system NFS or SMB Mount point so whichever one you want to utilize obviously if one's the windows one is not so it kind of makes sense there Amazon S3 provides a costeffective alternative to on Prem storage when utilizing Amazon S3 file Gateway so in that sense here is our diagram we uh have the ability to deploy our gateway to an on premise virtual machine that runs one of the following either VMware esxi Microsoft hyperv or the Linux kernel based virtual machine you're going to see this repeated multiple times in this section about uh gateways being deployed on premise for these offerings uh the files stored in S3 basically act as full s file so anything you could do with a normal S3 file you can do that with these ones here using Amazon S3 file Gateway whether that's bucket policies versioning life cycle management cross region replication or metadata uh for the protocols we have NFS version 3 or 4.1 or SMB 2 or three the Gateway can also be deployed to VMware Cloud on OS the reason I have a Aster there is just because uh VMware was acquired by Broadband or broadcom I forget the name of the company but the point is is that they've been kind of uh uh cutting up the VMware offering and so the question is is VMware Cloud still around I don't know I'm not a big VMware head but I'm just want to point out that itus documentation says VMware Cloud whether that is still an operational service for VMware is another story you can deploy the gateway to an a Ami uh V an Ami in the Amazon ec2 instance this is a great way if you need to learn how to use this service cuz if you're not deploying on premise you deploy with an E E2 instance to learn how to utilize it uh the idea here is that S3 file Gateway integrates with IM KMS Cloud watch cloud trail ads CLI so there you go hey this is Andrew Brown in this video we're going to set up a storage Gateway using file Gateway so I'm going to go ahead and make a new gateway called my file Gateway and uh what we're going to bridge is actually to an ec2 instance because all these other ones are pretty complex but uh this one will be a great way for us to learn how to use file Gateway we want to launch this into a VPC into a subnet I'm going to just be very explicit and do 1B so that my life is a little bit easier we're going to need a key pair so go over to ec2 uh Management console and we'll say my Gateway key pair um I want a pem I prefer pem because I always use op SSH and not putty in uh particular and so that file is now downloaded there we'll go back over to here give this a refresh and we'll choose our key pair we'll go next um you must launch the instance before moving to the next step fair enough so we're going to launch that instance and we will wait here okay all right so that is now created we'll go ahead and hit next um connection options so we have IP address or our activation key since we have an E2 instance it should be pretty easy for us to use IP address here um service endpoint we'll say it's publicly accessible and we could enable fips but I'm just going to go ahead and hit next next and for some reason it is not it's not proceeding forward for some reason there we go um I had to click a few times I have no idea why but uh here we have our options we'll go ahead and activate our Gateway okay we do have some cloudwatch options here uh I'll leave that alone and that is creating here so we'll create a new log group we'll make the recommended recommended uh ones here we'll go ahead and configure there we go and now I'm just waiting for everything to get running here okay I mean it says it's running but again there's no alarm State stuff but I could have swore it was still doing something here so let's just go back here for a moment oh maybe it is ready to go but just give me a second okay all right so I just did a bit of reading and last time I used storage Gateway it the whole process was a little bit different um but if we go over to our instance I just want to show you here that notice that it has an additional drive that's attached and this one is acting as our cache volume so we didn't tell it how large it would be it just made it 150 gbes um and the idea is that this Gateway is supposed to act as um something we'll connect to with NFS so um let's go back over to sorry here to the security groups CU another thing is that we're going to have to have um particular ports open and it looks like it might have already done it I think 2049 is for um NFS so look you'll notice it opened up for NFS and SMB so that is in good shape the other thing we're going to need is a file share so we'll go ahead and create one now and so we'll choose our Gateway we'll choose NFS and then we need a bucket so we'll create a new bucket here this will be our yeah we'll let it just randomly generate there we go this service is so much easier than to use than last time and access for NFS client no access restrictions there we'll go ahead and create this file share okay and so now we have our file share um so I'm just carefully looking at this here and so now we have our mountable examples down below um so you know my thought process is that this IP address is our instance so we over to here I just want to confirm if that is true 172 3161 161 and it is and so the idea is that maybe we can use NFS right to mount to this and then be able to work with files there so that's what I'm thinking that we can do here um so just give me a second here what we'll do is launch up another ec2 instance okay we'll call this one you know our work machine so my work machine and we'll go down below we'll choose Amazon link 2023 I'm going to choose a T3 micro because that's what I prefer to use these days I'm going to go all the way down to the ground I'm going to go to Advanced details I'm going to choose SSM Ru which we created in another video in our ec2 section if you haven't done that go do that it's very straightforward or just look at what the basic requirements is to do sessions manager should even cover in their Cloud partitioner so there's no way people don't know this by now we'll go over here uh and we'll wait for that instance to launch and then we'll connect to it and then we'll see if we can establish an NFS connection I believe that NFS functionality is part of Linux so I don't think it'd be hard for us to hopefully establish that connection okay all right so I think this might be running let's go take a look here our status tcks are passed let's go ahead and establish our connection and we'll go back over to here and what I want to do is grab this line here or copy it and we'll do pseudo Su hyphen ec2 user I mean I think we'll have to create a mount directory first right yeah the mount path so what I'm going to do is where am I PWD I'm just going to call this one um sgw storage Gateway I think that's the initialism we'll go ahead and paste this in here and so notice this asking for the mount path I'm going to just put in here home sorry home for ec2 user sgw and see if that works um and so it failed just give me a moment okay well you know what what if we try pseudo there we go okay great so now the question is did this work and the only way we're going to know is if we go into this directory do lsph la you'll notice that that it has special highlighting there so it must indicate something's going on here I'm going to just say touch hello.txt okay great and so that has now occurred so the question is is it working and the only way we're going to know is if we go over to S3 so we'll go over to S3 here and we'll go into storage Gateway and I don't see anything so now the question is what happens if I was to drop a file in here would we see it from the other side so just give me a moment to get a file any kind of file um here's a file it's like a dog dunking a basketball or something I don't know and we'll drag that into here okay and then we'll go back over to here and we'll type in LS and so I don't don't see anything so just give me a second okay we'll see what's next the one thing I am noticing is that the status isn't updating so I'm wondering if this was actually even completed so now it says it's available okay let me go back over here LS no I don't see anything in here we'll go back over to S3 so hello is showing up okay that's good LS and we'll go back into here great so the hello is there I can say touch goodbye txt I'll refresh this we'll just give it a moment okay all right so I just waited a little while and then it showed up here so that makes sense here okay but just notice that the direction is from our storage here and being backed up to S3 so just understand the direction that it's going in apparently we can check our cloudwatch logs to have an idea of um when things are uploading but you know just understand there is a bit of delay and I think that kind of showcases the use case here for um for file file Gateway so I think we're all done here so I'm going to go ahead and just tear this down so we'll say terminate my work machine here excellent and then we'll go over to storage Gateway and I think we'll have to tear down the file share first okay I don't use this service very often I think I've set it up only uh once for a customer and even now it's just like it looks like gobbly goop to me but um you know we did the most basic thing we had to do to get this to work we'll stop the Gateway please power off the virtual machine okay can we just terminate it terminate terminate did it have protection on it I'm not sure it did just check the details here disabled okay so that can terminate and we'll go back over to here um we'll see if I can terminate this now or delete this okay just education here um so now that Gateway is destroyed that FAL share is destroyed um we can close this out we'll make our way over to S3 and we'll just get rid of uh those files we created in that in that bucket because I have enough buckets as it is um we'll go back here and we'll just say empty and we'll just empty that and I'll go back to bucket here and we'll delete this and we'll do this there we we go um but I think everything's gone so yeah we are in good shape here I don't think there's anything else that might be missing but yeah that is um file Gateway okay let's talk about Amazon FSX file Gateway I was a bit confused but then I had to learn about the whole FSS FSX offering so now it makes a little sense to me and we have a section just on FFX so that makes it more clear but Amazon FFX file Gateway allows your files to be stored on Amazon FF FSX Windows File storage so wfs allows your windows developers to easily store data in the cloud using the tools they already know we don't have any fantasy diagrams for this one because this one's really straightforward you must have at least one FF FSX uh Windows file server file system you must also have an on premise access to ffs for f Windows File system either through a VPN or it it's direct connect connection you deploy your gateway to an on premise virtual machine that runs one of the following hypervisors essx hyperv KVM or Hardware appliance that you order from your preferred reseller the Gateway can also be deployed to VMware Cloud as I said VMware cloud is questionable in terms of its longterm support or Ami and Amazon ec2 if you're learning how to utilize the service and you aren't really deploying on Prem you can do that there you go volume Gateway presents your apps with dis volumes using the internet small computer systems interface also known as I scuzi block protocol here is our diagram so you have an idea you have your local storage it's using ice scy to your volume Gateway and then I can send it over to S3 okay data written to volumes can be asynchronously backed up as a point in time snapshot of the volumes and stored in the cloud as an adus EBS SN shot snapshots are incremental backups that capture only change blocks in the volume all snapshots stor all snapshot storage is also compressed to help minimize your storage charges we have stored volumes and the other kind uh uncashed volumes whatever you want to call them but let's take a look at the difference between them well cach and uncashed I believe stored volumes is the uncashed version here so stored volumes store primary data locally and asynchronously back up the data to adabs so it's backing the data up to AWS provide your on premise applications with low latency access to their entire data set while providing durable offsite backups create storage volumes and mount theme as a icei device from your on premise servers any data ridden to uh ridden to stored volumes are stored on your on premise storage Hardware EBS snapshots are backed up to S3 uh stored volumes can be be between 1 gab and 16 terabytes so there you go on that for hosting could be deployed to a VM Appliance or a hardware Appliance or to an ec2 via Ami if you want to learn how to use this thing that's a good option now we have cach volumes cach volumes store primary data in Amazon S3 while retaining frequently accessed data locally in your storage Gateway so primary data is now on ads it's caching files on premise you got to know the difference between these two it's very important minimize the need to scale your on premise storage infrastructure while still providing your apps with low l data access create storage volumes up to 32 terab in size and attach them as an i scy device from your on premise server your gateway stores data that you write to these volumes in S3 and retains recently read read data in your on premise storage Gateway cach and upload buffer storage cache volumes can be between 1 gab and 32 gabt and size for hosting you have your VM appliances so hypervisor hyperv kmv you'll notice in the slides I'll kind of vary this I'll call them VM Appliance or hypervisors I'm just trying to give you more um showing the flexibility of the language here we have deploy as a hardware Appliance deploy as an ec2 instance so there you go okay tap G is a durable cost effective solution to Archive your data in itus Cloud the vtl interface it provides lets you leverage existing tape based application infrastructure here is a diagram and you can see the vtl is being stored in S3 Glacier the stored data is on Virtual tape cartridges so that's an image of actual virtual tape cartridges that you create on your tape Gateway tape storage has proven readability for 30 years so they're very durable or or uh yeah they're very durable for a very long period of Time Each tape Gateway is preconfigured with immediate changer and tape drives which are available to your existing client backup applications as an ice the device you add the tape cartridge as you need to Archive your data it's supported by uh netback backup exact and VM VM is a very popular um solution that a lot of people use uh you can deploy to a VM Appliance at this point you should kind of be used to what we're seeing here hypervisor uh yeah e es esxi hypervisor hyperv KVM you can deploy it to a hardware Appliance you can deploy it to an ec2 instance let's talk a little bit more about tape Gateway and specifically these vtl media Changers so a media changer is an is an I don't know why I chose that word Ana anulus is similar to a robot that moves tapes around in a physical tape Library storage slots and tape drives I I I chose the words I'm D lexic I don't know why I did that uh tape Gateway has two possible media Changers we have the ABS Gateway vtl and the STK l700 which is emulated this is what the actual l700 tape Library looks like so it's a big box and it it looks like a big juke box to me I'm sure it has moving parts and does all the stuff there different backup applications can use both or one specific media changer So based on what you're doing is based on what the media changer you use I think both of them are virtual anyway so but the point is is that on the right hand side if you're using v v might only work with both or some or whatever depending on what you're using okay before we talk about elastic cache let's talk about caching an inmemory data store so caching is the process of storing data in a cache so a cach is a temporary storage area and the purpose of caches are to optimize for fast retrieval with the tradeoff that the data is not durable so an inmemory data store is where dat data is stored in memory so think of RAM and the uh tradeoffs of storing data in something like Ram is that it has high volatility meaning that it's low durability there's a a risk of data loss but access to the data is very very fast so often caches are stored in memory they don't have to be but if you want them to be fast that's where they're normally going to be uh so elasticache is a fully managed inmemory data store for either the open source data stores mcash or reddis and so elasticache is intended to cache data or HTML fragments specifically mem cach is is is known to do the latter to greatly improve response times uh in the tens of hundreds of milliseconds so when I say that I mean like down to that time not that it'll shave off 10 to 100 milliseconds otherwise that wouldn't be very useful uh so there is kind of an architectural diagram of elastic cache but the key emphasis there is that um it elastic cache is deployed within a VPC and so your elasticache um cluster or data is only accessible when you have resources deployed the same BBC iTab us tells us the reason why for this limitation is so that ensures low latency because if it goes across um regions uh or or other places it's not going to be as fast could you uh use vbc lattice or um peering to bring that cash somewhere else probably but the point is is that same vbc for the same resources elastic Cache can be deployed in multiple availability zones for high availability alasa cach can uh be deployed on premise via of its Outpost alasa cach can use rbac for Reddit 6.0 so you can manage user access via the itus Management console and I actually didn't notice this until I came back and looked at the console but uh itus has another service called um memory DB and it's big sell is that it has a UI to manage rbac but it's interesting to see elastic cash got this so when they got serverless elastic cach serverless seems like it's competing with uh their own other service elasticache can be replicated across region via elasticache global data store so that is just a feature um additional feature to bring your data into other regions you can reserve nodes to save money when using elastic cach standard uh elastic cach can be automated to perform backups of data stores so the last three are all or actually last four things are all new um alas Cas was always a very simple service but uh now they have backups they have global data stores and uh they have Reserve nodes and they have the serverless mode so you have two different deploy options you have the standard mode which they just call elastic cache uh why why don't they call these capacity modes I don't know why is the serverless mode not called on Dem band I don't know but there's other services that are similar but for whatever reason they call this one elastic cach serverless is it serverless um I guess so because it scales to zero but uh I'd rather to just call it like fully managed or something as a lot of things are not servess anymore but um we got servus and standard the use case for serverless unpredictable workloads standard predictable workloads management automatically scales for serverless for standard you're managing the cluster in nodes for billing it's going to be based on data stored and EP uh ecpu units for serverless I have no idea how those calculations come into play but apparently that's a thing for the serverless one and for standard it's based on the number of nodes and then the type of node just like picking ec2 instance you pick a size and it's very similar there there the standard one can be deployed to on premise via a Outpost so that is another thing that is worth pointing out uh there are two caching options we have memcache this is generally preferred for caching HTML fragments it's a very simple key Value Store the tradeoff of being very simple is that it's very fast then you have reddits which can perform many different kinds of operations on your data um it's very good when you have structured data as when we talk about uh reddish you'll see that they have a lot of data structure mcash is just strings Keen value um and so the the fight here is that which one's faster and generally uh it could be argued that mcash is faster because reddis has less or it has more going on in it but uh I don't know it's very competitive and I think that maybe redis is now the fastest um but anyway the point is is that we have these two options and depending on what web framework you use some are plug andplay some are easier to use so it's really up to you um in terms of feature set you can see there's a bit of difference so I'll just get my pen out here to highlight the very key difference here but you can see uh that mcash does not use Advanced data structures does not have snapshots replications transactions Pub sub L scripting geospatial support um another thing I noticed which I didn't make a slide about this but uh when you have reddis you can uh for very specific versions I'm not sure which but you can have uh encryption in transit and encryption at rest but I notic for mcash it didn't do that so specifically for elasticache so I'm not sure if it's just because they're writing the older version of mcash or those capabilities are not there but red is appears to have better security at least on AWS so there you go all right so mcash is an open source distributed memory object caching system it's a caching layer for web apps mcash is a key value store which supports strings it can increment and de decrement strings if they represent an unsigned 64bit integer that is the level of complexity we have here with memcache it talks about objects but objects are just string values so they're not really any kind of complex data structure as it's implying you can use various sdks here's an example of using Ruby uh to use mcash and the way you'd interact with it is telet if you want to directly interact with a server there is no C tool at least I didn't see one so generally these are your two options let's take a look at the commands and these are all written in Python so just understand the last example Ruby these are in Python so here we are showing us setting a value and you can set an expiration time uh for it or we'll use whatever the default is you can get a value you can delete a value you can increment uh a value up and down if it's a number um or an integer in particular uh you can only add if the key doesn't key value does not already exist and I say key actually should say just the key the value would be what whatever you pass along here you can replace an existing key um you can flush all data which means basically just delete everything you can supply a number if you want to delay that you can append or prepend uh string data uh to an existing value you can get stats and I'm not showing it here but there's actually a a bunch of sub commands for stats uh about the cluster so yeah that's mcash there you go hey this is Andre in this video we're going to take a look at elasticache elasticache is a um a service that can have a managed version of uh of either redis or mcache and one of the caveats is that you have to have it in uh the same region in order to utilize it so that is something that is uh very important to know um but let's go ahead and uh do this now normally I would do things programmatically I think for this yeah we could still do it programmatically I think we'll just go ahead and use the CLI for this as I don't want to do cloudformation templates for elasticache there is another service that Abus has called memory DB if you just want to have reddis um so you could use either or just depending on what your use case is um but um you know if it is reddis memory DB supposedly is easier to use I guess we'll find out if we make a a follow along here let's go take a look at where elastic cache is so um we'll just keep type keep typing elasticache and when I sat the server what was surprising is that they didn't have uh they didn't have a simplified highly available distribute caching instance with no servers announcing elasticache server lless what caching which enables okay but then what's the difference between memory DB all right sure I mean hey I I like that idea that sounds good to me um but does it scale to zero does that El of cach redit for service enables you to create highly available cash and under admitted eliminates the need to provision instance or configure nodes or clusters I mean that sounds a lot better than configuring clusters so maybe we'll go ahead and give that a go um as that sounds like something I would really prefer uh to do so but I guess how would you set that up I'm not 100% sure I'm going to go ahead and just take a look here and so yeah they have a serverless cache where you can design your own cach where you have a cluster um so we'll go ahead and make a new folder here we'll just say elasticache and we'll just call this basic and I'll just make a read me in here because again I don't want to make a cloud formation template I just want to get this stuff set up so we'll type in C CLI version 2 and we'll make our way over to the version reference and in here I'm going to go over to AWS and what I'm looking for is cash so we have elastic cache down here and the question is is there like something for serverless here oh there is right here create serverless cach let's go ahead and try that out and do we have any examples no it us is too lazy to give us examples so we'll have to figure this out ourselves um we can probably go ahead and just use the uh you know our tool here so we have itus whoops create serverless cache so say itus elastic cach elasticache create serverless cache okay um and so we need the serverless cache name my cash AB couple letters here so we'll go ahead and grab this index and we need to specify the engine okay what are what are our engine's options does not say so we'll go back over to here and we'll look at engine options the name of the cach engine to be used for creating the cash serverless cach okay what what are my options so clearly must be so new that it doesn't even tell us here and uh somewhere in here must tell us how to specify the engine so let's go look up elastic cach server lists because there might be a blog post anytime ad of us releases something new they'll have a blog post yeah so 2023 not that long ago from this time date and they're using everything via the CLI see I would have thought engine would have been like engine version here so well that that's major engine version so I'm going to take a guess here they're not really saying we're going to say red it's cuz it's either mcash or redus and then we'll say major engine version so major engine version and I'm going to match the version they have in this blog post so this is 1621 hold on is that reddis oh that's a mcash that's a mcash one uh so how would we know what versions We have here so maybe if we go here we could just take a look so Serv lless new cash okay but how do I know how do I know what version It's using it's not telling me anywhere here would I have to go to the next step engine version s okay so is mcash not allowed here anymore hold on here design your own cach because it used to be like reddis or M crash and so I'm I'm noticing here it says options to create a reddish cach oh maybe because when we went through the UI okay so I really don't like these UI changes they really serve no purpose but anyway we're using Engine 7 I believe version is seven so we'll go ahead and do that uh so seven here yeah sometimes how it is when things are not fully documented is there anything else that we need to specify we'll give it a space here cast us usage limit set the cash uses limits for for storage um I don't care about that so I think we have everything uh a list of identifiers subnets where the VPC endpoint for the serverless cache is I wonder if we don't set that if we have any way of accessing this because these things are usually a pain to access um so I'm looking for what was it they said Nets list of identifiers of the subnets where the VPC end point for the server cache will be deployed all subnet ideas must belong to the same VPC so it doesn't say this is required but we'll go ahead and just hit enter and see what happens and it did create so that's pretty cool and it looks like it's choosing the default subnets but I mean that's my guess we could probably check and just double uh just see here assuming this is in uh CA Central 1 which is where I'm expecting it to deploy but we'll go over to our VPC and yeah so this is one of the subnets there so that's good um and so we'll go back over to our redis cache here never used the servess version before but I'm really interested and so it looks like it has to get running up and provisioning so that's fine so we'll just give it a moment to get going here okay all right so this appears to be available and we have an endpoint uh to access um this here so generally when ever I've tried to utilize um elasticache you've always had to access it via the same vbc so this this point here is not going to be accessible but we can try and test it here to see what happens question is do we have redis install no so we'll just go ahead and say Brew reddis reddis client I think is how we install that so we'll go ahead and try that um no not RP rpki so install reddis Cent Ubuntu or Debian yeah could be this as well so we'll go get go ahead and try this out stall redest client YouTu okay and so just say yes here so probably would do a hyphen Y and I just want to see that it doesn't work that's that's what's important here so we'll go ahead and we will grab this end point and so I think it's just like reddis redis client it says we installed it so we go here oh yeah red CLI I guess that's what I'm thinking of so we'll just type in redis CLI probably didn't want the tools install and we'll go over here we'll go to Linux I guess here it just says redis it's not even tools so we go ahead and copy this I guess and we'll see if this works so we'll grab the first one a lot of work just to get this installed but we'll we'll we'll do what it asks so we'll paste this one in and then we'll grab the next one says the uh gpg failed I'm just going to try to install it just going to skip all those steps maybe we'll just install it there we go it installed it okay do we get red of CLI now this thing is bugging out as per usual it does look like it it tried to make it a connection so that is good um so I think that probably the way it works is we didn't need all this junk here we'll take that take that out there but um as far as I remember the way we connect to redis is we just provide the link so we just say like redis C and so that should establish a connection connect to instance um and so I expect this to fail because like why would it you know what I mean why would it so yeah connec refuse which makes sense so the the real question will be like how can we um test this ourself and so I think what I'll do is I'll go over to chat gbt and um I'll ask it to generate out generate out a uh Docker compose file to run red is 7 okay and so this will be our our means for testing it so once we have a Docker compos file we can launch up red our great I'm just hoping that this works I don't know version 3.8 is the latest for Docker but it should be fine 6379 is the default Port data that looks all good so we'll go ahead and copy this and I'll go back here and we'll just make a new file here we'll say Docker compose yaml and we'll paste paste this in here I'll save it we'll say Docker compose up and we have to CD into our elastic C Ser uh um directory here and so that's now pulling down the image and now it's running on Port three uh 6379 so the idea is that we should be able to connect to that so I'm going to go over to another one here we'll type in redis CLI uh 21701 uh what was it 6379 6379 we'll try this first okay that didn't work um anything special we have to specify for the endpoint doesn't seem like it try z00 Z no how about Local Host all right so let's go take a look at how this actually is supposed to work red CLI uh we'll just say help I'm not using reddis on a dayto day um but let's go take a look here so Redi options we have host name Port the default is 20 21701 and the the default Port is 6379 so it seems like it's already defaulted to do that and we could Prov provide the username and password obviously we don't configure any of that so that is uh pretty straightforward so what we're going to do is go ahead here and just type in Redi and that should enter us into red reddis there and so we are in there um I don't remember reddis off the top of my head so we'll go take a look at some commands um it should be very straightforward I've done it before so set key name hello so just say set hello world there we go we'll say get hello there we go great so redis is working locally I'm going to just type in exit here we'll type in clear and so the idea is that we still again want to connect to this one here and and all we really need here is the actual um host so we go ahead and change that out there again I'm not expecting this to work but I just want to test it okay great so it says that's not possible is it like host name try this again here oh hyphen H hyphen H okay and so we'll go ahead and copy this all the way down to the ground we'll paste it in we'll hit enter and so this shouldn't work because the only thing that should be able to connect to it is something that's in the same VPC as this so this is going to hang which is totally fine um since we're doing serverless I'm just going to rename this to serverless because I have a feeling we're going to do the cluster okay and I'm going to go ahead and just um sa this so far so basic setup basic uh elasic cache e see and so what we're going to need is an ec2 instance cuz we're want going to want to have to log into that so we already have some code laying around for this just say um template. yaml and we recently set one up in well definitely did want a knackles so let's go grab that one I think we can pretty much use everything here the only thing I don't want is uh the the user data script here so we don't need this okay um and these are not up to date I remember we had recently done this with the default the default VPC with elb so I'm just going to go in here and grab it from here just going to go here and grab some of these values just see making sure nothing's hardcoded here uh yeah so I just want to go ahead and grab this here this one seems to be the same I'll just grab a subnet one here oops we go ahead try that again okay so yeah so we'll be able to log in that instance and the thing is is that um when we launch up elastic cach normally we'll have a security group with it let's see if it actually does it's using this one is this just the default one this is the default Security Group so I think in this case we'll go go ahead and use the default Security Group and I'm just going to change this to be um let just say like Security Group Security Group ID and we'll go down here and just say type type string default we'll paste this in here and what I'll want to do is just reference that here okay and so if they're in the same Security Group then they should be able to talk to each other okay um so that looks good to me I think that is fine uh this is not actually referencing the uh group it's we want to place this here okay we'll go ahead and create a deploy script it probably would have been hard to make it a a cloudformation template but whatever deploy ec2 instance ec2 instance uh to log in uh to interact with redis um I might also want to install the reddis client so reddis client install a Amazon Linux 2023 as I it would be nice to know what we need to do to install that I'm hoping that we just do this it depends on um how old this is but I'm going to give it a go and see if that works if it doesn't that's totally fine but we'll we'll try here so we go here and going to go ahead and grab the other one here wherever it is um down below here I want to just get that user data the start of it and then we'll go back to our serverless one here just say paste okay and so we definitely know that we need to do pseudo yum update I don't think we need GCC but we're going to need uh reddis for sure and this is the old way of doing it so I'm pretty sure that is not correct yeah this looks correct to me and technically um we can use yum I think but dnf is what is the new one and for some reason I just don't seem to do that very often so we go ahead and copy uh this code here because that looks like the right stuff to me here here okay pseudo yum pseudo dnf update maybe did they do an update on that no they didn't but they just installed it but it should work fine was that one up y that one's up to date here so I think this is what we want our script to do um but anyway we'll go back over to here we'll just say deploy E2 instance interact with that so just be like ec2 for uh elastic R for redus so easy to for red is okay and so I'll just chamod that once we get into that directory here we's say elasticache server list chamod plus x uh deploy and we'll go ahead and deploy that we'll make our way over to confirmation and I'm going to go ahead into here and we're going to go to the change set take a look here we have our instance let's go ahead and deploy that and so we'll wait for that to deploy okay all right looks like we're having some trouble with our instance I mean the only thing we set was that user data the subnet just doesn't exist um so I guess I must have set that in correctly we'll go all the way back to the top here I mean I thought it would but I guess it didn't so we'll go ahead and just copy this one I suppose copy that one we'll go back over to here and we will paste this in excellent while we're here we might as well just check the VPC um since I don't trust it looks like it's wrong as well yeah so we'll go ahead and do that and you know I've said this multiple times I like t3s so I'm going to switch over to t3s uh we'll go ahead and tear down this other stack okay and we'll give this another attempt at deployment there we go and we'll just give it a moment okay well first obviously we have to accept the um change set all right and then I'll be back here in just a moment okay all right so our web server appears to be ready we'll go over to ec2 and go and use sessions manager to gain access whether installed uh Reddit we won't know until we go into that I'm just going to go ahead and just say redis reddis client server and we'll go ahead and connect we will'll say pseudo Su hyphen ec2 user and we'll type in clear and we'll type in reddis client and I'm just going to type in help to see if it shows up so that command is not showing up well sorry it should be CLI and uh it's not there so it makes me think that our installation script did not work as expected which is fine what we'll do is just run it line by line and see what the issue is so um the first would be to install well that says red is six and we're actually using red is 7 um so I'm going to go ahead and just take out the six on that and let's see what happens if we go ahead and do this we'll paste this in here oh okay I thought uh this is a 2023 instance I'm surprised it's not um working with that but we'll go ahead and Amazon 2 core is this not a is this not an Amazon 2023 instance let's go here and take a look look so what I'm looking for here is the instance type 23 micro instance details our Ami is right here oh this is Amazon Linux 2 well I don't want two I want I want 2023 let's go ahead and go here and I want 20 23 I'm going to go ahead and copy this Ami I'm not sure how we ended up with two I guess maybe we must have used it prior um so we'll go back over to our script here and I'm going to go update this value okay because dnf is not available on on Amazon El 2 surprisingly none of our other um tutorials mucked up but it really depends on what you're doing so we'll go over to um actually we can just update this in place because that's all we have to do really um so I'm going to go ahead and deploy this again and we'll go over to cloud formation and we'll go over to our change set and we will click into it it should replace it doesn't say that it is but it is going to replace it so we'll just wait for that to update okay all right so our instance apparently is updated we're going to give this a refresh here notice that it is initializing at least it kept the name so that is really nice I'm going to click into here I want to make sure that it is using uh the newer Ami and so we can do that by looking at the instance profile here we'll click through that and so now this is Amazon so this is Amazon 2 so did it actually pick this up is my question here this one says e5c and then the other one here is clearly not that so am I in the wrong file here no I'm not and this has definitely been updated so what I'm going to have to do here is just uh destroy it because it's clearly not picking it up and so I'm just going to delete this and we'll just wait a moment okay all right let's take a look here and um okay the stack is now deleted so we can go back over here and this time we should be in better shape come on that's fine that that's gone um no should be creating it there we go and we'll go ahead and execute that change set we'll be back here in just a moment when this is provisioned all right so let's go take a look at our new ec2 instance and so here it is good uh I'm still wondering if the last one actually might have worked because it makes me think that maybe we just didn't refresh uh the browser here I really don't trust our browser I'm going to give this a hard refresh here and we'll click into this and so this should be utilizing 2023 right so we go here Amon link 2 what is going on here am I crazy we went to instances right MH uh what's going on adabs it's now not showing me all the instances okay let's go back to instances is here launch an instance 2023 right here and then this is the code I used right read more 2023 kernel okay what we can do is check the uh template as well so this one is using that code here us this is not a problem okay uh and we'll go back back over to here so it is that one there Ami 015 6B c yeah okay I think that's the same right e5c e5c yeah that's our template we go over to ec2 um that's not ec2 we go back over to ec2 here refresh we're checkbox on that one right okay 2023 all right so not sure what the confusion was um but anyway the point is is that now it should have the instance that we want hopefully the Redi is installed so we can focus on what's really important in here we're going go ahead and type in pseudo Su ec2 user and um I'm looking for reddis CLI so we'll go ahead and do that does not look like it's installed so that is fine we will walk our way through this uh these commands and see what we need to do I'm just going to try the first one and see where it starts to fail okay that one's good we did take off the number six on there so maybe there's like a seven or something that we need to do here enter so no argument what if we put seven on that okay what if we want to install redis so there is no reddis that we can install on here okay um Amazon or we'll just say redis install version 7 okay so I'm looking for the install steps we got 7.2 okay where's my steps to do this install reddis CLI we'll go here we're on Linux this one in particular is for Debian um we need this installation instuctions for Centos so redis 7 C OS install Okay anybody that sols it for us is great is that going to really work because I'm pretty sure that um the eepl does not work with Amazon Linux 2023 because they've told us that so many times so we'll just say Amazon Linux 2023 install okay so this is a case where I don't want to fiddle with this all day so what I'm going to do is I'm actually going to change the instance I'm not going to launch an Amazon link 2023 I'm going to launch them with a buntu or Debian so that our lives are way easier and so there is a buntu right there so I'm going to go ahead and choose that and I'm going to go and grab this I'm just going to delete this stack here and we'll go back over to here and we will paste in um our image ID up here okay and we'll go back to the installation steps and so here is snap snap might be part of the new Ubuntu so I'm going to give it a go and hopefully it will install via snap if not we'll run those commands manually go back to our template here we'll just paste this in pseudo snap install snap is a alternate package manager and I think uh the latest auntu should have it so we'll go back over to here we'll give this a refresh we'll wait for that to tear down okay all right so that's teared down we'll go back and launch this Ubuntu instance and hopefully we'll have an easier time uh getting the Redi to work and hopefully it's easy to disc connect to our um elasticache Ser lless version so we'll just wait here for a second and we'll go ahead and accept the change set and we'll let that get going here okay all right so our instance uh should be running we'll make our way over to ec2 again for the billionth time and so here we have our running instance I'm going to go ahead and connect to it and this one is Ubuntu so it's probably not going to use the E2 user um so we'll probably type in Ubuntu ubun there we go and so I want to see if red CLI is there nope um so yeah here it's saying pseudo app get install R tools sure if it works redus tools so we're going to install that there okay um how about just redis pseudo snap install redis redus is already installed okay redus then I mean I want to use the reddis CI okay but it just told I can do this the Snap Store provides reddest packages that can be installed on the platform that support snap cool well that's not really helping me too much so I'm going to go ahead and say pseudo snap uninstall reddis I wonder if I can do that nope all right I'm going to just try to manually install this might muck things up but we'll see what happens here so we'll try out this First Command and then we'll try the next command and we'll try the next command the problem is that we've already installed it so I kind of feel like it's going to conflict um this is supposed to not have this echo part in it and I don't think it's supposed to have that on the end there nope they could have made this a little bit easier for me to copy paste but whatever we'll copy this again right click paste enter okay we'll do this and even though if RIS is installed what I'm looking for is that CLI tool right so that's what I'm hoping for here we'll give this a moment to uh D its thing there we go that was quick we'll say yes and it was showing us what it's installing here so no app report indicated following errors while processing red server redus I just want the CLI tool so I don't know if that actually worked but we'll go ahead and just type this in there we go okay great um now it seems like we're also running a red server which is not exactly what I wanted but that is totally okay so what I want to do is I want to remotely connect to our reddis instance in elasticas so we'll go over to alach and we'll go ahead and grab the endpoint we don't need to specify the port because it looks like it's using the default Port but uh just in case we might we might do that in a moment here but we'll go ahead and hit reddis um this is now not responding for some reason great so we'll just terminate that and we'll reconnect to our oh are we over here that's why I just had another tab open I guess and so we'll try this we'll hit enter and we'll be very explicit about our Port as well just in case this is an issue oh no it connected okay so it'll say set hello world little bit slow but as long as we're connected that's all that matters server closed the connection um this used to happen to be quite a bit and it's usually when I would configure a rail app to talk to elasticache and what it would normally have to do is with encryption um so that's probably our issue here so just give me a moment to see what we can do interesting enough here for Amazon 2023 it just tells you to install red6 which is whatever that's fine uh the only difference here is that they're putting hyphen hyen TLS on this so let's go ahead and try that um so I'll just type exit here and we'll go hit hyphen hyen TLS and now it instantly connected so that's great so we'll say hello world great so now we're writing to our remote cache um and so that's all there is to it um would the redis 6 work with red is 7 I'm not 100% sure um but obviously red is 6 is easy to easy to install on Amazon L 2023 but um I think that serves as our purpose here um there's like no way to really see the data here so it's not going to help us in any other way like to to explore the data but that's all I really wanted to show you so what we'll do is go ahead and delete this we don't want to back up we'll go ahead and delete our cache and we're going to go ahead and tear this instance down so we'll go back here and we'll just delete that and I'm just going to update this to have the proper install here um here y since Pudo snap was kind of useless for us go paste that in there we go but we can use a server for other things say elastic cach there we go and I'll see you in the next one okay ciao hey this is Andrew Brown from exam Pro and we are looking identity access management I am which manages access of Adis users and resources so now it's time to look at I am core components and we have these things called I am identities and those are going to be users groups and roles so let's go through those so a user is an end user who can log into the console or interact with Adis resources pragmatically then you have groups and that is when you take a bunch of users and you uh put them into a logical grouping so they have shared permissions that could be administrators developers Auditors whatever you want to call that then you have roles and roles uh have policies associated with them that's what holds the permissions and then you can take a role and assign it to uh users or groups and then uh down below you have policies and this is a Json document which defines uh the rules in which permissions are allowed um and so those are the core components but we'll get more in detail to all these things uh next so now that we know what the core components are let's talk about how we can mix and match them starting at the top here we have a bunch of users in a user group and if we want to on mass apply permissions all we have to do is create a role with the policies attached to that role and then once we attach that role to that group all these users have that same permission great for administrators uh um Auditors or developers and this is generally the way you want to use IM am when assigning um uh uh roles to users you can also assign a role directly to a user um and then there's also a way of assigning a policy which is called inline policy directly to a user okay so why would you do this well maybe you have exactly one action you want to attach to this user and you want to do it for a temporary uh amount of time you don't want have to create a manag role cuz it's never it's never going to be reused for anybody else there are use cases for that but generally you always want to stick with the top level here a role can have multiple policies attached to it okay and also a role can be attached to certain AWS resources all right now there are cases where resources actually have uh inline uh policies directly attached to them but there are cases where uh you have um roles attached to or somehow Associated to resources all right but generally this is the uh mix and match of it if you were taking the uh adus security certification then this stuff in detail really matters uh but for the associate and the pro level you just need to conceptually know what you can and cannot do all right so in I am you have different types of policies the first being managed policies these are ones that are created by AWS out of convenience for you for the most common uh uh permissions you may need so over here we' have Amazon ec2 full access you can tell that it's a managed policy because it says it's managed by ws and an even further indicator is this Orange Box okay then you have custo customer managed policies these are policies created by you the customer they are editable whereas in the manage policies they are readon um they're marked as customer manage you don't have that Orange Box uh and then last are inline policies so inline policies you don't manage them because uh they're like they're oneandone they're intended to be attached directly to a user or directly to a um a resource uh and they're and they're not managed so you can't apply them to more than one uh Identity or resource okay so those are your three types of policies hey this is angre brown in this video I want to make it really clear difference between a manage a customer and an inline policy so what we're going to do is make our way over to IM uh and I already have a user that I can work with so you can do this on your regular user whatever you want to do I have this junk user I keep trying to get rid of I thought I got rid of this one of the last video but I guess not and there it's gone great so no it's not what the heck give me a second oh no there it is okay great so I'm going to go into adups examples and you know you can make yourself a user if you're not comfortable doing doing this on your on your regular one but the idea is that you have policies and if you drop this down we can add a a policy here and add user to group no copy permissions no attach policy directly so we're going to go over here and now what we're able to do is attach a policy now notice over here in the center we have iTab us managed okay so if you drop this down you can see one's based on job function and I find this really use because uh these are really good um Bas level uh policies that I think that you can leverage um and you can see there's one for admin billing stuff like this so if you're not sure you can always give people like if you have a developer and they need most access but not everything you give them power user if you have someone coming into your account and they only need to have read only access you have that uh there's view only access which I'm not really familiar with but we can open it up and take a look here so it looks like they can view stuff not exactly sure what the difference between this one and that one is the point is that that is managed uh based on that but then we have it us managed over here so hopefully that is clear then you have your customer managed ones these are ones that I've created so if you have not created a policy you will not see any here um so that should be very clear so let's go over and create ourselves a new policy it doesn't really matter what it is I just want to show that we are creating our own customer managed policy and we'll just choose a service like ec2 we'll give it a moment and we'll just give it all access to ec2 we all the way down the bottom all the way down to the ground we'll just say next next oh this is annoying because it wants us to fill in all this stuff I just wanted to uh just give me access to everything what what's the problem here let's go over to Json sometimes that makes her life a little bit easier and so it's asking for what action so I'm going to type in ec2 sometimes like I don't really like the this wizard because I find that it just makes me makes a lot of extra work for me unless it's really simple stuff but I'm going to go ahead and type in ec2 uh colon asteris so just say let's allowed to have access to all ec2 instances from everywhere okay and so that should fix our issue here it says there's still an error on line seven uh this has to be in double quotations or single I suppose uh same thing with this one sorry there we go we'll go ahead and hit next say customer managed policy we'll create that policy just zoom out here and so we have that so if we go back to our user and we want to go ahead and add permissions and add permissions here we can go and attach that I'll just type in customer manage policy and then we could add that there uh there's a third option which is adding in a um which is adding in an inline policy so if we create this this just allows us to write Json straight straight ahead here so we can go ahead and type in ec2 Colon Aster and then down here we'll go ahead and do that we this the exact same thing we wrote before um it's interesting that's asking us to name it we'll just say my policy because it's supposed to be in line I'm not trying to create a policy I'm trying to add a policy in line it is in line okay I was I just wasn't 100% sure because we're naming it but you can see we have an abis managed one a customer managed one and a uh customer inline so the idea is this one's only applied to me this one can be shared with other users this one is managed by a so hopefully that is really clear I'm going to go ahead and just clean this up really quickly and I want to go here and just uh remove that one as well um actually what we should probably do is just do a little bit of um cloud formation because I think that is also a really great thing to uh learn as we're working with um policies so what I'm going to do is make my way over to theas examples and I'm going to so yeah I removed all that stuff so that's good but I'm going to go back to our policies here I'm going to look for that c customer one I want to go ahead and delete it okay so say customer managed policy good and so here what I want to do is I'm going to just hit period I'm not going to use um code spaces or or get pod today because I don't feel like generating access key for this but what we'll do is we'll go over here did I change something from last time why is it showing me uh that no reason just a number one but what we'll do is we'll make a a new folder here and we'll call it I am and I'm going to go ahead and just make a cloud information template and we'll just type here uh types of types of policies okay I'll bring that in and so what I want to accomplish with this is I want to write a cloud formation template that is going to have all three of those attached to that specific user in fact we could even user here if we want to so go ahead and type in a CFN user okay and we'll go here give it a moment we'll go down to um Properties or return values actually just looking for an example normally normally they show that at the bottom they don't it's right here and we'll go ahead and just copy this we'll just say parameter or sorry resources and just follow along the best you can this is a yam so it should be two space two spaces here going to go ahead and grab this link at the top and I always like to um put the link to this actual resource so I can hold control or yeah control and click through to it later on when I need documentation uh my user here of course we could be using um chat GPT or other tools to write this out but it's so simple that you should really just write it out here and half the time those things mess up a lot uh so you can see we can apply groups um but what I'm interested in is is uh just setting up the base user and giving it some permissions so we'll go ahead and provide a username so we'll go here and say username I I call this uh user uh my cool user and right away we can actually attach uh a managed policy AR to uh to this user so we can go here and it looks like a list of IRS for am policies so we can do this but like it's easier just to go to the next line and do um a hyphen to have a managed policy so let's go ahead and grab one um so I going to go here and just pick a job function one this a refresh here just clear that out I'm going to choose uh view only axis because that sounds really restrictive so I like that one and it just it said to paste in the orang so I'll just do that so that should add that manag one there the next would be to add a custom policy so what we can do go down here contains information about an attached policy so we go here and yeah so we can attach policies here okay policies and sometimes you have to click into these to figure what it is so we go here and it wants adjacent structure of this so we'll put a I think it's an array of Json objects so if we do that and then we say policy document okay and then we say policy name my cool policy and it's asking for Json does that mean that we can just provide the Json as a policy yeah so this looks kind of like an inline policy I'd be really interested in seeing what happens there but here's an example and I'd actually rather grab the yaml version cuz yaml turns into Json if uh you know much about yaml so we'll go ahead and just paste that in all right and uh we need that do we no we don't actually need it there there we go that makes more sense because otherwise thatd make this an this the the first array item the second one so we want this to be the same object um and this is listing users which is totally fine I think I'll stick with that uh so the question is is that that going to be in line let's go take a look attach a uh an attached policy is a managed policy that can be attached to a user so it makes me think that it's going to create an attached policy which is fine I kind of want to do an inline policy let's go back to the user there for a moment while we're there I'm going to open up this one for a moment and I'll click back here we'll click back I just want to see if there's any other options here so policies manage policies looks fine we'll go to I am policy here adds or updates an inline policy document that is embedded to a group user or Ro that sounds really good to me so go down below here I'm going to go to examples I'm looking for an inline one and I'm going to call this in my inline policy I'm going to go ahead and just grab this URL up here to save myself some trouble actually normally normally put this link right above the type that's where I actually like to put it and so we'll go down here and we have kind of an example of one this ISS user oh no no no no we want to be over here okay that makes sense and we'll go ahead and copy this okay we'll paste it in so let's see what we have we have the type as I am policy the policy name my inline policy this is allowing from everywhere which is a bit crazy we're just going to save from everywhere for uh ec2 and here it's attaching roles but we won't attach this directly to a user so if we go back up to the syntax there probably is something for this here users exactly and it wants the name of the user doesn't say R and says name of the user so we'll go here and just say uh users and we can probably reference this one so I click I hold control and click so I can get back to that reference and we'll go down to return values and this says when you pass the ref it Returns the username that's exactly what I want so I'm going to type in um exclamation RF and then we're going to put in my user so that should return it and so I'm hoping that we have a manage policy uh a custom policy again I don't know what this one is and an inline policy um and we can go a little bit further cuz I think we should and we should just uh make a new role and then attach the RO um to the policy and then attach the policy to the user so if we go back to users we could probably attach rules uh oh no we can't Sorry rules are for services right so we can't that makes sense but my my one thought was that it'd be nice for us to create a policy uh separately but would this be an inline policy that's what I really want to know um how do we know if it's inline or not so let's go back over to what what's this ads or updates an inline policy document that is embedded into a specific user I mean this sounds more like it'd be uh that than the other one let's go back to policy here for a second adds or updates an inline policy document uh okay so this one says it's inline as well um and then what does this one do uh let's go down and take a look here so we have policy document policy name username all right let's just copy this one okay we'll just say here my second inline policy grab this one here whoops uh say my second inline policy for this one I'm going to give it access to S3 all right uh and here we're specifying the username again say ref my username my user it looks pretty much the same as that one so so why that's what I'd like to know why do we have one that's so darn similar I can't really tell the difference all right well whatever we'll just go ahead and create it so we have our uh our document here I'm going to go ahead and copy it or sorry just download it so we'll say download and I'm going to uh find that in my downloads just give me a moment here and what I'm going to do is make my way over to Cloud informations so I'll type in CFN and you know for cloud information we do it all different way sometimes I do it uh uh This Way by directly uploading other other times it's just all over the place um I'm going to go ahead and create a new stack we're going to create the new resources I'm going to upload a template file I'm going to sorry about that I'm going to drag over this template file you can't see but I'm dragging this off screen for my my windows folder and we'll go ahead and hit next um it says there's an error line 22 would have been nice to know that ahead of time so we go down to line 22 it's because I'm missing the comment there in the front I'm going to delete the file out of my downloads I'm going to download this one here I'm going to go back over to cloud formation I'm going to try and drag this over again there we go we'll hit next uh my policies for user we'll go ahead next we'll go all the way down the bottom sure we'll hit next we'll go all the way to the bottom we'll say I acknowledge an IM resource so that it has IM capabilities we're going to wait for this to create and see if it's successful okay all right so we have some failure here we'll go over to our events we refresh this here it says validated failed for the following uh resources roll back requested by user properties validation failed for resource my user with name extrus key username is not permitted so sounds like we might have introduced a tiny mistake here not really sure where as yeah I'm not exactly sure what it's problem is here let's go back and take a look again extr key username is not permitted property validations failed for resource my user with message exerience key username is not permitted so it's suggesting that I provided the wrong parameter let's go back to I am user here and take another look maybe I did write it wrong username the name of the user to create do not include the path to this value the regx pattern a string of characters consisting of an upper lowercase characters with no spaces you can also include hyphens if you do not specify name it Cloud information will generate a unique physical ID well how about we just remove this out of the equation okay and let it autogenerate its own name and then maybe I'll have less of a problem but what the problem is I'm not 100% sure about I'm going to go ahead and delete that template we'll go back over to here we'll delete our stack and uh this should be pretty darn quick because it didn't do much and I've I have to download this template again make sure it's saved before you download it we'll download it again we'll go back over to cloud formation and yeah probably been faster if I just configured this so we could deploy it but uh with the c but whatever learn from your mistakes okay okay so save my cool user next all the way down the bottom next all the way down to the bottom I owledge submit I'll see you back here in just a moment okay all right so that has created let's go take a look at the resources and so we have an inline policy here a secondary policy which is uh type I am user yeah that makes sense uh and the IM am user itself let's click through to the user and actually take a look and observe what permissions are attached so we have customer inline customer inline customer inline manage so I wasn't able to uh other than this one I wasn't able to get a customer manage one attach there's probably some way to do it but it's not that important but I wanted us to get some practice with the CLA formation with I am policies and I I really just wanted to show you that you can do them in line like this which is really nice but I would say that we are uh done here and so what I'm going to do is go ahead and just delete this stack and I will see you in the next one ciao so it's now time to actually look at a policy um here and so we're just going to walk through all the sections so we can fully understand how these things are created and the first thing is the version and the version is the policy language version if this changes then that means all the rules here could change so this doesn't uh change uh very often you can see the last time was 2012 so it's going to be years until they change it if they did make changes it probably would be minor okay um then you have uh the statement and so the statement is just a container for the other policy elements so you can have a single one here so here I have um an array so we have multiples but if you didn't want to have multiples you just get rid of the um the square brackets there you could have a single uh policy element there now going into the actual policy element the first thing we have is Sid and this is optional it's just a way of labeling your statements so Sid probably stands for statement identifier um you know and again it's optional then you have the effect the effect can be either allow or deny and that's going to set uh the conditions or the uh the access for the rest of the policy um the next thing is we have the action so uh actions can be uh individualized right so here we have I am and we have an individual one or we can use Aster to select everything under S3 uh and these are the actual actions the policy will allow or deny and so you can see we have a deny policy and we're deny ding access um all to S3 for a very specific user here which gets us into the principle and the principle is kind of a conditional field as well um and what you can do is you can specify an account a user a role or Federated user to which you would like to allow or deny AIS so here we're really saying hey Barkley you're not allowed to use S3 okay then you have the resource that's the actual thing um that is we're allowing or denying access to so in this case it's a very specific S3 bucket um and the last thing is condition and so condition is going to vary based on uh the um based on the resource but here we have one and it does something but I'm just showing you that there is a condition in here so there you go that is the makeup of a policy if you can master master uh these things it's going to make your life a whole lot easier but you know just learn what you need to learn right hey this is Angie Brown in this video what we're going to do is get some practice with IM am um so what I want to do is I want to go over to our I am user uh I have one called it examples you can use whatever you like you can create a new user if if need be um but what I'm going to do is go ahead into my a examples and this one already has permission for admin so I'm going to go ahead and remove that and what I want to do is I want to um lock down this user so that it can't really do a whole lot but it what it can do is change its own permissions and that way we'll be able to experience what it's like um trying to access something not having access updating the the uh policy and making it more restrictive and I want you to get an idea of like what the uh autonomy autonomy anatomy anatomy of an IM policy is and so that's what I'm hoping that we uh get some good experience with here so now that we've removed all our permissions we're going to have to go ahead and add a add permissions and specifically we're going to create an inline policy I'm going to go and uh to Json because we really want to get more practice utilizing uh the policy editor the nice thing is it will kind of tell you some suggestions here but for the most part we'll be using G pod and writing our own policy but anyway what we need here is um we want to give it access to IM and so I'll just go ahead and say give me access to everything and I'm not sure if we're going to need access to STS but I'm going to go ahead and just do that as well we'll just say all actions for STS for everybody and so that should put us in good shape so let's go ahead and uh just double check this as it's complaining I think here's just saying like hey we don't recommend doing wild cards that's totally fine we'll call this one um uh I am policy playground okay and we'll create that policy so now that policy should be attached to that user I'm going to go over to those examples and I'm launching this up in G pod you can use whatever you like but understand the configuration is going to be a little bit different you'll have to establish your um uh credentials and create a connection to ads so let's give this a moment to launch up I actually recently was having issues with get pod um and so in some videos you'll see me complaining about it saying it's not working what I found out was that I had two extensions installed that were uh that had the same thing so if you do run into problems with G pod you can always disable all extensions and see if that resolves your issue but usually the issue will be with um uh browser based extensions if you saw what was happening my other videos where things were hanging up you will definitely see them in some videos but uh that's fine we we end up using Code spaces or something else in some videos I'll go ahead and type clear here I'm again just waiting for all these extensions to load um while they're loading I'm going to go ahead and just test our CLI we'll say STS get call or identity I think this is already configured for ads examples it is and it's good thing we gave access to SCS otherwise we wouldn't be able to make that call um but yeah I'm just going to uh wait for everything to load I'm not sure why code Cloud keeps wanting to install I'm not using that today uh so just go ahead and install that it's kind of driving me crazy um but anyway we'll just wait a little while here for everything to be installed and um yeah just a moment okay all right so my extensions have loaded I'm going to go back just up to here and uh what I'm going to wanted to do here is go ahead and go into my IM directory and I'm going to make a new folder I'm just going to call this policy and we're going to make a new file here called policy. yam I thought it'd be fun to get some practice with yq so I'm going to go ahead and install that Brew install yq and once yq is installed what we're going to be able to do is turn that into Json dynamically I'm going to go ahead and just make a read me uh up here and just provide some information so convert uh to Json and so that would be yq hyen o uh Json and then we specify the fall policy and then what we can do is convert it over into um our Json file let's go ahead and start writing some stuff here so um the way policies work is they always start with version and so this one's going to be 2012 1017 of course you can write this as Json but I just want us to get some practice utilizing yaml where we can and using conversion uh there can be multiple statements so we'll go ahead and write one the Sid is just going to identify what this is so this one will give us access to S3 access to S3 um and then we'll have an effect here so we'll say allow we'll have an action here and then this is going to be all of S3 so we will lock it down to uh very specific resources but for now we're just going to provide it to everywhere okay and so we'll just say for all resources anywhere S3 uh and so that's a very basic permission sometimes there's a principle in here it depends if you need to lock this down for a very particular account or user uh but right now we're just going to say uh resources as such um and so that's pretty straightforward as a policy let's go ahead and test our conversion here this this one here and so we can just copy it I'm going to CD into our IM directory our policy directory here and we going go ahead and paste this in let's see if that works we'll hit enter and it does convert it over so it looks pretty good I don't want to have to remember this constantly so I'm going to go ahead and just make a a new uh bash file here and we're just going to bring that command over here command the bash script and so this one's going to be uh convert okay and so we'll just go grab this command here I'm going to need uh the heading for it I always forget what it is I'm just going to look for one anywhere I think in S3 we have a lot of bash scripts we did a lot in S3 as you can tell I have a section here just for bash scripts we'll go ahead and copy this S3 is a very important service so it it deserved all that attention we'll go ahead and paste this in okay I'm going to go ahead and chmod this chmod and I'm noticing that this is becoming unresponsive this is a problem that I was continually having with Git pod and so what I found out is that I can just disable uh the extensions uh so I'll go ahead here and just I mean we could try to reload as well that might also just resolve it really quickly let's just say reload uh window all right and uh yeah now I got control back so that ever happens to you can do that I thought it was because I had two vims installed I do have like two G graphs maybe installed so there could again still be an issue with something that I have installed what it is I don't know um but uh yeah it would take time to figure out what that process of elimination would be anyway I'm going to go ahead and chmod this and we'll say policy Json LS LS typh and LA okay and so now I'm going to go ahead and just delete that delete that file make sure this still work so we'll go ahead and delete it and I'm just going to run convert here convert LS did I not chamod that maybe I didn't chamod it and I thought I did there we go and so now that should produce that so now we have a a way of um working with that so that's really good um but anyway the real question is will the ADI allow us access to S3 so down below we'll just type in ads and if you don't have that prompting there is a environment variable that you can look up the docs for keeping Auto prompt on I'm going to go ahead and type in S3 and we'll say LS because I want to list all buckets notice I do not have access to list buckets so the idea is that we want to to um attach this policy and maybe the word convert is not great I'm just going to rename this to update because the idea is that uh what we want to do is we want to update that policy but uh first we need to create it so I'm going to go over to it CLI and just Google it give Google to a moment to catch up here excellent we'll make her way over to S3 and in here there should be a way to uh create the policy so we'll go down here we'll go it says S3 I want a I am sorry so go here and we'll go to am and there should be a way of creating a policy so we'll just say create policy there we go down here look at our examples we'll grab this and I'm going to go over to our read me here and paste this in create I am policy and our policy file is a Json file I'm going to call this my fund my fun policy so we can remember what that is if you get an error because you might already have one named that that that could occur and you might just have to change the name um but yeah that looks fine to me I'm going to go ahead and copy this says when creating policy syntax error in the policy so there's an issue with our policy I'm just carefully look at this here uh resources spelled incorrectly so go back over to this and it's not supposed to be plural either so we'll go ahead and try this again and I'll run update okay and then we will go ahead and try to create this policy again an eror Cur Mal form policy document syntax in the policy so we'll go back to our policy and look at in the Json so resource looks fine action looks fine effect looks fine it all looks fine to me so if we're ever having issues with our policies locally we can always go over to ads we'll go to policies um there's like a policy analyzer I'm not sure if that is gone or moved as it keeps changing things on us but there definitely needs to be a way of testing out policies to see what you had access for but I can't find it so I'm going to go ahead and just paste this in and see what it complains about and there is a problem oh it thinks there's an array okay so really this is supposed to be ajacent object and on an array so if we go back to our file here we just take that out there and then it'll actually probably generate out a um adjacent object um so let's go ahead and try this well I want to I want to do the update first and again this is hanging so I'm going to go ahead and just oh there we go it's back so I'll do the update again we'll take a look here and so now it's just producing the Jason object which is perfect and we'll go ahead and attempt to run our um our policy Jason script again and it's becoming unresponsive so if you if you find this ever too unresponsive you can just reload the window but you know again I think it has to do something with one of my extensions I'm not exactly sure which one it's very frustrating and I'm hoping it's not get get pod but I could go in here and probably like get rid of a few things um yeah not exactly sure what it is but I really don't like it but we'll go ahead and we will just clear this out going to copy this again paste this in we'll hit enter still doesn't like it when creating the Sid must be a alpha alpha numeric what what's wrong with it Sid let's go back to this okay we'll take this out okay is that better Sid come on so I think it doesn't like the spaces in here so we'll just take those out all right and actually we got to do that in our yamel file so all right we'll just type in clear here and we'll try this again we'll update it first of course and then we'll create the I policy there we go so our policy is now created now the idea is that whatever we every time we change this we want to update it and so there should be like an update policy in here not exactly sure but I I have a feeling there probably is one that or it's like create version update policy so notice there's no update so I think we saw it earlier we saw um create policy version creates a new version of the specified manage policy to update a manage policy you create a new policy version okay fair enough so we'll go down here we'll copy this and this time this is going to be brought into our update script and in here we need to get our exact um the exact uh Arn here so here's the RN I'm going to go ahead and copy that right here we're going to hardcode it but you know you could be a little bit more clever with it if you want to I'm just trying to make this work here today so we'll go ahead and paste that in and so the idea is that we have our new policy document uh it's always going to be policy like this we'll go ahead and save that and so now so now the idea is that um this isn't attach it though so that's one thing that we'll also have to do before we proceed forward because I want to te test the that before we create our new policy so go back to IM and we need to attach the policy to a user uh so maybe there's like attach attach user policy here it is and we really don't have to do this once so go down here below and we'll go back to our documentation attach policy to user and we'll paste this in it expects the R so we already have that right here so go ahead and copy that and we'll paste it in it's am we'll continue we'll leave and we'll go to users and we'll go to it us it us examples and we'll paste this in here and we'll go ahead and copy this and we'll paste it it we'll hit enter and so that should attach the policy to the user if you type in OS um S3 LS it should Now list any buckets if we have them says we still don't have access let's go make sure that this uh user actually has that that permission now so I'm going to give this a refresh here and we do have it attached we expand it it says S3 for everywhere so we should have access to S3 because we just updated that we'll try this again there we go so there was a little bit of delay for that uh for that to take effect but it clearly is working you'll notice that this is very permissive it's showing everything if you don't have any S3 buckets it would just show nothing there so just understand that as long as you're not getting error then it probably is working okay but um my thought here is that we we want to iterate on this very quickly and so um what we'll do is we'll I suppose we can well I guess our script is already done my my other thought was like okay do we want to always create out the Json file and have it here because then it's kind of like sticking around I was also thinking that we could pass the uh string here because that is something that we can do so that's what I would like to find out is if I just take this line here and if I were to do this would this work really curious to see if that would work um so yeah let's find out it might not because I'm doing something really weird but uh you know it doesn't hurt to try and so here what I want to do is I just want to change this to um not be for everything so let's go over over to S3 and look look up permissions see if we can find a list of it here S3 it's actually action so you say S3 actions I just want the list list reference okay let's try this here there we go I think this is it and what I'm looking for is list list bucket this is not very helpful so another way we can find them is if we go over here and let's just pretend that we're editing this policy and if we go over to the visual uh visualizer here if we expand it it will show us all the actions so we go list and let's say we wanted to be able to list bucket that's what I was looking for which was that so we'll go here and just say list bucket okay we've saved it now I want to see if our new script works as expected so we'll go ahead and do an update here I'm expecting it to fail because I I have a feeling we're supposed to use XR or pass it some other way and uh I'm in another bash tab here I'm going to go back up to here and this one is still unresponsive which is interesting but I close that out we're going to CD into the S3 um not S3 IM policy directory and we'll just do that update here and here is saying unknown options because it didn't capture this as a string so what I'm going to do is just put double quotations around this okay and see if that fixes the issue cool and I wasn't expecting it to be that easy but we need to make sure that actually does work by refreshing this and we'll expand our fund policy and notice that we now have our update so now we have a script and I believe that when we update it now we would need to have a junk Json file here okay and so it is nice to have the Json to analyze it but we'll worry about that later okay and I'm just trying to delete this file for whatever reason this thing is uh again it's struggling I'm G to go ahead and give this a refresh yeah I really thought I had fixed my git pod issues but I guess not I've been wrting support on this um trying to get help they're not very helpful which is uh making me rethink G pod but anyway we'll go back here and I'm going to go ahead and try to delete this file again and so now it instantly deletes so that's really excellent here um but anyway so we now have a means to uh rapidly update our files just because my G pod's kind of acting funny I'm going to go ahead and just update the progress so wiip policy updater um and so we want to practice um uh PP right the principal lease privilege right and so what we could do is we could take it further by saying we only want to get access to a very specific bucket so right now we're saying all resources but I'm going to make my way over to S3 in a new tab here and I just happen to have it here so I'll click onto that and we'll give this a moment and what I'm going to do is create a new bucket um I seem to have a lot of junk buckets here but we'll make a new one and call this bucket my cool bucket we'll go all the way down to the bottom here uh you know what that's not going to work as a name we'll say ab and then some random numbers and we'll go ahead and create this bucket this one's being creating USC to one it doesn't really matter because we are uh like it's a global Service so it should list out all buckets um so what I'm going to do is I want to get that specific AR for that bucket so if we go into here it should list the AR somewhere if we go to permissions no I mean we could assemble our own AR it's kind of annoying that I'd have to do that here's the AR there it is and so we'll go back over to here to G pod and we'll paste it in and so the idea is that um it should only list out that bucket now I should I should say that different actions require different kinds of resource references so if we wanted to see items in a bucket we'd actually have to provide it a bit differently like here but I'll just show you what I mean if I go over to the policy Creator so if I go and just create a new policy we're not actually going to create one we're just going to see what happens when we specify a couple options so if we go to S3 here and let's say I wanted to list so I go list bucket and I go down here notice it's saying like if you want to um specify an exact bucket you have to provide the bucket AR okay so if I go here I'd actually have to provide that uh bucket AR there the one we just did no if I if I checkbox any any it would do this but it would want to have this value here okay so we go add the Aron okay we're not doing this for real um and I'll just paste it in here okay it could be that but now let's say we wanted to read objects like get an object no now it wants the object resource and it wants the the AR structure for that so we know what the bucket name is it's my cool bucket so I'm just going to copy this here right and now it's saying what object are you specifying often we say any object and so that is what that's going to look like and if we go and take a look at the Json you'll notice that we're specifying uh this for both of these so this one's the bucket resource and this is objects resource and it's going to be for specific ones you could break this statement up so that um if you really wanted to be particular you could take this and say okay I want this to be exactly what it's supposed to be and you could go ahead and take the get object out of Or List bucket out of here and then this one so it's exactly that and then you could go down here and say list bucket and then do that the only thing I would say about that is um just get the comma off of there it should be uh should be fine now I think it doesn't like the fact that there's two two SIDS with the same name update that there we go and so you can do this the only issue with um having a lot of these statements is that there's a limit of how many statements you can have in a policy I believe the reason I know that is when we wrote Our validators to test us we hit um restrictions on there so let's just say policy uh policy uh limits I am let's see if we can find that here statements the maximum limit of attaching a managed policy so there's I guess managed policies but the interior is it just a character size limit so it's not really calling out to what that issue is but I just remember from experience that there were limits around here whether it was character space or statements obviously having multiple statements is going to add more characters so the more compact you can make it the better but you have to strategically think about about it and making sure you're not being over permissive for things that you should not be for let's go back over to here um and so we have our uh very specific bucket which is great um and what I want to do is go ahead and save this and let's do our update and we'll have to keep going back into here okay and so now our policy should be updated let's confirm that let's go back over to our user and into examples and down into my fund policy and so that doesn't look updated to me let's give this a hard refresh because we cannot trust the console maybe the new console will uh have less of that issue there and so we go down here we have that resource is now updated so let's go back over to um G pod and we're going to go ahead and type in a ss3 LS and let's see what it lists out and so notice that it says access Den n because we only have access to a single bucket and in order to have access to all buckets we actually have to give it a wild card for the buckets otherwise it's not going to work okay so um even though you know we're specifying that one resource it's just not going to work for that um that action there okay um You' think that it would just list only the buckets you need but let's go back and take the ACT take a look at the actions and see exactly what so we'll go here and we'll go here and we'll look for list bucket so notice it says grants permission to list some of all buckets and then here we have the bucket wild card we'll click into that and so yeah we'd have to have a wild card on the bucket here so let's go back here and we'll modify this so now we're saying all buckets right um and I don't think there's any other option here it's not like you could say list only two buckets and get those two buckets back which not sure why that is but um yeah okay so we'll go back over to our policy I'm just type in clear here and we'll do that update and we'll go over to examples and we'll give this a hard refh because we of course we know that this is an instantaneous so expand that okay it was pretty quick there we go good and we'll go back over to here and we'll now try ads S3 LS and notice that it's not working give it another try give it another try give it another try okay let's go back over here for a second AR adabs colon S3 colon colon colon is it not supposed to have three colons we copied it we did what it told us to do if you're wondering like why there's colon here it's because normally there'd be like region account other stuff in there but uh right now we're just trying to get all of it and if we're not confident about it we can go back over to here and just double check um so Json object is this this is for all of them so I'm going to go here and just delete this and try this again because I want to just make sure that I'm writing that um resource correctly and so I'm going to go for list bucket and we'll go down here and I'll say any bucket and that looks like what I'm providing so I'm just again double cheing it's identical let's try this again and it's telling us we don't have access why not we're doing exactly what this policy is we go over here we take a look at this policy it's the same policy it's identical we'll go back over to here we'll give this a hard refresh we'll make sure that policy is attached it is okay so let me list all right so this is where like you can run into issues where like you know what you're you're doing you know how to do this but for whatever reason you don't get the result that you want so what I'm going to do is I'm just going to try this here really shouldn't have to do this but we'll wild card it go back to normal we don't have to have uh array but we can another thing that we could do is we can just put it down the next line here which is a lot nicer that'll still turn into an array we'll type in clear here and I'm going to do an update I'm going to go back over to here I'm going to give this a hard refresh list bucket okay good access Deni access Deni I'm going to type in ads STS get caller identity because I I feel like there's something weird going on here no it's the right user so all I can think of is it's not propagating just give me a second to figure this out all right so what I've been trying to find is the policy simulator which um I haven't been able to find it via the UI it used to be easy to find it was like right around here um I wonder if it still exists I'm going to click on this link here that I found here it is super old but I mean it's always been a really useful tool so I'm really surprised that they've kind of like shuffled this um into nowhere so I can click on itus examples and we have our policies attached here and the idea is that I can um ask if I have certain access to certain things so I go ahead I type in S3 and we'll go to list I couldn't search it but we'll go down here and find list bucket right and so then we're going to say run the simulation and it says I'm allowed to access this okay so again this is just a sanity check to going why doesn't this work because clearly it should we're going to make our way back over to uh here and we'll try this again and it says list access denied so what I'm going to do is I'm just going to save this and um I'm going to stop this workplace workspace I'm going to start it up again I want to just see if I somehow magically have access because there's nothing that should be preventing me to list out buckets because the I policy simulat says I have access I clearly have attached access um so it should just work but we'll just give it a moment to uh shut down there and I'm going to just go ahead and launch up a new one and hopefully we might get some kind of different result I'm just trying to rule out was like some kind of weird caching that's going on my side with um with the uh ads client all right so we'll go down here we'll type ads S3 LS okay this is crazy it's it's like the easiest action but this is what I what I talk about when we say cloud is hard because you could be doing everything right and it just doesn't work um so what I'll do is I want to go to this this is my fun Poli I'm go back to continue here and I'm going to check this here all resources full access my fun policy we'll edit this S3 we'll go to the visualizer okay we'll do it visually that way there's no way we're doing it wrong okay we'll say list bucket the heck is list all my buckets grants permissions to list all buckets owned by the authenticated Center oh okay and then we have all I'm going to check the policy it didn't do exactly what I wanted we'll go back here I really don't like using the policy Creator but it gets really good for debugging we'll say all possible bucket buckets we'll go back here there we go we'll go down and hit next we'll save those changes before a manag policy can have up to five versions before you create a new version you must delete a version okay so that's probably I don't see how that would be related but obviously that is a problem for us so we need to delete previous versions I didn't know that because unless you are constantly frequently updating like here in video you wouldn't know so we'll go ahead and see how we can delete previous versions delete policy version we'll go examples and then we have that version number so we'll go over here and we'll take a look and see what what kind of versions We have my fun policy policies so how do we know where the versions are over here we have policy versions how do we know what the version ID is like it's just 1 2 3 four five okay so let's just see if we can delete it first notice that we can't delete the latest one because that's the default one um so go back over here I'm going to just paste this in here for a second make our way back over to IM go to policy here we'll go to sorry to read me deleting policies also I kind of wonder if like when you're working in um data console management management console if it just automatically deletes old versions because I think that would be really interesting um because it might right because like I I just thinking like every time I updated in the console like I never hit a limit so what's going on here so we'll go ahead and do this uh we'll say one delete version one I'm just going to make this a little bit easier and put this on one line and I'm just say version 2 3 4 okay let's see if we can just run them all I don't know if like when you delete one if the version two becomes version one let's try this out okay we'll go back over to our policy we will give this a refresh here so this is our latest version here again look correct to me there's nothing else it could be right um and we'll type in a S3 again I'm just going to see if it works for some weird reason now still doesn't okay great we'll go back over to here and go to permissions and we'll edit this we'll go to the visual visualizer we'll delete this out we'll go back to S3 all S3 actions let's just give it full access here okay not very PP of us and I want to go back over to here and I want to test this again okay something really is up because there's no way we don't have any access whatsoever ABS STS get caller ID okay be back in just a second so I've checked out the user and it's still saying uh this for our policy and can give this a refresh maybe it's just delayed okay so that's not helping we going go back and take a look at our policy we'll filter this and so we'll just see ones that are managed by Us customer managed ones and we'll go down to my fund policy again we'll check its permissions did did we not save our update version six oh it's still just list I thought we changed this to be everything delete delete permissions uh we'll go ahead and edit this we go back to the visualizer I'm going to go and say all actions for now we'll go next we'll say save the changes we're going to check the policy versions the latest one is version seven we're going to expand it it shows all actions here everything access to everything right we'll go back to our user we'll check this out and we'll go down below here and take a look at our permissions we have full permissions full permissions and we'll go ahead and try this again all right so now we can list all buckets which is insane um it makes me think like there's some additional permissions that we need to list out all buckets which I don't what else what we need to know because we had what we had so what we'll do is we'll go back over to uh we'll go back over to our policy and I want to edit this and I just want to take this part out okay we next we'll save the changes so much for our our script e and so now we've updated this I want to go back to the user I give this a hard refresh we'll drop this down now it's only listing out buckets let's go here and take a look and see if we still have our problem from before access Den night so it makes me think that we're missing additional permissions um so if we go back here let's go take a look at list bucket grants access to list some of all the objects and then these are the conditions that we can apply on it weird I'll be back in just a moment all right here's one thought that I had okay um what if no I don't think so cuz I was thinking the other thing is that what if I can't just list out buckets that I created like it's a bucket but I I don't have permission to create that like I'm not the creator of that bucket but I I don't think that's actually the problem if we go like I just want to show you what if I go over to S3 see how flustered I am with this because it's so easy to do and it's not working here today um so if we go to our bucket there is a crater of this bucket right um so we go to properties it should say who created it like there's an owner I guess the owner would be the account and not the user so I was just trying to determine whether like buckets were at the user level for ownership but I they're not they're at the account level that's that's what I remember them them being but it's really interesting that this is not working um the other thing I can think of I mean this seems really silly but I'm going to go ahead and just type in um region Us East one really doesn't seem like I should have to do that no so yeah I I don't know why I don't know why it's not working but um it's a waste of time for us to fiddle with this all day long so what we'll do is we'll just move on and we'll just give it access to the actual bucket itself but it's a little bit disappointing that I couldn't get that to work but uh you know sometimes it's just how it is let's go over to our policies and we'll go ahead and go to our um customer manage ones and we'll go down to my fund policy and in here I'm going to edit this and I just want to give it access to um read objects right so if we go here I want to read what's inside this bucket so it's really get object that we're looking for and then we'll go down below and we'll say We'll allow it for um any bucket any object and so what I really want to show is the fact that you could put additional conditions here um and so those are going to be really based on what we saw earlier which was over here so if we have get objects you can see that these are uh types of conditions that we can put there's ALS also Global conditions that we could restrict here okay and so if we say add a condition we could drop this down and so I think everything that's listed here is what we're allowed to uh utilize so uh you know some of these will expect like headers to be passed during um uh the condition or there could be other stuff here like you could restrict this on on like this would say only allow objects with this particular prefix to work so this could be something the string equals equals hello right and so now you have a policy that restricts the prefix there which is interesting because you could also just do this right but anyway this does provide some more flexibility um I'm a little bit bum this didn't work out exactly how I wanted to but I think that we learned enough about policies to understand how they work I I'm confident that you'll understand how conditions work resource action and we did get to play around with I am policy simulator which is a bit old and and dig up some other stuff so um you know I'm satisfied that we learned something sufficient but uh you know again it would have been nice if we could have automated this a little bit here I'm just going to update this file here and we'll go ahead and just clean up just going to just stop my workspace and I'm going to go ahead and delete this bucket because I have enough buckets as it is we'll go ahead and delete this bucket great and I want to go ahead and just revert my user back to its original state where it had admin access moove that policy and we'll go to our policies say my fun policy and we'll delete that policy whoops and we'll see you oh nope almost done did again this did I give this user admin access hold on here there we go and I'll see you in the next one okay ciao so you can also set up password policies for your users uh so you can set like the minimum password length or the rules to what makes up a good password you can also rotate out password so that is an option you have as well so it will expire after X days and the user then must reset that password so just be aware that you have the ability to do password policies hey this is angrew brown and in this follow along I want to take a look at password policy for IM am uh so this is pretty straightforward and I'm imagining that they've probably changed the interface since the last time I've used it but what we'll need to do is make our way over to IM go to the dashboard as I imagine that's probably the most accessible way to find it and I'm going to scroll on down here and I'm not exactly see it there but we'll go to account settings okay and so here it is so now it's over here which is fine we'll go ahead and hit edit and we can see some settings that we would like to do so if we go over um to custom we can change the the password length we could tell it to require at least one upper letter case one lower letter case at least one uh one number require at least one alph alpha nonalpha numeric character you can turn on password exporation after a few amount of days you have password expiration requires administrator reset so the admin has to do it allow user to change their own password prevent password reuse so this stuff is pretty straightforward uh will the exam every you ask you a question on this probably not um but yeah I just wanted to show you where it is and there you go okay so let's take a look at access keys because this is one of the ways you can interact with AWS pragmatic L either through the ad CLI or the SDK so when you create a user um and you say it's allowed to have pratic access it's going to then create an access key for you which is an ID and a secret access key um one thing to note is that users can only have up to two access Keys within their account so down below you can see that we have one as soon as we add a second one that gray button for create access key will vanish and if we want more we would either have to we'd have to remove Keys okay um but you know just be aware that that's what access keys are and you can make them inactive and you're only allowed to have two so let's quickly talk about MFA so MFA can be turned on per user uh but there is a caveat to it where the user has to be the one that turns it on because um when you turn it on you have to connect it to a device and your administrator is not going to have the device so it's on the user to do so there is no option for the administrator to go in and say Hey you have to use MFA um so it cannot be enforced directly from an administrator or root account um but what the administrator can do if if uh if they want is they can restrict access to resources only to people that are using MFA so you can't make the user account itself have MFA but you can definitely restrict access to um API calls and things like that hey this is Angie Brown in this video I'm going to show you how to to turn on MFA in ads it's very straightforward I probably show this in the cloud predition but for my other exams just in case I didn't going to make this video again and I have to be on screen for a little bit because I'm going to actually have to show you uh my phone to show you the the end to end process okay but I'm just going to get off screen for now so we'll get out of here and what I'm going to do is I'm going to go over to users I'm going to go over to it examples I'm going go to security credentials we're going to sign MFA and so here what we'll do is we will assign an authenticator app you can see there are three different options here security keys are nice but uh I just don't ever seem to use them because authenticator apps are so easy now always wanted a key then I got one and then it just never never went into use so properly so just say uh my MFA doesn't really matter what we call this here we'll go ahead and hit next and so it's going to show us the QR code so what I'm going to do is I'm going to get out my phone and all phones have this capability but the idea is that you're getting out the phone and you're going to your um phone your your phone thing right whatever opens up the phone I'm saying phone your photo your photo app so mine's down there it's the red one in the middle right there right Android over here and so when we open it up I'm going to hold up my phone to the QR code it's going to make little arrows around it and then I'm going to click the link and now it's opening up like this didn't exact work how I wanted it to so that didn't work out if that doesn't work out the other thing you can do is have like an authenticator app installed there's a lot out there I like aie but there's aie Microsoft authenticator Google Authenticator there's a lot of them uh so I'm going to open up aie and aie I have a pin so I have to put my pin in here I I can't really see my screen but uh you know once you're in there if you click the um the three ellipses sorry I'm just trying to find my uh screen here I'm going make this a little bit bigger so you can see me a whole lot better hey hey everybody fat Andrew skinny Andrew anyway uh you know going up here in the top right corner those there are those three ellipses as you can see there I'm going to drop that down we have add account it's the top one and now it's asking you to scan the QR code so sometimes if you have a QR code it will automatically know to open up the uh the app but it's not doing that so now I'm scanning it's showing me like this little thing so I'm trying to get that into the QR code box it's picked it up and now it shows me that it's connected good time to rename this so you don't get confused what it is especially when you have multiple ones in here we'll go ahead and hit save and so now I have this code that's spinning right and so what I have to do is I have to enter in the first code as quick as I can here I'm going to get this off the screen 3434 oh 3434 and what I'm waiting for is the next code to show up okay so this one's still on the screen and that thing's going all the way around and I'm just waiting for it to uh elapse so I get the next number give it a moment here all right and so now I got to enter that one in in the second one here 562 298 and we'll go ahead and add that MFA all right so now MFA is uh on and so the next time I log in it's going to ask me to enter that code I really don't feel like showing that I've done this so many times in so many videos it's very straightforward though I will point out that sometimes MFA does get kind of messed up and I don't know why what can happen is like because this St is like synced to your the time of your phone or whatever and so I've had issues uh in these accounts where I couldn't get back in because like these other user accounts because MFA was messed up so I had to remove my MFA add it resync it notice there's a resync option here so sometimes you have to resync and enter the codes in again um and that's absolutely happened to me so just understand that that could be something that happens to you but I'm going to go ahead and remove this because I don't want to have MFA on right now you should have MFA on but for me this is a development account and uh I need to do a lot with it so for me it makes sense not to have it but for everybody else you should have MFA turned on um when you're applying MFA for your root account um you're not going to be able to find your root account over here you'd have to go somewhere up here drop down and go to account or security credentials I guess so if you go here this brings us to the same similar page so I'm just telling you this for the root account if you're looking for it it's under here okay because Zs has this checklist to say add MFA to your root account you really should I have for this account because I don't care there's nothing in this account it gets blasted away every other day but uh yeah there you go we'll see you in the next one okay ciao hey this is Andrew Brown and we are taking a look at S3 one of the uh most used services in ads and connects to basically everything so we are really going to cover this uh very thoroughly but let's talk about what is object storage so object storage is a data storage architecture that manages data as objects as opposed to other storage architectures and so S3 can provide you with unlimited storage I put an aster there because uh if you're using outposts that's not necessarily true there are limit um and there could be other limits that are not necessarily limiting the amount of objects but there's some other things that might prevent you to have unlimited storage but for the most part it has unlimited storage uh you don't need to think about the underlying infrastructure it is a servoless service so um that's really nice the S3 console provides an interface for you to upload and access your data which has get been getting better year after year um but let's go break it down and talk about the two most important components we have S3 objects uh these contain your data and they are like files and they will consist of uh things like a key which is the name of the object a value which is uh the sequence of bytes of the data it will it could have a version idea if you have versioning turned on it could have metadata then you have S3 buckets these are what hold objects uh buckets can also have folders which in turn hold objects S3 is a universal namespace so bucket names must be unique it's kind of like picking your own domain name and sometimes it's fun sometimes it's not uh and the other thing is that you can store an individual object of Z bytes to 5 terabytes in size so you have uh that range that you can work with so there you go hey this is Angie Brown and we are starting with our S3 fall alongs we got a lot here to do it's such an important service I just want to remind people that we're going to try to stay away from click offs as much as we can we're going to use click TOS as a means to look into the services and understand the feature set there but we're going to try to focus on using the CLI the SDK uh infrastructures code all those things because those are the things that are going to really matter uh when you work in the industry the exam uh depending on which one you're taking they do care about this stuff to different degrees but they're not going to uh test that so so so much to speak so we're going to make sure that we do put a large emphasis on that programmatic part because I know it is such a issue for people to get certified and then they don't have that other component so anyway we'll get started here and we'll make our way over to S3 so we'll go to the S3 console by going to the top here typing search and clicking on the S3 service this is a global Service so uh buckets are deployed in particular regions but it's always going to say Global up here so when we create our bucket that's when we're going to determine what region it's going to live in so what we'll do is just go through a very basic practice of creating a bucket throwing an object in there and then we'll do it programmatically and from then after that when we get the programming down and all the different variations we'll then go look at the expanded feature set of S3 and there's a lot here in S3 so we'll go ahead and create a bucket here and we have our bucket name now the most important thing is that these buckets need to have uh very unique names because they're in a global names space think of them like domain names they also have strict naming rules not as bad as Azure but there are things that you just cannot do you cannot put underscores in the bucket name you cannot do um capitals in the bucket name so I'm going to name this uh something so this be you know my example bucket uh AB for my initials Andrew Brown but yeah if whatever you type in here doesn't work just put something in till it works down below we can choose our region so you know North Virginia is fine I'm in Canada so I would like to go ahead and do that notice that some regions are disabled it's possible that you can enable regions somewhere in the Idis console that would probably be in a separate video so we will choose uh whatever appropriate region is near you if you're in North America choose East us or if you're in the west coast choose West if you're in Europe choose Europe ones choose what's relevant to you so we'll go ahead and move on to object ownership so here it's talking about ACLS acl's is one way of accessing S3 will'll definitely have a separate fall along for this we're going to leave those disabled we have our block Public Access setting for this bucket this is always turned on by default when we want to do particular things like work with ACLS or bucket policies or allow people to access uh the bucket anonymously from the internet we'll have to disable that but we're going to leave it um blocked for now because we don't need to worry about that we have bucket versioning so here we can enable our bucket versioning to keep track of files we can apply tags we have our encryption levels uh make note that it is set to SS S3 so serers side encryption S3 by default this is a managed um a managed type of encryption that adabs provides for you notice that you cannot turn it off that could be an exam question so just remember that now in the past was it was there ever an option to have it off I can't remember it's been so long but at least we know now it's it's on by default so that's great then we have have bucket key so using the S3 bucket key for S ssse KMS reduces encryption cost by lowering the calls to a KMS so they're just saying that they probably have a existing one that you can use for KMS we'll talk about KMS later on that is uh its own thing and it's it's pretty large we can do object locking so this this gives us uh write once read many we' cover this in the lecture content we'll come back to that later on we don't need any of these fancy settings if this is the future and the UI is changed well just work your way through it um unfortunately adab us likes to change their uis often on me um so that's just how it is but we'll go ahead and create this bucket and so now we have this bucket it's called my example bucket AB you can see I have a a bunch of buckets because this is a active account and I'm going to go ahead and type in my example bucket ab and we'll just click back here for a moment and just make note of the region uh make note of what access it has when it was created we'll click into the bucket and so now we have this opt options across the top for some reason adus has shuffled the um options here so many times but you just have to get used to finding where they are they used to all be on one single page which was really nice now they're broken up in tabs and then they kept resorting them around but anyway here we're on our objects Tab and so this is where we're able to upload objects create folders um so the thing with the thing with S3 is that all the files are stored flatly so when they say folders they're not like real folders they're basically prefixes to the buckets um and it's important to remember that because if you just know that they're not real folders they're not going to act in the the same way as You' expect uh when interacting with them programmatically down below we have a lot of actions that we can perform but uh we should first get a object in here so I'm going to go out to the internet and get a graphic it'll be back in just a moment okay all right so I have a uh image on my desktop that we can utilize here and uh we can actually I think drag files right into here but what I'm going to do is go ahead and click the upload uh button here and notice we can drag and drop our files actually I think before you used to be able to drag right into here I think they might have gotten rid of that nope it looks like you you still can but we'll click in here anyway and you can see that you can drag into this area as well we could manually add files I have a folder uh on my or just my desktop it's a folder necessarily but I have this image here that says 0 0 it's me from uh one of my boot camp photos I'm just going to go ahead and rename it we'll just say Andrew Brown cool and again just get whatever image that you want to use yourself I'm going to go ahead and drag on this image into this area here and it should uh add it down below so the idea is we're saying we're queuing up what we're going to add in and then we're going to go ahead and upload them so this is really great when you want to add a bunch of stuff in place notice that it's telling you what sizes you're allowed to upload so to upload a file larger than 160 gbt use the CLI SDK or the Amazon S3 rest API I remember that being a smaller value before uh maybe it's larger now I don't know they're always changing things on me but let's go down below and expand the destination details so here uh there are some things that we can do it's just reminding us that there's no bucket versioning on that it's our encryption version we and we don't have object lock the reason it's telling this uh telling us this is because when you put an object into S3 uh sometimes the initial state will be different when the when you upload it the first time and you don't have these things on and then you turn these on later on so this might indicate to you oh I really want these to be on because the object isn't going to act the way I think it's going to act in the future when I turn these on later on so here it's suggesting you turn on bucket versioning I don't want to do that right now um even though it's recommended it does have some drawbacks um in terms of how you work with S3 so we'll leave that alone down below we have permission so this bucket has the bucket owner enforced setting appli for object ownership so that means that we're the owner which is fine we'll learn later on in a follow along that we can have someone else pay for our usage then down below we have our storage class so here we can change uh the storage class of um of I guess it's the object level I would think that this is at the uh bucket level but anyway you can see that we have some options that we can change here oh no no it's fixed okay so I I was concerned I thought it was letting us change it in an object level because I I remember oh maybe I guess you can we'll come back to this and look at this in more detail because that's a big topic but in practice most people just set it to standard we'll go down below we have our encryption option so this should be at the object level um do not specify an encryption key specify encryption key so we could do that there on that object or objects we're uploading we can do uh check sums so that's just to ensure the data Integrity here so apparently we can turn that on or off um and apparently we can choose its function and some other stuff so that is really cool we can apply tagging to the objects and then we have metadata metadata is something we'll definitely cover uh and it's different from tagging it's very useful thing to have we'll go ahead and hit upload all right and so we have Andrew Brown cool jpg now I know looks a lot like something like Google drive or um one drive or Dropbox but the thing is it's more low level than that and you're not going to get um a lot of the things that you would normally get with those tools so like once you have something at least right now once you have something here you can't really move them around easily so like if I have this bucket here and I make a folder let's go ahead and make a new folder here uh we'll just say um images and go ahead and create this folder notice that it's not very I can't like drag this on over here now could I move it this was not an option before but we can go ahead and try this right now so I can go ahead and say bucket and we can browse S3 but look how much work it is to move it's not like simple drag and drop we can go here and um we can choose images here and say choose destination and we'll go ahead and hit move okay and then we'll go back over to here to our S3 bucket and we can see that we've moved it but you know previously it was really really hard to move stuff you would have to do things programmatically um it's a little bit easier but again it's not like drag and drop um but it is it is much better than uh S3 years and years ago here we can go here and rename our objects um we can edit the storage class so I guess it is yeah so storage class is at the object level so if we wanted to go here we can change it and I could change it to something else like standard IIA so hold on here before we do that um minim minimum storage duration 30 days per object fees apply for objects uh greater than 128 kilobytes so I'm going to go ahead and just change that into the standard IIA okay so that's been moved over and then we can go back and take a look and observe it so yeah storage class is at the object level so that's important to uh note there I can go back here we can change it again and we'll say storage class this creates a copy of the object uh with the update settings in the uh the new last modification date you can change the storage class without making a new copy so I guess the question is is it going to make a cop what's it going to do when we move it back there we'll go ahead and do that so in practicality you don't really need to change again storage classes too often but of course the exam wants you to know because if you're working at scale uh there could be a case where uh you like that so you can see it's pretty straightforward we can go ahead and also uh Delete delete this here so we'll go and just delete it um and we'll just type in permanent delete because that's how you delete things every service kind of does it a little bit different they might say put the word permanent delete in put the name of the object in it varies but uh that's the way S3 likes to do it currently let's go ahead and open up Cloud shell so that's up here in the top right corner Cloud shell is going to allow us to um utilize the adus CLI so we'll just go ahead and let that load it runs Linux bash so that is the environment is running I can't remember if it lets you run Powershell um it might the reason I'm I'm confused is because when you go over to Azure it also has a cloud shell and you can toggle between them so I usually expect there to be uh similar feature sets here but anyway we'll find out here in just a moment but anyway so this is now uh spun up and so we can go and start interacting with uh S3 using the CLI so there are two different ways to interact with S3 there's the S3 API and there's just the S3 commands and this is not that uncommon if you go over to like Google Cloud they will have a separate uh utility called GS util for Google cloud storage utility and then you can use the the usual gcloud uh commands I think Azure has something similar and the reason why is that you have these higher level uh simpler operations that you can do with the standard CLI and then they have the specialized one so that you can do more lowlevel operations I don't know which one came first but uh you'll just see people alternating between them and they work a little bit different so let's go take a look first at the um it S31 so ads of course is how you use the command or or or use the C and then the second thing is the command and then you have your sub command so I want to see all the parameters here I'm going to go ahead and just type in help and see if that produces it for us so it does not know that flag which is totally fine um when we're using CLI we can have it uh go into this uh Auto prompt mode which will give us more information and that's the way I like to work with it I never remember what the flag is but we'll go ahead and take a look uh what it is we'll type in ads we'll type in ads CLI Auto prompt the nice thing about Azure is that you can create a um uh like a CLI type of file in Visual Studio code and it can uh give you a lot of information but adab Us's approaches to have this Auto prompt and have everything built into the CLI which is cool as well it's just a different way to do it so we can provide uh the command line option once but I want this set every single time so I'm just going to look for where that is we'll go configure Auto prompt and I want to set this with environment variable so we'll go over here and it's this so it's adabs CLI Auto prompt so I'll go back over here and we'll type in adabs CLI Auto prompt now cloudshell might not remember this so you might see me in many videos setting this every single time we want to use it so it's not that big of a deal but we'll have to do that a CLI Auto prompt so we'll say Auto prompt and for this I want it on completely sometimes you want it on partial it's a little bit less annoying if you know the command we'll do on partial actually that just means like if you don't complete the command or the command is wrong that it will bring you into the prompt so we'll do this and we'll just put an export in front of it that will set this environment variable to make sure that it is set we're going to type in EnV this will list out all our environment variables but we can do EnV pipe grep and then just type in adsor CLI and we can see that it's set to on partial if this is spelled wrong it will won't work if you spell it wrong you can go up and I'm just hitting up on the keyboard to bring back the previous commands and you can change that stuff out so we'll go ahead and type in clear and now if we type in it S3 it should now open that auto prompt and there we go so there are other options here we can look up documents ation and stuff like that it doesn't really work very well in Cloud shell and it doesn't really work well in um Cloud developer environments but when you have it installed on your local machine um it works really really good but you can see right away we have S3 so highlevel S3 commands and then we have uh S3 API so simple storage service which uh doesn't really best describe what's going on there but that's more lowl commands then there's S3 control um I'm not exactly sure what that is we can go take a look there and just take a look at those commands because I'm not really sure and it actually has very similar uh stuff there so I'm not exactly sure why there's a third one but we'll go take a look quickly uh you know again adabs is always changing it's very important to go take a look when you see something you're not sure um so this says provides control provides access to S Amazon S3 control plane actions okay and maybe it has something to do with access points I'm not a 100% sure but we'll go down and look at uh create bucket because this looks like the same stuff that we're we're going to be looking at so go here and creates a new Outpost bucket okay so maybe this has to do with outposts um and outpost if you're not familiar with is outposts and we cover this somewhere in the course it's uh basically um ads that sits in your data center so they have a rack here and it's running those services locally there but we'll go back so we'll ignore uh S3 control and we'll take a look here at first S3 and so we have a few actions we have LS which stands for list we have websit so obviously for doing static website stuff CP for copy MV for move RM for remove um sync for uh syncing multiple files MB make bucket I would imagine and then RB would be remove bucket and then presigned would be for presign your else is there more stuff there could be no that's all that's showing up here right now if we're not sure what commands there are we can always look up at the ad command CLI I just go to Google I just type in ad CLI here like this and I usually want the reference documentation here command reference and then I click ads here in the top left corner and I try to make sure I'm on the latest version because often I clicked up here instead often there's like a version one version two and Google likes to always show us version one but I'm going to go take a look here for S3 and we'll click into here and it's not prompting for number two so we must be using the latest uh CLI command and if we scroll on down here we should start to see the commands so yeah here they all are so um I think we saw all all of them there I just wasn't sure if there were some off screen here so I wanted to confirm it but we can go take a look and see what these commands do so probably the first thing we want to do is upload something here so we can go to our copy command says copies a local file or S3 object to another location locally or on S3 and here we have all of our flags of course and the easiest way is to go over to the examples and take a look of a simple example so we have adus S3 uh CP test.txt and then we have this uh bucket URL okay and notice it has a special protocol on the front there uh for whatever reason ad us created their own uh like URL protocol and so this is a short way of pointing specifically to that bu um and you're going to be using that quite a bit there so we're going to go ahead and try to uh copy a file now we are in Cloud shell so we have to try to upload a file here or create a file which is not that hard to do um but yeah we'll just do that here in a second all right s I'm back I was just making um a couple extra slides because I S I thought there was some good things we could add there to the course but anyway so here is the uh General command it's not too uh difficult to uh remember after you've done it a few times but we'll go ahead and type in CP p and well actually before we do that we need to make sure that we actually have a file to copy so I'm just going to hit enter we'll type in clear if I type in LS there's nothing here could we upload files to here probably uh we can upload a file uh this way so I could go ahead and upload that we could also just create a file um as well but I'll go ahead and upload a file and I'm just going to go to my desktop here and upload Andrew Brown cool now I'm not sure how much you can store in Cloud shell so obviously don't go crazy here and just assume that these files are not permanent at some point they could get erased because Cloud shells not intended to be a permanent uh thing it's just something that you can quickly utilize the ad CLI and things like that we'll go ahead and type in ads S3 and hit enter and so that will enter us into this prompt mode here also you know what's interesting it says S3 control and then you have S3 Outpost so still not 100% sure but obviously S3 control is not what we want to use for for all right so um I went out to the internet and there is um you know this post here in the a CLI and so you know for me I recall that um at least I recall that in the past that there was never an output option for adus S3 LS and you had to use S3 API and so people are saying the same thing um here there're showing that S3 API works with output Json and then down below it doesn't work and so the confusion is why is it documented because uh if we go here you see 26 like likes it says output is not supported why is it documented then and I think that is a large frustration there that um a lot of people have and so you know you can't always go bu the documentation or the Learning Materials you have to test things out and see what's going on here and so this is one of those examples where um you know that doesn't work okay so we just have to know that this just doesn't work and we're not crazy so I just wanted to make that point there anyway so we'll go ahead and type in clear and what we can do is go ahead and remove that file so that's what we'll do next here okay all right so yeah we want to go ahead and remove uh that object there now I do have to point out that before you can uh delete a bucket and I'm not going to actually delete it here but I'll just show you what it looks like when you try to delete a bucket with files in it is that uh S3 is going to say hey you got to empty that bucket first so if we just look for that bucket here there we go that's our bucket we'll go ahead and hit um delete notice it says the bucket is not empty buckets must empty before they can delete to delete all objects in the bucket uh you go to the empty bucket configuration and then prompted so we can easily empty this bucket here via the The Click Ops but let's go ahead and look via the CLI we'll go back to our commands here we're going to go to our RM our remove command and there are a few things here we have a dry run uh we can tell it to not give us output we can do it recursively we probably want to recursively delete let's go to the bottom here we have to delete one file but uh um here we can just say recursive it'll delete basically the entire consents of the bucket or it should the following uh remove command recursive deletes all objects under specified buck and prefix so if we provide nothing it should go under everything uh prefixes don't matter so if we do delete that one file we can remove the bucket but uh let's go ahead and actually try to delete the bucket first before we remove the file file and see what happens in terms of the CLI at least so say adus S3 um and we'll give it a space here and I'm going to type in RB for remove bucket and we're going to specify this bucket so we'll say my example bucket ab and hit enter it says remove bucket failed when calling the delete bucket operation the bucket you tried to delete is not empty also just make a note that um well maybe not in this case but a lot of times the CLI action will reflect the permissions it doesn't in this case it's not called delete bucket it's called um RB but for most other CI commands they are pretty um pretty much the same but we'll point that out later on this is just not a good use case I suppose but anyway we'll go ahead and type in a S3 remove and we know what bucket we want it to be we'll go ahead and type in my example bucket ab and then we'll just say recursive if you can spell it uh sometimes they have shorter Flags I'm not sure if they have that for this one so if we go up here sometimes they uh show it somewhere here we type in recursive um does this have a short flag no so you have to always specify the full flag name there which is fine and so this should delete the contents of that bucket so delete those two files we can go and confirm that by hitting up and we'll do an LS if your up doesn't work it's because you uh were doing it inside of the auto prompt tool we'll go ahead and type in LS the contents of the bucket is empty now we can go ahead and delete this bucket so we go a S3 RB for remove bucket and then we'll specify the bucket name here good and so now we can do Abus S3 LS and we shouldn't see that bucket there anymore it looks like it is gone so that is really good if we wanted to filter out for a particular bucket we could do that using um the query command but we'll look at that when we have another bucket here so so what I want to do now is take a look at the other inabus S3 commands U of course we didn't do every single one here um we didn't we didn't uh do remove bucket sorry we make bucket but that's pretty straightforward make bucket pre sign we'll do that later on website is a a different story but let's go take a look at at the adus uh S3 API and so right away we're starting to see more options these ones are more reflective in terms of what the name of the um uh permissions are so no it said like you could not take that delete bucket action and so now it says delete bucket as opposed to um uh you know those shorter forms there but anyway we have a lot more options so this is a lot more robust what's really interesting is that uh when you go to other providers like Google Cloud um it's usually um the generic one which has less and uh well for output so you know one of the key difference between the uh the S3 API and the adus S3 is that U the S3 API can do the output of Json but when you go over to Google Cloud uh and you use the gsutil tool it can't do Json and it's actually the primary gcloud commands that can do it so that's a little bit backwards but of course they're different providers but you can see there's a lot of different commands here um but you know often I'll use S3 API because it is more um robust in terms of what you get back as output which is that Json which we really really like to have when we're doing things programmatically but let's go take a look at S3 API down below we'll click into it and we have a lot of actions here and some of them are repeated of course so uh you know a bucket can have a lot of stuff attached to it and so if it has cores you can delete the cores you can add the cores all sorts of things but let's go ahead and create ourselves a new bucket so we'll go here and uh yeah we'll have a bunch of flags so pretty pretty similar Flags but we'll go ahead and try this so say S3 API uh create bucket and I'm going to call this we'll say bucket name here we'll say my example bucket ab2 um we could use the same name as before uh because it shouldn't be taken by anybody else but just in case it is or there's a propagation issue I'm just going to go ahead and use this flag we might also want to provide some other uh information like the region it's going to be in now we are in global right now but uh this is going to default to whatever is the default here um and this is based on whatever that region is so if we go ahead I'm just going to clear this cell for a second and type in clear if I type in EnV grap ads it might show us the default region which is CA Central so it is already going to deploy in that region because that environment variable is set but if we want to override it we can do that and I'm going to go ahead and do that here so we'll say my um example or my example bucket ab2 and and uh this case we have to provide the um create bucket command we can't just uh write it like that and we'll have to say bucket here and I think we can just put in region on the end here I'll just say Us East one so I'm not deploying in CA Central 1 I'm doing U Us East one then there's of course a lot of other options but I'm going to be fine with uh just that so we'll go ahead and hit enter and notice right away we get Json back so now we can work with Json which is a lot easier we'll go ahead and type in ads S3 a and we'll say list uh buckets I think that's the command if we're not sure we can again go hit enter here and we'll just type in list buckets in terms of what sub commands are named they're not conventional across um Services each service team decides on a naming convention some of them follow what others do some of them just make up whatever they want so just understand that it's not always same but it's great when they're the same name as the actual permissions command so here we can see our list buckets we're getting it back as Json so we can um uh see some information there one called bucket mucket um I'm trying to find the one where it is for um this my example bucket we can go ahead and just um filter that or or query it so I'm going to go ahead and type in ABS S3 again here and we'll just say uh list buckets and we either have a filter or a query it doesn't look like we have a filter but we probably yeah we do have a query and so in query here we can use the syntax to um make this a little bit easier to read so maybe I just want to see the the name so we'll type in buckets because this object here is wrapped in a Json bucket so we'll go there and if we hit enter now notice that it doesn't have the buckets now let's say we just want to get the name so we'll go ahead and hit Q hit up again and we'll try that again so we'll say uh list buckets query buckets and then I think um we can do is it this haven't done it in a while so I don't remember is it that no that doesn't work uh we'll try that again you know once you start using the CLI more it becomes second nature but if you haven't been using it in a few days that becomes a little bit harder so we'll say buckets this is called um it's like uh JS path I think um JS path queer ads so it's whatever that syntax is JSM path here it is okay great so we'll go here I want to go to the main website because it'll show me the syntax and all I'm trying to do is I'm just trying to um select it with the name so I thought that we could just do that or we just do name on it or Square braces. name let's go try this out Square braces. name oh there we go that's what it is to get it with all the names so that's a little bit easier to read I would say and then if we want to make it even easier we could go ahead and type in a S3 list buckets um query buckets name and then we' say output text and that didn't work there must have been an error here we go back here hit enter buckets oh capital N it's I guess case sensitive there output text okay uh not as nice as I'd like it to be but um at least it is in text I just uh I wish it was on new line so maybe Json is the better one we could even say table table might be nicer there we go so now it's lising the um them in another format here and let's say we just want to get that specific bucket how would we do that let's go back over to JM path here it's saying uh question mark State equals whatever we could try that I suppose um yeah we'll try that so we'll go back over to here and I'll just try name question mark I'm not sure if that's how you do the query but sometimes we just look and we uh C and paste I know I made a video on this language somewhere but uh you know remembering every single day is a different story so I'm going to say my example bucket ab 2 because that's what I think we called it and we'll go back like that and it did not like that syntax probably because there's this here that's not supposed to be there hit enter no closing quotation um back over here oh that's using singles and we're using doubles that might matter hit enter and it's really not liking it there maybe we could try uh doubles around this one sometimes that's what you have to do you have to fiddle with us a little bit and we'll try singles here there we go and so now it's only listing exactly the bucket we want back if we just want the name we can just do dot name on that I think now like that okay and then you know we just want that text we can just say output text but query and filter are your friends you really should learn how to use them um or worst case use something like chat gbt to produce it and just get comfortable with uh being able to manipulate the Json response because it's really useful but we'll go back over to uh here into the CLI and uh we'll click back and we'll just look at list list objects or buckets sorry and if we go down here I'm just curious if we get any kind of different kinds of output besides Json I like yaml quite a bit um it does you don't always get yaml it's going to vary based on what you get here but uh you know if you like yaml I love yaml we'll say it us S3 list buckets output uh yaml and apparently I forgot to type something there it's not liking it for some reason databus S3 we'll just hit oh API list buckets that's why and then we'll say output yaml there we go and so that's kind of like in our uh yaml format there let's go back and type in ad S3 API well before we do that we well no we actually have a file we can upload it's already locally here I was thinking we could create a file and we could do that actually we'll we'll do that right now we'll say clear LS I'll say uh touch and we'll say uh hello.txt so that will create a text file should probably have zero bytes let's go ahead and look at that this has um where's the hell of txt it's zero bytes so maybe we can go in that file and add something if we want to add something maybe we could use Nano type Nano hello txt enter and I'm just going to type something and say hello world to give it some data and then down below we want to quit this so this we want to save and quit uh it should be contrl S I believe contrl s and then crl X there we go well I'm going to hit up there LS hyphen La list uh long and so now we have a bit of bite data in there I think it's good to have that in there and so I'm just going to make a new folder uh here to put those two files in so we'll say make di so make a directory a folder on Linux and we'll just say images I'm going to do move MV this is Linux commands by the way this is not ads par in particular but you should know this stuff I'm going to move that into the images folder I'm going to move our hello txt into the images folder we'll do a clear and do LS SL images and we can see we have our two files there so we have two files there and we might want to move them in bulk um we did talk about there is a sync command so that's something that we could do uh we could also probably move the files individually which is not as fun but we'll we'll go ahead do typ databus S3 API and see what options we have here so we got copy object um but is there a sync in here let's go take a look all the way back here and that might be only in the other one I'm not sure because again the ads S3 is high level and this one is low level so it might not be doing uh batch uh batch stuff but it can delete multiple objects but can it copy multiple objects we have create multiart upload copy object I don't think it can do multiples so that might be one distinction between this and the other one we'll go here um multiples multiples multiples let's just go down to examples and see what we have Copy Source so and it's changing the key name which is kind of interesting so I'm not seeing like a multi copy option so maybe that's what we would use the other one for so let's go ahead and go back to S3 here instead of S3 uh API we'll do sync and so now what we can do is specify a local folder so I'll say images and then I'll type in S3 SL my example bucket ab2 this is really confusing since we keep having this one here so I'm just going to click off it's a little bit easier see what we're doing my bucket oh okay we didn't name it two I thought we named it two all right did we not name it too oh or maybe we didn't delete the old bucket there it is okay I think it's because when I clicked back it was uh holding in what what was ever in the browser there so uh you know that's why again I I don't like the console it can be very misleading but we'll go ahead and put in two here so the idea is it will it will basically copy all those files into this folder we'll hit enter and notice that it did that so that's a really nice and convenient way to do that um the S3 API apparently does not have that functionality which is totally fine I'm going to go ahead and type in clear one thing that we didn't do is we didn't download files so that's something else that we could do um let's go ahead and type in ads S3 uh API and we will do copy object we'll say copy object and this one might actually have flags for all the options it might not have that shorthand that we saw in the other one so we'll go all the way to the top here and we'll look at what flags we have there we go so we have a lot of flags so we have Copy Source if match Copy Source if Etc content type I mean there sure is a lot of stuff um we'll say Copy Source and we'll take a look at what that flag tells us what what it will do specify the source object for The copier operation uh could we specify the actual um S3 U it's not saying that here specify the source object for The copier operation for objects not access through access points no we're not worried about that for object access through access points okay and there's key here the the key of the destination object maybe there's copy object and there's download object let's go take a look maybe there's like a download option instead because they might be uh separate separate actions whereas uh CP can um and the other one can go both ways which we didn't do but we can go take a look at that in a moment um so do we have like download object we also have put object which it's a little bit more normal have like a download option here no let's look at put object I guess copy might copy objects within the bucket whereas put will actually put the buckets up in terms of the files yeah that's probably the difference here let me just take a look here okay oh sorry creates a copy of an object that is already uh in Amazon S3 okay great so you know just understand that when you're using adus S3 the standard one and it says CP copy can also mean put object that's the underlying one or it could literally just copy an object uh in place so that is a a key distinction so really we're more interested in this uh put object here let's just go take a look at an example as there's way too many flags um and I bet it can go both ways so we have this is S3 API yeah so we have put object we specify the bucket we specify the key um how do we download okay so I'm not exactly sure with the uh the API let's go over here and we'll open up a new one uh how do we download a file from S3 using the S3 API get object oh of course of course it's get object so that's what we really wanted all right so what we'll do first is we will um download a file so get object we'll specify the bucket so that's going to be um my example bucket ab2 and then we'll probably need to provide the key so we'll say hello.txt and then what else do we need out file so that's going to be the name of the file um that notice here that there's no Flags in front of it so we probably just put the name in front of here so I'll just say hello to we'll say world. txt here we'll hit enter notice that it it says um it looks like it's downloaded it shows the link the files we'll type in LS hyphen LA and we should have in here world. txt so there it is I'm going to use Nano to open it up and we're going to go and change the text here so we're going to say hello world and put an exclamation marks on the end here I'm going to do contrl uh s notice when I did contr s it said r one line I'll do contrl X to exit that and so now we know how to copy let's go ahead and use the put command uh so we'll type in ads S3 API uh put object and we'll take a look there so we have bucket and we'll say um my example bucket ab2 and then we'll say key and we'll say world. txt and we need to specify the actual object so notice here it wants a body we're not necessarily specifying a file this is something that is really really frustrating when you're working with uh the lower level API because um you have to kind of encode the uh the file or the content in order to pass it along it's not necessarily um specify the name of the file and that is definitely something that is quite the challenge something we should set as content type so here I'll set the content type as string or text uh I think it's plain text that's what it is y and if you don't set the content type it's not going to know what the type of file it is that can be an issue when you're working with websites we'll find that issue out later when we do the static website hosting but what we need to do is solve for body um this one's always really tricky so what we'll do is go back to the CLI and see what we have uh as options oh it looks like we can just specify the image okay I wasn't 100% sure I thought it would be a lot harder Noti you can use the file protocol to specify the file or it looks like you can um uh not provide it there but when you're working with the SDK a lot of times you have to uh you can't just pass a file along it's it's quite painful we'll go ahead and we'll try this we'll say world. txt and we will hit enter and hopefully that uploads so it looks like it did upload we'll go here and refresh we'll click into it we have our world. txt notice it says the type of file it is um again if we don't specify the content type in this lower level one it might not pick it up um it might not know what it is which can be quite frustrating uh let's actually see if that happens if we were to create a new file so I'm going to just say um mkd or sorry we'll say touch we'll say index.html and we have no contents in it whatsoever but we'll go ahead and type in index or Nano and go index HTML just say my website I'll do contrl s to save notice it wrote the line contrl X to exit and we will now go ahead and copy um this similar line here and paste that in and I'll say body index HTML and we'll say index HTML here it hit enter and let's see if it picks up the content type and it did okay but just want to point out that it all it doesn't always pick it up and if you're using the SDK or something else you might get different results but uh at least there it's been uh okay so that's really good there let's go ahead and uh list objects so we'll go ahead and say adabs S3 API and we'll say list objects and we'll see what other options we need to specify probably bucket and we'll say my example bucket ab2 we'll hit enter and so notice that we are getting more of a uh big Json format not the easiest way to look at this but we might want to just look at the key here so what I'm going to do here is go ahead and hit up and we'll try does it this have a filter I'm not sure filters can be really useful um filters happen server side but they're not always available query definitely is here so we'll go here and um we want contents so we'll try this and we'll hit enter oh we didn't specify bucket okay so we'll go ahead and just copy this paste it in down below it will'll say bucket my example bucket ab2 we'll hit enter and notice that we're getting contents back we really want to select just this key so uh what we learned last time is put Square braces and then we just put key like this and notice that it's not putting the prefix in here so um or I guess these aren't in folders okay so um I thought we upload this into a folder did we not uh we'll go back over here CU we we uh executed that sync command earlier right and I thought we could have I thought we specified um uh an images folder maybe we did not we could probably just hit up until we find that command I think we uh executed it here or we just search can we search no we cannot search so you know I don't remember but I thought we um I thought we had created that images folder but let's go ahead and make a new folder so enter objects here uh we're not getting the the new folder oh it's right here sorry I was looking at the wrong place so we can go here and call it images and go down below and create the folder and let's just move one of those files in there so I'm just going to take the I'll take the two text files or no I'll just take the image because that makes the most sense and we'll say action copy and we'll move that into our destination so here choose that destination and copy the reason I I want to do this is I want to see if um the uh list objects will list the prefix as well it should but just in case it doesn't let's find out so to do a S3 list objects and we'll type in what are we going to type in uh oh the bucket right so we'll type in bucket and this will be my bucket example ab2 we'll hit enter and we did something wrong here oh it's S3 API I will hit enter the specified bucket does not exist so I must have type the bucket name wrong it's my example bucket not my my bucket example we'll go ahead and type that in great and let's clean that up so we'll type in query and we'll say contents Square braces. key enter okay and so notice that we are getting the prefix it also shows the prefix folders so this is not necessarily an object notice we said list objects but we're getting um this one in here so you know you might need to filter that out uh there could be a a flag to get rid of uh folders let's go find out if that's the case go back here list objects there's also a version two so at some point they made improvements to it so sometimes you want to use version two sometimes you want to use the original one um they look very similar this one has a little bit more text um in terms of what I'm looking at they kind of look the same so I I can't really tell the difference but you know when you start digging into it then that's when it kind of matters I'm looking at the flags to see if there's any particular difference nothing that's jumping out at me I think this one actually has more Flags maybe some of these CI options I'm not sure nope they're there as well so you know this one has a start after flag that's probably one of the big differences here to start listing from Amazon S3 starts listing after the specified key that could be useful if you have a very very large bucket so not really seeing much of a difference there but um what I wanted to see here in the list objects was is there a way to exclude the prefixes and I don't think so but we can set a Eliminator which is kind of nice okay so anyway um that's that so I would say I'm pretty satisfied with what we've done with the a ss3 in terms of uh the CLI uh I think next what we should do is um open up some code and make some back scripts so we have some reusability look at some SDK code look at some infrastructure uh infrastructure uh uh as code code I and then I feel like we've uh really rounded out our skill set here for basic operations for ads3 okay so I'll see you in the next one ciao hey everyone it's Andrew Brown and we are continuing on learning about S3 uh programmatically and so we had ran uh CLI commands using the S3 and S3 API I don't think I deleted the bucket I mean keeping that bucket around is not going to cause you any issues but before I get started here I'm just going to go ahead and delete that empty bucket you can do the same as well as we're going to create ourselves a new bucket so I'm just going to delete this one notice it wants the bucket empty I'm going to go click empty bucket and we'll go ahead and permit delete um so you know if there's something very serious I would go back and edit a video make sure that uh you don't end up with any spend but an empty bucket or a bucket with very few files is not going to cause you much grief so um you know we use the uh Cloud shell to uh run commands but now what I want to do is start using our code base and so in another video I show how to set this up of course you can use whatever you like but um I only going to provide instructions for what I'm using because that's easy for me and I'm going to go ahead and just close up some of these other tabs here and bring this tab in with our other ones come on Chrome there we go and we'll go ahead and open this up in git pod this is going to um install the a CLI and it already has keys loaded in here so again refers to that video um it's not that hard to uh get set up here and if you can see there are no folders here uh different videos will have more folders some will have less it just depends on what order I'm doing them in so just pretend that there's a bunch of folders here already uh and we'll create them together when we need to so we are working on S3 so I'm going to make a new folder here and we're going to call this S3 and I want to uh get some practice here with you so we're going to go and make a a new folder in here called um I guess bash scripts we'll say bash scripts okay and I think it's really important to learn how to write bash scripts so what we'll do is we'll create some so we'll make one for uh creating a bucket so we'll say create bucket we will um uh destroy a bucket or remove a bucket I'm going to try to match them based on what the actual commands are so this is um destroy bucket create bucket put bucket is it delete bucket it's probably delete bucket yeah it's delete bucket so we'll go here we'll make a new folder here we'll say delete bucket oh that's not a folder I want to make a file I'm being silly here today we'll go ahead and delete this permanently we'll make a new file here say delete bucket and I will make another one here and we'll say list list objects and we'll say put object and then we can put a sync in here so we'll just do a sync I think that's a file that's a file Y and so you might be asking Andrew why are you not putting extensions on any of these files well um when I create bash Scripts uh I don't put the dosh on the end of it because I want the script to be portable and so it's just a a convention where instead of giving a concrete extension you're going to add in um here a shebang and a shebang is going to say what kind of file that uh this file is and it's going to be interpreted by the um bash terminal and that's the way I like to do it so what we need to do is specify um uh the shebang up here so it's something like this I always forget the order let's go look up which way it is shebang why it's called shebang I don't remember I remember a boot camper told me why once and I don't remember why anymore but uh even though it says Shang it's pronounced shabang that's how we all say it but some so there are obviously some alternating names so it's the uh pound and then the bang okay so we'll go over here oh that's the order that I have it in and so there's a few ways that we can specify this I always kind of forget which way to do it but the idea is that we want to say the address of it so they'd be like user user local bin uh like bash or something like that so what we can do is go down below here and just say where is Bash and there it is there is a difference between bash and S um I believe we want to use sh as um one is more complex than the other so before we actually proceed let's just go ask Chach PT what is the difference between bash and Sh um so Born Again shell yeah so that's the one we want to use because Born Again shell is has more advanced features so um how should I specify the shebang for using bash because there's two ways you can do it there's this way um and then there's another way so is there another way to specify bash in the shebang maybe something that is more portable you know chat gbt doesn't know how to think it just can filter so there are two ways of doing it there's this way which is is is pointing specifically to The Bash file now you have to check to make sure that is where it is so if I type that in this is a user bin bash so this one here right away would have not worked um and then there's this one here if you want to make a script more portable and avoid hardcoding path use this because this is just what is based on common convention for where batch is expected to be and that's almost what I did I came pretty close and we'll go ahead and whoops put that in here like this great and so now we have our bash um so I want this for all of these files we'll go ahead and paste that in save save save save that looks good to me there good and uh another thing you need to know is that these files are not executable we type in lsy La uh notice that oh well first we have to actually do that in the folder so we'll type in S3 here and so notice that um uh we have drwx and so we have rwx so read write and execute and there are three groups um and so we want the user to be able to execute this stuff so what I'm going to do I'm Type in chamod which change change modify it's for changing um permission levels within uh Linux files I'll do U for user plus X to make it executable and I want to apply it to everything in that S3 folder so I'm going to do uh this here s3b scripts Aster to apply to all of them and now we'll try this again we'll hit up on the um bash scripts okay and you know what I I guess I I I was uh not paying attention but here did not listen any the files out but notice here that um these are now these are now all executable it was like this before I'll show you what it was like if we do minus it'll remove it so we'll just type in clear here and I'll list them out again so notice here it says read write and there is no X here and if I hit up with the plus X they're now all executable okay and that's important because if they're not executable we're not going to be able to use them it'll just complain and say it doesn't work but let's go ahead and write some bash scripts so the first is our crate bash uh or crate bucket so let's go ahead and get some practice here so we have eight of us S3 uh API and we'll say create bucket and we'll type in bucket and we'll give it a name so here's another thing is like we might want to supply our own um bucket name as an environment variable Let's go ask TBT um how do we pass in and and or check for the presence of a um input variable for a bash script okay it's going to be like dollar sign one and two so what we want is this kind of thing here there's a few different ways of of doing it but here it's just going to check if there's at least one argument so we'll go ahead and copy that uh if you need to pause pause and go copy and paste that in here okay so we'll go ahead and paste this in and so the idea is that we want to say uh no bucket name provided okay and so then here this would be dollar dollar sign zero I thought it started at one I guess not oh here is let check if at least there is one argument well that I don't wanted to do that I saying check if there is an argument not if there is any arguments again chat GPT is is not the smartest all these tools are not that smart you have to know what you're doing so this is actually what we wanted this makes a lot more sense okay so um I I've seen this so many times hyphen Z I don't even know what it means but I just know that's what it uses to check for it so um there needs to be a bucket name so EG slucket my bucket name so that will be our first argument now that we have that down we can make um check for bucket name now we can specify no I guess that's it I was thinking like maybe we need to specify an object but we actually don't need that so this is going to be dollar sign one I knew it didn't start at zero that didn't make any sense and so um you know this should create ourselves a bucket so what we'll do um is we'll just type in clear here and now we'll do period so that is a way of executing the bash script we'll say forward slash that's for executing any kind of executable script not just bash scripts if they have the shebang that it will work we'll do S3 bash scripts and we'll say create bucket we'll hit enter there needs to be a bucket name so I'll just say um my new shiny bucket AB it says the unspecified location constraint is incompatible for the region specified endpoint this was sent to um okay I've never seen that one before before but um so illegal location constraint H well let's saype in ads SDS uh get caller identity I just want to see where I am right now or not where I am but like am I actually connected to anything I am good so there's something he doesn't like oh you know what maybe we should we should assign it to an environment variable that's usually what we normally do do um I'm not sure if that's our actual problem but we'll just say bucket name here we'll say dollar sign one and then we'll just say dollar sign bucket name here okay you think I know better but uh you know sometimes the things get the best of us we're still having an issue here says an an a recurred a legal location constraint examp exception um you know again I'm not sure what it is maybe the name is too long um let's go ahead and just Echo out this command so that will print it out I'm just going to put it in um parentheses there and so we can see what command it's running before it does it interesting so let's go ahead and uh copy this command here and paste it in below and hit enter what if I change the name okay what if I specify the region CA Central one all right well let's go take a look at what this means haven't seen this one before uh what does this error mean you know I think a location it's either the region or it's the name it doesn't like when you trying to create a bucket in a specific region but you're not specifying the region correctly so probably the the issue could be um my default setting for my um region so which is unusual because I I wrote it in here manually so that shouldn't be an issue okay well I mean I'm pretty sure we made a bucket in CA Central before at least I did so I'm going try us e Central one okay that worked so why can't I make that in uh CA Central One Al I'm typing it wrong but we'll go ahead here into uh into here I'm just going to delete that bucket out it's very important to work with ads to find out where you run into issues because this is the the hard part it's not memorizing all this stuff it's working through these issues so what the heck um the region looks right to me let's try this again it it could been I thought maybe because the name was too long okay why is this error only happening for CA Central 1 but is not an issue in Us East one okay so let me just give it a read all right so this is not really telling us much um it was just saying like the default reg is the default region so let's go take a look at this out to the internet and see what information we get it has to be unique in the sense that no other bucket can exist with the same name I mean is that the only reason why maybe it already exists but the thing is these are globally scoped so it shouldn't be really an issue here uh we'll just type in 1 2 3 four the unspecified location constraint is incompatible for the region specific endpoint this was requested to okay well um I guess I'm just not making things in CA Central right now or I'll have to just pay attention to that but um you know sometimes things happen and you just don't know why like why would that happen for C Central one we'll just go here and try this again go here uh regions outside of usc1 require appropriate location constraint to be specified in order to create the buck in the desired region why I've never seen that before okay um I mean like I think that if we were using the other command uh like we go back up to um here and we were to we were to write this command for you know the maybe we can do like itus S3 make make bucket I I feel like we wouldn't have to specify that but since we're doing it low level apparently we need it so we'll go back over here and we'll look up that location constraint here it is by default the bucket is created in USC you can opt to specify region in the request SP can to constrain the bucket creation to a specific region you can use locate location constraint okay so I guess that's the way we have to set it and then this is a specifying location constraint so what we'll do is we'll go I guess go ahead and provide that so we'll go here and very interesting by the way like always learning something new so we'll put that in here and well I'll take we don't need uh we don't need the echo here anymore that was just temporarily helping us out we'll go here and bring this down now I like to bring these onto separate lines it makes our lives a lot easier so these backlashes allows it to execute a multi line here now this is um supposed to be adjacent object as you saw there so we could do curries here but it usually has like a short form that you can use in line so we can literally just yeah says shorthand syntax so we can literally use that shorthand syntax in place so we'll go ahead and just copy this will'll just save us time uh with typos and I'll put this here I would recommend putting these in double quotations always always always to make your life easier otherwise it might throw an issue I'm going to say ca Central one so I thought you specify the region here but I guess you have to specify it this way and so what we'll do is give this another try oops another thing I like to do is when we have uh uh S3 commands I'll copy uh the location of it so that when I need to go read about it later I can easily find that documentation so go ahead and paste that in here like this there we go great great great great and so now we can then go run this command again we'll type in clear and I'm just going to hit up till I can find this command and uh I think we'll have to just put some numbers there and it has a problem because there is a mistake notice that it's prompting here maybe because the bucket name is empty so I'm going to stop that there for a moment double click this this matches that so that shouldn't be an issue so I'm just carefully looking to see if there's any mistakes it says line 13 command not found bucket oh it might not be called bucket it us S3 API thought it was though bucket it uh create bucket no it is so for some reason it's um not taking kind to our backslash here so I'm just going to go and put this up a line here and we'll type in clear we'll try this again there we go and it worked why it didn't work on a new line there I don't know maybe I had like a space or something weird there um but I want to go get go ahead and get rid of these buckets here that we creating so just going to type the word shiny in here and we only have the one bucket here so we'll go ahead and delete it out we'll delete that out great great great great great okay so we'll type in clear and um I want to run it again just make sure that it works no problem so there probably was like a space or something that was messing with it and we have the location of our new bucket um I'm going to just clean that up and we're going to say query and we'll say location I'm not sure exactly how we would query just that can we can we just do that let's see what happens there clear and again I'll go delete the bucket as we'll just finetune this a little bit delete I like an Azure because they'll give you like a copy paste to do the quick delete which is nice ads will probably update their um interface at some point but uh for now it's it's okay I want to go ahead and try this again I'm trying to get back there we go so that's looking better notice that there's double quotations around it that's because it's Json so we'll go here and say output and say text and we'll do this one more time okay we'll go ahead here and say delete and copy this we'll paste this in good good good good we'll go back over here and we'll type in clear so try this one more time and now we get a nice output here so there we go that's that's pretty nice um the next thing I want to do is I want to delete a bucket yeah we'll delete a bucket so that will be our next one we can pretty much copy and paste this stuff copy this and paste one thing I'd like to do is probably have an echo here just to make sure we know what we're doing so up at the top here we'll just say Echo I'll just do this and say create bucket and then we'll bring that over into our delete bucket here all right so we'll go down here and I believe it's just delete bucket we don't need that um I don't know what we get back as output I think we could can we do dry runs on these let's go take a look here quickly I think that commands for everything I don't usually run it so I'm not 100% sure um but now I just kind of have interest in doing that so no dry run does not show up here though I thought it was a global flag let's try it anyway I again I I'm not sure but I think it's a um it's a global flag so it should work with anyone even if it's not in the documentation here and we'll just change this and we'll say delete bucket I I already forgot what the bucket name is here so we'll go back and give this a refresh that's the bucket name right here good and I'm going to go ahead and paste that in and no it does not like the dry run command so I thought that that would be there but I suppose it's not we'll go ahead and type in clear and we'll try this again delete bucket and now we get back and none so it's not showing well there's no location to query so let's just take that out there and we'll take this out as well and we'll just go ahead and create our bucket again uh crate bucket very good so that is our our bucket and then we'll go ahead and go ahead and try to delete this and we get no output back so I thought maybe we' get the name back but I guess not so that's totally okay I suppose um so that's delete bu it let's go ahead and list some objects we're going to do something very similar here of course we'll say list objects we might as well use the version two since it's newer sometimes newer doesn't necessarily mean better it's just that there's an alternate alternate one and uh you have to make the best decision for that for this one I can't really see uh really any difference we have our our bucket name that we're going to want to get here um let's go ahead and well before we do that we should probably create some resources so this is okay we'll come back to this this one looks pretty much done but uh what we'll do is we'll go ahead and create our bucket again so we'll go back up here create that bucket notice I keep using the same name I'm not hitting any conflicts and let's put some objects so I'm going to go here and I just want to create a bunch of files so I'm just trying to think about this for a moment um let's go Asic BT create um between 5 to 10 random files using uh bash and so we could just use that touch command but I want it to randomize I don't want have to figure that out um sometimes the bash syntax uh varies based on what you're using so here there's the random output file I don't really want to um create these anywhere in particular I would probably have the output directory in uh temp so all Linux systems will have a temp directory uh so we just say uh temp like that and that way they'll get cleaned up afterwards even if you're on uh uh regular Linux and not in the cloud developer environment ensure the output directory is created so that makes sense so we'll just say where will'll store these files let's create the output folder and I'll just go here and just say like S3 um bash scripts I'll say mkd uh p and then uh I'm just going to keep these uppercase it's a little bit easier to read when it's like that sometimes you'll see me alternate from it but it should really be that and P means it will create every folder it has to so if we made more folders here we create them all so p is always a great FL when you're doing M mkdr for making a directory generate a random number of files so we'll go over here and we'll bring this down I'm going to change this to num files random generate a random number to determine how many files to create do that we'll copy this here this is using a for Loop for Loop is a basic programming perimeter perimeter that you absolutely should know um I mean it this one's a little bit different than what I'm used to looking at but I understand how it works the first one is the uh value it starts at one it's going to count until the number files is greater than or equal to that number and then it will stop so say numb files here and then it'll add it each time so here it's going to generate a random file so I'm just say output out put Di and then um there's that one file and you know some people might consider these uppercase one as constants and these ones are not but I'm just going to put them all in uppercase so my life is a bit easier I'm just to say file name here file name I don't need to see all the files being created I don't care about that I actually don't know what this DD command is um let's see if it actually is here we'll say man DD converts and copies a file sounds good to me just make sure that your Linux Has It by typing man DD if you don't you can go ahead and install it or you'll have to adjust the script based on what you're using or just use get pod like me and you'll have no issues so here it's going to looks like it's going to um create a random it's going to create a file looks like it's going to put some contents in there it looks like it's going to randomize the contents and it's going to Output it to uh this directory here with the file name okay so I think that's going to work correctly and so that means that we expect to see these uh files there I want to um see the contents of those so there's a command called tree I think it might be installed here is it nope um can we install it uh pseudo apt get install tree surpris that doesn't come in preinstalled tree there we go so uh you might want to go ahead and install that I'm going to add that to my G pod yaml file I say utils here say before and I'll paste that in here whoops that is not what I wanted we'll go back and hit up I wanted this um so I'll try this again so let's just say name uh before I'm going to say pseudo apt get install tree and that's just going to give us that tree view which is really nice to have I say name will be here will just be utils so just random utilities okay and I'll just type uh clear here okay so once that's created I'm going to type in tree here before anything else I probably want to remove the cont because every time we run this we'll probably want to empty the uh the folders or the files in here so probably what I'll want to do is first remove everything in that directory so I'll say um well let's see what happens if we run it twice that's that's what I'm curious about so we'll go here first and say output dir okay and let's go ahead and run this file and see what happens so we go S3 bash scripts put object and it's up putting the file so that is working out pretty well I like that um so that is good what happens if we run it again okay so it's not creating new folders would mkd delete I don't think it does does mkd p delete a directory when it when it runs multiple times I don't think it does no it wouldn't it wouldn't so my only thought here is that um this is probably aing out it's not telling us that so there's a command we can run like set zero um what is the set command to stop all commands if there is a failure in a bash script it's like set hyphen there we go so that's a that's a thing that we'll commonly do so I'm going to just paste this in the top I mean it's not saying that it's failing but I I can see that oh we are getting more files let hit this again I think the thing is that uh when it does this it's going to write them over back in place so the thing is that when we run this multiple times if that file already exists it's not going to uh wipe them out it's just going to leave that there so I think we should really clear out that directory every time we run this so we get something different and so I'm just going to say uh remove hyphen R hyphen hyphen f so well I don't think we need the F but we need the r for recursive f is when you need to get rid of a folder uh which technically we need to to do so actually I'm going to do that I'll go up here and I'm going to remove the folder if it already exists sorry and we'll just do output di here got to be very careful with these remove commands and these um this stuff here so we'll go ahead and I'm going to hit enter and so now it should notice how we're getting different numbers of files because it's clearing out the folder each time so that's really good all right so now we have our files and what I want to do is I want to copy those files so there's two ways we can do it we can use that sync command or we can use the put object command uh the put object command obviously gives us a lot more options but in practicality I don't think it's very fun to use but we do have um that sync over here so maybe this script should actually really be in the sync because sync is for multiple files so that's what I'm going to do actually I'm going to go ahead and I'm going to cut this script and I'm going to put this actually this will be our sync script instead and just make sure you don't make a mistake like me up here and we will start this off with an echo okay we'll say sync sorry for doing this out of order I know I shouldn't be uh going all over the place here but anyway so now that we have that we can just go ahead and say MK or sorry um ads S3 sync and then we'll specify these files which is in the output Di and then we'll do dollar sign sorry not dollar sign what is it it's not dollar sign oh S3 Cole SL slash and then here I'm going to say uh the bucket name okay so we'll go back over to create here and I'm just going to copy this stuff here and I'm actually going to let it have a second parameter and we're going to say check for bucket name check for file name and or file name Andor prefix with file name okay and so what we can do here well no I'll just say file name check for file name prefix not the bucket bucket prefix which is something else and so here I can just go Um well this is one this is fine one I'm just going to bring this down here below like this and then the next input will be uh two and so we'll just say uh file name prefix and we'll say two here and then we'll scroll all the way down the bottom here and so instead of file we can just say here um file name prefix that's what we called it yes it is okay and then we'll go all the way down to the bottom so now we have the bucket name and here we could specify we just say files I'm going to put files here and so I'm hoping that it makes a prefix and puts them in the files CU last time I I didn't see it go in there and I was second guessing uh ourselves here but let's go ahead and type in clear and obviously we'll have to come back to this put put object script because we kind of skipped over it it but we'll go here and type in bash S3 bash scripts sync hit enter there needs to be a bucket name of course there has to be what is the name of our bucket I forgot maybe it' be useful to um list our buckets so we can see the names list buckets and yes I'm doing things out of order but you know this is the way you do work when you need something you write it and you uh deviate so here we're just going to say us S3 and we're going to use S3 API CU I believe we get more information that way list buckets uh well actually no you know we can just do Aus S3 LS because that's going to give us a flat list and that's a lot nicer I think it was S3 LS so um we'll just try this here we'll say S3 bass scripts uh list buckets or List buckets that's a new file it's not executable so if I just go ahead and type in lsy LA and we go and look at that so we'll say S3 bash scripts um here notice that this one does not have an x on it so that is something we're going to need to do so we'll say chamod U plus X so make the user executable the user group the group called user S3 Bas scripts list buckets we'll type in clear I'm going to go ahead and try this again list buckets and it's listing buckets I don't really want these time stamps here I mean the time stamps are good if it's the latest bucket but this isn't ordering based on that I wonder if we can get it to order uh in a different way um I'm not sure if we can but we'll get ads S3 or we'll sorry we'll go up here and we'll say ads S3 or sorry S3 here we'll try the simpler one first maybe it'll be a little bit e easier to work with um we'll go down below and so we have LS and I'm I'm looking for the flags I want to see if there's a way we can order it Query profile region version summarize page view I don't think so I mean like the thing is is that um this is going to return probably like a 100 well you can only have 100 buckets but I'm not sure what the pay AG sizes by default maybe it is 100 so you can get all the buckets in one go let's see the default value is a thousand okay uh maybe you can have more than a thousand buckets these days I thought you could only have a 100 but um I guess if it's listing objects sorry that's not very clear um so maybe what we should do instead is actually use the uh the other command the itus S3 L uh S3 API list list buckets and I want it to go into the auto prompt here sorry list buckets let's see what options we have not a whole lot um I wonder if we can sort using JSM path see if that's possible here we have filter how do we sort okay so what I want to do is get back the uh Json results we could probably use JQ to do it as well so we'll just go ask chbt um can you sort the list buckets result so the the latest created buckets are on top and so the question is can it do this naturally yeah it's going to use JQ so that's what I thought JQ is a um a library it's uh that allows you to sort Json and so that's going to be the recommended way to do that uh most things should have it installed let's go take a look if that's installed here so we type in JQ and it is if you don't have it installed you might have to do like a pseudo app get install JQ JQ is definitely a very important tool for us to know how to use there's another tool called yq which works on yaml files but it's similar to JSM path where it can uh filter information but a lot of times if you get chat TPT or some tool to generate out for you it's a lot easier I'm going to go ahead and just copy this and take a look here so you know the data we're getting back here if we just scroll up is well it's not showing it anymore but if we just go ahead and type in it S3 list buckets we are going to get back uh creation dates right but that's not how this is sorting this looks like it's maybe alphabetical which is not what I want um I realized I've been working with really small fonts here I guess I'll bump it up I know I'm notoriously bad for that but uh sure what to tell you there so let's go bring in that JQ command and so we have list buckets and it's going the pipe is going to pass the Json to JQ and then we have hyphen R I don't know what it does but uh we're going to select the buckets because the output um if we go back over to here and look at it again you will see that it has buckets so we're going to say okay period select the buckets we using before when we're using query it had a different syntax language and so this one is similar but different so we say select the buckets and then sort by the creation date and then it'll probably order them uh oldest to new or sorry newest to oldest or oldest to newest and then it reverses it because we want actually new ones first then select the entire array and then give us back the name so you know you don't need to know how to write JQ by hand but you should be able to NE like be able to to make sense of it and read through it as I'm doing it here let's go ahead and see if this can uh do that for us we'll go ahead and hit enter and paste it in here down below what do we get and yeah our new buckets on the top now I don't want to see all of the results so maybe we can uh limit it now we can uh limit the results probably using the um CLI but we probably don't want to do that because it's going to uh muck things up and I'll explain here in just a moment when we pull up list buckets so we go to list buckets here we have a probably like a limit option usually there is one nope there's not but if there was it would be a bad idea so like in query you can do some things there but any my point is is that when you do a filter filter happens before it leaves the server and if it's going to be ordered in this creation date or sorry in alphabetical and we said only give us 10 it would give us 10 from the alphabetical list whereas we want 10 from the newest created list and so we want to filter them here in JQ so um only return uh the top five and if there are more then follow up with three ellip with a string of ellipses okay let's see if we can figure that one out and so here we can specify a range so 0 to 5 so that makes sense here let's go ahead and modify that 0 to 5 that's fine and this command can can Cate the bucket's name with a new line so they appear one per line the output don't know why I need that um don't think I need that but what we'll do is we'll go ahead it didn't exactly do what I was asking for but that's totally fine so run this command again sorry we'll go ahead and copy this and we'll clear this out we'll go ahead paste the S we'll hit enter we get null so maybe the 0.5 has some kind of weird effect on it and so we do need this part here let's go double check here did I use a period or a colon oh I used a period That's not going to make any sense okay so we'll go here and just change this to colon there we go and uh I'll try this here locally and here it says cannot index an array with the name String okay did we miss something oh we did okay so we have reverse and then we have our usual thing so we we have to select it and then turn it back in an array or something I don't know so let's just try this here there we go and so now we're getting our results um I do want to indicate that there could be more so I'm just going to put an echo after this and uh now we'll try to run this command ourselves and say S3 bash scripts okay and so just say list newest buckets so that's a little bit more indicative indicative of what it is and so this way we can um we can get the latest bucket now or or the or the most recent buckets now I might also just want to get a single bucket so we's just say get latest bucket here I'm going to copy this command here and we'll go to this one here latest we could also just say newest I feel like newest is a better word than latest newest maybe newest is not a word I don't know but uh it makes more sense in my brain here yeah newest bucket and the idea here is I just want one bucket so and I this command is not the command that I actually want here we'll go ahead and copy this and then we will go back over to here and I'm going to paste this in here and we'll go ahead and just say one how do we select just one I'll just say h how about just select the first bucket after it's sorted let's see if it uh do that for us here it's just zero okay that's that's a lot easier do we still have to do that weird piping no we don't so we'll go ahead and change this yeah JQ very very useful tool and so this will get us only that bucket the latest bucket we'll get rid of these ellipses and I'm GNA go ahead here and we'll try this so we'll have to chamod that file though it's a new file bash scripts get newest bucket and then we'll try to run it here so we'll say bash scripts bash scripts or sorry S3 I'm trying to autocomplete the wrong thing uh get newest bucket and I just wanted to list that out so I'll just take that out there okay and so you know my thought here is that maybe I can then uh pipe this over to the other one that's what I was trying to do with this one but that's fine we'll just leave it alone I don't need to get too complex here let's go back over to our uh our list of objects that's what or sorry our sync that's what we were trying to do right so we'll go ahead and we'll type in uh S3 bash scripts sync and then we can bring in this one here and we'll paste it in if it lets me copy paste here anytime okay and so it wants a also a prefix so we'll just say my file and we'll hit enter notice that it's not printing out that so clearly it's not uh creating the address correctly it did upload the files which is great but but um that's not what I wanted right I wanted it to show that uh that name there so we'll go back to our sync and we'll go back over here and I'm looking for here here's the issue so see how we have prefix and then we have a PR underscore it's it can't handle that so maybe if we do this will that fix it um H I'm not sure but let's try it let's see if that works so I'm going to hit up again my file two and it does not like that so I'm going to go out of here and let's just ask chat TBT if it knows the answer here so go here there there is supposed to be a uh underscore it's not file name prefix I how can I fix this in bash let's see what it suggests oh pretty close so I thought it was parentheses it's it's cures okay so very close to it can't remember it all don't need to remember it all just figure it out okay so let's try this instead we'll hit up and so now we're getting the files and it's uploading those files so this is looking in good shape so that is more of what we're looking for now I want to um put a single object in here so we'll go to the top and we'll copy uh some of our code here and we will try um put object yeah that's what we want this will need the bucket we'll copy these two here and we'll just say check for file name this is not actually correct this should say uh uh there should be a uh file name prefix this would be uh my file name prefix here like that okay great and so we'll say there needs to be a file name file name and so we'll go down here we'll say ads S3 API uh put object it's going to want the body as we're starting to kind of remember what all these things are we it needs a key it needs a bucket um so this will be the bucket name we'll go ahead and grab these two here paste that in uh this will be file name we'll have to give it an absolute path because that's going to be the only way we can uh Prov that there uh we can also say the key name if we want um no we could just make it the same as the file I think that if we don't provide a key it should probably automatically use the same name so I'm going just take that out I'm going to go here and just say file name and so I think this should work yeah I think that's fine I'm going to go ahead and type in clear here we're going to need a new object so I'm just going to make one temporarily so I'll just say uh touch sltm new file.txt and so let's go ahead and make sure this command works so we'll go ahead and typee in S3 bash scripts uh put object and we will place the bucket name already forgot what the bucket name is we'll go ahead and we'll type in get newest bucket B scripts get newest bucket great and then we'll go here copy that it will say S3 Bas scripts put object and we will paste that in here and then we'll give it a space and we'll say sltm new file.txt otherwise it's not going to know what to do but here's the challenge though is that if that's the name it needs to know what the key is and the key can't be the same so I'll just have to make a separate key here unless there's a way we can extract out the name so um uh in bash take the absolute path and just get the file name and extension and so we'll have to use some stuff here so we have well we don't need to get the absolute path we already have it so I think what we can do is just use this as such I'm going to paste this in here and so this will be the key uh object key we'll just say file name and then we'll go down here and specify the key and we'll say file name or sorry object key okay so that's very clear and we'll just type in clear oh I shouldn't have done that cuz I got to I got to write it all out again that's not fun so we'll go ahead and we'll type in Period S3 back scripts um so we put object we want to provide uh this name here and then we'll give it a space and we don't need to provide the key because we can extract it from the file name here we're going to go ahead and do something we're going to uh provide the the name of the file so it'll be temp uh temp what temp something uh what do we call it new file there it is okay great hit enter and it's complaining so it doesn't like something we had this before notice that it's highlighting weird here I'm not really sure why it's doing that is it the trailing space Oh it's a trailing space that messes it up okay I did not know that great we'll try that again we'll hit enter and so now it looks like it's created it it gave us back an e tag which is great um we'll go back over here here and we will go and take a look at our buckets here and so there's our new file so we're in really good shape the only thing that I want to do here is go ahead and um before we delete the bucket we need to delete the objects I'm going to go here and make a new thing saying delete objects and we'll go to the top we'll copy this paste this in we will grab the bucket name here we'll paste this in here we will go down to I guess we're at the bottom now we just say ads S3 API delete objects and we'll just say bucket and we'll say bucket name I think that's all we need so we'll be delete objects we'll go ahead and try this out we have to chmod first of course so we'll just say S3 B scripts delete objects good and I'm going to go ahead and delete them by running this actual script here so we'll go here and say bash scripts delete object and then we'll provide the name of the actual thing here paste it in we'll hit enter and it's completing contain for the request yeah okay let's go read up on that I thought uh we already had done this once but let's go take a look here unless it was um no it wasn't it wasn't in S3 it was S3 API so S3 API delete objects the bucket name contain the object delete the container for the request okay maybe we didn't do this before but the um object identifier is the unique identifier objects so it looks like I don't understand what does it want why does it want that we'll go down below to examples okay so the following deletes the following object so it looks like we actually have to Prov a um a Json file but I want to just delete all the contents there so I guess that's not going to really work how we wanted it to work so in this case what we could do is we could get the list of objects and then iterate over them and then delete them so that might be a uh a better way of of working here so let's go take a look here we have list objects and we'll go into our uh Delete objects here and we can run a command within a command so I'm going to try this I'm going to do well we don't need to grab it from this one I'm going to go ahead and and just grab this command it's so simple we'll go ahead and grab that like that and I'm going just put this I'm going to wrap this in parenthesis and this is going to allow us to get the output and so here I'm just trying to think here how could we pipe this over because I want to get back the values and then I want to iterate over it so let's ask TBT um get a list objects from S3 then iterate over them and delete the object and let's see if we can do that okay so let's see what it is producing for us we have list object so it's going to dump it to a file I suppose that's something we could do in fact I wonder if we could even just use the object list after that let's go take a look um let's just try that for a second so I'm going to just type in clear here we'll type in ads S3 S3 API well which one was it using was it using the standard one oh no it's using list objects okay S3 API list objects uh bucket I keep forgetting that bucket name because it's not a normal bucket name it's this one right here we'll go ahead and type in uh make sure paste that in there so say S3 bash scripts or no we'll just do it regularly ads S3 API list objects like this bucket we'll hit enter and so we're getting back contents right but the um the and we could do it this way here it's iterating over the object and deleting them individually but if it wants back a Json object we can probably just transform one into the the shape it wants say objects keys so instead of this we'll say um list op so we'll say usings S3 list objects uh get the Json results back and transform it for the delete option for the delete object objects um input and see if it will do that for us we're just going to let that generate out there so here it will output it okay but um we need in a particular format right so here we'll do that and then we'll get the list of it um I'm not sure if we have to um really have it outputed as a file what we could do is take this right and go down here and we could just pipe it so contents. key the important part is it comes back is Json right so um you know of course we can um have this just return Json I suppose we could just do that as well let's just see here if we do this I know it's suggesting you use JQ but if we don't need to use it why are we using it um and so we'll just try this and we'll say key and so I mean that that's fine right there I like that um and then we want to pipe it over to JQ to create an new JQ because that's what it would do so we want to place the keys in there see it dumps it to um a text file but then this say dollar sign key down here why oh because it's reading the file and then it's iterating through it and it's creating a a request per key is that really the way it's supposed to work that seems very inefficient objects key so I'm just trying to think about this like how could we um uh create this query this is the the format that we want right and you know even though chat gbt is suggesting something it might work it's definitely not uh uh the most efficient so what I would do is I would go here and I would I mean we can get the result from here and we just say uh result no we'll just pipe it over so I'm going to just take this here and we'll pipe it as such so that that'll get us back an array and maybe what we can do take a Json array and then using JQ insert the array JQ hold on here insert each array item as the key EG in an object as such I wonder if it's going to understand what I'm saying because that is a little bit messy what I'm trying to ask it to do I still think we might still have to iterate to get that uh object but sometimes if we ask in a different way it'll be a little bit more intelligent so here we have the keys which makes sense up above that that's great and then um yeah so that's kind of what we want exactly so why wouldn't have done that before I don't know um but anyway what we'll do is we'll copy this and here it's inputting the the Json so can we input from a pipe as opposed to adjacent file and it can get really finicky with JQ there so yeah we can use the hyphen end flag I guess and so this is what we're hoping for so we'll go ahead and copy this and I'm going to go ahead and paste it in here as such and I just want to single line this and so I mean this is fine so we'll go here I'm just going to uh break this up so it's a little bit easier to look at so we'll do that and then we'll do this and so what I'm hoping for is that it will um give us what we want here while this is doing JQ again we don't need to do it that way we can uh just use Query right here so that's what I'm going to do here I'll just uh paste that in here and then that way we can get rid of this line there's no reason to double pipe it I'm just going to again make this a bit easier to work with okay and yeah there's our there's our uh pipe so let's see if we get our result that we want just type clear down here whoops clear clear clear clear clear clear and uh yeah again I already lost the bucket name we'll go ahead and copy that just to make it easier I'm just export that name we'll just say bucket here for now and that way my life will be not so miserable and so we can just say it us or sorry we we'll go execute this now so we'll say um adabs S3 bash scripts delete objects and we'll pass in the bucket here command not found line 15 um oh all this stuff here is is is not supposed to be there we'll go ahead and try this again no such bucket uh it's because we need to have a dollar sign here sorry and so now we're getting back the object so that that is looking pretty good there um it's showing all the possible files so that looks good I'm not sure what's going to happen if it tries to delete a file that's not there but um that's fine so that looks pretty good to me so now that we have that we can then uh pass this on to the delete objects so we'll go here and say delete so the question is how would we go about doing that um we could probably just say uh you know delete Json file or Json here and I'm going to just try this I don't know if it'll work but the idea is that uh this will set an environment variable with the results of that Json file and maybe we can just say delete Json like this and that will work so that's what I'm hoping for so what we'll do is go ahead and do this fingers crossed unknown options so there's something it doesn't like about our uh file here well that's the name right key let's go back and check one more time here so um I mean that looks fine is that what it wants as the output let's go back over here to the delete that looks like the structure and it says unknown options it could just be the way that we're passing it um noticing that it's look it's breaking them all up and it's doing something really weird sorry so okay so the question will be really how do we push it so um this is not working how can I pass the Json along to the delete object I guess the other thing is that we could just make our lives easier and just output this to a file there's really no reason that we have to make this super complicated um what we can do is just say um uh we can output this to Json let's go back here do this provide an example um can we output the Json or the JQ file to a specific file EG temp um delete Json I'm not sure sure if we can okay so it's similar yeah so that's what I thought so so we'll go ahead and try that and we'll say delete objects well maybe it this goes on the outside double check here yeah on the outside so we'll go here and say delete objects. Json and then this way we can actually inspect the file as well and we're not going to do this we'll just say temp delete objects Jason okay that looks really good here and we'll close that out we'll do clear and I'm going to go ahead and try this again we'll hit enter this is um okay what doesn't it like is it because it's a full path and it's not a file all right well let's cat out the file let's make sure the file was created first it's there so that file's fine um I don't think it's the file that's the problem it's it's probably just it wants to like it might want protocol in front of it like this sometimes it wants that so we'll do that it's triples because two and then one for absolute path again I'm just guessing I'm not 100% sure here fingers crossed and it looks like it deleted those files very good we'll go back over to here we'll refresh it's empty so that's perfect now we can do our final our fin final thing which is delete our bucket uh which we already did before but let's go ahead and just delete that so we'll just go ahead and say S3 Bas script and delete bucket now remember we just temporarily set this here the bucket um environment variable if you restart this environment it might be gone but there we go so now we're all cleaned up and we did a lot of impressive stuff with bass scripting is it important to do bash scripting absolutely yes you're going to be doing this on a daytoday basis so I know it seems like let's just learn ads and all this other stuff you got to know this stuff because this will not get covered by any other content instructor it's just me who likes to make sure you have this stuff because I really want you to get the role that you want whether it's Cloud engineer or devops engineer even if you're working in Tech and you've been doing it for years if you haven't been writing bass scripts this is your time to uh get that under under your uh your tool belt okay so I'm going to go ahead I'm just going to save this stuff so we'll just say um bash scripting for S3 there we go and that's it I'm going to stop my environment and we'll see you in the next one okay ciao hey everyone it's Andrew Brown and we are continuing on working with S3 still working with common operations we did Bash scripting we uh we just worked with the uh CLI in general and learned the differences between them did some click offs I want to do a bit of Powershell scripting similar to bash scripting uh we're not going to do a lot of it because it's not a huge Focus for ads um when you're using Azure it it comes up a lot but I just don't want you catch you off guard not have any Powershell experience so we're going to do a little bit here um I really don't like Powershell that much but uh beo loves it um the other Andrew who uh works here at exam Pro but we'll go ahead and open up a g pod and we'll figure it out so um of course it's a lot easier if you're using Azure because they have the uh the shell already built in here but there should be some way for us to get Powershell um I I imagine there has to be some way but uh let's just type in PS or Powershell let's go figure it out so um ads Powershell Linux I'm sure you can use it with Linux um is it already preinstalled on on git pod I'm not sure but uh the trick with po shell takes like forever to install so here we go um start a power shell core session by running the following let's see if we already have this and an enter that'd be really nice if it was already here no it is not to install databus tools on Linux um I look up get pod Powershell it seems like they have a a workspace for it already so I'm just going to take a look at what they're doing uh to utilize it I'm going to look in their G pod yaml file um nothing interesting there there they have a Docker file here BTU so I'm looking for Powershell pwsh so we'll just like install Powershell for Ubuntu or Debian it's very similar what we're using here and let's go figure it out for fun not that fun but let's see if we can do it again you know if you don't have that much interest in Powershell and and you just want to watch go ahead and watch here and just kind of observe because again this is just painful to be honest um but I I I just want you to get that experience okay or get that exposure to Powershell so I'm going to go through I'm just going to follow through the lines here and we're going to see if we can get any power shell to work we have a Debian package so that should work with Ubuntu because they use the same package manager so looks like we're adding uh the Microsoft package manager to in order to install that package and now we're registering the package it looks good we're going to go ahead and remove the package now that we're done with it we're going to go ahead and do another pseudo up get app up app get update and then we're going to go here and install Powershell and if that works then I'll make some instructions here so we can save ourselves some trouble good and now can we start Powershell great oh that's cool okay so what I'm going to do is I'm going to go to my G pod yamel file and I I don't really need Powershell but I'm just going to write it in here yeah I guess I'll install it every time why not who cares I might as well do it and we're going to go grab these here again if you're watching another video it might already be in here um I'm going to go grab this link so people know where it is okay should probably do that for the at the same time as I'm here install here we go we'll grab that so if folks are coming in late they're trying to figure this out but uh I'm just going to bring in these commands in here just copy all of this stuff for the most part and we'll paste this in and then we'll pair it down so I don't need to start Powershell that's totally fine just going to go and get this prepped for next time there we go beautiful power shell which you always dreamed of okay so um we now actually have uh the Powershell uh terminal open so if you're on Windows you probably have Powershell I always forget get if it's uh installed by default or if it's a special one but there's like a a special Powershell window and it's blue and um allows you to run Powershell commands and so we uh technically have Parell running now and as far as I remember Pell has command lits and so we have to install the Plugin or commandlet to work with AWS so we're going to go over to uh the AWS tools for Powershell and it says to install tools for Powershell you can choose from two packages we have the mod modular version of adus tools each adus service is supported by its own individual small module or net core the single large module version of adus tools adus services are supported by the single large module which should we use I don't know but um I think that uh we'll go with the first one because the modular sounds a bit better and maybe we can just install what we need so to run uh tools for Powershell core your computer must be running Powershell core 6.0 uh we on seven so we're in good shape and um install Theus tools using commandlets and AD us tools installer so that's something we want to do so we'll go ahead and install module so Powershell like has this um Convention of how everything's named it always starts with title case and and this particular kind of format we'll go ahead and copy this it's for like a more Powershell video we'll hit enter and so that's going to install it you're installing the modules from an unrusted repository yes to all I think it'll be okay we're trusting eight of us here and so now it's installed that was really fast when you install um azures tools it takes forever now you can well I guess the thing is we're not installing everything we're going to just install the base thing and then install what we want so it says you can now install the module for each service that you want to use um so that sounds good I mean I'm going to just grab this for later so I have to keep looking this up um this is obviously not going to work here and the other thing I'm not going to remember is the Powershell command which is this uh this one here and so I'll go here and I'll also just go ahead and grab this as well so I have to constantly look for it there we go we'll paste that in nice nice nice nice and um so we'll go back over here and so looks like we need to install very specific tool so here we just want S3 going to grab this here I don't want all of it but I want some of it and so I'll be very careful and we'll take out ec2 because we're not working with ec2 right now we'll head enter and we'll say a for all to install that and so now we'll have the uh um install adus tools module so now we can execute um Powershell uh commands and if we want to do some of the same operations somewhere there's documentation uh we probably want to do script execution uh command isn't available on non Windows systems you can uh run get execution policy which shows the default execution policy Etc um okay so that's great now can we get references for Parell here I'm zoomed in that's why it's been a big old mess here so let's go click into the tools and I'm looking for the documentation so we'll say Ad ofus tools Powershell documentation which is over here and we're looking for the reference guide that's what we're really looking for here I'm going to copy this and I'm bring this into our information here so I can find it later good I always tell people to document as much as they can I'm doing it very lightly here because you need to make your own notes but just understand that um so on the left hand side we have all of our services I'm going to look for S3 and in here we have S3 and we have all of our fun commands so maybe we can go ahead and try to create a bucket here create bucket so here it's new S3 bucket so notice that it's not following the same conventions in terms of what the apis are named and again this has to do with how uh Powershell conventions of naming things work and so that's why they're they're different it's called new S3 bucket so we'll click into here we should get some documentation uh one thing I really like about Powershell is it always autocompletes so we should get that experience when we're typing here uh new well it's not doing it here in the azure I would absolutely do that it'd be really nice S3 bucket why am I not getting nice Auto completion um we'll say uh Powershell auto complete I want to see that it's so nice I'm looking for it here to enable this feature in your power sessions it has to load every time you open Powershell uh this will open up the profile you can edit this what we're looking for auto compete power shell so maybe we can uh copy this in here and see if we can get that be really nice to have um autocomplete nope we're not getting it okay that's fine sorry I I did the best I could here type in clear here and we'll just start again here so we'll say new bucket it's really nice nice when it auto completes and uh again that's the experience that we normally get here auto complete we get any auto complete nope nope nope NOP nope nope okay so we'll go back over to here and so we have new S3 bucket I really don't want to do too much in power shell not as much as we were doing in in Bash that's for sure and so we need to have a bucket name okay um and this will be my power shell bucket AB I'll just put some numbers there I remember it wanted the um the the location last time not really seeing here we'll just let it create in Us East one and we'll see if that works we'll hit enter and we're just waiting to see what happens oh you know what I think we need to give it two flags NOP no just one actually sorry I thought I need double hyphen sometimes like when things go wrong I find power shell hang so I'm going to wait a little bit while and be back here in just a second okay all right it's back it took a long time but we got back our our Powershell and so it says that it created the bucket so that's really good uh let's go see if we can um uh put an object it'd be nice to put these in scripts let's see if we can make a a script just give me two seconds okay all right so I got a bit more familiar with power shell again again I don't write it all the time so Baker knows it off the top of his head but anyway I'm going to make a new folder here we're going to say folder I'll just say uh Powershell scripts and apparently the way there these work is um this will just be like S3 and we'll say PS1 that is the name for a Powershell script why I don't know notice that we have this nice little highlighting thing here I wouldn't be surprised if there is some kind of poers shell extens for vs code as um I imagine that this is something that's really useful if we go over here develop power shell modules commands and stuff like that the extension provides a rich power shell language support syntax highlighting Advanced code Snippets intellisense power shell that sounds really good to me let's go ahead and install that make our life a little bit easier I would say down below here we'll type exit and uh now that this extension is installed we can go over here to Powershell and so the thing that we're going to want to do is install our um module that'll probably be the first thing we'll want to do so unlike bash we uh I mean we can kind of change scripts together but um Parell kind of has more of like a programming thing going on here where you can just say start saying what you want so we'll say it s tools S3 and um we're going to need to set an environment variable so we'll just specify what region we want this to be in I'm going to just use East one to make our life a lot easier and we're going to need a bucket name now we want to input this Bucket from uh the user uh I can't remember how to do that just give me two seconds yeah so there's a special commandlet called read host I think it's built into Powershell so we can just start using it and that will allow us to enter in a prompt um so we'll just say prompt and we'll say uh enter uh the S3 bucket name okay and so we have that probably be nice if we could print or Echo uh stuff out I don't remember how to print just give me two seconds okay all right so I think there's like more than one way to do it so I'm just going to do it the way that I know how to do it and people don't like it well that's that's their problem we'll go ahead and type in write host and so what I want to do here is um I want to uh provide that bucket name so we'll just say bucket name here and I'll go ahead and just say uh S3 bucket here and I'll just go above here we'll just also say uh ad best region let see our region so that is fine and so those two are now set uh let's go ahead and uh write that S3 file so we can type in new S3 bucket and then we'll specify the bucket name so I I wanted this autocomplete when we're writing it down here but for whatever reason it wasn't working so of course there must be some way to turn it on we also specify the region and so I'm hoping that this will create uh that bucket now we did create one just a moment ago so I'm going to make my way over to ads and we'll delete the one that was created recently um I don't really remember what was most recently created I guess we could just use our scripts as we do have other scripts here I thought we exited out of this uh contrl c q quit the turn quit uh is not recognized how the heck do we exit exit exit two seconds you know what I think I'm overthinking it because it's open it's open over here so maybe we don't have to worry about it too much I'm going to go ahead and just close that there and yeah it's complaint saying hey terminate it I don't care what I want to do is go ahead and use our our previous scripts to figure out what the last um uh the latest ones we made so we'll just say uh list buckets here and so we yeah we have that power shell bucket and I want to go ahead and just delete that bucket so I'll just go ahead and do that quickly bucket we'll say my Powers shell bucket it looks like I didn't delete one from there before so I can probably go ahead and use my script right now and clean that up okay good uh oh it says that one doesn't exist but it has a two on it there we go so now uh we have uh no buckets there and so we can go ahead and uh run the script so let's go ahead and do that okay so what we'll do is we'll we'll go back and well before we do that let's make sure that file is executable so we'll do S3 and we'll say Powershell scripts and then we'll choose that and I'll type in clear and so that is now executable um we'll type we'll start up uh Power shell again here hopefully it doesn't complain um we'll do PWD now those aren't actually the native commands uh for Powershell I'm pretty sure they are something like get location will get you the same thing but uh of course when people are working with Linux they're used to those commands so they give you all the variation so if it's PWD or di which uh people are used to um older stuff you you have more flexibility there but these of course um are not necessarily the native commands so like for instance LS or dir um or get child item so you can see that there is Sim similarities and things that are different but anyway what I'm looking for is I want to uh execute uh this script so I'm going to do period S3 and then we're going to do bash scripts and oh not bash scripts Powershell po shell power shell that's what we're doing here today Powershell scripts and we'll do S3 PS1 and hopefully this works it's asking us what do we want the bucket name to be so I'll say my Powershell uh ab and then I'll just put some random numbers in here and then hopefully creates notice it was super fast this time uh maybe because the first time it ran it had to install something or something I don't know but um it says that it created so that's really good we can continue on with our our script here so we've created our um our bucket it might be uh great if we uh we've already printed out the bucket name so that's fine maybe we can go ahead and create a file so let's create a new file so we'll have file name and we'll say my file name my file. text and then we'll have the file contents we'll say hello world and see I remember some Powershell uh set content path file name and then there's I believe it's like value value path and it's something like yeah it's value it's value um value file content so you know that should set that there and so now what we can do is go ahead and write to the bucket so if we go back over to our documentation wherever that was here there should be one for writing or putting or something so go back all it the reason I say right is because again it's based on convention in terms of names and I have a feeling that's what it's called and it is I believe it would be WR S3 object here so this looks very similar to our put so we'll go back over here and start writing that so we'll do write S3 object and we'll do bucket name and so then we'll put the bucket name here and then we'll have the file uh our file and this will say file name we'll have our key and then we'll make this also the file name because we're not doing anything fancy here we might as well just keep it simple here I don't know if we need anything else I think that's about it for that so let's go ahead and try this again now we already have um a bucket I wonder if there's like a way of doing conditionals um here let me figure that out all right so I looked it up and uh what we can do is we can create a a function and that's probably the best way to do it so what we'll do here is we'll go ahead and create ourselves a new function we'll just say bucket exists okay and uh from there what we'll do is we'll first get a bucket to see if it already exists first so say get S3 bucket and we'll provide its bucket name and we have to provide I believe uh an airor action so we probably want this to look this up error action silent continue that's probably what we want uh Parell script halts or a portion of the code does not work what action do you want to happen so the idea is that um we just want it to silently fail I think that's that's probably the best option again we're not getting really into the de uh The Depths here of Powershell just trying to get a Powershell script working and get kind of comfortable writing some and so then we'll just specify what uh this here to return the bucket otherwise will get null um so I guess the next thing here would be to actually check now so for our thing here we'll say if not and and then we'll say bucket exists okay then do this otherwise do that and so if the bucket doesn't exist do that so and down here if it does exist we'll just say bucket already exists we go up here bucket does not exist if that's the case then I'll just skip that step so hopefully that will uh work as expected we'll go ahead and hit up and I'll just um I already forgot the bucket name it it's my power shell AB 64 uh 36 the term uh get bucket I think I forgot the hyphen there yeah that's why it wasn't highlight properly try this one more time and we'll go down here and not sure how to get that out of there as it's complaining quite bad so sometimes that happens we'll just close that out and I'll open up uh this here we'll try this power shell oops oh this one's this one's done too there we go now it's back power shell and yeah we'll go back to mhm and I don't remember what we named that bucket so we'll go back over here it was like power shell or something power shell AB 542 okay called this my PO shell bucket my Powershell bucket AB 542 a maybe 52 5 524 524 not even saying the right numbers here we'll hit enter and fingers crossed fingers crossed it's thinking so that's usually good but yeah again I don't know why there's lag with Powershell I have no idea Bo probably knows but um I'll just be back I'll pause so we don't have to wait here forever okay that was really slow and now waiting for the next step while I was waiting there was asking be why is power shell so slow and he tells me it's because of net because it has to uh uh stand up and tear down stuff um it sounds awful but that's what it is so here it looks like it it created the bucket did it uh actually write our file I don't know I didn't see anything for that so we'll go back over here fingers cross that we have a file we do have a file and so I would call that uh nearly successful we didn't actually write the contents on the file so going to go back back here and we'll save that and we'll try it one more time okay so I'm going to try this one more time and we'll put that name in again and then we'll call that good enough for for using Powershell enter I'll see you back here in a bit okay all right so waiting a little while there it looks like uh our file is probably in here now and it is we're in good shape uh we are all done here I don't even care about deleting the bucket that's how tired I am of of Powershell I'm going to go ahead and delete this file I don't really want this file to be uh uh right there uh I just created it in place probably should have told it to create it in the temp directory but that's okay we'll go ahead here and just add these files and we'll say Powershell scripting and you can't say I didn't show you power shell uh yeah for eight of us but again you know not going to happen very often but get some exposure to it and that should be sufficient there just stop my workspace and we'll see you in the next one ciao all right so let's get to it with Amazon S3 and utilizing sdks as we've done a lot of different programming things with S3 we've done Powershell scripting uh bash scripting we know the difference between the S3 CLI and the S3 API and so one of the most common ways of interacting with adus services and specifically S3 is by using software development kits this allows you to use your favorite programming language uh to interact with uh the cloud service provider API adus has probably the largest selection of languages that uh in sdks that you can use and they even have my favorite which is Ruby and we will be using Ruby today as well as Java uh one of my lesser favorite languages but I think it's very important as a cloud engineer or devops engineer or any technical role that you're comfortable with any of these languages you don't have to be an expert in them but you need to be productive and so I think that we're just going to keep mixing it up every time we have a chance to use an SDK so maybe one we'll use PHP we'll see but let's go ahead over to uh the ad examples website I'm open this up in G pod of course use whichever Cloud developer environment or local developer environment that you want to use but just understand that you may need to uh provide your own configuration or make your life easy and utilize get pod as well which also has a generous free tier all the configurations are set up in the git pod yaml file and they're already installing them in so on the right hand side you can see these are installing um we have the adus CLI I already have my environment variable set in here my adus credentials so that is something that you'll need to do uh but anyway what I'm going to do is go on the left hand side here we're going to make a new folder called SDK and within that folder we're going to make a new uh two new folders one called Ruby and the other one will be called um this one will be called Java so let's go ahead and work through some Ruby code uh Ruby is not going to be too hard here so the idea is we'll need a a ruby file so the Ruby file we'll just call it s3. RB and we're also going to need a gem file so the way Ruby works is uh it has a package manager called bundler and its commands are called bundle now this machine already has Ruby installed I can just go here and type in Ruby version it also has Java installed so understand that uh you might have to install these on your systems this one's using 3.2.2 so that's really good I want to go ahead and create a um a gem file so we'll use bundler to do that so I'm going to go and just CD into the SDK Ruby directory here I'm going to say bundle a nit and that's going to create my uh create me a gem file so in this gem file it is pointing to ruby gems ruby gems is where it gets its libraries from so we're going to install a few here the first we'll close the editor here but the first is going to be the gem ads SDK S3 I believe that's what it is I'm going to also want pry for debugging that is a debugging tool for making break points in Ruby very useful and I think we have to install noiri or it's going to complain noiri is uh it's for parsing things like parsing XML so when you're working with apis and going out to the internet they need some kind of parsing Library you get to choose which whichever one you want noiri is the classic one for that but yeah uh Ruby utilizes a module SDK a lot of these ones do that I believe Java does that as well um I think they do well yeah I guess they would because you include packages separately we saw with powerwell that it was a modular but yeah that's usually the approach just include what you want but we can go over to the Ruby tab here and we can go over to it SDK for Ruby I think they're on version three now so if we go into the API reference docs over here uh you I would say that the reference docs are good based on what is good for your language so Ruby is really good for languages python is pretty decent Java is horrendously bad um but uh you know it just depends on the culture and how documentation is generated out but you can see that there is modular um libraries here so you have to kind of know what they are or you probably just go to the Ruby SDK on GitHub ads Ruby SDK um GitHub because it is a public repo and I'm not sure how I know which ones are which but um somewhere here they list them yeah they're all here so here's all the uh the possible libraries that we could modularly install I'm only again interested in the adus sdks S3 so now that we have these three libraries defined in our gem file I'm going to go down below here and type in bundle install that's going to go out to the internet pull down those gems gems are what we calls packages and they're now installed so we can go ahead over to our s3. RB file and we'll write some code here so we'll do a require inabus sdks S3 we'll do a require pry and I'm going to also require secure random so secure random we didn't install that it's just built in as a standard library and this is so we can generate out um like random values I want to bring in a bucket so uh cuz again we're working with S3 and so so what I want to do is maybe create a bucket and put some files in it so we're going to take that as in as an environment variable so when you want to take environment variables in you just type in EnV in all cases and then Square braces and that will uh be accepted into this program so I'm going to go ahead and just do a puts and let's just test this out so far and make sure that we're in good shape um the way it works in Ruby is when you want things to be executed in the context of the the the packages you installed you put bundle exec in front of here then the program you want to run so just Ruby and we'll say s3. RB and we'll go ahead and hit enter and so notice it's not outputting anything that's because we have yet to set a environment variable called bucket name so I'm going to go here and just type this in I'm going to say uh my cool bucket like this and we'll hit enter and so basically that environment variable is being executed in the context of this line and it gets passed in here and gets printed out there so very useful so far uh now the next thing I want to do is actually go ahead and create ourselves a bucket so we'll go back over to our code and we'll go over here on the left to uh look for S3 so here it is and in S3 we are going to find a bunch of things you can see bucket here but we actually want client because we first need to create ourselves a client to work with S3 I'm going to go over here and just paste this in now you notice it's asking for like a a credentials object so here we there's actually another thing you can create here a credentials object pass it in here but our credentials in this environment is being loaded in from environment variables it generally adabs will recommend to use the credentials file the the the hidden. adabs credentials file we're not doing that because that's not the best practice when working with G pod so it's just going to vary based on what you are doing so both the credentials and region are already set via the environment variable so I don't need to set those there so we'll just ignore that and the next thing I want to do is go head and create a bucket so probably here uh it will show the operations that we can or methods that we can call off a client and there should be one here for create bucket so I'm just going to search for it uh the naming of API operations vary based on SDK so you know I'm familiar with this one so I knew it was already named that we'll go ahead and copy this in here and we'll go back and we'll paste this in okay and so now we have our bucket name here so just say bucket name and then we'll we'll just bring up our region here at the top region I'm going to say ca Central 1 cuz that's where I want to deploy and we'll just put region here as such Ruby should not like trailing um uh commas some languages expect you to have it like goang Ruby probably will complain some don't care if it's there or not like JavaScript so just understand that every language is a little bit different so we have our bucket name our region those are going to get passed down here I got rid of that puts putz's print for Ruby and um what I want to do now is create some random files so we can use ran to say generate a number for ran I'm going to add one so it's always between the range of one and six and I'm just going to assign that to a variable called number of files and just so we know how many uh happens here I'm going to just print that out here say number of files and then do interpolation so we can put that into the string and that will print out the number files so Ruby has this really nice uh easy way of repeating things so if we do times it'll say take a number and do this x times so we could say like 10 times 20 times so this will be whatever that random number is and we'll go here and say each uh do and then we'll Pro provide the I that's the in uh the index so that'll be the number 0 1 2 3 4 5 whatever it is it'll start at zero and so I'm just going to put that out so I can see the numbers as they count up and I want to make a random file so so I'm just going to say like file uncore I and then we'll just make an empty txt and then we need an output path so I want to place these in temp because I really don't like it when files drop in here otherwise we might commit them to our repo by accident so I'm going to put in temp temp is a uh Linux folder standard where you if your machine is turned off or whatever those files will just vanish so that's a great place to put temporary files that's where I like to place them and so in Ruby we want to uh open a new file if you're wondering how to do files in Ruby you can just go ahead and go file uh Ruby and usually the Ruby guides is really good so they have a bunch of different examples here as file file open things like that I think that in the SDK they might Show an example uh how to write objects probably like if we go with put object here they might Show an example so where is it here it is here like opening a file so that's probably what we'll do is use that example when uh doing that but first we need to actually create a file so we'll do file open and there's like a ton of different ways to create files in rubies but this is the way I like to do it and so the W is for WR so we're going to write to the file this here is just file I'm just shorting it to F to make it shorter and so I'm going to write to this and I'm going to use secure random secure random and we're going to generate our uu ID so we get a random value in that file so that'll be good the next thing I want to do is I want to uh now write um a file to Ruby so we'll go back over to here I'm going to go ahead and grab this code this code looks pretty good to me and we'll go ahead and paste this in and so I think what I want here is yep the output path here as well so we actually we create the file locally and then we uh then we read it it'd be nice that we could do in one go but it might be a bit too tricky so I'm just going to keep it simple here today and we need to to supply our bucket so I'm just going to bring these on to new lines you'll notice like sometimes in Ruby we can have parentheses or not they're optional Ruby likes to look like more like English and so that's why you're not seeing those there but I could I could remove them on these ones if I want to but uh you have to know exactly when you can remove them and not uh but you know if they're if they're making uncomfortable you can go ahead and add them in it's up to you I uh I just been working with r for such a long time I just change change how I like to do that I want the file name to be here as we want just the file name without the address and then we will pass in here the actual file it can be really tricky um uh putting an object I think I said this earlier but depending on your use case it could be very tricky to pass or write the file to S3 and so this thing I think takes a string or an input output stream um but you have to be careful because sometimes when you put objects it won't be the content type you think it's going to be so just be aware of that and sometimes you have to explicitly set the content or fiddle with this but um I've done this enough that I I'm not going to have that uh problem here so hopefully this is going to work let's go take a look at our code so this looks pretty good maybe we can go ask chbt to write some documentation for us that might be nice so I'm going to go here and we'll go up here uh document our code and maybe just make this a bit easier for you like I know what's going on here but it's important that you know as well and I'll just give it a moment we'll come back when it's done okay all right so I let that generate out there and so we have some uh comments here we'll just save us a bit of time uh we could have handwritten all this stuff but uh you know again I want to to make things nice and snappy here so we can focus on cloud but let's go back to the top so we're requiring our libraries the first one is the SDK for S3 operations this is for interactive debugging which is super useful this is going to Generate random strings this is a builtin Library so we did not have to include it in our gem file here we're INF fetching the bucket name for environment variables we're specifying our region we're creating a new instance of the S3 client for most things in the Ruby SDK you're going to be doing this there is some variation on this so it might look a little bit different based on which service then we are creating our bucket we're determining the number of files we want to create great we are randomizing number between 1 to six We're looping through them we're writing to those files and then we're going to upload them so that is our script hopefully there are no problem problems in here I'm going to drop some binding priz so that we can just take a look at some stuff so I'll put a binding pry here and I'm going to um place a I guess we only need the one as I'd like to see that response there and see what gets returned well let's go ahead and run this script so we did this just a moment ago it was well it was uh this here and we're going to have to give it a bit more of a unique name my cool bucket is probably already taken so I'm going to say my uh Ruby SDK example and I'm going to put ab and just put a few numbers on the end there and we'll go ahead and hit enter and hopefully it will work what's going to happen is it's going to hit that break point here on line 20 that's going to allow us to write whatever we want so let's go ahead and just write something so I'm just going to write Rees oops RP and I'll hit Q there because it had end it was kind of scrolling now I get the prompt and so I can type it in I hit Q to to get out of that mode there when it head set end and we type in Rees and this is what it's returning here so you can interact with anything and just help yourself debug anything it's really nice I can type in bucket name so if you're never sure what's going on your code you just throw a bonding pry in there when you're done you just type exit if you get really stuck you can type exit with an exclamation mark if you type exit it will continue on if you want to quit completely you just give it an exclamation so I'm going to take it completely out there I'll leave this in just in case we want to have a debugging option but we'll go ahead and try this again and let it run for real and we get an error not a big deal but it says uh bucket exec Ruby SDK or sorry S3 RB has an error uh you your previous request to create the bucket name succeeded and you already own it oh did we create it twice by accident maybe um how did we uh oh we put the binding pry here okay so that makes sense so here it's throwing an error and that's totally fine so what I'm going to do is I'm just going to go over to S3 we'll delete this uh bucket so just say Ruby we'll give this a nice refresh I'm going go ahead and just delete that bucket makes sense and this is the big difference between the SDK and infrastructure is code infrastructure is code would uh detect that the bucket are existed and just do an update in place or do not whereas um the sdks are not ID deponent so ID deponent means like it knows there's the state of a thing and it will work with that thing as opposed to uh Ruby it just it doesn't know that anything exists it's just going to keep trying to do what it wants to do the SDK not Ruby in particular and uh that's a that's a huge concept to know we're going to go ahead and enter again and it created two files for us that's great let's make our way over to the bucket that looks good we'll click into the bucket and let's confirm the contents of these files I think that's really important you should always confirm the contents of the files so that you know that it's doing what you want it to do we're going to go ahead and um download uh this file so we'll go and do that we can't preview it it's a text file so I'm just opening up here locally I'm going to just drag it on into get pod so we can open it up and we have a uu ID so this has been successful so that was pretty darn easy um though again Ruby is my favorite language so I would hope that that would be easy for me hopefully it's easy for you as well I'm going to go ahead and Commit This and we'll just say uh Ruby SDK example so there you go so not too bad let's go take a look at Java Java is a little bit harder um but I I can get through it so we're going to go into our Java directory the first thing that we need to do before we put any in here is we need to um we need to generate out a maven project that's something you always need to do so we'll go ahead and do that um I'm going to go and look up Maven and we'll see if we can get the documentation I'm going to go to the maven in 5 minutes and somewhere here it's going to tell you to CD in and you want to copy this line here this is going to give us this nice big command to uh get working here I'm going to just make a readme file here for Java because Java gets really messy really quick and I just don't want to uh struggle so I'm going to go here and just you know comment this here and make this a little bit more manageable so Maven uh Maven is one of the uh tools that you can use to uh assemble packages and make a project in Java the other one's called Gradle uh that's all I kind of remember about it but I know that if we run this line it'll get a set up and so we have a maven archetype quick start I think that's going to set us up with the template that we want to uh work with it there might be already an archetype an archetype set up for um AWS I'm not really sure we could look into it but I'm just going to keep it simple here today uh it's going to create folders based on what we call the group it's a very common uh practice in Java to name uh the app then the company or the immediate project or company and then the organization or something anyway there's these three things and it will create folders for it you'll see it uh nothing too complicated here and we'll go ahead and well before we do that I'm just going to make our documentation a little bit nicer and we'll just say bash and say uh create a new Maven project and um I'm thinking that we're going to have a problem here like if you look very closely it might be hard to see on the screen but there's this little gray dot after it and so uh what happens if when you try to break a new line for bash scripts if there's a new a space after it it'll muck things up see so if I run this I expect this to break I'll just put a space here to show you what I mean CU it's such a common problem I want you to see that happen and oh maybe maybe there's not a problem here sometimes when you copy and paste it's a problem but when it copied and pasted it into the thing it made it one line so it didn't uh do what I wanted it to do but just understand if you have a problem here just make sure you get rid of uh those empty spaces on the end for whatever reason it worked out fine for me but let's go here on the left hand side now we have a folder called my app and we have a source and we go into uh main I thought it was going to make folders called Comm my company app I guess it didn't I I definitely know that I've seen that before oh it's right here never mind I was like where's those folders here they are Java com my company app so notice it's based on that convention there um goang kind of has that kind of pattern um usually you name it based off like your repos we'll go into the app JS and so here we have some stuff here we're going to to configure our Palm XML this is where we uh uh specified dependencies you can tell jav Java is old because it still uses XML and uh XML is okay but I I'm not a huge fan of it but we need to get into this XML the configuration for Java so let's go back over to the SDK Pages the developer tools let's see if we can go find that quickly so here we'll make our way over to Java and I'm going to go into um uh we have samp Le code I want to get started with Java where's the uh we say Java UNS we'll try this not exactly what I want we'll click back I don't know why the Ruby one's so easy to get to and the Java one is such a nightmare but I'm looking for that reference guide that's what I want or getting started with Java it was SDK for job let's try this one over here this first one here we do want Maven that's right develop and deploy applications SDK Maven yep that's right here we go okay developer guide that's what I wanted I'm not sure why it was so hard to find but we'll go here and it should give us some instructions on how to start working with it so we have the SDK we need to set it up first let's go to the setup and on left hand side we're going to maybe go to basic setup and what I'm looking for here is the maven stuff set up here maybe it's uh what set up tools ah poy Maven here we go so we have a very similar command notice it's a little bit different it says archetype Lambda D service S3 uh D region West 2 this might actually be a bit better it might give us everything that we want so I'm very tempted to actually try this out so what I'm going to do I know we just did this but I'm going to go ahead and just delete this um uh this here and we'll see if our Life's a bit easier otherwise we have to manually configure this Palm file I really don't want to do that and this one appears to already be configured for S3 so let's see if that will uh do what we want so I'm going to go ahead and just uh copy this and we'll put this down in our documentation okay and paste that in and we'll just kind of observe what the difference is I don't want to be us West two I'm G to do us East one we'll do something a bit more normal we'll make our life easy just do Us East one it looks like the service is S3 we have archetype Lambda we have archetype version so that's something we'll have to um Supply here it' be nice if they just showed us on this page uh replace with full package to use the latest version of archetype replace from latest Maven Central so this is probably where the repo is so we have it here it says the itus SDK for Java uh may have an archetype for Java Lambda function using itos SDK 2.0 so is it for Lambda or is it for S3 the SDK for Lambda uh sorry the SDK for Java Maven archetype for Java archetype for Java Lambda function say Java Lambda function so I think it's not talking about Lambda like adus Lambda it's talking about Java Lambda function which is a bit confusing but that's fine so what is the latest version here it's 22129 so we'll go ahead and try that out 22129 good we'll go back over to here and I lost it here it is Maven Central so I I assume that's like ruby gems that's where all the uh dependencies are going to come from and hopefully that is just going to work fingers crossed here we'll go ahead and copy this notice the uh the group ID is a little bit different but as long as it works that's all I care about we'll go ahead and hit enter and it's generating stuff out fingers crossed please just work excellent so now we have our app here we have our Palm XML that is good let's go take a look at that and there's some things that it would tell us to do it would tell us to add this dependency management block uh for the uh for that here it would also tell us to um add some adus dependencies in here so we have S3 so you'll find that a lot of times you can copy and paste this file or there's other ways to do it again I'm not uh uh my Java knowledge usually when you use Java you use like a like a Clips or something specific for Java and we're using a generic uh Visual Studio code so we don't have the same kind of powers that we do uh with other IDs but usually Java is so complex for configuration you don't manually configure it but they're probably our libraries or extensions here for Java so we go here on the left hand side I wasn't looking for them but um there's probably something so yeah like here and I'm not sure if it has to do with a specific version of java so provides Java language support for Eclipse JDT language server uh yeah I think I'm just going to leave it alone I don't think I really need it just yet so but um anyway so now that we have that project set up let's get some code in it and so what I'm going to do here is just drag this on down let's drag this on down here and let's go back into our uh our main into our app so we have this we also have this dependency Factory so that's going to help set up our S3 client so we already have that ready to go so oh yeah let's take a look here so here we have our app it's already set up for the S3 client and yeah it looks like it is ready so we already have it in place and so we can just write our code down here so just give me a moment uh to get refreshed with some Java S3 code okay all right so we learned how to work with the SDK we've worked with Powershell bass scripting understand the difference between the CLI uh versions for the S3 API and and S31 now let's go take a look at infrastructure as code as this is the other way that we will want to interact with S3 and also just get better experience with using I tools so what I'm going to do is go over to the examples repo that I've been working on uh in GitHub and I'm using git pod as my cloud developer environment of course use whatever you're most comfortable with and uh again git pod has a free tier so if you want to follow along one to one you can do so or choose whatever works for you but anyway what I'm going to do here is uh make a new folder here we're going to call it IAC and we're going to start with cloud formation so cloud formation is something that everybody should know how to do um it's not that difficult uh to work with you basically create a yaml file and from there I'm going to call template. yaml uh from there we can just start configuring configuring it so the yaml extension is a bit funny because it's supposed to be yl some people use yml some adus Services prefer yml as the default some prefer yl I believe that in the case of Cl foration deploy the uh the I command will be using to deploy this template it's going to be it might actually I think we specified the template but I think the default is yaml so just understand that it's not even consistent with an AWS uh with that naming convention so you know we might have to fiddle with this and make the best choice there so now that we have our template yaml let's go over to the documentation so we'll say S3 uh adus Cloud information and Creator sells a cloud formation file so we'll go over here and cloud formation can be presented in Jason or yaml uh the reason why there's two is Jason came first yaml came second Json is great for pratic use but um you know when you're writing it from scratch it's a bit of a headache to utilize um uh Jason having to write Jason but I know some people that still write Jason because they're that old school so basically this is all we need to do to create a um a resource in cloud formation so I'm going to copy this example here to get us started uh there are a few other things things that we should have in our cloud formation template so we should have uh the template version at the top here so let's just go get some boilerplate stuff if we go to the top here uh probably in the getting started learning the bit template basic let's just see if we can find one I just want the version at the top and they're just not showing us to uh to us here uh okay I'm just looking for that top part there as there's always a version working with templates template formats there we go here we go so under template formats the thing that we want is this thing at the top um this tells us what version of cloudformation we're using in terms of its language here this thing hasn't updated in forever so I'm very confident that this is the latest version even though it says 2010 I know that's old but I'm just trying to tell you that that is uh am I copy Cy paste mucked up there so I'm just going to go here and uh again not that I'm bad with copy paste this is an issue with um uh browser get pod stuff so we'll put that here at the top and apparently we got our t on the end there so we need to have that first line that first line is very important as it's always supposed to be there we can provide a description which is a good idea so I'll do that second just say a simple S3 bucket then we might want to have uh we'll have our resource but we'll probably want to have a parameter so that's a good idea it's always good to pass in parameters so I'm going to go here and just type in parameters well we'll leave it out for now but the idea is that we want to create an S3 bucket and so anytime I'm uh working with cloud formation I like to always always always always link where the resource is so that when I come back to my template it's a bit easier so I'm just clicking back to get back to our S3 documentation here I'm just going to clear out that anchor at the top here and to copy this bring it over here and I'm just going to paste it above the type and so the idea is that whenever we have to come back and and reference this I can hold I then control on my keyboard and I can just click through the link and open it up and get the quick reference so I really like uh that as a way of documenting my cloud information but let's go down here and look at the properties and the best way to learn cloud formation is just to step through all the requirements so the first our properties and what we're looking for is the yes like what is required so if we type here I don't think anything's required so this is one of the few Services where you you're not required to even put a bucket name if you don't provide a bucket name uh they're going to just randomly generate one for you so that's really nice but what we'll do is go ahead and just take out the deltion uh the delution policy we will uh take out the bucket name and we can literally just have this and this will create a bucket so we'll just say we don't need to to supply a bucket name as adabs will generate a random name for us and that's really nice I like that and so what we'll do is go down below here and we want to deploy this template actually what I'm going to do to make our Liv super easy I'm going to make a new folder in here I thought I did this already I'm going to call it CFN for cloud formation I'm going to drag our template yaml file into it and I'm just going to make a new bin script here I'm going to call this one deploy and so in here we will have our deploy script now we did Bash scripting earlier so we're just going to grab that shebang and maybe the echo in the top here and we'll go back here and so the idea is um this will make it so it runs in the context of bash it'll just say uh deploy cloud formation or S3 bucket via CFN cloud formation that's the short form for it and so now what we can do is make that uh readable writable here so say S3 IAC um deploy yeah that should do it nope doesn't like it um S3 I see oh CFN deploy there we go and so now this is an executable script and we can put in our CLI command here so I'm going to just type in a CLI cloud formation as I want to grab all the cloud formation uh command comms it's just one command but make sure you're on version two here sometimes it points to the old docs I'm not sure why it still does that it's kind of frustrating but we're going to go here and look for commands so there is a create stack delete stack stuff like that these are older commands and everything has been uh consolidated into the deploy command so you really want to use the deploy command and we have a bunch of options here so we'll read through this here the first thing we'll do is go type in um adabs I think it's cloud formation deploy so that's going to be use to do the deploy if we want to we can use the CLI I'm going to go to this tab because this is the one with auto prompt on it this is the only one that will'll have it as we have that in our get pod yaml file from earlier where we set that nbar uh over here just pointing this out if people are watching videos out of order and they want to know why things are happening a certain way we'll go ahead and just hit ads here that'll bring us into the auto promp mode I'm going to type in cloud formation if I go space we can do deploy and then it'll tell us what's required so the template file is required the stack name is required so those are two things that we're going to want to put into here so I'm just going to um break this into separate lines we'll say template file and we need stack name here we can also specify the name of the S3 bucket where this command will upload uh that there I'm not going to do that right now uh we're trying to create an S bucket so it's a bit funny that we are um doing that there and so the template file is just going to be template. yaml and then the stack name will be whatever you want want to call it I'm just going to put it up in a variable up here so it's a bit easier I just call this one CFN S3 simple and sometimes there's uh there might be limitations on stack name if there is we'll find out here in a moment so that that might be most of what we need um I know that we need to set capabilities a lot of times you need to set these so it's just good to just set them off the bat so we go down here capabilities a list of capabilities that must be specified before it called information can create certain Stacks so certain Stacks need them and there's um capabilities IM and named IM Im so it really depends on what you're doing so we might run this and it might say hey you need to go add this flag um and I always forget which one it is I honestly just leave them in the template once they're working um so that's fine we can override the region and other stuff here but uh I think that's okay we didn't actually specify the region in the template yaml so we noticed that when we want to deploy buckets let's say CA Central we have to uh specify that configuration maybe we'll go take a look and see if that actually is here as that would be a good option for properties so carefully looking through here that was called um uh where is it inventory configuration no it's some kind of configuration and it might not show up here it might not be necessary let's go take a look at one of our older bash scripts to see what that thing was called create bucket configur so that's what I'm wondering is like is there a bucket configuration so I'm not seeing that there is it a separate object sometimes that could be a thing it is not so I'm thinking the way it knows what um region to deploy it in is just the one you specify and you don't specify regions I generally don't specify them in the cloud formation itself you're going to place it here so I'm just going to go ahead and say region in we'll say ca Central 1 now I should already do that because that is my default on my computer but I just want to explicitly set it as we know that we run into issues when we're not doing Us East one there's additional options so be interesting to see that we don't actually have to do that stuff there so what I'm going to do is just close this and close this and we have ourselves uh something that looks good but I guess the question is will we run into a problem I don't know so what we'll do is just hit control C here click close that out I'm going to CD into the directory so we'll go into S3 IAC CFN and do uh clear and we'll do LS and I want to go ahead and deploy that so I'm doing period SL deploy and let's see if that command works so it's creating a change set very common for cloud formation always create a change set first as we're waiting for that to execute let's make our way over to cloud formation so we'll go and type that in here the huge advantage of cloud information is that the state is managed by anus cloud ad service and that is free so if you use something like terraform terraform is great but um they do charge you eventually for the amount of resources that you are managing with it uh though the offset is you can take your state file anywhere and uh it can do multicloud so that bucket was created you didn't see it but there was a change set that was created and then it was accepted so I normally like to always accept my uh my uh change set but let's go take a look at um the resource that was created here so we have S3 bucket let's click through to that S3 bucket and look at its name so its name is based off the template and then there's a random value here in the end where was it deployed is a good question so we'll go here and it was deployed in CA Central 1 so that's really good now let's say we want to uh get rid of this bucket well we'll we can just tear it down uh one way is through cloud formation here and we can just go ahead and hit the delete so I'm going to go and just do that there and we'll let that roll back and that's pretty quick um you know some things aren't as quick as this obviously but it's going to vary based on what you are doing so let's go make our way back over to uh here and we can see that it created a change set and and then accepted the change set and it did a deploy so one thing I want to do is I want to modify this so that we have to be the one that um accepts the change set because I think that's a better practice and we'll go ahead back over to here and going to go up here and paste this in it'll say no execute change set so that'll be one major difference that we do I don't think let's change the the region let's go somewhere else I'm going to say us West 2 I think that's a region sounds like one and uh so we'll see if we run into any issues or make sure that it actually creates where we're expecting it to be so go ahead and just hit uh deploy again try that again notice that we did not have to provide the capability so there's whatever we're doing does not require it sometimes you have to go add that in there so it says waiting for change set to be created change set created Run the following command to review the changes um which we can do let's go try that right now I usually just go to the console for this let's go take a look and see what we get and um okay did we paste it in wrong nope so for whatever reason it's saying the change set doesn't exist but let's ignore that let's go over to CL formation again that's not the way I would normally do it refresh it now notice that we don't don't see anything in here because everything is region specific so let's go over to U Us West 2 and so here we can see it over here it says review in progress we'll go to our change set and it the change sets created we'll click into it and the idea is that we can see what it's going to change before we accept it as remember that um IAC infrastructures code is EP poent meaning that it's not going like it it's aware of state and it will either update it or tear down and recreate something and that's really important to understand of course we'll cover this more in our cloud formation section but anyway so what we can do is if we want these changes to uh work we're going to go ahead I'm not sure why we have a little X here uh it says this does not exist okay well we're we're in it you can tell me that all you want but we're clicking into it and it's clearly there but we'll go ahead and say execute change that and so we'll just accept it and that lets us roll out so I I like to always have that set I think it's just good practice uh CU then you can check and see what is going to be replaced the reason why that's important imagine you have u a resource that cannot go down or cannot have downtime and you go cloudformation it says it's going to to in order to update it has to tear it down and recreate it that's where that really matters and so you should really be reviewing that of course this is going to be very quick or should be very quick so we'll wait a little bit of time here we can watch as it's being created in here use us will tell us what's happening and now it's done so we'll go over to here and we have our bucket so that's good what I want to do is now uh tear this down so there should be a command for that again I usually just do it in the the um CLI but we'll go ahead and make a um a new file here I'll just say um delete stack I suppose we'll just copy some of our code here so we'll go here I'll paste this in of course uh well actually we didn't have to worry about the name of the um the bucket because it's randomly generated so that's easy but we'll go here and we'll now go back to the a CLI we'll click back and we'll just get the delete stack option here and seems like we just specified the stack name I think it's as simple as that we can go down to examples as well and take take a look there yep it's as simple as that so we'll just go ahead and grab this we'll go back over to our cloud formation or sorry um our Cloud developer environment and we'll paste this on in here and I'm just going to replace this with the stack name I like this to hug so I'm just going to do that instead and so now we have that I'm going to go ahead and show mod that file so that we can execute it and it's not aut Auto completing because I'm already in the cloud information directory so I can just do delete stack right here there we go and let's go ahead and see if that will delete our stack good we'll go back over to here we'll give this a refresh it should be uh deleting the stack I mean we didn't specify a region in here uh that might matter because this will default to my CA central region which is not what I want so we're going to have to say uh Us West 2 no didn't like throw any errors so delete stack for that's re bucket here so that's probably why it didn't work it didn't throw any errors and we'll go back over here now and take a look there we go now it's deleting so it should delete if there's files in that bucket it wouldn't delete it would complain we'd have to work through that issue um but uh that is good so that's a really good example of working with cloud formation we will definitely do more uh here with S3 and CLA formation or some other cloudformation stuff so we'll get a lot of practice here uh but we're going to keep this one really simple is I want to move on to a different IC providers so now that I'm happy with that I'm going to go ahead and just save this here so say cloud formation simple CFN example for S3 great I'm going to stop this workspace and I'll see you in the next one okay ciao hey everyone we uh just worked with cloud formation now I want to go ahead and use terraform as terraform is a very very very uh popular multicloud solution for IAC and it's definitely something that in the industry you'll come across uh you might ask why would people use a third party IC tool when uh it us has CD a cdk Sam cloud formation well it has to do with multicloud workloads it might just be easier for development um there are a variety of reasons why but you know the right tool is based on what your team wants to do and your business use case so we should learn a bit about terraform so what I'm going to do is open up our ads examples here and get rolling as that's going we'll need to install the terraform CLI so I'm going to go over and type in terraform CLI and we'll go over to um the documentation here I'm going to type in install and I'm going to get that configured in my G pod of course uh if you're not using get you're using a different Cloud developer environment or local uh developer environment these instructions are going to vary uh I'm not going to show you every variation it's too hard to do that but um it's going to be pretty straightforward so here what I want to do in my uh G pod file I'm just going to add a new a new task here we'll just say name and we'll say terraform and then I'm going to add a before with a pipe so I can do multiline and then we'll go ahead and grab the installation instructions this sub is pretty straightforward so we'll go to Linux here and uh you know there are like there's home brew and stuff like that I don't really want to install it via home brew we could do that on Linux uh I'm on Linux I I again I kind of prefer to do it this way want to Debian Noti there's a a simpler version for Linux but it varies based on what you're using so G pod is using Debian underneath so we're going to stick with I think it's Debian it's either Debian or Ubuntu so we're going to just go ahead and work with this it's pretty straightforward we just go down the line here paste paste paste I'm not going to do multiline for this I'm just going to let those pipes happen like that and uh you know make sure you read the instructions here I'm copy and pasting because I've done this hundreds of times and I've read it before but if it's your first time doing it go read through the docs sometimes like Google will if you go down the line they'll actually have like different ways that you can do it and you can't copy paste it all the way down and then you'll wonder why it's not working happened to me a couple times so you know I'm just looking here and just making these single single lines here bring this on to its own line whoops whoops whoops whoops sometimes the copy paste messes up here just going to double check that it looks like that's cut off now the copy paste here is notoriously AFF finicky so I have gpg here so that one's good I'm just going to redo that here going to give myself some room as I copy paste uh we'll grab this one here grab that in full and I'm going to paste that in and I'll grab the next line here and I'll grab the next line here we could also extract these out as scripts and then run just the script that's totally an option as well I'm not sure why we're not doing that as that probably would be a bit easier but I didn't think of that till now so we have this this is piping so this is going to be a single line over here as such okay so we'll do this great and so that should install it that should install it so what I'm going to do I'm just going to do a dry run down here and then I'll restart this to make sure it installs go ahead copy and we'll paste this in we'll hit enter and that stuff here is fine you should expect to see that and I'm just going to type in terraform good so the terraform CLI is now installed what I'm going to want to do is just Commit This and save this for later so I'll go ahead and just save this say uh um save terraform install and again you could make this a bash script you just take all these commands put in a bash script and then just run it manually that's another way of doing it that's actually probably how I would do it in GitHub code spaces but uh so we've committed that uh that there I have to commit it for to restart this workspace because I wanted to install every time I launch this environment up of course if uh you want to you can always just click that and have these tools installed and work with them and play around with them but we'll let that stop and we'll spin up a new one as that is going here come on workspace there we go is it launching a new one or the old one cuz I don't want the uh the existing one I want a new one I think it's launching a new one now the thing with um terraform is that the SE like the when you use it locally it's going to store the state file here when we use cloud formation you didn't have a state file it's it went straight to the cloud you never know what it looks like but uh you know terraform is a little bit different what we'll do is we'll go into S3 and we'll go into IC and we will make a new folder here I'm going to call it terraform hopefully it's at the right level here good I think that's right that doesn't look right I'm going to just drag that onto IC that looks better good and we'll make a new file here and we'll call it main.tf um terraforms interesting because you can create multiple files and they can get rolled up really easily so uh you know I could create one file here and then another file for the S3 and I might just do that just to show that that that terraform can do that so let's say uh terraform here and we want to configure the ad uh the ANS to work with terraform so I'm going to go and type in terraform registry to go ahead and get the um well that's not what I want somebody's paying an ad to to uh beat out terraform we'll go here to the tform registry and we'll go to Providers and providers is basically your connection to uh the cloud provider we'll go ahead and grab this code as that's what we need to start a configuration so say allow and so we say require the provider this one's 526 uh 526 if this is the future this code might need some tweaking because they update these providers often and can break the uh syntax here but the Tero provider basically provides um API access to uh the cloud service provider and it also will have Primitives in terms of um objects so on the left hand side there's one here for S3 go here on the left hand side somewhere there's one for S3 S3 S3 where are you it's all alphabetical I just got to stick with it here here that's R 53 S3 simple storage good S3 bucket all right so here's a very simple example and so uh the way the syntax works is always starts at 8 bu because that's the provider then the service S3 and then the object or resource so bucket so we have a bunch of stuff in here and so this is the very simple example so I'll go ahead and copy this in here now I can place it right down here below but I'm actually going to place it in the well I meant to put all this stuff in the main TF but we'll paste this into here and we will cut this out of here and we'll put this in our main file all right uh now we can provide our credentials here but since we've set them in our environment variables they're going to get loaded into um they're going to get loaded into this uh uh naturally now when you're looking to configure any uh provider they always have it on the first page I noticed so if you go up to a was provider here they'll explain authentication and there's different methods so this is the one we're doing environment variables but uh there's definitely a lot of ways you can configure it it's going to be dependent on your uh team and use case but this is the way that's going to be easy for us anyway so we have the provider here and that doesn't need anything additional um at least I don't think it does and so here we just typing the resource type so it's going to be a bucket and this is going to be the name of the The Logical name of the the resource I didn't talk about this in cloud formation but when we go back over to CFN for just a moment here notice we have S3 bucket here we could name this whatever we want we say my S3 my S3 uh bucket and this is a logical name that will show up in um uh as a reference to that thing it doesn't show up in our code or anywhere else it's just more like how do we know how do we uniquely identify uh a resource within our um template so we can call this um my S3 bucket I think we can do hyphens there and here we have the bucket name now I want it to be randomized I'm just going to take out the name I think we can do that and we'll have some tags for fun here we'll leave those alone and so this should in theory create ourselves an S3 bucket we're going to go down below here go to our terraform tab where terraform is installed just type in clear and I'm going to type in uh terraform anit it says terraform initialize an empty directory um that's because we're in the wrong folder it's looking for terraform file so we'll we'll go CD into the correct directory here we'll type in clear and we'll type in terraform init again and so what it's going to do it's going to pull some stuff that we need so it's going out to the internet and we make this hidden folder called terraform and it's actually downloaded a binary for the terraform provider and we have a lock file to lock what versions We are using so that's really useful uh we don't really want all that stuff there so I'm going to go get a get ignore for terraform and I just dump that in my uh my file there so we'll go ahead and just copy this I'll go over to here apparently I haven't created one yet okay well we'll make one now get ignore I'll ignore that as I really don't want to commit those binaries they're they're really large and they'll cause issues and so I just want to individually commit my G ignore so add a get ignore we'll commit that separately first and I'm just going to commit that and sync that um anyway so what we'll do here now that we uh initialize it we can go ahead and type in terraform plan so terraform plan is going to it's like kind of like the change set when we saw in um in cloud formation it's going to confirm what's going to happen so here it's saying and this is basically a change set that's what this is but we're see we're seeing okay we're going to create a resource here are all the properties on the resource uh it's going to not know until it actually we do it apply because it will we'll only know after creation this is going to created in our CA Central 1 as we did not specify a um a region but anyway this looks good so I'm going to go ahead and say terraform apply and that's going to go ahead and deploy that it wants us to confirm so we'll write in yes down below I like how terraform tells us by default to uh confirm it we're going to go over here to the left hand side look that there's now a TF State file if we look at this this file can contain sensitive information so of course you do not want to commit this to your repo but this is our state file if we lose this file we lose the state of it and we can no longer uh keep track of the state and that means that we'd have to manually tear down the um the file so make sure you do not get rid of this file you really need to keep it if not you'll have to be tearing things down manually within AWS so we have this let's make our way over to um I want to show you in cloud formation there is nothing in there because this has nothing to do with cloud formation so just be aware that uh we're in CA Central and there is no nothing here terraform is its own thing okay if we go back over to um uh s three now let's go see if that resource is actually there and I don't know what the name of this is it's down here below terraform and it has a big old number down below here so that's what it generated so this is um the thing that we created now the interesting thing is that we can manage other things with terraform we could actually even create um objects um we didn't do this in cloudformation because I don't think it's possible but in terraform you technically could create objects and do all sorts of things here um you don't really want to do that um often like if you're if it's data you really don't want to put that in I tools generally because um I is for infrastructure and sometimes you don't even want to put uh buckets in infrastructure really depends on your use case uh but um in what we're doing it's totally fine so that's all we wanted to really show for terraform let's go ahead and tear that down if you want to learn more about terraform I got a terraform course you can go through it I just want get you through the basics here so you're a little bit comfortable with it and you know how to at least get the CLI installed to work with it so that is our simple example it says it's destroyed it let's go back and confirm we'll give this a refresh says it's gone well there's an error because it's gone now so that's good and I say that would be good enough for terraform let's go ahead and commit our changes again make sure there's no State file there that's good uh we'll just say terraform CI simple ter form simple S3 example very good all good and we will see you in the next one I'm going to stop this workspace and we'll move on to something next all right ciao all right so we are continuing on learning about infrastructure as code solutions for S3 uh we did cloud formation we did terraform let's go ahead and do cdk so cdk which stands for cloud developer uh developer kit it is BU adabs development kit it is a way for you to use your favorite programming language to create infrastructure as a code and it produces claw formation underneath so that's the way it works um some people really like it some people don't like it as much I would like it more if there was a ruby um a ruby uh provider for the language but there is not so for me I'm I'm I'm disinterested in it because it doesn't have what I like but anyway what we can do is get started with it so I'm just trying to find the instructions here it's probably on GitHub I've done this many times but I always struggle a little bit with cloud formation or sorry cdk because it is a little bit of a pain to set up at the start but I'm going to go over to cloud formation cuz I think that I already have part of it implemented from before and I want to just tear that down so in here I have the cdk toolkit this is what you need to have installed um in order to work with it so and then we have the workshop stack so uh I'm going to tear down both of these I just don't want to have these in place but you shouldn't have these they you set them up once at least not sure about this one but this one you definitely only set up once um so I'm just want to tear those down this might take a little bit of time just give me a moment actually we'll just continue on as we're waiting here so cdk we have it for JavaScript typescript python java.net go the first language that came out was the typescript one so these are not always 100% up to date so you want to work with the one that is the latest we're going to work with typescript here today we'll go to the getting started and there are actually a few different ways to install the cdk CLI I believe that uh mpm is not just the only way but it's the way that I like to install it so let's go ahead and do that uh so we'll open up our G pod environment use whichever Cloud developer environment you want to use I'm just using this one because it's super easy um and it's easy for you to utilize as well so we'll open up that there and that's the first thing we're going to want to do is installed that CLI tool so we'll go down here uh you know I'm going to put it in my G ignore file or sorry my git pod yaml file and I'm going just say name we'll say cdk we'll put a before here and I'm just going to install this on line command here now I'm just doing this for later but I'm just going to install it in place I'm not going to restart my uh git pod just for this I'm just going to make a new tab here and we are going to install this so this is installing with with uh mpm and it's globally installing that's what the hyphen G flag is for so we can just write a cdk anywhere we'd like uh to utilize it or just cdk we can apparently initialize a a starting project which is a great idea I like that idea I'm going to just CD into our S3 I directory and I'm going to call this project cdk so we'll go here and I'm just going to paste this in I'm just going to change this to cdk so that it generates the folder is that and it's going to set us up with typescript so if we drop this down uh into our I we should see that folder uh unknown okay so I guess that's the the sample template I thought that we that was the name of it but apparently not and actually we have to create our folder first so I was wrong sorry let's do this and I guess we'll just go with the sample project not how I would do it but that's just uh how the cdk team rolls so go into here um I cdk we'll paste that in again so I guess that's the sample app it's going to do some stuff for us we'll just expand that it's going to make a bunch of stuff and we do get our own get ignore in here so it should ignore all the stuff that we don't want to have so now this project project is set up uh with mpm you usually want to do an mpm install it might have already done this but I'm going to do it anyway I short for install we can see that it's installed because we have a node modules directory let's take a look at what packages we have here so we have um a cdk library okay typescript of course it's configured to use it typescript can be a little bit of a pain to use if you don't know how to use it but uh in the constructs of this it should be very easy so the way it works is we have this cdk uh no this is just configuration for cdk it's in our lib directory there should be yeah here it is cdk stack and we can create multiple stacks and include them in here we're not going to go super complicated here but we have an example to create a simple sqsq and then it's creating a topic I don't want to do that I want to uh create some S3 stuff so I'm going to get rid of subscriptions and this here and it's very conventional so I'm going to assume this is S3 it autocompletes to that so that looks correct to me and I'm just going to go and start typing it in maybe it'll start Auto completing that'd be really nice oh it does this is so nice because cdk I believe has a um an extension see here does CD have an extension I don't know but it is auto completing which is really nice abs cdk oh you know what that's just typescript I forgot typescript does that so uh you know if you have a and there might be a plugin for it but that's just your us usual um typescript autocompletion so I'm not sure why it's autocomp completing but um I'm just now remembering typescript is really good about that so let's just start typing and see what we get so there's S3 uh I'm going to say can we do bucket no create bucket oh you know what we have to call this S3 otherwise it's not going to aut complete correctly there we go so we go here and we'll type in S3 create bucket nope just type in bucket here create probably pass the first in here probably the bucket name let's just go look up the docs I was trying to get away with it without having to look anything up not that it's that hard to uh check the docks here but we'll go to the API reference because that's always what we want to look at and somewhere here there should be um the St uh the uh S3 here so I'm it's really hard on the eyes here this font but we'll click here and we have constructs and so we'll go to bucket and so here we have a simple bucket so let's go ahead and copy this one I we could just type it and it would just autocomplete it's usually pretty good that way so we're creating a bucket here I believe this would be the L iCal name of the bucket that will show up in cloud information we'll make I'm just going to call it my bucket so it's more clear I mean by default it should block Public Access and it should use manage S3 and it should force SSL I don't want versioning so I'm going to say no we don't I don't think we have to include these things so I'm going to go ahead and just remove all this stuff and see if we can get away with it as it should have um intelligent uh defaults actually before we do let's go take a look at what the defaults would be so we scroll down here does it tell us what the defaults are no it does not uh what was the one that we saw that was specifying the versioning let's I just want to make sure versioning is turned off because it's a bit of a headache if versing is turned on whether this bucket has versing turned on or not the optional default is false okay great so I think what we can do is we can take all this stuff out here and just have this and we we're not creating sqsq so let's take that out we don't need a topic so this should be all we need to make a bucket and it should randomize our bucket name so it looks pretty straightforward and simple we aren't using duration here let's just take it out so it doesn't complain uh typescript will tell you exactly what you're using what you're not using we're not using duration so I'm just going to take that out and so we have a very very very very simple stack um but there's a little bit more that we have to do before we can start working with cdk so apparently we can just start doing cdk deploy but it's going to ask us for that uh that toolkit because if we don't have the toolkit it's not going to work so let's just try to go ahead and type in cdk deploy and see what it tells us to do so we'll just give it a moment uh bucket at my bucket should be created in the scope of a stack but no stack found oh am I missing something here I mean it's in a stack this is the stack right here we'll try this one more time I could have mucked up the code without paying attention here but it looks fine to me the Stack's right here my bucket should be created in the scope of a stack but no stack found what are you talking about it's right there let's look at the uh Hollow World example does anyone else notice the problem because I sure don't all right let's go ask chat gbt I'm getting an error about no stack for the S3 I really don't see what the problem is there because uh usually with cdk you have the stack and then you have the Constructor and you put in the Constructor and it should just work but clearly I'm missing something very very obvious so we'll just give it a moment oh of course it doesn't work when we need it help it does not work okay so just give me a moment I'll go figure it out I'll be back in just a second all right so the suggestion here is cdk synth and bootstrap which I said earlier that we need that um uh toolkit file so I'm thinking maybe it's just because the stack doesn't exist yet so maybe we need to run those two other commands I always forget with cdk but let's go ahead and just type that to get the full list here and we'll go up and so we have Bo STP deploys the CD cdk toolkit that's what we need uh and then synthesizes creating the template I think deploy will automatically do that so let's go ahead and do bootstrap and see if that solves our problem it's probably what it is and it's still airing out so there clearly is something wrong with our code um I'm going to go ahead and just comment this out for a second and let's go ahead and hit up and see if that helps it really doesn't like uh the way I'm creating a bucket now it could be that I'm just not assigning it to a variable it seems kind of silly if that's the case I'm just go here constant bucket new but anyway this is going to set up our toolkit in fact it creates its own bucket when when doing this and apparently it's rolling back the N the stack name CD cdk tool kit failed it needs to be Man created manually deleted from the itus console I thought we we did oh it it uh why did it fail the following resources failed to create why because this bucket already exists okay since we already have a staging bucket this is not a problem that you'll have this is a problem that I'm having because I already have this bucket so I'm going to go ahead and just delete that bucket problems unique to me so sorry we'll go ahead here and um it's not really helping we'll just type in cdk and say we have one here for CA Central 1 I'm just delete this empty the bucket permanently delete you know I would say that any time you're working with any language or anything with Cloud it's always a bit of a rocky start but once you uh get enough code down things go more smoothly so I'll go back to my cloud formation and we'll go fix that we'll go ahead and we'll delete this delete refresh great that's now gone so that was a lot easier we'll go ahead and hit up and let this uh try to bootstrap again I don't think this is our problem now I did add this part of the code here so it might still air out on this still complain about no stack found sure whatever and we're going to do cdk bootstrap while that's waiting I'm going to go back over here look at the example that was here and just see what I'm missing like we don't pass anything here it just be scope that's how we would know what to use and it's failing again um it's now saying something else already exists we'll go back over here to cloud formation we'll take a look at our events I I just deleted that bucket I just deleted that bucket I don't know what else it wants for me I deleted the bucket we'll go back over here I'm just going to go pause the video I'm going to tear down that bucket manually and I'll be back here in a second and I'll just try to run it make sure it works before making you watch me uh debug forever here you know what I think it might have been is see this it says May 20 2022 I think what happened is that I didn't delete the bucket we emptied it but it didn't delete it so maybe I just missed the step but I could have swore we did this yeah the bucket's gone good please just work it's just the staging bucket it's messing up on it must be that so yeah I think this should work there we go okay so this is creating uh fine no problem there but we still have this issue of they complaining about um the stack so that shouldn't be a problem because again it's going to find out via the um scope here that's how it knows this one says construct though and then over here this one says cdk app so why is that different I mean constructs are are ways of creating your own constructs and I mean we don't necessarily want to create a construct uh we just want to create an app or like a stack the stack is right here so what's going on so that environment's been created let's go ahead and let's try synth I'm going to just uncomment this here now remember we're following the getting started by ads so if we're having a hard time what's going on ads so we'll go back back over here so we did the sample application here notice this this says sample app this one says cdk doapp and this one generated a construct so the generated code is not the same based on what they said it would be so clearly something's changed and it's not us so I'm not crazy um what I'm going to do is I'm going to take out the construct here if we just do cdk app but we need to Port uh cdk it' be nice if we had like a full example as we don't really have that here maybe we can go and take a look at an example be nice if there was some examples because all I'm looking for is that um that inport for uh that one line there so I'm again I'm just trying to find some code here project structure and ah here it is this is what I was looking for and we'll paste this in here as such and we'll go back over to here I don't know if we have to do that because we can just say cdk app like that so I'm hoping that this will resolve our issue and we'll try this again we'll do synth all synth does is I believe it just creates Cloud information templates from what you're using the my bucket should be created in the scope of Stack no stack found what is going on can chat TPT are you back I need your help here oh thank goodness I I promise I won't complain anymore let's see here it appears that the issue uh code is related to the context in the S3 bucket is created specifically the scope parameter in the new scope it sto talking what is going on come on so we'll go here I need chat GPT today it's something to do with the scoping come on you can do a chat gbt so the Area Counties for my bucket um is due to the way the scope is you're passing in the scope to S3 Constructor which is expected to be an instance of cdk Stack however in the Constructor class scope is of cdk app okay so we can just pass in stack there for a scope but didn't the code say scope it did all right um okay let's undo this let's go back all the way to construct and I'm going to try this instead and we'll see what happens because maybe like this is the new way to do it because that's the example that's showing there let's try this again we'll try cdk synth I think if you did deploy it would do synth anyway okay now it's generating out um our CL information so that's what we should get we'll type in clear we'll do cdk deploy and if it works it works but just understand that you know when you're reading documentation it can be misleading or have parts missing so you have to kind of piece it together um and this is me using zdk like multiple times like we have a like in one of our boot camps we like used it for like over a week so it's definitely not a lack of my knowledge here but it's just again cloud is challenging you have to piece all these things together so it is creating a cloud information template it's it's uh doing that deploy we'll make our way over to cloud formation here I'm going to give it a refresh and so we here we have our cdk stack um not the coolest name but that's what it's called and it's created our resource so now we have our bucket and then it has some cdk metadata to help manage it some extra stuff that cdk has but yeah there you go that's the easiest simplest way to uh work with cdk uh of course we'll cover cdk more later on and and more concentrate when we need to do that but that's enough for us to get rolling here we might need to update our get ignore file actually we don't because it has its own in the repo post that's nice and clean so we'll say simple S3 cdk example and we'll go ahead and commit that and sync that but yeah I usually just use cloud formation because it makes things super easy or I use terraform but cdk is really good once you're working on large projects and you have a lot of programmers but that's it for this oh well you know what we should probably tear down the resource so I'm going to just go ahead and type in cdk here I'm so ready just to leave cdk here they're just forgetting to tear stuff down and there should be a tear down command uh cdk destroy so we'll go ahead type cdk destroy but you know one of the really good things about cdk it is easier to manage Stacks in it versus cloud formation cloud formation it's um a lot of work to string cloudformation templates together but there's a lot less moving parts so I have more trust in cloud formation so that stack should be teared down we'll go back over to cloud formation here good we'll at least cdk toolkit here totally fine to leave it there it's it's kind of like a oneand done thing but uh what we'll do is we'll go ahead and I'm just going to stop this in workspace and we'll see you in the next one okay ciao all right we're going to continue on learning different I tools for uh S3 and this time I want to look at palumi so palumi is an i tool it's different in that it allows you to use any programming language you like I say that with a big Aster because it does not have it in Ruby so for me it's shameful but that's okay there's still a lot of languages um so it's like cdk in that regards but the other part of it is that it works across the cloud so it's going to be like terraform in that sense so if you are looking for that kind of combination that is their offering I believe they also um host their state files in in their platform so that's probably how they monetize just like how terraform would do that but let's go ahead over to the documentation and get rolling here so we have uh some options we're going to go to AWS now it has the install and config guides but it doesn't really uh tell you exactly here uh how to install at least I didn't see it um cuz there is the palumi uh what we call this the palumi CI tool and for some reason it's kind of skipped over here at least I don't see it so that's okay I know another place where we can grab it so I'm going to go over to the howto guides and from here I think we go to static website hosting we should have those instruct instructions somewhere here or maybe it's install it's somewhere I know it's somewhere here yeah I'm not finding it but we'll go ahead and just type in plumi uh CLI install Brew because I know that's how it gets installed I mean there's a a a single line script I suppose we can use it again I'm just hoping for the Bruna I mean if we're using Linux we can use the single line command I suppose that's what we can utilize here I think that you can install it either way because I tested both and it worked but what I'm going to do is go over to the repo that we've been using uh thus far so that one is called um adus examples and normally I would launch this up in git pod however when I was uh testing this out I ran into an issue where this particular py Library just refused to install and um I narrowed it down specifically to uh the distribution that um git pod is using so uh in this case we're actually going to use GitHub Cod spaces so what I'm going to do here is go to code and I'm going to uh go ahead apparently I already have one running so I'm just going to delete this one I want to just make it very clear that uh GitHub codes spaces is different from gpod in that you have to really make sure you shut these things down um I mean of course with Git pod as well but the other thing is that when you launch up GitHub code spaces it's always launching up the last environment you had whereas git pod um it usually tears everything down so just understand that the Environ the way they run is different but what we'll do is go ahead and create a new code space on this repo of course use whatever uh Cloud developer environment or local environment that you want to use but you might run into issues and in particular with this one you could really run into issues so you know if it's working for code spaces for me you might as well give it a go a good Cloud engineer is comfortable using any kind of uh CDE so um you know using a mix and match is not a bad idea so when it first launches up I don't have a theme so I really want to change that otherwise my eyes are going to cry so we'll go here and whoops we'll bring that back over to here and we'll switch this over to GitHub dark so that is a little bit nicer there and what I want to do is go here on the left hand side into I we'll make a new folder here called plumi so pumi there we go and I'm going to want to CD into that directory Great and now let's go ahead and do that single on line install so we'll go ahead and copy that and we'll say allow and we'll see how that goes we'll just give it a moment there we go and now that that's now installed we can go ahead and use the CLI so I'm going to type in pumi I always want to put an end there but there it's not and maybe I'll bump up the font here is a little bit small over here on the defaults so let's just bump that up real quick here bring this up to 18 and we'll bring the terminal size up to something more reasonable there we go so we'll bring this up to 18 as well there we go I think that's much better um and so if we hit enter we should get all the commands assuming we typed plumy correct enter says it's not installed so I'm oh please restart your shell or add it to here um okay so I'm just going to restart the shell when I use Brew I didn't have to do this so I guess it's slightly different so what I'm just trying to do is Rerun bash profile should be at uh period bash profile maybe bash RC will do it let's try this again there we go so I just refreshed it there's lots of ways of refreshing but I found with um again Brew you didn't have to do that so let's go take a look here we have a bunch of commands and what we're going to want to do is create a new palumi project so we'll go ahead and type in palumi new and actually before we do that we can give it a flag called L and I think this will list out all the flags for possible templates so they have a lot of stuff here and we're working with AWS and python so they have here a minimal python uh program that we can uh utilize and the way we do that is we provide the third parameter as the template that we want to use we'll go ahead hit enter and right away it wants us to log into palumi uh with terraform cloud of course in terraform you do not necessarily have to create account right off the bat you might not have to do that palumi but I sure as heck don't know how to do that but we'll go here I'm going to sign in with GitHub and it'll just create me account really quick and you can see I actually just created a token here a moment ago but I'll go ahead and create a new token and we'll just say um uh pumi uh SDK S3 simple so we know what this token is for I'll end up rotating this out I know you can see it and you should not see my token but don't worry I'll get rid of it and it'll be less of an is we'll go back over to here and paste it in and hit enter now there's probably like an environment variable that we can set I don't know what that is at this point but that's okay and we're going to have a project name so I'm going to call this S3 simple SDK and I'll just say a python project for adabs S3 to Simply create a bucket we don't have to go too complicated here the stack name will be Dev we'll keep that the same uh the region we'll stick with us East one just make our lives really easy here I don't want to make things too complicated and it's just going to create a virtual environment so it is installing um underneath because I was doing this earlier it's actually installing this uh python Library so palumi AWS so we'll just wait for that to install shouldn't take too long and almost done there there we go so now that is working so it says to perform an initial deployment run palumi up now just I want to point out when you do that palumi command that palumi new make sure you are in your destination folder or it will complain but let's take a look at what it's generated out so we have our main Pi file and look at that it already is specifying a bucket now this is for creating Yep this creates an ad bucket that's exactly what we want so they've already uh made the work really easy for us here it's also generated out virtual environment uh for python so we have that we have a a palum yaml file over here for its configuration we have a requirements txt for python to say we're including that pumi ads and also palumi up here as well we also have a yamell file for uh development configuration so I'm assuming this would be uh the generic one and then we might have uh development and prod and other different kinds of environments or workspaces uh that you would have there so now that that is there I suppose we could do a pumi up we already have our environment variables configured so I believe that palumi will um pick up on those cuz we can see that it's talking about getting these set so I think that it'll already clue into those things so let's go ahead and just write pumi up and so we can view this in the browser it looks like it's pulled out my uh my my GitHub handle there but I'm just going to go ahead and click through on that we'll hit open and so here is making an attempt to deploy we have a failure says unable to validate the ad's credentials so normally if these things fail um it's because this doesn't have credentials so we have credentials locally but we don't have credentials on here that's my guess sometimes the credentials it depends on like who is the agent that is deploying so the agent is deploying here that could be the issue but I'm thinking the reason why is that we need to set those up in uh palumi so make sure you have your region set that's not our issue uh you can use plumi escape to set up Dynamic credentials with adus ocid that sounds great but I think really what I want to do is just go and configure this on the platform so that's probably what our problem is so we have our Stacks over here and this is one I made just a moment ago I never did anything with it I'm not sure how we delete Stacks let's go in here how do I delete you um I'm not sure but we can come back to that in just a moment okay so I'll click into this stack here so somewhere there's something that we can configure it's pretty common for most we do have environment variables this is environments I'm assuming no that's not environment variables that's environments so let me just take a moment here to figure out where we need to configure this I'm sure it's not too complicated all right so I'm thinking about it I I think I know why I'm having an issue and uh it should be really obvious it's because I don't have credentials set up on this code spaces environment because I'm used to having them in my git pod so what I'm going to do is I'm going to launch up this in G pod and bring over my uh credentials or I suppose I could just rotate out new credentials that's probably a better idea so I'm going to make my way over to AWS and we're going to go over to am it's probably a good idea to rotate them anyway so and we'll go over to uh adus examp samples and I'm going to go to security credentials and we are going to go ahead and well just create another access key no actually you know I'm going to rotate out the other one that's what I want to do here today so I'm going to go and just delete this one deactivate and delete out our old key here and then from here we'll create a new access key so let's say command line interface yes yes yes I know how to use uh these ke keys and so I have my keys here what I need to do is set this up in um GitHub code spaces I've only done this a few times but I think they call it Secrets yeah they say codes spaces manage user secrets so I'll click on that and you'll see that uh it's showing that I have some ads credentials here but I'm going to go to manage on GitHub so this is a lot easier and so these are um these are fine but these are for a particular repository so these can be tied to specific repos I'm going to go ahead and just delete this because these are really old here I'm going to add uh new ones I kind of wish that I kept them here so I could copy the names but that's okay so we'll go here and add some new secrets so this will be one for the access key so go back over to here and paste that in and I need to get the Nars name so I think they're they're shown up on here so we have the adus access key here and this is adus examples there we go so I'll go ahead and add that one make sure there's no spaces in front of it or behind it then I'll go grab my secret of course uh never share your secret with other folks I'm sharing it here with you because I know what I'm doing and I'm going to rotate it out so there is not an issue uh in the future for myself we'll go ahead and grab uh this one here and while we're here we might as well go ahead and set up um the the region as well so that was somewhere here there it is adus region and yes this is painful but uh we got to do it so I'm going to set this as CA Central 1 I know that we in our code we've overridden it as uh yes uh Us East one so that's totally fine we'll go here and grab that secret excellent and so we have um these three set on the correct repository we'll go back over here now oh it says your code space Secrets have changed so it looks like we can reload this environment so I'm going to click that reload to apply and it's now reload the environment excellent so we'll give it a moment here but um yeah codes codes bases is fine um G pod is a lot faster I find the configuration easier to use I like that uh it's always restarting your environment so you're having less configuration issues but uh you know they're both both great tools a lot of times when I'm utilizing Azure we'll use code spaces because of the synergies between Azure and code spaces since um mic owns both companies but I'm just going to close this tab out here and now we should be in better shape what were we trying to do oh uh plumy plumy plumy up or plumy deploy I think it's plumy up right and we have to make sure we're in the correct directory as it's moved us somewhere else Plum me up there we go and it says do you want to perform this update it says create two it's going to create the stack it's also going to create the bucket I'm not exactly sure what this would show up as a resource in AWS I assume that's just a resource in plumi and we'll say yes and notice it's saying performing an update even though the deploy was not successful it still has that stack there and we can go click through into the browser and take a look here and let's take a look at what we have so that's interesting it must be that the um uh plumi must have the agent on the uh client so terraform Cloud allows you to have it either or you might be able to set it here but I certainly don't know where that is um so I'm not 100% sure but uh we can go over here and we can see outputs configuration this is what I thought that it was asked like we' have to set something in a Secrets manager environment variables or something but uh we have our bucket so there's our bucket let's go over to to AWS we can probably click here to uh well let's see what happens if we click into it I thought it would this would open AWS but it's actually up here so we click there and it's bringing us to us East one and there is our bucket so yeah not too shabby uh so I think that's all good let's go ahead and tear this down I think that's good enough for this example but uh they give us the code here so we'll go ahead and grab that let's this deletes the stack resources I actually want to delete the whole stack so that's what I'm going to do I'm going to assume that it's going to also tear down uh everything when we do that so we permanently delete the dev stack uh please cons confirm if that's what you'd like to do we'll type Dev now the question is is that going to tear down the resources or is that separate make sure that Dev the dev stack is the one you want to destroy um Dev still has resources okay great so that answers that we actually do have to um tear everything down first so let's go ahead and do that it looks like we don't even have to specify um uh these these commands are suggesting we don't have to specify the stack handle here but I suppose um if you have lots of stacks it's good to have that there we're going to go ahead and perform that there yes excellent and we'll go back over to um here and we'll go now tear down the stack and we'll type in the please confirm by typing in the stack name I think it's asking us for and now the stack is removed we'll go back over to plumy notice I didn't have to enter my credit card or anything in so uh it's pretty easy I still have this stack laying around so I suppose if I want to get rid of it um and again this is not an issue that you you'll have but it's an issue that I have but I can get rid of this by typing the test stack here so I just don't want things lingering around we'll type in Dev oh I have to type the full name in here Omen King test Dev enter there we go go back over to plumi we'll make sure those resources are gone and this is a reminder that if we didn't clean up any kind of buckets before and you see them of course you can go ahead and delete them you don't need to keep buckets around forever here there's nothing crazy we're doing with these S3 buckets so I'm going to go back over to here and um I believe it's gone I don't see it in here so we are in good shape but uh yeah so that is pollum me so before we leave let's make sure we shut down this environment I'm going to go to the command pal in the bottom left corner and for this I'll type in code spaces and we'll carefully read to look for there uh that stops this say stop current code space we'll go ahead and run that that's going to shut down that environment make sure you shut that down don't walk away make sure it's shut down if you're not sure if it's running over here it will usually show up here um as running environments but I believe since is closed it should not show up anymore let's go here notice it is still kind of there but it's not running anymore that was the new one the super uh duper dollop not a huge fan of the names so that's what it is but yeah there's palumi so of course we didn't do a whole lot of coding but um it did all the work for us so that's that's it ciao there is a lot going on with bucket just as there's a lot going on with objects what I want you to know is that S3 buckets are infrastructure and they hold S3 objects we're going to be looking at a lot of stuff here like the proper way to name our buckets this is really important as um in ads generally you can name things and they don't have to be unique but with S3 buckets they're really strict with the naming uh there's other restrictions limitations things we can and can't do with buckets that we need to know there are multiple bucket types but generally when we're talking about buckets we're talking about flat or general purpose buckets um there's a concept of uh bucket folders now S3 doesn't really provide uh folders for general purpose buckets but if we're talking about the directory type it actually does have folders so we have to talk about that there's bucket versioning um there's encryption there is static website hosting and the other thing I really really want you to know is that even though S3 is a globally available service when you go up and look in the top right corner and you see what region it's in it says that it's Global the buckets are in a specific region so they don't exist everywhere uh and I think that's really important to know but uh some of these things like bucket versioning is also object versioning so really it's an object concept but we might set it at the Bucket Level but anyway we'll get into it um and so we'll cover some bucket stuff up front and then it'll be um sprinkled all over the place but let's get into it okay so when you go ahead and create a a bucket you're going to have to name it and those buckets follow very strict rules in terms of how you need to name them so S3 bucket names are similar to valid URL rules but they actually have more rules and this is because S3 bucket names are used to form URL links to perform various htps operations so an example of a bucket name here is my example bucket and notice that it's being used as a subdomain uh on the Amazon ads.com domain so for whatever is valid for uh DNS or URLs that's what has to be uh valid there but let's go take a look at all the rules so the first is length you can't have bucket names longer than 63 characters but they have to be at least three characters long um in terms of characters you can only have lowercase numbers dots and hyphens everything else not allowed for the start in the end they must begin with a letter or a number so you can't have any um exotic things things like a period or a hyphen in the front for adjacent periods you cannot have two periods side by side so you can't go period period for IP address formats they're just not allowed so if you have something that looks like this it's going to just say no for restricted prefixes you cannot have xn hyphen hyphen S3 hyphen or S3 hyphen configurator because they're used somewhere else where they're used I have no idea but adabs does not want you to use them as well as we have suffix suffixes that we're not allowed to use like S3 Alias or o S3 uh in terms of uniqueness the names have to be unique across all adaas accounts in all adaas regions within a partition um in terms of where it says within a partition who knows what partition it's in the point is is that it's like a domain name when you have it you have it and no one else can use it uh in terms of excl exclusivity a name can't be reused in the same partition until the original bucket is deleted they keep talking about partitions but the point is is that if you have a bucket name and you delete it now someone else can use it uh in terms of transfer acceleration they have one exception where you cannot have dots in the names and when we cover S3 transfer acceleration we'll we'll cover that again but the quickest summary or the easiest rules to remember is no uppercase no underscores no spaces and the bucket names you remember those three things it makes it very easy and everything else is pretty Common Sense uh other cloud services like Azure everything is a domain name so you're probably wondering why are you making such a fuss about this because it's not the norm in ads to have these um these URL like names for uh for naming resources so I need to make extra emphasis on this one but we'll look at some examples to really hit it home in the next slide here okay okay so we have some examples for valid and invalid S3 bucket names and the reason I really want to hit this home is that you might have exam questions which ask you whether uh a bucket name is invalid or not and you should be able to spot one if there's something that looks kind of funny with it so let's go through it the first one will be this one where we have my bucket hyphen w123 no issues there as that is a valid name uh this one is not valid because it is is in the format of an IP address as we said is not allowed this one is not allowed because it contains title case or up some uppercase characters in there this one is not allowed because we have a repetition of two periods this one is okay as it doesn't have numbers in it but it is totally fine in terms of length and size this one here is not allowed because we can't have that xn uh hyphen hyphen in the front of it this one is not allowed because it has an underscore which is not a valid character this one is valid because it is it just looks like the other ones this one here S3 hyphen in the front there is not valid as it is not allowed uh test. bucket. data that is totally fine then we have new hyphen bucket hyphen S3 Alias so that one is uh not valid because of the ending on there and the last one is totally okay so the only thing I didn't show here was like funnier characters like maybe at signs and stuff like that which would not necessarily be valid I don't think you can do that so you can't do at signs you can't do pounds you can't do dollar signs so just understand that I didn't point that one out but um it's very simple in terms of what you can utilize and hopefully that really hits at home but just try to remember the three rules which is uh no uppercase no underscore no spaces okay we'll see you in the next one ciao let's take a look at some bucket restrictions limitations this is not the exhaustive list as we will cover that later in the cheat sheets but for now I just want to get some um top level information to you so you can remember this as we're working through the labs and the lectures here the first is that you can create up to 100 buckets within your adabs account if you need more you can open a service request limit to 1,000 buckets it's pretty easy to manage 100 buckets uh i' I've I've definitely filled out 100 buckets before but mostly it's because there's a lot of applications that can um randomly generate bucket names for you and you might have a bunch of junk buckets so you just have to better manage it often you do not need more than a 100 buckets you need to empty a bucket first before you can delete it this will be something you will see again and again and again in Labs that we'll have to empty our buckets so make note of that there is no Max bucket size and there is no limit to the number of objects in a bucket um there are limitations based on some things so for instance files can be between zero and 5 terabytes I want to make an emphasis that you can have a zerobyte file uh and you can have a a file as large as 5 terabytes have I ever attempted to upload a 5 tab file no but the documentation says that you can do it uh files that are larger than 100 megabytes uh iTab us recommends to use multiart upload which we definitely show in this course um and uh the CLI does make it pretty easy to do uh for S3 for a Outpost it has limits uh we'll talk about that uh in The Outpost section which is separate from S3 where we talk about all all of output services and the CH the differences there but the idea is that Outpost allows you to run a rack of servers which is inabus Hardware loaded with inabus software and it has the S3 service loaded onto it and so it just cannot do the exact same thing as the cloud version of it but it's still very powerful to have if you can afford it adus uh says in the documentation that for get put list and delete operations they're designed to be highly available but for create delete and configuration option operations you should run them less uh it's odd that they would say uh that this is highly available for the delete but don't run the delete very often but I guess the idea is that um you know they're talking about high frequency of configuration changes or uh high frequency of these things so you know if it's reads uh reads or updates totally fine if it's these other things then you know you might run into issues but uh again I've never had issues but it depends at what scale you're working at but the things I want to take away from this is 100 buckets a thousand buckets you got to empty that bucket first uh there's no Max bucket size and no limit on the number of objects it's good to remember this as well zero to five terabytes that definitely shows up on the exam they love to hit you with that one there but there you go okay ciao let us take a look at bucket types because apparently adus has more than one type for S3 there used to be only one but adab us recently introduced at least when I made this video a new storage class and that necessitated a new bucket type but let's talk about the original bucket which is general purpose buckets so for this general purpose bucket the data is organized in a flat hierarchy um that means there are no folders uh there are folders in a sense and we'll talk about that um in a future slide here but the idea is that everything literally sits in one folder uh one namespace this is of course the original S3 bucket type so whenever you are spinning up uh a bucket this is what it's going to be using unless specified otherwise and it's recommended for most use cases and because it is it's going to work with All Storage classes except for one in particular called the S3 Express one zone storage class there aren't prefixed limits uh uh there is a default limit of 100 General buckets per account which is that limit that we talked about uh earlier in our restrictions and limitations but let's talk about the the new bucket type which is directory buckets and as the name suggests it organizes data into a folder hierarchy which is a little bit different for object storage and it can only be used at least at this time with the S3 Express one zone storage class I'm not sure if adus has more plans for this or if they just had to make a special bucket type just for this storage class um this is recommended when you need singled digigit millisecond performance on put and get and that's exactly what that S3 Express one zone uh storage class does and we'll talk about that when we cover it in another uh slide uh there aren't prefixed limit limits for directory buckets so it's really interesting because you know they say there's no I mean I know there are no limits but um I put an aster there because I'm not 100% sure why we need to point that out but it seems that there could be some kind of limitations that I'm not aware of as the language is indicating but for my observation using uh both bucket types I haven't seen any limits so I put an aster there just just in case there's more to investigate uh individual directories can scale horizontally so that's really important to understand because now we actually have folders and when you have a flat hierarchy it's very simple to understand that that's going to scale so do the folder scale yes they absolutely do there's a default limit of 10 directory buckets per account can you use a service limit to increase it possibly I didn't look into it again it's very very new uh uh when uh this feature came out from the time I'm recording this so I don't have all that information here but the most important thing is to understand there are these two bucket types and the takeaway is directory buckets are for S3 Express one zone storage class okay there you go if you remember earlier I was talking about how S3 uses a flat hierarchy um in generally when we're talking about general purpose buckets but when you go to the adus console it allows you to actually create folders and I put is in uh parenthesis because uh you actually aren't creating true folders found in hierarchy systems What's Happening Here is in the S3 console when you go and you create a folder it's actually creating a z byte S3 object with a name that ends with a for slash which indicates that it is a folder and notice that screen grab from uh the S3 console it says folder uh as the type notice that the size has nothing there it has just a a hyphen indicate zero size so S3 folders are not their own independent entities but they're just S3 objects S3 folders don't include metadata permissions and all these other things um S3 folders don't contain anything they can't be full or empty S3 folders aren't moved S3 objects contain the same prefix are renamed so if this is confusing uh hopefully showing this little code example will make it even more clear so let's say we're using a C and we're trying to list all the objects in a bucket so I created a bucket here called test bucket ab123 and I created only two files I created a folder and then I put a file in that folder and so what it's returned back to me is a object with a key of folder name forward slash and notice that the folder is of size zero and then down below this is the object that's in that folder notice that that object has the prefix my folder for/ file name it's not in a folder it's just named to have that uh prefix as part of its name and so uh that's what we mean when we're saying it's a flat hierarchy so hopefully that makes sense there but uh yeah that is how uh bucket folders work in S3 when we're talking about general purpose buckets there is a lot going on with S3 object um but what I really want you to know is that they represent data and they are not infrastructure that matters because adus has um things like ion permissions and resource tags and those work at the infrastructure level but this is not an infrastructure thing and so you're going to have alternate services that seem like they we should just use the other ones but they have to be separate Services there's also just a lot of stuff going around um or that is around s objects that we should know so let's take a look at the list we have eags this is a way to detect when the contents of an object is changed without without downloading the contents we have check sums this ensures Integrity of a file being uploaded or downloaded we have object prefixes this simulates file system folders in a flat hierarchy there actually can be folders when we have um the directory bucket type but for the most part uh just pretend that folders don't exist uh we have object men data this allows us to attach data alongside the contents to describe the contents of the data we have object tags uh this gives us kind of the benefits of resource taggings but at the object level we have object locking this makes uh data files immutable we have object versioning this allows us to have multiple versions of data files uh there's definitely more than just this but this is some of the most important things now let's go dive into these things okay let's talk about eag so eag stands for identity tag and it is a response header that represents a resource that is changed without the need to download the value of an eag is generally represented by a hashing function such as md5 or Shaw one eegs are part of the HTTP protocol so it's not an ITA specific thing it's just a broader concept of HTTP eags are used for revalid validation for caching systems so caching services like cdns and things like that definitely make use of eags so s objects have an e tag if you query uh using list objects you will notice that there are e tags uh as it is uh indicated right here in the uh Json output eegs represent a hash of an object eegs can represent whatever uh the person wants them to be but ads has decided it represents the hash of the object or object contents it reflects changes only to the contents of an object not its metadata and we'll talk about metadata here in another slide it may or may not uh be an md5 digest of the data object it depends if it's encrypted or not but for the most part it's using md5 e tags represent a specific version of an object as we will learn uh S3 allows you to have object versioning e tags are useful if you want to programmatically detect uh content changes to S3 objects that is the primary reason that I find eags useful um and so we might come across a lab where we'll demonstrate that but that's what I want you to take away from this e tags it allows you to uh see the changes to content without the need to download them so there you go hey everyone it's Andrew Brown and in this fall along we're going to play around with eags in S3 to see how we can uh best utilize them so we obviously have this ous examples repo I've opened it up in G pod you use whatever you like but this is where I'm going to be working and I already have my environment variables loaded up so that I can uh use the a with CLI we are going to be using terraform we do have a separate video on how to set up the terraform CLI the instructions are also in perer in this file here as um uh the script what I'm going to do is go ahead and create a new folder in here I'm just going to call it eags and basically the the thing that I want to show you what you can do with eags is to use it to programmatically detect when changes occur because that is basically uh the good use case for it so I'm creating a new main TF file here and I want to write some uh terraform so what we'll do is we'll make our way over to the terraform registry where we can get some configurations now if you're watching this in the future inabus is always changing or terraform is always changing the way the inabus register works so you might have to fill around with this to get to work but hopefully it works the first go here we have the provider I'll go ahead and grab that here paste it on in and so we have hash 8s uh 5.30 etc etc there's two things I want to do I want to create an S3 bucket and I want to create an S3 object so in the documentation here if we type in S3 we will find the resource at a S3 bucket and we can go ahead and copy this so you'll notice that it is um supplying the uh name here but if we take out the bucket name it will randomly generate and we don't have to do anything for it just going to call this bucket so that it's straightforward actually we could just call default I actually prefer that um do we need any metat tags on there no no thank you so this should go ahead and create a bucket it should pick up my ads credentials locally um because I've set them via environment variables that's one way of pulling them up you might have to do some additional configuration here which would appear if we go to the documentation uh it's usually the first page here so if you go right here and read about it you can see the different ways that you can authenticate but again I'm using environment variables it's the easiest way to do it but you're using configuration file you'll have to do maybe something a little bit different like configure uh the profile to be used so if I type in here yeah right there so it says profile and chooses It Anyway let's go back over to S3 and in here we should be able to create objects directly now in infrastructure as code you're not really supposed to use um create uh objects because they're kind of a data component not infrastructure but there are use cases where you might want to utilize them and it makes a great example for E tags uh what's this multilanguage provider script oh you can write terraform in Python now and other languages constructs that's for cdk terraform stack okay sorry it's just new I never saw that there before but I'm going to go back over to terraform default and here it's saying that uh this resources is deprecated and now to use this one here so we'll go ahead and type that in and you really have to watch out for these changes because they do this all the time uh change things but anyway so this is going to help us grab this information here I paste this on down below and so the idea is that we have a bucket and we want to create an object within it so here this is going to be uh resource um adabs S3 bucket default and then the key name here is just going to be my file.txt and then we can supply a source so I'm just going to make a new file here we'll just say my file.txt hello Mars and we'll go here and say I think this is relative so we can probably just type in myfile.txt and it should pick it up relative to this file here and then we have our e tag now we shouldn't have to set an e tag pretty sure it will automatically set one for us but it looks like we can generate one out ourselves so I don't really want to um change the tag but what I'm going to do here is let's go ahead we'll just comment this out here let's go ahead and deploy this and so what I'm expecting to get is a bucket and object in it right so I'm here on my terraform tab I have terraform CLI install if I type this in I mean I should are you serious where's my terraform C okay um you know if it's not there I'm just going to go ahead and just make another folder here this will be great for the future but I'm just going to go ahead and just go terraform CLI install you'll already have the script because you'll be able to see it but I'm just going to modify this because I actually like this to be a different way to be honest just going to go ahead and copy this and place this in here and I'm going to bring this over like this and I want to create this as a bash script so I just need um this header here okay this is just going to allow me to install it in isolate so I'm just going to go ahead and do that bin terraform CLI and we'll go back over to wherever this is this is in um somewhere here here we go and so instead of doing that I'm just going to write run source bin um terraform CLI install sh and that way uh that I don't have to uh I can keep this kind of a bit thinner this is this is a really good way I should have done that with power on all the other ones but whatever and I'm going to go ahead and just do that so let's go ahead and run that see if that installs it while I'm here I might as well just do that for the rest ads C install Powershell C install sh and yeah if you're watching you're like Andrew these videos are a little bit inconsistent well it's just the life of a developer you got to figure things out and work through these issues but I'm just going to go ahead and just do this now to get these out of the way go ahead and grab that not what we're here to be watching but that's fine save that bring that back a level go here bring this back a level and I'm just going to go ahead and just go bin power shell CLI install.sh okay I mean this this is really nice because it's a lot more modular adabs CI install there we go there we go that's nice anyway um and I'll just chmod all those all those there b and Aster great so anyway uh we should have terraform CI installed and we do look at that no problems there and what I'm going to do is type in terraform AIT that's going to initialize uh the project and download anything that we need for there I think our get ignore already ignores terraform save files make sure you're ignoring all this stuff just type in terraform get ignore and get all that good stuff from GitHub so that you're ignoring stuff and not committing these to your repo but anyway um we've done a terraform a nit this is in the wrong directory so I'm going to have to S S3 into the E tags here or CD into it we'll type type in ter from inip because it's expecting to find that main TF file it's downloading the providers and the required stuff that we need I'm going to type in clear I'm going to go ahead and type in terraform plan so this is a very simple script we shouldn't have any issues with it uh requires explicit configuration add a provider block to to the block uh there now there's nothing really we need to configure so what I'll do is I'll go back over to here I'll type in adabs uh here as the provider block and it should now hopefully work no doesn't like that I guess I supposed to put for writer in front of it so in here this is where you would uh configure things like uh other configuration methods or the default profile I'm not going to go ahead and do that it's all going to default the ca Central one anyway and uh I mean this should work retrieving a Account Details validating provider credentials um 403 line 10 what's your problem here I'm going to make sure that uh I'm actually connected to my ads account so we'll ads STS and I'm going to hit enter um I'll do this on my ads command here so I can get the auto completion I can never seem to spell get call or identity I always mess it up somehow see even when it was Auto completing I still messed it up I Dent to T there we go and so it looks like there's um a configuration issue here so give me a moment I'll go ahead and fix it so I just loaded my credentials in but um uh maybe I did not export them so just give me a moment okay all right so I ran an issue it looks like I copied the um the example key and that's why it's not working so I'm going to have to go regenerate my credentials load them back in here I'll be back in just a moment all right so we should be in better shape now I'm going to just double check here get caller identity and so now I am logged in so I can go ahead and do my terraform plan and so that should allow us to create an object in a bucket so here it's going to say We'll create a a bucket that's great and then incorrect attribute value type for here um maybe we have to put name on the end here like this go ahead and hit up um I mean that should work the object has no argument name okay what am I doing wrong here it should be pretty straightforward oh you know what I'm putting the word resource in front of it here we don't need to specify that I don't think let's try this again or if we do yeah so their example they don't do it so we'll try this again okay we'll take off the name okay what does it want dat accessory bucket default ID well I mean this one's using an ID maybe it wants an ID instead we'll try that I mean name and ID should return the same thing there we go and maybe maybe I'm mistaken but when you look at uh this stuff here we go down below it'll tell us what they are so ID I guess it's ID I thought it was name and we don't need resource in front of there so what this should do is it should pull up this file and send it to S3 it's going to create these two objects we're going to go ahead and in terraform whoops uh and this got changed back to here so we'll say terraform apply Auto approve because I don't want to hit the yes I just wanted to go up there and upload it and so it's going to go ahead and create it now remember the state file is being created locally so make sure you don't leave and delete the state file or you'll have to clear this out manually so now that that is done let's go take a look and see if we can actually see that object uh in there so I'm going to say itus S3 uh I cleared that out so I don't actually know what the bucket name is now but we did did write a command earlier to list out all the recent buckets there's one here somewhere if we go to uh our common commands this this is earlier but we have some bash scripts and we can say get newest bucket so this is the command that I have here go ahead and grab this it's in the repo you can just get it too and so what that will do is return the last bucket so this is the randomly generated bucket that we have here so I'm going to go ahead and grab that and we're going to say uh adus S3 um LS and I'm going to say S3 slash and then we'll paste that in there and hit enter and we'll see if we can see the contents of the file so there it is we want to go ahead and download that and see if we can um see its contents so we're going to go and type in ads S3 API um get object we'll say bucket and from there it's actually easier than that we just do Aus S3 so Aus S3 copy and we'll say S3 SL this stuff here like that and I'll say SL myy file.txt and I think that if we pipe it to cat it should just print it out um and the problem is I just have some additional duplication here I think ads S3 CP no no that's right okay maybe I'll just do my file.txt like this okay definitely download the file um so I just wanted to cat it out let's just go ask chat GPT how do we download a file from S3 and print it straight to the terminal I could have swore what we could do is just pipe it like that okay in one line using the using the ad CLI oh it's it's this okay so I was kind of close and I don't think it supplies a name yeah so normally what you do is you supply a name as the third parameter that you want to download the file as instead what we're doing is you're just putting this hyphen and then it looks like it carries it over uh to the cat and then it prints it out okay so it's down the file and so we can see that's the contents online all right and um so that's pretty clear so the thing is is that what we want to do is we want to see if we can update this object so I'm going to go ahead and change the contents of this file and say hello world and I'm going to go back here I'm going to do terraform plan so I've changed this file right I've changed this file does terraform know that I've changed this file and notice it says no changes well how can that be right I changed the contents of this file and it's saying that there's no structural changes because this has no idea what the contents of of this file is okay it has no idea what it is so we need to tell it that it's changed and so the way we do that is by um uh generating out an md5 hash of the eag and and supplying it uh and and that would probably be the best way to do that so what we'll do is go over here and let's go back to S3 object and notice here it's saying eag file md5 file to path because that is what the eag is going to be it's going to be an md5 generally of it it might vary based on how adus generates it out but it's saying let's get the file generate N5 and assign to the E tag and that's how we'll know what it is so what we'll do is Ty my file.txt and so now we'll try this again we'll do terraform plan and it will notice the changes because ter form is going to uh now track the eag change and that's how we're going to be able to have this object State um track anytime we change it so here it's saying oh the eag has changed because obviously we generated a new one here and so I'm going to go ahead and say terraform apply and we'll say Auto approve and we'll go ahead and deploy that could we do this with cloud formation probably not cloud formation S3 object okay notice like I'm trying to look up object and I'm not getting it so um the thing is is that um there is no cloud formation for an S3 object and the reason why is that an object is data it's not really supposed to be part of infrastructure but terraform can make anything uh anything as long as an API uh man manageable as if it's infrastructure and so this is just one of those edge cases but the reason I'm showing you to you this is because it's is a good use case for E tags so anyway it's it's done a change here and so what we'll do is we'll go ahead and make sure this works by changing it one more time so we'll say hello Jupiter and let's see if it um picks up those changes okay and it notice that something has changed right this isn't random it's every time we use md5 it's always going to come with the same value so if something changes in here it will change the E tag it allow us to track it so that's all I really wanted to show you for eags as a practice IAL example I'm going to go ahead here and just commit this as my eag example so we'll just say refactor uh refactor B scripts and um provide eag example there we go we'll go ahead and commit that I'm going to just uh sync that there or make sure it goes up sync sync sync and I'm going just do a terraform destroy I probably should hit Auto approve there because that's going to tear down that bucket and that object since I don't need it anymore we'll just say yes tear that on down and we'll just give it a second here to tear down still destroying we'll give it a second okay there we go it's done and I'll see you in the next one okay ciao so what is a check sum well it is used to check the sum or amount of data to ensure the data Integrity of a file so if data is downloaded and if in transit data is lost or mangled the checks will determine if there's something wrong with the file uh this sounds very similar to eags which we just covered um the difference is that eags is for checking if the contents is changed where check sum is to ensure the data Integrity if something is uh wrong with the file so uh seems very similar but they do different things Amazon S3 uses check some to verify data Integrity of files on the upload or download and adus allows you to uh change the checkm algorithm because you can use different algorithms depending on what your use case is and the ones that we have available to us is crc32 crc32c Shaw 1 and Shaw 256 so hopefully that is clear that check sums is for data Integrity eags is to see if the contents of a file has changed um but there you go hey this is Andre Brown and in this video we're going to explore additional check sum so the idea is that when you upload a file how do you ensure the Integrity of the file well um by default it's going to use md5 and so an md5 hash is delivered alongside with your actual S3 object and that it's confirmed by AWS and this happens all by default there's nothing that you have to do but let's say we want to um upload an object and change the way um the check sums work so let's go ahead and I again have this adus examples repo I'm going to make a new folder here and we're going to call it checksums and inside here I'm just going to go and uh create a readme.md and we can start getting things out here so the first thing is I want to do is create a new bucket so we'll do here um just put some mark down here so we have some documentation we'll say adab us S3 make bucket S3 call SL slash um we'll say check sums examples AB that's for my initials and then just some random numbers so that's the bucket I want to create so I'll go ahead and do that of course I have thec installed so this uh goes really easily itus S3 MB make bucket no such file am I not in this directory hold on let's try this again says I already own this bucket maybe it did work and it just mucked up 24 34 is that it okay so created I'm not sure why we got an error there but um clearly the bucket was created despite it's saying it wasn't created we'll also need to uh create a file so let say my file uh create create a file that we will do a check sum on and the idea here is going to be say Echo hello Mars and I'm going to write that to a file called myfile.txt okay so we'll do this and let's convert this to a checks so turn the uh get a check sum of a file and I think it's for for md5 so there's probably like a a tool like check some for md5 of file there's probably some Linux command Linux Linux Linux Linux Linux Linux Linux show me something here I'm just trying to find yeah md5 sum that's what it is because I've seen it before so if we type in md5 sum and we Supply our file this is what it's going to get back for us right so we'll go ahead and um put this code here so we can see so now we do know that when we upload a file technically e tags use md5 so I'm actually kind of curious whether this is going to match there might be some other additional information that they Supply when they md5 and so it might not be a onetoone match but let's go take a look and see what happens so upload our S3 file our file to S3 so here we will go ahead and grab our bucket I'm going to keep it simple and just say um S3 copy and say my file to txt like this um I kind of want more information than just uh the file name so I'm not sure if this is going to return back to us the E tag well we'll try it anyway we'll see what happens yeah so it didn't return back the E tag which is kind of annoying that's why I should have used the the fancier one there the um uh the one by the um S3 API the put object but that's okay so I'll go here and we'll just go get that object now so say Aus S3 API get object and we'll say bucket here and we'll grab this and then we'll say uh key and this will be my file.txt and the idea is we just want to get that e tag go ahead and hit enter uh maybe we don't need the double hyphens thought we did though no try this again get object in fact I don't even want the to get the object I just want the head of the object but let's go ahead and try this it's saying something's wrong with my command oh there we go that's fine and so now we're getting back that e tag and so notice that it's 8 e d 2D 37f so that the eag is actually the check sum so technically you can do data Integrity with the Tech but you're really not supposed to um that's not why we're I mean like you can but it serves a dual purpose uh but let's say we wanted to use some kind of different um check sum right so we can change that and itus actually has a tiny tutorial here and the idea is that when we upload our fil is when we can change our check sum check and what they're suggesting is that what you do is you say I'm going to do additional check sums and then you're providing the precalculated value so that they know what to check against when you you upload it so let's see we can figure that out using the CLI so we have uh put object which is the more verbose one cuz obviously CP is not going to be enough here so okay we'll say let's upload a file with a different kind of check sum and let's see if we can actually do it here via the CLI so looking at our flags I'm just going to zoom out here so it's a bit faster to find this stuff um what I'm looking for here is some check some example ah here it is okay so we have algorithm and then some other options here so I'm going to just jump over to this indicates the algorithm used to create the check sum okay uh so I'll grab this one flag and I'm not sure which one we should use I'm not that familiar with all these other ones but let's say we get CRC 32 I hope I do not regret choosing that one and so we'll Supply that as the example this header can be used as a data Integrity check to verify the data received of the same data originally so I'm assuming that this is the uh the actual additional string that they're asking for that we want to compare against so we'll go here and do this and so what I want to know is how can we do a CRC check so we'll go here how can I uh convert a file to a check sum for this in Linux and apparently that's the tool we can use so we'll go ahead and hopefully that is just preinstalled and I'm going to go ahead and try this so we'll say my file txt and we'll paste this here o is an invalid option okay the option that specifies you want to calculate that so I'll go hit enter and see what it says does not have an O option um I mean I don't know if this is installed on this machine it's not but op SSL should be so let's do that open SSL is almost installed on all Cloud developer environments again if you're using your own local development you'll have to figure it out yourself well let's go ahead and try this instead invalid command does not provide it so I'm going to go ahead and try to install this so clear this out and I'm going to say pseudo apt get install crc32 and it looks like I've been able to install that and we'll go back here and try this again so my file.txt we'll hit enter it's saying the command is not found um why did I just install it if the command's not found oh it says unable to look at the package so okay how do I how do I do that um so let's try this again crc32 Debian because that is what G pod is using here uh command and so so I came up with this although it's a little nasty should work on most Platforms in modern Linux anyway okay um is that really what we want to do well let's find out if that works hey if it works it works right um where's the file how do we Supply the file I mean that's the string so here we're providing um the string but that's not for the particular file so it's not necessarily a good example I don't really like that cr32 okay let's go ask them how do we install crc32 on Debian okay we definitely did that app get install and app get are the same thing or app install one's just shorter form so I'll try this again the only thing I didn't do is I didn't update so usually you should do that before you try to install things I was just trying to save myself some time and that could be the reason why it's not pulling it because there could be uh a later one here no it cannot find that package give me a second okay all right some people are recommending rash as a package I'm going to go ahead and try that so we'll do um well is it already installed rash no it is not so I'm going to say R yes I mean that installed so that's a good indicator I go back here and change it really silly with that we have to go through so many iterations but that's sometimes how it is with Linux um and then I'll go back over to here and this one will do this I just need the the check sum of a single file here here so we'll go ahead and paste this in here as such and this will be my file looks like it can do multiple files if we want let's go ahead and try this out okay there we go so we have our hash and so now we can go back up to here and I suppose we're supposed to supply the value here as such right so say Ad us S3 put object and we'll want the bucket here and we'll want the key and we'll want the body so the body will be my file.txt the key will be my file.txt this is going to be the bucket here I'm actually going to make this one different I'm going to call this one crc32 because that other one might still be there and let's see if this works again never ran this before but I have confidence that we can get this to work we'll go ahead and hit enter it doesn't like something that I've written here um I don't know but I'm just going to just to be consistent here I'm just going to go ahead and kill that out say equals and just wrap these up so they're all consistent we don't want to give it any reason to complain so I think the reason it's not working I I spotted this while writing this here is because I forgot to put S3 API in front of here try this again hit enter uh body blob values must be a path to a file well it is um oh it's not an actual file sorry I want the key to be called that and not the actual uh body so we'll flip that around there and we'll try this again we'll go ahead and hit enter and it says the specified bucket does not exist it doesn't I'm pretty sure it does maybe we lost like a character yeah we lost a number there on the end so we'll try that again we'll go ahead and we'll try this we'll give it a go so it's saying that the check sum is invalid so that's interesting we'll try this again okay if you don't use Simple formatting option then the rash will default to a default a different format and this may not be what you want well maybe it is what we want so let's go ahead and take out simple and try this again generated by the key the key difference I noticed is that this one is all uppercase so what happens if we do this is it case sensitive that'd be interesting to find out try this again an error has occurred okay give me a second all right another idea I have is maybe we can create a file in here in Ruby Ruby's already preinstalled in here I'll just say cc. RB and we can just grab a bit of code here like zlib H which I believe is a standard library in Ruby and see if we can get the output here so I'm going to go here and just say puts zlib input field value so I really just want this file so I'm going to say file. read myfile.txt and I think that this should work this is why it's really important to know programming for cloud because sometimes you got to work work around these problems and coding is super useful for that so I'm going to type in Ruby let's say cc. RB it doesn't know what Z lib is um I thought that would have been built into Ruby that's totally fine if it's not so I'm going to make a gem file here say bundle a knit that'll create a gem file for us I'm going to say gem zlib save and I'll do bundle install another reason why you should use cloud developer environments they already have all this stuff here so now we'll go ahead and do bundle exec Ruby um CRC so that way it'll know to execute in that in that context there I don't want to put Z lib I just want to do crc32 was that the ER we had before uh we probably didn't have to do a gem file it was just because I I wrote the code wrong and so this is the actual value we're getting so maybe this is better um so this did not work out so I just take this out so we'll go here and just say bundle exec Ruby cc. RB and type clear here and we'll just try this again so let's try this value and see if it likes that we'll try this again hit enter no okay so we're not really having much luck with this uh CRC but we could try another one because Shaw I feel like is a lot easier to use use I don't know much about CRC and I'm going to have more confidence with something like Shaw one or Shaw 256 so let's try this Shaw one instead and we can still use Ruby for this we'll say um how to calculate sha one using Ruby for a file and so this one's even easier I don't think we need um to have the gem file here so I'm going to go ahead and just delete this and I'm going to go and try this instead save my file.txt and I'm just going to do this and say puts and I'll just rename this to sha one Che some we'll type clear and we'll try this again we'll say Ruby Shaw one check some no initialed file here doesn't like that I can try this file. path or file. jooin so maybe that will work here we'll try this instead no such file or directory my file do oh it's not t t t it whatever or whatever let's try this again maybe I spelled it wrong my file.txt and we'll try this again we'll type in clear go hit up okay that looks like something so I'm going to go ahead and grab this we'll go here and paste it in here we'll try this other algorithm uh sha one right did we replace that in our code we did good and we need to say that this is Shaw one please work I'm G say Shaw one and we'll copy it we'll hit enter it really doesn't want to work okay give me a second all right all right so as a sanity check I looked up what it would be to just run it using Linux and it's producing the exact same thing as our Ruby script so clearly we have the right value but it doesn't like something some we're doing something wrong here and it's not clear as to what it is so I'm going to carefully read here this header can be used as a data Integrity check to verify that the data received in the same data that was originally sent the header specifies the base 64 encoded uh for the object so there's something that is missing here is it not Bas 64 encoded let's look at the CLI here you can put the object I love how it's like look at the example and doesn't show you an example that's always great isn't it um here's a really complex example here I do not like this one this one is yucky so yeah they really should provide better examples but okay it didn't work this way but could we just manually upload this and see if it works so we'll go over here and uh we have our file and I want to uh go ahead and download this so I'm just going to go and download this to my local computer and what I'm going to do is I'm just going to rename it to my file um Shaw one okay I'm doing that right here see and what we'll do is we'll go back over to here I'm going to drag this on in and then we're going to go to additional properties here no maybe not that uh permissions nope destination details no usually when you um when you drag this up oh we had to go under addition uh properties we just had to scroll down further so when we're here we can say additional check sums we'll use the shaw one method and then this is the precalculated value so if it if it works this way that means that maybe it's supposed to be base 64 encoded but isn't this already base 64 encoded so go ahead and say upload the precalculated value you provided for this object does not match the expected checkm value for the selected check sum functions check the value and try again or leave the field blank and S3 will calculate the check sum when you provide a precalculated value for a single object less than 60 megabytes S3 Compares it with the value it calculates using the selected check some function okay let's just provide nothing here and see if it can do it and it uploaded it and I guess it must have worked okay so I'm not sure if we're like fundamentally misunderstanding something but I could have swore oh here it is look here's the value why is it so different than what we're looking for maybe that is BAS 64 encoded okay so how about there's probably like a tool online like string to base 64 okay so we go here we put that in there encode it that starts with a y does this one start with a y going back to uh S3 wherever that is w07 MF all right okay I really want to know why that one is different so let me do a little bit of research okay all right here's something that's interesting what I'm looking at the SDK uh this is Python and they're writing uh a uh well actually they're providing the file there so yeah I'm not sure um how do we provide the check sum so here it says or I can compute the check Su myself okay but how are you Computing the check sum they really should show how they do it so that other customers can do it as well because I'm still missing that part I thought maybe uh there was some indicator there that uh that was the answer but give me I guess another moment and see if I can find anything all right so I tried a few other things like for the code to utf8 I mean there's a video of a fellow ATS who's actually able to do it um but if the code is here I sure can't find it so yeah I'm going to say that uh this is a bu's fault because it should be this freaking hard to do it and all the instructions say that I'm doing it correctly there's clearly something missing in terms of the encoding but at least we understand what's happening here is that if you change the change the type they will automatically do it for you or you can provide um a pre to delated Value but why can't seem to get the same results I have no idea why that's okay as long as we understand how this functionality works and we spent enough time on this one here so we'll just call this a wrap and I will see you in the next one okay ciao S3 object prefixes are strings that precede the object file name and it's part of the object key name so this whole thing is an object key name this part is the prefix this part is the file name since all objects in a bucket are stored in a flat structured hierarchy object prefixes allow for a way to organize group and filter objects so this is the way we get folders and we talked about folders in another slide a prefix uses the for sliminator to group similar data similar to directories or folders or subdirectories prefixes are not true folders when we're talking about general purpose buckets for the direct bucket uh type I have no idea as it is such a new bucket type when I'm recording this so I'm not sure exactly how it works but uh there is no limit for the number of deluminator the only limit is the object key name cannot exceed 1024 bytes with the object prefix and the file name combined so uh yeah there's no limit in terms of eliminators I I don't know if you can make you probably make eliminators that are literally one character long so you can make as many as you could fit in there if you like but uh it's not a limit on the eliminators or subfolders it's based on that bite size there you go hey this is Angie Brown and let's take a look at prefixes for S3 objects very simple uh concept is that they are basically the virtual folders for your objects but I thought it'd be really interesting to see how far we can stretch this idea of creating prefixes so what I'm going to do is um go over to our environment this is on aabus example so again I'll just show you where this is is it's on GitHub exampro cabus examples and I've opened this up in git pod use whichever environment that you like to use I make a new folder here I'm going to call it prefixes and in here I want to um just go ahead and create a readme.md and I just want to create a new bucket so we'll say databus S3 make bucket S3 col SL slash this will be prefixes fun and ab etc etc of course use whatever works for you we'll go ahead and copy that and now we have our new bucket and what I want to do is I want to see how far we can push those prefixes so what happens if let's say we just want to make a folder and not necessarily a file can we just do that by just providing the prefix with a forward slash on the end so let's see if we can do that so I'm going to say ads S3 um put object and then here I will supply the bucket name so say bucket and I'm going to say key and we'll just say hello forward slash and let's say if it can take that okay so it's complaining because there's no body I'm just going to give it no body so what if I do this can I do that say body and I'll just leave it blank because it probably wants a body at the very least and uh I don't know why it won't make many equals there just going to make sure this is all correct probably the reason why this didn't work was because um this is not S3 API as usual so I'll take the body off and I just want to see if it'll actually create this folder here okay we'll hit enter and it's saying it's not correct what's not correct about it and it's showing me the commands for S3 I don't think it copied the command I'm going to try this again copy paste enter there we go and so it looks like it just created a folder we'll go over to our bucket here and take a look this bucket had some kind of name prefix fun and we'll go in here and so it created a folder so we can create folders by not supplying a body and just having a for slash there on the end let's see how far we can push this so create our bucket create our folder and and create many folders so there is some kind of limit limit for prefixes S3 let's go take a look here what is the length allowed I'm not sure so I know it's in my course lecture content we'll say what length can a uh can a S3 object with prefix name be a maximum of 124 bytes all right so I'm going to go to Laura myum and we'll go here and I want 124 bytes according to this this is 124 bytes now I don't know if it includes the spaces or not but let's push it to its Max so I'll take this and we'll go back over to chat gbt turn this into a prefix with slashes between the words and stop and then shorten the entire length to be 1024 let's see if it understands what I'm asking we'll give it a moment okay all right so after waiting a little while it uh tried to do something here it still has the periods in here which is not great um so what could we do to make this a little bit easier I suppose we could just replace ourselves and just do some formatting here so I'll go here and I'll just take this out and I'm going to go replace it so we'll say edit replace and for any space I mean should only be for what we selected uh I don't want the whole freaking file just what we select here um okay I'm going to use uh mark down here I'm doing like a for slashes all the way down here and I'm so I'm just substituting it so I'm saying space substitute hyphen or sorry back slash slash SLG oh my goodness how do we do this it's only for the selection I want it to change okay let's go back here edit replace and I only want it to be the selection find in selection there we go okay that's better and so then I'll put a for slash here and we'll replace all there we go and then if there is a comma we'll replace that with nothing and we'll say replace all replace all and then we'll say period and we'll replace all okay so now we're kind of getting something here I'm going to go ahead and count this and see what we have so we'll go ahead and I'm going to bring this into Ruby so I'll just say Ruby or IRB for Ruby I'm going to go ahead and copy this string I'm going to say s for a variable and then paste this in and hit enter and I'm just going to get the size of it so it's saying 992 characters so we are a bit short here so maybe we can bring it up by a little bit and I'll just put some things on the end and we'll go grab this again again we're trying to push this to its limit and see what it can do I don't think it grabbed the whole string we'll try this again clear um s equals this s. size 116 so we're pretty close um 16 17 18 19 20 21 22 23 24 so I'll put the four slash on the last and so that I'm hoping as an experiment to see can we make the key literally this long now if we made it this long there would be no room to put anything in this folder but to see if we can do it is the fun part does it have a pract practical use case absolutely not let's go ahead and see if it'll let us do this so I'll just exit out of the Ruby here and we'll hit enter and we'll see what we get and it actually created it so let's go back over here and take a look and did it actually make it there we go so we got laurum ipsum doar sit Mt and we could click here all day all day because that is how long it's going to be that's how silly it is now the next thing I want to do is let's go take a look at what um list objects return so say Ad of us S3 I'll just do it the simple way first and maybe this will be enough and notice it's only returning uh these top ones and it's not returning all the interior ones that's really interesting I'm going to make it a little bit more complex and we'll say S3 API list objects and we'll just specify the bucket as such there is one mistake here I need to have um this off of here and so notice it's not breaking these up as separate folders it's just one single folder if there was something that we inserted somewhere else I'm sure that would happen but uh do we have enough room to dump on another file on here because if it 1024 is the limit we should not be able to um uh push an object here so what I'm going to do go to the bottom here is I'm going to copy this paste it it'll say try and break the 1024 limit and then we'll go back to the bottom here and so that's the key and I'll just say here hello.txt and we need some B so I'll just say body my file.txt or hello.txt and then before we do that I'm just going to make a new file here we'll just say hello H txt hi there we'll go back let's see what happens I'm just going to be a bit consistent here with our parenthesis and this should not work because the limit should break it must have a value to a path well I thought it was going somewhere it's it looks like we're in the check sums folder I'm just going to go into the prefixes here and make sure we're in the correct place I'm going to go ahead and copy paste this make sure you're in this folder your key is too long so we can create an entire folder name and not have any files in it because it's too long but uh yeah I thought that would just be kind of fun to do and and just to make it clear that you can have as many prefixes as you want um but that's about it I'm going to go ahead and clean up these folders I think I have another Bucket from uh earlier that is just around so I'm going to get rid of uh the prefixes here empty the bucket and we'll go back over to S3 here and now we can delete this bucket while I'm here I'm going to just delete anything else that uh is junk so I think we did check sums earlier if we didn't delete it like I'm not that concerned about spend in S3 so um you know know for the most part we're pretty good at getting rid of all the buckets but you know if there's stuff that's laying around you don't want it there make sure you're diligent and you get rid of it okay I don't know when that bucket was created but uh yeah there's there's prefixes there you go see you in the next one let us talk about S3 object metadata so first of all what is metadata it provides information about data but not the content itself metadata is useful for categorizing organizing data providing contents about data uh and I want to point out that metadata is not any of a specific concept is just a general Tech concept around storage just as eags and checksums are not any specific Concepts uh Amazon S3 allows you to attach metadata S3 objects any time we have two kinds we have system defined and user defined what's really interesting about this screenshot is generally system defined you're not allowed to to set the value as Amazon or ads will set them for you but there are a few exceptions for those content types such as the content type of system defined I need to point out that there are uh things that look similar to metadata like resource tags and object tags um but uh it uh those things object tags and resource tags are intended to provide information about Cloud resources and not the contents of the object which is what the purpose of metadata is um so yeah there's a bit of confusion there but they're not the same so let's go talk about the specific types of metadata so system Define metadata is data that only Amazon can control but that's not technically 100% true because there are some values you can set uh the first being content type but most of these you do not set uh they're set internally by ads let's take a look at the system notifying metadata we have a content type so it tells you the type of file it is cache control you know how long it should be cash for Content dis uh disposition this is if you want to change the name when you download it um but again you don't have much control over uh this one at least here content encoding so you know is it a compressed file it's language uh when it expires uh whether it needs to redirect I assume that is probably something for static website hosting so yeah exactly um you don't uh change most of these values and there could be other ones here that aren't listed but these are the ones that I was able to find us will attach some system toy metadata even if you do not specify them so when you upload a file it's going to already attach them if you want additional ones that are not being returned with the object then you specify them some system defined metadata can be modified such as the content type I've mentioned this like three times now so just understand that generally you do not modify system uh defined metadata but there are a few edge cases let's take look at user defined data this is metadata set by the user and it can be whatever you want it to be um and the only thing that matters is that when you name the key of your metadata it has to start with X hyphen AMZ hyphen meta hyphen so let's look at some examples so what you could do is set it for Access and security so we have encryption access level expiration date uh we uh have the media file so camera model photo taken on location we have custom application I ran out of space there so I just didn't put in the um uh this here we'll just assume that it is there but we have app version data imported source project uh specific so we could have project ID Department reviewed by document versioning so what is the version of this file when was the last modified what was the original upload date content related so again shortening this because I'm running out of space but title author description maybe you have this for compliance and legal so is it on a legal hold does it have a compliance category retention period backup and archival so maybe backup status archive uh date recovery Point whatever makes sense for you to set for your userdefined metadata but there you go hey everyone it's Andrew Brown and in this video we're going to take a look at metadata for uh S3 so what I want to do is make my way over to the it examples repo and in here I'm going to make a new folder and this is going to be called or I tried to make a file a folder called uh metadata and so the idea is that we want to attach metadata when we upload or update it later on if we're allowed to I'm just going to create a readme.md in here and we're going to do what we normally do we'll create a bucket so I'll say itus S3 um adus S3 make bucket S3 col SL slash this will be metadata fun ab and some numbers here I'll go ahead and paste that in I'm going to make sure I'm actually in the metadata folder before I do that of course you have to have the installed etc etc you should know by now as we've done so many Labs with this um so the bucket is now created and the next thing I want to do is create a file so the question is can we upload metadata at the time of creation or is that something we should be doing after the fact so we'll go take a look at the CLI and we'll go over here and we'll say S3 API and we we will look up put object and actually before we do let's just see if there's anything for metadata notice there's nothing for metadata so I'm going to assume that's how we're going to update it is through here if we search for metadata in here there is a flag for metadata so we can set metadata that way I'll go here to examples and I wonder if they have any metadata examples it doesn't look like they do that's totally okay um I'm going to go ahead and create a new file so we'll say create a new file that will be Echo hello Mars and we will pipe that to hello.txt upload file with metadata and we'll say what will we say here we'll say adabs S3 API put object we'll specify our bucket and we will specify key so we'll say hello.txt and now we want to do our metadata so let's take a look at how we Supply our metadata and it's a map so we can provided in this kind of format here remember in the video I said it had to start with like X Amazon there's some kind of like prefix to it the question is will it automatically prop uh append that for us um it doesn't have much information here but they were saying it had to start in a particular pattern but maybe the CLI will automatically do that for us so here I'm just going to say um Planet because we always change what planet we're saying hello on so this will just be Mars so that we know and actually it's going to be this it's going to be hello so we'll say planet and then Mars and so hopefully that will allow us to set that metadata so we're going to go ahead and we need to also specify the body so it'll actually upload the file there we go we'll copy that paste it down below see what happens it has a problem we'll hit enter and figure out what it doesn't like body must have a file to a path I'm in metadata oh we didn't create the file let's go here and create it and then we'll try this again and it's upload the file so is the metadata there how could we observe it let's go take a look by using get object uh get metadata through I mean maybe head object is better that way we don't have to download the file so we'll go here and say head object and we'll just get rid of this stuff here and let's see if we can get our metadata back all right and so notice that the metad says planet Mars we didn't have to put the stuff in front of it it figured it out that doesn't mean that it's not there but let's go take a look at the adus console and see if there's any kind of differences that are going on so in here we have metadata fun and we'll go into that file and I'm looking for that metadata so notice that it already it appended it for us X AMZ meta planet and it lowercased it if we come back over to um here it is lower cases so the key is always going to get lowercase and it automatically attached that for us notice if we go edit here um it detected that the file was set as an octet file which is interesting because it's actually a txt file so it says binary uh octet stream that's like usually what happens when it doesn't know what the file is and we can change its content type here now understand that if uh just because we change this value here does not necessarily mean that it will serve it as this file it's just this is metadata attached to it right so we can override this and we can say this is just plain text because that's all it is we can also add additional metadata notice if we go system defined and we choose something like something else here uh before I said like hey you can't change all these values but it's allowing us to change all of them so I'm like what is going on here so there must be other metadata that can get assigned because it's definitely true there's ones that you can't Define but I guess anything that's in here we can change and there's probably additional metadata that um ad of us can supply so I'm going go back and refresh this okay doesn't like where I am so we'll try this again and we'll go back to metadata and so anyway I can just show you that we can override that system defined one and we'll go ahead and save but I do need to point out that you know if you uh there is system data that adabs will set and you cannot change it all right so yeah that's pretty much all we really need to know about setting metadata and working with it obviously using head object is a good way of finding out what made dat is attached which we can see there um but yeah that's about it so we'll go ahead and just save this so metadata example and we'll sync it we will go ahead and just delete this bucket so go into here and sometimes it's even just faster cleaning up here I'm just getting tired of clicking stuff so I'm going just say S3 um remove and this will be the address here and it'll just say hello txt there we go and then maybe we can just remove the bucket I'm not sure if that actually removed the bucket let's go take a look there I feel like it'd have to be RB to get rid of it it's still there so um instead I'm going to try RB remove bucket there we go and that's a little bit faster so we'll just say clean up I really should start doing that more because it is quite the headache to click click through the CL and then here would just be this so yeah there we go and I'll see you in the next one okay ciao what is worm well worm is not what you think it is it's not a bug in the ground what it actually is is write once and read many and it is a sour compliance feature that makes data immutable mean that you cannot change it once it's been written uh you write once and the file can never be modified or deleted but you may read it unlimited times or is very useful in the healthcare or financial Industries where files need to be audited and untampered uh in Cloud this is going to be in arival and object storage so an example of uh a real world example of worm uh not specific to Cloud but uh if you've ever played old video games and you have those cartridges inside of them they have chips and inside there there's a ROM and a ROM you can write data to it once and it's now permanent and then you just read it many times when you want to play it so there is a very practical example of worm but yeah we're setting that up to understand uh how worm will be used in Cloud here okay S3 object lock allows you to prevent the deletion of objects in a bucket and this feature can only be turned on at the time of creation of a bucket object lock is for companies that need to prevent objects being deleted uh to have data Integrity Regulatory Compliance and S3 object lock uh is regulatory compliant with a a few different things like sex 17A hyphen 4 ctcc finra do I know what those are off the top of my head absolutely not but they definitely sound important if you care about uh Regulatory Compliance you can store objects using the right once read many model just like an S3 Glacier that's what we were talking about worm earlier to lay this up for S3 object lock you can use it to prevent an object from being deleted or overridden for a fixed amount of time or indefinitely uh object retention is handled two different ways we have retention periods so this is a fixed period of time during which an object remains locked we have legal holds it remains locked until you remove the hold and one caveat that you have to remember with S3 object lock is that it can't be used as a destination bucket for Server access logs so uh hopefully that is very clear now in terms of how you would go ahead and set an object lock on objects uh you can you do this only via the IUS CLI so or I should say the the aabus API so CLI or SDK and so we would have a put object and we what we would do is set some flags on in here now this is just one example as there are a few different ways of locking objects and we have different modes but I just want to point out that it's based on the or it's only through um uh the ads API you cannot do it through the console and you need to remember that okay uh so there you go so S3 object lock has retention holds and legal holds let's first talk about retention holds and it actually has two modes within it we have governance and compliance the previous screenshot in the previous slide was showing uh for governance but let's talk about the difference between these two so for compliance is prot is a protected object that cannot be over written or deleted by any user including the root user um the lock period cannot be changed or shortened ensures object version Integrity throughout the retention period deleting an object in compliance mode requires deleting the associated it account so it's really hard to uh uh get rid of that object once it is locked uh for governance mode protected uh if it's protected it cannot be modified or deleted without special permissions uh the lock period can be changed or shortened or removed by special users it can be be used for testing retention settings before enabling compliance mode so if you aren't ready for compliance mode go ahead and use governance mode as it is a uh a more flexible version as we will say there there are special permissions needed to override the lock settings in governance mode uh because governance mode allows you to override and uh to change things uh you'll need to have the S3 bypass governance retention on uh you need to H pass along uh that request header if you're using the a CLI it's going to automatically add that header in there for you you can use the a ads S3 API put object retention uh and that's going to change the lock settings there so uh yeah just remember that compliance mode totally locked the only way to uh to delete that object is to delete that into's account and this one gives you more flexibility um okay let's talk about legal holds which are pretty easy to understand they're like retention periods but legal holds prevent an object version from being overridden or deleted uh however there is no Associated fixed time and it remains in effect until you remove it so it's really as simple as applying This legal hold flag on when you put an object you have to do put object legal hold so it is a different API endpoint you do have to have uh permissions for that in particular legal holds are independent from retention periods placing a legal hold on an object version doesn't affect the retention mode or retention period for that object version it's totally a separate thing uh but yeah there you go it's as simple as that all right so we're going to go ahead and utilize S3 object locking we have retention holds and legal holds retention holds has two different modes the governance and um other mode being compliance and so remember that when we get to the compliance mode we're not going to be able to remove that so if you decide that you don't want to do it I'm going to warn you and say hey don't do this just watch along so that you can uh get the experience without having to deal with having this file that's you're never going to be able to get rid of so what we'll do is go over to the it examples repo as per usual and in here I'm going to create a new folder this one is going to be object locks and I'll just create a readme sometimes I like creating bass script sometimes I just like creating the readmes and we're going to create a new folder so create a folder and this is going to be pretty straightforward we'll just say ads S3 make full make bucket um S3 Co SL slash this would be object lock fun and we'll say ab and put some numbers here so we'll go ahead and do that I also want to point out that we should be in our metadata directory here so we'll type that uh let's see how we can turn on object locking so I'm in the ads CLI here I'm looking up S3 API I don't think it's a separate command I feel like it is something we do when we upload the file just kind of trying to remember what it is because when we put an object we can set the object locking mode so let's go look at um when we create that bucket there cuz we didn't specify any option right so we'll say up we's just look at bucket here cuz it's bucket configuration I'm in interested in put bucket configuration yeah I'm not sure so let's go over to the bucket itself and I'm just checking I don't think that uh I don't think we have to turn on a a functionality on the bucket maybe all we have to do oh it's down here it is that's what I thought cuz I think this globally turns it on for everything so that I did not click static website hosting so I wanted to have um object lock here and so this is what I want to enable so I thought there'd be like a configuration option here configuration to update it configuration here it is get object lock configuration so there must be like a put object lock configuration that's what I wanted so here I think this is where we're going to be able to uh set it so I just go ahead and copy this we'll just say turn on object locking and so we have uh ads S3 API put object lock configuration object enabled and then there's like a rule for default retention I don't want to actually set any kind of default uh rule here especially not for compliance I'm going to switch this over to governance governance because that seems a bit scary to me I don't know if it'll let us do this but I'm going to select one day go verance okay so that's set to governance and what we'll do is we'll copy this command and um we need to specify our bucket so this is our bucket up here and we'll go ahead and we'll copy this and we'll paste this and we'll hit enter versioning must be enabled so yeah we have to have versioning turned on uh we know how to do that we've done that before so we'll go here and click back we'll go back to S3 API and we'll look for versioning put bucket versioning examples and that's all we want here so we'll go back turn on versioning and we'll grab this part here we'll paste that there we'll copy this hit enter and so bucket version is now turned on so we should be able to do this I'm really hoping that we can do governance one day I'm not sure what the minimum is XML you provided was not well formatted well it's not XML but I guess that's good to know that we can provide it there um what we're trying to do is supply uh Json so I'm going to use the shorthand because Shand is going to give us uh less problems so I think we can do it this way where kind of becomes problematic is when we have this uh interior here like this say so I think this will work but if it doesn't that's totally fine we could also just Supply adjacent file which might be a less of an issue but I want to see if this works first so this one that one and we don't need that n one so I'm hoping that we can do this as a shorthand and it doesn't look like it likes it it does it doesn't so what I'm going to do is just create a new file here again it could be Json or whatever so let just say default Json that could be XML as well and I'm just going to undo this a bit so that I don't have to uh do so much work to rewrite that and so we'll cut this out paste it here not what I wanted but that's okay did I actually get uh the code I wanted no no that's the old stuff we'll go ahead and say cut and then we will select all of this and we will paste and now what we'll do is just specify the file so we'll say default Json I don't know if we have to put the file protocol from there it's just kind of a habit to do that um we'll go back over here so I don't know if that's going to be an issue I'm just going to leave it like this cuz I don't have how to give it its absolute path so I think this should give it less trouble yeah I think that's fine we don't need this here and we don't need that there and let's put one day I think we have to have a least one day here and let's see if it'll take this instead and it still doesn't like it yeah cuz that's all right we'll try this instead okay we'll try this again and I'm going to try to use autoc completion I thought I was going to open up and tell us stuff there but it isn't so sometimes when you copy paste commands it just doesn't work exactly the same way because there's some kind of weird thing in the way so that's why you'll see me do this a lot where I'll I'll wrap these like this it's either that or maybe we're not supposed to specify the bucket that way but I'm pretty sure we copy pasted this so it should just work and I'm going to go ahead and just make this a single line this is going to just reduce any kind of misconfigurations if it's just like some weird line thing we'll go hit enter still doesn't like it oh my goodness I'm going to just quit this here I'm going to drag the whole file path in here and I'll copy this and place it in here as such let's see if it likes this better we might also have a separate flag for just specifying this sometimes that happens eh where it's like it wants that to be the inline Json no so we'll go look this up then it's not being nice to us here today and we'll look at the flags I mean this is what we have huh days years yeah I I don't know what else it want we're literally providing it the same way as it has provided it to us oh okay um maybe we can't Supply a file here sometimes these flags don't let you do that eh sometimes they're a pain like that so what I'll do is I'm going to copy this again even though we didn't do anything wrong here I'm going to change this to governance I'm going to just take out the guesswork I'm just going to copy this here I'm going to change this to one day if that's even allowed that could be our problem by the way if we're setting a value it doesn't like we'll copy this we'll paste this here we'll attempt this we'll paste it and we'll hit enter I don't know what it was maybe we're missing the the this one on the end here but it's working now so who cares right we're going to make our way over here and check and look at the object locking so we have metad data fun and did we already upload a file oh no that's from our last bucket sorry and uh yeah so we're in here now and we'll go over to object locking notes in here somewhere management it must be properties so it says enabled enabled governance one day so we go here and yeah maybe one is the lowest we can provide here it kind of looks like we have to provide a value but now we have governance set up a setup so if we were to upload a file it's automatically going to have this mode governance right so let's go and um bring a file over there so we'll create a new file so say new file and I'm going to say Echo this is the gov it'll say gov. txt and I'm going to make sure I'm in the right folder object locks we'll say LS and let's go take a look here so I'm going to say umab S3 copy gov. txt to here hit enter that file has now been uploaded and if we go back over to here the question is can we delete that file so there are additional permissions you need to have that and um I mean as the type of user we are we should have full permissions to do whatever the heck we want right so it's possible the files in there and that we can we we can still delete it but we really shouldn't be able to unless we're a different user um so let's take a look at what is set for this legal hold is disabled but notice retentions on here it says retain until December 15th so that's tomorrow um I'm going to go ahead and attempt to delete this so notice as I look at this file we do have a delete command I'm going to do it through the CLI because I kind of prefer to do that ads S3 remove file and we're going to say this is the path gov. txt can we delete it so we're able to delete that file and the reason why again is because we have permission to do so we were the one that created it I think it's if other people want to try to delete it so the only way we're going to be able to test this out is to really have another user so we'll go back over here we'll give it a refresh the object is gone um but I guess the other thing is that there's versioning on so is the file actually gone because we turned on S3 versioning that might not actually we might actually uh be mistaken to think that the file is actually gone so I'm going to go ahead and type in a ss3 API um just say just make sure we put these other commands in here I want to put this in here delete the file we'll say itus S3 API list object versions and then we will supply the bucket here and then we'll say the key here and is truncated oh maybe it's get object version sorry that's what I want I can't remember what the command is so we'll go back here and take a look versions list object versions I could have swore that's what we did that is what we did and we're kind of getting different information this is uh this is different we don't normally see this right like when we work with um when we work with versioning we'll see the actual version information so yeah what the heck are we looking at there's a key marker okay give me a second so yeah on repost here you you know they're saying that you can delete the objects but they're still going to stick around and you need to specify its version and so that's why we came here and we're trying to get the version information but as you can see it's not showing the usual thing that we see because we've done this before and it would show versions and delete markers and all we're seeing is version ID marker so I am a little bit confused as to what is going on here um because it's not the normal thing so yeah I'm not sure what we're looking at here like even for theirs they show something different now I wonder if we did the bypass governance retention if it would actually permanently delete it but again this one specified the version ID what if we add another version let's see what happens if we do that that might make more sense so this is the gov two we'll say Echo and then we'll try this again okay and now we will go up and we will just go look at more versions and it's not listing out object versions I'm really confused as to why because object versioning is turned on all right let me go investigate okay I think the reason I was confused was that we were supplying every single time with it the the key and I don't think we ever actually did that once before we always maybe supplied it with the bucket at least that's what I think is happening so maybe that was just my confusion there because I was using the CLI a little bit differently than usual but we do have our delete markers and we can see the objects are there so let's go ahead and see if we can actually delete the object now um so we will attempt to delete the version file we'll see if we can delete that specific version so we go here and this is the new one we pushed and so we'll specify the version ID there's a period in there that can really mess up the CLI so that's especially where we're want going to want to do equals on that um we have remove here but we'll have to do uh Delete object bucket I don't know if the the the version ID flag is on the remove it could be I don't remember so I'll just do it the way I I definitely know it will it will work there we go let's give this a try here we'll have to make sure this is S3 API now we should have permissions to do do deltion but we'll see um it doesn't like something here we have double bucket in here just take that out we'll try this again AIS and so it's not letting us do it because we do not have permission to do so it seems like we'll need an additional flag notice it say bypass governance retention flag let's go ahead and uh well before we do that let's just see if it shows up here in the documentation and it does so let's just put that flag on and see if we can delete it all right and I'm going to go ahead and paste it in now so no notice that we're able to delete those uh those um objects there so we'll go back here and I'm just going to go and list the objects and I want to get rid of all of its versions if it's even possible and so we have that version I want to get rid of this version paste that in here and I'm going to see if there's still delete markers we still have a delete marker I'm not sure if we're allowed to delete that let's go find out can we delete a delete marker apparently we can okay great so we we're able to clear that out that object completely and now it 100% does not exist so that is governance um compliance is going to be really straightforward it's just going to uh be where we change it to use governance mode or sorry compliance mode and to do that all we got to do by the way don't do this if you don't want to deal with the files but apparently we can have it for around one day so it's not so much of a risk that I thought originally when I warned you earlier on in the video at the start okay so use uh compliance mode for S3 object so we want to do ads S3 put maybe we already have this here put object somewhere where is it oh we copied it this way so we'll have to use itus S3 API put because I don't know if it allow us to put the object lock mode flag on there we'll say put object bucket and then we'll grab our bucket and our key here and uh did not copy wanted it to right click copy right click paste and uh we want to supply its body so we'll say body uh gov. txt this will actually be compliance compliance so we know what it's for okay we are the Borg if you like Star Trek you'll know the reference and uh we need to have an object lock mode and so this time it'll be compliance and then we need to set an object lock retention retain date let's see what options we actually have for this maybe there's an easier way to set it I think you have to set a date though object yeah it's date until not my favorite way because I don't like really providing time stance but we can absolutely do that so we'll say um object lock retain whoops object lock retain until date and then we will just say something like um today I mean you choose what works for you obviously do the next day but I'm just going to go ahead and do 20 23 uh 12 15 because today is the 14th for me and I'm going to put t 0 colon z0 colon z0 uh Z actually I'll add one extra day because of time zones I don't know what time zone this thing is so this will actually hold for me for a couple of days assuming I got the format right then this should be good this format is some kind of time stamp so it's pretty standard time stamp I just don't know uh what its proper term is and so it's apparently cre that object and it's going to be compliant till then thank goodness I didn't put 2024 in there um we'll go back over to here and I'm going to go and take take a look at object lock and we have that file let's take a look here uh did I call it gov oh I did whatever that's fine and notice it'll have retention until tomorrow so that's good thing I did 16 because the time zone it ended up being the 15th which is actually tomorrow so I should not be able to delete this file so what I'll do try and delete specific version because we know we can delete versions right I don't know let's find out what happens if we try to delete the version okay now let's see if we can delete the um S3 API list object versions the version bucket and so we'll grab this version ID here and we'll try to delete it okay we'll hit Q we'll hit enter of course there is no bypass governance retention you can't pass that that's not going to do anything um and so so it cannot be deleted so we'll just have to wait a few days like don't worry you're not going to wait with me here that's all right and um so that's the two types of retention holds let's take a look at legal holds this should be a lot straightforward all we have to do is put legal hold status on and that's about it so we'll create a new file we'll say um touch legal. txt and let's go upload that file so we'll say a S3 API just new file touch legal txt and we want to upload this so This ads S3 API put object so first we need to upload it I'll just upload it the normal way so's say CP grab the bucket here S3 Co SL slash I hate when it does that when it copies all of it by accident and this one will be legal. txt so that will upload the file there and to put a legal hold on it we'll have to apply S3 API put object legal hold and we'll specify the bucket and we'll specify the key and we'll say that the legal hold is now status as on okay we'll go ahead and apply that the legal hold is now on let's go take a look at the um the bucket itself and so we have legal here let's click into it and it has object Lo lock hold on so it says prevents object from being deleted or overwritten until the hold is explicitly removed so probably just like the other ones we can delete it but it's not actually deleted it's just creating a deletion marker so we'll go ahead and type in ads S3 remove and we'll see if we can remove it so it looks like it has been removed but it actually hasn't because we do ads S3 API and we say list or not or list version objects and then we Supply the bucket we can didn't like that accessory API object versions object versions sometimes that's frustrating so I'm going to go ahead here I'm just going to just copy some of these commands and make our lives a little bit easier we've typed this so many darn times there's no reason for us to keep repeating ourselves if I can find it in here no I'm just going to search for it um versions oops versions no I guess I'll just have to type it by hand ad us S3 API get object versions bucket and then we'll grab this name here and we'll try this what does it want uh is it list maybe it's list list object versions there we go and so we're looking for um the legal one so here is its version ID so we'll go ahead and try to attempt to delete this and we will need to take its version we'll cut it notice there's periods in it so you really want to wrap this in double equals here and this is called legal. txt it should not let us delete it because there's a legal hold on the object so what we'll do is we'll turn the legal hold off okay uh put when calling the put object legal hold operation specifi method is not allowed against the resource so it's not as easy as we think to turn it off so turning off legal hold ads I don't want a batch operation I just want to turn it off um S3 put object legal hold permissions required to your IM roll or to remove the legal hold object so I guess my question is do I already have that because I'm an admin right this person is an admin so they should have full access um I'm not sure who this is Let's Take a Look ads S3 get caller identity so this is adus examples and I would think that they would have that permission already unless it's something you have to additionally add so what I'll do is I'll go over to IM and maybe we'll just add that in as a sanity check and see if that's what it wants so we'll go over to A's examples I'm going to go ahead and add permissions and we're going to attach a policy directly um I want to create one actually I just want to create one in line so we'll go here and say policy maybe they already have one for legal holds let's go find out legal holds no so I'll go here and just say S3 we'll say legal put and get and we'll just give it for everything for all we'll say next get legal or like control legal holds get and put for legal holds cuz maybe we don't need to have it to in order to set it initially but to to take it off we might need it so over to users I'm going to go over to us examples we're going to go and add permissions and oh you know we could have done an inline permission well whatever we created it anyway I forgot that's fine we'll go ahead and try this and so now we' have attached the policy these are usually pretty darn fast so should instantly work so what I'm thinking is is that maybe we are it's not called off the specified method is not allowed against the source so I must be just providing the wrong thing and we already have the permissions but still it's good to show how to add those permissions there so I'm going to go over to put object it says off so what does it want uh status off let's check this again this one's a little bit different there's like a hyphen in it are we missing something notice that this shows it like this what if I put that in here like that object lock legal hold status oh but that's on the put object right okay but this is on put legal hold sorry so we're on the wrong wrong uh CLI command here and we'll go over to S3 API and then we'll make our way all the way down here and I'm looking for that legal hold option off it says off so it should just work if we set it to off right status equals off yeah that's what we did we'll go back here um when calling the put object legal hold operation the specifi method is not allowed against the resource why not maybe the reason why is that we have a delete marker and so maybe we have to specify the version is that a thing version ID okay so maybe that's our issue so what I'll do here is Go version ID and we'll just specify it and maybe that was our problem aha so now the question is is the delete marker still there and I'm looking for it I mean we have well we turned it off right so the legal holds is off I guess the question is can we delete the delete markers now so like could I delete this delete marker or does this have a legal hold so I don't know but I'm just gonna I'm going to just apply to this one as well so this will be for the delete marker I just have a feeling that I have to have to do this so we go ahead and try this out enter does not specify specific version well it does but maybe the delete marker it's not required so instead of this let's go ahead and try to delete the object now so I'm going to go here and um that's a specific version we'll try to delete that oops clear we'll paste that in access denied why why why why why not we turned it off so go back over to here and we have this one here and we turn the legal hold off on that object right let me try this again so no error there and we'll list this again and then we have this one we'll try this again I might have just copied it wrong I don't know and we'll close Z we'll try this one more time hit enter okay so the version we're able to delete the object version which is fine let's try this one again access denied access denied why we have permission um give me a second okay oh you know why we can't delete it I was just thinking about this is that because we still have governance turned on because we have object locking right that's the reason why so uh we need that flag from before that we were using call like governance or something here it is bypass governance retention and then we'll put that there and that's probably the reason why it wasn't allowing us to get rid of it there we go okay so some caveats when they're both kind of turned on so that's kind of interesting um I can't really get rid of uh what we have there right now I don't even think we can turn off object locking I think once it's on you don't turn it off right at least that's what I think well let's go find out or at least probably we can't turn off any objects are locked but let's go ahead and take a look and test that theory cuz I could be wrong here it's so hard to remember everything so it's just how it is um and we'll go into here and we will go into properties and we will look for object locking enabled enabled Etc let's see if we can disable it once S3 object is lock is enabled you cannot disable object lock or suspend versioning for this bucket we'll save it so all we can do is turn off the default option so now it's not going to set governance or compliance but we cannot turn off object locking um um so I imagine that once we are able to get rid of all the files then we can delete the bucket that's not going to happen till tomorrow um if you follow through you'll have to delete that bucket on your own time but yeah we covered object locking so hopefully that was uh super fun and I'll see you in the next one okay ciao so in S3 you're going to see this S3 bucket URI and it is a way of referencing the address of an S3 bucket and S3 objects and the way it looks like it starts with this protocol called S3 SL if you're used to seeing htps or file or other protocols this one looks a little bit unusual but uh the reason why that OS has this is to make it easier to work with the CLI so when you're doing like copy commands and things like that you can quickly reference um the uh the S3 buckon and its objects it us has this other thing called ARS Amazon resource names and they're super long so this is definitely a more convenient way uh where it's used in other places I'm not 100% sure I just know for sure it shows up the CI commands some other things might use it but uh yeah that's all I really wanted you to know here is that it's called the S3 bucket URI and it's just pointing to the bucket let's talk about the itus S3 CLI because there's something a little bit different with this one because S3 uh is such an old service and does so much that it doesn't just have one command it has uh uh four different commands we have the adabot S3 command the S3 API command the S3 control command and the S3 oost command and I don't remember there being so many but uh before there was just two there was S3 and S3 API now there's S3 control and S3 outposts so they all serve different purposes and let's talk about it the first one is it was S3 this is a highlevel way to interact with S3 buckets and objects and so basically this is like when you really don't want to do level level stuff and you just want to copy and move files and you want it to uh put a lot of magic in there for you and write the least amount of code to do what you wanted to do you're going to use the ads S3 so it's really good for daytoday usage um I think that when you are pro programmatically working with bash scripts you should really use the S3 API because it's more robust and this one is a lowlevel way to interact with S3 objects and buckets and so here's an example of the exact same thing that at the itus S3 is doing above there as it's putting something in the bucket but notice that it's a lot more verbose so you have a lot more finetune control in terms of what is happening here um but yeah abis S3 could be doing multiple things under the hood whereas S3 API is really single action single focused then you have S3 control um this one's a little bit confusing because it has a lot of commands for s3o posts which I really would have thought that was what it was s3l poost would have been for but S3 control is for access points S3 Outpost buckets S3 batch operations storage lens so a lot of the newer features are placed under here um and so here's an example of describing an S3 control uh job for something I don't really remember what but it that's an example of that then you have s o poost which um doesn't really manage everything with posts it just manages the end points for S poost so there's very very few um uh a CI commands for this so just understand that for the bulk of your normal operations you're going to be using S3 control uh when it when it talks about buckets and things like that um and probably a mix of the other ones above there but uh yeah there you go for a good overview a high level overview of the commands for adus S3 CLI in terms of subcommands we'll talk about that and we'll cover them throughout uh the S3 section so that's why we're not covering them here but yeah we'll move on from here okay so an S3 when you make a request there's actually two different styles of requesting that information and the the first is the virtual hosted style requests uh this is where the bucket name is the subdomain on the host and then you have the path style request this is the bucket name uh is in the request itself confusing don't worry we'll show you uh it much more clearly right now so on the left hand side the idea is that when you have that request um the bucket name is in the host uh uh here as you can see so it's up here and then for the path style request it's actually part of uh the path which gets appended to the end here so why did they uh make this switch I have no idea there probably is a logical reason for it uh probably has to do with something with DNS uh or something like that but uh there's two of them and path style was the original V virtual hosted style I believe is the newer style so S3 supports uh both the virtual hosted style and the path style URL path style URL will uh URLs will be disc continued in the future so for the time being they both work but it's best to uh utilize virtual hosted now I think that um I would like to think that the a CLI is always using the virtual hosted style path but uh you can globally set it to uh force it to use Virtual so if you're not sure which one it's using you can absolutely uh configure that and there is a point where it's very important that we make sure we use Virtual because some features of S3 will not work unless it is set to the virtual hosted style so I had to point that out there but yeah it's just a a change in terms of how that data is delivered as opposed to in the path to being as a subdomain in the uh the host there okay let's talk about dual stack endpoints so when you're accessing uh the Amazon S3 API you there are endpoints but there's actually uh two possible endpoints that you can use for uh most General commands I put an aster there because there of of course is more than two types of endpoints for Amazon S3 but there's two uh generic ones uh for commands that we should know the first is the standard endpoint which looks like that uh looks pretty normal this is for handling ipv4 traffic and then you have a dual stack endpoint and the key difference is it has the word dual stack as a subdomain and it handles both ipv4 and IPv6 why would we need to when uh when dual stack clearly does both well at one point all there was was the ip4 for address so the dual stack is designed to be the future replacement as the ipv4 address space is running out and IPv6 uh has a larger uh public address space that we can pull from so generally you want to use the dual stack endpoint uh to Future prooof your apps but of course the standard one still works uh I believe that the C uses dual stack underneath um I'm not sure because you'd have to do a little bit uh work to uh dig and find out but I'm sure that at some point if it decides that it has these dual they'll just update this a and again I want to point out that there are other S3 endpoints uh things like static website fips S3 controls access points so there's of course other endpoints but these are the two that um are for the the general commands like move copy uh make bucket things like that okay hey everyone it's Andrew Brown and in this video we're going to look at how to swap out uh the end points for S3 this is this is just general knowledge for working with um the a CLI but uh there could be a case where you might want to explicitly set the endpoint Ur all for it and so I figur it's a good opportunity for us to get practice so if you are here on the adus CLI uh and you click the adus logo there is a link up here called command reference and this is going to provide us all the global options and in here we're going to have one for endpoint URL and that allows us to override the endpoint uh to whatever we want so this again is applicable for multiple services not just for S3 but I figure we'll demonstrate it in this one I've opened up our um uh git pod environment so here with the adist examples of course use whatever you like I have a new folder called dual endpoints and we have a read me and the idea is I want to go ahead and create uh this folder so hopefully my credentials are already loaded up here so I'm going to go and copy this and paste this in down below and say allow and uh hopefully I already have credentials here made a bucket actually I'm not even sure who I am right now so I'm going to just go ahead and make sure I'm not the root account owner no I'm just a US example so that's fine and so that bucket has now been created so we'll just say create a bucket and uh the next thing I want to do is I've created the bucket now I want to um uh upload a file to S3 using standard endpoint and then we'll just say this one with the dual stack end point and I don't know which one it is using by default I'm not exactly sure how we can uh see that in a CLI I've never figured that out but um what we'll do is just touch a new exam uh new file so say touch um standard. txt and then for this one down below it'll be dual stack and then we'll say a S3 copy standard txt and then we'll grab this here and paste that on in and then we'll put in dual stack all right so we have standard and dual stack here um but the thing that we need to change is the endpoint so I'm going to say endpoint URL and I'm going to explicitly set this to be S3 and so we're in CA Central one so we have to write it this way if you're wondering like where I'm getting these from I'm sure we if we looked up ads s3x points we could probably see a big list of them somewhere in the documentation and so here they're showing all the possible ones so I said in the lecture content there is more than obviously two and there's a bunch of variants so the idea is that we need to find one that makes sense for our region so if I'm in Canada CA Central 1 I'm just going to look for that here all right so we have this this this like fips dual stack Etc and some variant and it's important to check because this end Point could vary uh in different locations it should be pretty much standardized but just in case it isn't you might want to check that out so I'm going to go ahead and um paste this in here and we'll go back over to here the other thing we want is dual stack this one here as well Noti there's like a double Aster on the or on the bottom that usually indicates something here supports request for S3 buckets over IPv6 and ipv4 that sounds great great so that's all they're saying there and uh so for this one we'll just trying to paste it somewhere here that won't mess up we'll paste that in and we'll say endpoint URL and we're just expecting both of these to work right that's all we're looking to do here so CA Central 1 S3 CA Central 1 that looks good so let's go ahead and run each of these commands so we'll go ahead and before we do that I'm going to just make sure I'm in the correct directory so I'm not making a mess here there we go and so we will touch standard and hit enter we will touch dual stack and hit enter and so those two files are now uploaded and they both worked and there was no issue so that's really all I want wanted to demonstrate there that it's as simple as that and it should just work let's go take a look and see if those files um are in that bucket so this one was called I don't know what we called this one what do we call this bucket endpoint test there it is and so there are the two files so pretty straightforward I'm going to go ahead and clean up so we'll go here say ads S3 remove we'll grab this and we'll paste it in here say dual stack so remove those two there those two files and then we want to delete the bucket so we'll just go ahead and do the rest here so these three commands should clean this up just going to go ahead and put in our markdown here there we go so let's copy this one enter let's copy this one enter and then we can delete the bucket because we need to empty the bucket before we can delete it and there we go so that's all I wanted to show you and we'll see you in the next one ciao let's take a look here at S3 storage classes as there are quite a few for S3 um so adab us off offers a range of S3 storage classes that trade retrieval time accessibility durability and cheaper storage the idea is the lower down we go on this uh slide here the cheaper the service should get I put a big Aster there because it's going to be really dependent on your use case as you know in some cases it could be more or less depending on how you are using these storage classes the first one and the most important one for you to know is the S3 standard which is the default class whenever you create an S3 bucket it's going to be this one it's fast it's available it's durable uh so there's not a lot to say about it but it is a solid bucket when you are not sure um you know how you plan on utilizing your uh storage okay the next one is S3 reduced redundancy storage and you're going to notice that it's gr out because this is a Legacy Storage class the purpose of this um storage class was to give you a reduced cost of S3 standard by having uh uh less redundancy in storage however S3 standard has come down in cost over time and so this one doesn't really fit any kind of modern uh storage scenario but it still is something you can select in the Adis console at least for now uh but you really should not be using it then we have S3 intelligent tearing so this uses machine learning to analyze the object usage and determine the storage class uh this is great if you have a lot of objects that are not being uh optimized and uh if you uh don't mind being charged an additional fee to analyze uh the objects then it could definitely save you money the next one is S3 Express one zone so this provides a singled digigit millisecond performance so it's super fast it has its own special bucket type it only operates in one availability Zone that's why it's called one zone um and it's 50% less than standard cost so I imagine because it's only in one availability Zone if that availability Zone gets blown away you can lose your data so there might be um some sensitivities around that we have S3 standard IIA so the Ia is infrequent access um anytime you see IIA you should think that there's an additional retrieval fee because there always is it's fast just as fast as S3 standard but it's cheaper if you're not going to be accessing your data often so they suggest I think it's like if it's less than one once a month then you're going to end up saving money and being in standard IIA so it's not exactly archiving but it's some somewhere in between there is an extra fee to retrieve it's 50% less than standard um and the difference is of course you have reduced availability that's why it has I in the name you have S31 Zone IIA so um this one is going to only be in a single a um just like Express one zone but it's also IIA so it's infrequently accessed so the idea is that it's even cheaper than a standard IIA by 20% less uh of course it it's going to have reduced durability so your data could get destroyed um so you know again make sure that it makes sense for uh the type of files that you have and there is of course an extra fee to retrieve then we're getting to S3 Glacier so we have a whole section on S3 Glacier because it's a little bit more complicated than that kind of feels like its own service but the idea is that you have uh well ad now has three um storage classes for it we have S3 Glacier instant retrieval flexible retrieval and then deep archive a lot of times we'll group instant and flexible together as one thing and we'll probably do that in uh some other future slides so if it just says s Glacier just assume we're talking about instant and flexible uh even within uh these tiers like look at flexible retriever de archive you can break it down to standard expediated and block retrieval and that's going to determine how fast you want to get back those files and those tiers um as well as how much you want to pay because the faster the more you're going to pay for that retrieval cost so that'll make more sense when we go more deep into those sections then adus Outpost uh for S3 has its own storage class Outpost is exotic um offering that itus has where you can have hardware and software within your own data center so they bring the racks uh to you with the servers there with all the software pre preloaded but it's not like the full version of what you get in Adis Cloud so there are going to be limitations and differences there butth yeah there you go let's take a look here at our first storage class which is S3 standard and it's the default storage class when you upload to S3 without specifying a particular storage class it's designed for general purpose storage for frequent Access Data it has a durability of 119 I'm going to tell you right now all the storage classes have a durability of 119 which is interesting because some storage classes uh are one zone so they have fewer um uh availability zones meaning like they're in fewer locations but they still have 11 NS because in terms of durability uh 8s is going to make multiple copies even in a single a and so that's how we can have 11 nines of durability for availability it has four nines of availability so it's it's it's quite highly available compared to the rest of the storage classes for redundancy it's stored in uh three or more availability zones and I do believe that they make additional copies even within an a uh for retrieval time it's within milliseconds it's not the fastest um so when we say milliseconds you know milliseconds can go up to a thousand or less right so a th000 equals a second so understand that could be uh three digigit twood digigit even one digit but uh there is a specific class that is a singled digigit uh retrieval time which is that new class for throughput it's optimized for frequently accessed uh uh data and uh or if you require realtime access for scalability it can scale no problem to any number of requests uh it's use cases it's ideal for a wide range of things uh again we said this is general purpose so we don't have to name them all but you know basically if you don't know what to do uh standard is totally fine the way pricing works for this is you pay per storage of gigabyte per request there is no retrieval fee where some storage classes uh it's going to cost to retrieve the data and there's no minimum storage duration so you can um uh store things in here and delete them whenever you want okay reduce redry storage is a Legacy Storage class to store noncritical reproducible data at lower levels of redundancy than Amazon S3 storage so this was introduced back in 2010 and at the time it was cheaper than standard storage for that reduction in uh redundancy which means that you know if if uh there was an issue you could end up losing your data so that's why they say storing noncritical data however since 2018 something has changed within uh ad's infrastructure where S3 standard is now cheaper than using RRS and I think that uh people thought that they had some kind of similar uh infrastructure or architectures but when this happened clearly RS was architected a completely different way uh than S3 and it doesn't seem like it's ever going to um become cheaper uh and so basically it's not really recommended to use RS and it has no fit in any of the modern storage use cases because it's never going to be cheaper now it's still available in the actual ad console because some Legacy customers probably still have objects stored in there and it's really hard for ad us to um get rid of features but you're not supposed to use RS I don't think the exams will focus on it but just in case you ever hear the term RS you know what I'm talking about you understand that is a Legacy Storage class okay S3 standard IIA the I saying for infrequent access is a storage class designed for data that is less frequently access but requires rapid access when needed it has 11 NES of durability like S3 standard like every other storage class that is recommended for use uh its availability is only three nines as opposed to four nines like S3 standard that makes sense because this is for Less uh frequency accessed data so it's going to have a reduction in availability for redundancy it's still stored in three or more availability zones for cost effectiveness it's 50% less than standard uh but I need to point out to caveat is that as long as you don't access the fall than more than once a month and the reason we say that is because it has to do with retrieval fees so yes it is cheaper as long as you're not fetching the file a bunch of times the retrieval time is within milliseconds it's not single digit but it's still really fast like gu three standard same throughput as S3 standard same scalability as S3 standard you're going to want to use this for dis recovery backups longterm data stores uh but you still need to be able to get access to the data um in real time in terms of pricing it's storage per gigabyte it's you pay per request it has a retrieval fee that's the important thing that's why you don't want to uh request things often otherwise that 50% Advantage is not going to work the way you think it's going to work and it has a minimum storage duration charge of 30 days so if you were expecting to only have that data for a few days to save money uh and then delete it it's not going to work with this class and so there's a few where there's a few classes where you have to have data sitting there for a period of time okay so there you go hey this is angrew brown and this follow along we're going to take a look at changing the storage classes via the CLI uh so you know there's a lot of storage classes we're not going to get into all of them because it's just way too much work and uh it's really hard to set up the scenarios but we should at least know how to change between them so let's go ahead and make our way over to our ad examples repo as we've had uh been using this entirely through um this course and we're going to go ahead and open this up in GitHub actually I already have it open here I'm going to make myself a new folder and yes I have a lot of folders because they get created at different times I'm doing this all in different orders but I'm going to just say here change uh storage class and what we'll do is create ourselves a readme file in here and we'll go ahead and create ourselves a bucket so let's just say create a bucket and this one will be called adus S3 S uh and we'll say MB for make bucket S3 colon SL slash and I'm going to call this uh class fun ab and then some numbers here AB is for my initials you do whatever you want to get the bucket to create and we'll just say create a file and I'll go ahead and touch a file so we say touch not touch we'll say Echo hello world and then we will send this over to here and we'll do adabs S3 copy hello.txt S3 colon class fun AB 6 3 4 6 okay so we'll go ahead and create that file before we do that let's make sure we are in the correct directory since I don't want to make a mess wherever I am there we go and we will run this line that is not what I wanted to run sometimes the copy and paste does not work here for me and we'll go ahead and make sure we can paste that file up there so we have that file up in our bucket let's go take a look and in here we have this function here or sorry this uh file here and we should be able to see its storage class somewhere in here so we'll scroll on down and there it is so if we wanted to you can see we have all of our options right we have standard intelligent tearing IIA one zone Glacier reduce redundancy a lot of options there notice for um uh intelligent teering that when we choose it we're not choosing where it goes underneath it's always going to start in standard and eventually move to where it needs to go so we're not going to really play around with intelligent hearing but I think moving from standard to standard IIA is is kind of important to know um would the bucket let you set a default we might be able to do that to set a default um standard or storage class I don't see that here so maybe we'd have to do that through a life ccle policy but let's go ahead and change this storage class to standard IIA so the way we're going to do that is we're going to open up the adli because I cannot remember the command here but I have a feeling that we can probably use S3 as opposed to S3 API let's take a look here and see what arguments we have for copy and I'm looking for the storage class in here so I'm going to just search for storage class there it is and so we should be able to change it here all right so I'm just going to grab that flag we're going to make our way back over to s or uh to our Cloud developer environment I'm going to paste this in here and then we're going to grab this standard IIA and I'm going to go to the end here whoops whoops whoops whoops whoops whoops that copy paste is a nightmare okay so this I think should hopefully change our standard class so we go ahead and paste that on in here going try again hit enter so it's uploaded let's go back here and take a look and are we in standard I we are so there you go it's as easy as that obviously other storage classes are more challenging um if we want to do one zone Express that is with a special directory type so we're not obviously going to try that but that's sufficient enough let's go ahead and clean this up now clean up so I'm just going to remove this bucket and we want to remove the file okay I'm just going to copy both of them to save myself some time here there we go and we are done we'll see you in the next one ciao Amazon S3 Express on Zone delivers consistent singled digigit millisecond data access for your most frequently accessed data and latency sensitive applications this particular class gives you the lowest latency out of every single storage class it is 10 times faster than S3 standard it costs 50% lower than S3 standard so it is really really really coste effective um the data is stored in a user selected uh single availability Zone as it says one zone so it only is in one zone the data is in stored in a new type of bucket called an Amazon S3 directory bucket for this type of bucket it supports simple real folder structure which is not the norm for uh S3 buckets uh where you're only allowed to have 10 by default uh in your entire account so generally when we talk about S3 buckets we say they don't actually have real folders this one has real folders Express one zone applies a flat rate request per charge for request sizes up to 512 Koby so even though it is 50% lower than S3 standard it does have this one caveat of expense I don't know if I mentioned it in standard IIA but standard IIA I I think has a reduction cost of 50% to S3 standard we covered in other places here so if I miss it it's okay we'll cover it two three times over um but just understand that this thing is super super fast um but you know it does have that exotic bucket type and it does have that additional um uh charge so so you have to factor those in okay we are taking a look at one zone IIA storage class which is designed for the data that is less frequently accessed and has additional savings that reduce availability no the name it says one zone in IIA there's two other storage classes that one has one zone in the name and one has I in the name if you understand those two components you know exactly what this class is going to do it has a high durability of 119 as all searge classes have it for whatever reason uh it has a lower availability than standard and standard IIA so standard IIA has um three nines because it's infrequently accessed so of course it's going to have lower availability this one's even lower because it only runs in a single zone fewer zones fewer availability that's why they're called availability zones the storage effective uh or the cost effective storage here is 20% less than standard IIA I don't think I made a good enough emphasis on it in the last in the last one we talked about standard IIA standard IIA is 50% less than standard and then this one is 20% less than standard IIA so even even more cost effective uh for uh data redundancy data stored in one availability zone so if there was a disaster in that AZ you'd lose all your data so you might want to do cross uh region replication if the data is super important to you um but then again you're kind of getting the territory of of having standard IIA where it's going to be in multiple azs um but uh in another region of course it' be a bit different I think so retrieval time is within milliseconds even though it says one zone in the name this is not expressed so it's not going to be single digit milliseconds it's just as fast as standard and standard IIA the use cases here is ideal for secondary backup copies of on premise data for storing data that can be recreated in the event of an a failure is also suitable for storing information frequently access data that is non Mission critical for pricing we have storage per gigabyte per request there's a retrieval fee and it has a minimum day charge of 30 days um I do yeah that's about it so there we go all right I want to be honest with you and for some of the videos I was a little bit fuzzy about uh the storage classes because um I guess I wasn't 100% aware of what was going on but now I 100% understand it and I want to make sure that you understand it too and that's why I have this additional slide here and what we want to do is compare S3 Glacier Vault with S3 Glacier storage classes so S3 Glacier and I remember this when it first came out but I I could have swore it was just called Glacier and then they added the S3 in front of it to make you think think that it's part of the S3 service it's not it's a standalone service um from S3 and it uses vaults instead of buckets okay uh it's the original Vault service or archiving service for longterm objects it has these things called Vault policies uh everything usually is done through the a CLI there's not a lot you can do through the console uh and Enterprises are still using Glacier Vault probably because they have data that's sitting in there for seven years so the service isn't going any uh anywhere away there is a subservice called Glacier deep archive this is also part of S3 Glacier Vault so um just understand that there's a thing on the Vault side I really think they should call the service Glacier Vault so there's and dropped the S3 so it's less confusing but whatever they called S3 Glacier and they don't put the word vault in there so what happened was eight of us decided hey people want to be able to Archive supp let but they find it really hard to use Glacier Vault so they introduced a new um storage class called S3 Glacier instant this has nothing to do with S3 Glacier Vault it's entirely new class new technology allows you to instantly retrieve things from archival Storage then there are two similar looking classes which is S3 Glacier flexible and S3 Glacier deep archive these things literally map up to uh S3 Glacier Vault up here and this one maps to deep archive they are using from what I understand the apis to have Vault so maybe ads is managing a vault for you and they're just not telling you uh but as far as I understand it's a combination of the S3 API and the S3 Glacier Vault API um and when you work with these two classes these ones are different from this one uh this one will use the same stuff like determining the retrieval time so you can choose between standard exped expediated or bulk uh deep archive just has um standard and uh bulk but uh you're going to find some similarities there and just understand that these two ones are unique this one is uh brand new and uh it is confusing but the great thing about these S3 gr storage classes is that everything happens within S3 so it's as easy as changing other storage classes for the most part um but we'll cover that in more detail about um uh in our archival section like how to actually retrieve objects okay S3 Glacier instant retrieval is a storage class designed for rarely access data that still needs immediate access in performance sensitive use cases because normally when you use S3 Glacier it's for archival storage and so the assumption is you're not going to have it immediately accessible so this is a very special class it has 119 out of durability I've said this repeatedly but at the time of making this video All Storage classes have 11 NES of durability this has three9 of availability just like standard IA so not 49 but three9 um its cost Effectiveness is 68% lower than standard 8i that's crazy low uh it's great for long lived data that is access once per quarter so again there's retrieval fees just like how standard IIA you only want to access it once per month this one you want to access it uh the files uh once per quarter or not at all the retrial time is within milliseconds so it's super fast this is really great when you need rarely accessed data that needs immediate access image hosting online file sharing applications Medical Imaging and health records news media assets satellite and aerial images the way the pricing works is you have storage per gigabyte per request there is a retrieval fee we're going to assume that the retrial fee is more expensive than standard IIA cuz it makes more sense um has a minimum storage duration of 90 days so understand that uh you know that's an additional thing you have to consider in terms of cost that you're going to be at least charged 90 days for an object that is placed in there I need to point out that Glacier instant retrieval is not a separate service and does not require a vault uh just to make that clear between uh S3 Glacier Vault and these S3 storage uh GL classes that have the word glacier in them okay S3 Glacier flexible retrieval uh combines S3 and Glacier into a single set of apis and it's considerably faster than the glacier Vault based storage let's unpack this because this is what the docs say and it's a little bit confusing um because sometimes when you read databus documentation uh they're incomplete uh or they're just not well ridden for the most part they're really good but as of late um and specifically with gler this one was really confusing for me so um it says formally S3 Glacier now there is a service called S3 Glacier I like to call it Glacier Vault because it uses vaults instead of buckets this still exists it's not going anywhere customers are still using it because if they were using Glacier Vault 5 years ago they still have files that are locked in there so it's not going away um and Enterprises are going to continue to use it because they're used to it so understand that that service isn't going anywhere so when they say formally they just mean that it is leveraging the technology underneath um and instead of using Glacier Vault they would recommend that you use uh Glacier flexible retrieval instead and I just want to point out how confusing the docs are at this time whereas when you try to read up on Glacier Vault it will tell you to instead read about storage classes and then storage classes will then point you back to Glacier vault in a circle and you'll get no information it's great so uh there are three retrieval tiers for uh Glacier flexible retrieval these are the same ones that you would use for Glacier Vault the faster they are the more expensive they will be so expediate is going to cost more than the other two here this is when you need things in 1 to 5 minutes and you only can grab 250 megabyte archive file sizes for standard there's no archive limit but it's between 3 to 5 hours and then for bulk this is for 5 to 12 hours there's no archive limit it's going to be for pedabytes worth of data you're going to be charged for uh retrieval fee and the number of requests and then there's the cost of storage so you know similar to standard IIA uh there is also um there's also an additional 40 kilobytes of data that is added with every single archived object uh 32 kiloby is for the archive index and metadata information that's so they know how to retrieve it and store it and things like that and 8 kilobytes for the name of the object I don't know if I put this in any of the other slides but I'm pretty sure that 8 k is always allocated for the name of an object even I think even for standard but it only matters when you're retrieving data because you're you're paying based on the retrieved information um and so the thing that I need to point out here that's really important with Glacier flexible and this also applies for deep archive I'm not going to talk about in the next slide but I want you to remember this is that um you really should store fewer files and larger files in these uh storage classes because if you store thousands and thousands of files it's going to add 40 kilobytes every single file whereas if you take them and put them in a big zip file it's going to only add 40 kilobytes to that large archive and that's how you're supposed to use these classes so just remember uh that caveat there I'm going to point out that Glacier flexible retrieval is not a separate service and does not require Vault uh 8s might be providing a managed Vault underneath we don't know we can't see it um but uh for your purposes you just have to know that it has nothing to do with S3 Glacier vaults and you're going to be working with buckets even though probably is using vaults underneath S3 Glacier deep archive combines S3 and Glacier into a single set of apis it's the most coste effective way of storing archive objects in S3 period um so there are two retrieval tiers we have standard at 12 and uh 12 and 48 hours there's no archive size limit this is going to be the default you have bulk tier this is when you're working with pedabytes worth of data same time period I need to point out there is no expediated tier for Glacier deep archive whereas there is uh for S3 Glacier flexible retrieval it still has that 40 kilobytes it's still going to charge you uh like a similar scheme to S3 Glacier flexible retrieval it's going to cost more because the retrieval fee is going to cost more because you're going to be uh storing at a lower cost you're trading super low storage for a higher retrieval fee I don't list the prices here but I'm telling you that it's cheaper Glacier deep archive is not a separate service and does not require Vault just to not get it mixed up with S3 Glacier vaults there you go S3 intelligent tiering storage class automatically moves objects into different storage tiers to reduce storage costs but charge a low month cost for object monitoring and automation so the idea is that you can choose the storage class um view the API or via the console and by default it will go into um the frequent access tier notice that it says automatic meaning that these things are turned on automatically and as soon as it's in here this all these rules will start applying so it's kind of like having life cycle rules but it automatically happens for you and you don't have to configure anything I guess that's kind of what it is so again you know frequent access tier is the default tier you can think of it as S3 standard objects going to objects are going to remain in here as long as you are accessing them if you do not touch them for 30 days they're going to get moved into infrequent access tier this is standard IIA uh if you don't touch them for 90 days it's going to be moved into archive instance access tier this this is that S3 Glacier instant retrieval now there's the uh two other tiers we have the archive axis tier this is the S3 Glacier flexible tier and the idea is that when you turn this on instead of going to instant it's going to go here instead after 90 days you're going to get greater savings and there's more options to configure in terms of retention other stuff there but the point is is that after 90 days it'll go here instead you'll save more money um you can also turn on or activate deep deep archive access so after 180 days it'll go here and you'll keep saving money again I want to point out that eser intelligent hearing has additional cost to analyze your objects for at least 30 days because if it hasn't had access for 30 days it would never move it into the in infrequent access tier um but anyway the point is it's turned on there's that additional cost and then there's obviously the cost of those tiers that you normally would have and you can pretty much map them to what they're supposed to go to okay okay so we have all the S3 storage classes that I could fit in here the only one that's not here is reduce redundancy storage the intelligent tiering have their own access tiers but they're reutilizing existing uh tiers in here so I'm not going to uh include them redundantly here uh let's talk about uh this matrix it's not perfect but it was the best way I could fit everything into one screen so durability as I promised is 119 across the board notice I highlighted the ones zones uh because they are special they operate in a single AZ and because of that yes they have 11 NES of durability in that a because they're replicating copies of the data in that single a but if that whole a goes out you're going to lose that data so from that perspective from a multiaz perspective it's not as durable um but it still gets 119 technically for availability it obviously varies standard is the highest then we have intelligent tiering with which it says 99.9 but I think it's going to be really dependent on whatever it's utilizing underneath um for standard IIA it has uh uh 99.9 for Glacier incident 99.9 then the one zones are going to drop to uh one zone IIA is 0.5 and this one is 095 so and I don't think that's a mistake I think it is 0.5 on that one glaci you're flexible an archive they don't specify uh because you don't get them immediately right you get them minutes later so it's not really uh the same level of availability and that's why we mark them as na for azs everything pretty much operates in three or more with the exception of the one zones for minimum capacity charge per object um standard IA and one zone IIA is going to have 128 kilobytes in fact I think you have to have at least 128 kilobytes uh for these uh these ones in particular for IIA for flexible and deep archive these ones are a bit special because that's coming from the metadata and um it's on top of whatever you're putting in there so it's not oh I'm only going to put a 40 KOB file in it's I put a file in it's going to add 40 kilobytes to the file um so that is something that is happening there for minimum storage duration uh for intelligent tearing at least 30 days because it has to analyze the fall for 30 days before it can move it to somewhere else um as far as I understand that's how it works um I could be wrong on that where it's not 100% you're going to be charged for the 30 days but for this to be effective it needs to be in there for 30 days to move it to another tier for standard IIA and one zone IIA you have to have them for 30 days if you put an object in there and you try to take it out you're going to get still charged 30 days for instant and flexible it's 90 days for Glacier deep archive it's a minimum of 180 days and you know if you lock down your stuff right it's going to you can't get it out there for you're going to pay whatever the the the the permanent lock has in place for retention hold so retrieval fee um we're only going to see those for standard IIA one zone IIA Glacier instant flexible all those archival tiers there uh for uh first bite latency Express one zone is single digits everything else is milliseconds even Glacier instant and then these ones obviously vary based on how fast you want your data um for intelligent teering there's additional monitoring cost to use that um that knowledge underneath remember intelligent hearing is basically doing kind of what life cycles does but but automatically for you and then Express one zone has an additional charge for requests for files larger than 512 Kobes they want you to serve up smaller files and they're charging you if you go for larger files so that's the logic there okay taking a look at the security overview there was no way to do this other than to make it a big bullet point pointed list I just want to give you the scope of all the stuff in terms of S3 security that we should know um and we go more deeper on all these little things here so I'm just giving you a broad overview before we dive into it looks like like a cheat sheet but it's not just trying to get you the information so the first thing are bucket policies these Define permissions for the entire S3 bucket using adjacent based access policy language if it looks a lot like I policies it is it's just scoped around a particular bucket you have acl's this was a legacy feature it's still uh you can still use it but nobody nobody recommends to use it it was a way to give cross account access uh to other 's accounts and it has very simple permissions you have adus private link this is a way of uh uh of utilizing a connection from an eni uh to S3 to keep all connections internally within the a network um it does cost something but it is also very robust I don't think I have a list here but the other thing is Gateway endpoints um which is similar to this but free and we talk about that when we talk about inter Network privacy which is on this list as well cores so cores allows you to access things on other websites from your website this applies to S3 static website hosting since we can set S3 cores and say hey I want to be able to download these JavaScript files to execute on my website that's not from the same domain you have S3 block Public Access probably the most important security feature it prevents you from using acl's or bucket policies unless you turn this feature off I think also with access points as well um and there are services around that to make sure that you're aware when it's turned off so you have IM access analyzer and it will tell you hey you have block Public Access off and then these files are are open to the internet or open to these particular people is that what you actually want to have so that's what that does there enter Network traffic privacy this is just the concept of making sure that your data is uh secure by staying within the internal network of AWS um and so that would use private link uh which is interface uh interface endpoints from vpcs and then you have Gateway endpoints from vpcs and both can be used here uh we have object ownership so this is when you're saying who owns the data and that's just more to determine who pays for it when you change the ownership uh there's some security considerations to uh to consider or limitations that we should be aware of you have access points uh this simplifies managing data access at scale for shared data sets in S3 so the idea is that um instead of writing these giant bucket policies you can write subsections of permissions and put them on a separate endpoint to make management to multiple people easier you have access grants this is when you want to make your data available uh to identities or principles within a directory service like active directory you have versioning um so you can upload multiple versions and it doesn't really delete the file it just uh unv verions or uh makes the the version uncurrent and has delete markers you have MFA delete to prevent the deletion of files without uh without first confirming uh that you want to delete via multiactor authentication only root users can delete them so great great security feature object tagging so we can organize our objects um from a security perspective tagging is of course very important to take inventory and understand um you know what kind of maybe you have like encryption on particular files or files are sensitive so you want to mark them as such so you know uh in transit encryption so understanding the concept of uh TLS and that uh in transit encryption is not the same as serers side encryption which we'll cover as well there's a lot of stuff to cover in serers side encryption then we have client side encryption then there is compliance validation for Amazon S3 so this is just understanding that S3 meets the requirements of HIPPA gdpr and things like that uh then we have infrastructure security so this is understanding the under Ling infrastructure for S3 Services the data integrity and availability so the last two I'm not 100% sure if I have slides for them right now probably in the future I will but uh you know this is this is all the stuff you got to learn about S3 S3 is a core core core service so uh if you're going to learn about security you definitely better learn about S3 security okay let's talk about block Public Access for S3 as this is probably the most important security feature that you need to know about S3 uh and this is a safety feature that is enabled by default to block all public access to an S3 bucket because the number one security misconfiguration is S3 buckets when uh when people have anous access or unrestricted access to your S3 buckets so it us has made really really sure that uh that this is not going to happen to you uh and they have way more than just this single measure to uh detect and tell you about uh whether you have open buckets but there are four options uh that we have here so we have block all public access but once we check that off we can kind of narrow down exactly what we want to have it for so uh they have um a checkbox for whether you have new Access Control lists and whether uh they're allowed to um have uh Public Access then it could be for any control lists then it's only for new bucket policies or access points or any bucket policies or access points it used to just be for new bucket policies any bucket policies but then they added access points they lumped them together I'm not sure why they didn't make six checkboxes but that's what they wanted to do um so very straightforward the times that we're going to be turning off block Public Access is when we want to give an nominus access that could be you know when you are trying to provide static website hosting or um you want people to publicly able to download readon files from S3 so there are a few use cases but more more or less you can uh work around that issue by having cloudfront in front of it and reduce uh your security risk there but uh I just want to point out that access points S3 access points which we'll talk about it's a it's a separate um feature of S3 can have their own independent block public access settings uh which we'll see when we get to that section okay let us talk about access control list within Amazon S3 so acl's uh Grant basic read WR permissions to a other accounts you can grant Grant permissions only to other 's accounts you cannot Grant permissions to users in your account you cannot Grant conditional conditional permissions you cannot explicitly deny permission so why does acl's exist well they've been traditionally used to allow other A's accounts to upload objects to a bucket but the thing about this feature is that it is a legacy feature um it was uh this is before bucket policies but it us wanted to make some way for other accounts to have easier access to S3 so they came up with this feature but nowadays you wouldn't really have any reason to use it as there are more more robust ways uh to provide cross account access via bucket policies and access points um and to be honest I can't remember even how this service works because the only time I ever use it is when I'm making educational content uh that's how few times it ever comes up in terms of practical use case but we do need to note for the exam the thing you want to remember with this is that it is bucket policies but way more limited and it doesn't Grant user access it's for Access between A's accounts and you do not have much controls over your permissions if that makes sense there you go hey everyone it's Andrew Brown and in this video we're going to take a look at ACL or Access Control list for S3 it is kind of a legacy feature and I wouldn't really use it for any use case but there could be customers out there that like to use it for its simp icity or they're using it for um I don't know just it's just what they want to use so we should still know how to do it um even though I haven't done it in a long time but let's go and work our way through this so what I'm going to do is make my way over to the adabs examples repo I'm going to make a new folder in our S3 here and I'm going to call this one um acl's and what we'll do is go ahead and just create a readme file so we can kind of work through this as we're doing this I'm going to of course create a new bucket as per usual we always seem to create a bucket using the adus S3 um uh adus S3 commands I'm going to use the adus S3 API command just to mix it up here today and we'll go ahead and say create bucket it actually might be put bucket let's go take a look here because I really can't remember we'll go up here and we'll make sure we're on version two and we'll go S3 API and we'll go all the way down here below and uh you know again I think it's crate bucket it is Crate bucket okay and so this should be pretty straightforward we'll go down to examples bucket region okay great so what I'll do is I'll just say um bucket and this bucket name is going to be ACL example ab and I'll put a few numbers AB is from my initials put whatever you need to do to make sure you don't have naming complex and I can just actually explicitely set the region as us East one for fun and so that will create us a bucket and so we'll go ahead and copy this and we'll go hit enter and so we've now created a bucket here um I'm going to want to place some kind of file in here so I'm just going to create a file uh maybe what we'll do is we'll wait first and and look at our ACL options because maybe we might want to permit only specific things um based on the permission access but I'm pretty sure acl's are very simple so it's either all or nothing but I guess we'll find out here in a moment in order for this to work we're going to need another adus account you all you have to do is sign up with another ads uh another Adis account you can use the exact same credit card card it will start on free tier and that way you can practice uh cross account stuff cross stuff is really important to get your um uh your hands dirty with here and so this is a great example of one you can also use adus organizations to create another adus account I'm not doing that here I just have an isolate account in a separate organization um but yeah again the simplest thing is create two two ab accounts with the same credit card but anyway what I'm going to do here is I want to figure out how to turn it on via the CLI now of course uh for our bucket we can go do that in the um the console so we will go over here and we will ACL example down here excellent and if we go to properties there should be something for it' probably be permissions permissions are access points we should be able to apply ACL so here it is so this bucket has the bucket owner en Force setting applied for object ownership that's cool uh and when the bucket owner enforce is applied use the bucket policies to control access so notice we can't edit this at all and the reason why is that our block Public Access is turned uh on and so we need to turn this off and so um we have two options here block public access to buckets and objects granted through the new Access Control lists block public access to buckets and objects granted through any access control list so I want to uncheck boox both of those so that we can uh fiddle with this so that's what we need need to do first and again going to use the CLI because it's always good to get practice that way so I'm going to look if we can um change the way I put public block Public Access crater modifies the public access block configuration so that sounds good to me it says we have to have that permission that's fine we'll go down below and yeah that's that's that's what it is so we'll go ahead and copy it we'll say turn turn off block Public Access for acl's and I'm going to go down here I'm going to paste this in and um now that we have this I'm just going to again put that in back ticks for markdown just so it's a bit nicer for those who are reading in the future and we have our bucket name I'm going to go ahead and copy that here and paste so we have block public acl's true ignore public acl's true so I want to go here and say false and false okay notice that it has double quotations around here you don't have to have them there but uh it's a bit nicer when you do so I'm just going to leave them there so I'm going to go ahead and paste this in and hit enter and so now our block Public Access should be different we can go find out by using probably the get Public Access command so I'm going to just try this and see what happens I'm guessing it's just get you start to uh again find out over time the patterns here for the different CI commands we go ahead and hit enter and so we can see this is false this is false this is true this is true we can confirm this in the console we'll give this a refresh and in the console here we can see um block Public Access is off individual block Public Access setting for this bucket let's go hit edit so yeah it's turned off how we want it's kind of weird that it doesn't show it very clearly here I guess it it is it's just really gray out I'm sure they'll change the UI on us at some point here um but now notice that we still can't edit in here so that's something that's interesting so when the bucket owner en force is applied use the bucket policies to control access so I guess there's another thing with object ownership that I didn't realize ACS are disabled all objects in this bucket are owned by this account access to this bucket and its objects is specified using only policies didn't know that that's kind of cool so the idea is that we now also have to enable acl's over here which is interesting so I guess the question is where would we do that in the CLI and the reason I like going to the CLI a lot is that you kind of understand that there is abstractions that the console is putting and and not always the console matches the API end point points uh so we do have um ownership controls here so I'm going to go here and say put and we'll go down to examples and uh what are our options here bucket owner enforce bucket owner preferred object writer that you want to apply the Amazon S3 bucket the container elements of the object ownership of the buckets ownership controls objects uploaded to the bucket changes ownership to the bucket owner if the objects are uploaded uh the uploading account will own the object if the object is uploaded to the account so this is really useful because remember we have to uh there's requesters pay so later on when we do requesters pay this is going to um take in effect I imagine so that's that's interesting there so what we're going to do is change this to bucket owner preferred and see if that makes it happy let say uh change bucket ownership paste we'll go ahead and just copy the bucket name here and paste it in here and we said that we wanted to be preferred instead of enforc so that's something that I'm going to change here so say preferred here and I'll go ahead and paste that on like that and so I'm thinking that's going to do what we wanted to do I mean that's my guess so we'll go back over to here to the console we'll give this a refresh I'm not sure if it let us refresh um I guess it will and so now it says acl's are enabled bucket owner object is preferred so that is good um that means now we we should be able to set ACLS so if we go back over to here to the bucket and we go to permissions and we look at acl's here it is we now have the ability to change it right so here are our options so um bucket owner you're allowed to list and WR and the ACL axis allows to have read and WR um everyone so I assume that this would mean everybody so that's Public Access I'm going to assume that means internet access authenticated users anyone with en account that sounds really good I think I like that S3 log delivery group yeah I don't think we care about that so I think what we want is list oh no but this is anybody like all Global accounts okay so access for other AIS accounts no other a account associated with this resource so I can say add granty and we'll go ahead and from the other account we'll go and grab the account ID again I'm just guessing at this all right it's not like I 100% know how to use this tool but that's fine we have objects and bucket ACL so I guess this is for bucket control this is for object control so I'm just going to go ahead and checkbox all of them because I want this account to have full access I do not want to give public access to other folks or any of those other accounts and I don't care about the log delivery group we'll go ahead and hit save says an unknown expect error invalid ID um you sure about that let's let's clear that out again enter canonical ID all right let's take a look again here copy did it have like the hyphens when I pasted it in there no it didn't okay um we I guess we'll have to take a look at that I mean we can also set this through the CLI which is probably preferred here I'm not sure why I'm doing it here I just guess I got caught up and I thought we were going to set it there but we'll go ahead and do it through the CLI because CLI might show us an example and make it a lot more clear so uh change hcls to allow for a user in a another Abus account and we're going to go over to here and we'll look up access controls I'm going type access first nope how about ACL and there should be a put bucket ACL we'll go down below I'm going to copy this I I didn't read it but we'll take a look here and see if there's anything interesting we have the bucket we're saying Grant full control to specific emails that's kind of interesting and then we have the grant read uh URI so that looks like if it's setting for all users and specifying specific emails that's kind of interesting let's just go ahead and uh read some of these attributes so we have full control read read ACP write ACP so let's take a look at what ACP means allows granty to read the bucket ACL allows granty to create new objects in the bucket ACL this is assuming that we're going to provide it a string what string does it want it doesn't tell us expected bucket owner the account ID of the expected bucket owner if the account ID that you provide does not match the actual bucket owner request the request fails the HTTP St that's not very clear let's go take a look at the adabs uh ACL um documentation here and see if we can get a better idea of what needs to happen so we'll go here and uh we have a policy so that's kind of interesting Access Control let's when you create a a bucket object Amazon creates a default AC Grant uh Grant here this is shown in the following bucket ACL okay so we're seeing some grant information that kind of matches what we saw in the console but how do we do this view the CLI they're given us a lot of options here give me just a moment okay well I figured out one thing which is interesting uh you there's a canonical user ID for your a account and so it shows this as the alpha some kind of big long number here so it's not the account ID it's a user ID for the account so here so here we have S3 console so root user or I IM user uh find the canonical ID for the account go to account identifiers and find canonical user ID okay let's go take a look at that then um so this is for this person and I'm logged in as Andrew Brown on this one so I'll go to am I we'll try to follow follow what they're telling us to do I think it said identity providers no no it didn't I think we got to users and then we'll click into Andrew Brown and it was under security credentials and the idea was that it was somewhere here and I mean maybe they're talking about a value in here but that's the account ID huh okay let's go back up here and take a look again follow the steps to find the canonical uh user ID within your ads account I mean I would really prefer to use the CLI query owner ID use the list bucket command as follows to find the canonical user ID using the CLI okay well we can try that I mean if that's easier than navigating the console I definitely prefer to do that we'll open this one up here this is in the um other account my boot camp account we'll give it a moment to load I'll go ahead and paste this in and so we have this because the idea is that every time we create a bucket it's going to have the same owner ID if you're not seeing this you need to just go ahead and create a bucket really quickly but it was suggesting that we could find that somewhere else again I'll just double check here and read carefully sign into the console in the navigation bar on the upper right corner oh security credentials right so I always forget about this every time it happens but up here we have security credentials and then in here well it brings us to the same page we were just on okay well we did that oh it's right here you're probably staring at me being like Andrew what are you doing it's right in front of your face but when you're recording it's a little bit harder I think anyway um so now we have the canonical ID so I'm going to go back here and I'm just going to bring this on down and we're going to go back over to our uh put ACL options and so what I'm I'm thinking is that I mean it seems like you can provide yeah you can provide a policy right here but is that what that example is doing down below no it looks like there's also individualized Flags so we have this contains the elements of the ACL permissions for the object granty and then what's the point of all these flags allows the read write uh and read ACP write ACP permissions on the bucket again I don't know what ACP stands for so that's not very useful um so what I'm going to do is try to go ahead and create a policy um looks like you can create them in Json or XML so I think I'm going to work with uh Jason here I think that's what I would prefer so let's go ahead and just copy uh well let's go look at what the default one is first because they're going to have one here ACL and so this is the default one I'm just going to tell chbt turn this XML into Jason please and hopefully it will uh do that correctly I mean we could also just work with work with XML and and stop being so fussy for the time being I'll just work with X XML and I'll decide whether I want to swap that over so we'll just say policy. XML and I'm going to go ahead and paste that on in here we have the owner's canonical user ID owner display name Access Control list so I'm assuming this is me right and then down below here is who gets access that's what I'm thinking um so I'm going to go grab this one here again I'm just guessing we'll grab this one here and then we'll go and paste this in here like so and then their display name is I mean Andrew Brown so we'll do that full access full control seems good to me and then this one is also Andrew Brown so that makes it easier and I'm going to also use that uh shorthand command that they they suggested no I don't know where it is anymore but uh we'll just grab it from here then so we go into security credentials and we'll grab the KOC user ID for the actual owner which is um the other count here all right and then we'll come back over to here so we have uh put bucket ACL we'll bring in our uh bucket here and instead of using these flags because I don't trust them I'm just going to take them out I'm going to go ahead and just add in that control access policy so that here is Access Control policy and I always forget how to reference files even though I have a full tutorial on it uh files we'll just see if we can take a look here because the documentation should tell us how we can pass them maybe over here it is it's just file colon for SL for Slash the first two slashes are the parts of the specification if if the path requires uh to start with that then do that so I'm going to assume that this is relative I always just kind of forget if it is and sometimes some commands don't uh support it so then I I second guess myself even though I've done this like so many times so we'll go ahead and um try this because that's that's that's what it looks like all it needs really to me because those other ones just look like shorthand Flags to set them in a more convenient way so we going to go ahead and try this and see what happens um um it doesn't like something so I'll go ahead and hit enter so it says un unable to load pram file file colon p no such file or directory exists okay um I'm going to drag out the full path again and then we're going to just paste it in as such like this and we're going to go ahead and copy this again because it really should take it well hit enter and uh error parsing parameter expected equals okay so I'll put an equals here and then I'll provide it like this okay maybe this will uh satisfy it we'll hit enter what do you want for me I notice that when I pasted it in that uh no it had the that there there error processing parameter expected equal and got that instead so it actually is parsing the XML which is interesting um I'm not sure why it's expecting an equals so just give me a moment okay let me figure this out all right so I don't think I'm crazy I think it's just that some parameters while they look like they can uh receive um XML or Json I think that they just can't sometimes and this is where the confusion comes from because every single CI command is going to give you something different um so this is kind of frustrating and uh it's frustrating that they don't even have an example down below of of utilizing it that way but what I can do is probably try to dump that file to a string so I'm just trying to think here and down below they did a very bad conversion of that Json but I'll say um I have an XML file and I want to dump the contents of it to um to another command so I'll pass it here and see if we can do it so so instead of referencing the file dump the string directly to it let's see if chachy BT can help us here because that might be another way that we could um pass that policy along now here they're suggesting now this is not what I was thinking but here is suggesting that we should make a Json so maybe that is our issue and it's just out of date okay um give me a second I'm going to convert this over to Json and see if this makes any difference I have to manually do it because chat PT can't do it all right looking around here I actually found an example here under the granting and structure looks kind of similar to this but not really so um we wouldn't be able to do a direct XML to Json here and also the documentation does not show any Json examples so I guess for whatever reason the documentation is just not keeping up to date for whatever reason uh still showing XML examples but again I'm just going to write this really quickly here since I don't think you want to watch me fill this all out but the idea is we're just going to supplement it where we need to and I'll point that out okay all right so what I've done is I've copied over the um the owner's ID conical ID and it's the display name I'm going to assume it has to match whatever it is in your a account I'm not sure it looks like we have different options for granty I don't want to do an email address so I'm going to take that out um here we have to determine its type so this is going to be a canonical uh user so we'll do that and then do we need to specify the URI I'm not sure um I don't see it here so I don't think that we do so I'll take that out as well and I want to provide full control so we'll go ahead and do that and I think that looks okay to me so we'll see if we can actually use this with the a CLI if it doesn't I'm going to be really frustrated but we'll do our best here going take this out and let's go ahead and copy this and change this over to Json and fingers cross that it works we'll hit enter boom okay so there you go sometimes you have to work through issues even if you don't have the appropriate documentation um so that was well worth it okay so let's go back over to our um S3 bucket which is over somewhere here so go say look at S3 and let's see if the ACL policies has been updated so go to buckets and this one is for ACL so we just search for ACL down here below and I'm going to go over to permissions and we're going to see if acl's are set so looks like we have something here the console displays combine access grants for duplicate grantees so it might be obscuring some information here uh I'm not 100% sure but if we go down below notice we have our canonical uh user ID and these are checkbox so that's exactly what we were trying to achieve before so now the question is can I access this bucket uh from my other account so what I'm going to do is open up uh the CLI um command before I do that I'm going to switch over to I'm just go click here and make sure I'm over in Us East one I am good I'm going to open up the uh terminal here and so now the question is how do I access this bucket because I actually didn't think about that part um let's go ahead and do an adus S3 LS and see what we have so I don't think we're it's just going to show up here so I guess that's the next question is how do we access that bucket uh so I'll find out here one second actually one thought I had was maybe we could just try and specify the bucket name here in adus S3 uh LS and see what happens because maybe that would be the easiest way so I'm going to just give this a go and see what happens so here this is the bucket name and then back in the other account we'll paste in the bucket and it didn't eror out so there is no contents in that bucket um so let's try to place a file there I'm going to say touch uh this will be boot camp. txt because it's just for my boot camp account and I'm going to go ahead and try to move this into that Target there 52 35 and I actually have to specify the file or it's not going to do anything boot camp. txt we'll hit enter it uploaded it let's go ahead and Now list the contents of it great I'll go back over to um here and we'll take a look at the contents of this bucket it is there good we'll click into it I wonder if it would show us something about ownership of this object so I'm just going to type owner the owner is Andrew that's not very useful when we're both named Andrew but uh I'm GNA assume that uh that it's it's coming from the other account like that would be the other Andrew right um as the owner unless we can specify that when we upload it I'm not sure but uh yeah that's acl's and uh again it's mostly for utilizing it for cross account access as you notice there weren't any kind of restrictions on like prefixes or anything else it was very straightforward um but yeah there you go and what we'll do is just clean this up and um so to clean this up we will before we do that we'll just say like access Bucket from other account and maybe we'll just bring those commands in here so we had this one and we had this one and then we had touch boot camp txt again just trying to save this for anyone in the future that might be using the notes all right and we'll just do cleanup here so cleanup is very straightforward we will grab this here and go down below and I just want to go ahead and say ads sorry ads um actually we'll keep it even simpler than this S3 SL SL this will be a S3 remove bucket. txt or sorry Boot camp. txt and then this one will be remove bucket and we'll clear that out there and so I think that's just get rid of our bucket so we're all cleaned up delete what doesn't it like oh I got to put S3 in front of it right there we go so we'll go ahead and run that we'll go ahead and run that great so the bucket should now be 100% cleaned up and we are in good shape so I will see you in the next one ciao let's take a look here at s S three bucket policies which to me is one of the most important uh security features other than block Public Access that you need to know uh as you're going to be using quite a bit uh whenever you want to give people access to your bucket so S3 bucket policy is a resourcebased policy to Grant an S3 bucket and bucket objects to other principles such as another adus account users adus Services we said before uh that acl's is the old way of doing it and so something like a bucket policy is a lot more robust way and that's the re a way I would suggest for you to do it um the idea is that it looks just like an AM policy uh it's very similar except that it will it it will specify the principle uh and so for this example we're saying only allow a specific W to read objects with the prod object tag so if we go over here take a look we're saying allow the ability to read an object or an object version and it's only going to be for this role on this adus account um and it's going to be for this bucket everything in the bucket because it has an aster so that means everything and the other thing is that they have to have an existing object tag that's going to equal prod so uh a few things there I believe the object tag tag would be called environment right so we I think we talked about object tags have to um well anyway we'll talk about object tags later but anyway let's look at another example so this is restricting access to a specific IP so we are saying deny uh to anybody that is uh trying to use any s reaction for um the bucket or any of the objects in the bucket sometimes you see both uh these in the policies and then we're saying for this particular ipv4 address or ipv4 address range uh since it's not 32 it says 24 so that's multiple IP addresses uh there I believe anyway so there you go hey this is Andre Brown and in this video we're going to uh play around with bucket policies there's a lot that you can do with it um in this case what I want to do is do something that's cross account because any opportunity for us to do some kind of cross account programming or uh uh anything with adabs is really useful especially for the exams um so make sure that you have an additional adabs account you can use a credit card the same one you use with the other one to create a free tier account you can use ad organizations to create it all under one account it's up to you how you want to do that I'm not providing instructions uh here as it's very easy to create a second additional account but anyway let's go ahead and uh get started so as per usual we have this eight of us examples repository I'm opening it up in G pod um so that's what I'm using for that you use whatever you like to use I'm going to create a new folder in here I'm going to call it bucket policies or bucket policy here we'll keep it simple I'm going to create a readme as per usual and as we always do we're going to create ourselves a bucket so I say adab us S3 make bucket S3 slash um bucket policy example okay and um probably spell that right I'm going to put my initials on the end here and then put some random numbers we'll go and create this bucket great I'm going to type in clear and so what I want to do is I want to set a bucket let's go take a look at this bucket now and see if we can actually do that because normally when uh Public Access is on we're not B to do that so we go down to uh bucket policy example and we go to permissions we have a bucket policy knows that we can edit it when we did acl's we had to turn off block Public Access I mean maybe we didn't because all we had to do was actually change object ownership so it's possible in the last the last time we did the ACL one we didn't actually have to turn that off but um we can set a bucket policy here without uh toggling off BL block all public access um and so that's what we're going to do here of course I always like to uh do this through the CLI because we need as much practice as we can get they do have policy examples here so we can take a look here at the documentation and I'm looking for one that's going to provide access across accounts so we have required encryption you can see there's a lot of stuff that we can do here access to very specific folders so um I'm just going to get started with this and and see what we can figure out so let's go back over to here to um uh to our CLI actually we're going to have to go to the CLI commands so we'll say ads CLI S3 API and there's probably one here for bucket policy so we'll type in policy and there it is we're going to go all the way down to examples and we can see that we can load a file here so that's good and I'm just going to place this like this and we'll grab our bucket bucket name I'm going to go ahead and paste this on in here whoops it's not what I wanted to happen sometimes that copy and paste does not work great for me copy paste and I'm going to create a new file in here called policy Json when I run this command I want to make sure I'm I'm in the relevant directory so I'm just going to go ahead and do this cuz I think can pick up files locally um you can put a forward slash in front of there if we wanted to have an absolute link but that's not what I'm doing or absolute path um so we need to now uh implement this policy here so I'm going to go grab this one to start with and what I want to do is allow get object for our bucket so I'm going to go ahead and grab our bucket name here paste and I don't want to deny anything I'm just going to take uh that out and I want to specify very specific principle in order to access it so in order to do that we're probably going to have to create a cross account role I would imagine but uh let's see if there's any examples here in the documentation for cross account cross if the IM identity uh belongs to a different account then you must Grant a cross account access to both the IM policy and the bucket policy so bucket owner granting cross account bucket permissions let's go take a look here and this is I guess a full walkth through um so it's not that hard to create cross accounts but I don't do this every single day so it's not part of my uh permanent memory here so just give me a second to reload my brain okay so I keep looking at this and all I'm seeing here is them putting the principle of the user so I wonder if I don't actually even need a cross account uh a lot of times when you want to gain access to another edist accounts as resources you create a cross account Ro um and we can do that but uh maybe we don't have to and so I should just uh try something simpler here and just provide uh the principle to the actual uh value here so notice here it says for the root of whatever this count is notice this is fake because it says 1 2 3 4 5 6 7 8 9 10 012 that's how you know it's fake but what I'm going to do is go up to my other account if I can find it here and I'm going to find this particular user and we're going to test this and see if this works uh if not we'll have to spend a little bit more time here which is not a big deal but I'm going to go ahead and grab the AR so I believe that's what we want here and then we're going to paste it right into here so I'm hoping that that's going to say hey this particular user is allowed to get objects uh from this particular bucket now I'm not sure if we also need control to list the bucket so that might be another thing that we might want to provide here so I'm going to go ahead and just expand the permissions so things are a little bit uh easier for us to uh see here so going to go here and just say S3 uh list buckets I think that's a permission and these are all showing get object a CLI API permissions dat S3 permissions list and so I'm just looking for those actions it should have a list somewhere here for us they're not really showing it to me which is kind of frustrating um we can also get through the IM console so if we just go here and I go over to IM am and I go over to policies and I try to create a policy see I actually don't want to create one I just want to get access to that data and I'm going to go here to S3 I think it's just list buckets so we got list it's list bucket okay so I'll go back over to here I know this is just supposed to be list bucket and when you provide resources we'll probably actually have to specify the exact bucket as well so here I'm saying everything within this bucket but we also want to say um the bucket itself for probably this one here and I'm just going to put a comma there and I'll take this off I think that will work need a comma there and while we're here we might as well have put object as well just just in case we want to put objects so I'm hoping that that's all we need to really do to get access cross account and I'm going to go ahead and update my bucket policy and hopefully it won't complain so go ahead and hit enter it says the policy contains inv valid Json let's take a close look here yeah there's a weird comma here that's not supposed to be there we'll go and hit up and we'll try this again the bucket policy has been applied now just because it's applied doesn't necessarily mean it's working let's go double check and make sure our policy is there I'm giving you a refresh here when we had it open earlier and we can see our bucket policy um sometimes it's useful to utilize this in here because it can uh do error checking stuff like that I don't really care about that I know what I'm doing so I'm thinking that this is going to give us access to this bucket so let's see if that is true or not I'm going to open up um the console here and I'm not sure if it matters if we're in the same region as the bucket but I'm going to just type in here clear because I had some stuff there from before and what I want to do is I want to grab that bucket name whatever it's called here and let's see if we can um access it so I have my other window here so I say itbs S3 LS S3 col4 SL for slash I just remember that we don't have anything in there I didn't mean to paste the whole thing it's just my uh paste is all messed up here um but we should have put something in there but we didn't now there's no error so that's a good indicator that we can put something that bucket and we say touch um Boot camp. txt and we'll move that file into there a S3 copy boot camp. txt there and I'm just going to grab this here and hit enter so notice I was able to place that into the bucket I list it out great so I've just done cross account access and I again I thought it was going to be more complicated with the r but uh I guess in this case it's a lot easier so that's really nice um if we were to do this through I permissions let's say we're to create an a a uh a role and then attach it we would absolutely have to create a cross count role so this is just kind of one of the great reasons for bucket policies it does simplify some of those actions there um so just say create bucket policy in the other account uh access the bucket and so I'm just going to copy those commands here we had that one we had this one we have this one here uh touch boot camp okay so that's good but there's lots that we can do with these policies tons and tons and tons so if we go back here to the docs it should tell us like you can do all sorts of different kinds of access you can limit on uh prefixes if you want so maybe here's an example so here we could say the source count has to equal this not really getting good examples here but I guess the prefix we could just put it in here if we want to limit it so yeah maybe I'm not showing you the best examples here but I'm just telling you they're way more flexible than ACLS as you saw earlier I kind of feel like we achieved what we wanted to achieve here so what I'm going to do is go ahead and just do some cleanup so we want to get a rid of um the file that was created so going to go down here and this will just be remove the bucket and then we want to remove that file there we go and I'll save this for future folks and I will see you in the next one okay okay ciao all right let's compare the difference between a bucket policy and IM am policy because they seem really similar and it can be a bit confusing as to why would we have uh two ways of doing things but let's talk about it so S3 bucket policies have overlapping functionality as an I am policy uh to Grant access to S3 S3 bucket policies provide convenience over ion policies when granting S3 access so the idea here is convenience so uh let's take a look so with an S3 bucket policy it provides access to a specific bucket and its objects whereas an I am policy provides access to many adus services and can provide permissions for multiple buckets in one policy so you know if you're working with lots of buckets then I am policy makes more sense if you're working with a single bucket bucket policy makes more sense you can specify multiple principles uh to Grant access so think users think Aus accounts things like that whereas an i policy you always attach it to something so the idea is that it's going to default to one specific um identity so you're not going to use that identity uh key value in Json so if you want to specify multiple principles for a specific bucket then a bucket policy is going to be better a bucket policy can have up to 20 kilobytes in size whereas an i policy uh policy is based on its principle so if it's a user it's 2 kilobytes if it's groups it's 5 kilobytes if it's a roll it has 10 kilobytes so um you know sometimes bucket policies are better because they have that larger file size when it's one particular bucket one other thing to note is that uh block Public Access is turned on by default and will deny all anomis access even if a policy a bucket policy Grant uh is there unless the feature is turned off so just understand that that is a little bit different for uh than I am policies um so you know just make sure you're aware of that okay let us quickly cover access grants so S3 access grants lets you map identities in a directory service so think I am identity Center active directory OA uh to access data in S3 this is really useful if you know people uh just want to give them easy access and they're already using some kind of directory service so there are some components that we should know about the first thing is the access Grant instance this is The Logical container for individual grants then you have the location so you're saying hey I'm going to let all S3 buckets or this subsection S3 buckets to have access then you have your granular permissions which are your grants you're going to have multiple gr uh grants this is going to determine whether it's read right or both um whether you're giving access to uh an IM roll or an IM user and specific objects in a bucket you'll get issued out temporary credentials um to keep things really secure there's more that we can talk about access grants the reason I'm not getting really deep into this is because uh this stuff gets really complicated it's probably not going to show up the exam and directory services are pain to explain so just understand that Amazon S3 access grants let you map identities in a directory service and remember the terms active directory OA identity service and uh or Microsoft Entre ID which is azure active directories new name and then you'll understand what this service is for okay remember earlier when I was talking about block Public Access and I was saying that adabs provides more than one way uh to make sure that you are not having open buckets to the internet uh well I am access analyzer for S3 is that other service so it will alert you when your S3 buckets are exposed to the internet or other itaba services so the idea is that when this service is running it's going to tell you so there's an example of a bucket that has a bucket po in it's telling me um you know if it's active and and what can be done with it in order to use the access analyzer for S3 you need to First create the an analyzer in IM access analyzer at the count level um and just notice here it says 10 buckets are configured to allow access to anyone on the internet so that's kind of the warning that you would get saying hey you have 10 buckets you might want to go and investigate and you can view the buckets uh per region in here and then download the reports to get more information very straightforward and simple service but super good to quickly find out where you might have vulnerabilities hey everyone this is Andrew Brown and we're going to take a look here at IM access analyzer for S3 so the question will be um what can we determine from this service so I don't think I've ever turned it on at least in this account uh so what I'll do here is go to S3 and somewhere here yeah on the left hand side we have I am access analyzer for S3 it says the IM access analyzer is not enabled for this region um here it's specifying CA Central one but let's just go take a look and see if there's anything interesting in our buckets that it could actually even tell us so right now um we do have some accounts some are public some are not so I guess I'll go ahead and turn it on and see what happens I have this turned on in my production account I just do not want to show that here today um or I mean maybe I'll have to because if uh we don't have data I'll flip over to it but here it says to enable I am access access analyzer for this region visit I am access analyzer so we'll go over here and I guess we'll create the analyzer it says this analyzer scans for resources within your account this analyzer scans I am users en roles within your account so we have two options here um I'm going to go with external because that's going to be more useful for S3 because then we're going to know what is going on there it looks like we can create both analyzers as well so go ahead and just create that there and I'm just going to refresh this here and right away we're already getting results so that's kind of interesting I'm only interested in S3 so I'm going to go back over here and go this a refresh so maybe when it's turned on it works for multiple services and there we go so so we are getting some information region has no public buckets so that's nice to know um let's go find out and see what happens if we set one here so we'll go ahead and create a new bucket public bucket AB etc etc I'm going to go all the way down here I'm going to take this off and I acknowledge this here we'll go ahead and hit create bucket the question is how fast is this analyzer that's what we don't know here here so we'll go back over to uh here to S3 and then on the left hand side we're going to expand that column I guess it already expanded for us and we're going to go into IM access analyzer S3 and I want to say that there is a bucket available so I mean we have it turned off but we're not necessarily um have a bucket policy that allows access unless you require a public configuration for a specific and verified use case adus recommends that you block off public access to your buckets okay so I'm not sure if that one's going to show up but let's go make a second bucket so this is bucket number one and I'm going to say super bad I don't want to keep these around for too long because I'm going to do something uh very dangerous which is to open my bucket into the entire world so just understand that um I mean it's not that dangerous cuz I'm going to put it on only on get requests but just understand that uh you really don't want to do that as much as you can if we're doing static website hosting I guess it doesn't really matter so that's fine but what I'll do here is go to bucket policy and we should be able to get one for oh there's a generator if we click on that that'd be nice whoa we're not using that that's too old uh maybe if it was like 2018 I'd use that but that is one old policy so I'm just looking for any kind of code here that I can work with I'll grab this one I don't need all of it and what we're going to do is just change this a bit so uh principal is for everywhere I'm going to say allow I'm going to allow for get object I'm going to allow for this bucket here so I'm going to go ahead and grab this one and paste that on in apologies that it's so small I'll just increase that there for you and uh we want to save from anywhere so I'm going to put forward slash Aster uh and close this here uh do not care about all this stuff down here so I'm going to take that out I'm going to make a curly and a square and then another curly this should have been a square and so that looks like to uh to me a valid statement we could have built the statement over here on the side like that so uh allow all public I'm going to go down below here and save the changes and so now everything should be able to access anything like in this bucket okay not that dangerous cuz again it's only get but if it was put that'd be really bad if there was something sensitive in here that could be really bad and so the IM axis analyzer is supposed to help us out here so I'm going to go back here and give this a refresh the question is are these buckets that I created actually in the region that I think that they are in let me just double check here because we are kind of whipping through this so I'm just going to zoom out and this one was uh what do we call these oh public bucket so this one's in CA Central this one's in CA Central good I'll go back over to here here and I'm going to just click off and click back on so I guess the question is is how often does this thing run so I'm going to click into here run frequency all right how often does S3 access anal izer run it says it will find changes within 30 minutes when a bucket policy or bucket ACL is added U the update finds based on the change within 30 minutes okay um I don't want to wait 30 minutes so I'm going to try to force it to be sooner so I'm going to go over to IM access and we have our external one here I'm going to manage analyzer I'm going to delete this analyzer because notice it said last scanned one minute ago right so we go ahead and delete this and we're going to go ahead and create a new access analyzer that is fine uh when you enable IM access analizer service link is created for the rule etc etc okay that's fine we'll go ahead and click that again and so now I'm hoping that it's going to have some findings here we'll give it a refresh we'll go back over to here we'll give this a refresh as well we'll hit enter and let's see if it detected it because we have one bucket that is um that has block Public Access disabled and then we have one that actually has an open bucket policy so notice that it's only detecting one that actually has access and it's telling you that it's read AIS here right so you know this would be a really great way to um uh find open stuff but let's go ahead and take a look here here so we have some options here uh view findings so click here to view findings I'm going assume it takes us back to access analyzer so showing us even more information here um not super useful for for narrowing this one down but just showing the full list and there's archive so I guess if we don't want to deal with it we can just archive it maybe Markus active puts it back into uh the queue here but I'm going to go ahead and say block all public access and and basically that's just the same as turning that back on the Block Public Access there so now it's no longer an issue okay so pretty simple service um I'm going to go back over to IM and delete out the access analyzer but first I'm just going to go ahead and delete these buckets so we'll delete this one okay and then we will go ahead and delete the other one which was here there we go is there any other buckets I need to delete from prior projects Yep this one here I'm going to go ahead and delete that again you don't have to delete these buckets I'm just cleaning up whatever buckets I have from uh prior projects as we've been working through S3 um this is a one that I think I didn't even use I ended up failing on this one here okay oh come on let me delete this bucket's not empty okay that's fine perum delete always keep on top of your buckets and keep them uh keep them uh deleted here if you don't need them anymore so uh we'll go back over to IM IM because I want to get rid of that access analyzer I don't think it really costs much but uh or if anything at all but just in case it does I'm going to go ahead and delete it so I don't have to worry about it there we go we'll say delete and we are done there I will see you in the next one ciao let us talk about inter Network traffic privacy this is not a specific feature to ads but it's just a concept in security so what is inter Network traffic privacy this is about keeping data private as it travels across different networks and uh usually the idea here is that when we're talking about Cloud it's about keeping the traffic within the uh like aabus Network you know the Azure Network the gcp network internally so never goes out to the internet and it's going to benefit from uh the cloud service providers uh security measures um and things like that so there are two ways uh that we can have inter Network traffic privacy when we're talking about S3 the first is adab US private link so this is also known as VPC interface endpoints I'm not sure if it was called VPC interface endpoints and then renamed to it private link um but we do talk about these in our networking section in more detail but I'm just trying to remind you in this part what these things do so for VPC interface endpoints this allows you to connect an eni elastic network interface directly to other inabus services like S3 ec2 and Lambda it can uh connect to to select thirdparty Services via the itus marketplace and itus private link can go cross account it has fine grain permissions uh via VPC Point policies uh there is a Char for using Abus private link so it's not free but it's extremely robust um utility or networking networking uh feature to keep your traffic private and internal within the within the adus network so next we have is VPC Gateway endpoints so this allows you to connect a VPC directly to S3 or Dynamo DB uh staying private with the internal it Network vbc Gateway endpoints cannot go cross account uh it does not have fine grain perit missions and there is no charge to use VPC uh Gateway endpoints so when you can use VPC Gateway endpoints when it makes sense for your use case because it is ex uh free whereas VP adus private link or VPC interface endpoints uh they can add up pretty quickly and yeah I know I missed the S on there but uh what what can you do about it okay we'll see you in the next one ciao let us take a look at Cross origin resources sharing also known as cores and I have to tell you I hate cores everyone hates cores because no matter how good you get at cloud or uh working with websites cores is so finicky and you're always going to be second guessing yourself with these settings so don't get frustrated if you run into issues with cores everybody does what is cores it is an HTTP header based mechanism that allows a server to indicate any other Origins such as domain scheme Port than its uh own from which a browser should permit loading of Reon ources so the idea is you have a website and that website has a domain we'll say it's domain one and so when your website which is on the client side goes out to the server side it's going to say hey do are we operating on the same domain and if it is it's same origin if you have uh a website that that is domain1.com in your browser and it goes out to a server that's a domain 2 then it's considered cross origin and in that case you have to have um a header being passed along called Access Control allow origin or other headers there uh to ensure that you can access that data if you don't Coors is going to restrict which websites May access the data based on these headers so to access uh controlled via HB headers we have the request headers of origin Access Control request method Access Control request headers for response headers we're going to get back things like Access Control allow origin Access Control allow credentials Access Control expose headers Access Control max age access controlled allow methods access controlled allow headers so yeah I hate headers but you run into them uh probably like with cloudfront S3 uh API Gateway those are the itaba services where we're going to have to fiddle with them but you know once they're working it's all good but it can be very frustrating but we'll we'll we'll talk about that when that happens okay So within Amazon S3 you can set a course configuration uh for your bucket and this is specifically for static website hosting so then when you're are trying to access resources from another domain like let's say accessing JavaScript assets or something they you're going to be able to actually download them execute them on the client side of your browser so the idea is that this Coors configuration file can be in either Json or XML um in databus console it only shows Json and this XML used to be in the console they got rid of it um it suggested that you can still set it but when you look at a CLI you're only still showing Json uh options I know it used to be XML but I can't find a way to upload XML anymore um this stuff is pretty standardized it's not specific to AWS when you're configuring uh cores the XML will always pretty much look the same but let's take a look on the right hand side and see what we are doing so we're saying allow all headers for put post and delete methods for this specific origin and then the next one we're saying for allow all headers for put post delete for this other uh domain so that we could be getting resources from these two different locations and then down below we're saying uh we're not specifying any headers to be allowed but we're saying let's get uh let let's let uh be able to get anything from anywhere and generally you don't want to use this Aster here for the wild card as it would negate the protection of cores because you'd be saying hey uh I don't want to have any cores protection but a lot times in development you will set this and then once you uh have ches working you'll narrow it down and make sure it works so I I just would say in production you would not necessarily want to have a wild card there but yeah there you go hey this is angre brown and in this fall along I want to explore cores setting for um S3 so in order to do this we're going to have to set up a static website hosting we're also going to have to open open up block Public Access we're going to also have to um uh set our uh we're going to create a bucket policy so we've done all this before but we'll go ahead and as per usual I have the examples uh repo here I'm hopefully people are familiar with this by now and in here we're going to create a new folder and this is going to be for Cores and I hate cores cores always gives me so many problems and so I'm hoping that uh we don't have too many too much trouble here but we're pretty much making like a mini project so hope hopefully you find this part fun so I'm going to go ahead and say uh create ourselves a read me I'm going to want to do a couple things create a bucket um create a bucket policy so we're going to say uh open uh change block Public Access I'm waiting for my uh Vim hot key to kick in cuz I'm I can't navigate normally there we go okay um then we also need to uh turn on static website hosting upload our index HTML file and include a resource that would be cross origin and then we need to apply a cores policy we're going to do this all via the a CLI as per usual so I'm going to go ahead and type in a CLI S3 API because it's basically mostly going to be this way I'm going to go here and say ads S3 uh crate create bucket make bucket make bucket um and this will be S3 slash cores fun uh AB for my initials you put whatever you want and we'll go ahead and create this bucket I'm going to say allow paste this down into the CLI down below which is already preinstalled and so now we have our uh ours there the next thing we want to do is open up our access controls so we'll say block I say access controls block Public Access is what I meant to say and then we'll go down to the bottom here and grab this command here and go back paste it in here and I only need to enable a few things here we just want to have the policies we don't want these oh sorry we want those to be left alone we going to say false for this one and false for this one okay so now we should be able to access this um we actually have to put the bucket policy name or the bucket or blah the bucket name in here for this to work also just like to hug the wall here so that it's a little bit less of a mess and I'm copying these commands go ahead and copy this we'll type in clear we'll go ahead and paste we'll hit enter and so now we'll go back over to our uh S3 we'll look at that bucket and see if that bu bucket now has Public Access uh um course fun is down here we'll go to properties and I guess it'd be permissions really and it's off for those two there it's not very clear but if we click into it we can see that those are checked off so that is good the next thing we need to do is create ourselves a bucket policy uh we'll go back over to here S3 API we also have to be careful that we're using version two though I know never seem to really notice uh any particular difference between the two we're going to look up bucket policy and we're going to find put bucket policy from here we'll go down below to get an example I'm going to grab this one this looks pretty good to me okay in here I'm going to just make uh bucket I'll just say policy we say bucket policy uh Json and over here this will be bucket policy Json we'll grab grab the bucket name here paste it in and so now we need to get one for static website hosting so I'm just going to type in static website hosting bucket policy AWS I would think at some point I would know what it is but I don't because I always look it up there it is that's pretty straightforward we'll go back over to here and um we'll go here and grab the bucket name go over to the policy paste it into its appropriate location it's just going to get objects uh for everywhere so that's pretty straightforward very simple bucket policy surprised I don't remember that um and we'll go ahead and copy this and paste it in and hit enter and we will try this again hit enter and it cannot find the policy because I'm not in the right directory so I'll go ahead and switch directories sometimes that's why I like using absolute URL so I don't have have to remember we'll go ahead and hit enter and says the specifi bucket doesn't exist um it doesn't did we make a mistake we'll try this again we'll paste it in oh I think it was missing a two on the end there okay and we'll go ahead and hit enter the policy is invalid um do we have the same issue there where we copied the name wrong yes we do it's missing the two we'll go ahead and hit up try this again there we go we'll go over here and take a look we'll go over and check the policy it is there in place now whether that policy is correct or not is a different story I think it is we can turn on um sack website hosting here probably under properties if we go to the bottom if they could stop changing this on us and so when we enable it we're going to want to set it to static website hosting index HTML I think this is all One ads command um so what we'll do do is go over to here and go back up to S3 API and we'll look for static website hosting so I'll just type in static or maybe website and it'll be on the put we'll go down below and look at an example uh here we have some kind of example so we grab that we'll go back to read me we will paste this in here if I'm going too fast you'll just have to slow down the video or stop I apologize but we need to keep paste here cuz there's a lot going on and um we'll need a configuration so I'll go ahead and type that in here website Json I'm not sure what it wants for this very straightforward um I think it would defaulted that anyway so this is going to be the default settings but we'll provide it just because it probably wants it we'll go ahead and copy the bucket name here and we'll paste it in that looks good to me we'll go ahead and copy this put put enter and it looks like it is now there we'll go back and take a look we'll give this page a refresh and see if it's actually set it is now explicitly set we have seen those are set there so that is good our next step is to upload an index HTML file so that's what we're going to need um I'm going to go ask chat GPT to produce something or maybe just like um boiler plate or template for index HTML file the reason I'm asking that is because I can never remember uh the beginner text up here so I went to free C Camp which is a great place to grab this I'm go ahead and grab this code here we'll paste it in and so what I want to do here is I want to include something that is um cross account or sorry cross uh from loader from another domain name right so um I'm just thinking about this for a second and give me a second to think about this okay okay so I'm thinking what I want to do is I'm I'm going to serve up a uh a font from another another uh website so that's what I'm thinking of doing so I'm going to just take these out of here for now and I'm going to take this out here and I'm just going to uh put some text here so what do you think of my amazing website okay and and I don't care about this that's fine so this looks like a decent website we'll upload this file so I'm going to say adabs S3 uh copy index HTML and then we're going to upload it to this here oops oops oops oops all right we'll go ahead here and paste that on in and so that's going to upload that file now I want to get the website URL so say uh get the website endpoint for S3 and I wonder if there is a um command for that it's probably like get bucket website I'm just going to try that and see what we get so we'll go back up to um this command here I'm just going to change is to get I'm going to take off the website configuration off the end and I'm hoping from here we can get that uh endpoint URL so go ahead and paste that in hit enter um no that's not what I want uh endpoint endpoint endpoint how do we get that URL is there a way how to get the website endpoint URL for I mean it's probably based on some convention now that I'm thinking about it so we can probably just Assemble it uh we probably have that URL listed somewhere in our slides so I'm going to go ahead and grab that really quickly website just looking for it here yeah so in the slides it's suggesting either this or that format so bucket name. S3 website region. Amazon ads.com so I'm going to try those two variants and figure out which one actually works um so we'll go here view the website and see if the index HTML is there so this is going to be HTTP colon then our bucket name so this will be that oh that copy and paste it just does not like me right click copy right click paste and from here we want to have S3 hyphen website now I need to point out this could be a period or a hyphen it really depends on your region so I'm going to try with a hyphen and say ca Central 1 I think that's where my bucket is I'm going to go double check and make sure that's the case because I don't know where we created this it's very important you know where your bucket is this one here is here in CA Central one good so I'm going to go back over to here and I'm going to try Amazon aws.com so this could be wrong because of this one value here so I'm going to go up and just try nope that's not what I wanted we'll say copy and we'll try this again enter can't be reached so I'm going to just change this to a period instead I think it was right here is it working is it working it's not not not working okay great so this one needs to have um it's either this or it is this it will either be this it's this for CA Central other regions might use a instead so you're going to just have to uh fiddle with that and see which one works for you but it is as far as I understand this one here with the period so now what we need to do is load in a custom font so I'm going to go over here and we'll get a custom front from Google fonts and I'm hoping that this is just not going to work it's going to say hey you're not allowed to load this because we don't trust Google fonts that's what I'm hoping that it will do I'm not saying that it actually will do that we're going to grab this funky one and we're going to download the font Family actually I didn't want to download the font family I wanted to um get the URL where is it usually there's like an ed that you can get it uh test typer they keep changing this on me so I'm not really sure uh how to grab this okay okay so if I click that now it gives me a link so I'm going to go ahead and copy this link and I'm hoping that it just does not work if it works I'm going to have a problem here and I'm going to have to try to make a better example noce here it says cross origin so it's doing a preconnect so this might be too smart and just work for some darn reason because the link might allow us to serve something that is cross origin right but we'll find out here in a moment um so let's see how we use this font this is we'll just dismiss that this is with this specified CSS rules well where are they well provide them to me please um okay how how do I use them read our FAQ they used to not do that it used to be right in line you could just copy and paste it CSS um this is Fring can I use CSS feature like Shadow give me a moment to find it okay all right I'm not getting the information here so I'm just going to try and manually include it so I'm going to go here and just say um style and I'm just going to try and place it on all objects let's just say font family and I'm just going to write it in here I'm just going to hope that it works so it's going to be rubric bubble all right so I'm hoping that that just works we'll need to upload this file again so I'm just going back here and reuploading same command that should replace it we'll type in clear and just to clear that up I'm going to go back over to my website here I'm going to refresh and I'm hoping that it doesn't take effect and we get a chors issue now what we can do is we can right click here and go inspect right and um from here we go to network Tab and the idea is that we can go to all and we can see what it's fetching so this is not showing anything I'm going to give this a hard refresh I'm not exactly sure why this is hanging it should just instantly work usually doesn't hang did I lose internet for some weird reason nope so this is going really really slow um this normally does not happen usually it's instantaneous so we're going to go over to cores I'm going to go over to um the management all the way down to the bottom nope properties I guess I'm just looking for that uh that link here just making sure that I have the correct one uh I mean it looks the same I'm going to hit enter again there we go and it's loading it in so the thing is is that we did use this correctly but it has this preconnect cross origin thing okay so this isn't going to work as a very good example for loading in an external file and so what I'm going to do is I'm going to create another website and I'm hoping that um from that we can from another website we can then granted access so I'm just trying to think about this for a second because cross origin is dependent on whether it comes from a different domain and I think that if it comes from a the same sub domain it will still work so that's what I need to be sure of because if we use this they're going to be on the same domain um does cores block subdomains by default yes you have to enable it you can send to cores allow headers from server side your browser this is because the subdomain is a different origin okay so if that's the case we can just create another one another uh buet here with website access and hopefully that will uh allow us to simulate that but clearly this example did not work but it's okay that we went through that um so we didn't have to apply CH policy here so we'll go here and say this will be create website one create website 2 we'll go up here we're going to copy this we're go down below and we'll say cores fun uh two and we'll go ahead and do this and what I want to do here is I want to um also create another file but we'll we'll first change some of these options here so we definitely need these so we'll copy these two commands actually these three commands actually these four commands and we'll bring this on down hello and paste it in and I also noticed that uh we lost our bucket name for one of our commands here as we probably cut it out by accident so I'm just going to put this back in here like that there we go and we're just going to need to update this to be the new bucket name which is two right so we'll go here and make this two we'll go here and make this two we'll go here and make this two we'll go here and make this two and then we'll scroll on up and we've created our bucket the next thing we want to do is we want to go ahead and open those public uh public access block the bucket does not exist well I remember when I pasted this it looked like a comma got in there see like there and I'm wondering if it messed up the name at all it doesn't look like it did or we put the two in the wrong spot that's the problem okay so the two is supposed to be over here sorry two two and then we have two okay great so we'll try this again and we will open up the access here great and then we will apply we need another bucket policy because this is for this one in particular so we'll say bucket policy 2 Json okay I'm going to paste that in here and so we'll put this here like that two and so we'll have to go back over to uh this one and so we say two great I'm going to save that I'm going to copy this I'm going to paste this here hit enter and then the next thing is turning on stack website hosting I don't know if we actually have to supply website configuration because it will default some of those values so I'm just going to see if I can turn it on cuz that's all we really want to do here we'll hit enter and it is required okay so um I'm going to try this I'm just going to undo this for a second oops I'm going to just try to pass it an empty uh Json object because again we're we just want whatever the defaults are and if that's fine it takes that that's great okay so we do have to specify that's not a big deal we can use the same policy uh for this because they're identical but it's just that I'm not going to actually serve an index HTML file I don't feel like uploading one um what I do want to do is create a Json file or a Javascript file index.js or sorry we'll call this um hello JS we'll say console log hello world and so all that's going to do is print hello world to the console uh in JavaScript and then we want to reference that file across of it so we need to upload our Java we'll upload our Javascript file and so this will be hello JS and this will be to fun too so go ahead and upload that and so now the next question is can we look at that file in isolate so so I'm going to go here and we're just going to make our lives a little bit easier I'm just going to get the URL from here instead of having to try to write it out and assemble it and we'll go and look for that uh Second bucket and we'll go into properties and we'll scroll all the way down to the ground I'm going to open this up in a new tab um it's not going to complete because there is no um there is no uh index page but there is a hello GS page okay so this should serve up all right and so there it is the raw stuff for it and so that is what we want we want this whole URL and we're going to go back to our website and we're going to go into index HTML and we're going to load that in here so I'm just going to um place it at the bottom of the body and we're going to say script type equals this is I believe is text JavaScript and then we'll say source and then it should be this and so the idea is that it should load this script here and say hello world but the thing is it should complain and be like hey you can't load this as comes from a different origin and that's going to hopefully facilitate us uh being able to um check that out so we'll go back over here and we need to reupload that file so there's that index HTML file somewhere here here it is and we will upload it so now that it includes that we're going to go back here to our website I'm going to close this so you can see what we're doing I'm in Chrome by the way you should really use Chrome and we'll go right click inspect Firefox will have this it's way easier in Chrome forget Safari I'm not helping anybody with Safari uh Edge Edge uses the same thing as the chroman engine so you should have this functionality as well but what I'm looking for here is is it going to try to load that file so we have hello JS console log and we're going to go here and it's printing it out so that's working and I I keep expecting to have some kind of chors issue so let me think about this for a second okay all right so my next thought is that maybe if we really want to trigger cores we should um create an API endpoint using API Gateway that's I can think of in order to do this I'm very adamant that we figure this out because usually we run into chors issues and we're not here uh because I'm using two simple of examples so let's make our way over to API Gateway and what we'll do is we need to create ourselves an endpoint an API Gateway that's going to return to us um some data and that's all we want we just want to have a simple HTTP API server that's going to return something um and I want to do that on a post or put request so we can use HTTP API we can use rest API and so what I'm thinking here is we can try to build this out here um I'm just trying to decide here if I go build I'm just looking at it very closely because all I want to do is I don't really want to integrate um Lambda I just want to return a mocked value so can we can we return a mock value from API Gateway HTTP uh HTTP and I'm not sure if that can do it I know rest can do it rest API API Gateway supports but this is under rest right so notice here that Gateway has two different modes rest and HTTP so I think that's the way we're going to have to do it so yeah let's just use this through the console and and uh do it that way so we're going to make a new rest API don't worry this is all free tier and what do we have as an example this example has a get path I really want something that is like has a put and not a get so I'm going to go here and create a new API I'm going just call it uh cores fun API and this will be a regional endpoint are deployed in the current region so we're in uh CA Central 1 make sure this is the same region as your um bucket and we'll go ahead and we'll just go and create ourselves a new resource I'm going to call this um example or maybe hello and I'm going to go ahead and create that resource they've changed the uh UI since I've been last year and I haven't done the API Gateway refresh so I am a little bit unfamiliar with this new UI but um we have the hello path and what I want to do is I want to create a method here we go and I want to mock it I'm going to go ahead and do this on the post yep and we'll go ahead and say create this method and what I want to do is I want to return mock data so somewhere in here we should be able to specify the mock data so here we can manipulate the request and here we can manipulate the response I would think that we could change the response here like this isn't an option we can't change this so it must be here method response that's what I'm thinking we can look at integration response um this is for mapping different values as it passes through so I think that it probably is here this has a 200 response which is what we want and this might say hey we want application Json back which makes sense but where's the body that's what I want to see add model no well this is the response body empty error it seems like we have to choose a model let's go back here it's like model yeah okay so this is the default empty schema if we create this model here what what do we get here what does this one give let's take a look here properties type string not exactly what I wanted let's go back over to here and we'll go back to Method response I think this would just create an additional one no we don't want that we want to again manipulate this um this response here because I just want to return back some data right and so in here again for method response response body so it must be something to do with this model let's go back over to models and we'll create a new one and we'll just call it example application Json we'll look up rest API models API Gateway and um understanding the mapping templates that's not what I want um responses give me a second to find this okay all right so in the documentation under mock integration it's suggesting that we're actually looking under the integration response Tab and this is where we can supply additional information under template body so let's see if that's the case um I'm going to go back over to resources here we're going to go back into post we're going to go to integration response and apparently in here we can change some stuff so we'll go ahead and hit edit um obviously it's password that's totally fine mapping templates so I'm imagining this is where we can uh change our template so this is basically any kind of Json we want I'm going to go here and just say message hello world and so this is what I want to try to get is this response back here I'll go ahead and save this now we Chang that we got to go ahead and deploy our API for this to take effects we'll create a new stage here we'll call this prod and we'll go ahead and deploy that that's going to give us a new URL which is right here this is how we're going to invoke this endpoint um this should be open up to everywhere I think so I don't think we need to do anything special to access this but we are going to need a way of um querying this so I'll go down here and say um create API Gateway with mock response and then test the endpoint and so I want to test this endpoint and we can use that using curl or W get um so I'm going to get some help from chat GPT because I don't want to write that from hand we'll say uh provide or do a curl against this URL for post and pass along application Json for headers okay so it should understand what I'm saying there okay I'm not asking you to actually do it I'm just asking there you go okay and so the content type is specified there sometimes you have to also specify another value but that should be enough I'm going to go ahead and grab this this is pretty standard so um it really depends if our uh what is here so if we have curl we can do it so it does sometimes you have W get sometimes you have to install curl again use these Cloud developer environments like git pod or code spaces or whatever because they come preloaded with all the stuff here and so this is the uh command we want to run I can keep it one line I guess and um this actually has a point here saying for SL hello so we're going to need to put hello on the end here and let's go ahead and curl that and we should get back that Jason response so we do so that works that's really good now the question is can we grab this and apply it into our um index HTML so what we'll do here this index JJs is completely useless so this is no longer of any use to us so I'm going to go ahead and just um take this out of here well not exactly I'll take out the source but what I want to do is I want to write an HTTP request that's going to grab um uh to to to query for this okay so I'm going to look up and say um xhr request post example JS and I'm looking for a very plain one we'll use micros or um Mozilla because they'll give us a a very plain example and so here's a get request here's a post request we'll go ahead and copy that we could have asked chat gbt but I really don't want to because it might give us some um varying results and I want something that's very consistent so I'm going to go grab this here and paste it in and so we have an xhr request this is of course JavaScript and so now we need to place our URL in here so we'll go back to our documentation we'll grab this full link we'll copy it and I'm going to go ahead and paste this in okay and I'm going to go and indent here and I'm going to go bring this in here now okay and we going to carefully look at this so we have the the post to here it's going to set a content type um this is supposed to be application Json I believe yep Json and then on ready State that's fine it's going to just say that it's done when it's done there and um I mean we want to send I don't want to send anything to it so I'm just going to do this but I want to um parse the Json so the Json will get return back somewhere in here I'm not sure if they actually provided that there they didn't so we'll say xhr request application Json post example because that wasn't very useful for us let's try W3 schools this looks like what I want so I'm going to grab just this part here and we're going to go back over to here and I'm going to go down a line I'm going to paste this in here and so this should be more or less the same I just not sure if this is this is four and this is 200 so the thing is four and done are the same thing so this is identical um this one's actually better because it's using three equals and it's using a modern function all I want here is this this this one line here and I'll get rid of this um other function here since we don't need it and we don't need that there we go and we don't need to pass anything in here so what I want is I want to get the response text and parse it is Json so my response just say um results and I'm just going to console log those results I'll change this to const results results again you know if you don't know J uh JavaScript or coding uh very well that's okay just continue on and uh do the best you can to just get this code right or copy and paste it to get it to work so this should send a request we're going to have to update this and send this to our page um so we'll go up to our stack website hosting which is somewhere here and we are going to upload that file make sure that it's not the two because we clearly have more than one we'll hit enter great we're going to go back over to my website I want to point out that sometimes these Pages cash so make sure that you have disabled cach on here because this might not take effect if you do if if you're not sure how to get to this and right click inspect Network tab disable cach we're going to give this a hard refresh now notice we get a chors issue hooray I've never been so happy to get a chors issue um normally I'm unhappy to get them but uh they don't always happen on get requests uh generally the idea is that they happen on posts puts um and there there is this preflight check that goes out to confirm that uh you are allowed to uh do course here um so to get this to work we may we not only do we might need to set cores on the uh bucket but we might also have to set cores uh via API Gateway to return it and this is where cores gets hard because we end up fiddling with it a bunch so what I want to do here is I want to uh Now set a cores policy so we're going to go and uh find one here I'm just going to close out some of these tabs and we're going to look for the cores command and we're going to go down to examples and we have a very nice example here so we'll go ahead and grab that I'm going to go back over to um our developer environment and now I want to set cores on our bucket this is super exciting so we're going to need our bucket name this one has the two in it so I'll have to just change that out so it does not have the two and we'll have a chors file in here so I'm going to go ahead and say new file chors Json we're going to go and grab this code cut it go over to ches paste it go back delete this part we'll go back over here and so we're just going to say we want to allow from another origin um here the origin is going to be this so we're going to grab this and go back and we'll paste it in okay notice we're not putting the forward slash we just want the origin which is this right and what I'm always confused about is whether you want the HTTP or htps luckily they showed us in the example so we don't have to guess anymore um notice we're allowing for the put the post and the delete and I mean this one's showing the X server side encryption I don't really know if we need that it's not going to hurt it by having that there so I'm just going to leave it in and then this one allows for an authorization header that's if you're doing authorization that's a little bit more complex but we have this and so let's go ahead and apply this um cores policy so go ahead and copy this paste this in down below and that should be applied we'll double check make sure that is the case and we'll go over to permissions here and it looks like it is good so I'm happy with that um but the question is is that going to make cores work right away or do we have to fiddle with API Gateway so we go back over here and I'm going to close this tab out we're going to go to our website give it a hard refresh and notice that it's still having an issue okay so the thing is is that when you do cores it's going to send a preflight request and it's expecting to get back um that it has particular headers and so that's what we need to do for API Gateway um so what we're going to do is go here and I think there's an easy way to enable able cores on a method I'm just trying to find it here we're in stages right now so we got to go back to resources and we'll click here and should be able to apply cores they changed the API so I'm trying to find it here it is enable cores I really only want it on this method but it looks like we have to do it right here so I guess these are like the methods and then this is the resource so we'll enable cores and this will set some default core so notice here that it's it's passing um it's setting up the allow Access Control Origins so let's just go Google that really quickly and we'll go to the MS um the Milla docs and so the idea is that we need to send a request a preflight request and it's expecting to get these headers back and if we get these headers back then it's going to allow cores to work if that makes sense um so this looks good it says from everywhere we have some additional headers I don't care about those um we're going to allow for post Access Control allow methods and the reason we only have post here is because we don't have put or delete if we created methods on this resource then I I think it would show that to us we'll configure course for the selected Gateway responses um I don't think we're worried about 400 or 500 so we'll go ahead and hit save and so now Coors has been enabled I don't trust this API is up to date so I'm going to go ahead and hit deploy API make sure you do this because if you don't it might not work um and we'll go back over to our website fingers crossed okay fingers crossed let's hope this works and we'll go ahead and hit refresh and it works and no s below here um it Returns the data so we're getting the data back that's working our JavaScript has a little bit of an issue it says undefined is not valid Json so it's having a little bit of trouble um we don't need to really fix it we've solved ches here and we've used it but let's go ahead and see if we can just kind of make this work because I don't like doing things and not having them work now we have a this in here so probably what we want is xhr instead I'm not sure we we'll go back here and say um xhr request Json application example and I'm just looking for that example we saw web uh W3 schools because here was referencing this here and so it should automatically bind to this object but I think we can write xhr right here and it should work because that's the same thing we we assign it here we should be able to get it from there I don't think it makes a difference so what I want to do is upload this again okay again if you don't have cach disabled it might not detect those changes we'll give it a refresh and we're we got a different error so that's good uh but now it says expected property name of curly for position four in Json let's go look at our Json our Json is fine that looks totally fine we didn't wrap this with um a uh uh a string on the message I don't think that matters though and we'll go take a look here again so this is fine this is fine um I'm going go back to API Gateway we're going to go here and take a look at our um post notice now that there's an option so uh that's probably for the cores unless it was there before I don't know no it definitely wasn't there before so we'll go down here we'll edit our response and I didn't put double quotations around here so I'm just going to do this and see if this helps it another thing is that it might want all these things escaped I'm not sure but we'll go ahead and save this we'll go ahead and deploy again we'll choose our stage prod we'll deploy we'll go back here we'll give us a hard refresh and now the data returns back um does it even show here let's see here because this another hard refresh and we'll check the post so no no complaint still oh now it's printing out so I guess that actually did matter so that was causing an issue but notice that it's posting out results and not actually the results of uh the file here so I think we just have one minor mistake here we'll hit up reupload that file we'll go back to our website make sure disable cach is on hard refresh and now it prints it out so there you go we were able to work with cores um the thing to remember is you're going to run into cores mostly when accessing serers side information and you have to return those course headers um so hopefully that is very clear and we will see in the next one but before we do actually we should probably tear down all these resources um because I don't want you to uh expediate in your free tier credits so the first thing we want to do is tear down uh the stage we say prod and delete that um or confirm I guess here and once the stage is teared down we'll go over here and see if we can tear down the actual API because sometimes it won't let you tear it down unless you have the stage down first and so now that one should be gone I'm going to double check make sure it's gone it is excellent and now we just have a a bunch of buckets we need to empty and get rid of I mean a bunch I mean two and we're going to do this via the console because I don't want to fiddle with the CI and try to remember everything we uploaded so we have um two here going to give us a hard refresh so we have ab2 so we're going to empty that bucket okay I'm going to go back to our buckets I'm going to go to AB with that doesn't have anything specified one and I'm going to empty that bucket as well and we'll go back over to here buckets and we will delete this one delete and we'll go go back over to this bucket and we will delete okay and so now we're all cleaned up we deleted our API Gateway we deleted our buckets um so yeah there you go I know that one was a bit longer but um at least it taught us the limitations or where we should expect to see cores and where we're not going to expect to see cores so cores example but uh yeah you you tend to hate cores because getting the configuration can be difficult but this time it was extremely easy so we're very very lucky but we'll see you in the next one ciao let us get a quick overview of the S3 encryption uh options here and then we're going to dive into each one so the first is encryption Transit so this is when data is encrypted by the sender and then decrypted by the receiver it's to prevent people for intercepting files even if they're encrypted we don't want them to even get the encrypted files and that's what encryption trans it does we have encryption at rest there's two different options here we have client side encryption csse this is when the data is encrypted by the client and then sent to the server sounds kind of like encryption in transit eh um the client has the key the server will serve the encrypted file since it does not have the key to decrypt when the data is requested so the idea is that um the uh we're doing client side encryption but the server can never decrypt the file okay then you have server side encryptions SSE when the data is encrypted by the server so the server has the key to decrypt when the data is requested so those are those three broad options and we will now go dive into each one of these okay encryption and Transit is when data is secure when moving between locations and we'll utilize an algorithm uh such as TLS or SSL uh so the encryption ensures the data remains confidential and cannot be intercepted reviewed by unauthorized parties while in transit so imagine you have a PDF you want to upload to S3 bucket it'll get encrypted sender side with TLS and then it will get decrypted server side with TLS why would it just not keep the encryption until the person requests the file again um I don't know I mean it's just this these protocols like TLS and SSL are not designed for serers side longterm encryption all it's trying to do is to make sure that people cannot intercept the file so yes there is client site uh encryption and so you might think well we just encrypt it if somebody tries to get the file they're not going to be able to decrypt it but a better measure is them not being able to get it at all and that is the point of encryption in transit so there's two protocols that have been uh used there's TLS and so this is an encryption protocol for data Integrity between two communicating Computer Applications and so it used to be one and then 1.1 and those became uh no longer good to use and so then SSL popped up and then we had versions one two and three and then it went back to TLS um and so now it's recommended to at least use 1.2 but the current best practice is 1.3 how do you specify TLS when you're sending files I have no idea I I've never had to do it it just happens uh and so it's generally using uh TLS 1.3 but uh it us wants you to know that it to at least use 1.2 but you should use 1.3 so there you go all right let's get a quick overview of the options for serers side encryption and then we'll dive into each one so server side encription can be initialized to SSC and this feature is always turned on for new objects at one point uh this was not the case where S3 was not server side encrypted and you had to turn it on but now it's always on you don't have to worry about it it's always going to be uh encrypted with SS SS S3 by default but this is where Amazon is managing the key you and it's encrypting using AES GCM but usually you'll see it also typed in as AES 256 so understand there are some variations representing that algorithm there uh then we have SS KMS so this is going to be using Key Management Service so you get uh uh you know fips compliance and this is great if you need a certain level of Regulatory Compliance andus is managing your keys you have ssse hyphen C this is where you are providing the key you generate it out and you upload it but you now have to manage the rotation and things of it then you have DDS KMS it stands for dual layer server side encryption and the idea here is that you're encrypting the key or the data client side and then again server side so there's two levels of or maybe it's the key but anyway we'll talk about in that slide because I have all the information there but the idea is that there's two layers uh of encryption there um and then the other thing I want to point out is that service encryption only encrypts the contents of an object and not its metadata so that's very important to understand but let's go dive deeper into each of these uh encryption options okay let's take a look here at sss3 and this is where Amazon or ads is managing all of the encryption for you so S3 encrypts each object with a unique key S3 uses envelope encryption which all these methods are really using envelope encryption S3 rotates the uh the key regularly out for you automatically so you don't have to do anything by default all objects will have SS S3 applied unless you choose otherwise there's no additional charge for using SS S3 so it's a great option and it's using the 256bit advanced encryption standard so you'll see it written as AES GCM or aes256 and we'll see that exactly when we start using the ad C that when we specify the serers side encryption flag it wants AES 256 now I did say that this is the the default so you don't actually even have to specify the the the flag server side encryption if you don't provide it it's still going to be as e AES 256 so that is important to know a bucket key can also be Set uh for ss3 for improv performance and we'll talk about that functionality of a uh bucket key which sounds really confusing because it sounds like we're talking about um an object key for um an object but it's we're talking about bucket keys and this is a security feature uh we'll talk about that in a separate slide on its own so we are now talking about SS KMS and so this is when Key Management Service from ads is managing the key so very similar to ssse S3 but there is um a lot more going on with KMS to provide you a uh greater level of security to meet Regulatory Compliance so you first need to create a KMS manage key you then choose that KMS key to encrypt your object KMS is going to automatically rotate that key so you still uh have great automation there uh you have policy controls so who can decrypt using the key um KMS is really good for meeting Regulatory Compliance it's for fips uh the fips protocol for uh storing um Keys that's the the protocol that really matters here KMS Keys have their own additional cost it's usually around a dollar that's what it's always been at least in us or Canadian currencies there ad C must be in the same region as the bucket um because it doesn't go across regions for whatever reason that's just how it works to upload with KMS you need to have the KMS generate data key to download with KMS you need to have the KMS KMS decrypt IM permission so what would it look like to use it using adus CLI well here's an example you're going to choose adus uh cool and KMS the idea is you probably generated that key and that key will have an ID you'll provide it and that's how it's going to uh upload there um if you want to utilize it with uh for adus S3 which is similar to S3 API but more streamlined it's going to be very similar but the flag is a little bit different um but that's just for uploading I do want to point out that uh just as s SS S3 benefits from having a bucket key so does SSD KMS and we'll talk about bucket Keys soon enough uh so we know what that performance benefit is okay let us talk about ssse hyphen C the C stands for customer because it's a customer managed key this is when you provide your own encryption key that S3 then uses to apply AES 25 uh 56 encryption to your data so you need to provide the encryption key every time you retrieve objects uh when you do provide it S3 is not going to hold on to it they're going to get rid of it after each request so you don't have to worry about that there is no additional charg to use SS hyphen C uh s S3 will store a randomly salted hash based message authentication code on hmac of your encryption key to validate future requests so they don't hold on to the key but they do make ass salt of it so that future requests they don't have to constantly um uh have it around right so uh in a sense they do store um they don't store your key but they store a salt of the key if that is if that makes sense uh you get presign URL support with SSC hyphen C uh and the way you're going to generate out uh a key is just here's an example of generating out um a uh encryption key using op SSL and we're making sure that it's Bas 64 and the example here is US using put object and you can see that we're spec specifying the AES 256 encryption because that's what it uses and then we provide the customer key um and it's as simple as that so with bucket versioning different object versions can can be encrypted with different keys so that's kind of important when you're working with object versioning so if you're running into issues not being able to decrypt your data that's probably why you manage encryption keys on the client side you manage any additional safeguards such as key rotation on the client's side so at no cost uh you can use S hyphen C but it comes with a lot more maintenance on your end okay let's talk about about DSS KMS which is dual layer server side encryption KMS it's basically SS KMS with the inclusion of client side encryption I figured that it would make more sense to call this client side encryption KMS but they don't call it that because other cloud service providers use the term dual layer so I'm thinking Amazon is following suit here the reason why that doesn't make sense is that it's not encrypting twice server side okay because one's happening client side and one's happening server side side but whatever it's called dual layer server side so the thing is is that with dssc KMS data is encrypted twice uh and they the key used for the client encryption comes from KMS this is kind of interesting because think of s you generate it locally and then uh you send the key to ads but the way this works is that ads sends you the key that they want you to use client side and then you send the data and then encrypts it again so that's a little bit different there are no additional charges for dsse and KMS Keys um and so here is an example on the right hand side where we would uh specify our server side encryption flag with ads colon KS colon dssc and then you're providing the KMS key but again notice you aren't generating out a c a customer key and providing it because they're going to do that for you so let's talk about because it's a little bit different let's talk about how encrypt and decrypt works so the idea is that clients I request on a KMS to generate a data encryption key B uh using Deek a using the customer manage key KMS sends two versions of the deck to you a plain text version and an encrypted version you use the plain text deck to encrypt your data locally and then discard it from memory the encrypted version of the deck is stored along alongside the encrypted data in Amazon S3 so hopefully that makes sense if it doesn't give it another read uh for decryption you retrieve the encrypted data and the associated encrypted deck you send the encrypted deck to ads KMS which decrypts it using the corresponding uh customer managed key cmk and Returns the plain text deck use the plain text deck to decrypt the data locally and then discard the deck from memory um the thing I just want you to remember is that with dsse KS you are not generating the key ads is doing all the work for you okay so if you can't remember all the steps doesn't matter just remember that it's encrypted twice client side server side and the the customer key uh or the client side key is going to come from adabs okay hey this is Angie Brown and in this follow along we're going to go and look at um the encryption methods that we can use with S3 so we have ssse S3 ssse KMS SS C and D ssse hyphen KMS so um there's not really any reason to show SSC hyphen S3 because that is the default but we will create a um a single file and then we'll just see that it's already applied then we don't have do anything special but we'll go here and just type in encryption and I'm going to make a read me file so we can work with something here and while no yeah we just go ahead and do this we'll say create a bucket and this will be encryption um fun ab and then a few numbers on the end here and then we'll say itus S3 make bucket make bucket yeah S3 colon SL slash and we'll have a bucket that we can work with here so now we have a bucket and let's go ahead and create a file so create a file sorry it will say ads S3 copy hello.txt and we will copy this to our bucket that we just created we'll have to create that file so just say Echo hello world and we'll say hello.txt and we'll copy that out paste it in and get going there okay so now we have a file uploaded we'll make our way over to S3 and take a look at our new bucket and in this bucket we should see that file and it should have an encrypted um file in here so if we click into it and we just take a look here it should tell us somewhere about the encryption here it is so it's using ses3 so if we want we can change the encryption on here but it looks like it's going to create a copy of the object if we are to update this okay so use bucket settings for the default encryption override the bucket settings and then here we can then choose a different option as you know me I like to do things via the CLI so that's what we're going to do so let's see how we can uh change that so it looks like it happens on the time of upload I don't think we can retroactively change it for an object so we'll go take a look here and go to S3 API if we close uh carefully look here I'm just going to look for encryption or put and so there's no specific CI command to specifically update it so basically we are doing a put object so what we'll do is we'll go over here and we'll just say put object with encryption of KMS because that would be our next option is ssse uh KMS and we'll take a look and see how we can do that so we'll say aabus S3 API put object we know we want the bucket to be this so we'll go ahead and do that we know what we want the key to be so that'll be hello. TX T we'll go take a look here at our options we have a lot of options here um we'll probably want to specify the body so this will be hello.txt which is our local file um also notice that the txt was here we should have cded into our directory before we executed our Command I'm just going to go ahead and move that and then CD into our appropriate directory so now we need to figure out how to set our um permissions it looks like the way we do that that is with the server side encryption so we'll go here and look for server side encryption there it is if I can copy paste it here there we go and then we can choose our options so I'm going to grab this and uh seems like we're going to have quite a few here so I'm just going to bring these onto new lines okay and for this ads KMS how we will choose our option um I want to utilize yeah without it with dssc the reason I'm confused is that um I guess I'm not really confused but I'm just showing that that's how that works there the next thing will be to provide the actual key so there's another one here SS uh KMS key ID I believe let's go double check and make sure that's the case go to the top here take a look here what we say ssse yeah ssse here so I think what we'll have to do is generate out a key first because one's not going to exist um has a valid key this header etc etc is not supported by directory buckets that's totally fine so we're going to need to create ourselves an SS e kmsk key so let's go ahead and take a look at how we do that we'll make a new tab here I'm going to go over to ads we're going to make sure we're on version two we're going to make our way over to KMS and in here there should be a command to create key so we'll say create key creates a unique customer managed KMS key um is that what we want we go go back here and take a look see what other options we have then we have generate data key returns unique uh symetric key for outside KMS the reason I'm confused is that I don't want to manage the key I want to do the least amount of work here so what I'm going to do so ask chat GPT and we're going to see what it produces out um I need to run this command but I need a KMS key ID and I'm hoping that it will tell me what command I need to run maybe it is create key and I do prefer to check via the CLI so let's go ahead and take a look here and we do have some keys try this again we have a couple of keys Central one and then here it's still using the create key so you know my concern was that we' be managing all parts of it but I think all they're saying is that this would generate out a key for us so let's go take a look at KMS and just make sure we know what we're doing before we do it uh when you create KMS Keys they're like a dollar per month so just understand that there's no way to avoid those costs if you don't want to have the EXP don't do this just watch it what we're doing so I assume that there's AIS manag keys and here is one right there and then there's customer manag keys so my assumption was if we do create key it's going to create it here and then we are providing uh additional information which is not what I want to do I just want to use a adab US manage key um if we go back over to here just matching these keys I wonder if these are those two keys great um so I guess the question is how do we create an adus manage key right so if this wasn't enabled how would it get created it's created on behalf of You by Thea service you have permission to do this how do you enable the KMS manage key for S3 for the first time because normally what happens is that you'll upload something and switch it over to KMS and'll automatically do that and here they're suggesting that we still have to create a key okay so again my confusion is the fact that that key already exists let's go over to another region I know I haven't used like Ireland I just want to see if that key already exists so we go here you do not have permission to access KMS Keys okay maybe not Ireland let's try Paris instead still not allowed to access it okay that's fair enough let's go over to somewhere that's near me like let's try Oregon and I'm just trying to see maybe the issue is uh I just have to go back and entering KMS in here again let's just try this again here so I'm in Oregon I'm going to go over to KMS I just want to see that the key is not there that's what I'm looking for and so notice that I don't have that key there so what I'm trying to figure out is how do I enable a manage key as opposed to making a key and that's that's my confusion right because there's no option for like manage keys right and I don't I don't want to create a key here so no I I mean I want a managed key not a customer created key okay so let's see if it can tell us how to do that so yeah it's being suggested that way so maybe that is one way we can do it so again I just want to observe this so that you have that key my concern is that you won't have that key because you've never done that before but let's go back over to um here and I'm going to open up a new tab and I'm just going to quickly create a bucket in Oregon and I'm going to set it with KMS and I want to see if it's going to generate out that manage key so we go here and then we will go down to advanced settings oh sorry no just right here we'll say KMS okay and so we don't have anything and I need to create a key for the first time so I guess I'll click create a KMS key it looks like it's creating it so maybe we can create it here let's just go click back here for a moment so I'm in Oregon we'll go ahead and hit create so we have uh symmetric encrypt and decrypt Advance options KS okay so this is how we're going to know that it's a manage key it's if we choose KMS recommended then adabs is managing uh the key material and that's going to make the difference between a manage key and something that is like a customer key like one that you uh import or export so I guess there's nothing really interesting to put here um so I me again if you don't have it I guess we'll go ahead and just try this again so I'm going to go back over to here I just want to see if it it autofills all that information in so we go to next um managed key and I'll go hit next and then we choose our key administrator choose an IM user who's allowed to uh administer this key I'm going to set myself as that I'm also going to set um uh Abus examples because that's my uh information down there we have that information here I'm going to set the users to be the same as well we'll hit next looks good to me we'll click on finish and there it is but it still says customer manage key so how do I get that one that's what I want to know um choose from your ads Keys hold on here so this one wasn't here before let's go back over to here give this a a refresh all right so I have this key here I'm going to go ahead and disable it because I didn't really get what I wanted right like I'm again I'm trying to use the manage key and not to create a customer key right right away okay disable this disabling prevents you from using this key that's fine I'm fine with that we'll disable that key there it's disabled now that is great notice all we can do is uh disable it I'm going to delete it for um schedule for deletion I'm going to put it for seven days and confirm that I want to get rid of it perfect so again coming back over to here we don't see it here but if we go over to choose from ad best and we drop down to here okay and now we hit create it wants a bucket name so my adest key bucket test okay we'll go ahead and create that bucket so it's been turned on now if we go back here and we give this a refresh I would assume that that key would appear okay so I'm going to scroll down here and take a look uh for that bucket so that was what do we call that bucket I already forgot it is um this one here and again I don't see it in here maybe we have to upload something here first I'm going to upload anything so I'm just going to grab this here I'm just going to upload it S3 copy hello txt uh to this bucket because again I'm just trying to get it to trigger and by default it's going to use that key I don't have to specify it okay so that's now uploaded I'm going to go back here refresh and see if the key now appears here it still doesn't that's really really interesting and really confusing um so we'll go back over to here and refresh I don't see my object in here did I upload to the wrong bucket oh I did sorry that's why so we'll go ahead and grab this name instead and we'll go back and hit up I'm just going to paste this in again we're just doing this to see if it will now add that manage key here we'll give this a refresh ref fresh and now it appears so that's how that appears there um the reason why I care so much about this is I'm under the impression that this doesn't cost anything because it's managed by AWS right so things that are under here supposedly don't cost anything um but that's all I wanted to show for this part of it I'm going to go ahead and delete this bucket and I'm going to go back to my original region uh bucket must be uh be emptied okay let's empty it we go here refresh we'll delete this file we'll go back to this bucket should I have a nice big delete button here they just don't and um okay so I'll go ahead and delete this bucket there we go Okay so um you just stick in with whatever region you have where you just created this bucket I'm going to go back to ca Central 1 where um I actually have that key so I'm going to go back over to here and so now what I want to do is I want to go ahead and grab this value here okay I'm going to go ahead and paste it in and so now I have the key um this should all happen in C Central one because this one is defaulted to that region you have to change it to to whatever works for you I'm going to go ahead and copy this I'm going to go ahead and paste this and hit enter and here it says server side uh encryption command not found um we have to put on the for slashes here we'll copy this paste this in hit enter and hit enter again it uh doesn't like something here I'm going to put this in double quotations just in case that that is its tisue we'll do that here as well and we'll copy this again paste it in hit enter still doesn't like it unknown option SSC KMS key ID we'll make our way back over to here to the put object oh it was like that okay I don't know why but I thought it had a space if you look in my example up here I have ssse hyphen KMS oh no I guess it's fine over here but then down below in this example it has a hyphen so I guess there's a slight difference with the syntax whether you're in the S3 API or the S3 API and that's where my confusion is coming from but we'll go back over to here and we'll just adjust this flag to I guess be like that and we'll go ahead and copy paste it enter and so now that file has been uploaded let's go take a look at its encryption so go back over to here and this was ssse 3 before or S3 if we go go down here now it's KMS okay now it's KMS um but understand that the bucket default is not using that so if we go over to maybe permissions here I'm looking for that encryption information it's not here we can go over to properties this is still defaulted to ssse hyphen S3 so remember we can set that default another thing that we should really do is um utilize a bucket key but let's first see we can download this file without issue so uh what I'm going to do is just type in clear and I'm going to go ahead and delete this Tex text file and I'm going to go ahead and see if we can download this so we'll says S3 copy and then we will grab this address and we'll paste it in we'll put a space and then we'll say um for/ hello.txt and I'm going to say hello.txt and see if we can download that file and we were able to download the file why were we able to do that because we have permissions to decrypt the file um remember you have to have the option to have KMS decrypt and generate data key if you want to upload it and we are a user that has a lot of permissions when we go and do the KMS lab we'll look at these in more granular detail but for the purpose of S3 we just need to keep this really really simple um we have a few other options we have ssse hyphen C so I think that would be the next thing I would like to do so we'll go ahead and we'll say put object with ssse hyphen C and this will be create a file input object with encryption ssse hyphen S3 okay so that's there there and so this one is uh similar so this is going to be very very similar so we'll just copy this uh the key difference here is the algorithm so we have to uh decide what algorithm it's using so I'm going to say ssse here customer algorithm I can't spell it so I'll just go grab it and here it is right there so we'll grab that and we'll paste that on in here the next thing is we need to provide the key so there's that as well and then we want want to say it's an md5 so we'll do that as well so the algorithm here is going to be AES 256 um and then we need to provide our key so this would be like our encoded key here okay and we don't need this this is not useful anymore and then we need to um produce this as the md5 okay so uh to generate out a key we need to use some kind of program so what we could do is use op SSL so I just type in clear here and let's see if we have op SSL installed in here I believe we do we do and what we'll do is type in open SSL Rand so I'm going to go ahead here and type in open SSL Rand 32 and then I we'll see what it generates out so what's Ty been clear here here we enter And so there's our value doesn't make a whole lot of sense but it is being generated out then we'll do base 64 to convert it to base 64 so it is way more human readable so that's a lot nicer and I'm going to assign that to an a variable I'm going to call this base 64 encoded key and we want to export that as such so we'll do this around it so I'm just going to write export okay and so I want to make sure that exists so I'm just want to Echo it out now so we'll do Echo and we'll just make sure that we're able to generate that out so Echo and put a dollar sign in front of it there we go so we know we've saved that value for this command here we'll grab this base 64 encoded and place it here so that's the key and then we need to um enter in that md5 right so we go here is that the hash it is not telling us hold on here yeah it's just okay so it's just that so in order to get that it's a little bit more um this one we need to to decode it and then we need to get the sum of it and then we need to print it out and then we need to convert it back to base 64 so for this um we'll need to do Echo hyphen n and then pass in our encoded key so we're printing it out we are then going to pipe it over to base 64 so we're going to decode it basically turn it back into that thing that doesn't make any sense and then we're going to get an md5 sum we've used md5 sum before when we did um uh check sums right then we're going to use a a is similar to grep uh and we're going to print out the value here and we're going to convert it back to base 64 so a lot going on there let's go take a look and see what we get and make sure that this works hit enter and did it work so it's saying base 64 command not found um let's just try some of it so let's first try this part of it enter okay so that's good then we want to decode it so we're turning it back to normal great it looks a bit longer than before but I mean hey it should be fine then we want the md5 sum so let's try that out so I'm just slowly going through this and figuring out what works is a installed it is installed so let's go ahead and try that and maybe the issue is that we're using back ticks instead of single quotations so I'll try that instead there we go and then we'll B 64 it there we go and then that's the value we should get how different is it from the initial one oh very different Okay so uh this will be our export md5 value okay and I'm just going to do that and then I'm going to make sure that is there so I'll just do an echo on that md5 value good so now we can supply that here okay and so with that we should be able to pass this uh customer key over to S3 and then it will use this as its method for encryption as opposed to uh using KMS okay so we'll go ahead and try this out fingers crossed it this works it doesn't like something we'll hit enter unknown option ssy customer md5 oh I think it's supposed to be customer key md5 that's why we'll go ahead and try this again hit enter invalid argument when calling the put object operation the calculat md5 hash of the key did not match the hash that was provided so there's something wrong here what it is I'm not 100% sure so just give me a moment to figure it out okay all right so um I did find an example here like an ovh Cloud which is a I suppose another cloud service provider um they're suggesting here to use base 6432 and then they have a hyphen w0 on here I'm not sure why this one would really matter um noticing here they're not converting back to Bas 64 uh so it does look a little bit um more simplified so maybe we'll have an easier time with that so and it's not they're not doing the decoding here either so maybe we just fiddle around with this a bit it'll work better so I'm going to go ahead and give theirs a go it's interesting how close I was to theirs and it just wasn't even looking it up I wrote this myself so it was just very interesting and we'll go ahead and we'll use this instead and see what we get so let's see if that works hit enter it does good I'm going to go ahead and Export that instead and then I'm going to go adjust this so they do md5 sum here they don't do the decode okay um they AIT which is fine and then they do that there we can leave The Hyphen end there if we want I think that should be fine but just to make it the same just so that we rule up some issues I'm going to do that so I'll export this one I'll export this one and let's go ahead and try this and enter fingers crossed still doesn't like it so identical instructions on somewhere else that suggest it should work and it's not working so let me play around with this a little bit more okay this reminds me of issues we had before um because I can't remember what fall along we did somewhere in the in S3 where it was just so hard to uh pass yeah I guess it was for check sums it was just so hard to pass that value along uh this person uh here in GitHub is suggesting that both have to be encoded as B 64 this is BAS 64 this one is BAS 64 as well as we're encoding both of them so I'm not sure what it once um it's also suggested that we could leave that out and we will automatically calculate it the problem is is that we need the same customer key in order to uh in order to uh read it right so that's where I'm kind of concerned about is that we won't be able to uh decode it after the fact but um but I guess we have the key right here so maybe that's not an issue so what I'm going to do is I'm going to go ahead and I suppose get rid of this part and see if it let us upload without the md5 okay so we'll just grab this part here and then let's see what happens the secret key was invalid for the specified algorithm um well hold on here did I copy this correctly let's try this one more time okay so this is my frustration with uh some of these things it's like okay it us is like you can do this but try to find an example that works is really hard let me go figure it out okay all right so it just has a workshop over here and maybe this will uh be a easier time for us to work with it at the end of the day we just need it to work so I'm just going to go here and paste this down below we'll just say failed attempt here failed attempt and we'll go down below and we'll just try put object with um ssse hyen C via adabs um S3 and see if we'll have an easier time time with this so let's go ahead and try this out so here they output the key to a file okay um so that's great and then the next step they do is they do it this way so we'll go ahead and try this out um this will be hello.txt okay and I need to grab the bucket here paste that there and we'll say hello.txt all right so let's first try to generate out this key hit enter and here it says extra option is it missing a flag or something notice up here it says base 6432 down below here it just says 32 and we know that we can just do B uh ran 32 we did that before so what is it what's the problem um maybe there's different versions of open SSL so we can go here and say open SSL and type in help and we have a lot of options here standard commands maybe we do Rand it'll tell us more and so we can specify an out for an out file which is what we're doing there is the base 64 encoding here it says value so here it has a hyphen Rand load the given file with the into the random number generator but that's not necessarily saying that it has to be 32 in length that's just saying that you're providing a random value to be generated parameters num so it takes one parameter which is um the number of bytes so if we go here and we type in 32 we get that and we do hyphen o and what did they say they said um ssse SS I'm just trying to keep with the naming convention here and so this flag here I just replace this make sure this is correct and hit enter again Rand extra option for this so maybe instead of putting on the end here we'll put over here hyphen out SS sec. key it's weird that it shows that as the option and then it says it's an extra option so let's go ahead and look that up open open SSL without flag so here the argument parser became stricter in 1.1.1 if you run help as suggested by the text above you can see the Syntax for the command um I'm trying to use the following command and it's not working note that the number of bytes appears after the options okay so I guess what it's saying is that it's supposed to be like this there we go that is why uh it doesn't not matter how many workshops or instructions online you have to work through these things and that's why I record this stuff so you can see the struggle of getting this stuff to work so that is going to generate us the key and there it is and now let's see if we can upload it here there we go okay so I'm again I'm not saying that this is wrong it obviously wasn't working as expected but this is what's working for us we've upload the file now let's see if we can download that file so here it is hello hello world I'm going to go ahead and permanently delete that what is going to happen if we download this now so I'm going to go here and I'm not going to specify any keys so I want to see what happens if I try to download this hello.txt what do we get goad enter and it says bad request so it's not going to let us download it there now let's say we provide these two options we'll go ahead and do that hit enter and now we're able to download So if we do not supply the customer key it just will not download um it would have been nice to do that with S3 uh API but it was just too much of a struggle I'm not even sure if we should go ahead and attempt DDS because I believe that we have to utilize um the uh the put object and we already had some difficulties while doing this here but let's go take a look and see what options we have here for the iabs S3 go down here and we'll just search S3 because it shouldn't be that difficult to utilize it seems like it should just be double Flags but um let's just see what options we actually have when we do copy and so when we're here what I'm looking for is to see if we can do DDS SE or whatever it is and I don't see that option yeah D or D ssse so it's not there and because it's not there I don't want to attempt it here um if we do try to do that third option dsse then we should go ahead and do that as a separate follow along and try to get that S3 S3 API working but I feel like we've satisfied enough here um so yeah there you go and I guess we'll just clean up here and be done with it so I'm going to go into this bucket we're going to take a look for that the only Al also we didn't explore was bucket uh bucket key that thing is really straightforward you just turn it on there's nothing super exciting about it we'll go back over to here I'm going to go ahead and empty my bucket and then we will go back here and we will delete our bucket there we go all cleaned up all saved and we'll see you in the next one ciao let's talk about S3 bucket key I mentioned it prior when we're talking about ssse hyphen S3 and ssse hyphen KMS this is a security feature has nothing to do with object Keys it's just they both happen to have the word key in the name so when you use ssse KMS an individual data key is used on every object request and so in this case S3 is going to have to call adus KMS every time to get that key um KMS charges on the number of requests so this this charge can add up at volume might not be an issue uh for everybody but definitely can be uh an issue at scale so the S3 bucket key lets you generate a short live Bucket Level key from from databas key that is temporarily stored in S3 this will reduce the request cause by uh up to 99% this will decrease the request traffic uh improve overall performance what confuses me about S3 bucket Keys is that you can apply them both to S hyphen KS and ssse hyphen S3 and SS hyphen S3 is free to use so you're not going to get any cost saving with SS hyen S3 but maybe you get an impr improved performance because if ssse hyphen S3 is using KMS underneath maybe they have like a manage globally key for everybody I don't know but um uh I guess the only advantage of using with ssse hyphen S3 is to reduce or improve traffic performance overall but the way it's going to work is when we apply a encryption at the Bucket Level which is something we can do you saw that we were applying at the object level but you can just say globally hey I want to always use with this um uh uh the default encryption to use this KMS key but we can specify bucket key enabled so always use a bucket key um so a few other things a unique Bucket Level key is generated for each requester so it's not going to be for the entire bucket for everything but it will be for the entire bucket per requestor so unique request you can enable bucket key at the Bucket Level to apply all new objects that's what we are showing in that example in the S3 API you can enable bucket key at the object level for only specific objects I didn't show that because we show a lot of object specific stuff in uh a lot of other slides S3 bucket key can be enabled for S3 S SS S3 and SS KMS and again this makes more sense for SSC KMS if it's a cost reason if it's just a performance reason then maybe SSC S3 can benefit from this okay let us talk about client side encryption for S3 so this is when you encrypt your own files before uploading them to S3 this provides a guarantee that ads and no third party can decrypt your data I want to make a distinction here between this and SSC hyphen C where you are generating out a key but the difference is that uh KMS is the one server side that is decrypting it uh whereas in here there's no server side decryption happening okay it's all client side so there are various adus sdks that have builtin code to make it easy to encrypt your data here is a example with Ruby I've looked at all the different sdks some are much easier than others um but the idea here is that we are requiring open SSL we did this kind of before with SSC hyphen C where we use bash terminal to use open SSL to generate a key so we generate a new key here and then we're configuring it globally for the S3 client um so that anytime we do a put or a get it's going to automatically use that key and so here um we have a key and a bucket and then we provide some data so this is the secret data and notice we put the object so we send it to S3 and then when we do a get object when we read it it's going to decrypt the data but I need to make the distinction here that this is happening client side this this key never went to ads it just say on your computer and it's just here in the configuration if you were to create a new client uh so this configuration here is not going to be applied then it's just going to return the cipher text because that's what it is without the key and you don't decrypt it you just get a cipher text back so there you go hey this is angrew brown and this fall along we're going to cover client side encryption uh specifically for S3 though doing client side encryption you can utilize it for a lot of things um but I'm getting my environment set up here we're using the same adab examples repo as we have been so far um I'm going to make a new folder here I know we have an encryptions folder so I'm going to say new folder encryption client so it's less confusing and we'll make a new file here we'll call it readme.md so um we're going to have to first create ourselves a bucket so we'll say ads uh sorry create a bucket and this will be ads S3 makeb bucket S3 Co SL slash um encrypt client fun ab and we'll put some numbers here I'm going to go ahead and copy this we'll go ahead and paste this down below say allow hit enter and it over copied stuff there's no way I copied that line that many times we'll go ahead and try this again and paste this in down below and there we've created our bucket so we want to create a file so we'll do that create a file Echo hello world and uh say hello.txt as per usual let's make sure we are in the correct directory here so I'll go ahead and N CD into that and we'll go ahead and paste that in here we now have we should have that file in there I went into the wrong directory that's why encryption client we'll try this again okay and so now we need to um encrypt our data I'm going to find the easiest solution is to use Ruby so I'm going to go ahead and do that if you don't like Ruby tough luck I go ahead and do a bundle install here or sorry bundle init and we're going to initialize a new gem file and in here there's a couple things that we probably want we're going to want the ads S3 SDK because they have examples for this um every single um uh SDK has a different way to do encryption for client side this basically is the same code that we're going to look for for that we could just look it up and say client side encryption for S3 and see if we can get a code example client side encryption S3 and if we can find it here in the docs we have a ruby example and here it is very simple very straightforward that's why we're grabbing it so we have the adus STK installed another thing that we're going to need is something like noiri noku noou I'm going to just put oxin it's just an XML part so there's a lot of options you can choose from Ox is just easy for me to spell doesn't really matter what we use I'm going to just put uh encrypt here as the file name we'll go ahead and paste the contents in here and so this is going to use op SSL it's going to um create an encryption key then we're going to create a client with encryption key and then we can put our object to our bucket and then we can see how this turns out so I'm going to go ahead and grab my bucket name here and go back over to here and we'll just say on a new line bucket equals and then single quotations and we will supply that bucket here same story down here and we'll also want to provide the key so I'm going to go ahead and provide the key here so this will be hello txt so that's hello txt hello txt um we can Pro like I mean we created a file here I don't think we actually have to create a file I'm just going to go ahead and delete that because um when you supply the body here it's not necessarily going to point to the file like we use in the C live this is actually going to provide the actual contents of the file okay um here if we wanted to have a file we'd actually have to say like file. read and then make it a file and then pass it here and then that would work okay so this is actually going to be the contents that is inside of it um and this all looks really good so I think this should work we have a get a put um I'm not sure if it's going to read any of this stuff out to the screen so that's what I'm really curious about we'll go ahead and type in bundle install if you're finding it hard to get Ruby uh installed you should really use a cloud devel environment because they install these things by default for us so let's be 8 us SDK S3 like that we'll do bundle install here and so that will get our stuff installed give it a moment it is thinking let it think there we go and so now we can run this bundle exac Ruby encrypt RB if you're not sure what that is I'll go ahead and copy this and then if you want to grab it from here we don't need to create a file so I'm can just take this out so run our encrypt and upload run our script our SDK Ruby script we don't even need this we'll just take this out so so we'll run this and we'll see if we can get any output from this enter and it's saying doesn't know what ads is um we need to require that up here I like how uh code examples always miss the things you need it's great e we'll hit enter and it's complaining about um some information here this version of the S3 encryption client is currently in maintenance mode it was strongly recommends upgrading to the adabs version 2 so this is ad's documentation and they're pointing to the old stuff so it's good thing we ran this a let's go ahead and put in version two and let's see if that is enough to make this work it's getting better missing required parameters key wrap schema content encryption schema security profile so a lot of options here that we don't normally need to provide um but let's go take a look and see what that is yeah it says right up here so pointed us to this one um can we get version two by just writing it in here like this version two there we go okay and so that looks a little bit different so copy this paste it in and that's fine so we have RSA a p e something something bunch of stuff looks good to me what is it I don't know but we need it let's go ahead and run this and now we still have an error invalid key symmetric key required to be 1624 32 or whatever so it could be what we're Jing about here it does not like um let's see if this line is actually the same we'll paste this in go back here sorry I don't think it actually copied we'll try this again copy and I will paste that's identical right so I'm taking the code exactly as it is and it's telling me it doesn't like it give me a moment to figure this out okay all right so asking chat gbt it's saying like we shouldn't use this but at the same time it's telling me that I'm overwriting the variable so I don't think that it knows what it's talking about at least in the top part here um but we're definitely over writing that key so what we'll do is just call this bucket key or object key and so we'll have less of a conflict here and that's maybe what our problem is um we don't actually even specify the key here which is kind of interesting but I'm going to go ahead and just change this to object key and we'll try to change this to object key and see if that resolves our issue okay so I'll go down here type in clear we'll go ahead and hit up and now we have less of a problem it's now not outputting anything but um we can find out some stuff here so I'm going to go ahead and just type in RP here RP here and we'll just put some puts so we'll say uh put and this will be get secure okay and this one down here will be get not secure well it's not necessarily it's not secure but with key like with key uh without key and then we'll just put these we'll say puts response puts response and then this one down here puts response we'll take a look and see what we get and um so here we're getting some information back what I'm looking for here is the actual um body or response information uh this isn't very useful for me here so what I'm going to do do is put a binding pry in here so we can kind of investigate and take a look at what we want to see okay we'll do a bundle install we'll go ahead and hit up I'm going to run this again oh I got to put a binding pry in here so we'll go here and put a binding pry right here okay and so we got a response here so I can put response the that looks good so nothing useful yet but we'll go down to this part here oh by the way this one's called reps we'll say response and then this one has to be assigned to response that's probably why we weren't seeing anything different here we'll take out the binding pry now we'll go ahead type exit with an exclamation mark we'll go hit it up and so now we get it so we get get with key so we get handshake get without key we get it the without um we get the um Cipher text back so yeah there you go um that's pretty straightforward let's go ahead and just clean this up so we'll go back over to here and we'll want to remove this bucket just say clean up here and we'll want to remove the file which I believe is hello txt and there you go that is client side encryption we'll see you in the next one okay ciao what is data consistency this is when data is being kept in multiple places and it's about whether the data exactly matches or does not match hence the word consistency when we're talking about Cloud there are two terms you should know which is strongly consistent and eventually eventually consistent let's talk about what these things mean so strongly consistent is every time you request data you query data you expect consist consistant data to be returned within a specified time so if we say 1 second it'll come back in 1 second so we will never return to you old data but you'll have to wait at least two seconds for the creative return would be an example now let's talk about eventually consistent so when you request data you may get back inconsistent data within two seconds So eventually it will be consistent is not consistent at the time of when you request it so we are giving you whatever data ISR currently in the database you will get a new data or old data but if you wait a little bit longer it will generally be uh up to date or consistent so this really mattered back in the day when we're were talking about S3 because some operations in S3 were consistent are strongly consistent some were eventually consistent now S3 offers strongly consistency for reads writes and delete operations so prior to 2020 it was not the case for all S3 operations this is more or less all the operations the operations that you care about the read the wrs to deletes all the crud actions um so just remember that Amazon S3 offers uh strongly consistent or strong consistency for their S3 operations so the idea of S3 object replication is when you basically make a copy of a object somewhere else and the reasons why you might want to do this is to replicate objects while retaining metadata replicating objects into different storage classes maintaining object copies under different ownerships keeping objects stored over multiple abis regions replicate objects every 15 minutes sync buckets replicate existing objects and replicate previously failed or replicated objects replicate objects and fail over a bucket in another adus region so lot of cases where you're doing object replication um there are a few options in S3 for this uh and we will cover them all the first is cross region replication that's where you're copying objects from one region to another same region replication this is where you're copying objects um to another bucket in the same region bir directional replication this is where you replicated one place and back the other way and then there's batch region replication so a bunch of options but of course there's more to replication than just these four options as it can be based on time based on uh size of data things like that but I want you to know these four core methods of object replic so we can understand all use cases for ad okay cross region replication also known as crr is when bucket objects are replicated to a bucket in a different region why would you want to do cross region replication there's a few reasons why it provides higher durability and potential disaster recovery for objects some companies need objects to exist in multiple regions for regulatory compliance accessing objects in the nearest region will result in minimized latency uh so those are the main reasons let's talk about some considerations if we want to use cross region replication the first thing is you must have S3 versioning turned on both for the source and destination bucket you can have uh crr replicated to another uh it account so just understand you can do cross accounts with this there's nothing restricting you from doing that uh it will replicate new or updated objects objects before the replication rule will not be replicated so for those that are prior you might have to uh run a batch operation and that's generally why um we talk about the S3 batch operations later on but uh yeah consider that let's take a look at how we'd actually configure this uh VI the ab so you'd have um a put bucket replication and you provide it a Json with the replication rules so this more or less is what the replications rules look like obviously the Abus console displays them a little bit different differently but it's more or less the same and so the idea here is that we are saying I'm just going to get my pen out here we're saying that uh we have a replication roll uh this is active we're going to filter for uh anything with no prefix that's fine it's going to be go to this destination bucket and it's going to go to this storage class so there's some levels of configuration here uh but we'll go to uh the next one here okay same region replication also known as srr is when bucket objects are replicated to a bucket in the same adus region why would you want to use this when we already have cross region replication well imagine you're aggregating logs from multiple buckets into a single bucket this is a use case that could happen if let's say you have servers and those servers need to log to S3 and they're going to end up all in the same region and there's reasons why they'd all end up in the same region because of latency because of uh maybe the security tools they can only deliver or it's it's most cost effective to deliver in the same region they don't want to leave into a different region because of data transfer costs so it makes sense to dump them all to different buckets but now they need to aggregate the data so maybe they're trying to pick pick and choose different logs and then have a single aggregate log at the end so they'll all end up in the same region uh then we have replicating of objects for different workload environments so if you have a production uh environment you might want to have a development or staging you wouldn't put them in another region because you want to again best emulate uh what that environment is like so um you know if you were to put it in another region then you'd have different latency and that could have different results there could be sovereignty laws so in Canada there might be requirements that you're not allowed to have the data uh in the States or other places and there are no other regions so like in Canada we only have at this time one region they're supposed to have one Calgary so you maybe you need to meet the requirements of having a failover and so they say Well we'd rather in another region but there is no other region so we'll just do in the same region to to meet that requirements in our special conditions let's talk about some considerations it's going to be basically the same as cross region replication you have to have S3 version turned on for both the source and destination bucket you can do cross account same region replication if you like to and it will replicate newer updated objects OB before the replication rule will not be replicated to set up the replication rule it's pretty much identical it actually is identical to what we saw for the cross region replication in that Json file the only difference is you're just specifying a bucket that is in the same region so basically identical with the exception that the bucket is somewhere else in another region okay bidirectional replication also known as twoway replication is when you replicate data both ways so uh you have two buckets and they're going both ways anytime one makes changes why would you want to do this well a great reason is for failovers so if you have an issue with bucket a you're going to fail over um to bucket B now bucket B is going to uh read and write objects to it but when bucket a comes back online it doesn't know about all the things that bucket B has done and you got to bring that data back in that direction so that would be a reason for it um even if it's not for failovers let's say you have two different regions and there's two different servers and each server writes to their Regional bucket but you want to make sure the data is available in the other bucket that would be another reason as to why some considerations you got to turn on S versioning of course you can do uh operate this in different uh different regions or same regions you can replicate across accounts as we keep saying with uh replication here and if you are trying to uh serve that Second Use case with latency trying to serve to the nearest bucket you can use S3 mult multi region access point uh to do that we have a whole section on S3 multi region access point so if that doesn't make sense right now it will'll make sense later on the replication rules for uh bidirectional is the same as SSR as it is the same for cross region replication I wrote CCR but it's actually uh crr but you're basically just creating two uh replication rules in each Direction okay bat replication replicates objects uh to different buckets as an OnDemand option as all of the other options we specified automatically happen when a change occurs this is only going to happen when you tell it to why would you want to do batch replication uh well replicates existing objects that were added to the bucket before srr or CR as we said that it's those ones only apply for new objects not objects prior to when the feature was turned on so this feature is going to help you with previous objects it'll replicate objects that have previously failed to replicate so there it's totally possible for srr CR to fail so there might be some outliers that you want to make sure get replicated it can replicate objects that were already replicated so that is another reason uh replicate replicate replicas of objects that were created from a replic replication rule so there's another use case replicate on a need B basis so just whenever you want to do that um I really wanted to show you how to do that via the CLI but uh it's actually part of S3 batch operations and there's no CLI commands for this a particular functionality you could probably simulate it by writing a script uh but it's one of the few times you cannot utilize the API directly in order to uh take uh advantage of that functionality which is interesting but there you go let's take a quick look at some replication rule configurations that we can apply uh which I think are kind of important so the first we're going to look at is RTC which is replication time control this replic replicates 99.99% so uh 49 of new objects within 15 minutes includes replication metrics and this is additional fee that will apply if you need those 49 then we have replication metrics this monitors the total number and size of objects that are pending replication and the maximum replication time to the destination region you can also View and diagnose replication failures Cloud watch metrics fees apply we have delete marker replication this is when you are utilizing S3 versioning um so delete markers created by S3 delete operations will be replicated delete markers created by life cycle rules are not replicated a delete marker is a placeholder or marker used to indicate that an object has been deleted this is specifically for S3 versioning if you're not familiar with what we're talking about I think we cover S3 versioning extremely soon but anyway there you go hey this is Angie Brown and this fall along we're going to take a look at doing some uh replication uh cross cross region same site same site uh or same region replication they're pretty much the same thing um it's just depending on where you want to point it to so we have our repository open as per usual I'm going to go over here I'm going to create a new folder in here I'm just going to call this uh reg or we'll just call this replication and we'll create ourselves a readme file and as per usual we'll go ahead and create ourselves a new bucket so we'll say create uh and S3 bucket so be a S3 make bucket S3 colon SL replication fun AB for my initials and some numbers here we'll go ahead and create this bucket if copy and paste wants to work for me here today there we go so we have our bucket the next thing we'll want to do is set up uh replication on it so uh we'll go ahead and type in ads S3 API CLI make our way over to that documentation there and in here there should be something for replication I'm just going to search for replication there we go found it let's go down below to the example we should have a full example here we'll go ahead and copy this and we'll say turn on replication I guess the other thing is that we're going to need a destination bucket right so I'm going to go make another one here and maybe what we should do is specify a different region again if it's same or different it should work the same way but I'm going to go ahead and just say region Us East one and this will be us East bucket here and we'll go ahead and create this one and so hopefully that is in another region let's go take a look just to confirm we'll give this a refresh and so this one is in Canada Central this one is in Us East one so that looks like it's working as expected um so yeah we're going to want to have to turn on replication here there's a few rules we're supposed to apply um we need to have versioning turned on in both the source and destination bucket so that is going to be the first thing that we're going to want to do so before we turn on replication let's say turn on S3 versioning for both buckets so we go all the way back up to here to the top I'm going to go to version two S3 API we're going to look for versioning and we'll say put bucket version go to examples we'll copy this here and I'll paste this here that's one that is two this will be for the first bucket and then this will be for the second bucket and we will copy this and press enter copy this and press enter so now they're both turned on and now we can set up replication so I only want to replic from one bucket to another bucket I'm go and place that there we're going to need um this replication Json file so we'll go ahead and do that going to make that file there uh try that again new file and there it is let's go ahead and copy the contents have to click back here uh I want replication examples we'll grab this example here the destination bucket must have version turned on we've done that so that's okay and uh here is specifying an S3 replication role so I would think that we'd have to create one here then we have status enable priority delete marker replication disabled filter so basically uh go on everything and then the destination bucket so this one's going to run on this one and then the destination bucket is going to be us East one so we'll go ahead and go over here but my question will really be this um policy here that we need or this rule I don't think that already exists by default I'm going to go over to IM and take a look and see what we have sometimes ads provides us managed rules I don't think there's one for this let's take a look here replication I don't have anything here so we're definitely going to have to create a rooll um oh here's the RO permission so they have it right here and that's the trust policy um we can click through and just create it here we really should use the CLI and do it that way so we'll go ahead and say create um R and policy for S3 replication and we have these two objects so this will be the the permissions so we go here and say file new whoops file new this is one's going to be policy. Json and I'll paste the contents there and then this one down here will be the press relationship so I'll make a new file here and just say trust Json and we'll paste that on in here so this one's pretty straightforward for replic or sorry for this policy we need to swap out a few things this is going to apply to the source bucket and it needs permissions the other ones oh they list one one two two so one one must be the source bucket let's hope that's the case and two must be the other one so go ahead and grab this I wish they'd write like source and destination not just one and two it'd be a a little bit less confusing but uh that didn't really copy correctly we'll go ahead and copy again we'll try this again here it's really finicky the copy paste see I don't even know if I got that wrong now um we're going to have to go back and check hold on here this has um one two three okay so let's try this again just going to take that off there and say S3 one two 3 and then paste this in normally I work on Mac fim I don't use work in Visual Studio code I don't have any of these problems but uh when I'm doing these tutorials with you it looks like I'm struggling a lot more than I actually would normally so we go ahead and copy this we'll paste this down below here and put the for slash on and take that one out this one is very similar um I'm going to go ahead and grab this one here quickly and then I'm going to go back over here and we'll grab this one then we have to carefully select like this and paste this in notice that it has an aster there on the end so we'll grab that resource put this in parentheses there good and let me just go double check and make sure that's right so this one also has that as well so I just want to make sure I did not make that mistake by leaving it out good so we have our policy we need to go ahead and create this so going to go back here and let's go look up that command so to ads here and I'm going to look for IM and we need to create ourselves a policy first so we'll go ahead and create a policy we'll go down to examples and here is one we'll paste that in here so will be um what do we call this this is S3 replication example and this file is called policy Json just going to put that on one line so it's a little bit easier I'm going to make sure I CD into that replication directory and just type clear here so this should create that policy hopefully there's no issue with our policy Json it looks like it created so that's fine it also gave us back the policy ID and AR we might need those I'm just going to copy those really quickly out of there and paste this here your AR is obviously going to be different than mine AR let's go take a look at the rule now so we'll have create roll and we'll go over here we'll go down to examples we'll copy this and we will paste this here so we have what were we calling the roll over in our replication it was this so I'll go back over here and we're going to paste that name here um we need to specify our trust policy so I'll type in trust here this isn't attaching anything so that might create the RO but then we need to actually attach the policy so that'll be a separate command I think so we'll go ahead and paste this in here and that created I'm going to again copy this because I just assume that we're going to need both those values in a second here if it's just name that'd be really nice so we need to attach the policy so I'm going to see if that is a command attach rule policy so this seems like what we'd want that looks like what we want yep and so we'll go down here below and paste it in so we have our policy Arn told you we need that so we'll go ahead and grab that here remember if you're following along you cannot just copy paste my example because I'm going to have different values here right and then it just wants the r name so go ahead and grab that and we'll paste that in there and so that should attach it I I'm going to keep this Arn around because we're going to need it in just a second but I'll go ahead and I'm going to go attach this there we go and so that should now be attached I'm going to go ahead and copy this full Arn and go back over here and paste it in because this doesn't have a real um account number and now we have our real one in here uh just to double check I'm going to make sure that this all worked out correctly and I'm just going to clear this out give this a refresh we're going to type in replication and we're going to double check to make sure our permissions are what we think they are so here they are that looks good trust relationship that looks good we are in good shape I'm going to go back over to here just delete out this here and so now in theory we should be able to put uh bucket replication on so we'll hit enter and that is turned on let's go take a look at our bucket and see if it actually is turned on so we'll go over to S3 and we will check our source bucket for replication um so that would be this one here and we'll go to properties and I'm looking for those replication rules permissions no management here we go so we have a replication rule okay we could have gone ahead and created it here and this gives you an idea of the options that you can choose you can see we have those additional replication options which uh we talked about in the lecture content but um we'll go back over to here and take a look and so we can see where it's replicating to its destination region so we know that our our configuration worked because if it didn't it wouldn't show all this information right um and you know the only way it's across region or a same region is basically just where you point the bucket notice we didn't have to specify the region anywhere just picked it up if we click here it shows us more of the information let's go take a look if it works so what we'll need to do is place an object into uh this one here and see what happens so create a file and upload to our bucket so we'll go ahead and do that um this will be copy we'll say hello txt hello txt we'll type in Echo hello world and then we'll pipe it over here and say hello.txt I'm going to type in clear down below and we're going to go ahead and paste this in and we're going to go ahead and paste this in so the idea is that replication should occur now um how fast it occurs I don't know I guess we'll find out when we check the bucket let's go check we'll say check destination bucket to see if replication worked so we'll go ahead and grab this one up here and we'll just say it S3 LS and we'll list that out and see if it worked because this is our destination bucket right we'll hit enter and it has been replicated so there you go it's as easy as that to set up replication if you want to do it uh both ways you set up replication rules in both directions um but let's go ahead and just clean up now that we're done so I guess the question is if we delete a file will it delete on both sides that would be interesting to know right um the other thing is that we can do batch replication now we do batch operations in another video so I don't feel like we need to show that here but um yeah let's see what happens if we delete the file so I'm going to go here and and again remember it's one way so if we delete the file in the uh Source One I wonder if it'll delete in both so we'll say it ofs S3 remove and then we'll put this here okay so go ahead and we'll paste this in here we'll hit enter and then we will go and take a look and see if it happened in both directions it didn't so understand that we created in one place replicated the other place and then it didn't delete so could we change that in our replication rules not 100% sure um delete marker replication so yeah I'm not sure about that but at least we know replication means to copy as opposed to just delete as well um but let's see if we can clean up our buckets here so I also want to delete from our source bucket as well so there's this one not Source but our destination bucket this one's going to be S3 remove hello.txt we do have versioning turned on so hopefully we can still get rid of these buckets easily we delete them and I'm going to go ahead and attempt to delete these buckets okay we'll go ahead and hit enter the bucket you try to delete is not empty you must delete all versions in the bucket ug that is not fun to do we'll go over to our bucket here and let's see if we could just empty it because that would make my life so much easier if we can just empty it out empty the bucket okay I didn't read that message but I'm going to assume that it's going to empty out all versions too delete delete we'll go back over to here empty we'll delete it here and then we'll go back here and we'll go ahead and delete okay so now it's all cleaned up um you know you should delete the but buckets empty and delete the buckets easiest way to clear all versions so there you go that is replication and I'll see you the next one okay ciao let's talk about S3 version which allows you to store multiple versions of an S3 object so the idea here is with versioning you can recover more easily from unintended user actions and application failures and versioning enabled buckets can help you recover objects from accidental deletion or if someone attempts to overwrite a file so the idea here is we store all versions of an object in S3 at the same object Key address by default S3 version is disabled on buckets and you must explicitly enable it itus will always recommend for you to do this S3 versioning but you got to understand once you enable it you cannot disable it only suspend you can only suspend um uh versioning on a bucket so it fully integrates with S3 life cycle rules uh MFA delete feature provides uh an additional level of of protection against deletion of your data something we definitely need to know for the exam is f MFA delete um but anyway buckets can be in three different states with S3 versioning you you have unversioned so that means you don't have it turned on uh it's versioned so it's turned on or it's version suspended so you have suspended future versions of it so there's three states of it let's go take a look at how S3 versioning works with objects all right let's take a look at how to add object versions so when you enable S3 versioning all objects will generate a unique version ID so as soon as you turn on they're going to have uh this version ID you're going to notice that in the front there uh it's identified with the header X AMZ version ID so let's assume that's going to come back in the the API request when you update an S3 object it will store both objects at the same address so imagine I have an object called teroc no.png and then we are pushing an updated version it's going to have um the same key but it's going to have different IDs of course the ID is not 1111 or 12121 that's just kind of a filler for the more complex unique version key above there so the way it works is when you put an object it's going to turn back a version ID uh along with its zag so that's how you're going to know how to identify a version for later okay all right so now imagine we have an object in our bucket and we want to get a list of versions of that object well we have a special CLI command for that that is the list object versions so we Supply the bucket and the specific file so that one's index HTML and what it's going to return back to us is the versions you'll also notice here that it's going to also return back the delete markers we'll talk about what delete markers are in another video but I just want to point out that that's where they come from from this list object versions but in here we can get all the versions We have and in particular we can grab the version IDs so we can request the specific object version that we want okay let's talk about getting object version so when you use a get on an S3 optic it will always return the most current version of the object so you don't have to specify a version ID it will always just get the one that's on top now if you want a very specific version of an object you're going to have to specify the version ID flag uh and that way you'll get that exact one retrieving metadata will always return the current version so you need to specify the version object for the version metadata if you want it so just be aware of that okay let's talk about deleting objects which is where it gets a little bit more complex for S3 versioning so when you delete an object without specifying a version it will make the current version a noncurrent version and then it will add a delete marker as the current version so the idea with S3 versioning is that when you delete an object it doesn't delete it it just takes it out of the versioning so that you can recover that object later on if you need it let's talk about delete markers um so a delete marker is a placeholder for a version object it has a key and version ID just like a version object it doesn't have data associated with it it is it isn't associated with an access control list it takes up space and inures uh storage charge but the size of it is the name of the file so it really isn't taking up a whole lot of space if the current version is a delete marker it will return a 404 and response header of X AMZ delete marker true if a specified version is specified and and it's a delete marker you'll get a 405 response header of a uh X AMZ delete marker and last modified so the idea is that if you want to permanent delete an object version you need to specify it version ID and that's apparently how it works so that's how you're going to do that okay let us talk about restoring object versions as there are two different ways that you can do it I'm going to keep this really simple um but the first is copy a previous version of the object into the same bucket so the copied object becomes the current version of that object and all object versions are preserved the second one would be permanently deleting the current version of the object so when you delete the current object version you effect turn the previous version into the current version of the object so those are the two mechanisms to which we can restore object versions so there you go let us take a look here at MFA delete which is an OP security precaution for S3 versioning that requires you to use multiactor authentication before you can delete an object so MFA delete can only be enabled under these conditions you have to uh turn it on Via the it API you cannot use the console so the CLI or the SDK is how you're going to turn it on the bucket must have versioning turned on as this is a feature of S3 versioning so the idea is you use CLI you're going to pass the MFA code with the API request and you need to sure that bucking bucket versioning is turned on here's an example of turning on bucket versioning so here you can see we have a version configuration status enabled so the bucket version or versioning is turned on and MF MFA delete enabled is turned on and then you can see here that you're providing the MFA code so only the bucket uh the bucket owner logged in as root user can delete objects from a bucket so it is kind of a pain to delete objects but it does provide that extra of security so there you go hey everyone it's Andrew Brown and in this follow we're going to take a look at S3 versioning which is a really cool feature to ensure that you keep multiple copies of your file so what I'm going to do here is go over to my EDS examples website as I want to programmatically work with S3 versioning because I do think that is a really good way of um working with it and I'm just going to open it up and and write my commands in here but we will also use the cloud cell CLI in order to execute commands I'm just going to open this up here and I'm also just going to open up S3 as well so that we can kind of explore what that feature looks like when we create a new bucket so the thing I want to do is create a new bucket and I don't want it on this screen so I get it out of the way hopefully it stays over here and if we look at our creation bucket screen and we scroll on down we'll have bucket version enabled right here so this is where we can enable bucket versioning now you can create a bucket and not have bucket versioning turned on so we can turn it on retroactively and that's what I'm going to do here that's what I want to demonstrate so what I'm going to do is over here in S3 I'm going to make a new section here and we'll call this S3 versioning versioning versioning there we go and uh in here we'll just create some scripts so I'm just kind of looking at what I was doing here before if there's anything interesting no not really so we'll just say say create bucket and what I want to do is create myself a new S3 bucket so we know that this is s or AWS S3 um mkd for or sorry MK MK just for bucket let's go take a look here I'm I'm forgetting it's MK something but we go over to our CLI command documentation and we click here into reference and we make sure we're in version two and then we go look for UHS S3 here we could use S3 API but I'll just use regular S3 here there will be a command for oh MB MB for make bucket okay so we're going to say MB and then we need to specify the name of the bucket so I think here it wants a um S3 bucket address let's go take a look here we go to examples yeah it's an S3 bucket address so I'm going to name mine S3 col SL slash this one will be adabs uh S3 versioning and then I put my initials on the end and a couple of random numbers I'm doing that because again bucket names are randomized so that's what I want to do there so that will be my create bucket um action here just say create bucket and actually I'm just going to rename this to be um versioning because I'm just going to put everything in this file here S3 versioning um commands Okay so this will create the bucket um and so that's the first thing I want to do now before we do that we could also take a look if there are any options here to uh actually set them and I don't think so I think this one is extremely simple I don't think we get those options there if we were to go over to the S3 API I wonder if they would provide those examples to us so we'll go to our crate bucket here and we'll take a look because it might be um separate commands so we go here and I'm looking for versioning so it says version displays the version of this tool so that doesn't necessarily mean it's versioning and so I'm not necessarily seeing it here that doesn't mean that we can't turn it on at this level um but uh we do see object lock enabled for bucket so if this is here and that is a feature of uh versioning then you must be able to turn versioning here it must be under the create bucket configuration okay so probably under here is where we could specify it maybe so again don't see it and that's totally fine because there is a separate action to turn on versioning here I think so if we go here we can um probably turn on with put bucket versioning that's what I'm thinking yeah set the versioning state of the existing bucket so the first thing I'm going to do is create a bucket without versioning all right so we'll do that I'm going to go ahead and copy this and we'll go back over to the uh Cloud shell here and I'm going to paste this in here and notice that I'm in Us East one so I assuming that this is going to create the bucket in this location as I have not specified um any other address I know this is really small so I'm going to bump up the font here let's go even larger there we go and we'll go ahead and hit enter so this should create ourselves a bucket and so now we have a bucket there I want to go ahead and explore that bucket and and we'll open it up I just want to confirm that uh that bucket wherever it created here does not have version turned on so if we go over to properties maybe they're always changing this on me notice that it is currently disabled so what we should do is try to upload a file that doesn't have um any versioning on it so I'm what I'm going to do is go ahead and say uh create a unversioned um file and so to do that we're just going to say touch um my file.txt might want to place things into that file so I think we can cat stuff into a file I'm just going to go quickly ask chat gbt you can use whatever you like but I'm just going to go here and say uh how do I create an uh text file with some contents in it using a single uh bat command I think what we can do is pipe cat to it that's probably what it will do but let's see if it comes up with that and here use Deco so that's totally fine as well so I'm going to go and use that approach we could also use touch but that will create an empty file and I'll just say here hello world and um we'll take this part out and we're just we're not getting any uh syntax highlighting it's kind of driving me crazy so I'm just going to call this sh there there we go that's a bit nicer and so the idea is that uh we're going to create a file called my file and it'll say hello uh txt so we'll copy this and we'll go back over here and it closed out our Command shell or Cloud shell I should point out that cloud shell doesn't is not always available in every single region so at one point it was not available in CA Central 1 so if it's not available you might need to switch regions until you're going to be in good shape but if you opened up Cloud Channel wasn't working you probably already know that so what we're doing here is we're just creating a file locally and so if I do LS you can see that this file exists in cloudshell and now I want to uh copy that to our new bucket so we'll go ahead and type in ads S3 CP for copy and then we're going to specify the local file so we'll say my file my file.txt it's not autocompleting that's totally fine we'll do S3 cols S3 versioning AB 5 3 4 5 remember to use whatever yours is called there we'll go ahead and hit enter it's now uploaded the file let's see if we can see the contents of it so we'll says S3 LS and then we'll specify the bucket again here versioning AB 5345 we'll hit enter and so we can see that we have our file there now if we want to know if it's versioned we should go ahead and query that but I'm going to just go ahead and copy this and we're just going to keep writing these out so we have this information for later so we'll say um copy the unversioned file to S3 show the contents of the S3 bucket you really really should follow along and do all this because then it'll become second nature to you and you'll do so well on all of the exams not just Associates any exam that you're taking uh for adabs so show the contents of the S3 bucket uh notice notice that S3 LS does not show versioning information okay so this is not going to be very useful for us so what we want to do is actually use list objects we'll says S3 API and we'll say list objects there's also a version two but I think I'll just use the regular one because it's totally fine and then we need to specify the bucket so we'll do a S3 versioning AB 5345 the more you work with this the easier it becomes to remember these commands so I think that's what it is we'll go ahead and hit enter and that worked and so notice that it shows an eag and it's not showing any versioning information okay so we'll go here and that's fine uh get a more detailed listing listing of the bucket and we'll just say notice it does not show versioning information all right so there 's a separate command to get versioning information uh I think it's like something with versioning we'll go take a look here and see what it's called we'll just type version get bucket versioning list object versions that's what we're looking for so this is going to be the command to find out if there's any object versioning on an object so let's see what happens when we apply that to something that is not versioned yet so we'll go ahead and just copy this over here we don't have to write everything out by hand and say uh get object versions and see what happens on an unversioned object so that's what we're going to try here we know that our bucket here is called adus S3 versioning of course you have to replace this with whatever your bucket name is so I'm just putting mine in here and the name of this file is I think my file.txt so we'll go ahead and copy this we'll bring it back over into Cloud shell here we'll paste it in and of course we could be executing this stuff over in G pod as we configured that earlier but I want to try to use cloudshell a bit so going here and taking a look I'm just going to hit Q because I can't see what I'm looking at here we'll hit enter again and so here it's showing versions and it shows a version ID null so notice that it doesn't air out it actually still shows there's a version so make note that the version ID is null uh okay so it'll treat it like as if there's only one version okay so that's kind of interesting indent it so we know that's part of that part okay so now that we have created a file that's not versioned and we've inspected it to see if it has any versioning here the next thing that we can do is we can turn on versioning so that's what we're going to do next so let's go back over to the CLI here and let's go find that command it's going to be in S3 API because again that's for more lowl commands we'll type in version and I think that it's going to be put bucket versioning so we'll go here and this says sets the version state of the existing bucket enabled or suspended so we'll go down to the examples and here we have a command so go ahead and copy it we'll bring it back over to here we'll paste it on down below and we'll say turn on versioning for our S3 bucket so that's to turn that on um I'm going to go ahead and type in ads S3 versioning AB 5345 and it will set the status to enabled notice it's using the shorthand syntax so if we really wanted to we could pass this in as this right we could do this but it's nicer when it is using that shorthand so just understand that's Jason underneath if you ever need to manipulate that so that looks fine um let's go ahead and apply that okay we'll go ahead and paste that in here and we'll go ahead and hit enter and so now versioning is turned on it didn't throw any errors so that's totally fine let's go visually inspect to see if it's there we'll give this a refresh and notice now that bucking version is enabled so it is turned on let's see if we can see that via the CLI so we's say confirm bucket versioning is turned on Via the CLI I have a strong feeling it's just going to be get bucket versioning because we saw that earlier there or at least I was carefully looking I'm going to go ahead and just take out that I'm going assume it just needs a bucket this is my best guess we'll go down below paste this in hit enter and notice that that returns it back great so that can tell that that is now enabled so we'll go ahead and copy this so that is good um and so now let's go take a look at our versioning object and see if it actually has a version we saw that before we say check uh previous file if it has a version that's what I want to know so we'll go ahead and check that and we'll go ahead and paste that in we'll hit enter and um list operations um no it should be fine so it says it S3 my file the ER occurred when calling the list object version object the access key oh right this is not set up I've rotated my credentials out we need to do this over here I was getting mixed up and so now notice the version ID is still null so it says version ID is null so notice existing object the version ID is still null now to be honest I thought it was going to go retroactively added in I really can't remember even though I made a bunch of slides on it let me just go double check okay all right so I've done some digging and notice over here I say when you enable S3 version all objects will generate a unique ID and so I assumed it would be for existing objects as well but it turns out there's a caveat where if it's an existing object it's going to remain null it's going to be for future versions this is where if you're just reading the docs you could be missing that information because it's somewhere else or it's just really small um so it's really interesting to see that I I missed that cave yet but it is acting the way that we expect it to be and again it's not a big deal um but you know it's uh good that we could figure that out there so this has still version ID null so what we'll do is we'll go ahead and attempt to do an update so we'll say uh let's update the file F in order to apply version and we'll go up here and we'll say um we'll want to copy this and let's figure out how do we so how can we replace the contents of a text file uh using bash we'll say completely let's see what it says I wonder if we just run Echo again to completely replace it you you can use Echo command operation it it will just overwrite it okay that sounds good to me so what we'll do is go back over here we say hello Mars right and we'll copy this we'll go back over here make sure that file still there it is great we'll paste this in we'll hit enter we are going to type in cat and then my file text to confirm that is different it is good I'm going to go back over here and we're going to attempt uh a replacement here and hit enter okay so now if we were to go and inspect this it should have versioning so we'll check this again check if file now has a version and we'll go ahead and hit enter and now it has two versions so it has a version of null and this one here so now we have a history of that it should have two versions version ID and then version null okay so now the next thing I would say is that we've listed out the object versions um there's obviously delete markers there's other things we can do with the file so let's go ahead and update it again update again because I'm just trying to push more versions on it so we'll say hello what comes after Mars Mars Jupiter so we'll do this next enter and we'll go back here and we'll push this version and then we will check to see if that one is there now what we could do to make this a little bit nicer enter is I just want to see the um uh the version the version IDs right I don't want to see all that other stuff so I'm just going to go here and type in hyphen hyen um query and then I'll type in versions and then Square um and then I think version ID I always kind of forget how to do queries even though I do them all the time we'll hit enter and there we go that worked and then I'll just say um output text h that's not great table okay so that's a little bit better so I'll take that and copy that that'll make things a lot nicer so go here and we'll just say we'll do that so we can see our versioning uh versions that are happening so my thought is like let's say we want to get the contents of this file what's going to happen if we do that so we'll say um I just want to get the content of it I don't necessarily want to download it but I guess you have to always download it so I think that we could probably pipe it right so let's go ask chbt um I want to use the a CLI to download the contents of a text file and immediately and then and print it to the terminal in one command I'm thinking what it's going to do is pipe it over to to um cat yeah that's what I was thinking but it has that nice little hyphen in there so I wasn't exactly sure how we would do that so that's good that we have that so we go ahead and do this and here we have uh this we'll copy that paste this in here and we'll just say my file.txt and so I'm thinking that will get us the contents of the file so let's see the contents of the latest file and we'll go back over here we'll paste this and hit enter and so it's printing out that contents now we do have this file here so I'm just going to delete this to make sure that this is actually doing what we wanted to do we'll go ahead and enter great so now what I want to do is get a very particular version let's go take a look at the a CLI as I cannot remember if it's just a command on that probably is so we'll go back to ads S3 and we'll look at the copy command we'll see if this is actually available here we'll type in S3 here we'll go over here and what I'm looking for is the copy command and can I specify a version version the region to use display the version of this tool no so I want to get the exact version I don't think we can use the S3 we'll use S3 API so we'll say um copy object the only question is can we pipe this the same way because it might return back Json right um and the other one returns its name so what we really need is is the bucket name and the key together I'm going to go here and say can you instead use S3 API because what it will have to do is query it and do some manipulation right so notice here it says key to path and then it's getting the St out is that really going to work this command will retrieve the specified object text file and the standard output yeah I'm not 100% sure if that's going to work but let's go ahead and take a look here and see what happens does that work oh well we have to obviously update this um note that we cannot use S3 CP to get specific versions of a file because it's a high level S3 API we'll go down here and we'll have to copy this separately I'm very confident that this one has a um a flag for getting the specific versions but you know I assume this will output Json so I assume this this would print out Json and not necessarily the contents of the file but maybe it does work and I'm wrong okay so we do get the contents of the file but we also get um this metad dat here and some additional information that's totally fine um I'm fine with having that additional stuff there's a little bit more than I want so uh here we can print out the contents of the S3 file and get the versioning with S3 API get object and um coming back over to here it also Returns the version ID so notice that when you use get object it it gets it for um for list objects I don't think we get it but we can take a look because we should just double check that here I'm just hitting up till I get list objects somewhere in here there we go so if I hit this do I get version versioning now no we do not so notice that still if you want it in a list view you're not going to get versioning but when you get object you're obviously going to get it so what I want to do here is find out what that versioning tag ID is so it should be somewhere here version nope version ID and uh yeah I'm not having an easy time finding it here it should be version version okay so it's not showing version ID copy object oh well this is copy right and uh there's also get get object okay so retrieves an object from S3 so this might have the parameter you want version okay so this one does what did we use here we did oh we did a get get object I don't know why I said copy object I guess I was just getting confused but anyway if we go to version ID we can specify it here so what we'll do is say this and then we want to get what that one is uh we need to figure out what versions there are so I'm going to go ahead and copy this again we'll paste this in and then we can grab a uh no that's not the command I wanted we'll try this again it's oh object versions list object versions no no it's this one my my clipboard doesn't sometimes copy it's frustrating we'll go ahead and paste hit enter and so we'll get a list of versions let's go grab this version ID we'll go back over here and we're going to go and paste this in our version ID I'm going to go ahead and copy this and I'm going to go back paste it in I'm going to confirm it is what I want it to be since copy and past is sometimes finicky we'll hit enter and notice now it's saying hello Mars which is this version here let's say I want to get this one all right I wonder if we Supply null if it'll give us that because it says it's a version ID and it does okay so that is treated like a version ID that's interesting um so now we have those now imagine that we want to uh uh Delete so that might be the next interesting thing that we can do so let's go ahead and try that out so we'll have S3 or eight of us s 3ss S3 API delete object put bucket and we'll grab this paste that in and we'll grab the key here put enter and see if that deletes it it so it says it's deleted and it returns back the version so we'll go back over to here okay let's delete an object without specifying the version ID and so what happens here is it's going to I think delete the topmost version right and so what we can do is now go back and list our versions I want to get the more verbose one so I'm going to go up to this one here and we'll go back and we'll paste it in because I want to see if we have those delete markers so now it's showing um this one so we have one two three versions and we have a delete marker so for this one it shows that the version here is ca1 VB ZN okay so that's our delete marker I'm going to go up here and see what the delete marker is up here this is a completely different one here I guess the question is what were we getting before so if we go up here this is ftg G4 so notice that this version is still on the top here and it still shows that it has a version ID okay but it has a delete marker still here so now my question is what happens when I go and attempt to request it because it's still showing the version's there so what we'll do is we'll just go ahead and do a get object and hit enter and notice it's returning oh well that's where the the version ID is that so we'll take out the version ID because it's null it's returning specific one but we shouldn't see Jupiter we should see Mars right and so what's happening here it's saying an error occurred no such key when calling the get op object operation the specified key does not exist so that's what we kind of expect because we deleted the object and it's not going to grab the previous version it's going to treat the whole thing as if it's gone so the question is how can we go ahead and restore back our object because we have a few different ways uh that we can do it we can copy a previous version of the object into the same bucket uh we can perally delete the current version of the object so I mean they say the second one you can do but let's take a look here and see if there's anything else in the API that you know we might have missed when we're going through the lecture content so is there anything for markers delete so I'm trying to see here if like I can delete a um marker or delete object versions so no so that's really interesting but let's go figure out how we can restore this so um can we still get the version because we have the version IDs here and yes of course when we fetch the object it's not returning it there but let's see if we can go grab a specific version so I'm just hitting up on the keyboard here and I'm just going to replace that version so I'm just going to copy this here go back so let's attempt to fetch the object so we'll just copy this here and make sure our documentation is all up to date here get object I think this one where are you get object not sure why it's so hard to find but we'll go ahead and grab this one and I'll go down here and I'm just going to take out the version ID well we'll copy this because we'll need it here in a second so say this will return a 403 because the file is deleted and and if we were to go ahead and list all of our versioning get all object versions just say here make note there is a delete marker make note that all versions are still there they're all still there so here what we want to do is we want to see if we can fetch a very specific version the one that we actually deleted so we'll go back over here and the one that we deleted is this top one so I'm going to grab this one here and we'll go back and we'll paste this in here so say let's see if we can get the previous version which we technically deleted okay so go ahead and we'll copy this and we'll see if we can get its contents just as a sanity check I just want to make sure there's no files there good I don't remember deleting it but it's not there so notice we're still getting hell Jupiter so it's still there but we want to bring it back so the way we're going to do that is to use the copy commit so I'm going to go here now the copy command didn't necessarily have the version flag when we say copy object so we say version ID so it doesn't actually have that there so the question is how are we going to copy that specific version when we can't use copy object of course we can use um get object which we did and I suppose what we'd have to do is download it and then upload it again so let's bring back the last version by downloading it and reuploading it AKA copy it back because that's really the only way we're going to be able to do that so we're say get object here and then we'll say put object and will not specify the version say copy and we'll hit enter hit enter and it says error the following commands are required out file uh since when I mean we didn't use it before did we not we've we've been using get object this entire time right get object bucket version ID right key oh I guess here it usually specifies the file okay so the file contents that makes sense so basically we're downloading the file but here we said export the file contents here that makes total sense now so go here and say my file.txt and we'll paste this again hit enter and I'm pasting this in the wrong thing we're supposed to do this in Cloud shell no problem we'll go ahead and hit paste enter and so now we'll do LS we'll check the contents of that file it's there great we'll go back over here we're going to copy this command which is going to put it back up there great and now the next thing we want to do is we want to um look at the versioning again so let's check the versioning the question is is that delete marker still there probably is so say ads S3 list list object versions um that's for the table I want the full version of it without well we don't want table on this so we'll grab this again we'll say let's check the versioning versioning versions again go ahead and copy this and when you are working in the cloud it's really useful to do this and document so you have all your steps because often you're going back as you see I'm going back back constantly looking for that stuff so we have this version here this previous version there so we have four notice the delete marker is still there so I guess the way it must know the order of these things is based on the last modifi date but uh yeah so we're really getting somewhere here with all this stuff there's another feature to S Rion which is obviously object locking so we should take a look at that we should also look at unversioned files um so what we'll want to do is turn on multiactor Authentication as you can see here we can just enable it actually we can enable it in here we have to all do it through the um the CLI so let's go ahead and do that um I believe that we do that through the bucket versioning so just as we did bucket versioning before there's an option for it so let's go look for put bucket versioning because it's not an isolate feature it's just a feature of of this one here in order to enable MFA delete you must be the bucket owner if you're the bucket owner and want to enable MFA delete the bucket version configuration Etc etc etc okay great so go down examples and then this one um should show how to turn it on now this one's saying MFA serial 1 2 3 4 5 six which is great um so our serial number for our MFA so yeah I guess the question is how are we going to set this up because I'm just trying to think here if I can just use the app on my phone so give me two seconds okay all right some basic research and it looks like what we need to do is add a um MFA device to our user and then it'll produce that serial so that's where that's coming from what we'll do is go ahead and look up uh users and I believe it's an I am users because that's where I remember attaching MFA devices I'm am right now Andrew Brown so we'll go into here technically I'm ous examples but uh that's not what I'm logged in uh as right now so what we'll do is we will go take a look at um those devices I assume we're supposed to do it under a user security credentials MFA we'll sign an MFA device and so we have a few options uh if you're looking this up somewhere else it might show it in a um the screenshots will look different obviously but I'm looking for my phone I just had it give me two seconds I got to find my phone there it is okay so so here we have the authenticator app and there's a lot of apps out there Microsoft has one so Microsoft authenticator I don't know if that's what it's called Microsoft is actually really really good the Google has one called Google Authenticator uh there's one called aie I like aie I find it the easiest to use so that's the one I'm using on my phone you have to install something and and make a choice it's up to you what I'm going to do is just put something in here so I'm going to say aie um I mean that's what it is aie so I'll go ahead and just say Andrew or AB for Andrew Brown we'll go ahead and hit next it doesn't like the name so I'll give it a hyphen we'll give it a next and so it says install compatible applications such as Google Authenticator du mobile Ai and they don't list microsofts even though Microsoft is really good so it says open your authenticator show your QR code I'm doing that right now and somewhere in my uh app here once I get into it there should be a place to add it so I'll say add account scan the QR code I'm doing this all off screen you can't see me do this and I'm saying save to my phone and so now what I need to do is H fill in the consecutive numbers so right now it says 8 32 360 and I'm waiting for it to turn over to the next token so you're not entering this in twice you're entering consecutive codes in it's very easy to mess that up and I'm just waiting and waiting and waiting there we go so 141 5 0 we'll go ahead and hit add MFA it should hopefully work and it looks like I've added that MFA and I imagine it's supposed to have a serial number here right so I have this now added and I need to add a serial number so let me figure that out next okay all right so I just want to show you that I'm not crazy that uh there's a reason why I'm having a hard time doing this because when you go to the docs they don't really tell you in the example how you can utilize it with the virtual MFA now I know that you can do it but notice it just shows serial 1 2 3 4 5 6 um if there is more information maybe it's in here like it doesn't really tell you but um if we even look at the uh this here the the examples we go down below just shows you serial not really useful however if we go to repost people are showing that you can turn it on and specify uh this kind of thing over here to say hey turn on MFA uh attached to um a user's account right so notice here it says AR ads IM the account number MFA root account MFA device and then it has pass and so I'm assuming is the sixdigit example code that's generated from the MFA device okay okay great so what I'm thinking we need to do is grab this example this is going to be different for you than it is for me right and because this is such a weird undocumented thing I'm just going to go ahead and copy this okay so we'll turn on MFA create an MFA okay so just going to kind of expand this so I can see what I'm doing it's a little bit too much going on for me and we know this is our bucket here and we want to enable MFA this is already enabled so that's totally fine and we're going to go here and so need this this kind of AR so I know that we can get our account ID right here and we'll paste this in here the next thing is it says root account MFA device uh you know I don't know if that's what it's supposed to be but let's go back over to I and see what we have so it has this we could actually just copy this identifier and I'm hoping that this just works so paste that on in here okay and then on my phone there's those numbers that are changing it's those six numbers you keep on seeing so I'm going to go here and I have to be really quick with this because I only have a few seconds so what I'll do I'm going to wait till it counts down to 30 seconds and enter it in so I'm not so stressed out trying to copy and paste it over 9 seconds 8 seconds 7 Seconds 6 seconds I know you can't see this but it's happening and there we go so we have 660 226 great I'm going to go ahead and copy this and I'm going to try to go as quick as I can over to here I'm going to paste in this multiline command and we'll say paste looks correct to me just double checking we'll hit enter and did it turn on uh I don't know we'll go back here and check the versioning properties still disabled I must have copied this wrong put bucket versioning that's what I did and you know what it took so long that I now have a new one so I'll go here and try another one so we'll say 65 650 319 I'm going to try this again copy right click paste double check 7 Seconds enter maybe I didn't go quick enough last time uh when calling Buck version operation Dev pay and MFA are mutually exclusive authorization methods what what are they talking about there I have no idea what they're saying there let's go double check here Dev pay let's ask Chachi BT um so here what we're getting we're saying to resolve this issue you should check the CI configuration ensure that you're not unintentionally using both Dev pay and MF P what is Dev pay okay what is Dev pay for S3 is that where someone else pays Amazon Dev pay is a simple to use online billing account management service supports applications built on Amazon S3 and ac2 by allowing you to resell applications built on one of those applications I'm definitely not doing that so I'm not sure what they talking about there um mhm so maybe what I'll do here I'll just take out status enabled and let's see if we can just do this because we don't need to flip that twice right I'm just going to simplify this and make this one line and I'm going to try this again so 071 821 I'm not going to be quick enough I'll have to try this again just wait waiting a few seconds here okay 205 005 we'll copy this and we'll go back over here we're going to go ahead and paste the in we'll hit enter what is it talking about give me a moment I'll be back here in a moment all right all right so I remember in our lecture content that we did say that it had to be done via the root account but it's interesting to see that that's not what it returns back so stack over suggesting the error response typically gets returned when API cannot perform the MF delete Tas due to the requests being made with nonroot credentials the only way to turn on MFA delete is to use credentials from the user of the root account so that's okay um the only from the root account can you generate out access credentials from the root account I don't think so at least I don't remember you being able to so that's fine I can go log in as the root account we'll take a look here um but I guess my confusion is like if I'm root account how do I have access keys because I thought that they disabled that Al something thinking of another Cloud which is totally possible so what I'm going to do is sign out here and I'm going to attempt to log in again and this time as the root user so I'm going to go here and sign in as root I'm going to go grab uh that information and let me fill it in just give me just a moment here all right we'll attempt to log in here and we'll make our way over to IM and we'll go to root has no active access Keys okay so I guess we can produce access keys I just couldn't remember at one point if well can we where where do we get our access Keys using access Keys attached to IM user instead of the root user okay but we have to use access Keys here um now of course we don't we can just open up uh Cloud shell which is fine but I'm just curious as to where we would generate the out if we had to what where account settings policy password no STS no well okay I'm not exactly sure where we are supposed to generate the mount at the root user that's okay we don't need to anyway we'll just open up Cloud shell and what I'm going to do here I'm going to first make sure I'm in the same region I am that's good Cloud shells migrated to Amazon 2023 that sounds great and we'll go back over here and we'll try this again and I'll have to be like super quick about it so I have to log in again and I'm going to see if I can show this on the screen here because I actually I am here right so I can show you like it's generating out the codes right I'm using ay so that's what I'm trying to do right here okay we'll get off screen so um yeah anyway let's go get this code in here so this one I have to be again really quick super hard to multitask here while holding a phone and we will do 539 no I'm not going to make it a time I'll have to wait again let's give it a moment here 8 seconds 7 Seconds 6 seconds 5 Seconds 4 Seconds come on let's do it here we go 48 3 6 1 3 great I'm going to copy this as quick as I can bring it over here paste it in enter and it still doesn't work okay what else do you want for me I I I did what you asked 483613 48 3613 now I don't have that other flag in there I'm wonder if that matters so I'm going to go back up to um uh wherever that was I'm going to copy paste this I'm going to enter in the new code I got to again wait for the seconds to elapse again to enter the next one and we will do 423 626 and I have to copy it as quick as I can bring it over here paste it in hit enter still doesn't work what do you want for me I'm doing what you told me to do let me read a little bit more okay all right so this is what I'm thinking cuz it says root account MFA device I wonder if we can attach it to the root account and maybe the one on that user is totally useless so what I'm going to do and again I'm logged in as route you can tell because it doesn't show uh it just shows development up here but um which is the name of this account I'm going to close this out for a second I'm going to go back over here I'm going to go to security credentials I'm going to remove this MFA remove it because that clearly isn't working and somewhere in here I bet I can set MFA um on this on this root account now of course we can do it because we should be able to in account settings somewhere here where the heck is it um dashboard add M fa I can add it right here let's click it there so it takes me to security credentials let's go back over here so I'm looking at left hand side I'm trying to find security credentials security security credentials why doesn't it show up here anywhere so I guess my question is like once I add it can I get back to it um so that's something I'm not 100% sure about but we'll click that there used to be a link the reason I'm so confused there used to be a link to get to it and they've changed it so now we're going to add the MFA device to the root account so I'm going just say aie um and I'm going to use the authenticator app I guess since I do have a screen I I can show you what's happening here if if you really want to see so I'm going in here to my AI account okay so here I'm in aie right and in the top right corner there's those three ellipses so I click the three ellipses I'm going to add an account right and it says scan the QR code so I'm going to click that and in here I'm going to go and go next click the the code I'm scanning the code and it picked it up it wants add it so I go ahead and add it and now I have this stuff here okay so I'm going to have to enter in the sequence so I have 1 37 929 hit tab waiting for the next number here there we go 361 392 we'll hit add MFA and so now what I'm thinking is that this one is at the root level and it's going to give us less problems that is my guess nobody's telling me that so what I'll do is I'll copy this and we'll go back to our code over here and I'm going to paste this here and now I need the next digits for this this one is I going to go quick 95 or 975 7 22 copy this entire line copy right click open I am I don't even have it open darn it paste it in as quick as I can come on paste it in paste it in let's go let's go and it still doesn't work what the heck is going on okay let me do some more investigation all right so this is the same thing we're writing here and the next one says I got the same error when attempting to perform this using the adus cloud shell although I was using the root user and confirmed I was root using the cloud shell the same command worked when run from the local CLI okay so that sounds like that's part of our issue um if this is going to maybe resolve it I'm going to have to get those darn um access keys but the question is where the heck are they so that's my next thing I have to find out so I'll be back here when I find it okay all right okay so those security credentials they're right up here in the top right corner and we go here that is so frustrating that it's up there and not here on the left hand side but okay let's go over here and I mean I guess that makes sense like if you're an existing user that is the way you'd access it for any of them but I totally forgot that that's up there and now we're on here and we can produce access Keys now it doesn't like it when you produce access keys but I'm going to do that anyway because I need this to work and we're going to you have to do this in git pod I'm going to do this in a a separate local developer environment I am generating out access Keys um we need to do this for the root account at least I think we have to of course do not share these keys with anybody I'm going to rotate them out after this video uh your access Keys should be kept secret okay so I'm going to go here and paste this here and then I'm going to go here and paste this here all right the next thing I want to do is I need to figure out what those access key environment variables are access key Nars and we'll go over here and I'm just going to grab this as an example I'm only setting these temporarily in my git pod environment uh if you're using a local de de developer environment these will persist you can also use profiles in a credentials I can't use them here because uh profiles do not work in G pod um but credential files are really really cool to use so what we'll do is just copy this and paste here copy this and paste here this is in Us East one so I'm just going to default this to us East one I'm going to also wrap these in double quotations because if there's anything funny in here it might break break the uh or like cut off part of it and then not work we'll go ahead and copy this I'm going to go ahead and paste this on down below I'm going to do uh adus STS get caller identity and it says I'm typing it wrong so I'll go hit enter there's a um we covered this somewhere but there's a an environment variable you can set to enter that mode and then it'll autocomplete what you're writing so say a S3 or STS get color identity I have a hard time spelling the word identity that's the problem there and so this is saying rout so I'm confirmed that this is now hooked up so that is good um we want to attempt this command again I do not want this stuff to get saved in here so I'm deleting this out and saving this file do not commit that stuff if you can and so now what I need to do is go ahead and uh attempt to enable this again so I need to uh get this code here so 134 1 99 copy and I'm pasting it down below here this time and it worked so the question is would it have worked without a root user if it was still here I don't know it it is suggested you have to use root user so whatever it did work because in one case it said the bucket owner and then the other case it said the root user let's just assume it's the root user but let's go confirm that um MFA delete is turned on so we'll go back over to our um bucket and um let's just go here back to our S3 bucket here and we'll go to buckets the bucket was called this one and we will confirm that mfu delete is turned on which is under properties maybe yeah so it is turned on now so if that's turned on we should not be able to delete buckets right so a few things here is you need to create uh a so you need to attach MFA to root user you will need to generate out uh root credentials you cannot use cloud shell it's so bizarre that you can't use cloud shell for that one case but hey now we know so after this we're going to say let's try and delete our object so we have it somewhere here up here great I'm going to go and paste this down below and uh we'll undo here I'm going to do this via Cloud shell no I guess we don't have to um this is you cannot delete this object unless you are root user is basically what it's supposed to supposed to be so if we copy and paste this right now we use that it's going to work but what I want to do is I want to go attempt to delete it in here let's go ahead and try that I's say delete we'll say delete object and it worked and the problem is that I'm the root user right so of course I can delete that so that's not a really good example so I'm going to go back there and restore this um so let's go ahead and do that by grabbing a line here up here uh let's bring back the object so we will copy this here hit enter and then I'm going to go here and paste it back I'm going to go ahead and delete this file delete permanently and as a sanity check I want to list out the objects I just want to make sure somehow I didn't delete everything okay so our versions are still there we should now have two delet markers we do great and we'll go back over to here and refresh the only way we're going to be able to test this out is I'm going to have to log back into that other account so I'm going to sign out and I'm going to sign back in this one's going to be um developers exam Pro Andrew Brown I got to go grab my password give me a moment to grab that here and we'll grab that here all right and so now we'll go ahead and try to delete it from here we could probably just use the CLI to find out or the cloud shell so that's what I'm going to do I'm going to go ahead and use the cloud shell here M that's fine ignore ignore just type clear so I can see what I'm doing let's go ahead and attempt to delete this from our non root user and we'll go ahead and paste this in here and hit enter whoops paste paste it is not pasting right click copy right click paste there we go hit enter and it says that it deleted the marker okay so maybe it can delete markers it just can't delete the original object so let's go back to our versioning here say S3 and we'll take a look and we will go into here and the object is gone so what does that mean when it says MFA delete are we talking about not able to delete versions or not able to delete the object completely so just give give me a moment okay all right so I think there's a cabit we missed which is permanent deleting objects so if we delete the top level object we cover this in the lecture content but if we want to permit delete a specific version we have to specify that version and it'll get rid of it forever right so there is still stuff there it's version we just don't see it so we can delete the we can add a delete marker but we're not actually permanently deleting versions so what we can do is attempt to do that so let's do that so we'll say um this will actually delete this will create a delete marker uh it will um it doesn't actually delete objects so it's not going to trigger MFA delete right so let's go ahead and now specify a version that we want to delete so if we go here and say let's delete this version I assume it has the same flag version ID here let's specify a version to permanently delete what I want to do is I want to trigger that um I want to trigger that permanently right so we'll go ahead copy this and we'll go ahead and paste it in hit enter it says MFA authentication must be used for this request now um I should specify that down here this is the root user and I tried to delete it with the root user and it failed um if I tried to do it in here it should fail as well right because it's attempting to permanently delete so here it says MFA authentication must be used for this request which is totally fine but the question is can I do this if I'm not the root user and I use MFA I mean I assume I have to so there's probably like a flag to add the MFA key in here so what we'll do is we'll go back over to our um uh our API calls and we'll go up all the way up all the way up and we'll go to S3 API I type in uh Delete object here and what I'm looking for here is um the MFA so let's type in MFA okay and I'm going to assume we just apply the code here so it says the concatenation of the authentication devices serial number uh space and the value that is displayed on your authentication device required to permanently delete the device now the serial is probably the same thing that we used before so if we go back over to here I'm going to just go here and say MFA and we'll grab the same thing and the codee's going to just be different all right so go here and now I just need to make sure I enter in the latest code and hopefully this will work so it says 606 973 so I'm going to try this first over here and I don't expect this to work here because this isn't the root User it's not their MFA device so it still complains right that's totally fine and also that's never worked really before with MFA but let's try it over here in clut shell and it was able to delete the the the object and it returns back the version the question is that object permanently gone that's what we want to go find out here um so if we go and hit up and we list out our versions here this is the version QX notice that it is completely gone and one two three did it add another um delete marker I'm not sure but we'll go back over here we'll give this a refresh okay let's keep trying to delete some here because I want to know is it going to add another delete marker if we attempt to delete I guess we don't have to do that with MFA delete so I think MFA delete is now satisfied so what I'm going to do is go over to our bucket version try to turn this off notice we cannot turn it off from here we have to use the seal I so I'm going to uh go back over here and notice it removes the version permanently okay that's not how you spell permanently probably not but um I go down here and we'll say turn off MFA I imagine the process is going to be very similar if we go here bring this down and we probably just say just disabled and at the same time I wouldn't mind uh turning off um versioning cuz I kind of feel like we're done here so I'm just going to say disabled as well enabled disabled disabled I this has to be suspended and that's what I'm going to do here so now I have 721 1 91 and we'll copy this we'll hit X we'll paste this in we'll hit enter there is this big weird space in the front of it so I'll just take that out I'm going to try to get this in as quick as I can the serial number M code is not valid it's because I it might have changed while I was working with this or I might have entered it in incorrectly so say 785 772 we'll copy this I'll go ahead and paste this in again hit enter and it looks like that's both been turned off we'll go back over here we'll refresh this page here and now it's suspended and it's disabled so those two things have now changed so the next question is now that version is turned off what do objects look like so turn off MFA and versioning now let's go take a look at our objects if we go here and say list object versions let's look at our object versions I should probably look this same right it's just going to be not happening anymore so if we hit enter down here notice we still have our versions everything's still intact right but what's going to happen if we push a new version here so that's the question um I'm going to do this over here via the CLI so we'll go ahead and open this up I'm just going to clear this out and we're going to go back to our code here bring this down here let's put p a new or an updated object on suspended versioning okay and we'll go up here and I'm looking for some code where we might have changed this before here we go here's an example and so we'll go down here and I just want to say hello Jupiter Saturn I will say hello Saturn and I'm I'm going to just apply this here and now I want to see what it looks like when we use this information here hit enter okay and so we still have history 1 two 3 4 question is did it add that as a version notice the version is now null right okay so the latest one is null if we were to push it again hello Saturn hello Neptune would it add another version or would it always just be the last one so we go ahead and do this and we'll go ahead and try this again enter okay and still null so notice that it's just stacking on the last one so yeah that's interesting now the next question is can we delete this bucket so the thing is when you have versioning on I'm not sure if it'll let us delete the bucket I'm pretty sure it doesn't let us let's go ahead and see what happens now we can delete this it will just it won't actually delete the object it will delete just uh that file there but what we'll do is we'll go over to buckets here and we'll see if we can delete it so I want to say uh delete this bucket buckets must to be empty so we'll empty it and we'll attempt this here so we'll say permanently delete to empty the bucket so it says we have emptied the bucket let's see if we can actually delete the bucket now and we'll go back over to our listing buckets delete delete bucket and our bucket is now gone now I don't know why I thought that you can't delete a bucket if it's versioned um but I mean hey we got rid of it so we're in good shape and we've done a lot with S3 versioning so you can see that there's a few nuances there like Cloud shell and the root user and uh you really do have to double triple check to make sure you understand how it works um but uh yeah hopefully that is really clear this was a long one but S3 version is super important uh but yeah there you go let's take a look here at S3 life cycle which allows you to automate the storage class change is archival or deletion of objects let's not confuse this with um S3 intelligent tier class or storage class which is using ml to analyze your files to move them to cheaper storage this does um a bit more it doesn't use ml you just set up your own rules um but uh yeah let's take a look here so it can be used together with versioning it can be applied to both current and previous versions uh the idea that you might have is like let's say you have an object and you know after s days you always want to put in Glacier you could make a rule and move it into Glacier and then maybe you need to hold on to it for a year and so after a year you can set it to delete so that's kind of a use case uh that you could use it for there are two types of actions you have transition actions and expiring actions which we'll talk about um this one kind of is showing that example here where this one is the uh transition and then this one here is the expiring action okay uh so we have like life cycle rule action so you can move current versions of objects between storage classes you can move noncurrent versions of objects between storage classes you can expire current versions of objects you can permanent delete noncurrent versions of objects and you can delete expired objects um uh Delete markers or incomplete multiple uploads so these are really um here these are the transition ones um and then these ones are the delete ones if that makes sense or the uh expiring actions then we have filters that we can apply to say hey we only want to get these particular objects in our bucket we want to apply these rules to so you can filter based on prefix on object tags and on on object size we'll cover this more here in a moment okay let's take a look at transitioning objects so you can Define rules to transition objects from one storage class to another to save on storage costs and S3 has a waterfall model when transitioning between storage classes with S3 life cycles they used to have a really complex um flowchart that was really hard to read and uh somebody came up with this better one so it's really good the only thing you're going to notice that's not on here is um uh S3 uh onezone Express or Express one zone because at the time of this video it's such a new storage class there's no information about if it even works with S3 life cycle so I don't know if if it even does I don't think it's going to show up in the exam so I don't I don't think you have to worry about that but I want to point out that it is missing here because there is no information um so you can move things down uh so that's what you need to remember is that if you can kind of remember the order of these things everything can be transitioned down uh you can't move things back up um not to say that you can't change storage classes to go up but you just can't do it in F the S3 life cycle you can't transition from the following so any storage class to standard so because uh standards at the top you can't go back up as it's suggested there so just remember that you cannot go back to standard in S3 life cycle rules any storage class to reduce redundancy you'll notice that it's not on there I mentioned before that reduce redundancy uh storage is a legacy class and basically you're not supposed to use it so we're not going to have it here in S3 life cycles um you don't have um the uh the trans intelligent tiering or uh to standard IA so you can't go from there to there um which kind of makes sense because again it's lower here right so you wouldn't be able to go up like that so um that still follows the rules of can't going up uh the one zone IIA to intelligent hearing or standard IIA or Glacier instant retrieval so just taking a look here again so we have one zone IIA it can't go from here to here to here that makes sense and then standard IIA or Glacier retrieval so it can't either also go to um uh here as well so you know for the most part it's always just going up in one case apparently it can't go down so um that's a special case because instant retrieval is a little bit different um so that kind of makes sense but uh we should take a look at some other restrictions on how uh objects can can transition because there is a few rules here and does get kind of confusing okay all right let's look at a bunch of finicky rules for transitioning objects um if you're doing your Pros you definitely will want to uh know these if you're doing your Associates not such a big deal but we should still talk about them so the first is large objects so this is where there could be a cost benefit if the file is uh very large and so inabus is recommending if you have S3 standard or S3 standard IIA you can move them to intelligent teering if it's S3 standard you can move it to S3 standard IA or one zone IIA you don't have to do this it's based on uh you figuring out which use case it makes sense for you as intelligent Heering has an additional cost for the the analysis and of course IIA means that uh there's going to be a retrieval fee so you have to kind of uh balance that information there for smaller objects um ones that are 128 kiloby or less these transitions are not allowed um so you can't go from standard I standard standard IA to S3 intelligent tearing or Glacier instant retrieval um there is like an a minimum capacity object uh charge don't it's not 168 for uh Glacier instant retrieval something like 60 kilobytes but for whatever reason you cannot make these transitions uh it makes more sense for S3 standard to S3 standard IIA and one zone IIA because standard IIA and one zone IIA have a minimum capacity charge of 128 uh kilobytes so that one kind of lines up uh why isn't a bit different for the other ones I don't know but you have to remember that um another important thing is that you can filter objects based on object file size so that you're going to make sure that you can have legal transitions um and just set those in the filters now when we're talking about minimum days there are some storage classes that require objects to be in S3 for at least 30 days before you can trans transition to them and for that we're going to have uh from S3 standard to standard IIA so standard IIA wants you to have that if you're transitioning from standard IIA or the intelligent Heering or what whatever else S31 Zone IIA is going to want you to have that but the easy way to remember this is that if it's minimum days it's IIA that expects it to be at least 30 days in your account before you can transition um for a minimum storage charge these are storage classes that have a minimum charge of 30 days so it's not a matter of having it in your account it's just that as soon as you go to these storage classes you're going to get charged for the full 30 days um not upfront but um it's going to stay there you'll get charged over time so let's say we're in standard IIA or one zone IIA um and then we're going to uh these ones they have to stay in in sorry they have to stay in IIA for 30 days right so let's say you move it uh 30 days or less you're going to get charged the full amount right so you can't kind of like skirt it and be like oh I only had it for 10 days and I'll transition it out no you're going to get charged for the 30 days whether you use it or not so uh that's kind of important there um and the reason why is is just because standard IA and and on Zone I have a a a minimum that you have to hold them there for 30 days okay so for expiring objects things are pretty straightforward um it's going to be based on the bucket States so if you have a non version bucket it'll delete the object if it's a version abled bucket it's going to delete it's going to add a delete marker with a unique version which makes sense if it's a version suspended bucket it's going to create a delete marker with a null version ID because you're not using versioning anymore uh there are um some some things about minimum storage duration when deleting so if you attempt to delete something before it's done you're still going to get charged the full full expectation of how long it's supposed to be there so if you delete something in in an IIA uh storage class you'll get charged of 30 days if it's in flexible retrieval 90 days if it's in deep Glacier deep archive it'll be 180 days so there you go here are some configuration considerations you need to know about S3 life cycles the first is propagation delay expect a delay of a few minutes before a configuration for f S3 life cycle rules to take effect disabling or deleting life cycle rules so S3 stops scheduling new objects for deletion or transition after a small delay any objects that were already scheduled are unscheduled and are not deleted or transitioned existing and new objects configuration rules apply to both existing objects and objects that you add later so you don't have to retroactively do anything uh to get them to uh comply with your object rules or your configuration rules for S3 life cycle changes in building so changes in building happen as soon as the life cycle configuration rule is satisfied even if the action is not yet taken so what they're trying to tell you is that there are delays uh with configuration but just assume that when you set them they are happening okay all right let's take a look at the anatomy of a life cycle policy so life cycle policies via the ad can be either XML or Json I put an aster there because I can't find a way to provide provide it via XML um and this is kind of similar to um setting up cores I'm not sure why it's not easily accessible but it suggested that you can still do it uh you can have up to 1,000 rules in your policy and this is what a policy looks like they're not too complicated first you're going to specify unique identifier just so you know which rule it is uh you then have a status of enabled or disabled if you want the rule to be active you can apply filters that's just showing a prefix you could do other filters here like the size of the object and things like that uh then you have your transitions so this is going to be whether we want to move something somewhere so you can see that this rule says every or for uh in 30 days or 30 days move to the storage class standard IA then for expiration we can say after X amount of days it will expire there's of course more little configuration options here but this gives you pretty much a snapshot of how to configure it when we want to use that Json file we're going to provide it to the put bucket life cycle configuration and pass that file along there so there you go hey this is angre brown and in this follow along we're going to take a look here at life cycle policies that's where we can move um an object from one um storage class to another in an automated way it's not AI it's just whatever rules we set up there as per usual we're going to use the it examples repo I already have it open I'm going to go ahead and create my myself a new folder in here called life cycle okay so we'll say life cycle and in here I'm going to go ahead and create myself a readme.md I'm almost certain that we're going to have to create a policy file so I'm going to go in here and say life cycle Json and in our read me we're going to go ahead and create ourselves a new bucket so we'll say it us S3 make bucket S3 colon sl/ life cycle fund ab and I'll put some numbers there so we don't have any conflicts go ahead and it is not copying the way I wanted to copy here today we'll go ahead and hit enter and so now we have a bucket that we can work with the next thing will be to uh create life cycle life cycle policy um configuration so let's go ahead and make our way over to Abus S3 API and I really wish it' point to version two ands if you're out there just get rid of version one so we don't have to look at it anymore nobody's using version one it's so tiring I'm looking for life cycle in here I imagine there's probably some kind of configuration in here for it there it is excellent we're going to go down to examples we have an example excellent and party on Wayne party on I'm making it a reference to Wayne's World because they always say excellent unless that is um Bill and Ted's Bo Bodacious Adventures I get them mixed up no it's it's Wayne's World that's what I'm thinking about anyway so we have some information here um I'm not necessarily thinking that this is the policy that I want but uh we can set up rules we can say whether the rule is enabled um we can have it apply to very specific um stuff so I wanted to apply for everything so just take out this prefix here and I want to move something to IIA after a few days so we'll just say IIA move to IIA and we want this to transition after a certain amount of days so we can say here days and then I can say after 15 days we can move it there and then we can say standard IIA so that's something that we can do because maybe we assume that the first few days we're going to use it a lot and then after that we're not we can even set it to last we'll say 3 days we're not going to be able to observe this because we'd have to wait multiple days I don't want to do that I don't think you want to do that either but you want to make sure you know how to set out these life cycle policies and so here's a very simple one right so move to IIA uh enabled after 3 days so what I'll do is I'll go ahead and I'll type in clear and I'm going to go and uh see if we can utilize this so we'll go ahead and copy this before we do we should probably put our bucket name in here and I'll put paste and then we'll see if we can apply this life cycle rule we'll hit enter and there is a small mistake we didn't get the whole line so I'll try this again it'll hit enter it doesn't like something we'll go up here and see what it's complaining about out I'll hit enter what doesn't it like oh you know what I got a CD into the correct directory so make sure I'm in my life cycle directory here and I'll just sit up and try this again still doesn't like it now what um it has an issue with the days so I go over here I think it wants it this to be an actual integer so we'll change that out says it's malformed it says XML but it takes the Json it probably turns it into XML okay so that's kind of interesting and you can even see like I'm using the rule in Json here because that's examples that were showed to me online this is not of course the example that we're using exactly here but we're doing something very similar so why does it not like that I don't no let's go back over to here and take a look and this one's also using Json as well so clearly it uses Json and not XML um there is a space after here I'm not sure if that's messing it up so we'll just try this again copy right click copy it it can't be because it actually brought in it parsed the Json so it was technically working so there's something here that it doesn't like so did I spell standard standard i a that looks correct to me transitions looks correct to me rules look correct to me so this all looks okay I don't see any issues here um let's go take a look at some policies so I mean this is the one here maybe we have to specify prefix so this one doesn't have a prefix in it but let's go ahead and just like there's nothing other than that we'll go ahead and try this let's take a look and see if that works and then it says the transaction action must be greater than or equal 30 days I assumed it was going to be 30 days and that's fine so we'll go ahead and hit enter and so now it's applied let's go take a look at our bucket and see if it it's applied and we'll go into the first one here and then I assume it's under management and so we have our life cycle rule here right and notice it's transitions to standard IIA we'll click into it we can see after when something's uploaded after 30 days it will then move into here there is no expiration set up there we could also set that on our policy but I think this pretty much shows what we need to do it's very straightforward I'm not even going to bother uploading an object because we're not going to wait here 30 days to find out let's go ahead head and just clean up okay and before we do let's just go take a look at what it looks like to create a new life cycle rule and you can see they have a lot more options here and then you can just specify um them here so if you're not exactly sure what values you can provide here you can fiddle with it before you go ahead and write your Json policy but you know hopefully that is uh easy enough to make a sense of we'll go ahead and remove this bucket and we'll go ahead and hit enter we're all cleaned up we'll see you in the next one okay ciao let's take a look here at S3 transfer acceleration which is a Bucket Level feature that provides fast and secure transfer files over long distances between your end users and your bucket and the way we're going to do this is by utilizing cloudfront Edge locations to quickly enter the Amazon Global Network so we aren't actually directly uploading to uh the standard endpoint we're going to be using a distinct endpoint which is this S3 hyphen accelerate and they have a dual stack version of it of of course use the dual stack one because that supports both ipv4 and IPv6 um when you can now I need to point out that S3 transfer acceleration only supports virtual hosted style requests so we'll need to configure that before we use that buckets cannot contain periods for whatever reason and um it can take up to 20 minutes after uh transfer acceleration is enabled in order for this to take effect so how we're going to use it it's very straightforward what we're going to do is enable transfer acceleration on the bucket so make sure that feature is turned on and then what we can do is globally configure virtual hosted style request so that everything after that um is going to uh utilize that there and then we're going to specify the endpoint URL so there I'm just uh putting the standard endpoint in there and so it will utilize that um during upload if we want to always uh set the accelerated endpoint and not have to supply the endpoint URL every single time we can also globally set that as well uh but there you go so S3 presign URLs provide temporary access to upload or download object data via URLs and they are commonly used to provide access uh to private objects the way we use presign URLs is that we're going to use them programmatically I can't remember if you can make them in the console but the way you would want to use them is using the CLI or very often you're programmatically integrating them into your applications and using the SDK to generate them out so here's a very simple example of one that's going to expire after uh 300 seconds and so once you input that you're going to get back a very long link and that is your presign url you can notice there's a lot of stuff going on in there and that's okay we'll take a look in the next slide um what's going on in this link all right let's take a look at what the presign URL looks like so I've broken it up into multiple lines to make it easier to read as there's a lot going on here as you can see we have the algorithm that is being used used I don't think there's any other kind of signing algorithm but if there was you could swap it out there then we have temporary credentials so these are not Long Live credentials they're obviously short lived because they're going to expire after a certain amount of time the date to which the signature was created uh the expiry in seconds so here it's going to last for 300 seconds um indicates the part of the signing process so here it's suggesting that it's in the host and then we have the generated signature which we're not actually showing here we just have some uh placeholder text but hopefully that makes it really clear on what is going on in this big big URL okay hey this is angrew brown and in this fall along we're going to take a look at using S3 Glacier storage classes um now there is of course S3 Glacier Vault and they don't call it Glacier Vault but I say the word Vault underneath and so this is not what we want to use we want to do everything with an S3 um so I have this repo called a example I'm using G pod you can use cod spaces whatever you like um but I'm going to go ahead and open that up I already have it open up um and what I'll do is create a new folder in here and we're going to call this um Glacier okay and we're going to create a readme file here and get to work so the first thing I want to do is create a bucket now I do want to point out that if you do not want to create a file that you cannot get rid of for 90 days or longer you can just watch along you don't have to do the here but you know the more experience you can get the better we are of course uploading the tiniest file ever so getting any kind of major charge is very unlikely but you need to manage uh the money uh yourself in the risk that you have with your credit card okay so we have adus S3 uh make bucket S3 Co SL I'm going to type in S3 Glacier fun ab and put some numbers here AB for my initials random numbers so that we don't have naming conflicts and so there we go we got an object here so what I want to do is create a new object so I'm going to just say hello.txt you know the routine probably by now hello world and we'll just Echo that out I'm going to make sure that I'm in the correct directory here and we'll go ahead and create our file and then I'm going to type in a S3 copy hello.txt and this is going to go to this up here we'll paste that in it did not paste in of course it doesn't why would it want to do what I want it to do so we'll go ahead and do that and so that will copy to there but we'll have to provide a storage class let's make our way over to the ads S3 CLI and see uh for the copy command I just clicked on CP there um for the storage class name so if we go down here you can see have Glacier deep archive Glacier IR I like how this is 1.3 and they still reference it in here I'm going to go to two and we'll go down to storage class and uh let's just clar F read this grant specific permissions to individual users or groups oh that's for the next part here so there's Glacier and deep archive Glacier or sorry deep archive is obviously going to be longer um Glacier is going to be for um flexible and IR I assume is instant but I'm not 100% sure on that yeah it has to be because it's instant retrieval so um instant retrieval works just like every other class so I don't feel like that is interesting to do but Glacier is interesting because uh you have additional work to restore it so if you put in IR all you got to do is change the storage class but these two flexible and deep archive they're kind of tied to vault in some weird way um and so that's what I want to use so I'm going to go ahead and use Glacier here as a storage class and again if you don't want to do this don't do it just fall just watch and and learn okay we'll go ahead and copy this I'm going to go ahead and paste this and see if I can place that in the bucket it's in there now let's go take a look at that bucket going to give us a refresh for our buckets we're in here for Glacier we'll look for hello if we scroll on down uh we can see that it says formally Glacier so Glacier flexible retrieval so let's go take a look and see if we can see the object and it is right there we can see it so they say that when it's there we can't easily access the file right um so my thought is like let's just copy the file I'm going to go ahead and delete this right and I'm going to go ahead and say create S3 create a bucket with an S3 Glacier file let's see if we can copy the file this is my expectation is that I can't copy it watch it's going to let me copy and I'm going to be confused so we're going to go here and just uh delete this part out here and I'm going to go ahead and type in hello.txt and we'll go ahead and copy this and we're going to paste and see what happens copy paste enter oh you know what we have to specify the file just uh go here and type in hello.txt and we will copy and hit enter um I mean that looks right to me like how we normally would copy paste we'll hit enter a S3 CP oh this should lower case I'm not sure why that capitalized here we'll try this again hit enter says object is of storage class Glacier unable to perform download operations on Glacier objects you must restore the object to be able to perform the operation so that's what we're going to need to do we're going to need to restore it so what I'm going to do is go ahead and uh restore I need to go to the S3 API operations here so we'll click on the logo typee in S3 API we'll click here I'm looking for ReStore and so we have restore object I'm going to go to examples we'll grab this one this one looks good and we'll go down below here this should fail because you have to restore the object and then I'll go ahead and paste this in here okay and so here this is going to be restore the the object and we'll go grab our bucket name and we will go grab our well key name we'll just write it in here and so now we have to say the amount of days that we want to restore for can I just restore it for one day can I do that we'll copy that we'll paste it hit enter and so the idea is that we've restored the object um I guess my question is is like well I guess what happens if we do this we refresh here how do we know that it's restored oh it says here Restoration in progress to upgrade the speed of your restoration while in progress choose upgrade retrieval tier and we didn't actually specify the retrieval tier normally we can specify what it is um I'm pretty sure we show that somewhere here if we I'm just going to look for that um in the slide so we can just pull that up in the slide I didn't think about that because now we're going to have to wait three to five hours and that's not that fun um I'm just looking for it here uh the slides I just can't spell the word archive archive archive okay so just give me a second to find the slides here we go okay so notice here like we have restore requests and we only specified the day we could have specified those Glacier job parameters and changed the tier of it and because I chose standard now I have to wait 3 to 5 hours if we had chose a faster one it become faster it looks like we could actually upgrade it um which I'm really surprised that we can do that so I'm going to go ahead and choose 1 to 5 minutes apparently we can have provision capacity units uh helps to ensure that your retrieval capacity for the expedi retrieval when you need it each unit of capacity provides at least expediated retrievals I didn't know there was uh PCU in here that's kind of cool I'm going to go ahead and say initialize Restore for one day and I'm going to go ahead and hit initial restore and so now it should restore a lot faster so let's go back into that object and we'll click into it this object is stored in Glacier flexible storage class in order actually you must restore it first okay and it says Restoration in progress 1 to 5 minutes so now I don't have to wait as long and I'll I'll see you in a bit okay all right so after waiting a few minutes and I did have to refresh here it's saying the restoration is complete so now let's go take a look and see if we can access that file um so let's go ahead here and what I'm going to do is I again I don't think we can do a direct copy so we we'll try that for fun and see what happens we'll hit enter and we actually were able to download it so it's interesting because when uh when I looked up the instructions they said you had to make a copy of the copy um and again this is from the aabus documentation I'm not making this up so in practice maybe you still should make a copy of it um or maybe the CI is uh doing that but the thing is I mean in a sense we are copying it right we're making a copy from here to my local one so um technically that one is satisfied I think actually now that now that I'm thinking about it that still makes sense um so we are really performing that second one there but uh yeah so there we go I guess the real question is can we go ahead and delete all this stuff so what I want to do is see if I can delete this normally when you put things in Glacier they have to stay there for a period of time and if we can get it out of there we're going to have to pay for it for the duration that is specified if it's 90 days 120 days whatever it is whatever we said in the uh lecture slides um but let's go ahead and see if we can delete this bucket so the first thing will be to empty the bucket and we'll go ahead and try that so I'm going to go here and uh go back to buckets and then go to here and say empty and we'll say S3 or sorry uh permanently delete and see if we can empty that bucket and it did empty it so that's good the next thing I want to do is just go ahead and delete it and there you go what we didn't show you here was restoring for um S3 intelligent tearing it would just take forever to get it into that tier I don't know if there's any fast way to move into that uh archival uh tier in intelligent tearing but you know this is sufficient uh for the flexible or sorry the instant retrieval it's as suggested instant you don't have to do this restore object um but yeah uh there you go so we'll see you in the next one ciao S3 access points simplify managing data access set scale for shared data sets in S3 so access points are name Network endpoints that are attached to buckets that you can use to perform S3 object operations like get and put each access uh Point has a distinct permissions via the access point policy distinct network controls distinct block Public Access and you can access these uh endpoints two different ways over the Internet or through a very specific VPC um these S3 access uh Point policies allow you to write permissions for buckets uh for uh for a bucket alongside your bucket policy and if you're looking at that going that looks a lot like a bucket policy because it basically is uh the whole point of uh access points is to basically simplify the complexity of your bucket policy by moving a lot of the specific rules out into uh these access points points um so that you don't have to manage the giant bucket policy and you have these uh specific overrides just to get kind of a visualization here um here's an example of an access point that is accessing very specific files in the bucket from the internet and then you have uh let's say um containers or ec2 instances that are sitting in a VPC and they have a secure connection to um access points that are accessing very specific files so hopefully that really makes it clear okay multi region access points is a global endpoint to Route requests to multiple buckets residing in different regions so imagine you have two buckets and uh you have bidirectional replication on them you can have whatever replication you want on them but usually that's a good pairing with this and you want to uh serve data uh that is going to provide the lowest latency and so we have this a customer that's from the the us but to make this diagram work because I made a mistake and I shouldn't have called Us East one I should have called it some other place let's just pretend that this person lives over the Border uh like on the border of Canada and the town across from as Montreal it would make more sense to Route them to ca Central that's where that is in Montreal than to us East one which is in North Virginia so that's what this can facilitate if this sounds a lot like abis Global accelerator that's because it is underneath that is what it's using uh if you're not familiar with a global accelerator which we cover in uh networking sections it basically is going to uh route things to the closest location and so that is what it's utilizing underneath and uh it's very common to use S replication rules alongside with multi region access points uh because you generally are trying to give a copy of data um somewhere else and so bidirectional or cross region replication is what you'll normally use with this okay S3 object Lambda access points allows you to transform the output requests of S3 objects when you want to present data differently and I should clarify that yes it's doing Transformations but the original objects in the buckets are going to remain unmodified why would you want to do this uh a very common use case would be you need to give a uh data sample set of production data or real data to um a developer or someone in your um data science team but you don't want them to see the personally identifiable information or sensitive information and so you can scrub that information out before they get it so that would be a use case for that um S3 object Lambda can be performed on on the S3 operations of head of get and of list and then obviously list objects version two those are just two different API endpoints because for whatever reason um it us needed to make a a tweak um to list objects and so they had a second version called version two um the idea of this is that you'll need to have an object Lambda access point which will attach the Lambda function to your S3 bucket multiple Transformations can be configured uh per object Lambda access point so you can do a lot here just to get a quick visualization um just so you understand all the parts that are going on here you have your bucket you have your S3 object Lambda which attaches or uh attaches the Lambda and then you have the object Lambda access point which I believe goes through the object Lambda to get to your S3 bucket but it makes a lot more sense when we do it uh for real but uh yeah there you go let's take a look at mountpoint for Amazon S3 which allows you to mount an S3 bucket to your Linux local file system so mountpoint is an open source client that you install on your Linux OS and provides High throughput access to objects with basic file system operations so Mount points can read files up to 5 terabytes in size list and read existing files create new files M Points cannot and does not modify existing files delete directories uh support symbolic links support file locking so it's uh again it's very uh simplistic in terms of the type of access you're getting here um but it does give you some uh robustness there uh you know working with a Linux file system which is nice U it can be used with the following storage classes I'm actually surpris reduced uh reduced redundancy storage is on there because it's a legacy one but I guess we could just kind of ignore that one what I don't know is about um uh S3 uh S3 Express one zone because there's no information about it because at the time of this video um that that's a new storage class and so we don't have information on that I would think that you it would let you do that but it does have a special bucket type so I'm not sure um but not too worried about that mount point is ideal for apps that don't need all features of a shared file system and posix style permissions but requires S3's elastic throughput to read and write large S3 data sets so how are we going to use this well the first thing is we're going to install a mountpoint and this is obviously going to vary based on what kind of Linux you're using so this one is going to be based on um using the red hat pack package manager so whichever um OS flavor supports that is going to utilize that like red hat or Cent OS uh and the way we're going to use the mount point is we're going to create a directory and this is per line here so we are creating our directory here we're going to mount it to that uh that new folder that we've created we're going to go into that folder we're going to perform whatever operations those basic operations we want to do when we're done we will unmount it and that's it so there you go let's talk about archived objects so these are rarely access objects in Amazon S3 that cannot be accessed in real time in exchange for reduced storage cost I want to emphasize cannot be access in real time because I did not list all the glacier um storage classes here because we're talking about ones that take time to retrieve so there are two ways to Archive objects archive storage classes archive access tiers so you want to use archive storage access when you know your access patterns when it requires manual intervention to move data or you're okay with doing that because you can programmatically do it uh and you are looking for the lowest cost possible uh to do archiving so two of the things that I'm showing here is S3 Glacier flexible retrieval so that's minutes to hours and there's S3 Glacier deep archive which is plus 12 hours there is S3 Glacier instant retrieval and I didn't listen to here because the docs when they talk about it they don't they don't talk about it for some reason so it is an option but I didn't put it here because it didn't reflect that on the docs but it is technically an archive uh option and it's going to allow you to retrieve things instantly on the other side we have archive access tiers and this is when you don't know your access patterns when you want things to automatically move because you don't have those programmatic skills or other skills to doing that you want it to be nice and simple um and you're okay with living with uh a slightly higher cost for arive archive storage costs and this is when we're using the S3 intelligent tiering uh service for either archive AIS tier so that's within within minutes or the S3 intelligent tiering deep archive axis which is 12 plus hours so remember that S3 intelligent tiering has an additional cost to it because it's it's analyzing your files to determine where to place them and so that is what they're probably talking about when they're saying that's slightly higher cost for archive storage classes and remember that S3 Glacier instant retrieval is still an option here but again I'm just following what the documentation is showing us maybe they're just out of date and and I'm I'm a bit ahead of them okay let's take a look at how we restore archived objects so archived objects are not immediately accessible for the following archived storage uh classes and that will be for the S3 Glacier flexible retrieval the Deep archive and the two S3 intelligent tiering archive and deep archive access tiers that's because these all tie back uh to S3 Glacier Vault um and you know it requires uh some effort here into getting those objects out whereas instant retrieval is a new type of uh Glacier storage archive class and does not rely on the same kind of underlying Technologies so let's take a look at the first two which is how do we get an archive object out of S3 Glacier flexible retrieval or deep archive and the way this is going to work is we're going to make a temporary copy using the restore object functionality here so you see we specify restore object and then we're specifying the amount of days and you know if we're looking to use a particular tier like standard expediated or bulk depending on which one we want we can specify it there if you don't specify it it's going to use standard by default and so the way it works is that you're making a copy of the object S3 and it's going to be temporarily stored for an x amount of days in there and then if you want to keep it permanently what you'll have to do is make a the copy of that copy so when you restore an archived object for M Glacier you pay both the archived object and the copy that you uh restore temporarily why it works that way I don't know it's just the way it works let's take a look at restoring archived objects for the S3 intelligent teering archive and deep archive options so the first thing we're going to do is do a restore object but the difference here is that we are going to choose uh to not supply any options to it and that's going to just simply move it back to S3 okay so it'll be back in S3 and that's it so um the only thing is that you'll have to wait the duration for whatever it's expected for those tiers so notice we're not setting anything like how fast we want it back it's just going to come back when it comes back and it's as simple as that so there you go let's take a look at requesters pay this is a bucket option that allows the bucket owner to offset specific S3 cost the requestor that's the person downloading the data so as the bucket owner you're still paying for the data storage fees but the requestor is now paying for uh the per request fees and the cost to download so the transfer fees so when you want to share data but not incur the charges associated with others accessing the data examples of where you might use this maybe for collaborative projects where you want the partners to pay maybe it's client data storage where you want the clients to pay maybe it's shared educational resources where you uh where the institution doesn't want to pay the people the researchers have to pay um or maybe you're Distributing content and you want again the customers to pay so uh mostly just when you want other people to pay um the way it works is you just turn it on anytime you like it's really easy turn an on or off all requests must be now authenticated involving the requestor pays bucket uh requesters assume an IM rule before making their requests so we'll have an S3 request paay condition an ninous access to the bucket is not allowed on buckets with requesters pay which makes sense uh since you want them to pay to access the bucket and so uh the a account of the requestor will be charged so if we look here closely um we have a a a policy here and so we're saying on the get andp put for this bucket on all objects and then we're just setting the uh requestor pay to be requestor so that we know that it's set for that so yeah there you go let's talk about requesters pay header so every uh requester needs to pass along this X Amazon requestor uh pair in their request header so that's for delete get head post and put requests or is a parameter in a rest request so here's an example of a curl command and we're just pretending that we could do this with curl we wouldn't normally do it with curl and we're just passing along it as a header um but generally when you uh uh use the a CLI or SDK it's going to automatically insert it for you so notice here I'm I'm copying I'm doing pay request request here so that would automatically uh insert this header here um same thing with uh the SDK where we're specifying it there so hopefully that is clear how we're going to do that let's take a look here at requesters pay troubleshooting so these are some things you need to consider with the requesters pay uh if you run into uh most issues you're going to end up with a 403 which is a forbidden request I just want you to know that so that you know if you get an exam question that says hey what happens when you run into an error with requestor pay it's a 403 so this would happen when the requestor doesn't include the parameter X Amazon request or payer uh the request authentication fails so something's wrong with the IM roll or IM policy the request is an ominous which it cannot be it has to uh not allow that uh the request is a soap request soap requests are not allowed with requesters pay when turned on and most of the time these days we don't use soap soap is an older uh should be protocol or pattern for accessing data and it's just not used anymore so in the case a requestor forgets to include the header then a 40 through will occur no charge will occur to the requester no charge will occur to the bucket owner so there you go let's talk about the aabus marketplace for S3 so the adus marketplace for S3 provides alternatives to adus services that or they work with Amazon S3 to enhance the experience and I just want want to kind of get you exposure to some thirdparty uh providers so that you have an idea of what is out there at least by name so let's break it down based on category so for storage and back and Recovery we have services like FS FSX uh for luster tape Gateway file Gateway and so third party providers here could be VH backup for AWS drva for AWS backup and Disaster Recovery we always always hear about vhim all over the place um I imagine it's probably multicloud uh I haven't really had much experience with VH because I haven't had a use case for it but I hear about it all the time so vhm is one name you'll want to remember uh data integration analytics transfer family data sync and Athena so we have chaos search logs IO uh bite flow Enterprise um addition so this could be analyzing data it also could just be log collection so chaos search um allows you to search across your S3 logs extremely efficiently uh it's way more cheaper than a lot of other providers like data dog and things like that so definitely a lot of options out there for observability monitoring like cloud trail and Cloud watch you could use something like data dog blunk or Dino Trace those are the three that I know that are very popular for security threat detection we normally use gardi or Macy but um you know sometimes the rule sets in these things or the amount of security controls are not as powerful as we'd like them to be so you could use Trend Cloud 1 or Insight DB by or Dr uh uh for Rapid 7 or the VM series vir virtual Next Generation firewalls by Paulo Alto networks cuz they've made that name any longer for permissions we have IM Im so instead you could use one login Workforce identity file cloud efss or yaren 3 server I think Yen 3 server is just like a UI to uh access um S3 in a more convenient way One login is obviously an identity service so it's more like if you're already using that it can integrate with it but yeah there's a lot of things in the marketplace to enhance the S3 experience and I just wanted to get you a bit of exposure with some of those third party names S3 batch operations performs large scale operations on Amazon S3 objects and when we're talking about large scale we're talking about exhibites of data so what kind of operations can we perform uh quite a few we can do copies we can invoke ads Lambda functions we can replace all object tags uh replace ACLS we can restore from S3 Glacier we can do object lock retention we can put object lock legal holds on it so a lot of stuff in order to perform a batch operation you need to provide a list of objects in an S3 or Supply an S3 inventory report manifest file so having a manifest file is really useful um or having the SV file is useful as well so you can have batch operations generated a completion report to audit the outcome of bulk operations as as far as I'm aware of this still does not have um uh any functionality with ad I or the SDK so if you want to use this functionality it's 100% click Ops but there you go hey this is angre brown in this video we'll take a look at S3 batch operations and we're just going to perform a simple batch operation as per usual I have the a examples repo I already have it open in my cloud developer environment which is G pod I'm going to go ahead and create a new folder here we're going to call this one batch operations now I got to point out that batch operations can only be performed uh through the console at least at this point in time so we're just using this to quickly create our buckets so say create to buckets and um I mean let's just make one bucket for now and then we'll go explore uh batch operations it's say us S3 MB make bucket s34 slash uh batch fun ab and I'll just put some numbers in here and we'll go ahead and create a working bucket so hold on here copy paste enter there we go going to make my way over to S3 and on the left hand side do we see batch operations here it is we'll click into it and create a job and so there's a few options we have S3 inventory report so we can uh create a report of inventory we can uh produce a CSV of the Manifest oh sorry in order to apply the batch operations we need to First Supply it a formatted file for it to iterate over right and then we can perform some kind of operation so probably a good idea would be to utilize something like S3 inventory which I don't think I did in a previous video but I guess we can go ahead and do that now so for that to work we'll have to create uh several files um we'll just go here and make a b script so we'll say in chat GPT create um TW uh like 20 different files of various sizes between one BTE and uh you know two five like a thousand bytes in a folder via A bash script okay and we'll just let write that out there really quickly make a directory create 20 files yeah that makes sense it's going to randomize a value of size it's going to use DD which we've used before to make a file size when we did uh the multiart upload so that works out really well it's going to Echo out and say let create those files so pretty straightforward that's a very simple script I'm going to go ahead and copy that if you are following along you don't have chat PT okay you can just go into the repo and grab that file so create files and I'm just going to make a folder called bin we'll place that in here we'll say move and I'll just paste the script in here this is totally valid um we'll just make a new folder in here I'm just going to say files and uh I just want to make a new file here called keep I'm just going to drag this one out here so I can get the path and I'm going to copy this I'm going to go over here and paste in our directory and it looks yeah it'll create the file that way so that's really good I don't want to keep these files so I'm going to go ahead and just create myself a get ignore and I'm just going to say files file underscore like that so that way it won't keep any of those files and yeah that should be good so I'll CD into our batch operations I'm going to chamod the with up plus X for the bin create file so we can execute it we'll go ahead and we'll type in bin create files so that has created 20 files here that is great we need now to sync those files so we'll go ahead and go over to here and say say sync created files so I'll say adabs S3 sync U files file or maybe files like that and then I think we can just specify the path like this uh sorry the um the bucket here I'm just guessing I don't do sync every day but I'm going to assume that this will work and we'll copy this copying and pasing is super super hard in the browser and I don't know if it likes that so I'm going to just type in clear here and we'll look up adus S3 sync and we'll go down examples so I mean that looks fine objects to sync a local directory um it'd be nice if they showed examples of a sub directory I don't know why they don't do that I know we've done it before maybe if I just do this it will sync it maybe we don't need the aster there we go okay great so that's uh uploaded all the files there let's go ahead and do an inventory let's see if we can do that VI this uh CLI I'm not sure if we can so we'll go to version two and all the way to us here and we'll say S3 API inventory it looks like we can do an inventory so that's good we'll go down to examples um to send an inventory configuration of a bucket uh this one has no output this one has output not sure why that matters we'll go ahead and copy that so we'll say create an inventory we'll go ahead and paste that in my only thought is like how soon do we get our inventory is it immediate is it often so it S3 inventory we covered this in the course it's usually like after 48 hours or something right so if we go here we say inventory I feel like it happens after a certain amount of time and we have to also specify destination bucket this might be one of those things we got to just set up and and come back for later let's me take a look at our slides here because I'm almost certain that it's not going to be instantaneous looking for this here delivered within 48 hours if we do daily and the first one will be delivered in 48 hours so it's not going to be something that we're going to get instantaneous so maybe we don't want to do this because we just want to uh perform our batch operation so instead what we can do is just create a CSV file so create a CSV file and um let's take a look at what format it wants for the batch operations so we'll go back over to here and if we go to CSV CSV format is either two or three columns in the following order bucket name object key and option version ID so we really only need these two values here and the way this is going to look is we're going to have this I guess this is great if we're doing buckets across multiple uh or objects across multiple buckets so that's kind of cool we go ahead and paste that on in here like this and we have what 20 files so 1 2 3 4 5 6 7 8 9 10 11 12 you get the idea we'll go all the way down and I'm going to go to the end here you're going have to figure this out however you want to do it but I'm using um a vertical a vertical way of doing this and it is not it's not working so I might have to do this the slow way just like you would we'll say file one okay I'll go down to the ground here and we'll try this we'll say 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 all right so that's all it takes to make a CSV file it's not super complicated so I'm going to copy this and we'll just say file manifest CSV and we'll paste this in that looks good to me I'm happy with that um and so we need to upload this file so we'll go back over to here and I'm going to sync it again because I want that to be in there as well whoops we type clear here so we'll try this again I hit enter the only thing here that we're missing is the the folder did it upload it into a folder or just like Loosey Goosey let's go find out this was in our batch bucket batch down here no they're all loose in here okay so our everything's fine in here that's fine um so yeah we can just copy this contents here yeah it's already pasted in here great and now we can sync that file so that file has now been synced and so I'm just going to grab this path here and we're going to go back to our batch operations and create a job and we're going to say CSV and we're going to paste it in here as such we are not using versioning so we don't have to do that looks like we could view it if we click it here I guess that's it downloading to confirm that we actually got it it's saying zero bytes which is uh not a good indicator I'm going to try to open this up in Excel on my local computer I just want to see that it was able to uh get that information and it's showing nothing so that's uh that's not a good indicator I mean I saved this file did I not save file edit save let's try this again it uploaded it again so maybe um this file wasn't actually saved and my hock Keys weren't working sometimes that happens in Cloud devel developer environments you have to watch out for that I'm going to open this up again and let's see if Excel shows me that I have something it's still empty this is frustrating so I don't like this um what I'm going to do I'm going to delete uh this file here that's all there is to see V there's nothing special about it like it's just uh comma deiminated but uh cuz I don't trust it I'm going to open uh again this Excel spreadsheet that I downloaded and what I'm going to do instead is I'm going to go ahead and just copy the contents here and just assemble assemble an Excel you can do this in Google Google uh sheets whatever you want this is what I have so I'm using that one two 3 4 five 6 seven eight no you get the idea file one oops go all the way to the Top This is two this is three this is four five six I'll speed this up okay now one thing it didn't say if we had to do or not was whether we actually had to specify um uh the headings so it says the CSP must be formatted into columns in the following order so maybe it's not supposed to have any headings which is totally fine um so I'm going to leave them out in this one I'm saving this locally and I'm going to just try to upload this and see if I get less problems um I'm going to manually do it I usually don't do that but uh since we've been having so much trouble here I'm going to do that so we'll go over to our bucket and I'm going to go into batch fun I'm going to upload this directly error occurred not sure why we'll hit upload we'll ignore this error I'm going to go back over to um our batch operations we're going to have to try uh try this again so that it can load that file in and I'm going to go browse S3 and I'm going to go down below here and choose our batch folder right here and I'm going to choose that CSV which I don't see I have to drop down to S3 or sorry CSV here we'll try this again so batch and next and then there's our manifest file so we'll choose that path and I'm going to view it again I'm going to hope this time oh it actually went to a file okay so then it must exist the other thing maybe when it uploaded it didn't know what content type it was I'm not sure why it messed up but we'll go ahead and hit next so now we have some options we have copy invoke adus Lambda function replace all object tags a lot of fun stuff that we can do uh this is kind of interesting Lambda is a compute service that lets you do Etc so I imagine that what this would do is it would iterate through each one and you could process the function that way and that's kind of cool I like that um but I'm just going to go ahead and replace all object tags and just say hello world we'll go ahead hit next and we can have a completion report I'm going to place that in the same bucket as we've been working in so far everything else looks fine we're going to go ahead and hit next it really wants an IM roll um choose an iron roll with the required access permissions and the trust relationship why can't I just have one here um do I really have to create one that's not fun fine okay I'll go I guess I'll go make one I was just hoping that it would have like a single button press a lot of times the services will just like hey press the button you can create it but for whatever reason they didn't do that there they want us to work hard to be able to make this one so I'm going to go ahead here and create a new policy we'll call this an batch policy I guess this is for S3 and and this is for object tagging I guess I don't want to figure out what it is I'm just going to say all and I'm going to say all you're not supposed to do that but it's the easiest way to get this done S3 batch operation because you really should say only for put and get and object tagging because you need a couple things you need to be able to read the objects you need to be able to get objects on it we'll just give it full access and I mean hold on here because we actually had all the permissions here I just got kind of caught up with the um policy Creator but we're going to go back there and actually do our own custom one if it'll ever load and it's not so we're just going to hit continue and try this again so we'll say create policy okay uh I'll be back here when my internet is working again so just give me a moment okay okay okay let's see if this works now there we go I'm going to go over to Json and then in here we'll go back over to um the batch operation and here is the policy templates I'm going to copy it I'm going to paste it in here and we have a couple things Target resource the Manifest file and that so this is all in the same bucket so we'll go ahead and copy this I know that's really hard to read I'll just zoom in a bit here it's just the zoom doesn't work really well in OS uh they didn't design it to work that great um and so that looks good and we'll hit next and we'll say S3 batch policy fun so now this is a lot more limited right and it does exactly what we want it to do and then I'm going to go back into that batch policy there because we should be able to change its trust policy so if we go back over to here this is what we would oh sorry that's for the rolles right okay so we created our policy now let's go ahead and create our rle and we'll just say here um custom trust policy and then we go ahead and grab this we'll paste that in and we'll hit hit next and then we'll choose batch and we'll checkbox that on and we'll go next and we'll just say um S3 batch operations fun go ahead and create this role we'll go back over to here it's probably the most click Ops you've seen in a while as we seem to avoid click Ops as much as we can I'm going to refresh this so it shows up I'm searching for it there it is we'll go ahead and hit next after you choose create job the batch operation will start we'll go ahead and hit create job and we'll give this a refresh and it failed that was fast why did it fail failed to parse the task for manifest at bite offset zero unexpected number of task Fields expected two observed one um okay I mean we have two right here so I'm not sure what the problem is but give me a moment okay so here this person suggesting that we can't just Supply the CSV normally it must be you're all encoded so that character would get converted to this here um why it's like that I have no idea uh down below there appears to be a little bit of code in order to do that using the ulb 3 Library so we are going to need to modify this file which is really silly um but we'll go back over to here and I guess I'll just upload it in place I'm going to go ahead and delete this one and we will drag in our other one here so I'll drag this one in this is from my desktop and notice this one's actually not even using a comma it's using um a tab so maybe that's our problem first so what I'm going to do here is go back I'm just going to delete this file here because it says you can't use a comma but I don't know if I trust that so what I'm going to do is I'm going to go and see if I can save the format out in a different way and I'm going to say save as and I'll just save this to my desktop oh look it's defaulted as eliminated even though it says CSV which makes no sense and so I'm going to go with the um Mac or sorry the windows version I suppose I'm going to save that to my desktop if I can figure out how to do that here um usually there'll be like browse locally oh here we go okay so we'll go here and I'll just say manifest CSV and we'll save that that and what I'll do is I'll go grit that from my desktop and I'll drag it into here we'll take a look at what that looks like so this one actually now has a comma okay so I'm going to go back over to uh this folder here I'm going to upload that from my desktop okay and so now this one has a comma Now it still might not work as this one's suggesting that it needs to be URL in code again I don't know if that's really true but I'm going to clone this job it's going to have all the same settings and we'll go ahead and hit next and that will be the same we'll hit next and that will be the same and we'll hit next and we will try this again and see if it works you're waiting your confirmation to run so we'll click into it I guess we have to confirm it so I guess it's just saying hey the job is in good shape we can go ahead and run it now let's go and run it and it looks like it is running review the number of objects list in your manifest and the pricing information Etc uh we're doing very basic stuff here so I can't can't expect to see any cost here oh we go down below and then confirm it thought we ran the job already and maybe it's running now we'll give it a refresh here and notice it has an active time not sure how long it takes to uh run oh looks like it's completing it now 100% done I'm just going to pause and I'll get back here when it's done okay all right look it's looks like it's all done here let's go take a look and see if our object tags have been applied so if we go into the uh batch fund bucket here and we click into any of these files we should see our tag and our tag is applied so we are in good shape here now we are left to clean up so I'm going to do this all through the console which is a little bit unusual for us but we're going to do that anyway we'll go ahead and click delete it'll first want us to empty the bucket so we'll go here and I'll say permit delete and then the next step will be to delete the buckets we'll go back over to buckets we'll go down here to the batch fun bucket we'll go ahead and delete the bucket and there you go see you the next one Amazon S3 inventory takes inventory of objects in an S3 bucket on a repeating schedule so you have an audit history of object changes so S3 will output the inventory into the destination bucket of another S3 bucket you can specify a bunch of metadata that you want to be exported with it you can specify a frequency of every single day which will be delivered within 48 hours weekly which you'll get the first report in 48 hours and and future reports every Sunday your output formats a CSV uh or an orc file or a parquette file the inventory scope here you can specify prefixes to filter objects you can specify all or or uh only current versions if you're using S3 versioning and as we saw in the previous slide um the the Manifest file the output of this stuff can be used with S3 batch operations to make your life a lot easier uh it's very straightforward service but there you go Amazon S3 select lets you use structure query languages to filter the contents of S3 objects so the idea here is let's say we have a CSV file and we want to get uh particular information from it we can use something that looks like well it is it is an SQL expression in order to grab data uh from that file it only operates on a single file it's going to work with data formats as CSV Json or parat so those are all kind of structured data so that you can use the structure Crea language against it will also work on um objects that are compressed so if they're in a gzip or a bzip 2 uh it'll work for those as well but only for csvs or Json objects it works on objects that are serers side encrypted as well and you can get the results back in Json or CSV um this feature is supported for very specific storage classes reduced redundancy storage they're obviously never going to support that because that is a legacy class um for S3 Glacier flexible and uh deep archive and also same thing for the S3 intelligent tiering they don't have them but technically they do because there is a feature for Glacier called uh Glacier um select which is basically the same thing but for those particular tiers so yes of course Amazon S3 select does not work for those but there's basically the same identical service within Glacier Vault and then this one obviously they support it because this is not built on the same technology as the ones below um one zone or Express one Zone's not here because I have no idea if it's supported because it's so new of the time making this video and it has a different kind of exotic bucket type so I'm not sure if that would affect um its ability to uh work with this functionality here but anyway that's what Amazon S3 select does and it's really useful if you have structured data and you want to get it out of a very large file um uh so there you go hey this is angre brown and in this follow along we're going to take a look at Amazon S3 select which is a way for us to query uh data um within an S3 object so let's make our way over to S3 as well as let's go and get to our ads examples repo I'm opening this up in G pod you can use code spaces if you like or whichever Cloud developer environment you want and while this is open we'll go ahead and create a new folder here we'll just say uh S3 select and I mean there's a few ways we can do this we can use the CLA we can programmatically do it I kind of want to do some coding because we haven't been doing much in here and I'm going to use Ruby because it's my favorite language and you should know the majority of languages so we'll keep mixing them up here but we'll go ahead and we'll just say select. RB and I'm going to also I'm going to CD into that directory so just make sure you're in there S3 and we'll say S3 select okay we'll type in clear I'm going to just run bundle and knit to initialize a bundler so now we have a gem file and so we're going to want to specify um uh something for this so I'm not exactly sure what it would use for this so Aus S3 select SDK Ruby we'll take a look at what um it needs here and it looks like it's part of the S3 client so I go ahead and type in ads S3 sdks S3 I think that's what it is if this is correct we'll type in bundle install and it should pull that gem assuming I named it right it looks like I did so that is good and so when our select RB we can get started here so uh usually you want to have a client so that is definitely the first thing that we're going to want I'll go ahead and grab this um documentation here and we don't need to load credentials because it's going to get loaded in from the environment variables if you are using your own local machine you might have to change this configuration this is picking up in N bars um I want this to be in say Central one actually it'll automatically default to that so I don't really have to worry about it uh there is one other thing we'll have to do is create a bucket to start working with so I'm going to go ahead and type in readme.md and we'll just put in some stuff in here so the first thing is we'll need to create a bucket and then we'll have to create a file with data in it okay so I'll go ahead and type in adus S3 make bucket S3 colon SL S3 select um fun AB for my initials and then a bunch of random values we'll go ahead and paste that in down below so we should get a bucket there we go and then what we'll need to do is copy a file into there and it supports obviously different file formats we're going to keep it really simple and maybe upload um trying to decide what we should use because we can use Jason or C CSV let's use a CSV because that seems more like table table like data and I'm just going to say data. CSV and so we're going to need some CSV data so I'm going to go over to chbt uh generate out um a CSV of data with headings uh I want uh make it like some uh we could say about this is kind of hard so say generate out a list of the top animes the list of the top 20 animes um yeah we'll do do that we'll say make this tabular data make this for a CSV um have six different columns and we'll go ahead and see if we can do that for us there we go we're getting some good information here so I'll be back here when it finishes generating okay okay all right it is done and apparently looks like I can just download a file I didn't know I could do that with chat GPT so that's kind of cool um what if we could make it for more data ah we'll just stick with 20 we don't have to go crazy here I'm getting a bit silly and um what I'm going to do is upload this file to S3 select folder and so here we can see we have um information so that is great I wonder if we could preview this there's probably some way to do that but um well it is what it is so that's fine so we have that file there and that file is called top 20 animes and again to make our lives easier I'm just going to use an absolute link here sorry we'll go ahead and copy this and I'm just going to paste this in here right there and that way we can copy this to our bucket so type in clear here so now we have our file that we want to work with so we'll go back over to our Ruby code so we have our client again Ruby you can have parentheses you don't have to have them around functions I try to emit them where I can to reduce the amount of code I'm writing and so somewhere in here there should be like a way of selecting so I'll go ahead and type that in there this says select object content that kind of sounds like the command here the operation is not supported by directory buckets that's totally fine can I get an example here to see what's going on here um so I'm going to look through this example I think this is what it is because it says SQL expression yeah yeah so this is what that is um again different sdks might call them uh different things but it's the select object content here so we'll go ahead and copy part of this so we need this here okay and so there's some parameters we want definitely we're going to need the bucket we're going to need the object so those are two things we'll definitely need in this case our bucket is called this I'm going to bring this over of course you need to change it for your name whatever you're calling yours and then this is the name of the file so we'll go ahead and bring that in the next thing we're going to need is the expression type which is SQL there can only really be one type here because at least right now that's all there is but I assume they did that because they're planning in the future for something else um request progress I'm not really worried about that it's already set to fall so that's obviously not required we do have to have an input and an output so let's go ahead and copy this I'm going assume if we don't use a scan range it'll it'll R it'll select the entire file so go ahead and paste this in here um we'll go back here for a second and so yeah okay so just take this one off here because Ruby usually doesn't like that when you do that having like a trailing one some languages like that other ones don't so there's a lot of stuff going on here in the input serialization we are using a CSV so that's fine we're going to get rid of Json we're going to use get rid of parket because we're not using those we're not using Jason here yeah not using Jason here so I'll take that out and just just I want to point out I've never used this before but I'm very comfortable with SQL and writing code and this kind of stuff and when you have all those disciplines picking up these new things are like dead simple so anyway we have our expression SQL we have our input serialization file header info use ignore or none um I guess if there was file Header information I guess that's useful I'm not sure if it's talking about the headings in particular comments quote Escape characters okay so this is probably a CLI command so let's go ahead and take a look a S3 um API let's see if it has this get content select object content okay great and so it probably has very similar stuff we go over to our input serialization explains all these things here so first line is not a header oh no it is it is a header so we do want to use it use the first line as the header which it is okay good a single character used to indicate that the row should be ignored um we'll leave it as the default here we'll just take that out and then let's see what we have uh record Eliminator a single character used to separate individual records in the input instead of the default value well since this is a CSV we're going to stick with the default field Eliminator a a character used to separate the individual field records so I imagine that one is for one's like going to be a new line and one's going to be the um the other one quote character we'll leave it as default again allow quoted record eliminators I don't know what does our data look like uh I mean we do have some in here specifies that CSV values may contain quoted record eliminators and such records should be allowed so I'm not sure what it's trying to say there and then here quoted Escape character it's already doing that by default so I want to stick with what the default is so that seems fine so then one we really have to specify is that and that actually might also be the default as well but we do have to have something in here so I imagine that that at the very least we have to have that specified notice that these have this as a default these ones aren't showing defaults record Eliminator ass Single Character used to separate individual records instead of the default value so it's suggesting that there is a default but it's not telling us that it should really indicate it there I guess they just we're not good about the documentation we have no compression happening so that is fine let's go take a look at the output um indicates whether to use quotation marks around output Fields sure you can do that if if you want I don't care The Single Character used for escaping let's leave that alone same thing here ignore that ignore that ignore that seems like that's the only one we really need so we'll just have that there I'm going to take out these uh commas because that might mess things up so very straightforward um a lot easier to look at now we need to actually write the expression which is supposed to be right here I'm going to write it as a her do because I kind of prefer writing her dos so in Ruby a her dooc looks like I don't know here doc Ruby I always have to look it up that's where you can write a multiline string perfect for queries as you can see some languages use triple back ticks single quotations triple single quotations but we're going to write our expression there and so I'm just going to write here uh query and the next thing we want to do is write our query so how do we do that well we'll just look up the docs and go find out really quickly S3 select I can't imagine it's that hard so we'll go here and go into this and we'll go to examples no that's no use we'll just go to the reference I remember going in here when you look in here no examples again what is going on any of us used to have the best docs and it looks like you're getting a little bit lazy these days the table name is always going to be S3 object I remember looking that up there before so there's going to be like a from statement that's usually where we want to start with from so it says from table name the table name here is just S3 object so we'll say uh select from S3 object uh in each form from the Clause table name is the S3 object that is being quered using coming traditional relational characters etc etc so again I'm assuming that um it's not the name of the file it's just S3 object if it isn't I guess we'll find out here very shortly we want to select various particular attributes so we'll go here and make a tab and then look at our anime attributes so we have um Rank and title and release I kind of wish these were lowercase but whatever that's how they did it genre and IMDb and we'll leave the other ones out here for just a moment so I'll go here and type in S3 object again this might be called something else I'm just going to name it like T so that we don't have to go crazy when renaming stuff here that's how you set an alias um usually you can just put T right after the end of it but the syntax might be very sensitive oh you can do either or okay I'm not going to have the as I like I like having it like that and so that will select from that I mean we want all the data so let's see if that works we need to go back to um it was S3 rubby select content I want to get back to that example because well actually we don't really need that I don't think um I what I will do is I'll put a binding pry in here so I'll just say require pry and this allow us to put a break point so we can inspect the response and see what the result is we'll have to go to our gem file and add pry here and then we'll do a bundle install down below it is not being responsive here there we go bundle install and so now we should be able to run this script yeah so I can't imagine in the query we need to have the name here because the keys here so I imagine it's just that but anyway we'll try it so we'll say bundle exec Ruby select. and we'll see if it works um it doesn't know what this is so we have to just require it so we'll say require ads SDK S3 and we'll go up I'm sure you've installed or added oh they always want one of these that's fine okay gem it's required by the the um it's a SDK S3 is using this noiri is an XML parser um and so we just need to use one it usually takes forever to install but since we it's probably already preinstalled it went really fast so it's saying there's an error with our code um error par select missing from okay so I'm not sure if the issue is that this is a multiline object and that's the problem um what I'm going to do is I'm going to put a binding pry here and see if we can just kind of turn this back into a single file so I'm going to type here and and I'm just going to run this again so notice Bing pry stops us where we are so we can now write the word query and see the result and so notice that we have a bunch of new lines so I'm going to I want to replace this and make this a single line oh the other thing that's messed up here is these don't have commas maybe that's our problem maybe the um maybe the new lines is not our issue at all so I'm going to go ahead and just type exit here and try this again we'll comment this out so that it goes past this part and it says some headers in the query are missing from the file please check the file and try again so suggesting that some of these might not actually be there so we'll go back over here rank title release oh that's release year okay so this is release year the only challenge here is this is uh like that so we'll have to put single quotations or double quotations around it I always forget which one it is might be singles might be doubles we'll just flip between the two I don't think it likes that so we'll do doubles instead I never remember which one cuz one is for a string and one is for when you need to uh represent that has a space okay so I don't like uh this one here so I'm just going to take it out for now we we'll worry about it in a moment we'll try this again and it says some headers are missing in the query what is missing now oh IMDb rating is its own thing so we'll clear this again and we'll hit enter okay great so we got a response let's type in RP and we get back a structure with a payload so we can do RSP and I'll just try this load and apparently it is an event stream um can I go 2s please 2s body content I don't know what it's supposed to be so we'll go take a look a US S3 Ruby SDK and so we'll go to the client here we'll go back down to that copy that we had earlier so um content select content and let's see what we need to do with it afterwards words there is a lot of code going on here so we have like a select I wish this was highlighted I don't know why they didn't do that but you can see here that um we have our parameters oh then they actually have a stream here interesting geez they write a lot of code for this and um we can see our our response here so we have a numerator here that's not what I want I just want the actual payload for the record events available is event. payload okay so it looks like what we have to do is use it in a block because if we do that then we have our stream and then we can get our event information here so I'm going to copy this block again the reason why this is easy for me is I know Ruby really well so I'm not uh spending too much time thinking about it here but of course if this is uh not your first l anguage you might find this a bit challenging if you're com from python or something else so we have all this configuration here and what we can do is just take it and move this out of here okay we'll cut this out paste this down below and this will be our um I don't know attributes parameters whatever you want to call them configuration basically and so I'll paste that in there and so now we can just pass it in here and this will make it a lot easier when we're working with um this block here so we have to do do stream I mean I don't work with streams a lot but uh again I'm going to be okay working with it and so we want to cut this block out here and then put that there I'm just going to put a comment here so I can see where the end of this is so we're not getting confused and it looks like we could use that as well I think this one is superus this one down here so I'll take that out just to make sure I'm not mucking up any of my ends I'm just going to do this for all of them stream on error event okay and it said that there was other stuff that we can see this is going to raise the error and tell us if there's a problem it's just going to run into that here it's suggesting that we can get more error information out of it I don't really care about that um well we'll leave it in just in case but here it says the event type but what we were looking for is the actual data so if we come back over to here I think it's adjusted down below that we can get the the payload from here and then that gives us our input output so let's try using that and this Bonny pry here is going to be useless I'm actually going to put it here instead okay and we'll save this and we'll exit down here below whoops exit with an exclamation mark and we'll go ahead and run that again so we have an error here um unexpected end of input line 45 oh we have these three ellipses they don't mean anything so we got to take that out we'll try this again and so now it's captured the information here and we have an event so we'll type event and hit enter we have event records so there's different types of I guess event types so if we go back over to here we have record stats progress continuing and end so there's all these different ones and based on what happens here we might care about something else so we go ahead and just type event payload and see what we get can I just get the output no uh string iio Ruby do string okay so that's what it's suggesting or iuts so say um event payload what does it say puts how about do string string okay there we go so now we're starting to see data and I mean it's not the nicest looking but it technically is correct we'll go ahead and type in exit here and see if it does another event because it's going to keep iterating through it and so this one's the stats so now we can go and say event so we can see how much has been processed that's interesting we'll hit exit again and in this case it's airing out because um it can't print out the payload unless that's the type it is so we'll go here and just say if um puts we'll make a case and we'll say um Event Event type and then based on the event type we can print out different kind of stuff so say when and we had records when it is stats it's something else when it is end it will have something else I think we can figure out what all these are from here event stats progress continued end um so I mean this one's kind of interesting so we'll grab this I suppose and we'll paste that in here as such and then we can just put these out well have to say what it is so byes so this one scanned this one is processed this one is returned okay so we get those three here and then when we are in this case we can go ahead and just say puts and that will puts the uh that there so that's fine that string is not really useful to us I I would like a a better format than that but I guess it could just be outputting back in CSV I mean I would love Jason back if we can get that I'm not sure if we can and so now we can see we get our stats and this is coming back uh for for our records here it's not showing us the order of our headings um but clearly it is getting us information in the way that we're asking for um I wouldn't mind dumping this to Json if it's possible let's go take a look here and okay so I think we can actually choose how we want to Output it it doesn't have to be CSV so let's go back over to our um uh the CLI a S3 I think it' be easier to work with it work with it if it is Json at least in this case go back here version two it of s S3 API we'll go down here go all the way down to the bottom we'll go back into um content or maybe it wasn't at the bottom yeah it was okay we'll look for output serialization all right and let's take a look at um some other options so instead of doing CSV let's do Json because Json is so much nicer um the value used to separate individual records if no value specified s Amazon S3 uses a new line okay so what I'm going to do here is I'm going to go back and let's get rid of this we'll just do this instead and so this is suggesting that we can instead get um back Json and I really would prefer Json on so I think we got rid of our binding pry yep so we'll just go ahead and try this again and now we get back to ason which is a much nicer format if we're programmatically working with it um now that doesn't necessarily mean that this is in in a good format for us to work in because this is literally just a string also I'm noticing here that each thing here uh each line does not have a um comma on each line it's just a new line and I mean that's okay but it's not exactly how I would like it I'm not sure if it's it's streaming each line here as well because notice that we have all this stuff so I'm just going to go ahead and just do a sanity check and put this here I want to see if that appears in between all these lines it doesn't so that's perfect okay so what I want to do here is then get the string the results and we'll go up here and say um string results and I'll just say null here or nil and then we'll just say string results and then the idea is we have this string down here below and so we can split it on new lines so say split on the new line and that will give us back an array and then we can join it back together with a comma so now it's back to a string and so now it's formatted correctly and then we can do is because you know again it's going to like we want this to all be a string with an array around it and so then the next thing we can do is then um put around Square bases like this right and do that and then that should get us back an actual Json shape that we actually want so that's how our Json should look let's go take a look and see if that works so we'll go ahead and take a look here it didn't like the split for nil class so suggesting that this is nil I going to put this above here we'll try this again this might have to do with it being working in an async way which is kind of annoying sync uh string results no it's not set um you know why there's a puts in front of it here that's probably what's messing it up we'll try this again and I want to check string results there we go okay that's fine now um now this one it's all escaped so are these all new lines yeah they are okay great so we'll go back here and we'll try our code here and see if this works we just type exit here with an exclamation mark clear we'll go back and run it again now we'll type Json and this looks like how I want it to be because it has a square base in the front a square base in the back this is important when we want to parse it with uh with Json so I'm just going to go ahead and try to parse it right here json.parse we'll say Json and so now what we get back is a hash which is very easy working with Ruby it's kind of like a dictionary in um what do you call it python or Json object in uh JavaScript so we go here and we'll just say uh Json and we'll go back down here below and so we'll say json.parse Json and now we have our data and so now we can uh work with our data in a much easier way and that's basically how you'd want to work with it programmatically now I don't really want to do anything else other than inspect it and just puts it for now okay so we go ahead and type exit and then we'll just run this again all right and we might there might be a way to pretty print it so pretty print hash in Ruby let's do that or we convert it back to Json I think some people are suggesting that too well there is the require PP so let's see if we can do that I think it's part of the uh the standard library now so we'll try this out that stands for pretty print hopefully that works that is not pretty so um this is another way we can do it now we'd be converting it right back to Json but that's totally fine and we'll just say uh data here again I mean if we really wanted in Json we'd have to go through this whole rig Ro anyway and um we need to do a puts on this here so just do puts great and so now we're getting our data in a nice nice format so lots of stuff going on here but let's go take a look at our queries so there's a few issues here that we're having one is uh how we going to deal with this space so we'll go back to our select and we'll take a look here the first form of the select is with the Aster yeah we could do that to get all of them and so it's not telling us here what we can do when we're dealing with the space you can refer to the nth space uh you can refer to the n space column you can refer to n column so I'm not seeing a thing here for an issue when we have oh here we go so you can use double quotations to indicate the column headers which are case without double quotation marks object headers are required are case and sensitive okay but what about spaces it doesn't say anything about spaces but it does suggest that we can put doubles around it so we did do that before we can try that again so I'm going to say t release year and let's see if that works so we'll just go ahead and fix this here so that does fix our issue okay great so now we can go ahead and get the other one which is t i IM DB rating okay we'll get that one as well hit enter oh we this one has to be an uppercase here because it's case sensitive right and we'll try this again good if we want to change the names of these we can we can say as rank as year as rating as title as genre that's pretty standard uh SQL right there we'll try this again hit up I mean we should be able to Alias these see they have an as here I'm just carefully looking at what it's complaining about expected identifier for Alias at line three so one two three yeah I'm not sure what it doesn't like there what we can do is we just try one and then see what happens I'm just taking those two out of there that are kind of troubl Matic so those ones are fine what if I just try as year I don't know if like that's a reserved word and that's why it's complaining there we go so I think maybe when we write the word year it just might not like that because it's a reserved word okay so then we just say release here and this one could be as rating all right so that's a little bit nicer um we should probably just do a couple other things like try to sort this in the other way so we could say limit uh limit to five or maybe four records here okay one two three four so now we just get four that's nice the next thing we can do here is try where and we could look at this and say where rating is greater than look at the ratings we have here we have 9 1 9 8.7 so maybe we'll say less than 8 or maybe less yeah less than eight and we'll see what we get for this will that work unknown token found try this again here it might not allow us to use the um the Alias down here so we'll let try this IM DB rating like this will that work no it does not let us do that okay let's go back and take a look where that has a Boolean Boolean result okay so let's Che a look at our operators I mean we have the less than sign so that should technically work I guess the other thought is that this is not necessarily a number it could be represented as a string it doesn't know what it is like it doesn't know if it's a string or whatever maybe we can cast so we have data types here the general rule is to follow the cast function if it if it's defined okay so probably what we need to do is cast this before we use them we do have functions down here so I'm going to guess go ahead and cast this value to a uh to an integer or a value so we'll say cast and it probably takes a parameter as the second one here this one technically is a float so we'll go ahead and say float and I again I don't know what it wants but we'll go ahead and try this doesn't like that still I mean we'll go back over to here let's go to the cast function and see how it works as whatever okay so we have to go here and say as float we'll try this again I just want to take out the wear and see what happens if we do that that so we do get data back it's complain less also notice like see how these are strings this is now a float it knows what that is so we can also do this for our rank as well so we can go here and say cast and say as in and so that should return back that number but again I want to filter on that so I'm going to try this again say where I'm going to try rating again because I really don't want to have to write the other one there I'll say under eight and try that again still having issues what if we just take out the te on this try that instead no no good no good um conditional functions yeah we can use CES case so we do have quite a few interesting things here but I'm still trying to figure out the we which should be really easy to do but it's giving us some trouble let's go down to our wear again why does it not show us any examples of wear like where is like one of the easiest things to do and they don't even show us an example umq reference let me fill around with this I don't want to waste your time here so I'm just going to go figure out and come back okay all right so I played around with a bit and so I tried this and this works fine but for whatever reason I think like if we were to put in this operator let's try this again uh well it didn't error out so that's good what if we said um's the other way around yeah so for whatever reason I don't know what I was doing before but it just refused to work um but hey at least we have a wear that is working um doesn't really matter what case we we're using here there's of course other functions this is more about learning SQL and what you can do with it um as there are just a ton of different other functions but I feel like this is more of an SQL discipline as opposed to learning so much about Cloud so I'm going to consider this to be done so we'll go ahead and save this and say uh Ruby Ruby example for S3 select and we'll go ahead and save that and then let's just go and do some cleanup so to clean up we just need to remove the bucket so we'll go ahead and do that RB and then well we got to empty we got to get rid of the file first so we'll go ahead and delete that first oops and this will be for sltp anime top 20 anime CSV and we'll go back and hit up and we are done here see you in the next one ciao all right let us compare S3 select and Athena because these Services uh seem to do the same thing or they appear to do the same thing but they actually don't and they serve different use cases so let's clear that up so let's talk about the scope the use case uh the query complexity the supported formats Integrations there's definitely other things that we can compare but again I just want to make sure you understand at a high level the difference between these two things the first is scope so S3 select is for creating the contents of a single S3 object whereas a theme queries across multiple S3 objects and buckets in one shot uh for use case this is for getting a slice of data of a larger file um which is kind of interesting because we can use uh the range header to get a part of a file but this one S3 select is obviously more intelligent than that uh Athena is more like for ad hoc analysis uh maybe you want to leverage it with a business intelligence tool like Amazon quick site and it's usually focus on larger data sets for query complexity S3 select has very simple SQL Expressions where Thea can support things like joins and window functions and other things for supportive formats S3 select is just SV SC scsv Json and parkette where Athena has also a arvo arvo I want to say Avro but arvo and orc for Integrations um S3 doesn't really directly integrate with other services you're just supposed to use it programmatically where Athena has a lot of synergies between ads analytic Services Like adus Glue it has an adus console uh UI so you can just write the queries in place uh you can obviously do things programmatically but it's just a lot easier to use um so yeah hopefully that is clear uh that one is for a single object and one is for quering multiple objects and it's more of like a uh data analysis or uh data tool there you go let us talk about object tags um which allow you to apply tags at the object level unlike resource tags which are the resource level or you could say infrastructure level such as buckets why do they have separate tagging uh well objects is data and you wouldn't normally have resource tags on data so this gives you that similar functionality but at the data level you can have up to 10 tags uh onto an S3 object and the advantages or benefits of using object tags would be things like enabling fine grain access controls of permissions via IM policies fine grain object life cycle Management in which you specify a a tag based filter uh doing S3 analytics you can configure filters to group objects together for analysis by object tags and we have cloudwalk metrics which can display information by specific tag filters so it gives you a lot of robust functionality that uh You' normally expect to get with resource tags on on resource level objects but with object level uh resources there you go hey this is Andre BR and in this follow along let's take a look at object tags so as per usual I'm going to get over to my repository the ads examples I'm going to open up a new Cloud developer uh environment and we're just going to quickly create a Bucket Place an object and apply object tags to it so it shouldn't be uh too difficult here so now that this is launched up I'm going to create a new folder in here call this object tags we'll try this again here object tags and I will create myself a readme.md okay and we'll just say create bucket uh upload file so we go here and we'll say itus S3 um make bucket S3 col slash a uh object tags fun AB for my initials and we'll put some numbers after that then we'll go down here below and we'll type in adabs S3 copy and we'll need some kind of local file so I'm just going to copy this address here paste it in and we'll just say hello.txt as the example say touch whoops Echo hello world hello.txt so we'll go ahead and execute this so I should have my um it was credentials loaded up here yep that worked out great so we're going to touch that file here also just make sure you know where you are I'm right now in um uh the main route here so I'm just going to CD into the object tags directory here we'll type in clear we'll go ahead and try this again paste that in we'll go ahead and copy this paste that in and uh it doesn't like something here what did I mess up iTab us S3 CP oh you know we got to say what file we're copying so we'll do this I'll just hit contrl C to cancel that out we'll try this again we'll hit enter and it's going to go ahead and upload that file so now that file is uploaded the question is how do we apply object tag so we'll go over to the a CLI s S3 API because I'm going to assume that the regular one can't do it and make sure we're on version two here search for object tags so it looks like there is a thing for object tags called put object tagging we'll go down below and look at the examples we'll copy this example here M yeah okay that one's fine and then we will go over to here and we'll say apply object tagging and we will grab the bucket name here and we'll paste it in and we will type in hello.txt and then we have some tagging so we have Tag sets and then we have a key and a value I'm not sure why we have Tag sets I don't know if we talked about that at all Tag sets objects uh tags in ads S3 sometimes you learn about stuff unexpectedly Tag sets no I mean I guess it's just it's basically the set of tags the reason I'm confused is that often when you have other ones they just will have you pass the key and value right away but looks like they're doing it this way um could we do this in a shorthand probably I don't want to fiddle with it it seems fine enough I go ahead and just say key um hello and then this one is going to be world and hopefully that will work we'll go ahead and try this out and hit enter the reason I say I wonder is because sometimes you have to have prefixes but that's for metadata so I guess we're fine here let's go take a look at Theus console we'll go into S3 and we'll take a look at what's happening here I got to go loog in give me just a moment all right we're in here and so we have object tags and we'll go into this and we'll see if we can take a look at our object tag maybe under permissions properties I remember we saw it here before um oh here this tags so we have key hello Value World so of course object tags allows you to do all sorts of uh interesting automation um we don't really have any use case to intersect any of that right now so I think what I'm going to do is just say this is good enough and if there is an intersection for some other service like when we do cost management or something then we will utilize object tags when we can but for the most part this is sufficient enough for our learning we just want to make sure we know how to apply tags and that they're not the same thing as resource tags so we'll go ahead and just do cleanup here so there's a couple things we want to do a S3 remove and then we'll grab this full address here and then we wanted to get rid of that bucket try this again there we go and I'm going to go ahead and copy and paste and then copy and paste and hit enter and now we are all good so I will see you in the next one ciao we are taking a look at Amazon S3 server logs and this provides detailed records of requests that are made to an Amazon S3 bucket it's very easy to turn this functionality on and there's no additional cost to enable S3 server logs except the cost to store it in another bucket so it can be uh cheaper um than turning on cloud trail uh data events it's just going to be dependent on what information you're trying to capture it what frequency and things like that for that destination bucket you got to make sure that it doesn't have requesters payer payers requests always forget the order of those words but the point is you got to pay for the bucket not somebody else um you can't have object locking on it you can choose the format of the of the object key so you have a couple here and the other thing is that log files are plain text files with each recorded object action on each line by datetime this is pretty standard for Server logs so here we have um a few lines you can see it's really really long so it's not easy to make sense of what's going on there but I did write down um all of the attrib that you should expect to see so we can see when this has happened uh we can see the operation and a bunch of other stuff like what is the status code if there was an error code stuff like that there are other additional fields for copy operations but you know this gives you kind of an idea of information you can get uh yeah and there you go S3 event notifications allow your bucket to notify other IT services about S3 event data and that doesn't sound very impressive when you say notifying them but really what's happening here is that you're have an easy way to do application integration from S3 to other services like Lambda and uh this is an extremely powerful service this slide does not give it justice but it's just hard to visualize it at least here but let's talk about the kind of notification events that S3 event notifications can listen to so the we have things for new objects object removals restore objects uh reduce redundancy storage if objects are lost replication events S3 life cycle expiration events S3 life cycle transition events S3 intelligent tearing Auto automatic archival events object tagging events object ACL put events and so anytime any of these things occur what you can do is send them to other services like SNS sqs Lambda and then event bridge and when it goes on event Bridge basically can do anything um Amazon S3 event notifications are designed to be delivered uh at least once uh notifications are delivered in seconds but can sometimes take a minute or longer we're going to spend a little bit more time in the labs on S3 event notifications because again it's a super useful uh uh functionality here U but yeah hey this is Angie Brown and in this follow along we're going to take a look at S3 event notifications so S3 event notifications allows you uh to listen to events and then trigger uh trigger another service but really saying it's passing that information to a destination so I have a bunch of buckets here I'm just showing you in the UI really quickly um to get you a visual in terms of how this works we're going to do it programmatically though so if I go ahead and say create event notification you notice we can put a name we can say only apply to particular prefixes or suffixes we can choose the event type so only happen on a put a post or maybe an object when an object is removed or restored or an ACL or object tagging lots of different options that could occur here and uh we can send it to a Lambda an S SNS topic an sqsq we can specify a Lambda function here and uh respectively the other ones based on what we select there is also another option where we can use event Bridge so if we go down here notice here it says for additional capabilities use the vent bridge I guess is it using S3 event notifications I believe it is uh at scale using S3 event notifications and honestly event notifications might be using event Bridge underneath and it's more of a simplified way send notifications to Amazon event bridge for all buckets so if we were to turn that on I imagine that we'd have more configuration options there so what we're going to do is uh go into our um repository and I'm making a new folder here and this one is going to be called event notifications and I think that I want to go ahead and do something with um uh IAC so we could use terraform we could use cloud formation um I'm going to use terraform this time so we'll go ahead and do that and of course I've showed you how to set up terraform so you should know how to do that and we're going to go over and type in terraform provider or terraform registry and we'll go ahead and grab the adabs registry or provider information so we'll grab that really quickly here I'm a CD into the correct directory so that we can um go ahead and initialize this directory so we say terraform a nit and it's saying it does not detect that command there's terraform over here it looked like it ran into an issue when it installed I have these bin scripts here so I'll just run it again and see what happens I'm just going to CD back into the correct uh directory here I'm just going to go bin um bin I got go up a directory sorry bin terraform CLI maybe there's something wrong with my script it should work sometimes these things get janked and you just have to uh tweak them because you know terraform can change or other stuff can change so that should have been installed with no issue but whatever uh we're fine now so we'll go into uh S3 and then back into event notifications we'll type in clear I'm going to type in terraform and nit to initialize that project um it's going to pick up the configuration from this because we set our Nars in here uh I did this uh previously when I first set up git pod if you're not using git pod you will have to uh configure this and if you are not sure how to do that if you go to the documentation on this page it talks about uh the different methods for configuration for authentication and so the one I'm using right now is environment variables normally when you're locally developing you're going to use um uh the configuration files or maybe it's the credential files but when you're in a cloud developer environment it's always going to be environment variable so you'll have to figure out what's going to work for you um so what I want to do here is I want to create a bucket so we'll go back to the documentation here I'm scrolling on down and looking for S3 and we're going to go to S3 bucket I'm also just going to double check here on the left hand side for additional configurations so I'm looking for um event notifications I'm just seeing if it's separate what's interesting when you use cloud formation so we say S3 cloud formation uh all this is under the same object so when you're doing this notice that these are all the properties here under a single resource in um terraform they're broken up into separate blocks and that's totally fine it's just a different way of doing things but what I'm carefully looking for here is the event notifications so there's somewhere here I think it would be this one here so I'm going to open that in a new tab but we are going to need a bucket to start with so I'm going to go ahead and copy this and I'll go down below and paste this in here I'm not going to supply a bucket name because the bucket name will be randomly generated because that is how it works if you do not supply name I'm going to name it default this is a good practice in terraform if you have a very small project and you have everything default and then your life will be a lot easier uh when your project grows you'll have to change it a bit so we have that let's go over and look at bucket notifications um so here we have a lot of stuff going on here um here we have an SNS topic oh you have to create one first that's right let's grab this block which is for creating the um the notification I'm going to go ahead and copy this link up here and then we're going to go ahead and make a comment here so we can find this stuff a lot easier and uh would it be easier if we use something like um a coding assistant sure but I don't know I think it's really good to be able to navigate without a coding assistant but sometimes in projects I use them so I'm going to go here and just call this default as well this will reference the bucket up here I'm just going to say default and uh this block is obviously going to be different based on what you're supplying so I'm just going to comment that out for a moment because we have to decide what is what is it that we want to do do we want to send something to SNS to sqs to Lambda to event Bridge personally I'd like to do all four uh just so that we get get really comfortable with uh application integration to other services but um we have our bucket here so I'm just thinking here um I'm going to comment this out for now and let's go ahead and just go and create this bucket so I'm going to type in terraform plan okay and so here we have our plan I'm going to type in terraform apply Auto approve and that's going to go ahead and set up that bucket so now we have that bucket and it's called what I'm not sure we don't have an output on that so what we can do is go down below here and we'll say output output and I don't remember how to do outputs in terraform let's go take a look here what's important is that you know that it exists not that you can remember the code right off the top your head so here is a example we'll go ahead and P that in and this will be uh S3 bucket name and from here I want to get ads S3 bucket and this will be default and this will be its name and that will be its value so we'll go ahead and do terraform a plan and see if it likes that um the object has no argument nested block or supported tribute name that so did I copy this correctly yes it was S3 bucket default I mean that looks correct to Me Maybe it doesn't have a property called name and its ID we can check this by going over to the documentation and if we get this thing out of the way we go down to um maybe attribute references yeah there is no one called name it's going to be the ID that we want so we'll go ahead and save that I'm going to type in clear I'm going to go back and do ter for plan and it should be able to take my output we'll let it refresh there I'm going to go and write terraform apply there's not really anything for it to change per se but it's going to change it so now we have that terraform bucket name and if we type in terraform output I think that it will then print out all of our outputs so we do terraform bucket name output bucket name we can get it there so now we know what our bucket is the reason why I wanted to do that was so that we can go and find it over here and not try to figure out what the name is but of course they always apprend it with terraform so that's really useful or at least I think they do terraform there it is and there is our bucket I want to go over to properties and I just want to pay attention to the event notifications here so what we'll do is first take a look at our bucket I want to see if we can turn on event Bridge um at the at the Bucket Level or the at this resource area and so I'm carefully looking for Brent a vent Bridge um I'm not seeing it and also noticed that it used to actually have them all contained within here and notice this is deprecated so it was more similar to how um uh the cloud formation had it right where everything was in here and then they decided let's substract these out into their own blocks um is that a good thing sure why not I don't know but I'm going to go ahead and type in event Bridge so that's not there let's type in Cloud watch that's not there so I don't think that we turn that on uh in this resource here let's go over to uh here and take a look at what options we have it's also interesting that we can use different languages so terraform can work with cdk to produce um terraform but allow you to use your favorite programming language I'm going to stick with terraform because I think it's a much better language to use and so what I'm looking for here is the arguments so we have event Bridge so whether to enable event Bridge notification defaults to fall so that's all I want to change right now so we'll go and uncomment this block that we have here and I just want this part of it right now and I'm going to just turn on event bridge and say true I like how it's Auto complaining that's that's really nice and I'm going to go and deploy that and say Auto approve and so now if we go back over to our bucket and we give it a refresh here event bridge should be turned on it is good um now let's go ahead and set up some other resources so one thing that we will want is maybe an sqs q and some other things so let's see what we can do can we set up more than one at a time let's go look at the interface I think so because it allows you to create multiple event notifications um let's go take a look at how we'd actually do that in ter form so that's a special one uh use to configure notification to a Lambda function notification to an sqsq to an SNS topic then we have Lambda function and okay so then these are the the ones expanded here okay that makes sense so I'm going to go up and let's see if we can grab some code so one thing I'd like to have is an sqsq so we'll go ahead and grab that and I'll place that here if you don't know what these resources are right now don't worry we're we're going to stay in the free tier an sqs is a is just a queue it allows you to send messages to it and we can look at them um again I'd like it to randomly name it so I'm just going to take that out I'm going to hope that that works so we have that it looks like we are supposed to have an IM policy document for that so we're going to go ahead and copy this and paste this down below here you know if we're finding this to be a bit messy we can break this up into separate files it's not really necessary but um we might do that a little bit later here just so I can demonstrate to you that we can do it we have I a in policy document Q effect is allow the principles is for everywhere it's allowing us to send messages it's saying for everywhere we could specify exactly this here but we'll keep it here because that'll keep it simple um it actually might matter what the name is here so if we go back over to uh to this sorry notice that um it has this name here and it matches here so really the proper way would be to reference that um but the confusing part is that this needs to be this has to exist before this name would be generated so we actually probably do have to hardcode a name um so that's what I'm actually going to do I'm going to go back and add a hard coder name or it's going to make the policy really hard to reference so I'm going to go up here and um do I like that name for RQ not really trying to think of an easier name um I'm just going to do s s S3 evq I'll just shorten that there okay uh there's probably some way to include um the account ID and the uh the region but I'm just going to let those be uh wild cards because it's okay then down below here we need to specify our bucket so we're saying we can uh send messages and The Source has to come from a very specific bucket so that makes it a lot more secure so that makes sense and then we are passing that there so let's go ahead and see if we can create that so we'll type in terraform plan and see what happens and we'll scroll up looks good to me yep so we'll go ahead and do terraform apply Auto approve and that's going to go ahead and deploy that I think qes create really quickly so it shouldn't take too long and while that is creating we can go over and take a look at our queue so we'll go over to sqs it's really really important when you're working with ads not to just focus on a single service but how it connects to other ones so again I'm going to try to include as many um Integrations as I can to other services when we're working with these examples as long as it doesn't take super long to create these resources so I'm over here and I'm waiting for this oh we are in it right now okay so we're in our sqsq um it's been a while since I've been in here so there's a few uh new options I understand what all this stuff is but they changed the UI and they're going to change the UI again on me soon I'm almost sure of it just cannot seem to leave that alone so we created that sqsq but now what we want to do is we want to um actually create a a bucket notification for that so I'm going to go ahead and uncomment this and in here we'll need our topic AR so this one here is called oh we left we left it calling it Q yeah that's okay I guess well this is for a topic this is for SNS we actually want so go ahead and bring that one in here and I'm not sure what the options are so we're going to go and take a look here we should have one here here we go so I'm going to go ahead and grab this block as an example and it has an ID I'm not sure why it needs an ID I guess you could oh because you can name the actual um uh the actual bucket notification and so it says adabs sqsq Q Arn and when an object is created and it's in images then it's going to go into that Q so that is something that we can do um so I'm just looking at this yeah that's fine um so we'll go ahead and try this so we'll type in clear and actually I'm going to make a folder called q and I'm just going to make folders for their appropriate uh use case so if we're going to use Q will be there if it's for a topic we'll make a folder called topic and that's how it we'll know and I'm just going to say um event Q just make this a bit more generic we'll say terraform plan and see if we're allowed to do this here and we'll looks like it's okay so we'll go ahead and type uh apply what we should be looking at here is if it's destroying objects or things like that it's not really an issue uh the way we're doing it but if we were working in in a a production environment we really have to be sensitive in terms of uh what we are changing because it could tear down resources and they could be unavailable for a period of time when we tear them down so that should now be changed we'll go back over to uh S3 we'll give this a hard refresh and we will take a look and see what we have so we have our event CU so we have one there let's say I want to uh create another one so here's my question can I just put it in here or should I create another resource because it seems like we should create multiples of these right like that's my thought process there um but the other interesting thing is that we turned vent Bridge onto this one so it almost feels like a singular block and that's where I'm not 100% sure um but let's find out and and see what we get so we have a queue let's go ahead and create a topic so we'll go back over here I'm sure they have one for topic it is such a simple example it's probably right here at the top so yeah we have an SN SNS topic here so I'm going to go grab this and we'll paste it in here this is obviously uh SNS topic I'm going just shorten this to EV and this will just be topic could we use the same policy probably not it's actually good that we left this as Q because we're going to need another one let's go grab the policy they have to make our life a lot easier I'm sure it's very similar but we might have to make one minor change to it so we go ahead and paste that in and uh yeah the only difference here is that this is just going to be default so my whole like default thing just kind of fell apart I'm going to start breaking this up because it's becoming a little bit wieldy so we'll have one for uh topic TF we'll have one for uh Q TF and we'll have one for Lambda as well I'm not sure about vent bridge but we'll see I'm going to split this here and I'm just going to bring some of this code over so one thing I want to bring over is um this to topic uh policy so we'll cut this out here and paste it in here and then we can go up to um this here and cut this out and also make sure you're in the right file that's Lambda it's a little bit hard when you're recording to uh keep track of where you are I'll go ahead and close that out we don't have anything for Lambda right now I'm going to just split this and I want to bring a q over here close that out and uh we'll cut out this we'll paste that in it'd be interesting if we can create multiple uh blocks of those event notifications because then we could keep its um configuration within the scope of the individualized files and I mean if we really want we can make one for bucket as well so let's say new file bucket. TF and then we can go here and just copy the bucket specific stuff out of there there's lots of ways to organize your terraform files you just have to decide how you would like to organize them so that's starting to work out really good um I'm also noticing that uh we don't have a g ignore file in here here oh I don't think it matters because at this level we have all that stuff in there so just make sure you have that get ignore stuff and you don't end up committing a bunch of junk files or large files that are going to be a problem for your repo later on but anyway uh we now have this broken up here and um we have the topic over here we changed uh this one but we got to change this name here so we'll go here and just say S3 EV topic I'm going to go down here and type in clear and type in terraform plan and the whole thing with terraform is that if they're in separate files it will roll up all the files and treat it like a single file so you can just do that you can't do that in cloud formation at least last time I checked you can't I say that because cloud is always changing and they they catch you when you when you don't expect it so I'm hoping that will create a topic it looks like it did without issue I would like to have some more outputs but I mean like the names of these are pretty straightforward so it's not like I really have to investigate too hard we'll go over to topic the interesting thing about topics is that in order to use them you have to subscribe to them first so even though we've created our topic it's not going to work until there is a subscription so if you go here so you have to confirm it before you can use it SNS now supports in place messaging archive and Replay for uh fifo which is for sqs that's cool um so that one set up let's go take a look look okay so the question really is is that can we put this block here along with that one so what I'm going to do is just uncomment this and I don't care about suffixes I'm just going to stick with prefix here and this will be for topic and yeah we'll do this on object create as well we'll just do them all on object create it's pretty obvious you can swap those out and this one looks fine so the question is is this a valid configuration terraform uh plan let's see what happens and it is suggesting that we're allowed to do this it's not going to know until we attempt right so we'll try this out okay and so it's suggesting that it's added it so my question did it add it right here if we give this a refresh and did it it did okay uh now what's interesting is that the event name is randomized because we did not provide a name and I guess that's fine we don't really need to provide an ID for it so what I want to do now is I just want to comment out this block because I want to try and see if we can break it up into separate uh separate blocks and see if it works so we'll do terraform apply Auto approve and that what that's going to do is remove that stuff and while that is uh working apparently worked really fast we'll go double check that by going back over to here and giving this a refresh and we'll scroll on down yeah that is all gone that's turned off now that's great uh what I want to do is um break this up and see what happens so there's a couple things I want to break up one is the event Bridge option so I want to do this and then I'm going to go here and I'm just going to say uh event Bridge here and then for this one I'm going to say topic and then for this one here I'm GNA say q and I want to see if we are permitted to do this and we're getting some complaining here oh we have to specify the bucket that's fair enough okay so we'll go here and specify the bucket for each one all right and so these are now broken up into three we'll go ahead and hit up and say Auto approve and so what I want to see is will that work so it seems like you can either use a combination or use it as a single block it's really important to test these things out as you're working through your code um so we'll take a look here and did it work I mean we have one where's the rest did did we have a failure so that didn't work um so now we know that we can only have a single block so I imagine if we want to have multiple topics we would just repeat the topic block multiple times okay so that was no good so we'll go ahead and we will uncomment all of this or comment it out sorry I'll go ahead and do an auto approve and while that is running in the background we'll go back and reassemble it back into a single file default and I'll get rid of these two lines and that line and these two lines and that line and so now we're back to normal and we don't have to again specify the IDS it will automatically create those for us but yeah if we wanted to have two topics we just repeat it twice I think um but I was hoping that if we had these separate blocks I could put them in the respected files it's totally fine if we can't I don't mind that we'll go back ahead and do an auto approve as that is working through I'm going to go take a look at the Lambda example so we might have one here for Lambda sqs Lambda and Lambda is going to be uh probably a little bit more going on there so I'm going to go over here and open this and we will grab our code so we need um we definitely need a policy an assume roll do we need assume roll yeah we do we need an IM for Lambda we need permissions for the bucket we need a function so yeah we need all of this stuff okay so that looks all good to me the major change here is going to be our bucket if it if it's even referenc in here at all bucket this one's default and this all looks fine to me we'll have to adjust this here because we need to actually Supply uh some information for that um we probably just do that now and then add it afterwards here so let's take a close look here so we have a um a function the function can take a file I'm just trying to think of something really simple that we could um Implement um let's go look at uh python Lambda example and yes we could use chat GPT for this but uh I assume it us has very simple Lambda Handler function okay can I get a more full example so yeah here's a very simple one where we can just apply a a couple values so I'm going to go back here and I'm going to sorry I'm going to make a new folder here I'm just going to call this one uh function and in here I'm going to create a new file and I'm going to call this function. piy and I'm going to go ahead and paste this in here so now we have our very simple Lambda Handler and um coming back to our Lambda our function name is called what Lambda Handler so we'll go here and just say Lambda Handler and this is going to be for the python run times it'll go all the way back to the top I think if I click back it was showing me the run times here under Lambda python we want to use uh 312 because why not it's the latest and so I'll put that in here actually the Handler is down here so the function name here and then the Handler here oh you know what that might be for um the the configuration we have so we should really um look this up so what I'm going to do is look up a Lambda function I'm just going to leave this t open I don't want to lose it and we'll go ahead and copy this and I'm going to look up Lambda and from here we'll look for Lambda function it was Lambda function due to the a Lambda improv VPC networking changes that began deploying 2019 ec2 subnets okay I don't care about that we'll scroll on on past it and I'm looking for a python example if they have one they do not that's totally fine so I just need to read some of these uh tributes or arguments because I'm not 100% sure the unique name for your function oh so this has nothing to do with what your um what it's actually called it's I'm going to put this so I hope that it randomly generates unless we need it for our policy or permissions um I mean it grabs the function name this way so I don't think think so so I'm going to leave out the function name for now because this could be called whatever we want and then we'll take a look at the Handler uh back over here is the entry point the question is what do we call it for python the reason why is that when you have um uh JavaScript you all you you always do exports. whatever right and so that's pretty common but what is it for python we'll go back down to here we were here earlier but we'll go check it again and see if we can find what it would be called so I'm not 100% sure but you know sometimes what we can do is use click Ops to get that information so I'm just going to make my way over to Lambda and I'm just going to quick create a Lambda that we're not going to use we're just going to delete it but we just want to see what the default would be so if I go to create function we can choose a blueprint there's a hello world here and this one's in node.js I don't want nodejs I want it in Python so I'm going to type in Python here and we'll do this one this is like a hello world one so the runtime is this architecture is fine um that's okay I mean we don't want to do Json that's totally fine I just want to know what the the Handler is going to be called so we'll go ahead and create this function my Pi function just call that really quickly create that function and then that way we can uh find out that configuration okay so we'll go down and take a look here so somewhere here it should specify the Handler here it is so it says Lambda function Lambda Handler so the file is named Lambda function and then it's Lambda Handler so that's going to follow that convention if we go back over to um our code we have a function it's function the file is called function and then it's called Lambda Handler so this is going to be function Lambda Handler um here it's going to want a zip file so probably we need to do a little bit extra work here to provide that zip so let's take a look here and what it can do path to the functions deployment package with within the local file system and we'll go back over to here and yeah it still says zip so the question is is there like an easy way to package these files I have to look this up packaging a python function for terraform because we could use Sam CI to package it but there's probably some other way that we can do it this is four years old but it's probably fine and they are not really describing much here let's go back and take a look is that the same person no just give me a moment we'll figure it out okay there might be like there might be a way of of um zipping it with terraform but give me a second all right actually looks like we don't have to go far because they're using a null resource and so this null resource I mean this is creating a Lambda layer and it's executing it's it's creating uh the zip but that's not for our actual function right so Lambda layer here Source here this is again Lambda zip this S3 object I'm looking for not the Lambda layer but the actual Lambda itself so this is a Lambda function if I was to look for this cuz they're not specifying the locals here I mean it is right there so maybe that's what we can do we can package it using null resource I mean null resource is old but we can um modify that to use something new yeah here they're using null Resource as well uh now that we have un name for the zip let's go ahead and create it so this might be another way to do that archive file Lambda or sorry S3 or sorry terraform maybe we can uh use something like this this might be a builtin function this is deprecated use data source instead okay data source archive file okay it's deprecated but point me to the new one please data sources archive file okay so this is the new one so it looks like we can use this uh do we need a provider for this it's part of hashy corpse so it might automatically get included without us doing anything sometimes you have to explicitly require them but I think it'll bring it in here and so here it says archive a single file archive multiple files land a function this is kind of what we want archive file to be used with the Lambda using a consistent file mode um I'm not sure if we have to set a file mode for this but I'm going to grab this one because this seems useful I'm going to go down here and paste this in I mean I didn't see it in the example code that we were looking at before unless it was there and I just missed it so we'll go up here and paste that in you can see there's like a lot more work to be done for these uh for this one here but um that's just always Lambda functions so here it says source source file path. module so this should be relative to where we are and then this would be our function and then we' have our function Pi there's actually a little bit more to it because I think we have to include the requirements.txt however we're not really doing anything uh we're not including any Library so maybe we can get away without having it um then we can specify the output directory so I can go go ahead here and say function. zip and we just put it here Loosely here while we're doing that I can create a new GE ignore file and I just want to ignore any Zips in this folder going to make sure I actually am in that folder yes I am sometimes it's you end up being in a subfolder by accident um but is that enough to work because we did look at our um Lambda in the other spot wherever it was here and it doesn't have a requirements.txt so that makes me think that we don't need to have that requirements txt in there sorry especially since we're not utilizing anything else here so we'll go back over to our code and I mean this looks good and this one's not referencing anything but let's see if we can go ahead and just referen this so I'm going to say archive file dot Lambda function dot do I get a name do I get anything else not really I mean I could also specify the file um uh using using uh interpolation so we could just do this instead but it' be nice if we can reference it because when you can reference it then off then sometimes it'll have dependencies and it will'll make sure it creates that other one first does not currently created uh currently create proper zip Bar C when sourcing symbolic links okay I'm not worried about that can I reference this at all um I mean okay this one says archive file oh we just didn't scroll down far enough so we have uh output path so the output of the archive file that's actually what I want so let's see if we can go ahead and type that in there we are I still can't remember if I have to put data in front of here that's what I'm not sure but I'm just going to leave that out and just assume that that's kind of like an hour resource we are getting some errors here uh is not specified an attribute name function name is required so it really does want the function name and I'm just going to call this one Lambda python uh S3 events okay so now it actually has a name and that should create less issues for us and we should be able to Archive that and upload it and we should have all our permissions so let's see if this works that was a lot of stuff going on there hopefully you can follow along terraform plan inconsistent dependency lock um it's because we added this here so we might have to do a terraform a nit and so it will pull in another provider if we scroll up here we can probably see it notice it brought an archive which is what we needed for this so we'll go ahead and type in terraform plan and let's hope that we get something that's useful the manag archive file has not been declared in the root module ual okay well let's try data I always kind of forget if you have to put data in front of it or not and that's all it really wanted there so that was right and it looks like it's going to create stuff so I think that's fine we'll go ahead and do terraform approve or apply Auto approve and we'll see if it takes that in the uptick there it did create the zip so I immediately see that it looks like it's creating the Lambda without issue while that's creating I'm going to go here and delete this one because I don't want to get mixed up and keep looking at this uh one here go ahead and delete that Lambda going go over to my Lambda functions I really should only have one function in here so there it is um it looks like it's creating without issue it did perfect we're lucky and there's our Lambda Handler and function so now we have all those three um set up the question is what could we do with event Bridge uh cuz that's a third one but before we do that let's just make sure we add that here as well for our Lambda function configuration so go back to our uh code reference for S3 bucket notification and we're pulling out stuff here for Lambda in here there should be one for the bucket notification it's this block here let's go grab this there seems to be a dependon so it seems like it really wants to have that dependon in place and we'll go and paste this below and we have this allow bucket permission so it's bring it from here and saying this needs to exist before we uh go ahead and create that maybe it would create a conflict if it wasn't there first I'm not sure maybe we don't actually need it it's just super fless code sometimes that happens as well we're going to only uh do that on folders that have functions we have adus Lambda function. funkar um I think that's fine because we I think we just stuck with whatever they called it so that's perfect we'll go ahead and write terraform apply Auto approve I'm very confident in this code we'll go ahead and hit enter great so that is produced that let's go back and check our bucket confirm that that has changed we'll scroll on down we'll take a look now we have all three we have our Lambda or Q our topic we still don't have anything for event Bridge as I haven't really determine what we should do there um but maybe what we should do is just get all of these three working and and and have them uh do something so what I'm going to do is I'm going to go back here and we should have a read me we don't so I'm going to go ahead and say or well you know we'll make a new folder called bin and I just want to create a few things so we'll have q we'll have function we'll have topic and the idea here is that I just want to trigger um I just want to trigger an upload to each of these okay and then that way it will configure the event notifications so I'm just closing out these tabs and we grab this shebang for our scripts so we're going to put that in each of these here and what we want to do is we want to copy a file um we might even want to just make the file in place I'm just going to go and have a new folder in here called temp and I just wanted to ignore this temp directory so we can just create files and not worry about them so just say temp Aster ignore everything within temp so but temp will will still actually be there and I mean we could just make it so it's txt or Json I'm going to make it Json here and then we'll just ignore anything that's Json in here that way I can make a new um keep file in here and keep that directory around and it doesn't it won't go anywhere um so we will go back over to our topic like really doesn't matter we just want to get any of them there and we're going to say it us S3 we're going to pretend that we already have the file here so we'll say copy temp um just to make our lives easier I'm going to drag in the absolute path so I'm just trying to drag in this doke file down here here it is not dragging nope I'll drag out Q then there we go and I'll just grab this path here I was just going to assign it to a variable so our lives is a little bit easier so just say path equals this and then that way we can say dollar sign path forward slash um put quotations around this and this will be Temp and then whatever we want the file to be called so this one will be for topics we'll just say topic Json and then this is going to get copied to our destination bucket we need to know what our bucket name is here um since we have terraform in here we can probably get it from outputs so we can just say uh bucket and I can just say dollar sign parentheses and we'll put in here um uh terraform output bucket name I'm going to double check and make sure that is what we call it so checking our main TF oh that got copied out to the S3 file right so we go to our S3 file here or our bucket file bucket there it is this one's called S3 bucket name so I just want to make sure we get that there I'm going to just test this to make sure it works before I run the script we'll go ahead and paste this in well clear uh terraform copy paste hit enter okay so it does get us our bucket name that's good and so I can just go here and say dollar sign bucket going to wrap this in parentheses just so that we don't have issues here and we want this to go to our topic folder because this is for topic before this happens I want to make sure that we delete it if it's if it's there so what we can do is we can just say we can copy the exact same line and do remove okay because I want a new object creation and so if we want to run this multiple times we can just keep running this and we'll delete it and only run it on that case and uh yeah so this looks good I'm happy with this I mean this doesn't generate any data for it so the assumption is that we already have a file in there um I'm just going to create a file no no we'll just m manually make it here so I'm going to go ahead and copy this and we'll go into q and we'll paste this in here whoops and instead of topic we'll do Q and Q and we'll change this to q and q q and Q great we'll copy this and we'll do this as well so we'll say this will just be function and function function and function and the reason I wanted to do Json files is that um then we can parse it in the function the other ones don't really need it but I'm going to do that anyway so we'll go ahead and say uh in our temp we'll make some files and these are temporary so they're not going to be here if you look at my repo you'll have to create them yourself we'll have function Json we'll have a new one here we'll call this one topic Json we're going to make another one here we're going to call this one um Q Json and we'll put some values in here so for function we'll do cures and this one's going to be hello or just say first name Andrew I think we need to put double quotations around this here last name Brown okay so that'll be that one for that I'm going to go over to Q next oh I no yeah so Q's over here we'll paste something similar here I'm just going to say um uh Priority One we'll say type again I'm just trying to think of something here we'll say cherry uh we'll say name Sun gold select if you don't know what I'm typing here Sun gold select this is a type of cherry tomato it is apparently the tastiest cherry tomato it's an F1 hybrid if you ever have an opportunity to grow it go ahead and grow it you'll be really happy that you did they're really easy to grow to and they grow a lot okay so we have that there for Q for topic we'll paste this in here I'll just say priority email uh example at example.com again I'm just kind of varying the data so that we don't have exact exactly the same thing so now we have all our files let's make all these files executable so that we can run them so we'll say chamod + x uh Bin and then Aster okay and so now all those should be executable we can check that by doing a bin and so we're looking for that X on here see that X so that X is all now applied if we had done this before it wouldn't have been there so now what I want to do is I want to execute each of these and see what happens so we'll do bin Q and it's saying the adus command is not found um we might have had an issue with our install oh it happened here as well there must be something wrong my um my bash script because I must have changed it I I refactored this earlier in a separate video and I didn't double check if it actually worked I mean clearly works here just change these out to Source no I'll leave that alone for now um I'm going go ahead and just run this I guess I mean you shouldn't have to do this it says it's already here and that it was installed so it should actually work okay yeah yeah yeah yes please okay this is not respecting what I'm asking it to do um is eight of us here it is okay so then why is it not over here it is all right whatever um we'll try to run this again terraform commands not found it commands not found why why are they not found they definitely should know where they are so I'm going to just expand this here give me a moment to figure it out okay all right so I did a few sanity checks what I did was I tried to um reload my bash profile but the other thing that I did is I just took this line and put it S3 LS above here and it actually ran so apparently there's something that's messing up and it's probably um this bucket line is what I'm assuming so I'm going to go ahead and I guess change that I'm not exactly sure what's wrong with it though it looks fine to me what if I wrap it I noticed the highlighting just changed there did it we'll go ahead and try this yeah I did look look down here oh no it just temporarily changed hey well we'll try this and see what happens we'll hit enter still says command not found so clearly there's something oh I know what it is you can't name this path path is already an environment variable and and everything uses it to find where things are so we definitely do not want to call this path that's our problem so I'll just say uh work path I guess wow that is a funny one for me okay sometimes you just forget okay and so again hopefully work path is not a reserved path or um function but I want to go back and try Q I want to do function last unknown option S3 terraform for SL whatever I'm noticing that it has um quotations around it so I'm not 100% sure as to why maybe it's because I put those on here like this and uh we're running Q so I'm going to go back here I mean that shouldn't really be the problem let go back and try this again no so it's it's still putting those quotations around it so we'll say terraform output and then we'll say S3 bucket name so I wonder if if there's like more options for terraform output so we'll take terraform output command maybe there's a way we can get it not to have the quotations around it sometimes there's a flag raw that's what I was thinking was probably that raw flag raw maybe two uh maybe it has to appear before that so we'll just say yeah there we go so what we'll do is we'll just say um raw here like that raw here as well raw as well we'll go down and type in clear I'm going to go back and hit up and try to run the que it's looking better right so now it looks actually like the address it's still saying unknown option um and I'm not sure why say I mean it looks correct to me oh well I guess the first thing is that the file doesn't exist so it makes oh you know what it is uh this doesn't make any sense this remove file like what we want to do is do this and it doesn't take two parameters it's cuz I copied and pasted I wasn't really thinking about what I was writing so that's really what the command should look like and you know again we don't need to have those double quotations around there but I'm a bit paranoid so I'm just putting them in there so we'll go to this one and we'll try this again and then this one will be uh q q and then we'll copy this one and we'll paste it in we'll try function function good good good we'll type in clear down below we'll go ahead and write Q again and notice it's deleting if the object's not there it will try to delete it anyway and we uploaded it so how do we know that our event notifications worked I have a feeling that they probably have some way for us to track that I almost feel like you could probably turn that option on um if I click into this is there something for tracking in here there must be right no okay I mean if this is being triggered we might be able to look be a cloud trail let's go take a look not something I've been doing too much in here but we really should be looking at cloud trail as much as we can if we go over to cloud trail it's already turned on automatically and so if we go to our vent history it might show us here I just don't remember how how fast it is um put event notification so there it is that's our event that just happened if I click into it and we can take a look at it um actually no it's I don't think that's our event that's us actually updating it because the way I know this is that if we go here and we look at ad us S3 CLI right and we go into um API here put we'll say notification we have um put bucket notification configuration I mean unless it is let me go back here and check again to uh S3 events put bucket notification maybe it is maybe it is the reason I'm getting confused is because um it looks a lot like it could be the configuration for it and so we're just seeing the configuration change as opposed to it there but what we could do is we could just click back a bit and see what we have and so what I'm looking for here is event for when the notification changed um 1308 this is really hard because it's is not my oh actually it is my local time 13 would be 1 and the time right now is 125 so I don't know how long this will take to update so it might be that um it is happening but we're just not seeing it here yet and there's a delay so I'm not 100% uh is insights in real time that sounds like a real time feature that'd be nice no no it's an analysis tool so again I just wanted to see if there was some way we could see that event happening um but if it's not there we can just go to what we triggered so we triggered I think uh the que if we go back we going just close some of our tabs because this is getting a little bit out of hand all the stuff we have open and yeah I triggered the que so if we go over we can check our Q in sqs and see if there's anything in it so we go here we click back it might show us here messages available six six why is there six messages in Flight all right that's fine um and I want to see are messages so last I remember is that in order to view it should see here pull for messages let's pull ah here they are okay so here we're getting data coming in I'm not sure why we have six of them I guess the thing is that we ran that script multiple times so that actually could be the reason why it's just that it it didn't um let's go ahead and see what happens if we run this again so if we run this again okay and we go back over to here and we pull in the sqs here do we have more now no we do not have more so let's click into them body attributes details Okay click this again well here's another question how many files do we actually have in that bucket right now if we go over to our bucket and we check it into the CU folder I mean we it should only have one file right in fact oh it is literally a file so I wanted it to go into a folder called q and not necessarily trigger on that so I'm not even sure if any of this is right but what I'm really confused about is why is there anything anything even in our Q because I I don't feel like we've actually done anything yet so what I'm going to do is go back to our queue and I want to select all of them and delete them so we go ahead and delete them I want to pull again and just see if there's any here because I don't want anything we're still getting messages all right just give me a second okay all right so what I think is happening is that we actually haven't sent that many but um it looks like maybe S3 event notification is sending more than one uh event and so in in that case you're probably supposed to dup the duplicates there why sqs gets multiples I'm not sure but um I mean that is one consideration here but I did go ahead and delete them I mean I don't know if there's any more coming in I think they're all gone so yeah I just think that we're seeing duplicates because it was sending more than one and that's fine that makes sense so what I want to do is um go back over to S3 and I do want to change our code a little bit because it's not exactly doing what I want so I thought this was going to be in the folder called topic and then we'd have topic Json like this and then if we go here this one would be q q Json and then this one would be function whoops function Json the other thing I want to take a look here is back at our bucket and our filter prefix is this we could put a suffix on here as well again I just wanted to only trigger those Json files so we'll just go here and type in filter suffix and this will just be Json json. Json okay so that makes it a little bit less uh uh you know less ambiguous a very hard way to a hard word to say and we'll go ahead and do auto approve so that's going to go ahead and and update um that so that it's a a little bit less confusing and we'll type in clear and what we'll do is we'll go ahead and run that queue again um so I'm going to go ahead and hit enter and so it's uploaded that that object there and we'll go back over to our sqsq and we will pull for messages and notice again we're getting more than one and there have different sizes uh at least we're not getting like tons but I do think that what we're doing is is it's coming from there I almost kind of wonder if it's actually sending more than one kind of object let's go back and take a look here at this you know what it is it's because we have this wild card on here and we're sending more than one event so that's the probably the reason why because it's sending event data for these specific events and we have this wild card on here and that's our problem so let's go take a look I'm glad that I figured that out we'll go back to our bucket and we're going to go to properties and we'll go take a look at our event notifications and I want to go and edit the queue and let's see we have checkbox so actually has all of them I don't want all of them I just want it on the post so when we create a new one that's confusing because the question is when do we have a post and when do we have a put so when does S3 use put or post post cuz it post usually means a new one and put usually means replacing something takes dat in applies it to a resource identified by the URL here the rules are okay that's not very useful we'll go down below post the response that you get contains the object name this is useful if you upload multiple objects um not very clear but the reason why I'm confused is that if you go to the a CLI I'm not that confused like I know we're going to go ahead and try post first but if you look here it's not like you go post object it's always put object right and so that's why there's a confusion sorry cuz we're not specifying the method so I'm not sure if it wants post or put here no put is first so what I'm going to do I don't know I'm going to take a guess we'll just try to do put I'm going to update this yeah I guess if there's four in there let's go back here for a second we have 1 two 3 four and we go back over to here and if we pull again I mean we have two messages so did it do a put and a post maybe it did both of them all right but we'll go ahead and we'll just stop pulling here and we'll delete them and what we'll do is we'll go back over to uh our code if we can find it and we just want to specify that on the put so I think I I copied it I did good so we'll just do put put and put and so that's going to make things a lot more clear right so we'll go ahead and type in clear I'll go ahead and auto proove that run that again that's going to update very quickly we're going to go back over to our oh it didn't like something so it says creating bucket notification operation the event is not supported for notifications okay um for which one all right let's let's go back and and swap swap this out instead of doing this let's just change them one at a time and see what happens I wasn't expecting that so we'll go here that one changed good now we'll try Q we'll do put we'll try that it must accept that one it must be the lamb that's erroring out excellent let's go take a look here and and uh go look at the function individually and see what it will not allow so we'll go here here into our function we'll hit edit I don't see why not it's right here maybe I introduced a spelling mistake I don't know we'll go ahead and try this again and we'll hit up fine okay so I must have wrote something incorrectly and I just didn't notice it we'll go back over here here and just see what we have for our properties put put put okay great so we'll go back over to to our messages queue and I mean it says one's available but we're going to pull messages here I just want to delete what's ever in the queue okay we'll just delete let's just keep polling and making sure there's nothing else there okay that looks fine so we're going to hit delete and so I'm going to assume that there's now nothing coming in here okay so we really should only expect to get one and I'm pretty sure sqs D dupes them so like it wouldn't show you multiples or it shouldn't at least I don't think so we'll go back over to here and what I want to do is trigger the queue okay so that's uploaded we'll go go back over here and we'll start to pull the messages so now I only have one so that is a lot more manageable and if we click into here we can see the body of it so it says event version Event Source S3 CA Central 1 the name what it triggered on put and looks like we get more information so we can we have the response and it tells us the key so basically you know if we're programmatically working with this we'd want to pull that information let's go ahead and do that right now um so I would probably want to use code to do that with sqs so what I'm going to do is just make a new uh file here I'm just trying to think about this yeah we'll use Ruby because it'll be really easy so I'm just going to make a um a new folder here I'm just going to say um code and we're going to make a new file and we're just going to say q. RB and then we're going to CD into that folder LS clear and I'm going to say bundle AIT that's going to just initialize a gum file in here and then from here we will do gem uh ads SDK sqs so that we can pull the queue I'm going to also bring in pry in case we want to do some um debugging there and we'll do a bundle install to pull those gems down great so now we pull down those gems in our Q file we can go find the code for this we'll say sqs sqs Ruby um AWS and we'll go into our client and in here we should be able to pull messages right and so somewhere in here we we can list our cues not exactly what I want to do but I want to pull or uh Peak or look at stuff so I'm going to type the word peak in first I don't see that there somewhere here we should be able to pull it pull whoops refresh this here I don't know why I fully selected their pull receive message that's what it's called here okay so we'll go back to the top here receives one or more messages there could also be one where like you can send messages we don't want to do that but we can also just look at the queue or peek on uh peek onto it um there should be one in here all right well I don't see it that's totally fine but we'll click into here and so we have some code I'm go ahead and grab this code I'm going to bring it over to our Ruby file paste that down in we also need our client at the top so we'll go all the way the top here grab this client okay and we'll paste this in here and we'll just say require ads SDK um S3 or sorry not S3 sqs and then also say say require pry so I can do pry we don't need to specify any of this because it's going to pick it up in our environment variables your environment variables should be set um for your AIS credentials or you need to do some additional configuration here that's up to you to do there's a few things in here like we need our Q URL so what we can do is update our terraform to provide that so under our um code here we should have our q and then we can speci specify that output I don't remember how to write the output so I'll go here the bucket and grab it from here and we'll paste it in below and then I'll just close that Tab out so it's a little bit less messy and we'll reopen the Q do PI or yeah where is it q dotf and in here I want to get our Q URL because I assume that we can get it from here we'll have to go back over to the terraform registry and we'll click into adabs and from there we want to go to the documentation and we want to find Q so somewhere here this should be for sqs there it is I'm going to go down to attribute reference and we can get a URL that that's how we can get it great it says it's the same as the ID so that's great I'm going to go up and find our Q so it looks like that just going to copy that line here so it's a bit easier do q. ID okay and so now we can do a terraform apply Auto approve it's not going to change anything it's just going to give us that output um terraform plan let's see if that does anything oh you know what I'm in the wrong directory that's why so we'll say terraform uh apply Auto approve and then that way we can get the uh the Q so there's our Q URL it'd be really nice to programmatically grab it from uh in here in Ruby we could probably do that using the um uh system command so if we type in system there's lots of ways of doing this but I'm going to just try it this way really quickly here and see if it works so we have terraform output raw and then this would be Q URL let's see what that produces so that does work can I assign that to a variable I mean great I don't want the equals part uh let's say uh get output from system command in Ruby there's a lot of ways to do it in Ruby I mean you can use back tick that's what I was thinking so let's give that a go there's a lot of ways there's proper ways but I'm just trying to get it to work this might not be thread safe but we really don't care right now so that's going to work for us so we'll go ahead and copy that like that and so we'll just say Q URL it's really good to know how to integrate things across stuff so we'll say Q URL so that will get us that there um here it says ql attribute names we want all the attributes message attribute names I'm not sure what that is we'll go back over to here and take a look so we'll go down to receive message and all values the name of the message attribute where n is the index the name contains alph characters this is K sensitive they may must not start with this I mean like I just want all of them I don't want to necessarily exclude particular ones so I don't think we need this so I'm going to comment this out and then it says max number of messages um 1 to 10 I'm going to just take as many as I can get so we we'll pull 10 at a time if we can visibility timeout um the duration in seconds that it should wait I mean this kind of matters if you are iteration the received message are hidden from the subsequent requests after being retrieved so other people can't do it I think a lower number is fine in this case it really depends if you're working with different programs but um there's that the duration in seconds for which calls waits for a message to arrive in the queue so I don't think it would make sense to have the visibility lower than the weight in seconds we'll leave them as 1 second we'll see if that's okay I mean we could increase it to two seconds if we really wanted to let's just do that we'll just make it two seconds or even like 3 seconds receive request attempt ID what's that the parameter applies only for first in first out I don't think this is a first in first out Q because we didn't specify it as that so we'll just take that out of here and so we're going to get a response let's go take a look at what the response could be usually they they'll show an example response down below and I'm not seeing one so we're going to have to use a binding pry and then just figure it out from there they really should show that but you know again it depends on how well they maintain their um their code base so we're going to go ahead and type clear and so now we have our little program that can pull from Q um so what I want to do is run this and see what we get so I'm going to go into this directory here code and we're going to run bundle exec Ruby qrb and hit enter and it has a syntax error here uh it's because we're missing a comma we'll try this again we have another error that's totally fine every single time it gets me we got to include noiri NOCO giri or ox or something um NOCO giri is standard if you find it's taking forever to install you can install ax instead but it's just an XML parser a lot of libraries use it so it's kind of a default thing that should be there I have to do a bundle install after that it's airing out on the same problem and then we'll go ahead and run this again it was really fast install on this because they already have it preloaded because it takes so long to install U must be in a asky only so it's saying it doesn't like uh it there I'm going to go ahead and copy this and paste above let's go take a look and inspect its value so we go here and we say Q URL and we hit enter wow that's a mess it says no output fan found no outputs found okay so why not if I attempt this again do I get the output maybe because it can't run it where it is right so it doesn't actually know where the path is that's probably the reason why a really simple fix is to just bring this whole path in here so we just drag um any file that's here here and then just kind of pair it down so we could do this and then we could try this and see if this works unless there's a way to specify where to execute the command I'm not really sure um how to execute a command in a specific directory for back tick Ruby and then they might tell us to do something else yeah so this is the thing that we could have used which was pop three and uh I didn't um but we'll see if this works maybe this will just work if we do it like that it's kind of like a weird hack but if it works it works right um because it's not the name of the file I wonder if we could do this no that's messed up another thing we could do is if we just want to pass it along we can grab it via the environment variable so we could just do this EnV Q URL and I'm just going to write the command that we' actually write so we do um Q URL whoops I don't want to get rid of all of that we'll go here and just say Q URL equals and then we'll do this like that and then what we can do is bundle exec Ruby uh q. RB we'll comment that out and so the idea is that now we can just use this command when we need it and then this will do the same thing it's still having an issue so we'll put a binding pry in here and well I guess the other issue though is that again this is not in that directory right so could we can we do this like can I this let's see if I do this here oh my goodness um how do we execute a command in a different directory without changing directories make a subshell for an nbar okay well it seems to suggest that um I mean can we do that let's find out I guess I'm going to just run it here uh uh Loosely first I'm just going to paste this in here and I'm just going to what does it say end so we would say CD I'm going to bring that full path in here again what a pain in the butt I didn't want to open that file it was an accident um I want to go down here and grab this path and we'll grab all of this we'll paste it in okay and so now we have this so what happens if I do this if I run it here I just want to make sure it doesn't change my directory and what if I just do echo on this okay so it actually is doing it correctly so what we can do is clear this out now my next question is if I can do that why can't I just then go back to the back tis here and do that because it's in a subshell right so if I do this it can't change directories right I still put a dollar sign on it and it still printed out I think so let's try this and you know again I just would rather do this and not have to pass it as an Nar just save me some trouble um so we'll try this again H URL no luck what if I just take it out like this this work copy the whole line we'll paste it in that works so we'll just do that then who cares whether you want to do Nars or or do that it's up to you um of course hard coding values is not always the best idea but I do not care okay like it it just has to do with your use case right so it's not always uh considered bad so we're going to type in response and so now we can see our information we are definitely getting information um we have messages so I probably can do response messages right and I could probably say size of messages we have so here I'm just going to say response messages size is greater than one else we can just say raise no question no messages found and as long as we actually we can make this a bit nicer we just say unless or say if size is zero okay we'll do this instead and then we can just even make this one line that way that way anything below this will just execute so if it is zero then just tell us that we found no messages so we'll say RSP messages each do we can iterate through each message and so each message has a body that body is um looks like it's Json so we're probably going to have to require Jon up here it's a standard Library so we don't have to include it in our uh bundle install or our or gem file and so I imagine we'll have to parse it so I'm going to put a binding pry here and let's take a look at what we get for our message assuming we still get the message because we pulled it uh we'll go ahead and do that undefined message here we need to call this messages we'll type in clear we'll hit up okay we'll type in message good and so I want to say message body great and so it is it is Json so we'll have to parse it so we'll say json.parse message body and so we're getting information Jason parse message body we'll say data and so the idea here in the data is that we want to get specific information so I what I really want here just assign that to data here and uh so if I have data I want to try to get this particular part of the record so I think if we if we do Keys we can kind of see what we're looking at here so we have records it just loves to wrap things uh around stuff like that so now I'm getting closer to I'm going to say keys again see what keys are on that um size so there's one record so I'm just going to say first to get the first record so say data records first I mean there could be more than one so we could do this as well and say do this and say uh record like this but we pretend that we are we already have that one and so then in here I'm hoping that we have keys we do and so now we can get S3 so now I can say record S3 I don't mean to say keys here we'll just do square and so now we can see our bucket and our object so this is providing better information so I definitely want the bucket S3 bucket good and then from here we can get the bucket name so I want the bucket name and then we'll back up here and then I want the object I imagine it's going to be object key probably here object key exactly so we can go here and say object key right and um the idea is that I want to be able to download that file so we would probably have to go back to our gem file and then go add adabs SDK S3 we'll go ahead and type in exit we'll type in clear we'll do a bundle install going say Andrew do we really really need to do all this stuff to learn event notifications yes because this is why you'd be using it to integrate it programmatically so I just want to repeat here keep sticking with it because this is super useful for you it's super useful for multiple certifications uh and it's totally pointless to learn this if you don't know how to integrate it into something okay so want to point that out because the video is 100 uh sorry an hour an hour and 30 minutes in and we're we still got more to go here so the idea is that we've installed the ads SDK S3 I'm going to go to look up ads s SDK S3 uh or SDK here and uh we want Ruby and I'm going to go here I'm going to grab this for S3 right so we'll go ahead and grab that and I'm going to go back over to my Q code and I need another client you should create clients all at the top here together so just take this one out this one's going to be S3 client so it's not less confusing this can one can be sqs client okay and so we'll have S3 client and what I want to do here is just get an object I'm going to assume there's one here to get object there we are and if we go down here below it's as simple as this I'm going to grab both the response as well so I can see what I'm looking at and in here we'll go ahead and paste this in as such and now we have our bucket name and our object key I'm just going to bring this onto a single line so we can keep our code a little bit more condensed notice so we have a response up here we have a response down here so I'm going just say S3 response so we don't get mixed up and in here what we want want is the actual body or data of the actual content um so what I'm going to do is put a binding pry here and see if we can kind of play around with that to get it so I'm not exactly sure what command we need to use for this so I'm going to go ahead and type in clear we'll run this again I'm going also take this out of here I think we need to do a bundle install here a bundle install and I'm going to go hit up and we're going to run this queue again doesn't like it probably because I forgot to include it up here so we'll go ahead and do that we will save we will clear we will go ahead and hit up we'll try this again we have client so we're getting closer remember we renamed it to S re client so we'll have to do that we'll save this we'll type in clear we'll go up we'll hit enter there we go let's look at the S S3 response and so it's on body so does the body come back formatted sus sdks will do that for us so it's a string iio and so for that we type in string to get it um and then we'd have to do ad json.parse because it's not going to be able to make sense of it any other way here and so then we can say our actual data that we get from it okay and so I'm going to go here and I'll say our actual S3 data and then I want to just put it and I'm just going to put the data here and so that's how we would programmatically work with it now the thing is we keep receiving the message but to get rid of it in the queue we're going to have to delete it once we receive it so probably over here there's another command to do that so receive messages um what I'm thinking here is we probably would delete the message or we have to mark it as received give me a second to find out okay all right so my guess was correct it looks like we just have to delete it the question or the reason I was confused was because I couldn't see um that delete in there and I thought that there would be one here so like if we go back up to uh the que here sorry to here I wasn't seeing that delete message and actually now is here so that is a little bit less confusing so what we'll do is we'll copy that code in here and uh if we've successfully received it right we can go ahead and say paste and at this point we can go ahead and delete that message so we still need that Q URL which we have and then there's that receipt handle so I think in this uh we get a receipt handle but where it is I'm not sure so let's go back over to our message here and we'll say receive um going go all the way back to the top I can't spell the word receive so I have to kind of search for it here like a dummy um and so what I'm looking for is that receipt handle right so in here it's on the messages like that okay um so the way we'll do the way we'll get that is we'll have to put in message receipt handle okay and the idea is that this will delete the message and that will continue or complete um this portion of our code clear so I going go ahead and run this um we have one little issue here this supposed to be a comma here so we'll do that we'll type in clear we'll hit enter client delete message uh we have to make sure we name it Q client because I think we changed that to or sqs client there y sqs we will type in clear we'll hit enter and go up and there we go so the idea is now if we run this again we shouldn't see it right so notice that it's hanging no messages found in the runtime uh that triggered on this line here because it found nothing and it pulled for a while it actually pulled for the full 3 seconds and it didn't get anything so it said wait till 3 seconds we have nothing so that part of our code is now implemented that's really great um let's go take a look at topic so that one should be easier to do so we'll go ahead and type in topic RB uh we're going to go back to our gem file and we'll update and we'll say gem we'll say SNS so now we can use that we'll do bundle install to bring that in place and well I'm just starting to think about this for a second because the way SNS works is that it has a topic and then it can send it to an endpoint and for this to work we'd actually have to have a server to receive that SNS endpoint right um and so the way we would do that is we need to have a public running server so we could do that in Ruby really easily um and we could use Sinatra so we'll say write a simple Sinatra app uh I don't think we really need to do that we just get a Sinatra example so say Sinatra Ruby example and we'll get a very simple endpoint I mean that's as simple as it gets um say write a simple Sinatra app that is going to accept an htb endpoint that will pass uh pass it J application Json data and then decode the data okay and I do have code lying around but if we can just write it for me really quickly here that would be really nice as that is generating out uh let's go take a look and see what we need to do to um get our SNS working so go over to SNS and the one thing that matters here is uh when we have a topic we actually have to subscribe to it so see there are no subscriptions so let's go to terraform and see if we can do that there might be some way to create a subscription I'm not sure okay so we'll have to look this up we'll say um create an SNS subscription using terraform whether you should do this or not is another question but uh oh okay so there was one here I'm not sure why we didn't see it before oh it's right there okay provides a resource for subscribing to SNS requires the SNS topic to exist that sounds fine to me so you can directly Supply topic and the AR by hand in the topic AR so we can do it that way alternative you can you can use an AR property as the SNS topic and sqsq we're not using sqsq or we just want to um provide it to an endpoint we have a lot of stuff here what I'm going to pull out is maybe the top example here and then we're going to go back over to our code um and this is going to be in our teror form so we'll go down to our q no not our Q our topic and in here we'll Supply this subscription so go ahead and paste that in and I'm just going to say default here to make our lives a little bit easier um I don't understand why we can't just reference the AR here unless it doesn't provide us an AR let's go take a look here we'll go back over to here and I'm going to click into SNS topic we'll go down to attribute reference it does have an R so we could just do that so let's just do that instead I'm not sure why they don't do that in their example there could be a reason why adabs um SNS topic topic. AR the protocol we want to use is htpp because we don't want to send it to sqs or somewhere else I mean we technically could if we really wanted to but that seems a little bit silly um we'll go back over to our here and we'll go back to the subscription and I want to take a look and make sure that we know exactly what input we can provide here so the protocol has sqs SMS Lambda fire hose application we have a lot of options here HTTP and HPS are also available there as well uh I don't know if we want HTTP or HPS it really depends if uh this is on a secur link it is so I'm thinking that we want to supply https here then and then there's the actual endpoint um I'm not sure if that parameter changes based on what we input here so we have to double check so we'll go down below here so it depends on what protocol you use right delivers adjacent encoded message endpoint is the endpoint Arn of the mobile app or device okay so we'll go down below here to application I'm not 100% sure I'm going to go search here for an example they don't really seem to have one here it also says application is an option so I'm a little bit confused so what I what it says application I think what it's talking about here is not um a webpoint endpoint and it's actually talking about a um um an actual application application so I don't think we should have application we should definitely stick with htps but do we just specify the endpoint the reason I'm confused is that I'm not seeing uh the endpoint attribute here oh it's right here at the top okay so we do need an endpoint and the other thing is that we want a confirmation so in here somewhere whether the endpoint is capable of autoc confirming the description and I mean I'd like it to autoc confirm so let's go ahead and see if we can do that so I'm going to put this here and try to put this to true and then this is going to be whatever our endpoint is um it's going to be whatever our URL for our app is so let's go ahead and get our Sinatra app going here and we have a very simple example let's hope that this code just works if you're watching this video you don't have to write this code just go to the repo and grab it and paste it in here to save yourself some time so we're going to go over to a topic we're going to paste this in and we have our simple end point here which is for data um this is going to take it bring in data it's going to receive it it's going to print it out so to run this we can run bundle install first do we have to also install Sinatra I do believe we have to so we'll go ahead and do that we'll say gem senatra just give me a moment here I'll be back in just a second sorry I'm back I was just super hungry so I just had to go eat something but anyway uh we're coming back to this and uh the idea is that I want to run this Sinatra application so that it has a public URL now if you're doing this in a local developer environment this is not going to work easily because it's not going to have a public URL for the subscription to hit you'd have to use a tool like um grock iio and actually this is a really good service and I do pay for it um and grock and grock IO which what it can do is it can forward a local address like a local um uh program you're running and expose it to like a a actual address on the internet but um I don't need to do that because I'm already using git pod and another advantage of using Cloud developer environments anyway um so before we can finish this configuration we have to have an endpoint here and um we did a bundle install let's go ahead and try to run our little simple Sinatra server so if we go over to our topic here and we type in bundle ex Ruby topic RB oh it's saying we don't have installed we'll try to do bundle install I thought we just did that did we not did we not add it to the gem file here I did did I not save that file I'm pretty certain I did try this again bundle install I even forgot the L on the end of it and still worked and it wants one of these so we'll just say gem web brick that's built into uh that's built into Ruby or should be anyway I'll put puma in because it's better we'll put pum in to you this is all the same but I'll just put that in there and we'll say bundle install and so that will install Puma hopefully that doesn't take too long Puma is the web server and then uh the framework is called Sinatra every kind of programming language has their own little micro framework I'm just using this so if we say make public this is now open on Port 4567 so I would think oh the only issue here is that since this is on Port 4567 oh no this is fine because we get an absolute URL here so even though it say it's running 4567 it's actually running on 443 because we get this full address and the idea is that we want to utilize this data end point so I'm going to grab this whole thing here and the reason this doesn't work is that it's expecting probably a different protocol so if we go back over to our topic RB uh this is for a post and this is not a post right so um this is going to be the URL and if we restart this giod environment this value could totally change so just understand that if I want this not to be so variable I could grab the um uh the information out of um the envs here so I could do this and say grap and say itus examples like that and I could get the git G git pod workspace URL here and I could technically Supply it it's a little bit different though because it has this 467 in here um so I could do that the other part of it is I could grab where it says W uh wsus s107 maybe there's a separate one a variable for that there is so I what I can do is I can grab this environment variable and I can I was trying to think like I was thinking this was Ruby here for a second but actually you know what it's just going to be easier to copy paste this every time when we make a change so I'm not really going to worry about that um but anyway let's say we wanted to actually test this to see if it's working we can do a curl and um let's just go ask chat PT do that so write a curl that is a post that passes some Json and it it content type is uh application Json which is what we expect it to be so we'll go ahead and do that so that should be an example that we can use it's pretty straightforward it's just I cannot memorize curls to save my life so we'll go ahead and paste that in here so we have an example of it and so this is kind of an example of a curl that we want you obviously have to have curl installed here but we'll go ahead and hit enter we're getting a 40 a 401 or 403 because we actually have to utilize the correct address so I'm going to go ahead and copy this we can make this a bin script we'll just say like uh test server and then we can make this a little bit more uh flexible let's go ahead and copy this and I'll paste that in here because we should make sure our server Works before we attempt to use it otherwise we'll be scratching our heads and not knowing where the problem is so I'm going to assign this to a variable up here and uh actually this is a case where we can bring in those environment variables remember we had this we could technically take the host name here and replace it here say host name whoops host name um I'm going to go with Git pod workspace ID again this is only working for me because I'm on git pod if you're using something else you're just going to have to figure it out that is what you get for not using the same thing as I'm using and uh what was the other one called it was like EnV GP and that was us 107 so that's that cluster hose and so I'm going to replace that like that and then I'll put a dollar sign on it here I remember it doesn't really like it when we have these periods and stuff it can sometimes mess up I have to go do a test here and see what happens so I'm going to go and do an export on this whoops clear and so we'll just copy this and we'll say I didn't want to type export I meant to say Echo I just want to make sure that it's printing it out notice that it is uh it's correct yep that's good okay so now we have our URL and our it will go to the data endpoints and now I can just pass it in here instead like this un comment that I'm going to do a mod on our bin test server here we'll have to do U plus X on that uh this is in our um S3 event notifications directory we'll cha mod that again + x bin test server and we'll type in clear and so now I should be able to test our running server I believe it's still running if we go back over here it's on 4567 it's public it's open so I'm going to do this and see what happens and notice it says data received in process so that is what should be returned back here so that's how we know that it is working so with this SNS topic the idea is that we are going to um get the data right and we want to provide this endpoint over to our topic here because it needs to in order for it to register it's going to send um a request and it needs to get a request back for this to confirm uh HTTP confirm request uh SNS because it might need something in particular before you configure um your pH Cod needs to create a header called message type from the request process it is based on okay and so now so they're talking about PHP this person's showing an example looks like in NOS or JavaScript you can obviously have different variants here um so in this one it is confirming so we have this for sler post and then it's checking the payload type for subscription confir confirmation so uh can our notra app confirm a subscription for SNS S3 SNS maybe that'll just save us some time if we just ask it to do it for us okay yep that's great just show us the code now I don't know if it actually has to go to SNS set up a route in your Sinatra app extract The Thing If the message type is subscription uh extract the subscription URL but does it have to go to the same place because this one here is just going to rout it's not going to like a different endpoint so I would think that we' have to do that do we have really have to uh include net HTTP I'm not really sure if we really need to do that but I'll put it in anyway I believe that is a builtin Library so we'll put that in there and we'll see if we can incorporate this code again you know you don't have to uh watch this you can just copy paste it I have no idea what rewind does I've never seen that before and we'll just go ahead and do this say body to rewind it is parsing the data there and then we have oh using that ACP here get response all right sure why not and I'm going to copy this here and we will go and paste it in here now is that in the actual oh it is in the body okay great so I was just trying to decide where to put this code and so it seems like we really should first put this here like that and the thing is is that we really only need to uh confirm the receival of data here going to puts that so we can see what we're getting back actually it's not puts it's just what's going to get returned and this one's going to be the notification so the notification is we receive the message and then this one is we confirmed the subscription um this one will just will raise an error saying unknown type and down below here we got the URI so that's what it returns it returns a response back to that URI I guess that's how we have to do for Sinatra in other apps it's a lot more um seamless but that's just what it is so that code looks okay to me I'm going to go ahead and stop our server wherever that's running it's over here I did control C to stop that and go back and hit up um notice that I don't have to use any kind of library in here so I don't have to use SNS to confirm it even though we did include that Library there could be something in the comod uh the code to confirm it um but I think when you're doing HTP you should really confirm that way so we really didn't have to install the SNS Library into our gem file that wasn't useful anyway um so here it's really dependent on the SNS message type so so our curl is not going to work anymore because it needs to actually pass a subscription type so we'll go back to CHP update our curl command to pass the uh the subscription the expected header for the SNS message so now not really sure here because we have SNS message and it has the type here I thought it was going to use the header though if we go back over to here and the documentation no it's in the payload it says payload types description conf confirmation so it's there right but what about the actual payload that's what I'm not 100% sure about so what we'll do is we'll require pry in here I just really don't know what we're going to get here and I want to see what we're getting so I'll put a blinding pry here hopefully it'll work in a in a in that begin and stop and I got to stop this and restart it and so now let's go back over to our terraform code uh to our topic and now we can try to provision this because we have all the information to do it and this should try to autoc confirm it and we need to go to some other tab here because we're kind of running out of tabs and we're going to go into our event not or S3 our event notifications we'll do an LS and do a clear here and I'm going to go ahead and try and run this we'll say apply Auto approve clear terraform apply Auto approve holy smies okay let's try this again terraform great terraform apply hyphen hyphen Auto approve it's going onto a new line and I have no idea why it's just mucking up it's really annoying but it's working so that's fine and we're changing some things there so we're just waiting it's creating I'm not sure if it's attempting to try to confirm so that might be uh the issue we're running into here oh you know what we have a binding pry here so it actually is working we go here and look at the data now and so that's how it comes in so I'll just type in exit here exit I kind of forgot that it was doing that so it was attempting to uh confirm it if we look at our data we have type subscription or message ID or token uh the actual message so that's just for confirming though undefined local method me message SNS message um so I think it's because uh this is called Data so we'll just have to change this to data and maybe it'll try it again um our server is not running anymore so we'll have to just take this out here because it's going to keep trying a I got type exit here we'll clear and we'll go back and hit up we'll try this again because it might be still attempting so it actually failed and it stopped so we'll go ahead and we'll hit up and we'll try this again so this is starting this up and we should expect this to complain it actually is going very quickly here we got a post it returned back a 200 so I'm assuming that the subscription worked here and it it fired this one here we'll go back over to here and looks like it created so it was able to confirm it so our code is working that's really cool so now the next question is if we are to uh push our code will we get a response back from here so we'll go ahead and run our bin topic we'll hit enter we still have a a small little issue here it must have some old code um as it's complaining we must have changed something here let's go and compare um it looks fine to me terraform command not found on line four here what is wrong with it so Q definitely works I'm going to copy these lines and I'm going to paste these lines in work path this one still has path in it okay I thought we updated these maybe it didn't save all right and we'll go back and hit up and so notice that it created the object we're going to go back over to this tab and it looks like another post came in right so that's good it didn't print it out and I I think it did work the reason it didn't print it out is because um this is overtaking uh everything that we want to see here so if we really want to uh get that output we could just write it to a file I suppose um so I'm going to go into Catra like because we don't have any logs right so in this Essence we're going to just create a log file like a really simple dumb log file and what I want to do is I want to dump the Json to a file so after we get the data received which is not printing to the screen we can just dump it to a file so I'll say file. openen uh we'll say write a ruby file and just get some quick code here I'm looking for the block this is a really good example I'm going to copy this one here and I'll just go ahead and P paste this here like this and all I want to do is write the contents of this so we'll say data so we can see all of the data right and this is Json data so I'll just say Json and you know again to make this really easy I'm just going to drag out um a path so we have this folder here I'm just going to drag out any one of these files we'll grab this here copy it we'll paste it in here and so that will get us our log file for here okay so it should write the contents there so we'll restart our server we'll go back to another tab we'll run it again whoops unknown type so now we're getting an unknown type from the server unknown type I'm not sure why we didn't have that before if we get an unknown type let's just go here and take a look so I'll try this again oh sorry this is for testing our server we don't want to test our server we just want to um we just want to run that uh script the uh I think we had it over here that's why so we'll go to bin topic so that's going to trigger it we'll go back over to here and we got a post I think that's an old error so that doesn't matter I think this one worked we have a log Json file so that's not something we created it's um or I mean like uh it was created by our server is what I'm trying to say I'd be nice if this is a bit prettier it's really hard to make sense of this file I'm going to go ahead and delete this and we're going to tell it to pretty print it so we can do that by parsing the Json so I mean the Jason's already been parsed but it's like Json pretty generate I think Json pretty Ruby uh there it is pretty generate so that's that's what we want so we'll go here and we'll go back and I'm just going to go ahead and do this Json again so we're sending it we're turning it back into Json and then we'll write it to the file and this will make things a lot easier for us to read so I'm going to stop the server restart it wait for it to restart I think it's running now good we're going to go back here hit enter okay and we'll go back over to here and we'll now check our new log file and now it's a lot easier to read and the reason why we want to do that is we want to see where the data is and we can see it right here in the messages so if we want to work with this we would then have to parse this part out right and somewhere in here it has the key right right there okay so I'm not going to go through all that bother there we've done enough with this it was more so just how would you get to this point you kind of get that idea let's go take it our Lambda I don't think we're going to do event Bridge uh this time around because it's going to take too long but let's and we obviously have event bridge for separate section so we can do it there but let's finish off our Lambda function as that one is already set up it might just already work um or we might just have to tweak the code a little bit so what we'll do is we'll go ahead and Trigger it and see what happens so we'll go ahead and type in clear and we'll do bin or sorry bin and this one will be for function it'll hit enter stilling the file creating the file so now we have to go over to our uh Lambda and we'll take a look at that so go here and we'll look up Lambda and we'll go into our Lambda and in Lambda what we'll have is we'll have the option to look at logs so this should have logging built into it they changing the interface on me so I used to know it was right here now it's over here under Monitor and we'll click into view cloudwatch logs and so if it was triggered it will show some logging information here we'll give it a moment to load it says a log group does not exist that's not good because if there's no log group then that means nothing has been triggered okay so we really do want this to trigger to finish off our uh application integration stuff here so I'm going to refresh here I'm going to carefully look and see if it maybe reappears now because it could have been a delay no there's no delay it's just not being triggered I don't think um but you can see it has an incoming trigger from here we can click into it and see what it is this is going to be when a it's on the function and there's a prefix of Json so if it's not working we should double check our repo and make sure that file is there because maybe it's not there notice we if we refresh we have function Q topic we'll click into function FN u c t i o n Json so that is appearing there so that is good um but I guess the question is why does the Lambda not trigger we go back to monitor here and and take another look here your function does not have permission to write to cloudwatch log so that's our problem so the thing that they gave us does not even have the most basic permission so we're going to have to go back to our Lambda here and let's take a look at the permissions we got and our permissions are where this is for the assume roll which is fine I am roll I am for Lambda we can this can invoke the function and it's this one up here so this one doesn't have uh the ability to to do that so let's let's go over to Lambda and see if we can just find that code here as they should provide the example so what I'm looking for here is a more verbose example of the IM IM rule okay so I'm carefully looking through this file and this is what we want is the cloudwatch logging permissions so here we have our our a variable name we have a test Lambda and we have cloudwatch log group over here and then we have Lambda logs so this one actually creates a log for us and then we have Lambda log here and then we have our IM policy so it has all our code here so the first thing that we want is we want to attach um so we need this one but we need to also attach a policy so I'm just carefully looking at this here Lambda logging yeah we'll go ahead and grab all of this code here up to here and we'll copy this and we'll go make our way back over to uh git pod or Cloud developer environment and I'm going to scroll all the way down to the bottom I'm just going to make a marker here so I can keep track of what I'm doing might get a little bit messy and so that's where our marker was I'm just not confident with our marker so I'm just going to go ahead and put three lines here um this is optionally to manage the cloudwatch log groups for the Lambda function if skipping this resource configuration also log group ion policy below um I guess I don't think that we need to do that it should automatically create the log group but I'm not sure if that gets referenced anywhere here it's also saying this this is optional yeah I'm going to take this out I don't think we need that so we have it policy document Lambda logging so we'll go up to this one I'm just carefully looking I'm just seeing if I can merge it into the top one here this one's Lambda logging this is I am for Lambda why does this one not have the same stuff name assume roll policy give me a second to think about this okay so here's where my confusion occurs so we have the Lambda function and this is getting signed a direct role in our example and their examp example they don't assign a role whatsoever to it it's saying it's depending on these other ones so to me it's my assumption that one of these is attaching to Lambda but when I look through here I'm not exactly seeing that attachment happening and that's where the confusion comes from so you know we can see that we again we have these L like we have a policy document we have a policy we have a ro policy attachment but how does it actually associate with that um Lambda and that's where my confusion comes from so you know I'm just going to kind of ignore what's going on here and I'm going to have to just kind of actually go ahead and we're going to have to go ahead and just delete this part and try to modify and look up how this implementation works it's just because we have poor examples to work with um so what I want to do is look up the IM rule so that's what I want to look up here so let's go look that up in terraform so let's say terraform registry I am rule as this seems to be what's going to help us out and here we have the assum roll policy that's fine and I just want to see if we have both of there yeah so here it's getting the assal policy and then we could just have our inline policy so I think that's kind of what I want to do I just want to grab this inline policy and make our life a lot easier um it looks like we can also do the other one in line as well so like if we go back over to this one notice that the sum rule is also in line it just makes it a lot less confusing when they're not broken up into separate ones there's really no reason for us to do that we don't need that level of modularity for this so we going to go here and take a look and see if we can um copy this over so this should be here we'll grab that put that down here for a second let's go back and check um I mean it's really similar I'll go grab this here we have to kind of modify this a bit I know it's getting a little bit messy but there's really no other way around this um so that's fine the statement here is fine the assume roll is fine that's the action the allow is correct I don't care about Sid we don't have to do anything for that the only thing that's different here is the principal so we have the principal um we are doing a service and it's identifier would be Lambda so I think this would be the equivalent for this again I'm guessing here but I'm pretty sure this is what we need to do here I'm going to take this one out here and so to me that's a lot more clear okay um and then we have our inline policy down below here and in this and we can bring in the permissions that we want to have so we saw this for here that we need these three so we'll go ahead and grab those and I'm just going to go and paste it in as such can just bring this into multiline here okay and again just carefully looking at this um and we'll go back over to here here we're saying for any resources like that so I mean I mean that kind of makes more sense probably should just pair it down to logs here this will be um AR adus logs colon Aster colon Aster colon Aster so three of those basically for all logs so it's really flexible there um we'll go back over to here that is set so I think that's all we really needed in this here okay so I think this will fix it I know that was really complicated the thing that's really frustrating me is how many of these we have uh because they can't really keep track of these very easily so I might go here and just say like resource this one's inline policy just so that we can kind of keep track a little bit better here this is statement this is uh Jason en code let say policy equals Jason and code there or just Jason and code and that way we're not going to get mixed up and see think we're missing anything so I'm going to go ahead and type in clear and let's go back to our terraform now we don't need this server anymore so I'm just going to stop it typ in clear I'm going to make sure I'm in the right directory and we'll do terraform init we didn't change anything but I'm just out of habit I'm going to do it we'll do terraform plan and we'll see if we wrote Our terraform correctly to um take that in the uptick it's not complaining about anything that is good we're going to do a terraform apply Auto approve and we'll send that up there great so no problems there thus far um let's go ahead and take a look at our Lambda let's make sure that our permissions actually are what we think they are so in here we should be able to see that now we were getting that warning right here right and that warning is now gone at least it looks like that's the case somewhere here we should be able to check our configuration and look at our permissions so if we go here now we actually have those permissions in place all right so what I'm going to do is I'm going to go back over to our Cloud developer environment and we're going to go back and attempt to trigger that function again so deleting the file creating the file we're going to go back over to our function code here we'll go to Monitor and then I'm going to open up view cloudwatch logs and fingers cross so we get some logs here please please please please please please please please please we got some logs that's excellent we're g to click into here and notice we have an error it says key name first first name not found that's because our function isn't really that useful um the idea is that we are just accepting an event payload from S3 event notifications so what I'm going to do is go to chat gbt I'm going to supply it our code I'm just going to ask it to rewrite our function uh so I have I was going to say um can you write a S3 python Lambda function that is going to receive event data from S3 event notifications and just print the Json result of the events okay I was going to supply our code but I think it probably can provide us something very simple yeah and that is the simplest code you can get yep that that will work that looks good so we'll go ahead and we'll copy that I don't know if we have to have a requirements for Json we'll say does Json require to be included in requirements.txt or is this something that is built into python cuz I really can't remember we'll give it a moment to stop writing for a second there we go part of the standard Library okay great that's all I wanted to know so we don't have to do anything extra we'll go back over to our code we'll go into our can you feel we're almost done right we're we're we're at the home stretch here I'm going to delete this function file we're going to go into our function Pi we're going to replace this code here I'm just want to make sure that we know that this is running so I'm just going to put a print here we're just put something fun so we just do like this or we'll just say uh hello like that we'll save that so we have our new code here let's see if terraform can pick up that change no changes so it's saying that there are no changes but the thing is is that there actually has been a change because we've changed the code here um Force change update for terraform for archive archive file and that's probably the reason why it doesn't know that it's changed I assume that you're a case this we've all experienced this issue here I'm just looking for a workaround that's fine uh if you don't know workaround I suggest one of the followings um it looks like they're trying to use a random ID there's in in terraform there's the idea like um there's null resource and another one there where we can force a change but I'm not sure how much that's going to help so I'm just trying to think about how we can um force that change there and all they're doing is they're just randomizing the value so we could do that we could use random uid and then we could append it to the name uh when we archive it and then we could just specify it there and I mean that could help Force the change so I'm just thinking about this here for a second I mean why can't we just change this let's just change this we'll just say like 00 Z because I mean that should work still right it just has to have a property that it watches on yeah so that's the way we can bust it okay great so I'll go ahead and type in apply we'll say Auto approve and we'll let that deploy I'm going to assume that's going to update the code in place this might take a little bit of time so I'm just going to pause here so you don't have to oh wait no it's done that was really fast so let's go back to our Lambda we're going to give this a click here and um I wonder if it'll give us a preview of the code when you use a zip it usually doesn't but if it's really small it will and we can see our new code's in place so it's there so it definitely should work let's go back over to our Lambda or sorry our Cloud developer environment we're going to go ahead and hit up we're going to trigger this again and the idea here is we want to um see if we can get that output so we'll go back over to our Lambda we'll go back over to monitor we'll go into our cloudwatch logs and then we should have a new log file because we did do a major change so and there's been a bit of time so there should be a new log file sometimes it'll be in the existing log it just depends on a few factors that I cannot remember this state looks newer so we'll click into that one and notice it's printing out all the results okay so you know we could parse that and work with it and get the data out of it I don't really care um all they wanted to do is to make sure that you have good application integration skills functions cues topics those three are very important event bridge is something that's really cool we'll have to look that in another look up that another video but since we're done here what I want to do is tear down all this infrastructure um and because it is terraform it should be very easy we just have to do terraform destroy before we do that well let's see what happens because I want to know if it'll empty that bucket for us I think it's smart enough to do that for us in CLA formation it would complain but uh I think uh terraform is a bit smarter I'm just going to do auto proof so we don't have to wait around forever enter and it's destroying everything looks good so it did have an issue with the bucket and it needs us to delete all that all the stuff in the bucket first of course I was hoping that it would just take care of it for us there could be something in uh terraform that might do that for us but uh didn't look into it so we'll go ahead and say delete permanently delete this should empty the bucket out good we'll go back over to here try this again so it's always those nonempty buckets there we go everything is torn down and yeah hopefully you learned a lot and that wasn't too hard but uh yeah super super valuable to know application integration S3 event notifications super useful often used functionality of S3 and we'll see you in the next one okay ciao stor class analysis allows you to analyze storage AIS patterns of objects within a bucket to recommend objects to move between standard to standard IIA uh this is kind of interesting because obviously we have S3 intelligent tiering and we also have life cycle rules but understand that all this does is is uh analyze and then give you recommendations you can easily turn this on Via the API uh this is going to observe the infrequent access patterns of a filtered set of data over a period of time you can have multiple analys filters per bucket up to 1,000 filters uh the results can be exported to CSV export exporting the daily usage to S3 bucket and using Amazon quick site for data visualization provide storage Us visualizations in the S3 console that are updated daily after a filter is applied the analysis will be available 24 to 48 hours later storage class analysis will analyze objects for 30 days or longer to gain enough information why would you use this if you have um S3 int inent tiering because this is a manual process and it's more cost effective and it's not for moving uh between a bunch of tiers it's just for these top two tiers here you could pair this with life cycle rules um to kind of automate that process um and you also get some additional metrics so that's pretty much it but yeah there's a lot of services out there that have overlapping um responsibilities but it usually comes down to cost automation convenience and things like that okay Amazon S3 storage lens is a storage analysis tool for S3 buckets across your entire adus organization it can do things like determine how much storage you have across your organization which are the fastest grin buckets and prefixes identify cost optimization opportunities Implement data protection and access management best practices improve the performance of application workloads metrics can be exported as CSV or parquette to another S3 bucket usage in can be exported to Amazon cloudwatch uh and the way it kind of works is that it's going to aggregate these metrics and displays uh into an account snapshot as an interactive dashboard and it's updated daily so when I went to the service it was already on I don't remember turning it on maybe it was on by default I can't remember but you have all these tabs across the top and every single one has all these cool metrics and diagrams and stuff like that it's very straight for it it's just a bunch of uh metrics okay hey this is angrew brown and we are taking a look at storage lens there's not a whole lot to do in storage lens as it's more about providing us information but we should still know what it looks like so on the left hand side we have storage lens and we go over here to dashboards it already creates one by default for us so we can go ahead and go down to the default Account dashboard and what we'll get is some data so you can see at least in this account which is not a lot happening here that um you know we have different information for how storage is occurring so for total storage you can see we're having files we're deleting files we got object counts we got a average object size a lot of different information we can break it down by day week and month even though that didn't look a whole lot different there we can see Trends and change the time frame for what is happening there we can change on a lot of different uh values here as well again we can see classification storage you'll see mostly I'm doing standard storage because you know even though it just has all these storage methods unless you working at scale there's not a whole lot of reason to change that kind of stuff uh we can go over here to accounts and each of these tabs breaks information down from different perspectives so this is from an account based on account breakdown uh of course this is a single account so I'd imagine that if we were in our root account in an organization maybe we get more information uh we'll go over to it US regions so again it's similar information just all over the place uh nothing super exciting uh to report home but yeah there's that stuff there um we can take a quick look here and see what happens if we create a dashboard I don't think there's anything that interesting so you can just say if you want to include regions or buckets notice there's free metrics Advanced metrics includes options for additional metrics and aggregation and other Advanced capabilities dat is available to query 15 months so it looks like if you want more S3 advanced metrics you can bring stuff over from other places and get get richer information so I mean I guess there could be a good reason to uh create a custom dashboard and looks like we can export our metrics into different formats and have it dumped to a bucket so pretty straightforward so yeah there you go let's take a look here at S3 static website hosting which allows you to host and serve a static website from your S3 bucket now I need to make the distinction that this is a static website so you're not getting server side interactivity of course you can have client side like JavaScript executing and that isn't exactly static but it's static in the sense that we aren't dynamically rendering out serers side files and then serving them up um so S3 website npoints do not support https if you want https you're going to have to front load with a CDN so you can put cloudfront in front of there you could use a third party but of course you're going to have synergies with Amazon cloudfront and that's what the exams want you to know um for uh the actual address for the website endpoint S3 static website will give you one so if you want to custom domain you're going to have to use cloudfront for that but we'll talk about that later but I want to point out that when we have this um uh this this endpoint it can either have a period or a hyphen in here so notice that it has hyphen and then period and then here it has hyphen and then hyphen so for whatever reason uh it's implemented differently in in particular regions I'm not sure where but I know that this thing uh has caught me in the past so just understand that there is that minor difference there are two hosting types via the adus console um so you have host a static website and then you have redirect requests to an object um so it's really interesting that static website hosting can also just do uh redirects for requesters pay um uh which is a storage type these buckets do not allow access through a website endpoint um so if you're trying to turn a static it and then make people pay just for looking at the website adus is not going to let you do that because that's just an odd configuration but uh there you go we are still looking at static website hosting and now in particular the configuration options we have around it so by default it's already going to serve an index HTML file but if you want to change the index HTML file to be something else uh maybe index.htm if you're used to the asp.net world or you have a different file name you can change it there um does it allow you to put a prefix in there I don't know I've never tested that but um maybe that's an option but as far as I'm aware of it's just whatever's in the root of your bucket um you can also specify an error document it's going to only be for 404s and 403s as um S3 is not going to throw the other errors and serve that back to you if you need more customization you can do that in cloudfront as it allows you to set um uh uh serve error documents and things like that via cloudfront you also have the ability to set redirection rules which is going to be a bit confusing because s stag website hosting has an additional mode which is called um redirection requests it's not exactly the same thing that's where you're redirecting one bucket to another address or bucket but this is actually um for about rerouting um objects uh if you have different names so the rules here can be written either Json or XML um if you're in the console you're only be able to put in Json if you're using API you can put Json or XML nobody does XML anymore these days here but uh if you want to you absolutely can and so the redirection rules that we're able to do here is we're able to redirect based on status code redirect based on prefix redirect to a different protocol redirect based on multiple conditions let's just take a quick look here um at this graphic here so notice the condition it says key prefix equals product old product HTML on a 404 and then we're saying redirect here on uh this host name or domain and then it's going to go to the new product so there's an example there and if you want custom domains you got to do that at Amazon cloudfront okay so Amazon cloudfront allows you to attach um an SSL certificate and specify um uh like SSL certificate for a very specific domain and then you can redirect traffic to cloudfront and that's how you're going to get your custom domain it's not through any configuration here in S3 static website hosting okay let us take a look at redirect requests for S3 static website hosting so an S3 bucket can be configured to redirect all requests to another S3 bucket or a personal domain so when you choose an option you're going to choose between host a static website or redirect requests for an object and there I'm saying redirect this bucket to the host name exampro doco so I just want to point out that there can be some confusion because there's a thing called redirection rules for static website hosting and that is for changing how um content is served at the object level or um the actual Pages themselves or the assets themselves where this is redirecting everything okay um so hopefully that is clear redirecting requests is a common strategy when setting up a static website hosting since you will often need to create two buckets a naked domain and a w ww. doain with one pointing to another now technically you can get away with only having a single bucket and you could um do something in cloudfront to point to the same bucket for both um but often people will just create both buckets it's really important that if you're going to have a website try to go grab all the possible domains and subdomains as buckets before someone else does because another customer could go and take your domain and you'll never be able to create that bucket and it's really frustrating it's not ever happened to me before but it is totally something that can happen to you so you just as a visualization here we are saying mybu docomo to www.my.com okay so there you go okay so we turn on S stack website hosting we go to our website and guess what it doesn't work because by default uh the website will not be publicly accessible you have to do additional configurations um in order for people to be able to see it so let's talk about uh the different types of configurations because there's a lot of ways that we can do this the first is without cloudfront we just turn it on and what we'll need to do is turn off block Public Access for bucket policies create a bu uh bucket policy that allows all of the Internet to read it um this will work but the only disadvantage is you're not going to have a custom domain uh and you're only going to have it in a uh nonsecure way because it's not going to serve htps it's going to be HTTP so we add cloudfront now uh we still have to uh turn off um block Public Access for bucket policies and we still need to create the same bucket policy so we get the advantage of HPS it's secure we could also do a custom domain however people can still access it through the other S3 website end point so even though you went through all this effort to um make it secure and provide a custom domain they're still going to access it through that uh unsecure endpoint that S3 provides you for free so how can you just only force it through cloudfront um that's with an oi so this is an origin AIS identity so again you'll have to turn off uh block Public Access for bucket policies at least I think you have to um I I say say that with uh a lack of confidence because sometimes you don't have to and it's because in oacs you don't but anyway we'll turn it off we will create a bucket policy but this time we're going to only allow the cloudfront distribution to access it not all the internet so that's going to force all the traffic through cloudfront we get that htps we're going to get a secure link and that's great but can we make this even more secure well there's a newer way of accessing um S3 buckets and that is with origin access controls and the key difference at least from what I was able to tell is that you do not have to even touch block Public Access you don't even have to turn it off you can create your bucket policy and point to the OAC and so now all traffic is forced uh through cloudfront um it's even more secure because you don't have to fiddle with block Public Access you can have your custom domain and so this is the most recommended way that I suggest for you to do it there is no reason to use ois anymore oacs basically replace them um but they are still an option and they are provided uh amply in the documentation so yeah we have four different ways that we can configure it and it's just changed over time okay all right let's take a look here at bucket policies for S3 static website hosting I'm sure we've looked at bucket policies before here but this one is specifically for U when we're trying to make it public to the internet and so here's the example of what we have here and notice that we're only saying allow to get an object if we had version turn on we might have to have get object version in there um and then we're providing to the resource so this is saying to be able to fetch it from any in the bucket now this policy is going to look different if you are providing it to a distribution it's going to look different if we're providing it to an OAC we'll cover that when we do that in follow alongs okay ciao let us talk about single page applications because you can absolutely upload these to S3 and they will work with some configuration so what is an Spa well it is when you have a single page like an index HTML page and it's dynamically creating other Pages using JavaScript and it's happening client side notice that I'm using the word dynamic you absolutely can do Dynamic stuff on static website hosting it's considered static because there's no server side stuff happening you can't dynamically generate Pages server side because there's no server serving the pages uh or at least you don't have access to the server to write additional code to do so um so understand you can do Dynamic stuff now the idea here is that when you have a single page application everything has to go to the index HTML page however if you're going to put in those URLs into your browser the for slash is going to go to index HTML and when you attempt to go to a dynamically generated page such as Pages for slab you're going to get an error code maybe a 403 maybe a 404 um because it's going to expect S3 to have in it a HTML file that has a name of forward SL Pages forward slab it's still an HTML file even though it doesn't have the extension the reason it doesn't have the extension on there is that um if you want to have clean um URLs without the extension you can just name them without the extension and put them in there but that's not what we want to do here we want everything to go to index HTML so what we can do is when we're using cloudfront uh we can set custom err pages and so this is is some cloud formation um this is one way of setting it you can do it through the console you can do it through the probably through the CLI and the SDK but we're going to say let's send the 403s uh to index HTML and change it to Response Code 200 and that way you're going to Route everything to that index HTML HTML page um could you do it uh at the uh the error document possibly if you're not using cloudfront should it be a 404 instead of a 403 it really depends if you're using ois AC's your bucket policies but for the most part what always works for me is cloudfront 403 so that's what I want you to remember and and the optimal configuration is going to be utilizing an OAC of course we'll cover this again when we take a look at cloudfront as this kind of feels like more like cloudfront but unfortunately or not unfortunately but these Services work really closely together so we have to talk about cloudfront quite a bit we're talking about static website hosting so yeah hopefully that is clear the takeaways yes you can do Spas you got to configure a custom aor page and point back to the index HTML page there you go hey this is Angie Brown and in this fall along we're going to look at static website hosting we actually have looked at it in other um uh follow along or lab so I want to do something a little bit different for this one um I want to utilize redirection rules and I also want to attempt to make a single page application with cloudfront um because those are two interesting caveats of of using um static website hosting I think it' also be interesting to do this in um uh not just in uh the CLI as we've been using it uh repeatedly but maybe we can go ahead and use cloud formation as this would be a great example for using static website hosting so that's what I would like to do here um so what we're going to do is make sure we're in our itus examples repo and I'm going to make a new folder here and this one is going to be called Static website hosting and we're going to go ahead and create ourselves a new file here and this is going to be a template. yaml for cloud formation and we'll start writing our cloud formation template so there's a few things that we do need to have um so for yaml I think comments are there like that so there's a couple things we'll need we'll need to have a bucket we're going to need to have um static website hosting turned on we're going to need to have a bucket policy we are going to need a cloud formation distribution we are going to need to configure it to um uh route the four like route 404s 403s 404s to the um to the single page application and uh we'll need to create an origin access control so there's a few things that we're going to need to do the bucket should be pretty darn easy so we'll go ahead and go type in ad us cloud foration S3 and get some example code and in here I'm going to go down to examples and we have a simple one we'll grab the yaml one and I'm going to paste this in here there's a few other things that we need for our Cloud information template so I just going to go and find out uh template formats here maybe there we go we got some basic stuff so we're going to copy these two lines here and put this up here this will be Gra grab that uh template so you know where that is if you're looking at this and you can just look at the code and try to find it but this will be whoops here we'll paste that in so just say uh static website hosting for um for what uh for spa okay using cloudfront uh OCA which is an origin Access Control okay so here we have our bucket and I don't want to retain it I don't want to have a delution policy here the name really does I mean yeah it does matter when you name a bucket for web static website hosting because you want it to reflect the name of your domain so we're going to pretend that we have a domain I'm not going to do a custom domain as video that's a little bit too much work we might do this again with a custom domain with more configuration but right now I'm just going to make a pretend uh address we're going to say www. um uh I'm just trying to de side here of a website so I'm not sure so let's just get some random names what is a uh give me some fun domain names for example projects list 10 and we're just going to go with rocket recipe or my maze lab we don't need a real one just anything we're going to use and so we'll go ahead and bring that in here and say paste so www.m maze mad uh mine maze mad and so the thing I said there before is that we need to have a bucket that's going to um redirect to our other buckets so we should always create two buckets one for the www bucket so this will be our ww bucket and then we'll have our uh naked or whatever you want to call I'm just going to call the naked bucket that's just easier for me to remember and that's where it doesn't have this okay so the idea is that we're going to want to configure these buckets for S3 static website hosting um I'm not sure where that's done at but we'll go take a look here and go back to it was cl formation S3 and click onto here I'm going to grab this link go back over to here and we're just going to paste this URL on the top so we can find that reference really quickly and in here there should be static website hosting somewhere I would think down here website configuration and we'll click here and get an example so down below there should be examples for yaml um so here we have one and it looks like this is just going to have the basic website configuration so I'm going to go ahead and copy this those two lines and we're going to assume that the www is the main one and the other one's going to Route the other way so this is going to expect an index.html and an error. HTML for the for the other stuff down here we have redirection rules we might want to come back to that and play around with that later but what I want to do is I might want to have like a full redirection here so let's go all the way to the top and so we have this redirect all requests too so that's going to be the one that is when you want to completely re redirect to another bucket if you specify this property you cannot specify any other property that makes sense CU When you and you set up a bucket in S3 right we'll just look at any random bucket in here so let say we go into this one and we go into properties and we go into static website hosting notice it just allows us to have either this option or that option and so the idea here is that we are choosing that one explicitly by having this only so we'll go here and this one is going to still have website configuration but we are going to also pull p in this redirect request to all okay and now we need to see what it accepts so this is what a string what is it it's a type so we'll click into it and it wants to have these two properties here okay so we'll go here baste in by the way when you're working with CLA formation yaml it really really matters that you do not mess up any the spacing or it will throw weird errors so just be very careful here um the protocol here could be HTP or HPS I'm going to assume HTTP because we are not using um SSL right now I mean we could put it on HPS I suppose I think it'll be fine the default protocol is that is used as the original request so it's not even required let's just leave it out and then here the name of the host where the requests are redirected so we want to redirect this to the other bucket um and we can probably get the bucket URL here somehow so if we go back I'm going to grab this here so we don't have to look for this later I can just go grab this and put that there so we can easily find that and we'll click back again this is for the uh we'll go down where we here that's website configuration also grab this link here so in case we want that um for later all right and I'm going to click back here and I want to see how we can get okay um get some the return value as the actual domain name so here it says Returns the ipv4 DNS name of the specific bucket um that's not the static website hosting so that's not exactly what we want Returns the Amazon S3 website endpoint for the specifi bucket this is actually what we want is probably this so we'll go ahead and grab this and we will say we want to redirect to so we'll say um www bucket we want to get its attribute so it's exclamation get at and then we're going to put in www bucket and this should return that website URL here usually when I get started with cl information I always kind of forget the syntax and as we work through it it will it will throw errors and say hey you got to do this way you got to do that way and we'll figure it out as we go so anyway the idea is that this bucket should redirect to this one based on that uh its bucket address so hopefully um that will work there I wonder if that'd be an issue because if we put it in front of um cloudfront I would think that cloudfront would then you would need the cloud front address there but whatever that's fine I think we'll be okay doing that so we definitely have the static website hosting now configured um the next thing is probably the bucket policy so so we'll go back over to here and we'll scroll all the way to the top here and we'll just take a look and see what we need to configure so logging life cycle inventory cores we don't have to worry about cores right now we did that in another video um this all looks okay where's the policy though so this might be separate there is an option here for bucket policy so that's what I'm looking for so I guess there's a thing just policy name bucket policy oh here's an example so we have bucket policy here how do we know that that's attached to the bucket though again how do we know that that's attached I would have thought in here I'll just make this a little bit larger that it would have been there maybe there's a separate object for it sometimes that's the case so if we care if we look on the left hand side here I'm looking for bucket policy course configur I mean these are things underneath it I think it' be um in the order that you'd expect it to be so there is actually separate object okay so that's the way we're going to create it we'll go ahead and copy this here and we'll paste in our bucket policy paste this in this one this property will just be called bucket policy we don't need a bucket policy for um the redirect because it's not going to ever hit that policy so we don't need to do that so here we need to specify our bucket so we'll go here and say uh ref www bucket I'm not sure what it's going to return here for this we go down to examples or sorry if we go back to our S3 bucket and we go to examples it'll tell us what it returns or return values whatever returns by default so this will return The Logical ID I think that's what it wants here if it doesn't we'll find out later on then we have our policy document and uh we can put inline policies here we'll go back over to this let's see if we can get an example yeah there is a good example I'm going to go ahead and copy this bucket policies are pretty straightforward um in terms of their configuration there's not like a whole lot we can do with them the indentation doesn't look correct here so I'm just going to go ahead and bring it in a level or out of level and so we want to get that object that makes sense allow that's good here um we're specifying which bucket and they're doing a join this way and we can do it that way I don't think there's any issue with doing it that way but I would rather probably use uh interpolation for that so we do a string I'm just trying to remember how we do interpolation and cloud formation interpolation cloud formation string it doesn't work for everything oh sub that's how we do it sub so we can probably use sub for this to substitute the value so go here and say say sub and then the idea here is in a single line we can uh have something a lot cleaner now they keep sh sh it here like this but I I want to do it where it's a single line so see we can do do that so I'm going to try try to do that there and again that might mess up but we'll find out as we work through it you always kind of forget cloud formation as you're working with it and then eventually comes back so I'm going to say ref here and we'll say um www bucket and I'm actually not sure we'll get here because I don't know go back to the bucket here for a second go back to the return values yeah that will return the bucket name okay that's perfect so I just wanted to double check and make sure that's the case we'll hold on here when you pass the logical ID the bucket name is returned okay great so we go here and we can just reference it I always kind of forget with sub with the ref supported functions for string parameters you can't use any functions you must specify string value so here it says root domain name I'm just trying to see an example here I know like my other code has it I can go pull some up but I what I want to see here is um one that's going to have that inline because it'd be really nice if it was a single line so notice here they have Adis region super parameter VPC so yeah I'm not 100% sure but what I'm going to do is I'm just going to um leave that in there for now and we'll see what happens when we have that um the resource here is I think that's incomplete I'm just going to undo there yeah we forgot the for slash on the end so again we might have to come back and modify this but that's totally fine in terms of the principle we're saying from anywhere that kind of makes sense when we start to implement an O OAC we're actually supposed to provide that it's AC there but we can always work our way backwards then there's some string like stuff we'll just take that out we don't need this here and so now we have our bucket policy um so yeah that is fine the next thing we want is we want to actually have an application to upload so I think that's what I would like to do next uh well before we do let's get some outputs in here so let's just go look up outputs for cl formation outputs for cl formation okay so I'm just looking at the format because I always kind of forget what it is we'll just go grab one here and we'll paste it in and there's a couple things I want to Output here the first thing is the WW bucket uh URL and then I want the naked bucket URL S3 uh static website endpoint URL I don't need to export this because we're not passing it to another um CL foration script we don't need to do cross stack stuff right now for www endpoint for this will be the naked endpoint and to get this we can reference it with the get ATT get attribute so I'm going to go ahead and just paste that on in here some functions don't work in other ones it's just varies based on what you're doing so I'm going to go ahead and get the the naked bucket for this that looks fine yeah that's good okay so that will get us our outputs um and the thing with this is it's not going to do anything really interesting because we will have to upload the file separately we'll create a new folder in here we'll call it bin and we'll have one in here called deploy okay and then we'll have another one that's just like um upload that will upload our files so for our deploy we're going to want to have um a user user B script here so I know that we have some commands here from earlier bash bash script commands that we used here it is I'm just going to grab this Top Line because I don't remember what it is going go ahead and paste it in and then we'll get one for upload as well so for this here the uh the deploy this should be pretty easy we'll need to go get the cloud formation deploy command so we'll type in cloud formation S3 or sorry a and we'll go here and make sure we're on version two and in here there is a deploy so we'll go ahead and grab that and we'll we grab deploy go to examples at the bottom we'll go grab this examples we'll go ahead and paste this in we will bring this onto new lines there our parameter overrides um not really worried about that right now we'll just take that out we didn't do this in our template but we should probably have some parameters parameters and I'm going to have one called uh the domain name probably we can set it as type string I would think and um I'll just say naked domain name and then I'm going to just default that value here to be this mind mlb.com of course choose your own domain because these are unique right the and they'll have conflicts when we upload them so I'm just go go ahead here I just want to substitute that in there so say sub and that will be for the naked domain parameter and well for that one it's even easier I just do ref so I go ref naked domain name but for the other one well that's for www um sorry that's not the www bucket that's why I'm getting mixed up here but for this bucket down here we do want to have the WW in here so I'm just going to do interpolation again I think I have to do ref there and we will try that I cannot remember if I can put ref in here or not um we will find out as we work through this so coming back over to here we're going to call this Spa static Spa S3 I guess we'll just keep it nice and simple the template path is going to be template yaml we don't have any parameter override so we'll just not worry about that right now and so that might be enough to deploy it let's go find out and hit enter invalid template path template yaml probably because we're not in the correct directory so we'll go ahead and just S3 and then go to static website hosting here I'm going to give it its absolute path that way we don't have to wonder where this follow is I'm dragging it out and then just copying the entire path I can go ahead and paste it in here you know there's stuff that we can do to um make it look relative to here I just don't feel like doing that right now we'll do that in other videos not every single script has to be like super bomb prooof so we'll go ahead here and try this command again I just want to see if it works in isolate so it is creating a chain set one thing I always like to do is always explicitly ask for change set so I'm going to go over here and look for those in the parameters here and we're going to find it here somewhere just say chain set no execute chain set is what I want so that way we have to explicitly accept it we'll make our way over to um cloud formation and we'll see how it's doing so we have a roll back there's an issue let's go to our vent data the following resource failed to create ww bucket roll back property validations for WW bucket failed validation constraint word for pattern so there's something messed up with our pattern of our name I'm not exactly sure what so we'll go back over to here and I mean clearly we're not doing the substitute correctly if it's complaining so I'm going to just ask how to do inline ref for Sub in CL formation great thing is once you start having a lot of uh code you just start being able to reference it no problem I mean you can do it that way but I almost feel like you should be able to do it but give me a second I'll go find it okay so here they're not specifying the ref so I'm going to assume that that's what it does by default is it uses the ref and so we're just going to take this out here and I'm going to look where I might be referencing this somewhere else so we'll go ahead and do that as well um well this one's a little bit different because I'm actually trying to reference the specific the uh specific thing there but at the same time we do also get it from here so we could also uh provide that there because it would be technically the same bucket name so I think that's what I'll do we'll go ahead and say naked domain name but we'll have to make one change here because it's not going to have the www in front of it so I'll say www dot it'll hit save and we'll go back and hit up and we will well before we do that we need to get rid of this stack so we'll delete it and refresh Till It's Gone good it's gone and we'll go ahead and attempt this again so now this time it should wait for us to confirm the change set because we made that that change so notice it says review in progress oh this is new get sync get sync could can't be configured for S3 stack because it's not whatever that's kind of interesting so I'm guessing they're trying to add get Ops natively into uh cloud formation totally new feature haven't seen this before so what I'm looking for is that change set I'm not sure why it hasn't appeared it's really going slow it usually doesn't take this long maybe it's uh ignoring my chain set oh well I did hit up so we didn't actually um we didn't actually run the command uh as I thought we did cuz we hit up so it wouldn't had that no execute chain set in there that's fine so we'll go back over to our events we'll take a look at our information the following resource failed to create the bucket um well first of all did create our buckets so it did create our buckets no problem it had an issue with the bucket policy access denied for 403 executed request change set etc etc so it didn't let us create the the bucket policy and I think the reason why is because we have block Public Access set and so the idea here is that probably um if we had passed the origin Access Control into the principle then that's not a public facing thing and it wouldn't throw that issue so what we can do for the time being because I want to do this in Parts is I want to just turn block Public Access off temporarily and that will um save us some trouble in the short term so we'll go over here to block Public Access configuration I'm going to go ahead and grab this so we don't have to look for it later but I'm set this on the WW bucket and we'll go here and we will go ahead and grab do they have an example no but it's pretty darn straightforward we'll just grab this block and we'll paste this in here and we will go ahead and grab this and go down and paste this in put a coal in here and we will indent these here here so this is going to be false or sorry true false true false cuz we're not going to touch ACS let's put these up here together and again this is temporarily we're not going to keep it like this it's just for um the meantime here going to go back over to here and we'll go ahead and delete the sa oh there's this this is a new button detect the root cause they never had that before that's really cool so we'll go ahead and delete this and give this a refresh and wait till it's gone there's nothing in those buckets so they should delete very quickly and we'll go ahead and this time I'm actually going to trigger this command so I'm going to type in chamod U plus X and then we'll just do bin deploy and we'll say Echo deploy so we know that this is deploying and now what we can do is do this instead and we'll go over here and we'll give it a refresh we'll click back into here we'll go to change sets we'll accept the ch set we should always click into it make sure we know what we're deploying two buckets the bucket policy looks good execute chain set and now it's going to roll out I'm not expecting any issues but I'll I'll be back here when it's done okay all right so that's done creating in our outputs we should have those addresses and so there's nothing in this bucket um if we were to open this up we don't we shouldn't expect any results we'll go to www me to open this up in a new link I'm can go back here we get a 404 because there's no key and that makes sense because we haven't yet to uh create any kind of application so what I want is I just want a very simple single page application I'm going to try to use chat GPT and of course you don't have to write any of this you can just copy paste the code but I want to show you all the steps so you know where this is coming from so write a single page application that has a homepage and about page and a contact page fill it with content uh use react it should generate out should have a um mpm command to compile to static okay so let's see if it can go ahead and write this for us now I have SBA apps from scratch I just would like to get everything set up for us here so react router it could be really tricky here with um with react and so we'll try to try to go ahead and use what it's asking for so while we're working through this I'm just going to add a read me create static website um I'm just going to call it App instead we'll go ahead and do that and I can just change this to be y so hold on here whoops if you are using your local development environment there could be a lot more steps involved here again I asked it to copy this and there we go because I don't want to have to keep hitting y apparently you still have to hit Y there's not no way around it and then we'll want to CD into that directory here in a second install react router while that's going I'm just going to bring the the code over here and get it ready it shouldn't take too long yeah I think react router is six now um I don't really like using react but uh I definitely have used it quite a bit to know what I'm doing uh so here it says create three files named home about um and contact so here they're suggesting to make each of those those pages so again we're just going to make sure we put all the instructions here so this will be we have one for homepage and then we have one for the about page didn't really make content for us that's okay say Welcome to our homepage and so will be that and then we'll want a contact page as we're working on this we can preview it up here and see because this is what we're doing we're just writing the documentation out here and then we'll copy paste into our thing contact page and the code it's gener looks pretty good and then we need to update our um static stuff so that's totally fine we'll go back over to here and I want to click into our um well we got a CD into that directory first so we'll go ahead and do that and we need to grab this line here and say install react router Dom and then we will have here in our source I'm split this here we should have an appjs and so in here it's suggesting us to replace some code and all it's doing is adding those routes so that looks okay to me yeah that looks fine we'll go ahead and grab all this I'm just double checking triple checking here paste this in here so that's going to allow us to route two different pages right um then we need to create those individualized pages so we want to have our homepage about page and contact page so we'll say file new and we'll say homepage JS if you did my boot camp for those who did my like crazy adus boot camp you definitely remember doing a lot of this um we'll say homepage about page JS and we'll say contact page JS we'll go back over to here to our read we'll just go grab each of these here so this will be in our home paste that in there we'll grab our about you might say Andrew do we really need to know how to do this we working Cloud absolutely yes because as a cloud engineer you will encounter this stuff all the time you should have basic Proficiency in uh web Technologies you don't have to be really good at building apps but you should be able to do this um to the to the minimum I would say that's my recommendation even if you're devops I think you should have basic https um this kind of coding is good so we have all those pages um I'm not sure what we need to change in the package Json it looks like or JS it already looks like it does a build here so it's saying this but we don't have to add that it's already there and we can do an mpm run build to build out those pages so we could um just run this and test it first so the way we'll do that is if we go into um the package Jason we have mpm run start so let's say mpm we just going to do an install just in case there was something we're missing good we'll do mpm run or mpm uh start and that will start up the application it should start up a development server and we should see a website here it might need us to make this port accessible I just said yes and we have three pages here so that is our single page application notice that it's the URL is changing but if we were to refresh those pages don't exist so it shouldn't properly route well I mean it will here but um we'll find this out when we do the S3 website so this is good and so the next step we need to do is we need to compile this into a uh public directory here so I'm going to do mpm before we do I'm just going to double check and see if we need to ignore any additional files in here uh it's not uh dumping anything crazy so that's okay but I am going to want to ignore the public oh we already have it perfect so see here it already um ignores stuff and I don't know where this is going to build it says public here so I thought that's where it's going to build but maybe there's going to be a a build directory so we'll go ahead and type mpm run build for some reason when you have mpm start you don't have to put run in front of it but all the other commands you do we'll go ahead and hit enter that's going to build out those static files and so now we have a build directory okay and those are the files we want to up load so let's go take a look and see what we have so we have a bunch of files and this is a great opportunity for us to sync our files this will be a very simple script um I would definitely do something a little bit more complex usually but for this purpose we'll keep it simple we have ad of us S3 sync and we want to sync an entire directory so I'm just going to go ahead and grab this URL like this whoops try that again we're going to grab the index h drag on down here I just want to copy the path and I'm going to paste this in here and then we want to send it to our um S3 bucket so we specified our bucket name as what it's called this so we go ahead and paste this in here like that and so I'm thinking that this is going to syn the contents I'm going to just double check that sync command CU I always kind of forget um it's syntax ta I don't use it often I usually have like a custom script so I'm looking for Sync here we are local path yeah I mean I think that's what it should be so what we'll do is we'll go ahead and type in clear I'm just going to CD back a directory and we'll say chamod and I say U plus X on the bin upload we can rename this to sync cuz that would make more sense cuz that's all this is doing we'll save this first and then I'll rename this to sync okay and then we'll just name this sync instead we'll do bin sync and so what I'm hoping is that it's going to upload it to the destination now I think what the issue here is is we need to put that S3 forward SL slash on there so let's just type in clear anytime I'm exiting stuff I'm hitting control C if you're wondering how I'm getting out of stuff we'll go ahead and hit enter and so that's going to sync those files so that should now be there if we were to go to here and go to ww we should now see those files there um oh I want to go to outputs and we'll go to ww here we'll open this in a new tab so now our website's here look it works and this is a static this is a static or single page application sbaa now look what happens we're on the contact page if I refresh it's going to say 404 and the reason why it's saying that is because um this page doesn't exist right so if we go into this bucket this is what I talked about in that scenario in the lecture slides and I think again that's why we're doing Spa is because it's such a common use case uh for static website hosting but if we go here there is an index HTML file for this to serve up contact there need to be a an HTML file called Contact here with no extension on it for it to work okay um and it would have to have the exact same contents of this one but we don't want to have to basically make duplicates of the index HML file we want to redirect things to this thing here so maybe one thing we can try to do at least without uh cloudfront is we can try to send an error like a 404 to that page now probably in the lecture slides I wasn't 100% sure and I thought you needed cloudfront for this we might not need it let's go ahead and see what happens if we were to use uh or change the 404 so I'm going to go back over to here and we have this error document and I'm going to change it to be index HTML okay so we'll go ahead and we'll deploy and we'll see if we can make that change and see it will detect it I mean if that solves it then we're pretty much good to go we don't really need to use cloudfront uh as cloudfront might be a little bit too much for this project but I thought that we we have to use it it's very hard to remember all the use cases and so we'll go over to cloud formation here and we'll go into Spa we'll go to our change sets we'll accept them remember we chose to manually accept those changes we should have looked at what they were we're waiting for that to complete be back here when it's done okay all right so I'm going to go ahead and refresh still not done we'll wait a little bit longer it must be doing something more intense it's just modifying it so it's not deleting it it's now done okay great so we'll go back over here let's refresh and see if this works okay so that works so that confirms that if you're using static website hosting and you don't have cloudfront you can use the error error to redirect it and make single page application hosting working and that's great so what we're going to do is we're just not going to do the cloud front part because that's going to make things really hard here and we can save that when we go ahead and do cloudfront which I think is more interesting the other thing I want to explore here would probably be um redirect rule so we did redirection from one bucket to another let's see if that actually works if we go over to here and we click into our resources and we have our naked bucket it's supposed to redirect to the other one right so if we click on this one does it go to the other one and hold I'll just click back here I'm going to open this up in a new tab that's what I would prefer and notice that it's not working so that's interesting why is it not working so go back here and let's go actually click in and take a look cuz there's something wrong with the configuration normally when we set a bucket we can usually um just selecting the console it's a lot easier to do but I wanted to again do this programmatically so we'll go over to properties we'll go all the way down to static website hosting and we can see that it's redirecting to here it says Target bucket website address or personal domain I mean that looks like the other one here notice that this one is it's not providing HTTP so maybe that's our issue is that we have the protocol in there so I think that's the problem and if we go up to our redirect here we get that there so we need some way to take that off there's probably like a function to substitute function replace cloud formation okay oh yeah right so substitution can work with with a mapping or without a mapping and so um in this case we're going to want to have a mapping so that's what we want to do we want to say um but that's substitution right if you're substituting only the template parameters so I want to take a part of a string and get rid of it give me a second okay no this is not going to be useful for us here um so what we'll have to do is just reassemble that website URL by hand um and that's not too hard to do we'll go back over to here and we'll just grab this full URL because this follows a convention right so if we go here the only thing that matters whether it uses a period or not so there's a few things we can substitute so we'll go back here and just do sub and um here what we have is the actual naked domain so we'll go here and just type in uh naked domain so we called it domain name yep and then the other thing is that we want to have C Central one we don't want to hardcode it so we'll go here into do adabs region and this should use it a um pseudo parameter or builtin parameter I'm just going to look this up builtin prams or pseudo prams are down here CU I just want to make sure if it's double colon or not it's double colon and that will insert the region into there so we don't have to do the work and I think that will fix our issue so let's go ahead and do a redeploy and we're going to make our way back over to here to static uh to sorry Cloud front or sorry cloud formation cloud formation not Cloud front and we'll go into here and we'll go into our change sets and I'm going to refresh we'll click into it it's just going to modify the uh information there we'll go ahead and execute and I'll refresh here and we'll just wait till it's done okay all right so after a short wait um this is now updated let's go take a look and see if our other one is redirecting correctly so we'll go click on that I keep meaning to open it up in a new tab I'm just going to cancel back out here and it's still saying it cannot be reached check if there's a typo uh it looks correct to me whoops what if we click on the other link here does this one still work yes it does so we have this link here I'm going to copy the whole link I just want to see what could be different I could be complaining about I just need to open up a scratch Pad here for a second and we'll paste that on in here and then what we'll do is we'll go to this one and then um we'll see where it redirects cuz it's going to redirect right so that's the redirection and then let's go compare and see what the difference is HTTP colon slash oh okay sorry did we not uh did we forget to take something out of there let's go back to this yeah we did so we're supposed to not have this part on here okay I notice that sometimes I'm doing this or that it's totally fine any am you do one or the other in this case um so I guess we didn't have it exactly right we'll go ahead and deploy that one more time and then I think this time we'll have it so I'm going to go over here to change sets we'll accept that and I'll see you back here in a moment when this is done updating okay all right so that's done updating we're going to try this again so I'm just going to close out these tabs here and we'll go to outputs and we'll open up the naked one it should redirect to the other page fingers crossed and is it working is it working it is still not working let's go open up the other tab here and let's I guess compare it again you know usually is not this hard but uh you know if it takes multiple tries we'll have to try more than once what the heck why is it there twice refresh okay now I'm not sure if I believe it that it's still problem so let's go take a look here at the actual bucket itself this one here here and let's actually confirm what its content is because we definitely changed it like we removed it so that shouldn't be an issue anymore you know we can always be dealing with caching here so we could be doing things correctly and then the cach is just lying to us um so we go to properties go down below to our static website hosting this one doesn't have the HTTP on it and it's not specifying it so I don't believe it when it says that it's not working correctly I think it's a lie I think it is working correctly it's just caching uh something that's older there so we go back over to here I'm going to right click into a new tab so notice that it is giving us the wrong stuff so again it shouldn't have this on here so okay how do we deal with it uh deal this when when it's cached right because you know it's propagating it's going to be hard to fix this so another way we can check is by um using the curl command we can try to follow through on that so I can go above here and just like check uh redirect results and uh we'll say use Curl to return a uh to print out uh a website page and follow redirects that's a key thing that we have to ask it to do I just don't know what the command is for that if it's like hyphen X or hyph oh it's just hyphen l so it's as simple as that okay so what we'll do is we'll grab this URL here the naked route we'll type in clear and the L should follow through on the redirect and get us the true page and it did not copy what I want so we'll try this again copy paste enter and so notice here it is rning the page to us and and this is actually what it is because we know that this page um like if we look at the actual contents of the HTML here in our build directory this is the exact same code right says root somewhere in here it says root I'm sure right so this is the same code um that is here and for whatever reason you know when we go to that it's not showing but it is actually working so that's something you have to really factor in when working with this stuff I imagine I opened this up in Firefox I'll try this in Firefox right here but let's try to open this link up in Firefox which is a different browser and I bet it'll work so I'm here right and we'll go ahead and we will paste in this URL right this is the redirect one and notice it redirects so you know don't always trust um Chrome or your browser and try other browsers especially when we're doing dealing with websites now the last thing I want to look at is setting up a redirection rule so we didn't do that uh yet so we'll go back to our bucket a configuration here we'll go all the way the top and we will look at um the website configuration because in here what we can do is we can set up some routing rules I don't do these very often so we might just fumble through this here for a moment but if we go down here below and look at the example we should be able to set up a rule condition so when it equals a particular prefix we can change it to another Direction so we saying if we get a 404 and the key equals this then redirect to that so let's try and see if we can make our own rule and I'm going to go back to our um cloud formation and we'll go ahead and paste this in here and so we have a condition so when a 404 is met and the prefix equals we'll say contact us then we'll redirect to contact so I think that's what that uh URL is right here contact there it is good and the host name do we really need to change that let's go double check properties here routing rules we'll go look at the routing rules here containing for redirect information you can direct requests to another host to another page or another protocol uh container for describing condition that must be met for the specified redirect to apply if the request is for pages in docs and they want to redirect to documents yeah that's kind of what we want to do kind of let's go take a look and see if they provide that example here they don't is the host name required I guess we have to look at each of them separately so we'll go into this one now and click into redirect rule oh we have a few different things here so we go down here so it says um they really did not write good examples it's like somebody just took and copied paste the same example through everything and didn't make Vari ations us you really got to try a little bit harder with your documentation okay it's very frustrating but we'll go take a look here and we have host name the host name that is used to uh in the redirect so we'll just say no because we wanted to go to the same place so we shouldn't have to uh specify that so we'll take that out um and then we'll go back over to our rule so the redirect code we're not going to touch that because we don't care about the protocol so we hold hold on here HP redirect code to use okay so that's like the code that you use for the redirect protocol will remain the same replace key prefix with the object key prefix to use in the redirect for example the redirect request for all pages with docs to documents you can set a condition block for pre key prefix on docs to set in the redirect set prefix for documents okay so we'll go back over to here and hopefully that made sense so here yeah okay this is what we want it to be um HB error code returned equals because that's what it would be a 404 I think that would happen before we intercept the error. HTML I might have suggested the lecture content that it was very limited in terms of those overrides again it's always hard to know until you start working with it and you obvious forget once you stop working with it um but let's go ahead and deploy this and see if this change works so we'll go back over to here we'll give this a refresh we'll go to change sets we execute our change sets and we'll wait till this is done okay and our template failed so let's go take a look and see what happened here um a reason resource Handler returned invalid requests error document should not be specified if redirect request 2 is specified so one cannot be specified if the other one's spec oh so maybe the error can't be specified here so that's kind of interesting it seems like what it's saying here is that uh oh you know what it is um this code is not supposed to be here it's supposed to be down here okay because it's on the the www bucket that we're trying to do this and that was just in the wrong spot so again we will try one more time fingers crossed refresh this here going to go back to our chain sets we'll accept the new chain set we'll execute it and we'll wait a little bit here and hopefully this time we're in good shape okay all right so that is now done no errors so that's really good um so now the question is can we test that and see if it works so we are here and the idea is if we type in contact us it should redirect to contact and it does okay great so um we've done both types of redirect we can uh utilize a single page application and the best part was we didn't have to use cloudfront whatsoever um so again we'll hold uh off on cloudfront until we actually go to the cloudfront section as it can take a long time for cloudfront to update and it can be extremely frustrating so I would say this is good um in terms of this this all looks okay to me so I'm going to go ahead and commit that um to clean this up I'm just going to commit this first so uh my stack website hosting Spa so in order for us to just clean up and tear all this stuff down all we got to do is delete this here the only trick is that the bucket needs to be up uh emptied first so I'm going to go ahead and just do that manually and we'll go over to S3 otherwise it will have issues with cloud or cloud formation and it's not fun so we'll look for that www and we will empty our bucket I'm looking for that empty option maybe it's over here okay we'll say permanently delete that's now deleted so we're in good shape there um we can go ahead and delete our stack delete delete we'll just wait till that's done I'll be back here in just a moment all right so that is gone and we are all good so I'll see you in the next one ciao so Amazon S3 allows you to do multipart upload and that is where when you have a very large file you can upload it in a set of Parts why would you want to do this well it can uh lead to improveed throughput in the case of network failure you can just reupload the missing parts as opposed to starting over from scratch once you start a multipart upload you can uh upload the parts at any time there's no expiry uh for when you have to upload all the parts part s you can upload files while you're creating the file which is uh quite interesting and it generally recommends this if you have files that are megabytes or larger to utilize multiart uploads so how does this whole process work well the idea is let's say we want to get something in our bucket and we have a 100 megabyte file we are going to use utilize either the CLI or SDK to initiate a multiart upload and that's going to return back an upload ID that uh we'll use in subsequent requests to say hey all of these parts are part of the same object and then you need to break up your file into multiple Parts there's a few different ways of doing that and then we are going to uh upload each parts uh you can do them in parallel so that is really great when you want to get a lot of files uploaded very quickly you don't have to do them one at a time um and then once all the parts are there we're going to tell adus hey we got all the parts now Mark this object is complete and now we can access this object in terms of how that looks uh via the CLI and you really should know how to do it via the CLI is we create that multiart upload we specify our bucket and key it's going to return back that upload ID which we're going to use in other parts so notice here the upload ID flag there and the the body part would be broken up by some mechanism um one thing I want to note is that Parts can be numbered between 1 to 1,00 so you know think about how you want to divide it but if you have a really large file you're going to be having to subdivide it at 1,000 each um and for later on in the next step we'll have to collect the E tags but we'll see what that looks like here in just a moment so when we're ready to complete the multipart upload um we need to specify all the parts so we will have to create ajason file and we will have the part number as well as the associate e tag so it knows how to reassemble the file in the correct order and that's all it takes to do multipart upload hey hey this is Andrew Brown and in this video what we're going to do is look at S3 multiart uploads so we have our 8 example repo I'm going to open it up in my cloud developer environment choose which one you want to work with but of course if you use the same one as me it's going to be a lot easier so in order to use um S3 S3 multiart upload we absolutely have to use either the SDK or the CLI I'm going to keep continuing on using the CLI since I find that to be the most useful way of uh uploading stuff so I'm going to just put in here multiart upload as a new folder and we're going to go ahead and create ourselves a new file and um uh actually I meant to make that a folder so we'll try that again so multiart upload and we'll create a new file in here we going to call this readme.md and the idea is that we're going to have to initiate uh finish we'll need to upload each part upload Parts okay and so this will be a bit of a multistep process we do need a file large enough so I'm going to go ask chpt how do I create a file that is uh 50 megabytes in size and hopefully we have fsutil so I'm going to go take a look here fsutil it is not a command that is found and oh that's for Windows that's why so here it suggests oh we can use DD so DD is a a very common command so first we'll create a large file I don't want to commit this file to the repost I'm can just go ahead and type in get ignore and um what we'll do is we'll need to name this file so I'm just going to call this large file txt or large file we don't really need an extension there I'll stick with txt I guess they have that there as well and so this one's going to be called large file txt I want to make sure when I create this that I'm in the uh the correct directory otherwise it's not going to ignore that file and I'll end up committing this to the repo and I really don't want to make that mistake so I'm going to go here and just drag this on down below whoops drag this on down below come on drag this on down below okay I'll drag out the read me file there we go and we'll copy the address and paste that on in here I'll just say CD so I just know that I'm in the correct directory before running that command so we'll paste that in and then we'll go ahead and see if we can create a file that is large enough and so here it's suggesting that we have a large file we can check this by doing lsen La for human readability and it suggests ing here the file is 166 um maybe L on this like that there we go and so we can see the file is uh 50 megabytes right so I'll go ahead here and just just say PS we'll just prep this to GP and just say large okay and so this way we can see how large that file is right so now that we have our file we want to begin the upload process so before we do that we'll need to create a new bucket uh this bucket's going to be called it was S3 make bucket S3 SL multiart a fun ab and some random numbers we'll go ahead and paste that on in here whoops so we'll copy that and we'll paste it in come on here come on there we go I'm going to try this again hit enter so that will go ahead and create our bucket now we need to initialize an upload so we'll type in ads CL S3 CLI and we'll go see if we can find the command that is used to initiate the multiart upload um I'm going to type in multiart and I assume that's part of the name so we have create and complete so open this one in that tab actually before we do I'll make sure we're on version two and then we'll open up the complete and we'll open up the create and then there's a part for parts so look up for parts we have list Parts upload part here we go and so those are the three commands we're going to need so I'm going to stick with create go down to examples and grab this one here and we'll paste this in here I'm going to go to the complete go to the examples and we are going to I suppose copy this one here and then we'll need one for parts so go here down below to the example and copy that one as well okay and so we have created our bucket so that is good and let's see how we can work through this command um so we have multiart upload we'll need to specify our bucket and then we need to specify the key I'm going to just name it the same thing as the text file so large file.txt and what we'll do is go ahead and copy this and paste it in and enter and so now we have an upload ID that we got back here so we definitely want that so we will say remember remember to grab the upload ID so in this case it is this one here EG I paste that in there um because we'll need it for the next part here so this one's a little bit longer so I'm just going to go ahead and make our lives a little bit easier by breaking these up into separate lines and we have to specify the parts of the file and then that's our upload key here so I'll go ahead and grab this and copy it to here now here's the question is okay we uh we uploaded we initiated upload but what do we actually see in the console so if we go into S3 and we go into our new bucket which is multiart fun upload do we see the file no we do not so how could we see that um let's go take a look here and say U multiart or just part and somewhere here it should show us something create complete abort list multiart upload so let's go take a look there and see if we can see ours so initiate list multiart uploads and uh I'm going to assume it's ads S3 and we can just do that I can't imagine it would take any additional parameters other than the bucket go ahead and grab this okay let's take a look here and see if that works no doesn't like it it's CU I added this little three here we'll try this one more time we'll go ahead and hit enter and so we can see we're getting some parts and that's the one that we um are currently working on so I want to break up this file into 10 parts so that's something that we'll need to go ahead and figure out um so I'm just going to take a look here so we have take this and by the way if you didn't get it in this point we could get the um uh that upload key from this here so I'm just going to go ahead here and paste this in and let's say we wanted to get the upload ID we can grab it by going here and saying query upload and then we'll just say upload ID and if we do this it should then return them all so yeah so just in case you um you lost that there you can get it from there but anyway we want to upload each part we need to divide the part into separate files so that will be the next thing um how do we divide a 50 megabyte file into five files using the uh using uh Linux CLI Linux commands and so I'm just going to see if there's any kind of builtin command so it's seems like we could maybe use the split command let's see if that even exists split it looks like it does because it's timing out I hit control C to get out of that we'll go back over here we'll take a look here so we have split hyphen B 10 m d and then prefix prefix is the output files so for example if you have part part you're going to name the files part 00 1 002 all right so I'm not exactly sure what they want us to do here but what I'm going to do is just put the command and see what happens we'll say large file and say txt and now it's broken it up into those five parts so what I'll do is just delete this out here per me delete and we'll go back over to uh here and this looks pretty good split file and I'm going to go here and we were going to say large file part and that way it will split into separate ones I'm going to grab this here because I really don't want to again uh have any of these files for later I don't want them committed to my repo so I'm going to go here and just paste this in here and put an asct on the end so any file has that particular nameing it's going to ignore it um we'll go ahead and split the file and try this again hit enter and so now we have our parts and here we have different keys I think we have to use the same key here I'm not sure sure why they named it like that but uh large file.txt maybe we'll simplify it by having it up like this and then we'll just keep the body on a separate line and that on a separate line so we have part one and this is going to be named large file part Z 0 and then we'll just keep going through it here um if we want to make our lives a little bit easier we can set this to an environment variable so I'm just going to say upload ID and I'm just going to grab this here okay and I'm just going to paste it um to be uh this value here I'm just say export on this cuz I don't want to continually type this it's kind of frustrating eh so we ahead and we will just paste that in here and then I'm just going to do EnV GP upload to make sure that we can see that that has been set it has been excellent and so now what I'm going to do here is just go ahead and say upload ID and then we're going to reference uh this environment variable here as such and so that way um I think it'll be a lot easier for us to specify all the parts we keep everything as a single line now okay so we'll go here I'm going to delete this line out so we have part one two 3 four five we said we would have five files 1 2 3 4 five so we have 1 2 3 4 5 and this is going to be 1 2 34 the thing is is that each of these are going to return back an e tag technically the E tag is an md5 hash so we could actually generate it locally but I would rather get it back when we actually upload the part I want to see what we get returned when we actually upload that part there so we go down to upload part I want to see what the output looks like and so here it's suggesting that the output is going to be um pretty flat and just get a e tag so I don't think we have to do anything special it's going to be pretty easy to get each part but just in case I'm going to go here and type in query and I'm going just type in E tag because this isn't wrapped in anything a lot of times when you get these objects they'll have a highle object but it doesn't so I'm not really sure what to do for this query maybe period on this here um this part will definitely upload but we'll go ahead and try this out and see what happens and just remember to be in the same folder or this is not going to work so it doesn't like my query um the other thing that's going to matter here and I just realized this is that there's a period in the um the name and that's going to mess things up so well actually maybe not because we're passing it this way so let's try it without the query first first just going to make sure I still have all my parts it keeps deleting one out here we just take this line out here like that so let's just try this individually and see what happens so I'll go ahead and oh the other thing is the bucket is not the correct name so that's not going to work if we don't fix that so I'm just going to go ahead and I'm going to do the same thing here I'm just going to say export bucket name or bucket and we're just going to assign it as such and make my life a little little bit easier so we'll go here and just say if you're wondering how I'm doing that that's a Vim command um you'll just have to do it however you need to do it we'll go ahead and we will paste that in here and it's still complaining why what's the problem we'll go ahead and hit enter it says body value must be a file to a path so what we can do here is just do this and that might resolve our issue here so I'm going to go ahead and copy that and hit enter and we'll hit enter again unload load parameter no such file or directory so now it's saying this file doesn't exist so we have a spelling mistake I forgot the e so I'll just take it out of here or you know what I'll do is I'll just fix the file names we'll go here and rename this one will have large file and then this one will have large file and then this one will have large file and then this one will have large file I know I made one mistake there I'll go back and fix it here and we'll just keep fixing those names until everything is consistent I should have just named them Parts it would have been a lot easier I don't know why I had to put the name large file in there okay and so now what we'll do is we'll go over here and we will upload this part I'm going to hit enter and it still has a problem I'll hit enter now what is the problem now blob values must be a file to a path and also notice that this file is named incorrectly should be e hold on here did this actually work it is either binary or Etc so I'm not really confident with this here because this one the file was named incorrectly so I'm not even convinced that this actually uh generated out correctly so what I'm going to do because that that file was called l a r GE file text um I'm just going to go back here and let's make sure that we can see the file size here make sure sure we'll do this again and yeah there is 50 here these are these all are the right files but I'm going to go ahead and delete them I just don't trust them okay and I'm going to rectify our mistake and instead of having large file I'm just going to call this part and then up here we'll just call this um part and so that might make our lives a little bit easier here good and so um we'll clear this and let's see if we can get this to work we'll hit enter hit enter again the body blob value must be a path to a file okay what if I just put the body as this that worked okay uh my major concern is that that part might not actually be um a streaming blob so this argument of type is a streaming blob its value must be the path to a file and must not be prefixed with this okay so this is just how the API can be extremely inconsistent um and it can be really frustrating sometimes you have to do it sometimes you don't but we'll go ahead and do this instead and the thing that we want out of this is we want to grab all the e tags so as we're doing this remember to grab all the e tags another thing that we could do is check the list Parts here before we um decide to do that maybe we can grab them retroactively so I know that there is a command for list parts let's go take a look at that list Parts here we'll go down to examples and so this will list out the parts and it'll actually have their numbers so we don't have to really worry about collecting them right now and we'll just go ahead and execute all these here so we'll go ahead and do part two and then we'll do part three and then we'll do part four and we'll do part five and then we'll say get all the parts with their e tags complete here we might have had this uh previously I guess not must have delet deleted that but um here when we want to complete it we are going to need to provide it the um the multiart upload file so here we know we have our upload ID so we'll just take this out whoops um let go back here we'll say upload ID and then we have our bucket here this will be our bucket and then here this is just large file.txt and um there is that structure file um and what does it look like it expects it to be in this format here so I think that's exactly kind of what we get back when we list Parts it looks very similar so what we'll do here is we'll go and say get all the parts of the D tags of course if you're uploading more than one actually no it only return the parts for this upload so that would be fine we're not going to end up intermixing other parts so we going to go say upload ID here and we'll say large file.txt and then the bucket here is going to be bucket we're going to go see what we get back as a result because we don't want um all of it what's its problem the specified upload does not exist I mean I think it does because we got all these correct I'm going to copy from here no that's identical and is this identical yeah so then what's the problem we'll go back to list parts so it has bucket bucket key key upload ID we'll type in clear here I'm going to try this again I'm going to paste this in so now we get it back and we're getting the parts back it does have other data in there and I'm not sure if that's what we' want but I do know that we want the parts so I can go here and say query and say parts and if we do this we'll now only get back to the parts I only want very particular things in here so maybe I can do this by doing this and specifying the exact ones I'm not sure so we'll just look up jsme path I believe that's what um adab us is using it's j m well we'll click it here I think it told us here just a moment ago yeah there it is J JM path and I only want very particular uh information so I definitely want um the like I definitely want that stuff but I only want particular things so let's go to the tutorials and see if we can um just select specific information so here we get Fu here we can select very specific numbers this one's not very useful to me because I want to uh get um specific things oh it's all on the same page here okay so we got people so I will grab people and just the first name so again just kind of f how we can grab that information so here we can do name and select and we'll return it but I still want it to be in the structure of adjacent object here we go this looks this looks like what we want so I'm going to grab this one here and we're going to go back I'm going to paste it in like this and we only want specific things we want the part number which is part number and then here we just want e tag and we'll go take a look at what it wants so this um complete multiart wants part number e tag it's spelled exactly the same way so we can just do this part number e tag and then this is off of parts of course the only thing that I want is I still want this to be um I still want this to be wrapped in that part and so I'm not sure if that will do that let's go ahead and see what we get if we do this we'll hit enter it doesn't like um this part here why name name part part I don't really see the problem here maybe we need to wrap this in uh double quotation sometimes that helps with these queries when they get a little bit too complex because it might be breaking on the period a or somewhere else I'm not really sure could be the the curlies that's messing it up and so now we're getting our data structure back the only thing is that I still wanted to still return parts right so that's the only thing that it's doing that that I don't like so I'm not exactly seeing that but at least it it saves a little bit of the work for us so it's not exactly perfect but it's pretty close and I was just hoping that we could get an exact um output here that that worked for us so what I'm going to do is just dump this to a file and we're going to say um manifest I guess or parts parts Json and so I think that should work and there is our our file there what does it want in front of it it just wants parts and then a wrap like that so we'll go here and we'll say we'll hold on we'll check again curries Parts double quotation around this and then we need to just bring that on the bottom here and I'm just going to bring this in and indent it once and so I think this is the structure that it wants right we're going go back and check this again this one um it's escaping it here I don't really want it escaped the E tags so I mean they don't escape it so if they don't escape it I don't want to escape these so I'm just going to take these out so we don't have a problem we could also use JQ to make this a bit nicer but I don't want to play around with JQ right now I just want to get this done I am hungry it's near lunchtime so we have our parts here this looks good and so now what we need to do is supply this file so I'm going to go ahead and say Parts Json and I'm going to hope that this works so fingers crossed fingers crossed okay and we'll go ahead and we'll copy this and we'll hit enter no errors the way we know that this worked is if we go over to here and we refresh and if the file's there the file's there and I don't see our file so let's go back over here oh we did we have an error um okay I don't think it actually executed this command so I'm going to go ahead and try this again right click copy right click paste enter that's not a good indicator but we'll go ahead and hit enter anyway let me guess we're not supposed to provide um the path like there invalid Json there's something wrong here I notied that the other one you don't do file colon colon but this file col SL but this one you actually do um well there's definitely invalid Json but I thought we we took all those out so maybe we didn't save the file oh for freak's sakes I'm going to make this a little bit easier on myself and I'm just going to go ahead and flatten this a bit okay I'm just going to bring these up like this so I can see what the heck I am doing you know what happened is that we ran the file again and it took out the formatting that's what happened so it wasn't that we didn't do it it was just that um we mucked it up e and so we'll just make this a little bit more compressed and the advantage of this if you know how to do multiline select you might not be able to know how to do this but I can just go down the side and remove those extra ones but they're gone I thought I was going to have to remove them manually but they're they're not there so I suppose that's fine just make sure that this file is the same just going to double check here I'm going to hit Q here and type in clear let's go back over to the read meate we're going to copy this again we're going to paste this again we're going to hit enter and it looks like it worked we're going to go back here and refresh the file is there it's 50 megabytes um this is a random file that we generated so I'm not sure if it even has any useful information in it it's a really large file so we can't view the contents of it so it doesn't really matter we'll go ahead and delete this so we've uh essentially achieved what we wanted to achieve so that is great I'll delete this object and now we can get rid of this bucket so we'll go all the way top here and we'll just clean up say clean up and we'll say remove bucket and just in case you did not remove the file you can do it here as well large file.txt we'll go ahead and copy that hit enter and the bucket is gone the file is gone and I want to make sure that we're not uploading those parts so our G ignore we changed it so I'm going to go ahead and just switch it to this give this a refresh okay great and add this here whoops multiart upload and there you go we did multiart upload see you in the next one let's take a look here at Amazon S3 bite range fat chain and this allows you to basically fetch part of an S3 object by specifying um the uh the header range to get that information so here's an example of some python or Bodo 3 code that we have written here boo 3 being the uh a SDK for Python and the idea here is that when we're doing our get object we're going to provide a range and we're going to say the range of bytes that we want so 0 to 99 is going to give us the first 100 bytes and then we can read that data this is really great if let's say you have a CSV file or a Json file or a log file and you just want to get a section of it so Amazon S3 allows for concurrent connections so you can request multiple Parts at the same time just like how in multipart upload we can upload multiple Parts uh with bite range fetching we can uh download multiple Parts fetch fetching smaller ranges of a larger object allows your application to improve uh retry times when requests are interrupted typical sizes for bite ranges requests are between 8 megabytes or 16 megabytes um so now the other question is what happens when you want to uh Stitch things back together let's say you're trying to do basically a multiart download well what you can do is store all the parts and reconc them back together so here's kind of an example of us downloading a file so here we have the bite ranges of of or basically the parts that we want to download we're going to inter uh iterate through them and then we're going to pend them together then we will uh write the file out as a complete file and we'll end up with a single file so depending how large the file is you might need to write each part to disk if your program does not have enough memory to hold all the parts as in this code example we're holding everything into memory um but you know again bite range fetching it could be you want to do basically multi download or you just want a part of the file so it's really either or okay hey this is Andrew Brown and this fall along we're going to use the Amazon S3 bite range fetching option so we can fetch part of a file uh from S3 so what we're going to do is go over to our ad examples uh repository I'm going to of course open this up uh in my cloud developer environment and the idea here is that we want to upload um a file that has enough data that we can use this uh by range fetching on so while that's opening up I'm going to go to adabas S3 CLI I actually want probably the API one here let's go take a look if we can go find that flag um so we' probably be using get object and if we go into here it probably has the flag down below somewhere um there's something range right here range so we can go ahead and try to use this range as a means to grabbing the data so download the spe specified bite range of an object um there and so I guess that's how we're going to do it here we have more of an example down here so here would say the example below demonstrates the the range to download a specific bite range of an object uh that needs that there okay so this is now open I'm going to go and create a new folder here we'll say bite range and we're going to create a new file here I'm going to say um hello.txt and we're going to have hello world and so what I want to do is I just want to extract out some of the text from here and not all of it now the question is can we make it readable I'm not 100% sure but we'll do our best to figure it out so let's make a readme.md so we have a place to write our commands we first need to create a bucket so I'm going to create this bucket and it's going to be it was S3 um make bucket S3 C range fun AB for my initials and then we'll put some things in there some numbers we'll go ahead and create this so we'll say allow we'll hit enter and so now uh it's created that bucket and so the next thing we want to do is upload our files so upload our file so we'll say inabus S3 put object I'm going to do this just because we'll have to do get object here in a second and this will have to specify our bucket right and then the next thing we need to specify is our did not do what I wanted to do that's fine um is our key get out of here just things opening up on me while I'm trying to work here hello.txt and then we need our body and so our body is going to be hello.txt and I'm hoping that we can just specify the file here that doesn't always work it depends on what you're using but in here I'm going to go CD into the by range directory and that looks fine let's go ahead and upload that file we'll go ahead and hit enter I just type in clear because there's some something wrong here it doesn't like something something we'll try this one more time S3 put object bucket range that looks fine hello key that looks fine and it doesn't like the sub command oh it has to be uh API that's why okay so we'll go ahead and copy this and we'll paste this here we'll hit enter hit enter again what is it not like now okay must be a path to file no problem we'll change that copy paste enter okay great and so the file is now uploaded so now the next thing is we want to go grab a range so we'll go here and say uh get an object range and see what happen so we'll just go ahead and grab this example here and we'll go ahead and paste this in and we know what we want is our bucket and our key so we'll go grab those and here this would be just like the range so uh uh range just say range. txt I'm not sure if it's really dropping that to txt and the idea is that this file has a certain amount of bytes in it so if we do lsy la sorry lsen La this has apparently what 11 bytes let's go count this here one 1 2 3 4 5 6 7 8 9 10 11 there's 11 characters so the idea is that we want Hello Maybe 0 1 2 3 4 we'll get us that let's find out if that works 0 to 4 okay we'll go ahead and say copy and paste and hit enter it downloaded it let's take a look at the data we got hello that's kind of cool so let say let's get only the word hello get only the word world so we'll try to get the other word now and so here we have we said 0er 1 2 3 4 5 so 6 7 8 9 10 so 6 to 10 is the range for the other one so we'll say six to 10 and we'll go ahead and copy this we'll hit enter um didn't didn't create the file name we want but that's fine that's that's that that worked and so we got it so there there you go that is uh bite range very straightforward let's just clean up our bucket before we uh go here so let's just say clean up we'll say remove bucket here we want to remove that file which was called hello.txt I believe hello.txt and so we'll just clean this up here remove this and then remove that and commit my code just show bite range and wow that was uh that was pretty darn easy so that was great not always the case but we'll see you in the next one okay ciao let us talk about S3 interruptibility and the term interruptibility if you've never heard it before in the context of cloud services is the capability of cloud services to exchange and utilize information seamlessly with each other um so the idea with interruptibility is that it's just it's very easy there um there's no intermediate application integration service that does it it just works and uh the reason we don't cover all these functionalities in this S3 section is that we cover them in the respective um service that has an integration with it so I kind of wanted to just make sure that you understand that there's a lot more going on with S3 than just what's in this S3 section but here are examples of AD services that have interruptibility and that's by uh them dumping data to S3 or doing uh something otherwise and it's not a complete list it's just an example of popular services that are working with S3 the first is ec2 so it stores snapshots and backups in S3 RDS stores backups and data exports to S3 cloud trail stores API call logs in S3 cloudwatch logs export logs metrics to S3 itus Lambda outputs data logs to S3 itus glue um it stores its ETL results uh in S3 uh for Kinesis uh when we're talking about data streaming uh it can dump data from fire hose to S3 so there's a service called Kinesis fire hose and so it can stream to S3 or the results to it EMR so it uses S3 for input output data storage red shift it unloads data S3 uh it us data pipeline moves transforms data uh to and from S3 Amazon Athena outputs query results to S3 iTab I iot cor stores iot data and S3 if you are taking any kind of data related sort of ification they really want you to know interruptibility between services so make sure you understand the limitations and restrictions or what can and cannot be moved to and from with S3 and other services okay hey this is Andrew Brown in this video we're going to explore uh server logs for S3 so we'll make our way over to S3 and as per usual we have the it of us examples repository and I already have it open I'm going to create a new folder in here this is one is going to be called server uh logs and yeah server logs yep and from here we'll create a new readme file and as per usual we're going to go ahead and create ourselves a bucket this one's a little bit different because we need two buckets a um a source bucket and a destination bucket so create two buckets so we have ad us S3 make bucket S3 C SL SL this will be server logs um Source ab and I'll put a few numbers on the end here like this and then we'll go down here and say copy and paste and this will be the destination bucket so we'll have two buckets here to work with Okay and the next thing we want to do is uh configure uh server logging so we'll make our way over to ads S3 AP Pi CLI and I'm sure there is a command for this um logging so this one's called put bucket logging and we'll go down below and it looks like it needs a Json policy as well as uh this here so I'm going to go ahead and create a new file here called policy. Json and we'll go ahead and copy the contents of this policy and bring it over and paste it in and then we'll go back here and grab this command here and we'll go ahead and paste that on in okay and so now we need to specify the bucket so we're going to be turning it on in our source bucket and then we'll take a look at our policy so our policy wants us to specify which bucket so I'm going to assume it's resources for our current bucket let me think about this we want to every time we put objects S3 ser access logs and this policy is anytime we have that and it's coming from our source bucket so hopefully that makes sense but I'm going to go ahead and try to figure this out so we go ahead and copy this and this one says source so I'm going to place that here we have the abis account ID that's really easy to grab we'll go up here and grab that here but what I'm not 100% on is what is this is this for the source or the destination right so if the destination is what stores the logs is that what it's going to be or is it for what it's going to read and that's what I'm not 100% sure on let me double check okay I'm back here and we're just going to carefully read so the following put bucket logging example sets the logging policy for my bucket first grant the logging service principal permissions in your bucket policy so I'm thinking that my bucket is the destination bucket that's what I'm thinking here so we're going to go back over to here I could be wrong but we'll go ahead and try this out and we'll place this right here there's a little bit more to read so that's for that to apply the logging policy use the put bucket log it uh logging and I mean we definitely want to apply logging so let's go ahead and do that as well it's interesting that that this is not a single command and they're separate I'm not really sure why that's the case I guess we'll find out so I'll go on the end here and paste this in here this will be logging Json whoops and we'll have to create a new file here called logging Json we'll go ahead and grab this code and we'll go paste this in and and we'll flip back over to here if my bucket is the destination bucket we'll copy this target bucket yeah so the target bucket is that okay Target bucket and logs so it's interesting that says logs here yeah and so that would be the target bucket as well so I think we have it now correct let's go ahead and see if we can uh turn this on so we go ahead and copy this we'll paste it in hit enter fingers crossed it doesn't like something so I'm carefully looking here a S3 API that's fine the bucket name is fine the policy might be an issue let's see where we are we're not in the right directory that's our problem so go here I'll type in clear we will try this again I'll hit enter it still doesn't like something we'll go ahead and hit enter and see what it says the options are unknown for bucket logging status it's interesting that it says it doesn't know what it is because we copied it right out of here are we on version one or two let's go to the top here let's make sure we're on version two we'll go down below it still says bucket logging status I don't even think we see policy here let's go down here nope it has the same stuff so it's interesting that it's different put bucket oh this one's put bucket logging so this is the policy and this is the logging oh okay all right so they're actually separate commands it was very uh tricky there it looked like they were the same so go down here and I guess this is a bucket policy all right that makes sense um it S3 API put bucket logging guess we got to be careful with what we're doing here and we'll copy this and hit enter policy has an invalid resource well that's not exactly what we wanted to say um what resource could be invalid here logging S3 Amazon ads.com I'm just trying to see what could be invalid because it looks fine to me just give me a second okay all right so um you know chat PT said hey this looks fine and now they're kind of narrowing down and saying maybe it's an issue with the resource so in the destination bucket we don't actually have that folder and so maybe it wants to have that folder first which I don't understand why CU it seems like it'd be very easy to create that folder but let's go ahead and I guess create the folder and see if that helps at all so we'll go and say create logs folder for the destination bucket so we'll type in ads S3 um it S3 I'm trying to remember I did this the other day put object and then or we'll just say um copy S3 slash paste I did not mean to grab grab that we'll try this again paste come on just copy sometimes it just refuses to copy right click copy right click paste I'm not crazy this this is the one disadvantage of working with Cloud developer environments listening to me complain about copy and paste um and so we're going to go ahead and type in logs and put a forward slash and so I'm hoping with that for slash it's just going to know that there's no file to copy so it doesn't like that so we'll go ahead and try um put object instead so we'll say S3 API put object and then instead of this we're going to specify bucket here like that and then here for the key we'll just do this I know this will work for sure and so now that folder exists so let's go ahead and try this again go down below and see if that fixes our issue and it still doesn't like it what does it like I don't know give me a moment I'll try to figure it out my other thought here is let's just go try to apply it within the actual CLI or console sorry the console because sometimes it might provide additional information and so that's my uh thought here I'm going to give this a nice refresh make sure that we can see um uh the right folder I believe it's this one do we not have the word Source in it we did so where is it here back over here Source it's up here good and I'm going to go to properties and we'll go down to um permissions and I know this is blocking I don't care that it is but um it's my expectation we should not have to turn it off so go ahead and paste this in notice that there are no formatting errors it's not upset we'll go ahead and hit save policy has an invalid resource so my other thought is that maybe it's a block Public Access issue it really shouldn't be but we'll go ahead and just say keep the bottom two and leave these off or on sorry type in confirm here let's go ahead and try this again no no issue I'm going to just carefully look here and see if there's anything else that might be missing um what was it showing here for resource iron no that's the way you you specify resource iron we'll go ahead and try this again save changes all right give me a second let me figure this out all right my other thought is that maybe what we can do is we can just create it so what I'll do is I'll just clear this all the way out and we'll just try this from scratch we'll say add statement and the statement is for S3 okay and all we want is logging or sorry we want put object sorry put object okay if we click into the principal We'll add a resource here and we we want to select for objects we'll say add source for your own information it's really silly that they're doing it this way um it's because before this used to work in a different way and so it was a lot easier to um specify that stuff and so I was hoping that there was some way I could go through here but it looks like they've actually made this UI not as good as prior so that's unfortunate but sometimes it can can help rule out some of those issues there um another thing I tried is I I tried taking this out so I said okay let's increase the font here cuz it's super small let's say we took out conditions it is not going to work let's say we took out this and did this it's not going to work what if we just had the bucket not going to work um another thing we can do to rule it out is we can go over to our buckets and uh I'll just double check what we're doing here so we have destination bucket where we apping this in the source bucket okay that's right and so if we go over to our destination bucket here there should be an AR that we can grab properties here so I'll grab this AR and what if I copy and paste this exactly in place I get something else well hold on that's not the uh that's not the right bucket right because our bucket is called this right so um it's because I did this before and I I failed for the same reasons so I grabbed the wrong bucket we want this one here I'm going to go ahead and grab this one and we will paste this in place and then as a sanity check we'll go back over to our our inline bucket here and I'll paste this in place as well super small so I'll increase it and we will paste it it's identical so I'm not writing anything wrong here but there's something that it really really really doesn't like and it's really really frustrating to uh figure out um so let let me uh figure this out okay not much useful information here what I'm going to do is I'm going to go back over to um our buckets and I I know this sounds kind of weird but I'm going to go ahead and just do some do a sanity check and we're going to go into the destination bucket we're going to go to its properties and we'll take a look at its uh permissions here I'm going to turn them all off as a sanity check really we shouldn't have to do this but I'm going to go ahead and try this anyway and I'm going to go back to our other bucket here to our source bucket and I'm going to go ahead and completely turn this one off as well and say confirm and then in the source bucket which I'm in I'm going to go ahead and try to run this policy again and hit enter and it still doesn't take it this is driving me crazy okay give me a moment here's another thought maybe we got to turn logging on first before we can apply the bucket policy why would that be I have no idea but I'm going to try it anyway so I'm going to go ahead just copy this I'm assuming we have to turn logging on the um Source bucket so we'll go ahead and do that hit enter enter um just fix that here S3 API and we'll copy it we'll go ahead and hit enter we'll try this it's turned on now what if we try it no but logging is turned on so I guess that's good all right give me a second all right so I'm just thinking about this for a second and um you know this one's saying the bucket policy of the target group must not deny access so this is suggesting that uh the policy the bucket policy maybe is for the Target so in here it says source and Source coming from here and for this so you know what I think I got it backwards and that's my issue is that I was running it on the same one so so this actually isn't going to go in the source this is going to go in the destination okay and that kind of makes sense because if you're going to write a bucket policy it's for giving access to the current bucket not somebody else's bucket so when we're looking at this one yeah that makes total sense like I can't say hey from The Source bucket Let Me access from this from my bucket policy so that's probably our issue here so I'm going to go ahead and just uh copy this and so we'll go ahead and paste this down here so um turn on logging for for the source bucket and then we would say allow external buckets or allow Source buckets to put data into this bucket so that was probably our issue there all right so that's a lot more clear um I guess the question is what does it look like when we put data in there so we'll find out by pushing some data so we go ahead here and say like Source data and I'll just Echo some files here so hello world hello.txt and then um we'll need to copy that it was S3 CP S3 for slash go ahead here and copy and paste gets me everything I don't want everything uh we'll try this again copy paste and then we'll do/ hello.txt and we need to say what we want to copy hello.txt as such we will copy this down below make sure we create the hello txt file we'll go and paste it in here and so now this file should be uploaded we'll go back over to our destination logs and what I'm interested in is seeing some logging information here here which I don't yet see let's confirm that logging is turned in in our oh yeah no there it is sorry yep you know what it was I was searching uh the two terms there and that's why I was getting confusing um but yeah we have the source and the destination so let's just confirm that source is turned on so we'll go here to properties and I mean I when I say Source I mean logging turned on management maybe no permissions no logs here it is so it is turned on okay to this destination and it has its own determined um format here but we'll go over and we will see if there's actually any files in here not as of yet so where are they let me figure that out okay so according TOS um it can take a few hours of time before they are recorded so we're going to have to come back to this video and see the results um while we're waiting I might as well just place a few more commands in here so I'm going to go here and just make a few more files hello Mars Mars Mars hello moon there we go so we've placed a few files and we're going to have to come back to this later so I'm just going to commit this access logging and uh we'll we'll revisit this in a part two okay I mean it'll be stitched As One video but I'll be back here in a moment okay all right so it hasn't actually been 48 hours it's only been I don't know about 30 minutes an hour I did a different lab while we were waiting um but what I did is I looked in here and it looks like we're actually now getting access logs so um I guess the question is how are we going to view this and notice we actually have quite a few um which is interesting because I don't remember uh creating that many files but um I guess what we can do is go ahead and download these files and take a look at their contents so I wonder if we can sync in the other direction I'm going to take a look here and say a S3 sync and see if it can go in both directions so sync directories new updated files from Source directory to the destination only creates folders in the destination if they're created so what I really want is go is to go in the other direction we can sync from here to there to local directory so we can do that what we'll do is download logs we'll say whoops download access logs and we will copy this and we'll say sync and from here I will do this and we'll just say logs so this will be as such logs I'll make a new folder in here and we'll call this logs I'm going to just double check make sure that is the name of the folder it is good good so I'm going to hope that this just works hit enter and we'll go over here and I do not see any files so there must be something that it wants me to do um let's go back over here syn command Etc what if I put a wild card on the end here like this all right maybe chat jpt knows better than us let's go ask them um I want to sync files from a S3 folder to a local folder using the adus CLI and I wonder if we can use the sync command that's what I'm hoping that it will do we'll give it a moment to generate so taking a look here um looks really straightforward let's go adjust this the only difference is that they didn't have the trail the trailing ones on here and it's just not downloading do I have the wrong bucket oh it is this is the source we want the destination bucket that's our problem so I'll just change this to DK I kind of prefer um having the for slashes on here so we'll go ahead and copy this we'll paste it in there we go they all downloaded let's go take a look at the contents now so you can see we have stuff here um in terms of this logging information in this format it's not the nicest I bet we could probably send this to cloudwatch and then use cloudwatch insights we could also probably send this over to Athena to analyze so if we again wanted to extract any useful information we'd have to do some uh additional work to do that just give me a moment okay so immediately adab us actually suggests to use Amazon Athena so we could uh query logs that way uh I'm not sure if I want to go to that degree because it can be a lot of work to set that up but let me just make a decision here okay you know what I think we should do it we should go ahead and try to use um Athena because it is a really uh awesome service that is um useful to utilize and I think that I'm comfortable enough to do this very quickly and we're not going to eat up too much time I'm just concerned that we'll turn this into an hour video I really don't want to do that but what we'll do is go ahead over to Athena you can just type Athena to get there right and uh notice it's based on 5.5 terabytes so really not going to spend much money here we'll go explore the query editor and what I remember is that whatever you want to do you have to write a query to to create the stuff um could we use the CLI for this probably I'm just going to do this in the terminal or in the console because it's a lot easier and we can copy paste all this stuff in if you're looking for this link in particular I'll put this in the readme so we'll just say um query via Athena I mean we could use the CLI maybe we should use the CLI I'm always tempted to use the console in some areas but um we'll take a look here Athena it just depends how easy it is to find the command like is there a query command here because the last time I remember is like you have to set up a lot of stuff in here so it might not be straightforward yeah I think it's going to be a lot easier to use the console in this one because I think it requires M multiple steps and it's probably better to do it via code instead e um so we'll go back over to the documentation it looks like they have all the code for us uh I'm going to copy and paste it over here just in case in the future if they change it and you want to follow exactly along here so we'll create a database S3 AIS logs DB that seems fine to me I don't think it matters what we call it so we'll go ahead and put a I don't know if we need the semicolon on there and so I want to go ahead and execute this um they've changed the UI since I've been here last so I'm just carefully looking for where we execute it format query what if I put a semicolon on the end there why can't I run this it used to be just run it before you can run your first quer you need to set up the query result location in S3 okay so we'll go ahead and hit edit and we will then go select our bucket I suppose we hold on here query result oh where we're going to place the results so what we'll do is we'll create a new bucket for this and we'll just say um it S3 um make bucket S3 col SL slash and this will be Athena results AB I'll put a bunch of numbers on here so that's where I'm going to place mine we'll go ahead and copy this and hit enter and so now we have a bucket I can go ahead and grab this it wants the full path here I'm going to just grab this full S3 path we'll paste it on in here you can create and manage life cycles at don't care about that assign bucket full ownership over queries sure we'll go ahead and hit save and so now we have a place where um I guess the results are going to go we'll go back over to the editor and um I guess we now have that set up so we'll need to create ourselves a new database the only thing I'm confused about is how does it know that it's been set up there I guess it's under work groups is that how we set up the bucket not 100% sure but I think that's okay we'll go back over to here and I'm going to go now run this query so we'll say run and that's going to go ahead and create a database I'm going to assume that um it is now here so we can drop this down so that is good and now let's go take a look at Future queries here so in the query editor run a command similar to following the following recipe with the name uh that you want to give the database uh to specify the S3 location you you must provide the URL where the where your logs are to be delivered that's fine um the URI must include the bucket name and and prefix in the following format we don't want a prefix so it doesn't matter in the query editor run a similar command to the following query in the table schema so we'll go ahead and copy all of this and then we'll go on down below I'm just going to uh go ahead and just put these in back tick so it's a little bit easier to look at this is um SQL okay and we'll paste this in here and we'll take a look and see if we can make sense of this SQL so here we have the database and then we have the logs these are all the possible things here and then we're specifying the location I guess this is the location of the bucket so up here that's our database and we're going to put oh we're going to create thisable so we're creating a table and then we're telling it the source of the logs so um we definitely know what that is if we go back up here it's going to be this right here so I'm going to go ahead and copy that and then go down below and we'll paste this in and it looks like they do all the work for us now really I think it depends on the log format that we chose but I'm going to assume that it's going for the default one so here's all the columns and then there is a way to deserialize information so we're using this deserializer in order to do it this is the regular expression to select all of um The Columns so it's using regex say this is going to be that column that column that column that column so you can do that then it's our input format so we have raw text which is that is what the log files are and then we have our output format um I'm not sure what we're looking at ignore key text output format so it's outputting something and that's our query so I think that will be good we'll go ahead and copy this and fingers crossed this just works the first time a lot of the times you need a semicolon at the end here but I think it doesn't matter we'll go ahead and hit run and it's created that table so now we have this table with a bunch of columns but we don't necessarily have any of that data because we need to query against it so we'll go down below here and here we have a few examples so show who deleted an object object we didn't delete any show all operations that were performed by IM user show operations that were performed on a specific object in a specific time uh so you know we're just looking for a particular query let's go take a look at some of our data here and see if we can make sense of any of it um I see some tagging going on here yeah I'm not 100% sure what we're looking at it's really hard when uh the logs are like this again just carefully going through here I see ownership controls we have this user maybe we can just say what did a particular user do so I have this RN which is my particular user I'm going to go ahead and copy this out yours is going to be different you need to figure out what yours is and I'm going to go ahead and copy from a specific user so we'll go ahead and do this I'm going to go down below I'm going to paste this in here and so now I have my own AR you'll have to go get your own Arn that is not what I wanted to do sometimes the copy paste does not work as you know for me repeating again and again and again we'll go ahead and paste this in so that is a lot better and you know I don't know if I want everything um maybe the bucket name would be nice so it's showing us the wild card is there uh another way okay so it's just plainly like that so we can do that I'm going to just split this here so I can quickly grab the fields I want so we have things like bucket name key operation let's see if we can get anything interesting out of this so this is my query just type in SQL on the end here so we get some nice formatting here make sure you don't muck up the commas we'll copy this we'll go back over to Athena and it will print the results or it should print the results down here below but it should also put it in our source bucket so run this and we get back some operations a lot going on here so we have get versioning logging status encryption website object lock configuration replication tagging we're getting a lot of stuff in here um I thought we were just going to get when we placed objects in the bucket but um looks like we're getting a whole lot more going on here it's also possible that uh when we configured something maybe we got more information but hey we got logs we know how to query them uh do I really know what's going on with these logs no I'd have to work with them a bit more but at least we accomplished something here so I I I'm I guess I'm happy with the result that we have here let's go ahead and clean up this project so the easiest way because there's so many logs is to literally delete the bucket so that's what I'm going to do um you can delete the Athena bucket if you want I'm not going to CU I might use Athena later so I'm going to leave this one alone for now and I want to go ahead and delete our source and destination so we'll go ahead and I'm going to say empty this bucket and we'll go ahead and do that uh then I'll empty the source bucket and we'll go ahead and do that and I'm just going to go clean these up in here it's a bit easier so uh empty your buckets then delete them so go all the way to the top here and we'll go ahead and copy these and then I'll paste it in here I'm just going to put RB RB the reason I don't always highlight this like it's good to put all the formatting the backck around this it's just that I'm just trying to keep with time so yeah anyway there we go uh I don't really want all these logs down here so I think I'm going to go ahead and just add a um get ignore in here and we'll go back there we go finish off access logs server access logs there you go we'll see you in the next one okay ciao hey this is Andrew Brown from exam Pro and we are looking at Amazon S3 Glacier which is for longterm durable and secure Cold Storage uh also known for uh longterm backup and data archiving so uh looking at uh s Glacier it is for extremely Low Cost Storage and it provides you with durable storage with security features for data archiving and backup so when we say cost effective we mean for a period of months to years or even decades uh and the thing about S3 Glacier is that it's a serverless service so you don't have to worry about a lot of things you don't have to worry about capacity planning or Hardware provisioning data replication Hardware uh failure detection recovery timec consuming uh Hardware migrations so um with that all in mind it's a very easy service to have very very cost effective longterm storage so now let's take a look at what we would use Glacier for so a company who would need to hold on to financial tax records for a period of seven years to meet government and financial regulations is a really great use case for S Glacier let's kind of walk through what that would look like so imagine you have a sales manager and that sales manager uses some custom software sales software that your team has built and so that sales software is going to some kind of reporting engine and that reporting engine is a producing uh PDF sales reports and so these PDFs are being stored into an S3 bucket uh and you have this uh rule where you're going to hold them there for a minimum of 30 days which is something you have to do if you ever want to move something into Glacier um and so you use a life cycle policy to move them into Glacier they're going to get stored into what is called a glacier archive and then what you could do is apply uh a lock policy so those files cannot be deleted for 7 years and that's going to give you that guarantee to meet that government and financial regulations so that is a really great use case for um S3 Glacier so now we're taking a look at S3 Glacier security so Glacier is automatically server side encrypted using AE 256 so that is a cryptographic algorithm uh and as an additional safe card adus encrypts the key itself with a master key that is regularly rotated now for data in transit between S3 and S3 gler uh and that's going to be via life cycle life cycle policies because that's the only real way to move things between those two Services it's encrypted using SSL uh one kind of policy you can use is called a vault access policy and that controls who is able to access the Vault then you also have Vault uh Vault lock policy and that controls how a vault can be modified for a period of time and the advantage of this type of policy is that it can help you enforce Regulatory and compliance requirements as we saw in the use case prior and when the policy is applied to a vault the archive cannot be modified based uh uh cannot be modified based on the policy's condition so uh for monitoring S3 Glacier you have Cloud watch alarms Cloud watch Trails a trust advisor for S3 um Glacier compliance programs it's compliant with sock uh PCI uh PCI uh PCI DSS fed ramp and Hippa so there you go that's the security overview for Glacier so now we're going to take a look at the uh data model for S3 Glacier so the first component is a vault so a vault is a container for storing your archives uh you have to name your Vault and choose a region that uh that it's going to reside uh each Vault has a unique address per region and that is what that address looks like so if you just take a look there um you can see that we have the region in there and on the end there we have the name and you have your account ID uh the next thing is an archive so this is the base unit of storage so you'd store things like photos videos or documents anything that are files or data can be in an archive and this has its own unique ID which uh would uh you take the existing Vault URL and you just uh place the ID on the end there now we have the concept of a job and this performs either a select query on the archive it can retrieve an archive or it can get a inventory of the Vault and so you can use S3 Glacier select and provide a list of the S3 archive objects so that's one way of uh getting data uh the uh each job has its own unique ID and that's what it looks like so it's very similar to the last one but just take note on the end there it says my vaults for SL jobs and that is data models so now let's take a look at notification configuration so when you perform a job operation to uh retrieve an archive it's going to take a variable amount of time for that archive to come back to you which is dependent on the tier that you choose uh for retrieval and also just the size of the archive and so you're going to need some way to be notified when the job is complete and that's the whole point of notification configuration uh so what you can do is you can configure your Vault to send a notification to SNS topic when the job completes uh and when you make an SNS topic that could be delivered to what wherever you want whoever's subscribing it so if it's an email it's an email uh and what it's going to return it's going to return this Json object uh about um the events that has completed so say there it says like archive retrieval completed inventory retrieval completed so the notification configurations are associated with the Vault and you can only have uh one for each Vault so each notification configuration resources has its own unique identifier and it looks like that down below so just take a look there so it looks pretty standard we have uh Glacier the region our account ID The Vault name and then SL notification configuration so there you go so let's take a look at the job operations that we can perform on our Glacier Vault the first being list jobs and this is going to give us a list of jobs that are either in progress or have recently finished then we can describe a job and we can get uh certain information back uh from this operation such as job initiation date the user who initiated the job the job status code and message and the SNS topic to notify after SV Glacier completes the job then you have initiate job and this is where you uh want to pay close attention because these are uh key operations to S3 Glacier and understanding it the first one being inventory retrieval this is going to get you an inventory of the Vault uh so that's basically like having a list of files given back to you uh then you have archive retrieval so using the inventory retrieval list you're going to use this to then cherry pick out the archives you want to actually return then you have a select and this performs a select query on an archive this is a more intelligent way of selecting uh data without uh actually selecting or having to uh retrieve the entire archive uh and the last thing here is that uh we have get output job and this downloads the output of the job you initiated using the initiate job and I want to just point out that all these job operations can be performed either by the API the CLI and the cdk I'm definitely going to show you an example of all three um so there you go so now let's talk about Vault inventory and this refers to a list of archives in a vault and the reason why we have to use a vault inventory is because you can't just explore glacier like S3 view the console you have to get a list of files and then make individual requests for those archives so after you upload your first archive to your Vault Glacier automatically creates a vault inventory on the first archive upload it takes half a day and up to a day before that inventory is Avail available for retrieval and the Vault inventory automatically updates once a day and to retrieve a vault inventory you need to execute the initiate job and the get job output API call so down below we are using the CLI and you can see that we're using we do initiate job and then in the job parameters we're passing in the job archive retrieval Json file and then we're going to use the get job output and that's going to return the output and just to show you the contents of that job archive retrieval file you can see that it's specifying the type which is inventory retrieval we have a description we want the format to be CS V I don't think you can get anything back other than a CSV but that's what it is and then we specify the SNS topic which is going to be used for a notification configuration so there you go so now let's take a look at archive retrieval and so when you retrieve an archive think of it as a file but technically it would be more like a zip file or an archive uh and what you're going to have to do is initiate job and use the type as archive retrieval and so the example I have here is uh an HTTP request so this is literally using the API you don't have to use it this way you could use the CLI or the SDK I'm just trying to get you more exposure to the three different ways CLI API SDK um and so just looking at uh the payload here we can see that we're specifying the archive ID that we want to retrieve uh and we also are choosing our uh retrieval tier so we're choosing bulk here um and I just want to show you that you can also uh retrieve a range of bytes um so instead of retrieving the whole archive let's say you knew exactly what part of the archive you wanted to get exactly bites you could specify that bite range uh which would be um would cost you less so there you go so now let's take a look at a newer feature um which is called S3 Glacier select also known as S3 select um and the reason why it exists is that prior to S3 select the only way you could return um something from the vaults was you had to use archive retrieval and so if the archive was 5 gigabytes you would have to download uh or retrieve the entire five uh 5 gigabytes but with S3 select you can write a simple SQ expression and only pull the bytes that you need from those objects and yes we did see that with archive retrieval we have a range of bytes but this is a lot more flexible and using S3 select can lead to a performance increase of 400% that is quoted by AWS uh it's not a number I made up and I'm just going to show you an example of how you would do um how would you uh initiate that job uh for uh select and again this is using initiate job just like the last two are but now I'm showing you with the SDK so and if you're wondering the code that you're looking at python right now so what you'd have to do is you'd have to choose your tier so we have xade standard and bulk um you would have to choose an initial uh um an input serializer so that means it could either be CSV Json or parquette or it could be gep bzip 2 that just means that that's what it's Expecting The Archives to be um then you have your expression so I don't know if there's any other type other than SQL maybe they're just planning for the future all I know is there's only SQL and so we're just have a very simple query to select some data you have your output serialization so we're saying we want the output to be a CSV file you're going to choose the output location so here we're placing it in a bucket we we have called Glacier results and again uh we using the initiate job see down below where says glacier. initiate job and you could use the API you could use the CLI or you can use the SDK it's just up to you again I just want to get exposure to all those three so now let's take a look at retrieval times for S3 Glacier so when you initiate a select or a archive retrieval job you can specify how fast you want the data and how fast you want to retrieve archives affs costs and it's organized into tiers and before we get into the tiers I want to tell you that you pay per gigabyte retrieved and the number of request and that is a separate cost from just the the cost of storage so the idea is you don't want to be requesting files all the time we just going to takeing consideration the retrieval costs the first tier is expediated tier and for Urgent request you're limited to 250 megabyte archive size however it's super fast as the name implies you can get an archive back within 1 to 5 minutes it's going to cost you 0.3 uh cents per gigabyte retriev and it's going to cost you $10 per 1,000 requests the next tier is standard and there is no archive size limit and this is the default option if you don't specify a tier it's going to take 3 to 5 hours for the retrieval time uh for that archive you're going to pay 0.01 per gigabyte uh for retrieved data and then it's going to be 05 per 1,000 request look at the jumping cost $10 to 5 so just be aware of that the next one is the bulk tier there is no archive size limit and you can even request pedabytes worth of data um I don't know why you Menches that because I'm pretty sure the standard tier can as it says no archive size limit but I guess the point is that when you're doing bulk you're doing a lot of data so the retrieval time here is 5 to 12 hours and uh the cost here is going to be 0.25 so that is a quar of a cent per uh gigabyte retrieved and then uh we have actually the other one's a quarter no the other one's even smaller and then the last one is so sorry the first one is like a quarter of a quarter I don't know it's super small and then the last one there is 0 0.025 per 1,000 requests so there you go that's the breakdown for retrieval time so let's talk about all the different ways that you can get archives or data into S3 glaciers and the reason we have to talk about this is because there is no direct upload view the Management console like S3 uh and so in order to move data in S3 you have to take a few different approaches the first One is using this cool uh storage device which is called S3 snowball it also comes in another variant called snowball Edge which is just a more modern version or slightly larger smarter version of S3 snowball and then you have snowmobile so this is a rugged case containing Computing and storage to physically transport terabytes of data directly to S3 or S3 Glacier so that's how you get it directly to S3 Glacier uh you could also use snowmobile and this is a cargo container for computing and Storage on a tractor trailer that can transport pedabytes of data directly to S3 or S3 Glacier I like to call it a data center on Wheels cuz it's literally a giant truck with a cargo container uh with lots of storage on it another way is the S3 life cycle policies which youve probably seen multiple times uh through this section here you define it in the S3 console life cycle rules to automatically move data from S3 uh from an S3 bucket you have to let data sit 30 days within um S3 or any other tier before it will move into S3 Glacier it's just for some reason it's set like that that's just how it is and then uh the last way is using the S3 multiart upload API so you can't just directly upload uh via the API you have to use the multiart and so using either the API the SDK the CLI you divide your archive into parts and then you upload them in parallel when all the parts are uploaded you use the API to tell s through Glazer to assemble the final archive and so those are your three methods of getting data into S3 Glacier so now let's talk about how we go about deleting a vault because it's not as straightforward as you would think in order to delete an S3 Glacier Vault you must first delete all the archives contained within the Vault and there has to be no rights to the Vault since the last inventory check and because the last inventory check is done every 24 hours the inventory might not reflect the latest information so you might have to wait for the next inventory update before deletion of the vault is possible the S3 Glacier vaults cannot be deleted via the ads console so you're not going to be able to find a delete button at all you're going to have to use the CLI API or SDK and I'm going to show you the CLI example it's not too hard it's just ads Glacier delete Vault you give it the Vault name you give it the account ID and that's how you delete the Vault so now let's take a look at a vault access policy so Vault access policy manages permissions to your Vault you can create one Vault access policy per Vault to manage permissions and you can modify permissions in a vault access policy at any time and so if you're an S3 Glacier you just have this permissions Tab and this is where you'd set your Vault access policy you'd edit the document the the the policy format looks extremely similar if not the same for a vault lock policy so I'll show those together after we go through both of these policies IES uh but that's all we need to know here right now so now let's take a look at Vault Lock And The Vault lock policy so S3 Glacier allows you to easily deploy and enforce compliance uh controls for individual Glacier vaults with a vault lock policy and so you can specify controls such as a write once read many which is abbreviated to worm in a vault lock policy and lock the policy from future edits every time I read WR once read many that doesn't make a whole lot of sense for me so I've reworded it for you here to say worm means once ridden it cannot be modified so once uh once the vault is locked the policy can no longer be changed and so if we were in the uh aabus console and we're looking how to lock the Vault it looks like this you just look for the vault lock Tab and you create that Vault lock policy very similar policy to the um Vault access policy and so uh the the other thing we need to know is what are the steps to locking a vault because there's two steps the first step is uh to initiate so initiate the lock by attaching a vault lock policy to your Vault the lock will be set in progress State and returns a lock ID while uh while it's in the in progress State you have 24 hours to validate your lock policy before the lock ID expires so you have that 24hour window um then the second step is to validate so use the lock ID to complete the lock process if the lock Lock vault policy doesn't work as expected you can abort the lock and restart from the beginning cuz you really really don't want to end up locking your Vault with a misconfiguration because you're going to be stuck with it let's say you set a a vault lock for seven years you're going to be stuck with it so um it's good that we have that 24hour hour period to make sure we're doing the right thing so now we're going to take a look here at policy controls and so this is really just look looking at the format of what the policy looks like for either a vault lock policy or a uh Vault access policy there's going to be some differences but they're more or less the same so here's an example of a cross account that is only allowed to delete the user account that has MFA turned on so if we look at it here just uh you'll notice there's an allow effect and Glacier and we have Glacier will only allow people to delete uh things uh for a very specific account and we have that condition Bool on that says multiactor off present Tru so you have to have MFA and so if you're wondering what uh what type of um uh actions you can perform we have a lot there and if you read through it they're pretty straightforward so abort multiart upload add tags to Vault complete multiart upload delete archive delete fault describe job initiate job etc etc um and the conditions are going to change based on the action so we have the ability to set MFA for that but the there might be different conditions for all the different actions so there you go so let's take a look at data retrieval policies for S3 Glacier so what you can do is you can set data retrieval quotas and manage the data retrieval activities across your adus accounts in each region so S3 Glacier has three retrieval policies the first is no retrieval limit and this is the default option this is uh no retrieval quota is set and all valid data retrieval requests are accepted the second is free t only so this keeps your retrievals within your daily free tier allowance and not incure any data retrieval cost great great option when you're trying to save money and the last is Max retrieval rate so control the peak retrieval rate by specifying a a data retrieval quota that has a bytes per hour maximum and so this is actually what it looks like to set the retrieval policies and you can see in the max retrieval rate we can put a number there 10 GB per hour because again retrieval costs money so you want to to uh make sure that you're not overspending because people on your team are just not aware that it costs to grab uh archives so there you go so now let's take a look at provision capacity so this is where you pay a fixed upfront fee to expediate retrievals from S3 Glacier vaults for a given month and so uh each capacity unit costs about or does exactly cost 100 USD per month each unit of capacity insures that at least these three expediated retrievals can be performed every 5 minutes and provides up to 150 megabytes of retrieval throughput and so what you do is you go to provision capacity and you just go buy them for $100 USD each who is this for well it's for people with lots and lots and lots of data where they have situations where they might need to uh retrieve a lot of data in uh very very quickly so it's just a very specific use case uh but there you go so let's talk about archive metadata so an archive is an object such as a photo video or document or it's literally just a zip uh that that you store in your Vault and it is a base unit of storage in Amazon SV Glacier we've talked about this before you know that uh what you don't know is that when you have an archive and it stored n Glacier it adds an additional 32 gabt of index and related metadata which is used to identify and restore your object so if you uh if you upload n that's 1 Megabyte you actually storing and being charged for 1 Megabyte in 32 kiloby and when you see that 32 kilobytes you think well that's not a big deal uh but if you uh upload a lot of very small files though that 32 Gaby starts to add up rapidly and so you're just it's just you not being able to uh guess correctly the amount of storage uh that you are actually paying for so you really really don't want want to store a lot of small files so if you uh if you want to uh cherry pick out little files from a large archive that's where you use that select uh select range of bytes for archive or you use the select feature but generally you want to upload your files as big as you can onto uh Esther Glacier so there you go hey this is Andrew Brown from exampro and we are looking at AWS snowball which is a petabyte scale data transfer service so move data onto a AWS via a physical briefcase computer all right so let's say you needed to get a lot of data onto AWS very quickly and very inexpensively well snowball is going to help you out there because if you were to try and transfer 100 terabytes over a highspeed internet to AWS it could take over 100 days where with a snowball it will take less than a week and again for cost um if you had to transfer 100 terabytes over highspeed internet it's going to cost you thousands of dollars where snowball is going to reduce that cost by 1 now we'll just go through some of the features of snowball here it does come with an e in display it kind of looks like your shipping uh label but it is digital which is kind of cool um it's tamper and weatherproof the data is encrypted end to endend using 256bit encryption it has a trusted platform module TPM it's just this little chip here um uh and as it says here endpoint device that stores RSA encryption Keys specific to host systems for Hardware authentication so that's a cool Little Hardware feature um and for security purposes data transfers must uh be completed within 90 days of the snowball being prepared and this data is going to come into S3 uh you can either import or export so not only can you um you know use this to get data into the cloud it can be a way for you to get data out of the cloud and snowball comes in two sizes we have 50 terabytes and 80 terabytes now you don't get to utilize all the space on there so in reality it's really 42 terabytes and 72 terabytes and you're going to noticed that I it said this was a petabyte uh scale migration well they're suggesting that you use multiple multiple snowballs to get to pedabytes so uh you don't you don't transport pedabytes in one snowball it's going to take multiple snowballs to do that all right we're taking a look here at snowball Edge this is similar to snow go snow cone but with more local processing Edge Computing workloads and device configuration options so used to just be snowball and then snowball Edge came along then snow cone came and then they got rid of snowball because it just did do a whole lot the way you can identify this thing it has those orange handles on it so snowball Edge features we have an LCD display so I'm assuming that is something like a Kindle or something embedded there so that we have our uh e in thing to ship can use in a cluster or uh in a clust can use in a cluster in groups of 3 to 16 nodes which is kind of interesting supports the data transfer protocols here so nfsv3 4 4.1 S3 over HTTP or HPS there are five configuration or device configuration options the first is the storage optimized for data transfer the storage optimized uh which has more uh usable storage we have storage optimized with ec2 compatible computes so that you can use compute on the machine so it has compute with it then you have compute optimized where it has has 28 terab of dedicated nvme SSD so again just depends on what your use case is and then we have one where it has gpus which is the equivalent of one available instance type of a P3 so a much more powerful snow cone uh but there you go hey this is Andrew Brown and we are taking a look at the adus snow family so these are storage and compute devices used to physically move data in or out of the cloud when moving data over the Internet or private connections when the connection is too slow difficult or costly let's take a look first at snow cone uh so there it is then we have snow Edge and then we have snowmobile so those are our three sizes there used to be one called um just snowball but uh they retired that and so now we just have snowball Edge and we'll talk about each of these individually in their specs but let's just take a quick look at uh you know the shape they come in so snow cone comes in two sizes we have an 8 terabyte of storage which has um hard disk drives and then we have a 14 terabyte of storage with solid state drives then you have snowball Edge generally comes in two types we have the storage optimized and the compute optimized then you have snowmobile which is for transporting 100 pedabytes of storage as the picture implies it's that big car container in the back there's basically a data center of storage in there and you load up your data and they drive it to AWS this data is delivered to Amazon S3 so there you go we'll get into each of these Services now okay edus snow cone is a portable rugged and secure device for Edge Computing and data transfer snow cone can send data to adabs in two ways physically shipping the device back to adabs or utilizing adabs data sync which runs on the device's computer snow cone comes in two variants we have the 8 tbte version that uses hard disk drives and the solid state or the snow cone SSD which is utilizing solid state drives for 14 terabytes devices can run Edge Computing workloads that use ec2 instances when they say ec2 instances they mean that it's literally on the device um small and light enough to carry in a backpack use it with a battery based operation a light workload at 25% CPU usage the device can run on a battery for up to approximately 6 hours use the WiFi interface to gather sensor data if you're in North America everywhere else good luck offer an interface offers an interface with the network file system NFS supports Windows Linux and Mac OS multiple layers of security encryption capabilities can be used in space constrained environments where snowball Edge devices don't fit can collect iot data using ad iot green grass I've always wanted to try one of these out but they've never been available in Canada the snow family but now apparently they are so maybe one day you'll see me making a video with one uh here is basically just a closeup of it um so you might wonder like how do they ship this thing do they put it in a box they don't have to because it has an e in shipping label basically snowball Edge has the same thing as well uh what's interesting is that it's just an Amazon Kindle so I thought like oh okay they had some integration but then I read it's just an Amazon Kindle there so that's interesting there at the back we have two RJ45 Jacks a USBC and USB CPD us CP PD stands for power delivery that's how you plug it in and charge it um so basically use a suitable power adapter that can supply at least 45 Watts so whatever that is I wish they would I would think that they would send you the cord with it but I didn't look that up so that might be something that is important to note there's like a QR code on here I wonder if you get more information about the device on delivery but there you go that is a snow cone AA snowmobile is a 45 foot long rug rug ruggedized ruggedized ruggedized shipping container pulled by a semitrailer truck can transfer up to 100 pedabytes per snow mobile I'm not sure who's ordering multiple of these CU they're so darn large abis Personnel will help you connect your network to the snowmobile and when data transfer is complete they'll drive it back to adus to import into S3 or glac here it has security features such as GPS tracking Alarm Monitoring 24/7 video surveillance an escort security vehicle will while in transport is also optional there it is um there's not much point in getting to the details of snowmobile because so few people are going to be utilizing it and there's not much to say just understand that it's handling 100 pedabytes worth of data So based on your use case you know you might want to consider this I believe it has compute on board as well but there you go all right let's go compare the snow family so we have a general idea how these things stack up the first is usable hard drive storage so if you are using hard disk drive based on your variant for snow cone you have 8 terabytes for snowball Edge you can have up to 80 terabytes uh for SSD snow cone 14 terabytes and then for snowball Edge you can see that uh the uh the first one there has 1 terab but then you have one with 210 terabytes with nvme and then for the compute you have 28 terabytes snowmobile does not have SD Storage uh it's weird that does not specify it just says na for hard drive storage so I'm not exactly sure what's going on there but we don't need to know that much about snowmobile anyway for usable CP uh vcpu snow cone has two snowball Edge has 40 and then the other other versions of snowball Edge have 104 vcpus I don't know why but I thought snowmobile did compute but I guess it doesn't so it's just a big data storage thing that's moving uh you think they'd be really useful because if it was like connected and had a signal it could then tra like um crunch data as it's being delivered I could have swore that it could do it anyway the device sizes here vary um not much not that important to remember that device weight so the snow cone is 2.1 kilg so four 4.5 lb snowball edges about 50 lb and obviously we don't care how heavy the snowmobile is for storage clustering the compute uh version of snowball Edge can have between 3 to 16 nodes we have 256 bit encryption across the board hippoc compliant for snowball Edge and snow snowmobile if you are trying to get it and there you go okay hey this is Andrew Brown and what I want to do is show you how you could order um a a snow cone which I believe is the smallest unit of ruggable um storage that you could get for me to B I'm not going to order it for real and for the longest time you could not order this in Canada anyway so I was never able to go through the whole process I don't want to order one for real so um I'm just going to do what I normally do which is just jump in here and we'll take a look so we'll make our way over to Thea snow family here and uh what I'm going to do here is go ahead and say order a snow family device and then we just say my job and then you'd say where you want to import or export from so we'll say import and then we have our option to choose what it is that we want to order says due to the high demand of snow cone your order will be sent to you as soon as the device is available that sounds great snow cones do not ship with the power supply or ethernet network cables so you have to confirm that you will bring your own it's telling you what transfer protocol uses NFS um you could tell it to use some compute here so we have the Amazon Linux 2 compute I'm going to select any old bucket here if you don't have one that's fine just watch the video because we're not going to order this for real we could tell it to install the green grass option whether we'll have wireless enabled I'm in North uh North America so I can absolutely do that and then we have uh some device device manage it down there with open Hub then we have our encryption option then we have the shipping address the shipping speed okay and I'll just go ahead and hit next and it wants me to fill it in so I don't really want to fill it in but you get the idea of how this whole process works so we're going to leave it at that and I'll see you in the next one okay ciao hey this is Andrew Brown and we are jumping into RDS which is the relation database service by AWS it is a managed service and I wanted to say fully managed but then we had to distinguish that from Aurora but it is a managed service for multiple open source and proprietary relational databases so um pretty straightforward you have a compute that connects to your RDS instance your RDS instance is going to have databases or database behind it features include support for different kinds of database engines automatic and manual backups multi availability Zone uh for extra availability read replicas performance insights customized DB parameters RDS proxy for a connection Pooler various methods of authentication bluen deployments and definitely there is more stuff with RDS um let's go over just the types of engines that are available for RDS here quickly so the first is MySQL it's the most popular open source SQL database that was purchased and is now owned by Oracle offers replication partitioning features for scalability availability you have Maria DB Oracle bot myell so Maria DB is a fork or copy of myell under a different open source license at some point in time uh continues to maintain High compatibility with myql ensuring a dropin replacement uh capabilities we have postgress so this is the most uh popular open source database library or SQL database among devs it has Rich features over MySQL but at added complexity supports Advanced Data types and functions such as Json XML key value pairs for app development anytime I am showing off a database I'm using postest that's the one I like to use we have Oracle so oracle's proprietary SQL database well used by Enterprise companies you have to buy a license to use it features a complex architecture that supports large scale databases and multitiered applications we have Microsoft uh SQL Server Microsoft's proprietary SQL database you have to buy a license to use it because it's proprietary integrates simly with other Microsoft products and services including Azure cloud services imbs imbs ibms ibms database 2 uh it's ibms proprietary SQL database you have to buy a license to use it known for its high performance and scalability in large enter uh Enterprise environments then you have Aurora so it's uh an engine by uh adabs on RDS it's fully managed database service that's compatible with myell and postgress automatically divides your database volumes into 10 GB segments uh spread across multiple dis enhanced performance and reliability basically it is managing uh clusters uh of your of your database instances okay and we'll talk about that we have a whole section just on Aurora but there you go let's take a look here at encryption you can turn on encrypt at rest for all RDS engines you may not be able to turn encryption on for older versions for some engines uh it will also encrypt the automated backup snapshots and re replicas encryption is handled using the KMS Service uh you can only turn encryption on during creation you can take snapshots and launch new instances with encryption turned on so if you have to uh get encryption you can migrate to it but just understand you'll have to do that with a new instance very straightforward you just checkbox enabled encryption choose your key and there you go um adus also has encryption and Transit by default um so when you're accessing your database view the DNS endpoint and then you're going to have that if you access it through the IP address which you're not supposed to do which is hard to do but I would assume that that would not have and end to end um encryption uh for encryption and Transit there is the Nitro system which can uh which is Hardware by ads where you can offload the encryption to uh the hardware but that is is an advanced use case and will'll never show up on any exam so I didn't bother mentioning it in here but I'm just talking about it out loud but there you go RDS databases can be backed up in a few ways the first is automated backups so you choose a retention period between 0 to 35 days uh Z days would mean that uh automatic backups is turned off so if it's zero then you're not using automatic backups uh you can use point in time recovery so pitr to restore at any 5 minute interval within your retention period storage transactional logs throughout the day automated backups are enabled by default all data is stored inside of best3 you define your backup window storage IO may be suspended during backup there's no additional charge for automated backups I didn't write this anywhere but if you do have backups turned on okay like if you have it greater than zero and you spin up your instance it's going to take longer because it will need to take a snapshot at the time of launch so when I'm just playing around with things uh I will make sure that I set it to zero so that I have fast boot times for RDS or faster boot times for RDS uh for manual snapshots taken manually by the user backups persist even if you delete the original RDS instance um you can copy them so you can copy snapshots across regions you can share them with other A's accounts you can export them uh to S3 there are additional storage charges for manual snapshots okay okay so I'll use snapshots when let's say we're going to do a large database migration or something crazy to the database I'll take a snapshot of it just in case I have to roll back to it um so there's that uh so to back up a database your database has to be in the available state if it's not it's not going to work so there you go let's talk about restoring a backup because it's great to back things up but if you can't restore them what's the point um when you are restoring a back whether it's automated or manual you'll end up creating a new RDS instance okay so factor that in uh about uh how restoring backups work restoring uh from a manual snapshot is pretty straightforward uh you're going to provide the snapshot identifier and the instance identifier to do so for automated backups um it's very similar except you are also providing the restore time because automatic backups use point in time recovery okay okay and so it might back up every day but the point is it backs up at an interval I think it's 10 minutes 15 minutes I always forget the point is you can choose a very specific interval uh during that time of day so it's it's really useful in that sense restoring backs up is not a fast process because it creates a new database so consider that for recovery time objectives and the reason I want to call this out is because I've had times where I've had to restore database in production and then we're waiting around like 20 minutes do you know what I mean to the whole process to do it and so you just have to understand that it's not as fast as you think so if there's a case where um that happens that's just what's going to happen Okay let's take a look here at DB subnet groups this is a collection of subnets us see private subnets that you create in a VPC and that you then designate for your database instance I hate these things because they feel unnecessary but they're an abstraction from choosing which um subnets you need to use so each uh DB subnet group should have subnets in at least two azs in the given region uh RDS will choose a subnet from your subnet group to deploy your RDS instance subnets in the DB subnet group are either public or private for a DB instance to be publicly accessible all subnets in the DB subnet group must be public the sub is pretty straightforward when you're using using the wizard um like the Management console you just choose the subnets on creation C is a little bit more work but uh just understand that it's just a grouping of subnets and then it just chooses a subnet to launch your instances in okay multiaz is when you have one or two standby RDS clusters or instances in another availability Zone which fail over in the case an availability Zone becomes unavailable we we have two flavors of multiaz we have instance and cluster instance is for RDS instances because that's what RDS instance is it's just an instance and uh cluster is for Aurora because Aurora manages a cluster of um instances for you so multiaz will create exact copies of your database and data in another a and it will automatically sync the changes uh over okay uh in the case that the a goes down uh it'll automatically promote your U standby to primary okay to configure multi a it's very straightforward you're going to add that hyphen hyphen multiaz flag you want to make sure that you say apply immediately because if you don't you're going to have to wait till the next maintenance window so there you go read replicas allow you to run multiple readon copies of your database this is going to improve Rec contention which in turns improves performance and latency if you've never heard the term Rec contention before it's when multiple processes or instances are competing for access to the same index or data block at the same time you'll hear this term a lot when we're talking about issues with um with databases you must have automatic backups enabled to use read replicas uh the type of replication it uses is asynchronous replication which occurs between your primary database and your replicas you can have up to five replicas of a database for SQL Mario DB post SQL you can have up to 15 re replicas for Aurora each re replica will have its own DNS endpoint rcas will by default use the same storage type as the source database um you can change your storage type of a re replica if you need to do so you can have multiple uh a replicas replicas in other regions or even replicas of other read replicas so here is an example of multiaz replicas and we have cross read replicas replicas can be promoted to their own database but this breaks replication no automatic fail failover if the primary copy fails you must manually update URLs to a point at copy let's do some comparison here on how it varies so for my SCH Marb we're using logical replication for Oracle at post Cresent SQL Server it's physical all the way through for uh writing you can write to read replicas for myql and Mario DB uh it can be enabled for Oracle that is not the case neither with postgress or SQL Server you have backups for MySQL Mario DB automated and manual uh for Oracle you you have everything as well uh for postgress you have it h but it's only manual you don't have any for SQL server for parallel replication Mario DB MySQL Oracle can do it postos can't SQL Server can do it okay to create a re replica it's very straightforward you just have the create DB instance read replica and you create your read replica so there you go all right let's compare multiaz versus re replicas so we clearly understand the difference here so um multiaz has synchronous replication so it's highly durable re replicas are asynchronous replication so they're highly scalable um for multi Z only the database engine is uh for the primary instance is active for re replicas all re replicas are accessible and can be used for reading and scaling uh for multiaz automated backups are taken from standby for re replicas no backups configured by default for multiaz always spans two azs within a single region for re replicas can be within an a cross a or AC cross region for multiaz the database engine version upgrade happens on primary for re replica database engine versions upgrade independent from The Source instance for multiaz automatic failover to standby when a problem is detected for read replicas can manually be promoted to a standby database instance so there you go okay let's take a look here at DB instances so this is an isolated database environment running in the cloud DB instance can contain one or multiple user created databases so just understand you can make multiple databases but usually it's just one per instance uh some database engines require the that a database name be specified you can have up to 40 uh RDS DB instances per 's account uh 10 for each of the SQL Server editions uh 10 for the Oracle under license included model 40 for db2 uh under the bring your own license Model 44 myol Mario DB post uh postgress 44 Oracle under the bring your own license model uh so yeah it's a what you want to utilize out of that number set okay each database has user Define database identifier which forms part of the DNS host name okay so the DNS endpoint is how You' establish a connection notice that um it has the database identifier which is says my Rd instance it's the name of your database uh instance not your database your database instance and then there's also this adus Define RDS unique identifier which you know ever have to worry about but you just copy and paste it from the the URL uh the database identifier identifies the RDS instance is not the database name okay if this was a connection string URL the database name would be right here on the end all right uh let's just talk about uh the classes in storage before we move on here so the instance class just like ec2 instance class is determined uh determines the available compute and memory of a database instance so the DB class types always start with DB period okay so we have general purpose ones that look like this memory optimized ones which look like this and we have um the optimized X family the optimized Zed family and the optimized R family we have burst double performance this is what you're mostly going to use if you are just learning uh which we have T2 and by the way uh while I was recording this eight of us actually retired this one so T2 is no longer available and I think in the videos I probably try to use t3s anyway so I just want you to know like we tried to use the t2 because it was a free tier but I guess the T3 is now the free tier or probably already was um so just understand that that is a change that literally just happened uh we have optimized reads so um there's those as well let's take a look at instant storages for database so DB instances use EBS volumes for database and long storage you can utilize general purpose ssds provisioned iops ssds magnetic uh which is not recommended but it's something you can do in certain cases uh you have a maximum storage uh that most DB instance classes will have up to 64 terabytes which makes sense because it's the same thing for um EBS and it's based off the um the block size which is something like 16 kilobits or something 64 kilobits I forget uh this will greatly vary based on the engine type and instance type and size you can increase the storage size of an EBS uh we talked about this in another section but there's this thing called optimize reads and writes and those who use nvme uh for instead of using EBS um so you get much better performance there but for the most part EBS is what RDS is using um a reason that you might not want to use uh uh RDS is just because the storage is not fast enough and so some folks will run bare metal or just run a different configuration on ec2 instances um but for the most part it's pretty darn good RDS will not let you reduce the size of your storage only to uh only increase it to decrease storage size you create a new DB instance that has less provision storage space so there you go RDS performance insights helps you easily identify bottlenecks and performance issues by default performance insights is turned on providing one week of performance data uh for additional cost you can change the retention period to two years this thing is super awesome um originally it was not for all services it was only for Aurora I think and then it was for RDS but it was for only a very specific size of RDS but now it's for everything so it is great um so yeah pretty straightforward RDS custom automates database Administration tasks and operations allows customers to directly manage aspects of RDS instead of adabs for companies that require thirdparty apps or customization for their databases so what can you do with this you can install thirdparty apps install custom patches create your own automation how does it work you create RDS Uh custom DB instances you connect a RDS custom DB instance endpoint you can directly access the host to make any changes it works with SQL server and Oracle databases um here's a diagram I stole it from the adus uh blog because I just couldn't be bothered to make one here today um so anyway the way it works is you will have a connection so here it says connect to the RDS custom DB instance endpoint so it has its own endpoint um and then below you have your SQL servers you can actually connect to the underlying uh servers the compute uh and do whatever you want with them but you're going to lose um uh lose an aspect of management from ads and so ads had this whole section about like responsibility model being changed because now you're taking on more responsibility I didn't bother putting that in here because it's never going to show up in the exam but it should be obvious if you're doing more of the direct maintenance then you are responsible for it um this is great um you know because then if you you want to have a level of um responsibility that adus manages it as opposed to running on your own uc2 instance then this works uh good for the most critical parts of it but gives you a level of customization um but yeah there you go hey this is Andre Brown and in this video we're going to take a look at RDS uh so RDS is a service that allows you to spin up databases we can go over here and take a look at all the kinds so office so there's Aurora we have my school Maria DB postgress Oracle markof SQL server and IMDb 2 a lot of options here um I've done this so many times that I do not feel like writing this uh from scratch um and I feel that we just copy paste um some cloudformation template I could provide it to you and we can work from there so what I'm going to do is just open up uh my um my ad examples repo that we just keep using here and I'm going to open this up in get pod you use whatever you want to use as per usual so we'll open this environment up and I'm going to go find a repo where I might have uh RDS because this is something that I've again done so many times so I'm going to go over to my teacher seat this is private stuff you're not going to be able to access this but I'm going to copy it over uh over to where we need it to be just going to go into here into teacher seat and go to uh here and just type in like database there we go web server database and the nice thing about uh Cloud information it kind of lasts forever but in here I have a configuration file so I'm going to copy the whole thing over and I'm going to just pair it down a little bit so that it is uh better for us to use here and so just say basic that's a folder right folder basic we'll make a new file here call this template. yl okay we'll just paste the contents in here um and so there's a lot going on in here let's just walk through this quickly so web server RDS instance of Secrets manager that sounds really good I actually kind of like that idea this one kind of relies on serving in a web server stack uh it has the RDS secret arm we have a username backup retention instance class which just changes the T3 micro um engine this one's 11 it's been a while so let's change this to whatever the latest is if we go over to postest 16 um okay 16 point no we'll do 15 because there's more compatibility right now that so I'm going to do 15.4 just because uh earlier on we installed or in another video in Aurora we installed uh this and we could install 15 from Amazon 2023 so we'll make our lives Easier by doing this here we can set the public accessibility we can have um deletion protection I want this to be turned off by default so I'm just going to swap that out here okay and so we'll need to create ourselves a security group um and so what this will do is it will allow access from the uh other security groups so this one's importing from a cross stack template I don't want to do that here today I'm just going to take um I'm going to modify this so here I just want to reference a web server SG group ID we'll just call this here like that okay we'll take that out there we'll go all the way to top here and we'll swap this over as such so now this is just expecting a um security group so that looks fine um we also need a VPC ID go to the top here we'll just say VPC ID and we'll go here and Swap this out for ref bpc ID and we'll need a subnet group this is just a collection of subnets that we want to utilize this one again is trying to import cross stack which is a little bit annoying but uh I mean this is just how the code was and so we have the group name and description I wonder if we could leave those blank let's go take a look at that option it's required yes the name is not required so I'm going to not require the name and we'll just say uh you know my uh DB subnet group and in here it wants to supply subnet IDs um I mean it would make sense to split the values that is good but uh in here I actually want to import them I don't want to import the values I just just want to bring them in so I'm going to go ahead and just remove this part of it and we'll just say uh hold on here undo that for a second so we'll say ref and take this part out and then this is going to be subnet IDs so we're going to have to supply subnet IDs I remember I was doing this uh for something we had a subnet IDs thing here maybe it's for elb we'll look at this template here no maybe it's for auto scaling groups there we go so this is an example of one we'll probably have to swap these ones out or actually what I'll do is I'll just um take out the defaults here whoops there we go I'll just take out the defaults and we'll just have to set those manually ourselves okay so just keeping going keeping on with this template here so I think that should be good the RDS instance um we can have our deletion policy to be what we want uh I don't really want to have this on Snapshot because I don't want to back up anything because this is just an example one so I'm going to go in here and take a look and in here we have deletion policy I'm going to have this this to be delete and then we have the update replace policy I want this to be delete as well and so we have some properties here the username uh the password subnet group how much we want to allocate for storage 20 is good whether we want to update on major and minor versions that's good true passing the RDS Port enable IM database authenication we'll turn that off for now it's great functionality but we're not doing that right now the backup retention period which I think only matters no no that that's fine we we'll have that on there that's like like seven days whatever we want to choose there inance class database name engine and then we have deletion protection down here thought we just filled with those did we not yeah up here okay all right that's fine yeah um cuz you can have these on the outside of any kind kind of um it say here like whether like if you if you're allowed to delete like if you were to delete you take a snapshot there that makes sense engine version whether it's public accessible the security group that looks fine we have a bunch of outputs here database name RDS Port username stack name uh I don't think we need to export the stack name that's kind of useless no we'll leave it there it's fine the uh so we'll make a deploy.rb but we're going to do this a little bit different because we're actually going to pass I don't know why I haven't been doing this but uh we'll we'll go ahead and pass parameters so I'm just trying to create a file there sometimes that messes up I have to reload my window that's a g pod specific thing okay we'll try to make this fall again here there we go I'm going to paste this in so deploy RDS RDS basic and so uh we want to pass some parameters and I feel like I probably have this here I have this config dotl file uh because I actually have this special script to load them in but I'm trying to keep it really simple here so what I'm looking for is passing the parameter overrides which is this here uh so let's go take a look at what that looks like because we haven't really used it that lot that many times it was cloudformation deploy um here and and we have parameter overrides here so a list of parameters structures that specify it so it's going to be parameter key it's going to follow this shape here okay and so I created my own Library um in order to make this a lot easier but I'm not I'm just going to keep it real simple so that you folks understand how this works so IDE is we have parameter key and then the value I don't think we I think it's just a space in between yeah it's just a space and so this is where we can set some of our uh or a lot of our value um so let's bring our template over here and just split across yeah this one and so we need our VPC ID we're going to need our web server group ID like it's actually SEC like sorry web server Security Group ID so just copy this here we're going to need RDS RDS secret Arn we need our username uh backup retention is zero that's totally fine instance class we leave that on the default um leave the engine version alone Public Access is fine delusion production has turned off so that's fine why is this one complaining want a string I will leave that alone if Al it complains 5 for 32 that's default so the other one is just the subnets is what we'll have to set here so what we'll need to do is go over to Secrets manager um and we're going to need to set up a secret just understand that it's oh wow that really got lower it used to be like a dollar per secret now it's 40 cents that's really nice and so we want to set up uh credentials for RDS basically so I would like to do this programmatically I just don't know if that's going to be a lot of work um I'm going to go ahead here and make a new file in here call this readme.md and we'll go look up the CLI AB CLI Secrets manager and I want to create a secret go down examples here so we have some simple stuff here it's not that complicated um but I specifically want to use this create secret Secrets manager we'll go over here and uh take a look here so we have because there's different kinds right so like if it's for a database the secrets also include connection information to access a database or service which Secrets manager doesn't encrypt all right for the database credentials you want to rotate for secret manager to be able to rotate you must make sure the Json uh you store in a secret string matches the Json structure of the data secret okay so we're looking for the format of RDS or sorry postgress down here so we have to follow this structure I wonder if we could Supply the secret string as um a file because that would make our lives a lot easier here back here back again can I do file oh we can okay that's nice so we'll go here and we'll just say um I'm I'm going to write secret Json but I'm hoping that it doesn't complain uh the thing is that sometimes there's Protections in place about storing passwords and and stuff in your repo but uh none of this is sensitive so I'm not really worried about it as we're going to tear the stuff down um but I want to go grab this where did it go where did I paste it it read me here there it is so we're going to copy this or cut this here paste this in here and um well how the heck would it know if the host doesn't exist yet like how would we create that beforehand confusing yeah this is going to be 5432 54321 This is my password I'm just say Andrew Brown here H how would you fill in host when it's not there yet that's so confusing but like if we were to create this here I guess there are that's there as well but you know do we actually have to have I'm just going to try this Andre BR my password next you must select a database okay but then how would you if you don't know the you see my problem here like you'd have to have the database first but we're creating we haven't created the database yet okay um let's go back over to here for a second I mean obviously I've done it before because um it's passing in that uh RDS secret Arn so maybe we're just bringing it in we're not actually rotating it out yeah so in here it's just grabbing the secret string called password so I'm just thinking here resolve Secrets manager RDS secret R secret string password so I think that what we'll do say password like this and we'll make it really simple I'm just going to get rid of this we don't need this I would just say my password here because this is being silly but I guess I guess if we wanted to actually rotate it we'd have to do a bit more work but I just want to have a uh a simpler example here I'm having a really hard time because this is wrapping so we'll go ahead and just delete from here uh yep that's good okay great so that's what we're going to do so we'll just say my RDS password this is complaining so I'm just going to reload here my RDS password and we'll go ahead and create the secret H enter here and it doesn't like something it's not saying what though maybe something is uh mucked up here no this looks fine to me so I'll just check it us it us SS get caller identity that's fine we'll try this again again says it already exist so maybe it actually did work and uh it just threw back some stuff here so we go to Secrets here we have my RDS password and um we can take a look at the secret value so there it is it's being said in there normally when you name these you'd actually name them with like youd want to name them with like forward slash sorry I just sneezed again um and and kind of scop them with that but I'm not worried about that here today so that's fine so we now have our uh secret and I'm going to go back over to here I'm thinking that it probably wants the AR so we'll go here and grab the full AR and we'll go back to our deploy script and so yeah it wants the AR so we'll paste that in there that is long uh we need a web server group ID so I'm going to just use the default one for adab us so we go over to uh ec2 here or sorry uh yeah ec2 so we're going to launch this into the default VPC to make her live super easy here if I can find security groups and we will go and grab default I thought I deleted these in another video these are just junk ones lying around okay get rid of the security group then delete security group just give me a second here all right there we go just clean that up and I'm going to go ahead and grab this Security Group ID going to make our way back over to our instance I'm going to go ahead or get pod and I'm going to paste that in there I need to get the VPC ID which is our default VPC so we'll go over to PPC here and I'm just going to go ahead grab that here we go um username is going to be Andrew Brown so we'll just have that there and then we just need our subnets so we'll go back over into our uh subnets here um I'm not sure where these ones came from it must have been when we um another video just give me a second to get rid of the junk ones there we go back to our default so here we have B so I'm going to grab this one here paste that in and I might just grab all three to make my life a lot easier here because then I don't have to worry about if I got it wrong by not having three so we'll go back over to here and is it a space or comma I remember doing this before and it complained we'll paste this in here we'll have to go take a look though grab this last one here and I'm going to just go take a look at what that's supposed to be so if I go over to um I think asgs is where we pulled this from oh it's a comma okay great um so what I'll do here is put a comma so now our life is uh pretty straightforward just going to wrap these in double quotations that way I'm not going to have any issues with um the terminal at least I think it will let us wrap it like this okay and so I'm just going to copy this and paste it here and you can see we have a lot of stuff going on here for parameter overrides so I don't particularly like that I have to do this but there's no easy way around it so other than other than using our uh my my specific CLA formation plugin uh so yeah we have that set up um so before we can do this the only thing that we we need to have is the web server Security Group ID so we're going to have to oh we used the default one so that's totally fine so I guess we're ready to launch this and hopefully it works so let's go ahead and chamod we'll first we'll CD into the correct directory I'm having some issues here with G pod so I'm going to just give this another refresh here all right oh yeah I was just reloading I kind of forgot what I was doing uh but what we'll do is go ahead and try to run the script so before we do that we need to traod it uh well we have to CD into the directory first otherwise we're not going to have much luck here we'll chamod and say deploy and we'll go ahead and deploy this invalid parameter type properties so it's not a surpris that there's something wrong with our script but I mean we're modifying something that already existed so it seems like it should be should be less of an issue property properties is not allowed so give me a second just stare at this for a bit only thing is missing here is resources because we have these resources so right here have resources it's weird because I copied and pasted that script I must have deleted it out by accident um so anyway we'll go ahead and try that again RDS database name must have a value um did we comment that out all right okay we'll set one then RDS well database name where is that oh it's down here okay so just set a default on this call this one my database go ahead and try that again web server stack that is an old reference we don't have that anymore so we'll go ahead and take a look here web server stack okay that was the last reference there we'll try this again uh it does not like something in here I don't think we're doing any more Imports I'm me just going to double check here import no I have a feeling it has something to do with this just give me a second okay yeah it's probably because I call this FN ref so I'm just going to look this up um uh cloud formation split function maybe just ref yeah it's just ref okay so we'll go back over to our code here and this is just going to be ref and we'll try this again subnet IDs it's because we call these subnets here instead of subnet ID so we just go down here and say subnet uh I'm going to leave it as subnets actually and then I'm just going to double check our deploy script see if it's called subnets here as well it is good we'll try this again object requires two parameters a string Eliminator and a string to split the function or return a value okay so um there's another way we can write this so we could do this instead and so then we have this as the first part of the array and then this one will be the second so I can just say ref subnets here let's see if it likes that instead it is complaining though expected an array yeah that's what the hyphens are for or like what does it think that I'm doing here let's try this again I'm just try to run it anyway I'm just going to ignore it because hyphens that's how you make make an array in in there and so there you go it actually did create a change set so even though this is showing me an error it's totally a liar we'll go over to cloud formation and we'll go review this we'll click into here looks good those are three things that we want we'll go ahead and execute that change set and we'll see if this uh deploys here so we'll wait and see if we run into an error okay all right looks like we ran into an issue we'll go down below here failed to create the DB subnet group um some input subnets are invalid so it does not like how we inputed our subnets okay we can see what we're passing here by going to parameters and so this is our value for this okay we can go look up subnet IDs here here so we'll go ahead and look up DB subnet group we have here the ec2 subnet IDs for the DB subnet group is an array of strings and so here we bring in that reference and we split it so I'm not exactly sure why it's complaining let me stare at this for two seconds I'm not exactly sure what the issue is here but uh we can just circumvent that by just hardcoding it in you really don't want to hard code values in but I'm not exactly sure what the issue is and I just want this to to work so we'll go ahead and just do this here uh oh you know what it is I know what it is there we go that's what we need on that so I'll just undo this here whoops make sure my subnets are still there and uh we'll go ahead and delete this wait for that to delete there we go we're going to make sure we save these files and we'll go ahead and deploy this again this has an issue now requires two parameters a string Eliminator in a string to be split or a function that returns a string to be split well that's what this one is doing here let me just double check make sure that this is still being set here uh yeah is subnets good we go to the top here subnets that's good as well it's either that or maybe this is being interpolated as an array already and so we don't need to split it so what I'm going to do just going to try to try something here and we'll try this again there we go and so yeah it's already returning an array um there so that was my mistake we'll go into here we'll go ahead and execute that change set and hopefully it'll work this time so we'll just hold on here all right so it looks like it is finished creating the RDS instance and we haven't ran into any issues so that's really good um in order to uh connect to this instance we're going to need to spin up an ec2 instance um I mean it is public available so technically we don't have to but uh I'm just going to stick with the process that we've been doing for other videos and I'm just going to launch a new instance so let just say my RDS instance here and we'll choose a T3 micro I don't want this with a key pair uh we'll just use the default Security Group because that is where we actually we use this one so I'm going to just place this in this same um one as the postest server we'll drop this down and we're going to choose ec2 SSM roll we're going to go ahead and launch this instance and we're just going to wait for this instance to get ready okay all right this instance is running it doesn't pass the status checks but I think that is sufficient we'll go ahead and connect to it via sessions manager uh and over here I mean this is in the Aurora section because I did aora first um and we have this command here that we can utilize to install all this on Amazon L 2023 so we have to switch over to the ec2 user we're installing that there and if we followed all of our naming conventions here it should be the same thing here this doesn't work it's not a big deal we'll just debug it oh you know what I have to switch over uh this part of it because this is going to obviously be different this address here so what I'm going to do is go over to our instance our RDS instance and I'm going to grab this end point I just have to update this here I'll go ahead and copy this paste it in we'll say psql and that's assuming we set all these things to be that um actually the username is angre brown and it's not uh postgress try this again so yeah let's just look at our configuration because clearly I have not followed um my previous one here so the database is my database so we have that correct go back to here the password is pass via Secrets manager let's go take a look at what we set it for I mean we would know if we just go look at our readme script here down here my password so that is correct 5432 my database um let me think about this so it looks like our um RDS instance allows anything from a security group that is the same as its own I I'm not sure as to why this is not working because we did launch the instance in the same Security Group but what I'm going to do is just up update security groups we really shouldn't have to do this but I'm going to go ahead and try this I'm going to add the default one here and say add Security Group and say save we'll go ahead and try this connection again there we go so maybe it's the fact that it's in the existing Security Group and it's not allowing the same from the same one for whatever reason that's fine uh so we'll go back uh to our Aurora one we'll just make sure that this works there we go so we're able to create our table there we'll terminate this uh and there's a lot to learn about um RDS but um for this basic example it's very easy we do have Public Access turned on so we could have actually used a um like a a psql uh database manager client to establish a remote connection and we wouldn't have to necessarily launch an ec2 instance but for what we're doing here this is totally fine we're going to go ahead back over to cloud formation I'm going to go ahead and tear down this stack and I'm just going to commit my code here and we'll see in the next one okay ciao RDS proxy creates a connection Pooler so that shortlived databus Lambda functions connecting to RDS does not quickly exhaust all connections so imagine you have a serverless application and you want to use a relational database as opposed to a no SC database base the challenge here is is that every single Lambda that spins up is going to create a new connection it's not going to reuse a connection because when you think of a traditional monolithic application what it's going to do is it's going to establish connection and that one app is going to manage uh certain amount of connections like four five it has its kind of own little internal connection Pooler um and it'll utilize it efficiently right and so you Lambda is just constantly spinning up new connections and that's going to overwhelm your database if you spin up an RDS instance like the small size it's like 20 or something really small um so the idea is that RDS proxy creates a a pool of connections and we'll reuse those connections for uh new lambdas that are establishing connection if you ever heard of um PG bouncer basically this is like PG bouncer uh in the cloud but obviously it's for more than just postgress this thing is a pain to set up um every time I've wanted to use it it's always been uh a nightmare to configure but anyway it is a really good offering when you want to work with the relational database but have uh use Lambda serverless compute could it be utilized without Lambda I just couldn't find an example where you could connect with it I believe it's specifically for Lambda but there you go RDS optimize reads and writes allow database operations to maximize performance efficiency and throughput um so it says rights are 0.5 times faster and uh reads are two times faster that's what I could find out from the blogs if they're faster now I'm I'm not really certain uh RDS optimize reads and writes utilize nvme based SSD block storage instead of the ads elastic block storage for temporary tables for greater performance queries that use temporary tables would be things like sorts hash aggregations High load joints Common Table Expressions RDS optimiz optimized reads and writes are available for specific combination of instance classes engine versions because the instance class determines what Hardware is being utilized and so you're looking for ones that are mvme backed if you remember our EBS section we talk about mvme and why they are faster uh because of the way they um they use the their connection and things like that um but there are specific types so like uh myquel 8.0 dbr 5B and uh you know there's some that only so like when I read the the postest blog it was like they only had it for reads they didn't have it for wres but they for they had for reads and wres and they have different specific versions so just understand it's going to be um variable based on the underlying hardware and then both of them for mqu and postrest require a lot of configuration so it's not as simple to take advantage of it just because you're running the hardware um and right now I only see from bicycle postgress but might be for other ones but yeah that's optimized reads and writes okay so IM authentication allows you to authenticate with an IM authentication token to RDS instance databases instead of using a password it works with myell Marb and postgress an authentication token is a unique string of characters that Amazon RDS generates on the request using sigv4 each token has a lifetime of 15 minutes you can also still use standard database authentication alongside IM authentication users can use am authentication instead of having to use password ec2 instances can use I am authentication instead of having to use a password so there's two cases um where it's either users or ec2 instances so it sounds like I said the same thing but I actually said two different things there enabling IM am authetication RDS instan is very straightforward notice we have hyphen hyphen enable IM database authentication okay you do have to create a policy and attach it to a user or role to allow the ability to authenticate as specific users so notice here that um I'm going my pen tool out here we're saying you're allowed to I'm not sure my Pen's not come on pen we have a connection here and so we have a specific database uh sorry specific database user and um a specific database and then the the database username you do have to create the user as well so here we have Bo right and so Bo is created here as well and we're granting access to that specific database all right so hopefully that makes sense uh you generate off tokens to be used in place of password so this is basically the main way it works so you say okay here is my um uh DNS uh endpoint and then we go over to uh PG password sorry that's just the environment variable but we say generate the DB off token we pass in our host name endpoint and that's going to provide us back the password and then we use our password how how we normally would but it's actually a token that was generated out by RDS so there you go so kerberus is a network Authentication Protocol which is also directly integrated with Microsoft active directory and I want to point out I had to look up how to pronounce it because I always say it wrong it is I believe a Greek word um because it's that threeheaded um dog from uh Greek mythology but anyway C kerberus RDS supports for kerberus and active directory provides the benefits of single sign on and centralized authentication of database users it works with act ads directory service for Microsoft active directory your own on premis active directory it can be used with a Microsoft SQL Server postgress SP SQL Oracle Microsoft SQL server and postrest SQL database instances support one or twoway Forest I think that's spelled right Forest trust relationships Oracle DB instances support one and twoway external and uh twoway external Forest trust relationships um this thing is not easy to show it's very Windows focused so um just understand that it's here and it's a Windows thing for authentication okay iTab Secrets managers can manage RDS instance Master user password allowing it to be rotated out it does not work with Microsoft SQL Server Amazon RDS that has blue green deployments RDS custom Oracle data guard switch over RDS for Oracle with cdb um the secret will be rotated every 7 Days by default web apps need to be configured to access the password programmatically from Secrets manager if you delete a database instance the secret is also deleted uh let Secrets manager manage Master user password for RDS is our example here so you'll see it says hyen hyen manage Master user password RS generates the master user password and then manages throughout its life cycle in the secrets manager um so you don't have to worry about it if you need to use use it you're basically going to pull it down programmatically um and then place it into your uh your web server is the way you would work with it um Secrets manager usually cost something used to be like a dollar per secret I think they might have brought it down to 50 cents but when we're talking about uh the features when it's dealing with a database password might be more but just understand it's not free to utilize this that's why not everybody is using this just because it is awesome okay okay Master user account in RDS is the initial database account that that's created when you provision a new database instance the account is granted full administrative privileges on the database for creating tables creating schemas performing SQL operations so it's uh Master user account is very powerful Master user account username password is set at the time of creation of the RDS instance so notice there that we're setting the username and the user uh and the user password there uh the username will be visible in console so if you forget it you can definitely look that up of course the password you will not see uh it's recommended not to directly use the master user account for daily use instead you should create database users with the least amount of permission to perform specific duties you can reset the master user account password uh if you forget it uh password format and restrictions vary uh per database engine but generally passwords need to be at least eight characters long so there you go parameter groups act as a container for engine configuration values that are apply to one or more DB instances parameter groups let you change database parameters that specify how the database is configured so you can modify the parameter groups by changing the database parameters so here's an example of US changing Max connections and Max allowed packets each database engine will have completely different database parameters um postgress database examples would be things like workm share buffers maintenance work mam effective cache sizes checkpoint completion Target wall buffer so you do have a level of customization that you can uh tweak here for parameters for your databases but just understand that uh um if you need more configuration they going to need RDS custom or run your own um database on an EC two instance but there you go public access option changes if the DNS endpoint will resolve to the private IP address from the traffic from outside the VPC notice there we have the hyphen hyen publicly accessible so basically if you do not have this turned on you cannot access your database directly establish connection unless that the connection is coming from within the VPC um just be aware that there are security groups so just because you have Public Access does not necessarily mean you'll establish a connection if the port is not open um Public Access feature is very useful when you're confident with the password authentication security groups and you just want the convenience to connect to your RDS instance um so you'll have to make that determination yourself which case works for you okay let's talk about how we would establish a connection to RDS and we'll focus first on public connection so connections that are not within the VPC and we have a few options here the first is we can use something like a database management tool so table plus uh d d Beaver data grip navat these are um apps you install on your uh desktop computer it allows you to uh easily interface U if you provide your uh credentials like the DNS endpoint the username password the port and everything like that so that is one option you could Tech technically use cloud shell so Cloud shell does not exist in the same bpc um but it can it can uh if you don't want to uh use a local client you could use cloud shell and uh you'd have to install the DB client or a a database driver in order to uh do that but that is a possible option as well um the other way is that you have a database client or you're using a database driver with a programming Lang locally on your machine so you don't necessarily have to do that through Cloud shell you just do it from your local machine because if Public Access is open you can connect to that endpoint um the last one here is if you have a web application and this is generally the way that you are going to access things the web server is going to have a database driver installed so like if you're using Java it's jdbc if you're using Ruby uh it's it's like if it's postgress it's just the PG gem or active record if you're using the rails framework work and so that's the way that connection is going to be established um another another few things I want to take a look here is connection string URS because you just might not be aware of those um this is a single string containing all the parameters to connect to a database it's just a a convenient way uh to pass it all as opposed to having to specify every single thing as a flag and most databases will support this but there is variation for connection string because it's really dependent on the database driver or the database command uh command line how it's going to interpret it so this is kind of what MySQL looks like marbs postgress oracles uh SQL Server okay um and so the way you'd use it so for instance if you had the postrest client installed on your computer or on wherever you'd assemble that uh that link and create it so I'm just going to get my pen tool out to make this really clear so here we have the protocol so this is postresql there's a few variants for postest here this is the usern name the password then we have the actual um DB identifier the region um uh you know the service the port and then the database okay so that's just for postgress and then obviously you should know your Port so um you know this one we using 5432 you can change your ports on your databases to be whatever you want but you'll notice that we are using the port here so there you go all right let's take a look at how we would establish private connections this is for RDS instances that have Public Access turned off so one way would be Cloud9 so you You' launch Cloud9 it has to be launched at a public subnet but the point is that if it's in the same VPC Cloud9 has a terminal that you can uh use a uh database client or database driver I'm sure my school and post are already preinstalled on most configurations of of Cloud9 that you launch up um so that is one way you can establish a connection you can connect through a Bastion or a jumpbox again you're going to put this somewhere uh apparently I did not make it part of the diagram but the point is is that you'd have a Bastion or jumpbox launched in your public subnet and you tunnel through that so that is an option there you can launch an ec2 instance and just connect via SSH or sessions manager and just establish a connection that's very similar to a jump box but the point is that you're using sessions manager to connect it that's not visualized here but you get the idea you can use the adabs client VPN to connect your machine to your VPC and then you can just use table plus like a a a database ID whatever tool you like or the database or database client or um a database driver for on premise connections you can uh you can utilize it with Direct Connect uh because that's going to join your on premise Network to your VPC you cannot utilize it with Cloud shell because it's not part of uh your VPC now now it's in some VPC it's probably a abas manage one but it's not in your customer manage VPC so you're not going to be able to utilize that um and I didn't write it in here but you can see there's a web server and so if a web server had to access the database it is launched in the same VPC so it could access it there for you okay let's take a look here at security grps for RDS I just felt like I didn't really cover this anywhere so I just made sure to tell you about this because it's very important and that's the fact that um RDS instances have their own security groups and if you want to establish a connection from an ec2 instance or anything you have to open up the port on your Security Group otherwise it's not going to be able to access it okay um and you'll find that if you're ever trying to access an RDS instance uh via a uh like establish a connection to the database and it's hanging and nothing's happening most likely it's your group most likely okay so I just wanted to make extra emphasis on that okay RDS blue green deployment copies a production database environment in a separate synchronized staging environment so you can test database changes in a safe environment without affecting the production environment stay current with database patches and system updates Implement and test newer database features uh and so here's an example and the idea here is you can actually change the engine version so and the instance size so it doesn't have or class so it doesn't have to be exactly the same I didn't really show deeper instructions here because I found out that every single engine requires completely different prerequisites before you do replication so understand you'll have to do a little bit more than just run this command in order to utilize blue green deployments okay amum already s extended support allows you to run your database on a major engine version past the RDS end of standard support date for additional cost it gives you more time to upgrade to a supported major version RDS will supply patches for critical and high cve so that's common vulnerability exploits as defined by the nvd um after this oh sorry it's available for up to three years past the RDS end of standard support date for a major version after this point adus will automatically upgrade your RDS engine version so this is just a way so that you can have uh the ability to utilize a specific engine for longer uh even if ad us doesn't want to support anymore but they're going to charge you for it which kind of makes sense but there you go hey this is Andrew Brown and we are taking a look at Dynamo DB before we talk about Dynamo DB we have to define a couple things up front the first is what is nosql well nosql is a database that is neither relational and does not use SQL to query the data for results relational database is where you have multiple tables and they have relationship between tables that is not a thing with nosql uh you can uh uh kind of simulate it but it's not truly relational um and so the next thing I want to talk about is key value store so this is a form of data storage which has a key which references a value and nothing more and then you have a document store which looks very similar to key value store with the exception that knows that it can store a nested data structure something that would look like a python dictionary or Json object or things like that so let's talk about Dynamo DB so Dynamo DB is a nosql key value and document database for internet scale applications it features um its features are it's fully managed it's multi region it's multimaster it it's a durable database it has builtin security has backup from store it has inmemory caching there's definitely more features on that as you'll see the Dynamo DB section here is very long it provides eventual consistent reads which is the default and then it has strongly consistent reads and the way you would provide capacity is you would say I want to have a certain amount of Rec capacity um per second or 100 and that's going to help you determine uh your cost here so notice here that we are setting a read and a right capacity and then it's going to give us a calculated cost and then we have a guarantee of performance for are spent um all data is stored on SSD sto storage across three different azs um but yeah there you go hey this is Andy Brown we're taking a look at the anatomy of a Dynamo DB table so we have a table representation there on the right hand side and it looks like relational data but just understand that key values can be represented um uh in a table like structure but they're not actually Rel okay but let's break down the components of a Dynamo DB table the first is the table itself which is containing rows and columns it's the the main container for everything uh the next thing are items which represent rows of data then we have attributes these are columns of data and the idea here is that we'll have a key so the key is up there identifying the type of column and then the values are the actual data itself so hopefully that is clear CLE but it can be very confusing because again it looks like relational data but it's not okay let us talk about Rec consistency on Dynamo DB and Rec consistency is when data needs to be updated it has to write updates to all copies and data can be inconsistent when reading from a copy that has yet to be updated so you can choose with be the Recons syen to meet meet your requirements and we have eventual consistent reads and strongly consistent reads um there are other services like S3 where uh you used to have to uh uh factor that in but ums has improved that dynb is not at a point where um everything is going to be strongly consistent so you have to choose what you want but let's talk about eventual consistent reads which is the default option so this is when copies are being updated it is possible for you to read and be returned on inconsistent copy and so reads are fast but there's no guarant guarantee of consistency um all copies of data eventually become generally consistent within a second so you have to design your apps with that consideration uh then we have strongly cons consistent read so when copies are being updated and you attempt to read it will not return a result until all copies are consistent and you have a guarantee of consistency but the tradeoff is higher latencies or slower reads so all copies of data will be consistent within a second so you know again you just have to determine what it is that you want and design around that for your architecture okay hey this isre brown in this video we're going to take a look at Dynamo DB so uh there's a lot to cover here but I just want to do a a basic introduction to it um and get us uh some set up with some basic stuff so the idea is that what we're first going to want to do is create ourselves a table um and I think the easiest way to do this would probably be to programmatically do it as they keep changing the uh CLI here but it is nice to see that we have a bunch of options um I do know via Sam you can create a very simple um a very simple Dynamo DB table so if we go here and we go to the left hand side we have an option for simple table and so this is one way that we could do it another way would be to just fully initialize um Dynamo DB so so maybe we'll start with Sam as a simple example and then move over to the fullblown one so what I'm going to do is go ahead and make a new folder here called Dynamo DB and if you don't know you should go ahead and get set up uh with some kind of environment have your a CLI installed we're going to go ahead and um I'm going to grab some code that already exists maybe from this inline Lambda where we're using Sam because we do have a couple of lines here that we'll want like this transform line so go ahead and we'll call this template uh. yaml no name it with yml because that's kind of what Sam likes us to do and we'll go back over to our template here but before we do let's make a new folder called bin and I want to bring over these two lines here the deploy and build so we just go ahead and copy that and I'm just going to paste this in since we're using Sam C uh make sure you install it so I have a um a bin script here called it CLI um it Sam CLI okay I already have it installed but I'm just running it again as an example so it works pretty straightforward I can just open the script up here if you're not familiar with it I just grabbed this from the documentation um and reworked it for my environment so you should be able to do the same we're going to go into here I just want to rename this to be uh Dynamo DB table let's say DB table here uh and these scripts should uh work with no issue we'll go down to template yamel and and oh well by the way I I think I just renamed uh yeah this one I I didn't mean to do that I'm just going to change that back sorry because that's not ours ours is over here and so this one's going to be DB table and so we'll go into our template yl file I'm going to go back over to here we'll look at simple table examples and we'll grab this one and that is a very very simple way of getting this provision so we'll go ahead and paste this in and I'll just say uh simple table um can we let it autogenerate our name that would be really nice if we do not it will generate our name excellent um we don't even need anything we probably just put table here uh could we I think we'll have to actually have something so maybe in this case I'll actually name the table no you know I'll leave the tags that's what I'll do I'll leave the tags so we'll just have a tag here we'll just say uh hello well just junk tag here so we can get this deployed and I'm going to CD into the Dynamo DB directory here and we'll just chamod U plus X bin everything okay and we'll go ahead and give this a go so I'll first build this and then we will go ahead and deploy this we talk more about the Sam stuff when we do the Lambda so if you're doing Dynamo DB out of order or if it is out of in order you'll know more about it later on we'll cover it more thoroughly there um it failed it does not like it type object must have a type I mean we copied and pasted it so yeah why is the type not there that's a little bit funny looking so this is something we should have in here so we'll go ahead and do this and you know what I don't think we actually have to have anything uh here just take this out so I'm just going to take out the rest we actually don't need any of it and so we'll build this again and then we'll attempt to deploy this again and while that is going I'm going to go ahead and type in uh CFN for cloud information and this should create pretty quickly we have a problem let's go take a look here is it going to tell us why nope required key oh because we need to provide our a key schema okay that's fair enough so in here um we'll go down examples did they provide us anything here for a schema example well this is the full Dynamo DB table I want this one and they don't this is not a good example it does not provide us um enough enough stuff so it looks like we can specify a primary key and that's it so that's pretty simple we'll go ahead and add our properties while we're wait uh while we're doing that let's go ahead and delete that stack so go back here and type properties primary key uh an attribute name and type uh so if not provided the primary key will be a string with a value of ID so I mean it seems like we shouldn't have had to provide anything here required key schema key schema not found but it says that we could leave it blank so okay all right well we'll just try it again so we explicitly provided the key this time it's kind of silly that we have to do that but we'll go ahead and build this I'm going to go check the template just make sure that it actually is using our new stuff it looks like it is excellent um but this is complaining still key schema is missing so there must be something wrong here still properties table name tags H all right give me a second oh you know what it is I grabbed the wrong one that's why it's messing up so we actually have to have this a simple table that's why it's getting confused and um so I'll go here I'll just take these out again sorry we'll build this again and uh I updated the wrong template so just be careful about that we'll try this one more time okay so we'll build this and then we will go ahead and deploy this excellent we'll go back over to here refresh it and I'll go to the change set we'll click into it we'll go ahead and execute this change set okay so this is now uh should work without issue we'll give it a moment to get ready okay all right so it looks like our table has successfully created here let's go take a look um and so I'm going to go over to our uh or Dynamo DB okay and so we should go to over to tables and we have our simple table pretty straightforward here um and so we can see we have a partition ID key there is no sort key it's Set On Demand by default default it does create those oh it says no active alarms I don't really like having those default alarms when I'm testing stuff out because I find that that uh gives me trouble down the road here we can explore our table items which we currently do not have anything in this table so let's go ahead and actually insert something into this table so I'm going to go ahead and look up the ads CLI Dynamo DB commands here and we'll go over to uh version two here and in here there should be something for uh insertion so I'm just trying to remember uh what command it is it's going to be right feel like there's an individual Command put item there we go put item that's what I want and we'll go down to this example here and we will go through the whole uh CLI because that is something that's very important to know I think is that but for now we just want to have a very simple example and uh we'll go ahead and grab those two lines as per usual we are going to go and paste that in here we'll go grab our DB table name which is just DB table and uh we're going to want to insert some kind of record so here it says item Json I'm just going to go up into here and just type in item Json I'm going to assum we can provide anything we want here it will have to have an ID of course so we'll go ahead and just say uh and what is that is that just an integer string what is it I'm going to assume ID string okay so we'll just say hello we'll say fruit and we'll go here and say apple so I think it should be as simple as that for that to work I'm going CH mod this command here here so for our binp put um I'm going to have to adjust this a little bit because we need to have a relative path so that it doesn't mess things up so I'm going to go ahead and just copy these two here we'll just say item path here item path this will just be item Json and then we can just go here and say dollar sign item path and we'll go ahead and try this put now so it doesn't like something there's something wrong with this and it's saying uh for item expected equals instead of end of file h i mean it looks fine to me let me just stare at this for a second all right um yeah I'm not exactly sure I mean we can give it an equals if that's what it really wants here doesn't seem like we should have to put that there item Json enter air parsing parameter item uhuh uh I mean maybe we could just wrap this in quotations will that help it out okay what what your problem let's go take a look at item and see exactly what it wants a map of attributes okay but the example it shows us provides a path to Jason so we should be able to do it if it's showing this to us so not exactly sure what it does not like try reind denting this I suppose I don't think that's the problem I'm going to take out these other ones here oh this one's not even printing it now okay I'm just going to try this again so just copy all this I actually don't really care about these last two here to be honest I just want to do a simple record here we can talk about this those those in a later video we'll go ahead and hit enter here we'll hit enter um oh sorry we want to actually run the command there we go so yeah I mean that should work I'm going to go ahead and copy this paste wrap this in double quotations because I mean that's what I think that it wants that that problem is right okay let's just read what it says for here item item required a map of attributes and key pairs okay well I'm I'm providing you that I'm given what you want so what's your problem here we'll try this again all right let me see what I can do okay here it's saying I fix issue when I move the file locally and executed command with file colon colon and here they're suggesting that you're supposed to put it in there but I thought that we had it in there oh we don't okay so that's our problem sorry we'll go ahead and hit enter and we'll try this again um getting a little bit better here so parameter validation failed so this has to be a very particular format that's right invalid type of parameter ID um so let's go take a look at what that item item structure supposed to be uh we have key and then value and then string can I just get an example please okay yeah so that's pretty straightforward we'll just go ahead and grab that and we'll go down a line we'll paste this in and so this will be as such there we go hello and then we'll have fruit and this will be the uh same story here okay this has to be an S so I think this is what it wants we'll go ahead and try this again pin pinut resource requested resource not found all right let's go take a look at our oh our table name is not that it's this silly me okay we'll go ahead and paste that as such I'm going to just put it this says environment variable up here Dynamo DB or dbb and uh then we can just have this here okay and we'll try this again one or more parameter values we're missing uh Missing the key ID Okay so key ID string uhuh one of our parameter values we're missing missing the key ID in the item it's right there maybe it's case sensitive and it made it lowercase it's lowercase okay and we'll go ahead and just make this lower case we can also make this one lower case honestly I prefer when uh these tables have lower case names we'll try this again there we go we'll go back over to here we'll give this a refresh and we'll just explore our item tables we'll run it there it is there's our record and so I feel like that's a good start uh uh to this but you know often you're not going to provision tables using uh simple table you'll be utilizing the um more complex one one here I noticed I didn't actually make a folder here I'm going to just go go ahead and make a new folder here called simple and then we'll just move the contents into here okay simple yep and bring that there simple great so we'll go ahead and just tear this down and then what we'll do next time is actually properly provision a Dynamo DB table using Dynamo or um uh cloud formation okay because this is not sufficient we'll we'll go ahead go to Cloud here you know it really depends on your use case but most cases you should be fully configuring a Dynamo DB table so we'll see in the next one okay uh and actually I'll just wait for this to delete just make sure there's not like deletion protection on it or something so I'll just wait here all right there we go so uh it's all teared down so we'll go ahead and just uh wrap it up there okay see you in the next one ciao hey this is Andre round and in this video we're going to take a look at uh setting up Dynamo DB but uh not using the simple table and uh using the full CL formation uh that we can utilize I'm going to go ahead and make a new folder here and this one's going to be called uh basic as we're going to set up a basic Dynamo DB table it's not simple as in because we can actually do more configuration here um I actually already have uh this file from uh when we did the boot camp and it just has a very good example of a full Dynamo DB table so I'm just going to borrow from it and I'm we do a lot of cloud information in this course so I don't feel like we're missing out on much we can go over here to uh CFN and take a look at the table definition we should probably grab that over here anyway going to go ahead just type resources and um we will grab some of this code again if you don't you can't find this repo and by the way this is a public repo you can just of course get to the examples repo and grab uh the code code I'm going to pair this down a little bit but this was a really good Dynamo table because this was using single table design I'm not sure if we'll do that here today but um the point is is that this is a very well configured table um okay so the idea is that um in here we might want to Define our table class so here it is standard if we go over here we can go and see that that we have standard or infrequent access so standard is sufficient for what we're doing here and we have our billing mode so that's really important I'm going to bring that up and so you can see that we are doing provisioned so if we had another billing mode here this could be provision or I believe on demand or pay per request which is I suppose the yeah that's the on demand mode there then we have um attribute definition so that is us actually defining uh some Fields uh up ahead we have our key schema that this one's really important so here you can see that we're setting a key schema with primary key and then uh our secondary one is a secondary key this is a very common pattern when you're doing single table design uh and it's actually just a good structure to have in general when you want a lot of flexibility so we'll leave that alone um then we have attribute definitions I don't remember that off the top of my head we have provision throughput this is important because this maps to our uh our provision mode five and five is the standard that you normally would get if you spin up a new Dynamo DB table so we'll stick stick with that um I'm going to turn this off for now because I do not want um delution protection this one particular is configured to have Dynam DB stream so I'll take that out because we don't need that here we have a global secondary index which allows us to set up a second table for now we'll keep it simple and moove this out of here um so yeah we Define our attribute definitions here and so we have our primary C primary key and a secondary key but if we had other ones we could def find them here as well so I could go here and say something like uh fruit and we could say vegetable and so that would set us up with some initial uh field so let's go ahead and see if this uh works as expected so I'm going to go ahead and just make a new folder here call it bin and um actually I'm going to keep it simpler than that no we'll keep B because we might end up putting other things in here but I'm going to go ahead and have a deploy file in here here I'm just trying to think of something where we have a script that we can borrow from maybe from data streams we'll go ahead and copy this one we'll bring it into our basic here we'll change this over to dbb basic this I want to bring over a little bit more than just that because I want to make sure we're referencing that file correctly because it's in a subdirectory here so I'm going to go into our other deploy one which was Sam I just want to copy this root here here I wouldn't mind bringing over the Set uh set hyphen e as well we'll have our root path and then the idea is that we will assemble our template file so I'm going to go ahead and just say a template path here whoops I thought we would have had it in this one maybe it's in the build here we go and so I'm just going to paste that in here and then I just want to replace this with sign or dollar sign template path I'm not sure why we do it that way because I'm pretty sure we could just pass the dollar sign template path but whatever that's fine we'll just stick with what we have here and so this should um in theory work I'm going to go ahead and chod this uh basic well we'll CD into basic first bin deploy okay and then we'll go ahead and deploy this still says data streams up here uh ddb basic invalid template basic template gaml we'll go and take a look at our our template file and see what it doesn't like um I don't necessarily see an issue with it we could use CFN lint to lint it if there is something wrong with it invalid temp oh it's saying the the path is invalid so we have basic template I'm going to put a y am Al here there we go we'll go ahead and temp this again so I'll go ahead and create a change set we'll make our way over to here into cloud formation and we'll give this a refresh we'll go over to change sets and we will execute the change set and hopefully this will work here in a moment all right so we have a problem let's go investigate here it failed to create let's go find out why and it's not telling us why it'd be nice to know why oh let's go over to advance here we go the number of attributes in the key schema does not exactly match the number of attribute definitions fair enough and so I think the reason why it's not working is that um tribute definitions let's go look that up again because I clearly do not remember what this is yeah I thought we could put whatever we want in here yeah key schema so yeah we have our hash and our range and then they're repeated again and then we can have our additional ones but I guess the thing is that these ones are being used down here in our Global secondary index so basically it's not any field it's whatever we're using as a keys so I'll take these ones out I was Miss I was mistaken I'm sorry and we'll go ahead and just delete this okay we'll wait for that to delete all right so that is uh tear down we'll go ahead and give this another go but yeah defining Dynamo D DB tables this is a little bit verbose it's not the nicest but it's fine as we go into here we'll go ahead and execute this change set and then hopefully this time it will work all right so our table has created so that is great let's make our way over to uh Dynamo DB and we'll go to our tables and now we can see we have ourself a table primary key sort key this is all good this is a really good basis for us to do other things in dyn DB was to get this uh basic table set up uh protect table accident reads and writes Point time recovery they're always adding new stuff in here it's NonStop and it's funny because this is the new UI and they're already going to change this UI again and I know this for certain uh so ads who's ever there on the design team please stop changing things every two minutes you know it's frustrating but uh yeah we can explore items so there's nothing here so we'd have to add items you know like uh hello world okay but really I just wanted to accomplish uh setting up a Dynamo DB table here uh using this so we can use this as a basis for other Dynamo DB Videos so I'll see you in the next one okay ciaoo hey this is Andrew Brown and we are looking at cloud formation so before we talk about cloud formation we need to know what infrastructure as code is also known as IAC so this is when you write a configuration script to automate creating updating or destroying Cloud infrastructure doesn't necessarily have to be Cloud infrastructure but it is INF infrastructure generally uh IC you can think of it as a blueprint of your infrastructure IC allows you to easily share version or inventory your Cloud infrastructure rure and uh ads is solution there are a few but the one that is built into ads and is the most lowlevel I tool is cloud formation um and cloud formation in particular is a declarative uh style I tool meaning what you see is what you get it's a very explicit tool so you have to write more code uh but you're not going to get any side effects um and personally I love cloud formation and even though there are tools like cdk and terraform uh uh palumi and things like that um I just don't see the extra layers worth it and I just write it in cloud formation so again depends on the use case but I really like it cloud formation has this concept of stacks which are uh deployed using CFN templates so you'll see the word stack often as we uh do deployments and adus manages the state of your resources um so if you are used to using terraform you know that there's a state file and you have that is the thing that holds uh the configuration or state of your stuff and you can't lose it there's no issue with cloudformation here because they manage the state uh on their servers and you never have access to that state file uh so you don't even have to think about it but what would be a use case for cloud formation well a a great example would be imagine you wanted to create a Minecraft server subscription business where people want to say I want this size of server and I wanted to have these particular things installed on it well you could utilize Cloud information to create a template um and the idea is that if somebody went to your website they would click a button it would trigger an API call to launch that template and set up that infrastructure and so that is one of many use cases that you could use cloud formation for um but uh cloud formation comes in two formats we have Json which is the original format uh and it's still around and has a good use case which is when you have programs that can Pro programmatically generate out uh cloud formation because Jon is uh much easier for most uh sdks programming languages to read then there's the yaml format which is the more human readable and maintainable way to work with CFM when you're talking about humans so this is what it would look like with Jason and this would be its yaml equivalent and if you're not familiar with yaml yaml is Jason compliance so that it's very easy to go back and forth between the two but uh yeah yaml is what you'll see me utilizing quite a bit but we will write some and Jason just to get the feel for it let's take a look at the um anatomy of a template so here is a template and it's made up of these sections the first is metadata this adds additional information about the template so that would be things like the description or Theus template uh version this can be a little bit misleading because there is other things called metadata in cloud formation which weren't there before but are now uh there so there is U metadata that can be attached specifically to resources but we'll cover that when we get to that section just understand what I'm saying metadata here I just mean like description the version uh things like that and there will be another thing called metadata then of course we have our uh description so this is a description of what the template is supposed to do we have parameters these are the values to pass uh to your template at runtime you have mappings this is a lookup table it Maps keys to Value so you can change your values to something else I don't really like using mappings but it is something that you can utilize we have conditions where resources are created or properties are assigned we have transforms this applies macros which allows you to modify or change what you can do in the template basically like extending or uh creating shorthand uh types of templates we'll look at that when we see Sam templates which is a very popular macro then we have our resources which is the main thing you want to create here so it's anything that's an a resource that would trigger an API action then you have your outputs which allows you to assign values and um I want to point out that cloud formation templates require you to have at least one resource so if you don't have any resources in there the template's not going to work but there you go so now we're looking at the anatomy of a cloud formation template uh and so these are made up of a bunch of different sections uh and here are all the sections listed out here and we'll work our way from top to bottom uh and so the first one is metadata so that allows you to provide additional information about the template I don't have one in the example here and I rarely use metadata but you know it's just about additional information then you have the description so that is just describing what you want this template to do and you can write whatever you want here and so I've described this uh template to uh launch an E2 instance running aachi and it's hardcoded work for us to one then you have parameters and parameters is something you're going to use a lot which is you defining what inputs uh are allowed to be passed with into this template at runtime so one thing we want to ask the user is what size of instance typee do you want to use it's defaulted to micro but they can choose between micro and Nano okay so we can have as many parameters as we want which we'll use throughout our template to reference then you have mappings which is like a lookup table um it Maps keys to values so you can uh change your values to something else um a good example of this would be let's say uh you have uh a region and for each region the image ID string is different so you'd have the region Keys ma to different image IDs based on the region so that's a very common use for mappings then you have conditions these are like your IFL statements within your template don't have any examples here but that's all you need to know transform is very uh difficult to uh uh uh explain if you don't know what macros are but the idea it's like applying a mod to the actual uh template and it will actually change what you're allowed to use in the template so if I Define a transform template the rules here could be widely different different based on what kind of extra functionality that transform adds we see that with Sam the serverless application model is a transform so if you ever take take a look at that you'll have a better understanding of what I'm talking about there um then you have resources which is the main show to the whole template these are the actual resources you are defining that will be provisioned so think any kind of resource IM roll E2 instance Lambda RDS anything right and then you have outputs and outputs is uh it's just what you want to see as the end results so like uh when I create the server it's we don't know what the IP address is until it spins it up and so I'm saying down here get me the public IP address and then in the the console we can um see that IP address uh so that we don't have to like look at the ec2 console and pull it out the other advantage of outputs is that you can pass that information onto other cloudformation templates or creative like a chain of effects because we have these outputs but the number one thing you need to remember is what makes a valid template and there's only one thing that is required and that is specifying at least one resource all these other fields are optional but resource is mandatory and you have to have at least one resource so we're going to look at what a CLA formation template looks like and this is actually one we're going to use uh later on to show you how to launch a very simple Apachi server um but claw formation comes into variations it comes in Json and yaml so why is there two different formats well Json just came first and yaml is is an indent based language which is just more concise so it's literally the same thing except it's indent based so we don't have to do all these curries and so you end up with something that is uh in length half the size uh most people prefer to write yaml files but there are um uh edge cases where you might want to use Json um but just be aware of these two different formats and it doesn't matter which one you use just use what works best for you hey this is this is Andrew Brown and we are taking a look at ads quick starts which are prebuilt templates by adabs and ads Partners to help deploy a wide range of stacks uh this helps reduce hundreds of manual procedures into just a few steps and a quick start is composed of three parts a reference architecture for the deployment an databus Cloud information template that automates and configures the deployment a deployment guide explaining the architecture implementation in detail um the idea is that they have a variety of them so imagine you wanted to launch an ads Q&A bot uh there would be an accompanying diagram most Quick Start references deployments enable you to spend up with fully functional architecture in less than an hour um so do I like quick starts uh they're okay um I probably would never utilize them because a lot of times they require uh uh too much in terms of uh uh configuration in terms of them being like they're for production right and most of the times when you want to start working with something you want something uh not so expensive and so this is great for Enterprise clients that are not concerned about cost but a lot of these templates are Overkill but it is cool that they do have it so yeah there you go hey this is Andre Brown in this video we're going to take a look at quick start so quick starts is a way to quickly launch uh recommended projects um via AWS partners with AWS so it's been a while since I've used this and I'm trying to think of something that we could utilize that might be free so we'll go ahead and checkbox quick starts I guess they have eight of us solution implementations I'm not exactly sure what that is not really worried about it but you can see we have a a lot of options Amazon for RDS actually sounds uh very straightforward and simple so let's open that one up and and see what we get just in case you are not sure where this is I'm going to go ahead and make a new folder here call it quick starts and I'm going to just go ahead and make a read me here just so you don't have to hunt down the links if you are looking for them so let's read this the solution deploys the RDS database uh on an RDS cluster private subnet read replica a with a right replica and another Zone it sounds like it's going to do a lot of stuff here so this could be possibly expensive as soon as I hear that it's going to do more than one thing but we can look at the actual architectural diagram and so this is setting up yeah a uh a read replica and now Gateway so if you do not want to spin this up you don't have to I'm going to go ahead and do it just so that we can experience it together okay so here we have a bunch of options um but I want to go ahead and just start using it so used to be a button here how to deploy into an existing VPC so we'll go to this one here we go and so the idea is that it's already populating the template here and what's interesting is that we could actually take a look at this and view this an application composer this used to be the service used to be called something else yeah call information designer and so this is the newer version of it but I just want to see what it wants to create so here who looks like a lot of stuff but we have um what do we have we have an RDS Security Group an encryption Alias key so no not exactly sure what that is um I mean the part of Secrets manager I'm thinking maybe that Secrets manager then we have a bunch of cloudwatch alarms we have an SS SNS topic so they'll trigger that then we have our database it's this little icon is confusing me the little one down below there if I go to details I can find out exactly what this is it should say in here what this is yeah not exactly sure um and some other component now we can just look at the template and this makes it a lot easier to see what's going on here so we can scroll on down and yeah we have a lot of stuff I want to see the resources got parameters still what is this stuff up here oh metadata okay oh because it has a cloud formation interface okay so there's additional things going on in here and I'm going to keep scrolling down down down down password username cluster database Port Etc yeah so something's going on here but what I'm going to do because I'm satisfied with this I think I can go ahead and launch this again don't launch it if you're not comfortable with this but I'm going to click back uh here and so now we're back on this template I'm going to go ahead and hit next and notice that we have to enter in parameters so this is all the stuff that it wants um I kind of wish that I didn't have to do as much work here so what I'm actually going to do is go back I'm going to launch it into a new VPC I'm going to hope that it just does most of the work for me this time around so hit next and list of available zones to use for the subnet only three azs are used for this deployment can it just decide on its own I'm hoping that we can just hit next here we have the Linux Bastion host which is nice and this is using something really large I want something small like T2 t3s let's say T3 micro and I don't care what the password is I'll just put whatever in here like doesn't mask it there that's interesting oh that's the username okay so my username capital T testing 1 2 3 4 5 6 exclamation mark oh you can choose whether you want to have a z or not so that would actually make it cost effective that's actually good RDS multi Z database instance deployed so false and false um backup retention I'm going to set this to zero days um we can leave that to true that's totally fine I want to be on gp3 because that's cheaper we'll make this 30 because I don't need to launch something super large here encryption can stay though that's totally fine I don't want Auto scaling enabled that doesn't really matter um I'm not sure about the rest of this I don't need performance inside so I'll turn turn that off and we'll set this to one if not zero we have some additional stuff here so this actually isn't too bad this is actually quite a nice uh interface we'll go ahead and hit next and it looks like I got away with that and then hopefully this will work considering all the stuff that I'm doing here so we'll just give this uh two check boxes here even though we're not actually giving Auto expand because we said no in the template here it says performance retention period number must not be must be a number not less than seven whatever we're not using it because it's turned off but we'll go ahead and set that back on so I'm looking for this this will be back to seven here we'll hit next and next and go down below and check box these here and hit submit and so hopefully this creates so that'll be exciting and right away it fails so we'll go over here and take a look let's see what the problem is parameter validation failed value for parameter key pair name does not exist parameter value parameter name availability zones does not exist so it wants us to fill those things in I guess I'll give it another attempt here um I'm not really interested in doing this because it'll set up a lot of stuff so I don't know I guess it kind of satisfies it and shows you how this how quick starts works I do believe that we go to some other things that are more complex that it might show us something uh different in terms of the uh process here I remember it used to be like that so let's just pick anything that is not what the RDS database was and no it seems like it's all the same you just click to it and you get to a cloudformation stack but anyway that is quick starts and I'll see you in the next one okay ciaoo so when you have a cloud formation stack it's going to have a status uh code which will be in this uppercase uh underscore format and basically that is the state that your cloud formation is in it's going to tell you quite a bit about what you need to do or or what you think of your cloud formation uh stack but let's take a look at all of these these aren't structured in a way that uh makes any sense so I just want to go through the whole list here and I believe that I have a website we'll go take a look at that's a repo that visualizes how they all relate but we have roll back complete roll back failed roll back in progress create complete create in progress create failed delete complete delete failed delete in progress review in progress update complete update update complete clean up in progress update in progress update roll back complete you get the idea uh there's a few here and um the one that you'll see quite a bit will be this review in progress especially with me because I like to um use cloud formation where um I always accept the change set and so it always expects you to uh review stuff before you launch it for the most part you don't have to memorize these you will learn them as you use cloud formation and it'll just become very familiar I just wanted to show you how many states there are but there you go all right so I just wanted to show you there was this repo here by uh RV uh. RC um and down below looks like they've documented the entire uh States I mean this is like 7 8 years old but it's still really good because it lists out all the states and I don't believe there's any other states that are missing but if we go down below uh we can actually kind of get a visual visualization of how this works so the idea is you have a stack it's the created updated deleted and so those are uh the key ones there and so that's where we see a lot of variety uh in there but then down below we can look look at this more so we have let's say we have no stack it goes review in progress and then often this is where we will accept it it will go create in progress from there it'll go to create complete and um if you want to update it then you'd have update in progress update complete update complete you get the idea so it's pretty straightforward now if you run into errors uh then you can see there's a lot more going on so uh yeah we end up in this kind of thing here but my point is that you will understand these states just working with them and I've never really had to visualize this and understand any of this but it's just nice that the fact that this exists and I just wanted you to uh see this if for whatever reason you want to read more about it okay let's talk about stack updates so when you need to change uh make a change to the stack instead of deleting or recreating the stack you can modify the CLA formation template and push a stack update and CLA foration will intelligently change or delete uh or recreate resources so there are two ways to perform a stack update the first is a direct update so to directly update a stack you submit changes and adus cl information immediately deploys them uh you use direct updates when you want to quickly deploy your updates this is the default state of cloud formation I never use this one ever because I always like to review the change sets and it's good practice to do so so the other one is executing change sets you can preview the changes to Cloud information uh will make to your stack and then decide whether to apply those changes use chain sets when you want to ensure CLA information uh uh doesn't make unintentional changes and when you submit an update CLA information updates uh differently depending on the state of your resources and circumstances this is what you're looking for uh in the actual chain set the only thing is that the chain set does not list it as these three things and you have to kind to infer it based on a couple of properties but let's look at the three things the first is update with no Interruption so updates the resource without disrupting operation and without changing the resource's physical ID then you have updates with some Interruption so updates the resource with some Interruption and retains the physical ID and the third one is replacement uh which recreates the resource during an update and also generates a new physical ID so there you go hey this is Andrew Brown in this video what I want to do is uh focus on stack updates and see if we can uh see what happens uh if we try to change things in our stack so what I'm going to do is I'm going to make a new folder in our cloud formation folder here called uh stack updates and we're going to go ahead and copy over our template file and I really do not want to continuously have to um upload a file so I'm just going to grab from one of our many many deploy files uh a deploy file here and just paste this into here and just in case you aren't familiar with this yet um basically what we're doing here is we're using the cloud information deploy uh command here we're passing the template yaml which is that file there we'll give it capacity named I am we don't necessarily need to have that on there but I'd like to have that on there and we have no execute change set because I always like to manually review my change sets and in this case we definitely want to do that because we want to see whether the stack is going to get updated or not so we'll stick with our very simple example here first with the just the bucket we'll say stack update and deploy CFN stack update I even know why I have that line there I'll get that out of there and this is very simple it's it's still using our uh example here I'm just going to take out that part and take out the description as we don't necessarily need a description but um yeah so this should hopefully work I'll just CD back and we'll go to stack updates here and we'll go ahead and deploy okay I'm just working with S3 bucket first because I'm assuming that's going to be very quick whereas if we were to use something like an instance that will take a lot more time but we might still spin up an ec2 instance in this example so let's go over to change sets we'll click into our change set and notice it talks about replacement here it says true because there's nothing new oh sorry it doesn't exist before so it would have to replace and we have this on action ad so we'll go ahead and hit execute execute change set and we'll go ahead and do that so we'll wait for that to create all right so that is uh deployed and so now what we'll need to do is make some kind of change on our bucket uh I just happen to have the docks open here I'm not sure if I pasted it in here but if we haven't I'm going to go ahead and place this here um just so we can quickly get back to uh the bucket and I'll just take this part off on the end so what I'm going to do is change some kind of parameter on here I'm not exactly sure what I want to change but something that should be very easy to do like maybe tags we'll add some tags uh maybe we can see an example it maybe it has tags down here and it does not so I'm going to go here and just put in tags and we'll look at its format that it expects so if we go into here it's key and value so key hello Value World and so is this enough to uh trigger some kind of change we'll see and so we'll go back over to cloud formation we'll refresh and we'll have to go to change sets it's not going to reflect it here and we'll click into here and notice that it says modify okay so this one is going to modify and then notice here that it says replace false because it can just um modify without uh destroying um destroying that there so we'll go ahead and execute that change set all right and we'll just wait for that to complete all right so I believe our update is complete there let's go back here I was having some internet trouble so I'm not sure oh there we go what state I'm in here and give it a moment all right so I think my internet is back now I'll give this a nice another refresh here we go great and so uh you know we see we saw in our change set what was happening there um like go back to the last one whether it would replace it or not and what the the action would be let's say we can force a replacement um so this would be really dependent on changing something that is critical to it and I think that um for this it' be a little bit hard because we would have to uh because like for a bucket in particular it's really hard to replace it because everything that you do want to it is a configuration change whereas like if you made a change to a server I think that would be a better example so I think what we'll do is we'll launch up an ec2 server ec2 server ec2 instance CL information so that we can actually observe that change there so what I'm going to do is go down to examples and there should be like a very basic one I know we don't need a lot in order to launch an ec2 instance and I really want to do the minimum here so I'm going to change this out so I don't want the key name I don't want any the block device stuff and all I'm going to do is put in the Ami so if we go over to ec2 here just open that up at another tab here and we go over to instances and we attempt to launch an instance and I go ahead and grab uh this value here you know what I think I did another lab where I forgot to shut down shut down an instance and you know whenever you're doing any of these Labs just make sure that you double triple check any of your resources as you're responsible for them I am not but anyway so I have an Ami in here and so I'm thinking that this is all I need to do this it might not be the case but we'll we'll go ahead and see what we can do here so I'll go ahead and do a deploy the thing I'm not specifying is like the type so I'm not saying like instance type I would expect I would expect it it would want that right instance type T3 micro and I think that's what it is instance type what would this default to doesn't say um but I don't think I would trust to deploy this because I don't know what instance type it would utilize here so it definitely would create a new instance I'm going to delete this change set as I'm not happy with that we'll go ahead and try this again and I feel like this is a much safer bet to do so we'll go in here and see if we can execute this policy okay so we'll just wait here and it looks like that is completed that was really fast so easy to instance and so what I want to do is I want to purposely try to trigger this to to replace um probably changing the type would be something that would do that so if we had a t4g micro which I believe is a type and we go over to here what I want to see in our chain set here is that it says conditional and it says it's modifying so um a conditional replace means if it has to but it clearly has to because you can't just uh replace an instance without changing its type so this is that case there so I gu this kind of really satisfies what I was looking for in terms of uh showing you how how stack updates work the the thing that we did not show you would be maybe um if we were to go into here and just take out the no change execute as for the most part I really do always have that on in all our videos so you're probably not used to seeing it without the uh change set without that explicitly turned off let's go ahead and just do that and see what happens when we don't have the execute change set in there and it's still waiting for the chain sets I'm kind of curious did I not update this file oh it is so it says creating the chain set waiting for the stack update to complete so notice that it just proceeded forward and we did not have an opportunity to intervene there so yeah that's not how I like to do it but I I just again wanted to show you what that looks like and I'm going to wait for this to finish updating okay it actually looks like we ran into an issue I'm surprised here so what's the problem um t4g micro is not a valid instance type so I must have entered it in wrong I could have swore there is a t4g um which is fine I mean it didn't launch that just saves us some time here but I'm just going to go take a look here I'm just curious as to what it is then T4 yeah t4g micro but it looks like it's not maybe available in uh the canid essential region so that's probably the reason why it's not working there but that's totally fine as I did not want to actually create anyway so we'll go ahead and delete that um and we'll just clean up our stack here once that's deleted then we will consider this done I'm going to go over to here and we'll just Commit This code we'll just say um uh stack updates for this there we go and once that's done we'll wrap this up I I assume this is going to delete successfully so we'll just end it here okay let's talk about stack policies so these allow you to control if the stack update actions are permitted preventing a stack update could be uh be to prevent data loss or Interruption to services and the way it works is that you'll create a stack policy um this is not an IM am policy even though it looks uh very much like one but notice that we have anabis cloudformation stack set policy so you're pushing this policy over uh to cloudformation the key thing in here is that when we specify the resource we always say logical resource ID and then uh the The Logical resource ID in your Cloud information template that you want the policy to apply notice here that it's saying deny update for all types of updates notice that when we did the uh stack updates there was three ways to do it and so that is the the three things that we can specify it on so let's look at the possible actions we have update modify update replace update delete and then update for all the above cases the only thing that's different here is that this one is like if you were to update uh your your CL information and had to delete something okay so it doesn't exactly map to those three uh uh one one but the point is is that it's very similar okay nested Stacks allows you to reference cloud formation templates inside of other cloud formation templates this creates modular templates which gives you re reability or you can assemble larger templates which reduce complexity so the idea is that you always have a root stack uh and then you have your nested Stacks underneath uh parent Stacks have have immediate access to the child Stacks the root stack is accessible by all the nested Stacks uh so here is an example of us using a nested stack and notice that it's using the type is Adis CL information stack and then we are specifying another template now it says template on the end there it could just be do yaml I don't know why but for whatever reason um in the docs and uh the way I do it as I always name it do template but is really just a yaml template and so you can either provide a template yourl from an S3 bucket or you can provide the template body what was interesting that I found out during the lab and I didn't realize this is that you didn't have to give S3 any permissions or Cloud permiss to read to the S3 bucket it just seemed to be able to do it so that was really nice um as far as you remember but yeah there you go that is nested Stacks hey this is angre brown and we are continuing on with call for in this video I want to take a look at neted Stacks so neted Stacks is um a way where we can reference cloudformation templates that we can utilize in inside of other ones so uh let's go ahead and give that a go so I already have my cloud developer environment running you use whatever you want I'm go ahead and make a new file here and we'll call this one nested Stacks um I use a lot of cross Shing Stacks I don't use nested Stacks very often but I'm sure we'll be able to figure this out I'm just going to grab some code from our let's say our stack updates one here as we have a very good little basis for um this one here and we'll paste this in and I'm going to go ahead and just rename this to nested stacks and the idea is that we'll want one template which will be the one we reference and then the other one will be um the nested stack so I'm going to make a new file here and I'm going to call this one um s3bucket uh template and in here I imagine it's as simple as just taking the code here so I'm going to go and grab this and paste this in here and instead of having an S3 bucket and E2 instance we'll just have an S3 bucket and we have some defaults here which is totally fine um and then in our main uh our main template file I'm going to just take this out here I'm just call this my stack and then we will make the type here uh ads cloud formation stack and I'm just curious if we look this up what this looks like here okay and it says always do the updates from the parent stack that's totally fine um but I just want to go ahead and reference this here let's take a look at some of the parameters it has or properties we should say so we have capabilities that makes sense uh change set up ID uh creation time description disable roll back uh nothing super interesting here now we got parameters here parent ID rollar root ID stack ID stack name so I guess we'll just try to figure this out the best we can and so we'll go to properties here and right away it's already trying to populate the template URL so the idea is that we need to upload our our um template somewhere so what I'm going to do is we'll have to create an S3 bucket to make an S3 bucket which seems a bit silly but that's what we're going to do here today I'm going to go and uh create a new bucket before I do I'm just going to delete some of these buckets out of here all right so I got rid of a few I'm going to go ahead and create a new bucket this will be uh my nested uh nested stack templates I'll just put some numbers on the end here so we don't have any issues this is in I guess I'm in North Virginia right now um but I've been deploying to ca Central 1 so I'm going to go and change my region here because if I'm deploying things in C Central 1 I want my bucket to be in C Central one and so I'll just change it right here by changing the region once my internet returns all right my internet is back and so or at least I thought it was is my internet still not here no no I have internet okay so is it just a the best today switch me to ca Central one please okay so I have to put my bucket name back into here um and we'll go ahead and create this bucket and um I'm going to have to remember this bucket name so going to go back here and just paste it in temporarily here just so I don't lose it not that it I can lose it that easily I'm going to go download this file or we can just upload it to the bucket directly that's probably a bit easier just because I'm already here so I'm going to go into nested Stacks here and uh just in case you don't know the command I'll just write it out for you first before we do this and um I'll grab this here copy paste S3 col SL we'll say a S3 copy and then the local file which is S3 bucket template I don't know if we actually have to name it template but that's what I name it as so hopefully that is fine we'll go ahead and paste that in down below hopefully it copies the follow over it looks like it has excellent and so I believe that we're just providing uh the URL to it um how that would work if if it like if it says template URL does it have to be a public facing uh template URL that's something that I'd like to know like how would that work um so I'll go here and search oh could we just provide the body well that would be nice too so structure containing the template body with a minimum length of Etc um so I mean if we could have done that we could just reference it the entire time location file containing the template body uh youur must point to a template so my thought here is how would it know or could it reference it here if it was there I don't think so unless it was publicly accessible I don't think it'd be able to do that but that's fine if it doesn't work we can uh make a change here but I'm going to go S3 bucket template because again it's not public facing and I'm going to go ahead and give this a deploy and see if that works I'm expecting this to fail because again I can't imagine how would access that bucket unless your account just naturally has access to it which would be nice I would hope that that would just be how it would work so we'll go to nested stack here uh we'll go to change sets we'll click through here see what it has says adding a stack cloudformation type um is there anything else here no no no no um but it is bringing in the input so that makes me think that it is grabbing it from S3 and so I would imagine that cloud just has access to your S3 buckets like I didn't give it permission to and so it's already rolled back which is totally fine we'll go to parameters events um domain name specified in ness stack template is not a valid S3 domain so um I guess it has to be in the format of an S3 domain which is fine we could go over here and change this over to being something like uh s3. Amazon aws.com for like that and so maybe that will fix that issue still I'm not expecting it to work but we will try this anyway we'll go ahead and deploy and we will try this here all right it might quickly fail and it's is it failed again so probably failed for the exact same reason the specifi URL must be an Amazon S3 URL well I guess we could go take a look in here and see what we have and well hold on because I made it I don't think I changed out the S S3 part so we really want this one we'll grab this again yeah yeah I have the the wrong thing here and so we'll try this again save and we'll deploy it again uh well we'll have to delete the old stack first we'll go ahead and delete that there we go we will try this again and we will execute the change set I'm expecting this to fail because it does not have permissions to access that file or it should time out after a while oh no it's working okay great so yeah I just wasn't sure if it could pull it out but it's interesting because the first time we did that how did it know oh we have the parameter right here I'm not even paying attention that's why I was like how does it know the parameters there when we didn't specify but I guess it's just in the template by accident um and refresh this and so it created the N attack so yeah that was really nice it's still not done uh when you create an esta Stacks or reference or referencing other stacks can always be a little bit of a delay which is kind of annoying um so but it gives you kind of an idea like it's the time it takes CL information to wait for stuff so even though this stack was done there's still like a period to which it waits until it updates the other one um so what we'll do is we'll go ahead and now tear this down as this is now complete um sometimes you have to tear these down in order you really don't want to fiddle with the um this stack you always want to fiddle with the top level one so we'll go ahead and delete this one and it should pull down both of them there we go and we can consider this done uh well before we do that let's clean up our bucket so we don't have a billion buckets so I'm going to go ahead and delete this just empty this bucket and we'll go back over to here and now I will delete this bucket there we go and the stack should deleted excellent and we'll see you in the next one okay ciao so you have this concept of roll backs in cloud formation and that's when you um let's say when you have a Creator update or destroy and your stack ENC counters an error uh this could be whether you have a cloud formation template that has a syntax error or your stack is trying to delete a resource that no longer exists that cloud information is going to attempt a roll back uh and the idea is to put the state back into its previous state roll backs are turned on by default you can ignore a roll back by using the ignore rollback flag VI the a CLI Ru backs can fail sometimes you have to investigate and change resource configurations or possibly uh uh need to reach out to pay support to resolve the failed rollback I've been a situation where I've had a roll back stuck in a r or a confirmation stuck in a rollback and I had to contact AWS and they had to Jank something uh behind the scenes but for the most part that's very rare and hasn't happened to me uh much as of late uh when a roll back is in progress you'll see roll back in progress when a roll back succeeds you'll see update roll back complete when a rollback fails you'll see update rollback failed I didn't put this in the slide but um a more modern feature of cloud formation is that if there's something in the stack that gets stuck you can uh and uh the thing times out when you're trying to delete a stack you can actually tell it to ignore it on deletion I'll probably talk about that in another slide but there you go let's talk about drift detection so first of all what is drift drift is when your Stack's actual configuration differs or has drifted from what cloud formation expects why does drift happen when developers start making manual ad hoc changes to the sack most common example is deleting resources instead of changing configuration by updating and relaunching the cloud information template cloud formation can detect a detect trip feature um tells you if resources are deleted or have been modified you can see those options with the view drift results or or detect drift uh when detecting drifts on a stack CL information does not detect drift on a nested stack that belongs to the stack instead you can initiate a drift detection operation directly on the nested stack and I think that's still true um but uh because I mean this slide's a bit old so I didn't check it with nested Stacks but I think that holds true but they never ask that on exam so I'm not super worried about it uh this is what it would look like if there was drift it would tell you there is drift so this is drifted it can be of one of the following states deleted modified not checked or in sync in sync is what you want to see so here's an example of uh some stuff here and you can see that we have deleted deleted in sync in sync modified I know the time stamp there says 2020 it's the future right now and I'm going to tell you the UI looks exactly the same I just didn't bother changing the the graphic because nothing has changed with this functionality uh I want to tell you drift detection for a TOS kind of sucks because I don't find it very useful I bet you could probably automate it so that uh you could do auto remediation and things like that but uh I personally have not found this feature uh to be as useful as drift detection like in terraform or other services but it's nice at least it's nice that Abus has this as a feature there you go hey this is Andrew Brown in this video I want to uh see if we can use drift detection so what we'll do in our uh cloud formation uh I'm just going to make a new one here called Drift detection and so I need something to deploy um I'm trying to think of something a little bit more complex than something simple so maybe what we can do is grab oh I'm trying to decide here maybe a load balancer load balcer is probably a good example so somewhere in here we've utilized load balancers elb down below here so I'm going to go ahead and grab these two and I'm going to go and paste it into drift detection and I'm just going to update this to be um drift detection drift detection I'll just put drift here okay and so I'm going to CD into this one here and we'll go ahead and deploy it of course if you need to swap out what you need to swap out I believe mine's already configured for for CS Central one uh in my default VPC so whatever you need to do swap these out but you swap out the the VPC the Ami the subnet subnet at this point you should know how to do that anyway so we're going to go ahead and deploy the stack I'm going to make my way over to here and I'm going to accept that stack and then we will wait for that to finish deploying and then we will see uh if we can detect drift all right so our ALB is deployed and so in here we should be able to do drift detection this is something that you'd do via the UI so it's not something programmatic but over here we should have drift detection up here so we can see view drift results and right now notice that nothing has changed so everything is fine and if we go to detect drift uh we can manually invoke it to ask if anything has changed and uh then we can view our drift results but nothing has changed as of yet so let's go ahead and Fiddle with it so like our ALB um so we'll go over to ec2 here and I'm going to go down to our load Bouncer and the thing is that I think it's only going to watch properties that have changed so if we don't have a property set here it's not going to know that it's changed at least I don't think so but I'm going to go over here and first what I want to do is go to our load balcer go to actions and see what we can edit edit load balcer tributes so it's like I like toggle maybe HTTP off or change the packet mode to strictest or this to preserve um and so I'm clearly changing things right but would it know that I've made any of those changes and that's the question here so let's see if we can detect any drift and notice that it is not detected any drift oh nope it has wait hold on nope it's all in sync so what I'm saying is like I don't think it can detect things that are not properties that you're not changing on here but let's go ahead and um change something that it will definitely notice and what we'll do is we will maybe change uh one of the HTP listeners so if we go over to here into our Target groups and we checkbox on this we go to targets um we just edit here edit edit the uh no sorry the listeners listeners that's what I want to change here we'll go to listeners here and I'm going to manage the rules and edit them maybe we can add a rule another Rule and we will I guess add a condition and we'll just say if host header is W or like hello I'm just putting anything in here. example.com it takes that confirm there we go next next uh we'll just say return fix response there's some stuff in there we oh man really making this hard of me next create and so now we have changed the listener rule so I'm assuming that because we've added a listener it should definitely see that change and so we'll click detect uh drift what if we uh click on a specific one and it doesn't seem like it's noticing anything yeah still says it's in sync so what is it going to take for it to know that something is there because we've added something something H so again I think it's properties that it has set in here so if we go and take a look here yeah we have our listener uh what if we can't change the port I think it defaults on 80 but we could try and I'll go back over to here and this one is set to 80 what if we change this to 81 well let us do that yeah it did okay great and so now I'm I'm thinking maybe then it will detect it so we'll try this again and it's still not detecting any drift that is frustrating okay um what I'm going to do is I'm just going to maybe take down the web server I didn't say this is a great service I just was saying that it can detect drift but let's go ahead and actually uh Delete or terminate our ec2 instance because it's not going to spin up another one there's no ASG involved in here so it shouldn't do that and so now that is gone and we'll go ahead and detect drift again and now it says that it's drifted okay great so what has drifted is the question give this a refresh here okay so it did notice the modification there it seems like it was a bit delayed but notice that it hasn't noticed that the web server is gone so we'll just wait a little while here maybe we're just being impatient all right so let's take a look here again and see what happens so um that web server is definitely gone but hey at least it detected something um but the point is that this one is definitely gone to detect dri for resource how does it not know that it's not there yeah well there we go I had to specific tell it which resource um say modified it's actually deleted so yeah I mean there you go that is a drift detection um not the best service but at least it kind of does something we'll go ahead and delete that uh and we'll call this a wrap up here say drift detection okay and uh we'll wait for that that to delete I'll be back here if it deletes or not just because we deleted the web server it might get confused so I just want to make sure that it deletes successfully here on video all right so that tear down without issues so I'll see you in the next one okay ciao change sets indicate what will be changed before executing deployment for a cloud formation stack and I use these a lot so you'll be very familiar with them a change that is not uh unique uh to cloud formation it is a component of I tools um the way they look is different for each one so uh the way it change set looks for cl information is not the same thing as terraform and and not the same thing as using something like uh Azure arm um by default a chain set will be created and automatically accepted without the chance to review so notice this cloud formation deploy call uh this will just execute the change set immediately and I will always have the slide called hyphen hyen no execute chain set so that I can force a manual trigger but there you go hey this is Andrew Brown from exam Pro and we are looking at Cloud development kit also known as cdk which uh is a way to write infrastructure as code using an imperative Paradigm with your favorite language so let's get into it all right so to understand cdk I want to talk about transpilers here for a moment so a transpiler turns one source code into another and so cdk transpiles into cloud formation templates so you know just a simple diagram uh we have cdk on the left and then that turns into cloud formation templates under the hood uh and so this is uh the difference between an imperative infrastructure and a declarative infrastructure so let's talk about these two differences so imperative is when you have something that's implicit uh you know what resources will be created uh in the end State and this allows for more flexib less you have less certainty because you don't know exactly what's going to be created you don't you don't have fully control or visibility on it but you generally know what is going to happen but you get to write less code and so an example of something being imperative is saying you know I want an ec2 instance but you go and fill in all the other details I just want to tell you that I want to have one I don't want to have to worry about everything else and so that is what cdk is it's imperative then when we were looking at declarative uh on the right hand side here it's explicit we know what resources will be created in the end State uh there's less flexibility we're very very certain of every single little thing that's going to happen uh but we have to write a lot more code and so a comparative example to uh imperative is I want an ec2 instance and I have to tell you exactly every detail of it and that is what cloud information is it's declarative by Nature so I said earlier you get to use your favorite language using cdk uh and so let's talk about some of the language support it has so cdk was first available only using typescript and then they eventually started releasing for other languages so we have node uh typescript which again is just node uh python Java and asp.net so that's what we have so far if you're wondering exactly what versions uh that is what it supports I'm still waiting for a Ruby version and hopefully um you know when you're watching this video a Ruby version becomes available uh but generally I think whatever language is um is supported by AWS generally is what we'll see so I would not be surprised if they do a PHP one and also a ruby one here uh but I don't think you'll get one in Powershell uh just I want to make a note about how up todate cdk is with cloud formation so the cdk API uh they may have yet to implement uh specific apis for uh adus resources that are available in cloud formation it's just because it takes time for them to write this stuff and they have a lot of language to support but it's my best guess that typescript would be the one that supports the most ads resources uh and then the other ones would follow behind I would think python would be next and then Java and then uh probably asp.net would be last but um just consider that in mind so that is one of the things you have to think about with cdk which is if you need full control of what cloud information uh offers you might have to just use CL formation templates uh so you do have to explore there and see what you can do let's take a look at pseudo parameters uh these are predefined variables by Adis cloudformation uh you do not have to declare them in your template and use them same way as you would uh utilize parameters using the ref uh which is the reference so here's an example of us using uh the pseudo parameter adus region in our outputs all pseudo parameters begin with ads uh colon colon so that's how you know that they are suit parameters we have one called no value notifications AR partition region stack ID stack name URL suffix account ID and the ones that I find I use a lot is account ID region um uh and stack name or stack ID so those are the ones that are pretty common often you will utilize these Pudo parameters to build out irns because you need the account ID in the region so you'll see this a lot uh but yeah there you go hey this is angrew brown in this video we're going to take a look at pseudo parameters uh for cloud formation these are pretty straightforward so it should not take too long for us to figure this out I'm going to go ahead and make a new folder here can call pseudo it's very hard to spell PS e u d o I'm not sure what kind of English word that is that's insane uh we're going to go ahead and grab something simple so I'm going to just keep grabbing from our stack back updates one because that one was very very simple I'll paste this in here and this will be um pseudo so this will be uh sorry pseudo Sue example and yeah I guess we'll create a bucket here I'll just put some extra numbers in here so that there's no conflict in case there's an older one I'm going to get rid of the ec2 instance uh well actually hold on depends on what we're doing because there's a few things we need um like maybe we want region um or some other stuff so I'm just trying to think of a use case for this uh one thing would be maybe naming um the bucket or the instance based on this so you're not going to do I'm going get I'm not going to do bucket this time I'm going to just make it an ec2 instance because I feel like we can do more with that one and in here we want to provide a name for E2 instance I'm going to go ahead I know it's like tags but uh I'm going to just see if we can set that directly I don't think it has a name property no it doesn't but uh we'll go down to tags and is it tags just key value so it is it's key value so we'll go over to here and we'll say tags I mean I guess we could just do this on uh a bucket instead if we're just going to do tags what's the point so I'm actually going to undo that sorry I'm all over the place um but what will'll do is work with this tag here and the idea is that there are specific um parameters we can use so we have AWS region is one and then we have AWS stack name and then we have adabs account number I think those are all pseudo parameters we can utilize if we're not sure we can go look up pseudo parameters pseudo parameters best what did I spell I hit the Caps Key by accento parameters CL formation so this should have a list of pseud parameters that we can utilize oh account ID okay so account ID and if we really want to we just I don't reference them this way so we just say ref that so just say region and then have our stack name and we could have our account ID and we'll see if that works here so we'll go ahead and CD into pseudo and we'll give this a go going to make my way over to cloud formation give this a refresh here we'll go into change sets we'll execute that change set and we will wait for that to work now often when we use Sito parameters we'll probably using them with a join or a sub often to um create an Arn uh but uh I think for this is we can get this simple because I'm sure in other labs we absolutely do that and I don't need to kind of just stage one for this example here so we'll just wait for this bucket to finish making all right so our sud parameter has created refresh this we'll click into it and notice that we now have our tag so here we have regions CA Central One account ID and Sack name so there you go that is how we can use pseudo parameters I'm going to go ahead and delete this stack and I'll see you in the next one okay ciao let's take a look at resource attributes I believe they're called resource attributes because these are additional attributes that you can add to Resource that's outside of of the scope of properties because normally when you specify a resource you'll have type and properties and those are the two attributes but these are the additional ones that uh have additional functionality the first here is the creation policy this prevents its status from reaching create complete until it CL information receives a specified number of success signals or timeout period is exceeded you're going to use the CFN signal to communicate with CFN we definitely do that in a lab you need ion permissions to send C uh signals to CFN then there's the deletion policy um this is when you want to uh say what's going to happen on deletion so we have delete retain or Snapshot not all of these are available for all resources but for databases you'll often see that snapshot feature to take a snapshot um or backup before it deletes um but yeah those are options there we have update policy so how to handle an update for things like Auto scaling groups elastic cache um I don't know why it says domain uh maybe like a domain name for refug 3 or lamb or Lambda Alias so in that example here we can see we have an update policy specifically for Autos scaling replacing update and what would happen with that Autos scaling group I'm not sure why the p is missing here I guess I just missed it but there's supposed to be a p right here so nobody tell me about that I just uh released another course and somebody like pointed to like four hours in the course and uh like I released it literally the next day and someone four hours in like there's a spelling mistake right here so I try to correct those I see them on screen here uh we have update replace policy so to retain or back up the existing physical instance of a resource when is replace during a stack update operation so very similar uh to that other one uh we have dependson so this is when you want to say which resource depends on the other because sometimes certain resources have to be created before the other ones or the cloud formation stack will fail uh CL information is pretty smart about figuring out the order um but not always and sometimes you will have to specify them or you can force the order if you want them to be in a specific order then we have metadata um and so metadata is uh the resource distribute metadata and this is where you can add additional information um you can use CFN get metadata to grab this information uh I didn't put it in the slide here but we when we talk about Cloud AIT or sorry yeah cloud formation init CFN AIT um that is a very special type of metadata that initializes these2 instances so you'll see some special use cases for metadata but for the most part you can just put whatever data you want and then programmatically pull it uh to work with uh your resources but there you go let's take a look at intrinsic functions and you use these in your templates to assign values to properties that are not available until runtime and the most commonly used intrinsic functions are the ref and get at you will see us use these often the at stands for attribute so ref Returns the value of the specified parameter or resource uh get a Returns the value of an attribute from a resource in the template so uh you'll see when we look up cloth formation there will be a reference section for what each resource will return for ref or what will uh what other parameters can be returned on get ATT uh for every single resource it can be different so some will return the r as the reference or the name of the resource or it's or its ID or something else um but yeah just understand there's a lot of variation there then we have condition function so we have nend equals if not or if you need to add um uh conditional code in your Cloud information I rarely ever have to touch these uh if you are smart about writing your Cloud information you should not have to use these very often we have Bas 64 this Returns the basic4 representation of an input string I think we do this in a few places but I never explicitly think about this or ever have to write it in so um sometimes it shows up but most of the times I don't have to do that we have cider so this returns an array of cider address blocks we have transform this is for a macro to perform custom processing on a part of a stack template we have get A's returns an array that uh that lists the availability zones for specified region we have import value this Returns the value of an output exported by another stack we have a few more we have joins we have select we have split we have Sub sub is when you're substituting variables uh finds and map which I rarely ever use but that's when you're using mappings but I I don't really like using mappings but I would say that these ones are commonly used like the join I find I use these a lot too join select split sub that's why I put them together here uh there are a few other ones that are newer and these required to add a transform in order to utilize them but we have for each and two Json string for each is a big deal because for the longest time this was not something that cloud formation could do and this is why people had to use things like uh cdk or terraform or palumi but this for each component um uh brings that special level of functionality over to cloud formation uh if you want to just continually use cloud formation but let's take a look at ref and get attribute because those are the two most common ones and we really should know how to utilize those so ref returns different things for for different resources you need to look up each resource in the stocks to see what it returns so this could be an r and resource name or physical ID so here's an example of us using it with parameters we can use it with resources but this is a parameter example where you were referencing it with the address uh VPC now notice that I have this exclamation mark this is a special shorthand uh to reference it normally you would just write uh this like in line you just put in ref ref and then colon in the value as if it's uh that but this shorthand allows you to uh have it as a single line so you'll see me using that often I don't know if I have like a separate slide to explain that but um uh you'll you'll pick it up and you'll see sometimes we use it sometimes we don't and we're forced uh forced in a situation where we can't use it so when you need a value for resource and you can't get it with ref then you're going to end up using get attribute um and I don't know why I wrote the r on here because it definitely isn't on there that must be a small mistake as well as up to here as well but I just used to always writing ATR that's probably why but uh get a tribute and again that one shouldn't be there allows you to access many different variables on a resource you'll need to check the it stocks to see uh what is available per resource each one has different things available sometimes you'll find a resource you like why don't they give me this information on a resource this is frustrating but it's what you get is what you get okay so very similar the idea is that here we're getting it from a resource the security group and we're getting its group ID because that's one we'll commonly end up using quite a bit but there you go hey this is Andrew Brown in this video I want to focus on intrinsic functions so and functions are uh uh functions that uh can assign value properties that are not available at runtime and so there's a few that we will uh end up utilizing some that I rarely ever use so let's go take a look here and make a new folder and this one will be called intrinsic if I can spell it i n t r i n SI i c I'm hoping I spelled that correctly I'm going to go grab our depends on example and bring this into our intrinsic uh code here and and I'm just going to change this to Intrinsic and we actually have been using intrinsic functions because it's very hard to uh use cloud information without um working with intrinsic functions so what we'll do is just go ahead and delete this here and there's a few intrinsic functions that we can uh utilize I'm just going to write out what I want to accomplish here so we have ref we have um uh get at we have sub we have split um there's other ones like select or join join is one I use quite a bit and then we have import value which we'll worry about later on uh so yeah those are pretty good ones there's other ones I don't usually use often which like B 64 which can uh return the B 64 representation of a string but anyway let's focus on these because I feel like these are the most common and so we have an to instance here and the idea is that uh probably the easiest way is to just use it on outputs so let's go ahead and use outputs here I just going to look up outputs uh reference confirmation so I don't always remember the Syntax for this I always have to like copy and paste it and so this is the general format for outputs and in here we can get value so just say value one here and the value here is going to be ref I'll just make this a single ec2 instance and then we'll paste that in here as such so we can reference the ec2 instance um let's go take a look at the documentation for ec2 instances so that we can try to use more intrinsic functions and if I go down to return values this is what will happen if we return a ref so this will bring back the instance ID so if I go over to here we can name this as in instance ID now we're using the shorthand here but like if you didn't use the shorthand you type in function and then you type in ref so you could do it this way you could do it that way you could also do it uh this way but I like to use the uh shorthand syntax because it is a lot nicer but you could in theory use uh this one here and sometimes you have to because you can't mix uh with this little Shand with the exclamation mark you can't mix it mix it with uh ones that are in line with each other so sometimes you'll see people working through those issues but anyway so this will be uh instance ID and then I'll go down here and call this instance ID we don't have to set an export on that one but I'm just going to because it's already there and now see if we can grab something with a get uh get ATT so maybe we want to grab its private IP address or public IP address or maybe the VPC we'll do the VPC here and so say VPC ID and then the idea is that if we want to access something here this is where we use the uh the get at I don't know I wrote just gtt that makes sense sense down there again if you want you could do this get ATT this will be our VPC ID uh then we have sub so sub allows us to sub substitute a value um let's go look that up there so sub CL formation okay so the way it works is we have a string variable name variable two um I mean that doesn't look really clear as an example I don't really like that let's go down below here take a look here so I mean that would be a good one so yeah let's just actually copy this one I like that we'll take that and so the idea here is that we have an r and so we have ads region ads account VPC and then we have this here and so maybe I would like to go grab um the VPC ID here and so we could try this I'm not sure if it'll work in here but we'll try it anyway we'll save my ec2 instance bpc ID okay and say bpc RN and I'll grab this here and place this here but sometimes I have like is uh troubles um when we have a statement inside of that so we might have to fiddle with that to make it work so now we have our sub then we might want to utilize split so we have that over here let's see if we can find an example so this is an example where it's splitting on The Hyphen where this might be a good example is you might want to split on the comma so you might get back a value of or you might be inputting a value of subnets so that might be something that you're passing in and then down below you might need to split them as you pass them into something else if that makes sense so I'm going to go ahead and just copy this just say splits and I'm going to go here and I guess we can change this so that we bring in subnets so we'll just say subnets I believe there is a special parameter for subnets so we'll just say parameter subnets CFN and yeah there should be something for subnets subnets yeah it's this like a list of subnets so we go here and paste this in and so here it's kind of expecting us to supply some subnets I'm going to try to default this and I think that um the way it works is you just give it a comma and then it knows to turn it into a list but um what I'll do here is I'll go this is actually for joining not splitting now that I'm thinking about it but uh what we'll do we'll have to do both of them anyway so it's not a big deal but we'll go over to VPC here and we'll go to VPC and I'll just select my VPC here and I want to grab this subnet here and I'll paste it in as such and then I'll grab this other one here and I'll paste it in as such it doesn't matter if they actually work because I'm not going to reference them anywhere in the code like if I make a mistake here I don't think it will matter and here it might not like the type valid value is not accepted as the value why not maybe I can go see an example that parameters so oh I have a common deiminated list parameter I didn't know that you can use this parameter to specify multiple value strings of a single parameter U but what does it treated as um that way you can you can use a single parameter instead of many different parameters to supply multiple values I mean that is fine but I was hoping for this one to work I still think it's working so I'm just going to leave it alone I just don't think it's being smart here missing property properties oh CU it's under resources this is not a parameter okay so I'll go up here for a second hold on parameters that's why I was getting confused and then I'll just grab this and place this up here and actually this isn't going to be really good for the splits it's going to be better for the joins sorry so we go here and join subnets joins subnets and the value here is going to be join and imagine it works very similarly here so this would be um join on the comma and then we would Supply our subnets because it already is a a list array whatever you want to call it subnets there I don't think I need the comma on this one here like that uh like this and so you can supply this way I'd probably prefer to do it inline like you could just do this instead and this is I think a lot nicer to do and then I'll just bring this down here and then fix indentation so this should in theory join that um our splits is fine like again I wanted to use the subnets as an example of splitting something but if this works that's totally fine um now the other question is like can we actually split an output because I'm not sure if it'll actually work on an output it might work if we're passing a value somewhere else so if this doesn't work we can just take this off here but I'm expecting this one would might fail here but yeah there are other functions so yeah to Jason string there's a few that I haven't seen before like to Jason string length I don't remember that being there before four each that seems like that's a newer one so it's nice to see that we have uh some newer ones here but let's go ahead and like this one I'm really surprised the four each because for the longest time you could not do anything like this and so you'd have to use cdk or use terraform for something of the equivalent so maybe that is interesting here you must use the itus language extension transform that's why I have heard of it because you probably have to include this yeah at the top that transform so that's kind of interesting but anyway let's go ahead and see if this actually works so LS intrinsic int there we go and we'll go ahead and deploy this the value field of every outputs member must evaluate to a string and so basically this one here is just not going to work there so we'll just take that out which is totally fine and we'll say deploy then it says the output section contains a duplicate export we'll just take out the exports we don't need them and we'll try this again we'll go over to here well did that fail no that worked okay great we'll refresh this and we will attempt this chain set and so now we will wait uh for this grade and hopefully this will work without issue okay all right looks like that works we'll go ahead and change the templates outputs and so for our joins you can see it joined our subnets that we passed in because it would have went in as a list and then we're joining them together we were able to reference uh that there and then this almost worked but yeah you can see like a mess up see it says get ATT my in of BC so that clearly is not working correctly and I ected that it might not work correctly so we'll have to change the way this works okay so this is where our sub gets a little bit weirder um with its weirder syntax and so we'll go back over to our function because we have to get it to work otherwise you know what's the point um but we'll go ahead and look up sub again CFN and the idea here is I think that what we can do yeah I think I think what it is is that we put a value here like our VPC ID right and then underneath this is our string right like this and then underneath we can be like VPC ID and that's where we will set uh let's got to check here uh yeah key and value okay great so then the ideas I think we can do is is take this here like this so starting to come back back to me here and we can paste it in as such and so I think that that will work now would we have to fix these two I'm not sure I think those are fine I don't think I'd have to remap those but I guess we can find out so let's go ahead and see if we can deploy that or if it does not like that syntax unresolved dependencies vbc ID in the outputs block of a template okay and I think the problem is that I have um this sub here and this sub is expecting let's see here yeah we have an array and then another one here H okay can we get an example of that or no h yeah they never really show good examples for this I find but I know it's like interesting like they have these other other syntaxes but they don't show it to you which is a bit frustrating um so let me think about this for a moment VPC ID is up here so we're going to try and do this we's say region and then maybe uh ref AB us region like this and then um we'll have it was account account ID and this will be ref adus account ID because I know this works it's just getting it to work is a bit finicky and so I think that that I mean that looks correct to me try this again so one or more sub intrinsic functions does not specify expected argument specify a string as the first argument uh and an optional second argument to specify a mapping of values to replace it in the string okay so maybe if we take these parts out maybe we'll just do that I'm going to try this region account ID it's a little bit easier to read bpc ID so we'll try this we'll say account ID region VPC ID and we'll see if it takes that instead functions do not specify expect the argument specify a string as a first argument uh and an optional second argument to specify the mapping and so I'm thinking the thing is that we just have to take those two out there and that might fix that issue there it says one and second right and the second one is a key value mapping and so that looks like that's a bit better we'll go ahead and refresh this and we'll execute this and we'll see what we get this time all right let's see if we were able to correct the output here um no still doesn't work okay what does it want because like if you read it it's very clear what it says and this is where it's kind of frustrating where they could have done a better job with the docs and showed an actual example of what they want but it says string VAR name VAR value if you're substituting only template parameters uh resource logical IDs don't specify a a variable map so sub variable map example can we get one for this this is where I actually have to go ask chat PT this is so silly or cloud formation and let's see if it can actually give us an example but you know we read it so it seems very clear um okay so it does use the dollar signs in it and it looks like we can mix it then why didn't it work so I'm going to go here and put the dollar signs back around as such and we'll try this again then deploy says it's creating it now okay and we'll give this a refresh here and so now we're getting what we want so there we go so I feel like that satisfies our intrinsic functions section here we could also extend it and try to use uh four each I can't imagine that' ever be on exam or anything like that that's a more complex use case so we will uh call that done there and we'll go ahead say intrinsic functions example excellent and I'll see you in the next one okay ciao so weight conditions are used in two cases to coordinate stack resource Creation with configuration actions that are external to stack creation to track the status of a configuration process so here's an example of us using uh weight conditions you'll notice that we have the weight handle and the weight condition uh the weight handle creates a presign URL which you must ping to the count number which is over here the amount that we want uh of signals to consider the weight complete and when I was doing the lab I totally was not aware of this so you'll see me stumble through it but we do learn a lot there so I didn't go back and shoot it again because I thought it was valuable in the state that I made it in um but that was the big missing piece was that we weren't using CFN signal and the fact that the weight handle is generating out a pre sign URL and what's interesting is I use Auto scaling groups here but um adus does not generally recommend to use it for um ASG and ec2 even though you can and most examples show it that way and that's how I ended up using it uh so way conditions definitely are useful but for what I don't know because I can only ever think about this particular example but we do go through and uh utilize this with a weight condition and a creation policy so uh we'll we'll do both of them so that we have our bases covered okay I generator allows you to scan existing resources in your A's account once an inventory of resources are found you can select resources to generate out a cloud formation template and this is a feature that I think a lot of os customers have wanted for years this is something that uh as customers have always had because the way they built their platform was that anything you create always has an arm template um it's not as smooth of a process as that but we have to be happy that we have this over uh at a bus I was initially confused in terms of how the service works I can't remember if I had to rerecord the lab because I was so confused uh in terms of its uh user interface I had too much confidence going in that this was going to work just like Microsoft Azure which it did not but the idea is that um you scan and then once you're done your scan then you go press uh press a button to create a template and select the resources uh which I thought it would do something else but anyway there you go hey this is Andrew Brown in this video I want to take a look at I generator which supposedly can scan your account and allow you to quickly create cloud formation templates uh from the things that it finds okay so just in case you have nothing your account I'm going to go over to ec2 and we'll launch up an ec2 instance and so I'll go launch an instance going go ahead and say my ec2 instance and I'm going to go ahead and launch this template I'm going to go proceed without a key pair I'm going to launch this and that's going to launch up um so we'll wait for that to provision and once that's provision we'll go over and then do a scan okay all right so our ec2 instance is running so we at least have something in our account I'm going to go ahead and start the scan and we'll wait it says this could take up to 10 minutes but uh you know it takes a few minutes and then we will proceed to the next step of creating a template okay all right so let's go ahead and create ourselves a template now um we'll just call this my new template we have a deletion policy retention policy I do not care about these I'm going to say delete here today and we'll go here and now what we can do is Select resources so understand that the resources are limited to whatever um if I didn't mention this before what you have on cloud control API so if Cloud control API does not support it then you're not going to be able to utilize those resources but let's go take a look and see if we can find an ec2 instance so ec2 instance instance here and we have a bunch here some that exist some do not um question is what one is this one so we go over here I 01681 I 01681 which is this one here and notice that it's saying that this one's not managed by St stack so this is clearly the one that we want some are managed by Stack so obviously that's why we can't import them we'll go ahead and hit next and we will hit next actually sorry go back I wasn't paying attention and so we have additional related resources um the question is like if we add these do you selting recommend resource uh uh uh can result in errors while importing resources in the stack templat so my question is like if we import these what happens if we delete these do these go away cuz here we have like a subnet and a VPC is it going to delete my default VPC if we delete the cloud information template but I'm going to go ahead here and take a look because we have a deletion policy and a replace policy right so go ahead and we will create that again this is a test account so I'm not afraid of doing that but we'll go ahead and see what happens and it's interesting that we can have cdk we have template resources template definition I do not like cdk but I can see that we have that as an option here and so now we have our new template um so the thing is that even though we have this does not necessarily mean that we've created a stack so let's go ahead and create a stack now I'm just going to tell you I don't know if it's going to manage these uh resources when I delete them if it will tear down those default ones so if you're uncomfortable with losing your default VPC don't proceed here just watch me do it it and see me work through the problems so now I'm creating this new stack I'll just say my new stack we go ahead and hit next and we'll hit next and we'll give it a moment to review the changes I'm assuming we're importing the stack there we go so here's all those resources notice they're all set to import we'll go ahead and hit next and we'll import those resources and we're going to spin that up okay and so the question will be really what will happen if we try to delete this stack that's what I want to find out so the import did not take that long and here we have all of our resources and now the question is what's going to happen if I tear this down and so I think this is where we should have had retention on some of these things because if we go to our template you know we can look at this and this says what will delete or update and so this is where I think we could open this up an application composer okay this might be just an easy way for us to edit this in line here and what I want to do is just go and tell to retain certain things because I don't want to get rid of my VPC so let's take a look here we have this just looks like a mess of stuff I can't even make sense of any of it um so we have the network interface so I want that to go away we have the ec2 instance so I want that to go away we have the subnet I don't want to delete that update replace policy um I take take this out of here I'm going to say retain and then uh we have our ECT volume attached I don't care about that we have our actual VPC I want to retain this I don't want to delete that and I want to retain this um also just in case we back to that subnet I'll add that back in here this one okay and I mean that's pretty much all I don't want to get rid of um I don't think that's the default VPC so that's totally fine I'm going to go ahead and update this template so to export your template you need to create an S3 bucket that's totally fine going to confirm that so we'll create that template and so now we are going to go hit next because now it's referencing that template and we'll say next and we'll go hit next I want to see the changes well here's the thing we didn't change anything yet so I'm going to go ahead and submit this because all we've done is we've changed the um we've changed what we want to happen to those those things so we're not actually changing anything we're just changing the retention policies on the VPC uh and the subnet so I don't want those to go away so now what I'm going to do is I'm going to go ahead and do this again so I'm going to go and I want to change this templat we'll go to template and we'll go back into application composer I'm now liking application composer for this specific use case but everything else I really don't care and so what I want to do is I want to get rid of these resources so I'm just going to do that by removing them from the template um actually no wait no no no that's not what I want to do I actually just want to go ahead I'm thinking I'm using terraform terraform the idea is when you remove resource it'll delete it I'm just going to go ahead and delete the stack all right and so we have one issue here it says two validation errors to satisfy must satisfy the regular expression what are you talking about just delete the stack give this another refresh here oh it is deleting it okay great and so I'm going to go to resources here and it's failed to delete the volume which is not a good indicator so I'm expecting this is going to fail because of the volume maybe the volume has some kind of setting on it but we'll wait for this to fail and then see what we need to do to get rid of it okay all right so we have a failure here let's take a look and see what's happening so of course it was not able to delete this one here we'll take a look and see what the exact problem is um currently attached uhhuh so so what do you need to do to get rid of that that is not a clear error okay so currently attached request ID that doesn't tell me anything so when you get errors that are very not clear what we'll do is go over to cloud trail and we'll see if cloud trail gives us any additional information sometimes it's good like that it'll give us more stuff so I think it was like detach volume that it failed on unable to detach the root volume from the instance okay um I'm going to take a guess here and what I'm going to do is I'm going to just stop the ec2 instance so that maybe it can detach it so if I go here well where's our instance now it's gone so it is terminated but it wasn't able to detach it but it's gone so what I'm going to do is I'm going to try to delete it again and I'm going to try to delete it because if the volume's gone it shouldn't have an issue here worst case we can try to skip it next time but I would like to try to get rid of it okay so we'll see what happens all right and I was able to delete the stack so there we go yeah so again just having lots of confidence and just looking out for those things I guess that retention thing is something you'll have to watch out for I did not notice as we were creating that an option to tell it what we want to um specify for we still have the template there but I'm just curious like if we go back through that again yeah it's for everything or nothing and so you really would have to go in and tweak your template uh before you launch your stack um so just curious if we go over to here to this template um it doesn't show it here but I guess yeah import edited template well that mean we can just replace this one so I guess we could probably override this particular template here but yeah I guess you could copy it bring it in application composer save it to NE bucket go back here and import it I'm surprised they don't have Integrations for this part yeah that is I generator there you go Cloud information helper scripts provide um Python scripts that make it easy to uh perform very specific operations uh to extend the functionality of cloud formation uh and the way you would install that is that you would use the ads CFN bootstrap package but if you're using Amazon 2 or Amazon 2023 they will already be preinstalled and so um I've never had to install them um outside because I usually use the Amazon links distributions um or if the use case that I'm using these I usually don't use them uh in other distributions but anyway we have CF CFN init CFN signal CFN get metadata and CFN hup uh I'm not sure why it's called hup I'm sure somebody knows why but CFN andit is like an alternate way of configuring uh your ec2 instances instead of using user data um I imagine that it probably leverages cloud in it because I kind of remember cloud it having this kind of structure and then maybe CFN CFN interet is just a wrapper around that CFN signal we utilize this when we do creation policies weight weight condition in fact we probably have already done it uh if this comes later in the course so uh we'll know what that is for uh CFN hup is a very interesting one because it is it is um utilized to see if any instance metadata changes and if any instance metadata changes then it will trigger a hook of whatever you want to happen but we'll cover that in separate slides okay CFN a nit helper script allows you to install packages create files start services and more for your easy2 instance and the way it works is in your user data for your easy to instance you'll end up calling CFN AIT if you're using Amazon 2023 it's already preinstalled um but the idea is that you usually will call it config set so here I have a config set called config and actually it's spelled wrong because there shouldn't be two eyes in there but you can see on the right hand side we have config so that is my config set so hopefully you understand the relationship there and um yeah you can see we can do packages groups users sources files commands Services now I think that this comes from cloud in it um and if we were to go look that up I think that's where it comes from because I remember configuring things with cloud andit and this being very similar uh CFN andit provide uh provides a more consistent way to install and configure ec2 instances so I think that's the only reason why I can think of why you use this over a Bas script or it might have some things to do with like rollbacks or or uh something like that I can't remember why but the point is is that if you can use it use it but generally I just use Z data because I find it easier to be honest but there you go cloudformation hup watches for instense metadata for your ec2 incense and if any occurs it will trigger a custom hook the use cases for hup would be something like Dynamic updates so apply configuration changes to E2 instances without manual intervention CLA information integration so seamlessly integrate CL information for automated infrastructure automated remediation or just custom hooks in general uh or simplifying maintenance uh when uh things happen uh to use cloud or sorry cloud formation hup you need to use cloud information and nit to configure uh the configuration file for CFN hop which we'll see here in a moment you can troubleshoot uh if things go wrong by looking at your CFN hup logs and it checks every 50 minutes so if you have a hook and you change inance metadata you're going to have to wait 50 minutes for something to happen let's take a look at its implementation so uh there's two parts to this the first is in our user data we need to uh call CFN andit and we're going to have to um uh configure configure it now notice in the uh CFN section we would specifically specify a config set there can be a default one so we don't necessarily have to specify one if there's only one then down below we call the CF CFN hup Damon so it's running and it's talking to AWS uh for the metadata side of it where we initialize uh Cloud information it notice we create a file called CFN hup where we uh specify some configuration and then we can Define our hooks so notice that there's a folder called hooks. D and you can put as many as you want in there and if you look very carefully here you can see that we have an action and it's calling this upload to S3 uh sh so the idea is that there could be another file here you could use you create that in a cloud uh in the cloud formation init to do that this thing is so hard to get to work and I know I've had this working in the past but for whatever reason uh I just could not get a lab uh to work with this um and I don't really like this because it takes 15 minutes for it to check every time and it's almost impossible to debug if it if it's not working so I don't know um so I'm kind of torn whether I'll do a lab on this even though you will see in the a examples repo that I have code for it but for whatever reason it just refuses to work and I'm not going to bug support on this one but anyway that's CFN hop okay in cloud formation you can create custom resources which allows you to extend the functionality of cloud with your own defined resources this is useful when you want to manage external infrastructure using uh Cloud information and uh this the the simplest way to look at this is that you will have this thing that says custom my custom resource and you have this thing called a service token and the service token is either going to be um it's either going to be aess Lambda function or it's going to be an SNS topic and the idea is that you're going to pass on requests of a very particular format that are either create update or delete that are coming from cloud information and your compute has to be able to handle these um so uh if you actually look in the examples repo you can see that I set up an example but then I realized that this is way out of scope and really uh difficult to do I could have done a lab on it but I just don't see anybody uh utilizing it because there is a newer way of creating resources which is the cloud formation registry even that is really complex but just in case the exam ever asks you a question about it I just want you to know whether it is um despite that we're not going to do a lab on it okay Cloud information registry lets you manage Cloud information extensions both public and private such as resources modules hooks that are available for use in your A's account and this thing is a better version of cloud resource or custom resources because it has the following benefits it supports modeling provisioning and managing of thirdparty application resources it supports create read update delete and list so those are cred L operations it supports drift detection on private and third party resource types there's no need to associate an SNS topic or Lambda function to perform these cred L operations and we have public extensions so these this is just kind of like the public and private stuff but public extensions there's ones from a2s and one from third parties then for private extensions you have activated private extensions this is just if you are utilizing third party extensions locally or registered private extensions these are your uh extensions that you are creating in order to uh scaffold these extensions you can use uh CFN CLI I just want to tell you that this stuff is very involved very programmer heavy and um I definitely could do a lab on this but I think it'd be way out of scope um so unless I see on the professionals I'm just not going to uh do this um but I just want you to know what cloudformation registry is and that it's a better version of custom resources okay hey this is Andrew Brown in this video we're going to uh learn about cloud formation now we use cloud information a lot in uh my courses but I figured we'll just spend more time learning the specifics about cloud formation so I'm going to go ahead and open this up in git pod you can use whatever you like uh could have code spaces Cloud9 your local developer environment but this is already configured to have the a CLI installed and credentials in which I show that somewhere in the course how to do that if youve never done that before but we'll get this started up here I'm going to make a new folder in here called Cloud information uh and the idea is that we will uh step through the basics here so I'm going to go ahead and type in clear here we'll just say uh make di uh CFN for cloud formation and I'll CD into that and I'll make a new directory here and I'll call this one basic and so we want to um make a new file here I'm just going to say mkd or we'll just CD into that directory basic I'm going to touch a file and first make one that's in Json so modern Cloud information always uses yaml but I figured it would be fun to use Json because that is the original way to do it um so the idea here is that we'll need a few things the first thing is we'll need our um version so we usually type in ads uh template format version here and by the way if you have installed um I think the adabs adabs tools it will make Cloud information a lot easier to write uh if it detects it that it's a a file right now it's a Json file so I don't think it knows that it's that but generally what we'll do is set this to be 2010 0909 I've been writing lots of clir recently so my memory is really good about this right now and we'll call this uh we'll give this a description so I'll just say description and this one is going to be called uh Json example all right and then inside of that we'll need our parameters param meters and I guess what we could do is create like maybe a bucket maybe the whole point is is with sample bucket all right and so in here I'm going to go ahead and call it bucket name and inside of this we'll need to specify our type but you can see how writing Json would not be fun I know some people that still write Json because that's what they started with and they're just like why why change I already like Json I'm already good at this and I suppose that's fine but uh you know you do you I'll do me we'll go ahead and we'll put a default value in here I'm going to call this um uh Json example CFN and then put a bunch of numbers here on the end yours has to be unique for mine so just make sure if that doesn't work you'll end up changing that uh so we have that our parameters let's go ahead and do our resources but why would you ever want Json even if theal is available now well if you are programmatically working with some kind of service it might put Json so it is still good that we have um that option there um so that is something that I would say that we'd want to have here then we'll want the type I believe that this would be what ads colon colon S3 colon colon bucket that's my guess uh we probably have to type in typew right for this to work otherwise it's not going to work and then I will want the name in here I think it's bucket name and in order to uh pass this we'll need to reference it and so to reference it I'll need to use a reference function so I'll have to have brackets here and then we'll say ref and this is going to ref bucket name so it's going to reference this up here and it still doesn't like this say it's colon expected yeah between here and so this should give us a simple bucket um so now that we have that the question is how would we work with this so we can of course uh I'm make sure this is our Basics directory it is yeah we could um of course use the CLI and we should use the CLI but what I'm going to do is I'm going to just Pro uh use the UI here and see what we can do because I want to see if we actually allowed to upload Json directly um so here I'm in here I'm in C Central 1 you can make this wherever you want but I want to do this in C Central one and we have some options here choose an existing template and we can upload a file and so it says Json oramel so it should be able to take this file so what I'm going to do is download this template to my local computer you download it to wherever you want to download it to and I'm going to upload it so I'm just going to go and click that and select it and I have done so here so we have our template. Json and we'll go ahead and hit next and notice it says invalid Resource Property bucket name so the problem is is that uh this is not correct so how would we uh go look this up I'm going to go ahead and grab that type and we'll go over to Cloud information here it'll tell us what the parameters are and so if I got it wrong it says uh bucket name there but you know what's wrong is that I was supposed to wrap this in a properties so that is our our problem here we're supposed to have properties here whoops uh properties yeah allow me to paste properties thank you and that's probably what uh my problem was here okay and so that's probably going to resolve the issue I'm going to go ahead and download this template again and we will go over to uh wherever this is so let me just go back over to here and I will go ahead upload that now no it says template one that's how you know it's a near one we'll go ahead and hit next and now it has an issue resource object must contain a type member well I thought it did um so looking at this you know what it is the type has to be on the outside so this goes here and so now this is the correct file we'll download it again but you can see why you would not necessarily want to uh direct upload like this because it's such a pain I just selected the second template but it's nice if you don't want to utilize anything else also adus template format version is is incorrect I probably just spelled it wrong I did template format version and we will download this again again this is just for demonstration to show you how this works there we go and so we have a template Json there I'll just say uh my template Json example and notice that we have the parameter we could overwrite it if we' like to there that would be a great opportunity for you to overwrite it we have tags we have permissions uh what would happen in a stack failure options a bunch of other things down here I rarely ever look at these things or change these things just go down and I go ahead and create it so if I don't do it you don't have to really do it either we'll go ahead and see if this works so we'll just wait here all right so our bucket is created so you can see that we can utilize Json now if we go over to template uh it's showing it in the Json format as that is the means to which we uploaded it we can see our parameter here we have no outputs um but it's very straightforward so let's go ahead and delete this and I just want to show you that you could take this right you could take this and we can convert this over into uh Jon from Json to yaml so just type it Json to yaml use whatever site you want here just be careful because sometimes they throw ads at you and stuff but if we do that we basically get the equivalent in in yaml and this should just work uh if you do not understand uh or worked with the ammo before it basically is Json compatible um so the idea is that uh they translate over very well between the two um but anyway so this should we're complain this this should be a string but that's just more like the fact that it just it would just not error out like this but that is not an actual stastical error so you can just kind of ignore that um it's just the the yaml language there let's go ahead and see if this one works I'm going to go ahead and download this file and assuming that our stack has been deleted which it has excellent we'll go back over um to cloud formation here and we'll create another stack and then we'll upload the yaml file all right we'll go ahead and hit next say my template yaml and we'll go next and hit next and submit and now we'll wait for this to create okay so that template works I no so we go over to here it shows it in the yaml format which makes sense um so I would say that is pretty straightforward I'm going to go ahead and delete this file out of here and so that is pretty straightforward there now of course you can upload your file St 3 and reference them as you create the stacks uh normally if you use the CLI tools they might do that intermediately it just depends on how you configure them but I would say that um I mean this satisfies this part but yeah we will look at uh in other videos um uh the more specifics of the of clation so I I would say we satisfied this very basic example here I just again wanted to to show you could use Json okay um so I'll just say CFN basic all right and I will see you in the next one okay ciao so let's talk about preventing uh stack updates because it's possible in certain circumstances you don't want there for an instance to be replaced because let's say you had an RDS database um and it would the action that would be Tak that would replace the database you would that would result in data loss so you'd say no I don't want that updated or it could be that you have a critical application and there's certain uc2 instances that cannot be interrupted so what you can do is create a stack policy and specifically say that you know you're not allowed to do an update replace on this dyam DB table um and then all other actions are allowed so that's a a great way to just make sure that critical resources are not affected uh by these stack updates hey this is Andrew Brown and we're continuing on with cloud formation and we did Stack updates but I want to do uh one which will be for prevent stack updates so the idea is that we can add a a stack policy that will prevent us from doing stack updates and my question really is is where would that command be because there is one called set stack policy if you've already created a stack but imagine you're deploying for the first time could we do that uh with the deploy command so that's what I'm really curious about I know it's with the create stack command um but I'm just going to see here if it has it and I do not see one uh please if I'm not typing it right I do not see one there so I'm thinking that if we use the deploy command then we will have to um uh set the stack policy after the fact that it's been created so what we'll do is we'll go over to here um and we will go and copy let's say our stack updates files okay I'll copy this in here I'm going to bring this into the prevent stack update we'll say allow yes can paste those in there and this will be our prevent stack update and I'll go to this template and we won't have an S3 bucket we'll just have a u this here and we learned in our previous in our previous video we couldn't do T4 micros t4g micros in the ca Central One region and that's totally fine we can just switch this to something else here but what I'm going to do is I want to go ahead and deploy the stack so I'm going to CD into the cloud formation prevent Stacks uh one here and do a deploy and we'll go over to here give us a refresh and I'm going to go ahead and let that execute go to change sets and execute that change set now the next thing I want to do is uh while that is deploying I'm going to create a read me file here and we're going to uh get an example of what we need to do to do to set a stack policy so I'm going to grab this link here for you so you don't have to look for it and then we'll go down to examples and I think it should be as simple as this now we can make our lives a little bit easier by just um copying this and doing this in line so go back to our read me here uh where's the read me did I not just uh did I not just place it in our stack oh you know what I have two tabs open here that's why I'm getting confused um but anyway so you know we can uh reference a file but I'm just going to do it in line if you do a single quotation we can just do that this way and the idea is that I want to create a statement that is going to um deny an update for specific things and I guess I'm thinking here is that I'm just thinking about this for a moment here um is that I want to be very specific about uh what resource it is so this is logical resource ID bucket I guess we just say on all resources that might make our life a little bit easier but I was trying to think like okay how could we set it for a specific one um it looks like it wants the logical resource ID and then um SL what the type is but I'm not sure what we would do this for if it was an E2 instance maybe it is that that's what this is this is the prevents updates on to the production database resource so I think what we can do here is I can go over here and grab this here and then place this here like this okay and then bring this up on here and so now this should prevent this from getting updated I got to make that a single quotation I'm such in a habit of making double uh back ticks which look very similar but I think that in theory that this should work also probably help if I name this as an MD and not Mi file because I don't know what an M file is there we go and so in theory this should prevent us from doing a stack update now I wonder what other actions we can set here I don't know if it would show it here it does not if we go over to here what other ones could I set here and here are our options so we have update modify update replace update delete or all of update and so this one is capturing all of these cases okay um so if we go back over to here and refresh this is deployed so that is in good shape and so now what I want to do is I want to update the stack and we'll need the stack name so I'm going to go over to here and get our stack name I believe it's just prevent stack update yeah it is okay we'll grab this here and we'll go ahead and paste this in and we'll copy this now before we uh place this anywhere I'm just curious if it would show us this information anywhere so I'm just carefully looking down below here we have stack policy so notice we have no stack policy applied I'm going to go ahead and give this a go and hit enter hopefully this works uh invalid stack policy so it doesn't not like something about our stack policy um I think we might be missing a bracket here because I see these two here but I don't see this one so let's try that again that's just my hunch there we go and so now we go back over to our stack we'll give this a refresh and so now we have this stack policy in place and so the idea is that if we change anything uh this should fail I'm I'm hoping that it will and so I'm going to just change this T2 micro to a T2 small or if you're not comfortable doing that don't do it you can just watch me do this but we'll go ahead and uh we'll try to do this deploy here and see what happens so let trying to create the change set and it's created the change set we'll go over to here to our change sets and we'll click into here and notice it says replacement conditional it definitely is going to update this I I can't imagine it wouldn't and we'll go ahead and give this a go and so I'm expecting this to not work and so if we go and take a look at what is happening here it says the following resources fail to update action denied by the stack policy so there you go that is a way that we can uh prevent the stack from being updated now the next question is what would happen if we go ahead and delete the stack manually can we do that we'll allow that from here we're not necessarily updating it all right and so we'll see if that succeeds and there you go it's successfully deleted so it really is when we're just trying to perform an update and what kind of update action it could perform there so I I'll say that this one is done here we'll just say prevent stack update and I will see you in the next one okay ciao hey this is Angie Brown this video I want to show you dependson uh which allows you to uh tell CL formation basically the ordering to which it wants you to create your uh resources generally it's very smart enough to figure that out on its own but there are some cases where you would have to manually set it so I'm going to make a new folder here called depends I'm going to grab code from let's say our stack updates here I'm going to copy this and we'll go over to depends and we'll paste it in here and the idea is that I kind of want to force the order we only have a couple resources here I'm just going to uh take this out here and what I'm going to do is I'm going to have three ec2 instances I'll call this one one two and then three we go over here back to dependson and we'll just say uh depends or depends on stack and the idea is that I'm going to tell it explicitly which one it depends on so the idea is that if I want to force this particular order let's say I want one so say depends on this one three and then this one here depends on uh so let's see if if one depends on three then three must be created first right and so that means three will get cued first but then if if um two depends on or three depends on two then two will get created first and so our order here we're expecting is uh description create in the order of two three and then one okay so that is what I'm expecting for the creation order to be so we'll CD into our depends directory and we'll go ahead and deploy this and hopefully we can observe this the only thing is that I think that that the dependson um actually goes here because notice it's giving us um an issue with the being under properties so I'm going to try this one more time I'm going to go over to to the stack here and just delete this one this one's wrong and we'll go back over to here and attempt a deploy again here I'm going to make sure this is correct by looking at the template it's not showing us the template I'll execute it and I'll check the template now so this one has it depends on in the correct location so what we're looking for here is to see the order in which it deploys in but it already has a problem which seems like it's unrelated to what we're doing here so the architecture arm 64 of specified instance type does not match x86 64 specified Ami okay oh it says T4 mic t4g here okay so T T3 T3 and T3 sorry and I'll go here and delete this we'll wait for this to vanish any day now should not take that long I'll wait for it to delete okay all right so that's deleted we'll go ahead and hit up and we refresh this and I'm going to go ahead and execute the change set and so now what I'm looking for is to see the order to which it creates so notice that it's starting with two okay so if we go back to here we said two would be first and now it is working on three and so then last would be one so you can tell that our order is taking effect and that is working so yeah I mean I guess what we could do just to to make sure that it's not random what we'll do is wait for this to create and then once this done we'll tear it down and we'll change the order all right so that is done creating I'm going to go ahead and delete this stack while that's deleting I'm going to just add one more We'll add a fourth one here as I just want to really make sure that this isn't a fluke okay and so we're going to specify the order to be depends on we'll just take the dependon out of these here for a second and so so one depends on we'll say one depends on two and then two depends on three and then we hold on here so one depends on two so two will have to well hold on the logic can get a little bit complicated here let's start from the bottom so we'll say uh four depends on three happening first and then three depends on two happening first and then two depends on one and so basically we're now forcing the order to be 1 2 3 4 1 2 like in the sequential that it's supposed to be so what we'll do is go back over to here that's going to take some time so what I'm going to do as that one's tearing down I just want to get this going here so I'm going just call this two here as I can just do that I'm going to go ahead and deploy that and we'll go back over to here and then I'm going to go to this one execute this change set and so what I'm expecting to see here is this to happen in the order so we have one happening first excellent and so we'll see expect to see 2 3 4 I'm just going to pause here so we can speed this up all right and we can see our order is 1 two 3 4 so it's the exact order that we expected uh notice that uh I guess it's all completely done here so we're in good shape I'm going to go ahead and delete this stack because I don't need it to do anything um and I will consider this done so we'll go ahead here and just save this as depends on and I'll see you in the next one okay ciao hey this is Andrew Brown in this video we're going to take a look at creation policy so we did wake uh weight condition which um supposedly adus does not recommend to use the the uh the weight condition uh specifically with autoscaling groups um but we did utilize it for that because it just was the best example to show that here but now we go ahead and create a creation policy which I would imagine is more streamlined we still have to use um some way to notify it so we're are going to be obviously using the CFN signal uh which shouldn't be too difficult here but let's go into here and we have weight conditions I'm going to make a new folder here and we'll call this um creation policy and I'm going to go ahead and just copy contents of these ones I only need the I don't need to read me right now but we'll copy these and will yeah will allow us to uh paste it's a little bit upset here it's still thinking but I'm going to go ahead and we'll just rename this to be I'm waiting for my Vim Vim keys to kick in here creation policy and I mean this is all configured so this is all in good shape but what we'll do is we will look up creation policy and just change it to whatever it needs to be so uh let's take a look here so we need to have C attribute with the resource prevented status use the CFN CFN signal which we know we want to uh work with autoscaling groups so we'll go down below um does it have it directly on here it doesn't show it so I guess the thing is is that we just I guess add the attribute to where CU it's not specifically specifying it um so I'm just not sure if it goes under the auto scaling group or if it goes uh Loosely in our templates just give me a moment to figure that out so what I'm thinking is that it probably goes right under the Autos scaling group because I'm looking at this and I'm not seeing um something referencing it so that makes me uh think that the way it's going to work is we're going to go into our template here and we're going to place it under properties uh unless it goes under the object here because sometimes policies do end up over here not not necessarily under properties and I still don't know why my Vim keys are not here what is going on where is my Vim I can't work without Vim just give me a moment to uh figure this out there we go my Vim is back and so I'm thinking what we'll do is indent it here we'll get rid of the weight handle the weight condition and then what we'll do here is we have minimum instance uh percent uh I suppose that we want that to be 100% uh we want Count maybe signal resources it would be two timeout could be 300 as that's what we had it before and so hopefully that is what they want for that but the question is how do we use the CFN signal so that's the next question and I guess the other thing is that this probably doesn't take a percentage it's an integer so we'll go back over to here and we'll change this out uh to be that okay but what about the actual CFN signal so we'll look at that up CFN signal and it's not going to have uh this weight handle here so we'll just take that out of there and there was resource so I'm imagining what we're doing here is we're placing um the autoscaling group resource here so we'll try this and so to me this is what would logically work uh so let's go ahead and give that a go and see if that works here so we will go into CFN creation policy and we'll go ahead and see if we get any syntax errors unresolved resource dependency weight handle uh because we don't have that anymore so we'll just take that out try to run this again and it is so far working if works the first time I'd be very happy but if it doesn't that's totally fine so we'll go ahead and execute that and we will wait and see the results okay and we have a rollback so clearly there is something wrong we'll go ahead and take a look here um timeout must be specified in ISO 8601 format oh that's interesting so we go over to here timeout yeah it says that the length of the time Cloud information waits for the number of signals that was specified in the count property I don't even know what you put for that that's uh that's really silly can we get an example be like p5m what does that mean 5 minutes okay uh fine whatever I don't want to wait 15 minutes it seems really long um 10 minutes maybe so we'll go back over to here delete this and we'll go back over to here and we will attempt to deploy again assuming this one was deleted still not deleted I think maybe we should ask chat GPT for this one because there doesn't seem to be a whole lot of documentation around this so creation policy example uh policy example for uh cloud formation for an ASG the user data should call um CFN signal and maybe this will help us rule out if I'm placing anything in the wrong location just so that we're not wasting a ton of time and this thing also is having a very hard time tearing down sometimes uh it has to do with security groups but it's just taking time uh to tear down here let's go take a look and see what it thinks the answer is all right and so we can see the creation policy is actually on the exterior so that where I placed it um so that was a pretty good guess for me this is using a launch configuration we don't use launch configurations anymore we use launch templates and here they're referencing the autoscaling group so it looks like they're doing exactly what I'm doing I'm noticing that they have uh some additional steps here um I don't think we need these I think it's because it doesn't realize that we're running an Amazon Linux 2 or 2023 and so whatever needs to be here is probably already preinstalled so I'm just curious we go take a look at what this is here uh the CL information Alber scripts are preinstalled on Amazon Linux images on previous Amis uh the CFM bootstrap had to be used so it looks like our implementation is correct um they have for five and did we change it to five I changed it to 10 we can even make it lower and make it five I'm not exactly sure what it should be but we will set it to that I set it as two because I'm assuming um that for it to work we'll need to have um two instances of the desired and so there would be two counts and I don't know if we have to have this here it' be interesting to see if we could just admit that to keep our policy nice and simple and this thing is not required so specify the number of instances that must uh signal success before the setting group we can leave that alone I think it's totally fine and now that it's deleted and so let's go over here and attempt our deploy so we'll give this a refresh and I'll go over to our change sets we'll execute this and we will hopefully have this work the first time all right and it looks like it worked first try so that was really easy and straightforward um definitely easier than the weight condition not that the weight condition was too difficult but uh for whatever reason um it seems like creation policy is recommended as there is no additional component to it you know I don't know anyway we'll call this creation policy and we will consider this done and I will see you in the next one as soon as I tear down this infrastructure uh here yep and ciao hey this is Andrew Brown in this video I want to show you how to cross reference stacks and this is something I actually use quite a bit um because when you are creating large infrastructure you will have to uh Define where your boundaries are going to be for those cloud formation Stacks you naturally figure out where those boundaries are working with those services and understanding what you would want to peel back uh from your I onion if you will um but uh we're probably not going to get into a really good example I'm just going to show you how cross referencing Stacks work so we'll go ahead and open this up um and so I'm opening this up in git pod as I want to uh be able to use the a CLI of course as always use Cloud9 uh code spaces local developer environment but understand that I already have my stuff configured with a my credentials and you will have to do the same so I'm looking for our cloud formation folder which I believe is called CFN I'm going to make a new folder in here and this will be for cross reference stacks and um I need a simple one to start with so I'm just trying to think where I might have a simple one and maybe under stack updates we have a simple one yeah this one's really simple so I'm going to go ahead and copy this and we'll say allow and we'll go into the cross reference STX I'll paste this in here and what I'm going to want to do is I'm going to make a folder called bin and then I'm going to make a folder called uh CFN and this is actually how I organize my um when I have multiple scripts this is how I like to do it so you can do it however you want but this is how I'm doing it so IDE is I'm going to have our main template and I'll make a uh uh another one called um secondary yaml sometimes youd name this after what it is so maybe this could be like a bucket if we if we would and then the idea is that we have our one deploy script I was thinking maybe I might have to have multiple deploy scripts I'm thinking if we have cross reference stack yeah we're going to have to have more than one so we have deploy uh main so I'm just going to rename this to Main and then I'm going to copy this I'm going to paste it again hopefully it'll let me do that and then this one's going to be um bucket so the idea is that we'll have uh two separate deployment scripts so go up here and this one's going to be called cross stack main whoops cross stack Main and this will be cross stack bucket and I want these to reference their uh respected ones so I think if we do CFN template as long as we're executing this at this folder level I think that it will know how to reference it correctly we don't have to do anything crazy here so this will be CFN template for the main and this one will be CFN uh bucket there we go and so I think that will um suffice so I'll go down here and the idea is that we have our main template so we could say this one is an ec2 instance and the other one is a bucket so I'm going to just copy this file here unless these are already duplicated they're not just going to copy this one here and we'll paste this in here I'm going to take out the bucket as such and then I'm going to take out the bucket name here as well for now and then this one will only have the bucket all right so now the next thing we need to do is we're going to need to um have outputs because the idea is that when you want to reference uh across things you need to have the stack name so I'm just trying to think about this for a moment so the idea here is we have a bucket and then we want to deploy it and then we want to know what that stack is so I'm going to go here and say parameters and then we'll just say bucket stack name and this will be type string and then the idea is that this will be whatever we called this one so this one is called um cross stack bucket so this will be called cross stack bucket because we already know that it's going to be called that all right so that's that part there but we're also going to I'm just trying to think here um I think that's fine but uh yeah just give me a second here so I have um uh these are a little bit older but they're like three years old but the point is is that they these are where I was using a lots of cross reference stacks and so I'm just going to go in here and see if I can find some here so I don't have to like read up on it so you already see there's like template extend that might be one that is there and so what I'm looking for in my templates here is see where I have bootstrap stack so if I go down below here um uh I'm just again checking for this here I was hoping as I was hoping that as soon as I started working with this it would just start coming back to me immediately but it's not um materializing instantly here uh cuz I'm seeing also here um a nested stack but the idea is that I want to uh reference things and I I believe that we have to use import value for that to work so that's what I'm looking for is import value as an example oh here's one okay great and so the idea is that imagine that we want to bring in a value then we would um import it and then specify this way so this is the code that I'm looking for in particular so I'm going to go ahead and just grab this and um I'm going to go back over to here and the idea is that uh we'll paste this in here as such and instead of calling this you know bucket buet stack name I'll just say bucket stack and then down below here we would have to put in bucket stack and then whatever the exported value out is and so this could be something like name or say bucket name okay um and so in here we don't really have a practical example but let's say we want to do tags and we'll have a key here we'll just say ass Associated bucket and value um and then we could go here and import that value here okay so the idea is that um if we want to import this then we have to have uh an export so in our outputs here we'll do outputs and I'm not going to supply the bucket n I'm just going to let it randomly generate here and I'm going to look for an outputs example okay we'll grab this whole one here paste that in here and so this will just be bucket name and then this will be bucket name and we will reference this as such so we just say bucket S3 bucket I believe it Returns the name if it doesn't that's not a big deal and then this is our export so this is where we want to specify um Stacks I'm going to give it a sub and we are going to use the pseudo parameter stack name and then we'll say bucket name and so the the idea is that we can now reference this from over here but that means that we need to uh first deploy our bucket and then our uh I call it main but yeah our main template I suppose here let's just hover over here and see if there's an issue it's saying it's expecting a string there is no problem here maybe it just wants this notice that we can't use um two of these okay that's not going to work uh whenever you have two intrin intrinsic functions uh you just simply can't use the Shand twice and you'll have to uh utilize one of them using regular Json or yaml if that makes sense but I believe that this is going to work so we'll CD into our PL foration um uh folder here and let's go ahead and deploy so this will be uh deploy bucket hopefully that works um says when calling to create change sets parameters null values are not allowed so let's go over to here because maybe we are passing something in here uh since we have no parameter here so we'll just take that out of there that was the problem and we'll go ahead and deploy this again so that might have been our issue looks like it's creating I'm going to make my way over to cloud formation it did create it excellent we'll go over to here we'll accept our change set looks good to me excellent and we will wait for that to deploy first all right so one is complete here let's make our way back over uh to here and let's see if we can deploy our referenced stack and before we do let's just go back here and look at our outputs and notice that we have our export name is cross stack bucket bucket name so that is pretty straightforward um and we will go ahead and deploy Main and hopefully this one just works that'd be nice if it does and I'll go back over to here and we'll execute the chain set and so we will wait and see if that works wow that was fast it does not like something and we'll have to move this over here I'm not sure why this UI so awful um resource Handler the architecture arm 64 of specified instance type does not match the XA okay so I think it's not an issue of this it's just that this says t4g and we want T3 uh micro here so that probably will resolve our issue right away also assuming that the thing that we copy from is in C Central One um of course you know you might have to update these uh just in case so always be aware of that but I'm going to go ahead and deploy this and see what happens and we got to wait for the delete to finish any time now I guess I'll just wait a bit okay so that one is now turned down we'll go ahead and try to deploy again for our cross reference stack and hopefully we have less issues this time around we'll just wait around a little bit here and see if it fails seems like it's okay and so we'll wait for this to finish creating okay there we go so um our cross reference stack has been deployed let's go take a look at that ec2 instance as we want to see if uh we were able to get the name of the bucket to shop under tags so we go over here to tags and uh uh is it there associated bucket there it is there you go so that works um Now understand with these that you cannot just uh tear down this one because you have to tear them down in order and so that's just how Nessa Stacks work if we try to tear it down it would complain I'm not going to show you that there's no point um but we will tear this one down first and then we'll tear down the other one okay all right so that one's teared down we'll go ahead and tear this one down there's nothing in the bucket so there's nothing that's going to prevent it from deleting so we'll consider this one down and so we have finished our cross reference stack and I will see you in the next one okay ciao hey this is Andrew Brown in this video I want to show you a feature of cloud formation uh what will happen if you have a resource and it cannot delete it um and this is kind of a feature that helps you get around issues where you just say skip it and let's not have to deal with that particular resource I don't know what to call this but um I'm going to just do my best to name a folder here so we'll just call this um uh skip delete resource I don't know what we want to call this but that's will be that's what it'll be called I'm going to go down over to our stack updates repo as we have a simple bucket here I'm going to copy it and paste it into this uh folder here there we go and what I'll do just uh go here and just say stack or skip delete resource and I'll just create a few buckets so we'll get rid of the bucket name I don't want that I'll get rid of the E2 instance I don't want that and I'll get rid of the bucket name and I'm going to create three buckets we have bucket one bucket two and bucket three and we'll go ahead into that directory and we'll deploy this so we'll give that a moment there and we'll go over to here we'll give this a refresh and we'll go to change sets and we'll execute the change set and we'll go ahead and execute that okay great so um what I want to see is those three buckets created and then we're going to place any kind of file into into one of those buckets usually this is pretty darn quick we'll give it a moment okay all right so we've created our three buckets um so I'm going to go into the first one here I just want to grab the uh name here I'm going to go back over to here I'm just going to touch any file I say touch uh hello.txt and then I'm going to say a ss3 copy hello.txt into this bucket so we'll just go ahead and do that and so now we've uploaded that file over to S3 and what I want to do is go ahead and tear down this tax so I'm expecting it not to be able to tear down bucket number one it's going to complain here and we're going to wait uh for uh that complaint there we go delete failed because it simply cannot delete uh because of that one bucket so if we go here you'll notice it says you may retain resources that are failing to delete and so this is the case where we can checkbox this and what it will do is we will delete the buckets that it can do so I really should rename this to like retain I'm not going to rename it but it's basically retain uh retain resources is a better name I think notice it deleted this uh skip skip the delete and so now that bucket still exists so we have to get rid of it manually so I'll refresh this here and so now I will just empty this bucket manually but that's what I wanted to show you the fact that you can retain resources when deleting and when that happens just understand that you're now responsible for those uh resources okay so I'll go ahead and delete that and I'll see you in the next one okay ciao hey this is Andre Brown this video we're going to take a look at how to import existing resources into a new cloud formation template as that is something that we can do I rarely ever use this feature we're going to use the uh console for this though probably it' be nice to use it programmatically but when I was reading up about how to do this it seemed really confusing but I know it's not as confusing as the docs make it out to be so I'm just going to do it the way that I know is easy to understand and so we're going to go ahead and make a new folder here I'll just rename this to import and we're going to go ahead and grab our Stacks updates because we have a very simple template over here and say allow here and I will paste it in here and as always you can use get pod CL uh GitHub code spaces Cloud9 your local developer environment I'm using git pod here today and this is already configured to work with the a CLI though we could have opened this up in GitHub because I do not plan to use the CLI here um I do not want to use an ec2 instance U and by the way import only works for very specific resources so like it won't work with ec2 instances and then there's other things where like you have to have a delution policy for an S3 bucket I don't know why these rules exist but they're there for a reason um I've already created a bucket I think so you'll have to go here and make a new bucket so just just create a new one and mine is called this um but it doesn't matter if we provide the default name in here because even we have it in there it's going to ask us to associate uh the resource when we do the import so here is my uh script I'm going to go ahead and download it okay and I'm just going to go ahead and rename it just so it's just template and I'm going to go over to Cloud information and we'll create a new stack and notice we have existing resources import resources I'll click that I'll hit next I'll go here we'll upload that file there we go so that was uploaded and now it's asking me for the identifier saying what do you want to map the bucket to so I'll copy this we'll paste it in here and we'll hit next let's just say my imported stack and now it's asking for the bucket name here I'll just provide this uh twice and oh it's pending creation there we go so so the change set is ready and so hopefully it will import it correctly Import in progress so we'll give that a moment to import hopefully it works that import is now complete we'll go over to our resources and we can see that is maap so now if we go ahead and delete this there's nothing in that bucket so it should uh in theory tear it down um did it that was really fast did it actually get rid of the bucket let's go refresh this there we go and so that shows that it was it was being managed infrastructure I just want to point out that you really do have to map all the properties have to be identical um so there probably are easier ways to uh do this and that would be using the IC generator so the I generator um could take an existing resource and generate out a template and I think that is a a much better way to uh kind of do an import if you will as opposed to man importing it but that is how you use the import option so I will see you in the next one so there you go ciao hey this is Andrew Brown in this video we're going to take a look at application composer so this service used to be called a service called stackery it moved over it was acquired by ads it was only doing ser stuff it wasn't so great last year but I think they have greatly improved it since last year um and it now shows up under cloud formation so the idea with this is it is a visual designer allowing you to quickly create things so uh we could drag out components here right um and then will reflect here the changes here or the other thing that we could do is if we already have an existing template which I do we can go here and maybe we have one for like elb so I'll go down to elb here and I will just go in and grab this template and go to Raw code I'll just grab this whole template here and paste it in if I go to Canvas you can see it's all visualized right so that's pretty impressive um but let's go ahead and just undo this for a second and let's say I want to bring it into an dc2 instance I just want to show you at a more granular level we'll just type it maybe instance here the search here isn't the best and what I'm looking for is E2 instance and I want to zoom in and I'm going to go into details here and so imagine we want to configure this if I go over to here we might want to add in an instance type and then we'll say uh T3 micro and we'll go back over to here and if we click into this and go to details we can now see those settings here all right so pretty straightforward uh they used to have like options in here for you to change stuff but now they've I guess greatly simplified this um but I guess another thing we could do with this is maybe like I'm just trying to think of something that would connect to any St instance maybe like R 53 so maybe there's like a route for that so I'm just looking for rev3 here rev3 does that show up here and I might want to have a record set and it'd be nice if I could get that record set to point to it so I'm just going to type in record set I I'll use um chat GPT for this just say cloud formation for a record set that points to an ec2 instance a rough 3 record set and so we'll see what we get here but I could see uh people utilizing this in their workflow um it's not really that useful because when you think about it it's not like you can go here and just start setting things you really have to like all this really is doing is giving you a visualizer but we'll just take a moment here and see what happens once we get that record set so yeah we got our hosted Zone that's fine and so I just want to copy this and we'll go back over to our template here and we'll go ahead and paste this in my record set um and I don't really care about the hosted Zone we'll just pretend that our hosted Zone exists here but what I want to do is grab the instance and paste that in here and we'll take this one out I'm not exactly sure why it has a problem with this all mappings must start with at the same column so I think it's just a spacing issue I'm going to go back to Canvas and so it does establish a connection there so even if there aren't dots sometimes associated with it once you start having that relationship uh that works really well okay I might have more complex uh use cases if we go back over to GitHub here because we did the boot camp in 2023 so I'm going to go over to the creder boot camp here and we're going to go into the last week like week 10x or sorry uh week X is the last week and in here we should have some pretty complex uh CL information templates I guess they're all broken up so maybe they don't look as complex as they could be but let's say I go into our networking one I grab this one I wonder what we would get again just playing around seeing if I can break it and go over to here and it shows all the stuff that it creates okay cool um group this not exactly sure what groups do maybe it allows us to have multiple templates in here so could I add another group create template nope so that is interesting there's other things that you can do with this I know like there's the IDE Tool uh you can sync this to a repo so the way it used to work was that you would have it uh have a a repo and it would push the changes there once you're happy with them but you could um have these changes reflect based on your local developer environment um personally I don't really care about doing that right now um and it is nice that they have all the components in here the only thing I would say is that what I don't like about this is that it took away a lot of the conveniences so the idea of this was that you didn't have to know coding whatsoever to work with appom composer and now app composer is just a visualizer it doesn't give you any conveniences of the old stacker system so is it good I don't know but it is nice to have a visualizer um how is this any different than the old visualization tool I guess it's basically the same thing because before when you used to launch a stack you could um have a visualizer here and that's what it looks like they did they just replaced the visualization tool with this tool okay but yeah there you go that is application composer hey this is Andrew Brown from exam Pro and we are going to start with the elastic beant stock fall along where we're going to look at how to deploy elastic beanock a variety of different ways so we know it inside and out um I want to point out first before we get started here make sure you are in the correct region and we always do everything in Us East one because that's where the most uh abundance of AD services are available and it just makes things a lot easier so just go up here and make sure you're in Us East one and be very careful because adus likes to switch out that region on you sometimes so if you feel like things aren't going uh the way it should be going just double check your region so now that we have that out of the way let's go ahead and make our way over to Cloud9 because we're going to need a developer a developer environment uh to run and test our application and then go ahead and uh take that over deploy to elastic bean stock so I'm going to go ahead here to Cloud9 I don't have any region or um environments created here so we'll go ahead and create an environment I'm going to name this uh Dev n EnV which which is developer environment here uh saying uh not to use root account I'm definitely not logged in as the root account so I'm not sure why I'm getting that message but we'll go ahead here hit next and what we're going to do is we're going to make sure this is a T2 micro that's part of the free tier eligibility we'll scroll down here and we have the choice between Amazon Linux and new buntu uh Amazon Linux one is supposed to be uh unsupported at some point because they want use Amazon Linux 2 um so if you're watching this in the future maybe Amazon Linux 2 will be here you'll have to use a bunch but if Amazon Linux one is here absolutely use it because it is amazing uh we're going to leave the default uh cost saving settings here to 30 minutes so if we're not using we don't have any activity or we don't have the browser open here it will shut down the server save us money uh it looks like it wants to create an IM amerald we'll let it go ahead and do that we'll go ahead and hit next and down below it has some best practices for us um it just shows us a confirmation of what we're creating this is all great so just hit create environment and we'll just have to wait here a little bit and I'll see you here in a moment all right so our Cloud9 environment here is ready and just before I get started I like to use the Dark theme so I'm going to just switch it uh down here to the classic Dark theme uh and I also like to use Vim I would recommend just using the default but Vim is what I use um that rebinds all the keys for super efficiency so um you know just because I've been doing it for years but anyway um now that we have our Cloud9 environment let's actually get an application going here and since this is uh very developer focused I think we should try to use the terminal as much as we can to get as much experience as possible the first thing I want you to do is I want you to uh type in mpm I C9 hyphen G so C9 which is short for Cloud9 um this is a a nodejs utility that makes it easy to open files directly um from uh the terminal here so you know we have this read me file and also just notice that see where it says environment this actually maps to this Dev uh EVN directory if I hover over there you can see It'll autocomplete to that so I don't know why Cloud9 does that but that's how they name it but anyway I just want to show you how C9 works so we have a read me in here and if I just wanted to open it up it actually is I think open right there but if I just typed in LS and then I typed in C9 readme then it would open up that readme file so that is going to give us um a little bit of help along the way so now that we have C9 installed let's go set up the actual application itself um and so the first thing we're going to do is we're going to type in mkd which makes a new directory and we want to make that in our environment so I'm going use Tilda to make sure I'm always at home I'm going to type environment study sync is the name of the application we are creating today and you can see up here that it created a folder okay and we'll go ahead and we'll just create some additional files so I'm just going to CD into that folder to save myself some trouble um and the first thing we need to do is initialize an empty uh nit or uh node projects we'll do mpm a nit hyphen y okay and what that did is created a package.json for us here which we will adjust uh momentarily but we uh want to run a app so we're going to need some kind of web framework so we're going to go ahead and use uh Express okay so we'll go ahead and type that in and what what's that going to do it's going to add it as a dependency there so now we can use express um the next thing we want to do is we're going to need some initial files to work with here so I'm going to type in touch we're going to type in main.js uh we'll probably need um index.html actually not instead of main we'll call it index I think actually I normally call it index uh then we will have um index.html app.js and style.css and so that created all the initial files that we uh need to work with and so now we just need to uh populate those files so the first thing I want to do is I want to have a way of actually running our application here in node so we're going to add a new script up here called start okay and we'll just type in node main.js actually I'm going to call that index there and the next thing we're going to do is we're going to start populating these files so if you make your way over to the GitHub and you go to exam proo the free adus developer associate uh there's a folder in here called study sync 0000 and these are the files that we're going to copy on over so the first is the index.js so we'll go there and just hit raw and we'll copy the contents here and we'll double click on that and we'll just paste it then we will uh click back here and we'll go grab the styling okay we'll hit Raw it's not that important for you to know how to program um but I mean you know we need to get as comfortable as we can here so we're not going to really need to learn all the stuff that we're doing just copy paste it through um and we just need I think the appjs file yep the Javascript file here and we will grab all that data and so those are the three we need and just give you a very quick tour of what's going on here we have this index.html file oh I guess we didn't populate that okay give me two seconds here sometimes you think you do something and you don't so anyway the index.html file loads this style.css file which is located there what we're doing is we're using a CDN to pull in mythol which is a JavaScript frontend framework we are going to use app uh JS to uh load our JavaScript um going over to our JavaScript here we're using the mythal framework so it's very simple we have this app here and the idea is it's we're going to have a question and we have multiple choices and we can submit the the answer somewhere and then we just have some plain styling in CSS so um now that we have that all uh going the next thing we need to do is actually preview this application because before we can deploy it and package it we need to make sure that it is uh working here so I'm just going to go ahead here and close these tabs uh and there's just going to be a couple things that we need to do next okay so uh what we need to do is we need to get our application running to make sure that uh it's all in working order before we go ahead and package it um and so we can preview in Cloud9 but uh Cloud9 by default doesn't open up uh its ports to the Internet so we have to go ahead and do that uh this would be no different and then you setting up a web app on an ec2 instance you'd still have to open up ports um and so generally the ports that Cloud9 allows out to the internet is 8080 80 uh 8081 and 8082 uh so what we'll do um I just want to show you how you normally do this so you go here uh and you go to ec2 instances you go to instances left hand side and we find the one that's running that's our Cloud9 environment and over here we're going to find security groups and if we expand that uh and we check inbound rules we go here edit and we just add 8080 right and then we restrict it to our IP this is a development environment oops I got to hit plus there um but we're not going to do it this way because I want you to get as much pragmatic um experience because this is a developer associate uh so we're going to figure out how to do it completely from the terminal uh so we're going to do the exact same there I know that's this is much faster but like trust me this is going to help you uh in the long run for uh studying here so let's get to it so what we're going to do here um I'm just going to type clear here so my screen's nice and clean is we need to figure out what the MAC address is for the cc2 instance and then we're going to use that Mac address to get the security group IDs and then from that we'll use the CLI to update and create our own um inbound rule so whenever you want to get information about an ec2 instance that is where the metadata service comes into play and uh it's very easy to access uh on a server whether you're here in your SSH into an ec2 instance or you're here in Cloud9 you just type in curl hyen s HTTP colon SL slash um and then it's 169 254 169 254 latest uh and metadata so here it's showing you that there's a way you can get a lot of data in here this 169 254 uh you should know this IP address it should be etched into your brain because it's definitely a standard here when working with ec2 and as a developer you need to know it uh but we need to find the MAC address and here we have this thing that says Mac so we'll go ahead here and type in mac and now we have the MAC address uh the next thing uh is we're going to use this Mac Mac address to find out all the security group IDs uh for the um network interfaces that use that Mac so what we'll do is we're going to hit up and we're just going to back out here a bit and we'll do Network we'll just PE meal it because if you make the whole link sometimes it's a big pain in the butt and it's hard to hunt down the problems so I'm going to just keep on doing this bit by bit um oh it just shows the MAC address there that's even more convenient and we'll just hit enter and then we want our security group IDs and there that's so we only have one Security Group if there was multiples we like attach to cc2 instance we probably see more but we only have this single one here so now that we have this uh Security Group ID what we'll do is we will use the CLI the CLI CLI is is already installed on this Cloud9 environment because it's Amazon link one already comes preinstalled and Cloud9 also uh loads you your credentials from your user account so we don't have to uh play with the credentials file here if you're doing this on your local computer you absolutely would have to set that up we'll type in AWS ec2 and we'll type in authorize uh security groups um Ingress and there is a new a CLI and it has autocomplete so like you could hit Tab and it would complete that stuff there for you but I don't believe I have the latest one installed here so I have to do things manually uh and so we'll place in that Security Group uh we'll say what port we want to open up so Port 880 we want we'll have to specify the protocol it's going to be um a TCP and then we need to supply The Cider um so that's the IP address that we're going to want to be accepted ible so before we hit enter here we actually need to go get our IP address because that's what we're going to put in here um so what we'll do is use one of A's Services which is called check IP so um it's a very useful service let's go use that now so I just open up a new tab here and I'm just going to type in I'm going to type in um check IP Amazon ads.com and this will tell me what the my IP address for my local computer is there's other websites like what's my IP but let's use the ad service because they took the time to make it for us and we'll go back over here and we will hit enter and we want for sl32 that for sl32 is very very important because that says only a single um only a single IP address aider aider block is a range of IP addresses uh something we definitely cover uh in this course and you definitely need to know what's uh networking like CER blocks are in the associate but for the time being if you don't know what it is just understand that you need to put your IP address in there and type in forward slash and what we'll do is we'll go hit enter um and actually before we do that no no we'll just hit enter that's fine so it didn't show us anything um so I mean I believe that successfully created it so we'll go over here and just take a look here to see if it actually made it and there it is but let's say that uh we didn't want to make our way over here and we wanted to do this programmatically let's go confirm the uh Security Group um through the CLI so what we're going to do is type in AOS ec2 describe security groups we're going to put in that group ID it takes a um a bunch of them so but we only need one here and uh we'll output it that says text that allows us to uh generally it's by default Json but that's just hard to read in this case and then we're going to use filters so we type in filters name equals IP hyphen permission do2 hyphen Port values equal 80 so what I'm saying here is describe all the security groups to me and filter it out so we or only select this Security Group uh display this as text and then filter it out so that we only see um inbound rules that have port 880 we'll hit enter here um and we have a invalid command there so I'm just going to double check here I might have typed something wrong permissions does not look spelled correctly to me um yeah it's going to be P eer permissions and I'm still having a bit of trouble here IP hyphen permissions um to Port oh you know what it's a singular permissions it's not with an S I think there we go so it's a little bit hard to read but the uh idea here is that it's going to say this is our security group that we returned it's saying that Port 880 has been set and that's the IP address now if it hadn't been set and we ran this command it would just show nothing so the fact that something shows up here means that uh that the security uh that um that inbound rule was created but of course in practicality you probably just use um the the console so now that uh we have we'll just type clear here to clear this stuff up so now that we've opened that that Port uh the next thing is actually getting uh the application running but before we can even do that we need to know what the actual IP address of this Cloud9 environment is and I'm pretty sure if we use our ec2 instance here we can go here and we can go and check it and that is its public IP address I think that would be the same thing but let's again do it pragmatically so we'll type in curl hyph s HTTP colon1 169.254 do1 169.254 latest metadata I'm going to hit enter just so I I'm not having too much trouble here you can even see right here it's public ipv4 so we'll type in public ipv4 and it says 385 910 I'm going to go back here yep so it's the same one there so that's what we're going to use to um access the uh web application and let's go ahead and actually start this application up so to start it up we just have to make sure we're in that study sync directory here and we want to start it up on port 8080 so if you're wondering like why are we typing Port 880 there um the way this application works if you open up the index.js um it uses process EMV port and it's going to pass that uh port number to Express so it knows to start up on that port number and uh so what we'll do here is we'll just go and type in mpm start and we'll see if this starts up and we have a little error here and that's totally okay it says failed dep par uh package.json d data you know what happened we forgot a comma this is something I do all the time so remember we wrote this line in here we have to just make sure there's a comma on the end of it or it's not valid Json and I'm just going to go back down here hit contrl C which um kills that there and we'll hit up again we'll see if we're in better shape so now it says that it's launched on port 8080 so what we can do now is uh get that IP address that we have um earlier so it is somewhere in here uh um can't seem to see it so I'm just going to go here and copy this here and make a new tab in terminal and just paste it in of course we could just go to E2 instance but why do that when we can uh try to do it the proper way and so what we'll need to do is do this and say uh Port 880 we're not going to open up here but I'm just typing it out so that I'm having less trouble here and we'll just copy that and we will see if this works and there's our application so this application doesn't really do much uh you can select something and submit but it doesn't really submit to anywhere um maybe we will start hooking this up and do more with it I call this the study sync application it's supposed to help you study I guess um but you know it's just a superficial application so now that we have our application uh running and we can preview it from Cloud9 and we have some CLI experience the next thing is to get a get repository set up so that's what we'll do next okay so let's get a get going here and I'm just going to go ahead and close this uh bash command we'll go back here and we'll just stop the server I'm doing that by typing uh control C on my keyboard you can see that it says control C this little icon represents control and uh we're going to need a get ignore file because there's files that we just do not want to include so up here when we have no modules this is how uh node Works they just put all the libraries in line and we do not want that in our git repo that's just too much stuff uh and so with Git there's a file called get ignore um and what we'll do is we'll just make sure we create it here in our study stin directory so just make sure you're in environment study sync if you're not sure where you are just type in environment study sync as such and then we will just go ahead and touch a dog ignore file and uh that should now um exist I just don't see it here on the left hand side um it might be hiding yet or or we might have to hit refresh well I definitely know that it's there so because if we do an LS hyphen La uh there we can see that hidden file uh there's probably a way to turn it on there I'm just not sure at the current moment um but that's fine because this is all about learning how to use the terminal and ciza developer so if we want to open up that dotg ignore we're going to use our handy dandy C9 command top top get ignore and if you don't want to type everything just hit tab on your keyboard that saves a lot of time when you're AO a completing stuff works for most things and what we're going to do is type in node modules what's what's that's going to do it's going to ignore this file completely just folder completely because we don't need to include that with Git now that we have that set up you may want to set up your git config Global you uh username and email uh we're not going to worry about it right now but we'll probably be prompted for it and it'll just be an annoying message every time we see it so now that we have our G ignore let's set up our G uh let's actually set up this git repo so we'll type in get a nit and it's initialized an empty repo it's created a new folder called dogit here so if I just do an LS LS hyphen La you can see I have a dogit directory now um and we're not going to really get into details about get here but you know just showing you what's going on the next thing we want to do is we want to add all the stuff that we've worked on so far to our actual repo so if we type in get status it'll show that we have these untracked files meaning that they um they aren't going to be committed so let's add them to be committed so we have to do get add we'll do get add period that will just add them all so we'll hit enter we'll type in get status and so now you see it went from untracked to changes to be committed and so now we just need to write our get commit message hyphen M initial commit and they are now committed and there's that thing I was talking about that git uh git config username and email generally you want to set these with your you uh name and email since I'm just doing this for practice here I'm not going to do that and it'll probably keep on popping up um but we've created a g repository but it only lives on this Cloud9 uh environment and we really want to make sure that this is hosted somewhere in the cloud you could use GitHub uh but for the this course we are going or this uh project we are going to be using Code commit so that we get some handson experience with that I think a lot of developers already have a lot of experience with uh GitHub um but uh yeah we'll get to that uh next here so I said the next thing we want to do is get this uh this local repo that's this this folder here well we can't see it on the left hand side because it's hidden but we want to get this uh all the contents of this entire folder and this dogit file into a repo and we're going to use code commit now when you use the um the elastic beant stock CLI you're setting up a project for the new time it'll uh the first time it will set up a code commit project for you and so I figured that's the way we should go ahead and do it um so we'll just go up here and um make a new tab because the the CLI is not preinstalled so the a CLI preinstalled on this Cloud9 instance but not the elastic beanock one so what we'll do here is we're just going to go ahead and type in uh elastic beanock CLI GitHub and and because that's going to have the instructions for us to do this install here and so I'm just going to scroll down here and based on your environment you might have to install additional things you can see there's a bunch of things uh but since we're working with Cloud 9 there's not going to be anything that's too difficult here and all we have to do to install this is to run the get clone command so let's go ahead and give that a go um so we'll make our way back to our Cloud9 environment I'm just going to type in clear so we can see what we're doing and I'm just going to go back One Directory because this is going to clone that repo like download this folder and I just don't want it having in my study Sync here so I'm just going to go back in directory which is a CD do Dot and we're going to type in get clone and uh this is already complaining too many arguments oh you know what because when we copied it it already typed get clone for us it was trying to save me some trouble so I'm going to go back and there copy I wrote it in manually I'm silly and we'll hit enter and that's going to clone it so that's just going to download it to our local computer and now to run it if we go over here um it should be probably this command um yep that's the command so we'll go back here and we'll just hit enter and what it's going to do is it's going to install a bunch of stuff um this is probably not this Cloud9 environment is probably not using this version of python if we just go over here new tab we're not going to we're not going to mess with this one but if I just type in I think it's like just type in Python hyon version here oops oops oops I didn't want that um hyphen hyphen version maybe okay so it says version 3.61 and uh this one wants version 3.72 so you know that's just the state of Amazon Linux one right now and we're just going to have to wait this is going to take a a few minutes as it says here I'm just going to go over here I have every time I stop the video I have to I have to hit a command key it messes up terminal but uh we're just going to wait for this to install and this again takes several minutes so please be patient you know might be 3 4 minutes and I'll see you back here in a moment okay so just waiting here a little bit here coming back to our first tab we can see that it has completed so it just took a little bit time to install python um and there's one more thing that we need to do and it's just to add um this e elastic bean stock to our path so if we were to type in EP it shouldn't be able to find it because it just doesn't know where that uh binary is stored so we just need to take its recommend recommendation here we are using bash you can tell we're using bash this bash up here uh and we just need to Echo that command here and I'll just hit that uh it also seems to suggest this I don't remember having to do this last time and I think we can go ahead and do that I think it's safe to do so I hope that doesn't mess up this uh this uh follow along here but I'm pretty sure that won't and so now if we type in EB we have elastic beanock pop up so that's really great uh what I want you to do is just delete this folder we don't need it anymore it's just creating clutter so just type in lsph La make sure you are at this uh environment uh uh directory if you don't know just type in CD Tilda SL environment and we'll type in RM hyphen RF ad us autocomplete it hit enter and we saw that it vanished there so it's just a bit of house cleaning because we don't need that uh sticking around so now that we have the elastic uh beant stock environment uh installed or the CLI uh let's see what we can do with it which it will be actually setting up an application I'm just going to close this other tab here and I'll see you here in a moment so now that we have the C installed we're ready to initialize a new elastic beanock project now I want to point out that uh we are currently in uh Tilda environment that's our home directory it's very important that we run this command in the study sync directory because it needs to find this dogit directory in order for it to upload our code to code commit so just type in TD uh or CD Tilda SL environment um study sync and do lsy la make sure you see that dogit directory there before we get going here I'm just going to open up a couple tabs in AWS and we're going to go uh uh we're going to go to one that's actually at the CLI here so we're going to make our way or sorry to um elastic beanock and then for this one we're going to make sure that it's on code commit just so we can see what's happening in the background here so what I want you to do is I want you to type in EB it will give you a full list of commands um we're not uh we probably won't end up using all these commands but these are the most general ones and they tell you to use ebit EB create and then EB open for EB open we don't actually have the ability to use this command this makes it so the application uh opens up in the URL in the browser which is very convenient if you're if you're running elastic beanock on your local computer not in Cloud9 that's a great command to use but here we won't be able to use it let's go ahead and do ebn it the first thing it's going to ask us is the region we definitely want to default that to us East one always Us East one because it makes things easier um it's going to ask us to select an application it's going to be study sync it knows that because it's picking it up from the dogit folder there uh it's going to ask us if we're using NOS we absolutely are so yes do we want to use Co Cod commit we'll say Yes um and then enter the repository name we're going to call this study sync and then it's going to ask us if uh or what we want our our our default Branch to be we want it to be Master we'll just type in master we can just hit enter there either and what it's going to do it's going to go ahead here and in code commit it will show us that it created that repo so here it is so it it uh it's now here in code commit elastic beanock we're not going to see anything here yet this is just an old one here but if I do a refresh I go back here well you probably won't see this um but I I have I had older terminated instances here so you might not see anything here as of yet uh and then down below it's going to ask us if we want to uh set up SSH access I'm going to say no we do not need that and so now we've initialized our project uh not that it's created a new folder called EB uh elastic. bean stock the period means that it's a hidden folder we're going to go and open up this uh config.yml file and so these are some of the options we chose when setting up the ebnet so now that we've initialized our project that we have our code on code commit we need to configure this application so we'll move on to or the the elastic beanock environment so we'll move on to that next okay so uh the next thing we need to do is uh configure this environment um so you generally have to do this for all elastic beanock projects based on what environment you're using uh and configurations show up in theeb extensions directory so we'll have to create one ourselves so I'm just going to type clear here I'm going to make sure that I'm in the right place so study sync and I'm just going to type in mkd uh which is going to make a new folder EB extensions okay and so now I have that new folder there I'm just going to double check it EB extensions that is totally correct and we're going to need to create a couple files in here so I'm going to do touch uh or sorry I'm just going to CD into this EB extensions folder it'll save me some time I'm going to touch a file called z01 nvar do config and I'm going to do another one called uh node command and I'm just going to go back a directory so if we open up this folder here we now have nbar and this is where we're going to set some default um environment variables um by default you don't have to specify an environment here it would it would just go to the elastic beanock environment but we are going to be explicit here and this is what we want to type I always always type the word environment wrong e NV i r o n m n t we need callon on the end there and so I'm just going to set Port 8081 and we're going to say node EnV production and it's four spaces inent that's totally fine I'm just going to leave it alone and so that is uh one file configured then we will go over to the next one here I'm just going to double check this one here option settings yep that's all good and so the next thing we need to do is tell um elastic beanock actually how to start up our application because it has no idea so we'll do ads elastic beant stock a container uh and then nodejs so we're going to give it some uh nodejs specific configurations here first we're going to uh put uh provide no command that's going to tell what command it's going to run when it first starts up we want it to mpm start and then we need to specify the node version this is going to be 10.1 18.1 I actually don't have to put the parentheses there use double quotes I think you use single quotes but I'm just going to stick to what I wrote earlier because I don't want anything to go wrong in my uh fall along here and if you do node hyphen version you can see that it's using 10.19 Z and it is better to use the latest version but that doesn't necessarily mean that those um like elastic beanock can run that I definitely know that 10.19 is not out for elastic bean stock and 10.1 18.1 is available so we'll stick with this one but yeah that's the configuration there so we'll go ahead and we'll just commit it so get add all get commit hyphen M uh configuration uh for elastic beant stock it's very important that we do commit this and then we push it because if these files aren't there and we try to create an environment it's just going to air out because it always looks at what's in the um the repo here so if I go into here Cod commit EB extensions we can see we have have those files there so that is the configuration phase over here I'm just going to close a couple things here just clean up here a bit and uh what we'll do next is we'll actually uh create a new elastic bean stock environment all right so we are ready to um create our elastic beanock environment and to do that we need to use the EB create command but we're not just going to write EB create we're going to write EB create hyphen hyphen single this hyphen hyphen single this is a command flag what it's going to do is tell this to create a environment an EB environment that is running in single instance mode if you don't provide this flag it's going to spin up an elastic load balancer elastic load balancers cost money uh technically if you're using the free tier if this if you made this account uh and you're still in your first year you get one elb free running but um you know we just want to avoid the uh these kind of problems so let's just not use it there's we're not going to really be doing anything with elastic load balcer anyway through this uh walkthr so do EB single it's going to prompt us with some options uh I'm going to name this study sync prod even though we're our Environ or developer environment uh my ads account if you go back up here mine says exam Pro Dev um I'm going to pretend this is a production application and I'm just hit enter there we just want that to be the same we don't want to use spot instances but that's a great way to save money um and then it says this insufficient iron privileges unable to determine if this rule exists assuming that it exists and it's going to go ahead and spin that up I don't know if this is going to cause us a problem but we're going to have to wait here and this is just going to take a little bit of time to get uh going here so I'm just going to um wait for this to get started I'm going to open up another terminal here and I'm just going to go to study Sync here and uh I'm just going to type in EV status and this actually shows us the current status of the application right now the health is gray and the status is launching if we go over here you can see I was trying to launch some stuff earlier here those are terminated instances this is the new one here pending and you can see it's a nice dark gray and this mirrors exactly what we're looking at over here so this is going to take about 5 to 10 minutes to launch and I will see you back here momentarily okay so after waiting a little while here I go back here and it says that it successfully launched the ec2 instance so it looks like to me that it's uh all in good working order we go over here and we type in EB status uh you can see that says ready and yellow yellow is not a great status to have um so if we come back over to uh the elastic beanock environment I can't tell if it's finished yet but I'm just going to go back here to study sync click into the yellow here and it's giving us a warning it's saying unable to assume the role it elastic beanock service role so it's supposed to create that for us but for whatever reason um it just did not when I first wrote this fall along it definitely created that for me so I'm not sure why it's uh not creating it but if we go up to this link here and open it up here the application clearly is working so um yeah the yellow command is not that great but uh maybe it'll go away on its own I'm just going to hit refresh so I'm just going to wait a little bit here and see if it actually does go away or not okay so our yellow eventually became a red uh it really has to do with this Aus elastic beanock service roll uh this is confusing because when you run EB create it's supposed to create this for you it's supposed to actually create two different um IM roles if I go over here I don't see them in here at all now you could go ahead and manually create them uh I've tried to do this and I haven't had much uh success as well um but there is another way for us to create these roles without us having to do a lot of Manual Labor uh and again you might not have do this those those um ec2 roles or IM rolles may exist but in my case I'm just having a hard time today with elastic bean stock so to get them created what I'm going to do is I'm just going to start another elastic beanock project so I'll go here create a new app I'll just call test it doesn't matter because if we launch one from here it will absolutely um create us a um those rolles so I'm just going to go here go to web server environment I'm going to leave it as test I'm going to choose Ruby I'm going to go down here and launch a sample application that launch rails um I'm just going to write test in here check for availability that's all good hit create environment and so what that should do is it should trigger through the console uh that should go create those IM roles so if I refresh here now you see they exist adus elastic beanock ECT roll AOS elastic beanock service R so I have no idea why those aren't uh appearing now they do um but the trick is I just need to delete this environment now so I can't stop this as it's running so we'll have to wait till it goes through the motions of it uh for this test environment and then once it's done here we'll just go ahead and terminate it okay so the environment spun up and uh now we just going to have to go ahead and delete it I know this is really silly but I mean that's the only way I can get these roles to be created but you know they definitely definitely should be able to make it manually and this definitely should automatically happen but I'm going to go back um I just clicked to all applications here then we'll go into uh here on the right hand side we'll see if we can go ahead and delete it here um so I'll just put test in here and so what that will do is it should automatically start deleting this environment so if I go into here it's terminating it so um that's that um but this environment is just no good so um what we'll do is I mean we can terminate it I suppose I guess what we'll do is we'll just terminate this environment as well so I'm just going to go here and type this in here and I'm just going to wait for this one to uh Delete um and then we'll try e create again and hopefully we won't have any issues this time around okay so I did a refresh there and it's terminated um and what I'm going to do is I'm going to go back to Cloud 9 and I'm going to try this again so I'm going to do I'm going to go back to our first tab here I'm going to do I'm just hit up it's going to ask me for the name I'm going to do study sync prod hit enter again uh we don't want spot here and it's saying that uh 8s elastic being E2 roll it can't find it that's a okay when I first did this I had that error and it wasn't an issue um but let's just really make sure that it actually is there CU as long as it's there that's all that matters okay so the RO is there so we shouldn't have any problems this time around um but we'll just wait here and see what the result is I'm just going to go over to elastic beanock give it a refresh clicking it here and we'll just wait a few minutes and see how it goes okay great so um the this elastic bean stock has gone green so um us creating that temporary application even though as silly as it is uh fix the issue here uh hopefully you don't have to do that and those rules just exist for you um so if we go over to Cloud9 here I'm just going to do a clear here you do EB status that's going to show you um the status here so green and ready which is the same thing that is over here um if we wanted to go view this web application it shows the C name in here so if I copied this out we can see we have a link there um if we scroll up here we can also see that it assigned us an elastic IP address so that's another way of accessing uh the web application here is that IP address um if we typed in EB logs this would show us um what happened actually on the ec2 instance if anything was logged out I might have showed you this prior but I'm going to just show you quickly here again and so you can see that the application started up here I'm not sure what's going on uh down below here I don't think that matters but this is what we really do want to see we hit Q to exit that out there if we type in EB events that is going to show us the event history that has happened um so if you go over here and events that's the same information here really great way to debug stuff and I want to point out that the deployment um the deployment model we're using is all at once right now we haven't actually done a deploy yet we' I mean we technically have deployed but we haven't deployed to an existing um environment so I'm just showing you that it's using all at once um and so the next thing we're going to do is switch this over to immutable and see what the difference is there um but we're not going to do it in here we could just click immutable hit apply and then do a deploy but I want to do everything through the console so that's what we're going to do next here so let's get to that so to switch this over to mutable deploys again I said that we could modify this file but we want to do it programmatically and we're going to do that through the configuration files so what I want to do is go into e. extensions here it's not letting me type if that happens to you can just close bash and uh open a new window here I already have one open um and I'm just going to go into EB extensions I'm going to make a new file and I'm going to call this um 0000 deploy doc config I just want it to be ahead of those other ones I don't think that order really matters but it's just what I want to do uh oops it's not make we want to do touch so we'll touch a new file there and what we're going to do here is we're going to set option settings and we're going to do adus elastic beant stock here um and we're going to do command and what we're going to do is set the deployment policy to be immutable and then what we're also going to do is set health check success threshold uh threshold to warning and then we're going to set ignore health check to true and we're going to set timeout to 600 here so I'm just going to read this over very quickly here make sure I didn't make any mistakes here elastic bean stock that's correct a mutable or deployment pul immutable that looks right to me health check status threshold that looks good to me ignore health check so we're going to say over here what we're doing we just we're actually check boxing that off okay and we're pretty much setting the same settings that are here except this is going to be warning um and that should make the deploy really fast um and also while we're at it let's just make a superficial change so when we do deploy we can actually see if uh the effects have uh taken effect here so I'm just going to go to our actual application here I'm going to just change this to study sync version one if we were to actually check out this application before we deploy it here notice that this says hello world here so we're changing that to study sync version one we're going to go back um to bash here we're going to go back a directory just going to type clear make sure you're in the environments study sync in here and we're going to add all the changes so we made we modified that file we added a new configuration file so I'm going to add them both get commit hyphen M immutable deploys I'm going to push those changes I'm going to go ahead and deploy that so so we'll just type in EB deploy and right away it should start um switching over to immutable deploys because it's going to pull those configuration files to look at them and then it's going to decide on what to do so if I think we just wait here a moment it's actually going to tell us and we can see that I actually have uh an error here so I'm going to go ahead and abort that so I'm just type e abort um but we can see contains invalid key option settings so I probably just made a mistake here oops yep we'll try this again EB deploy oh well we have to commit those changes get push we'll do EB deploy again and so what we're looking for is to see if it will actually say that it's doing an immutable deploy and there it is it says immutable deployment policy enabled so we're going to have to wait for that deploy to happen I'm just going to open a new tab here because I'm going to stop the video here um and immutable deploys are a lot slower uh than uh than all at once but the advantage here is that it won't take our server out of service um it's going to create a new server and then when that new server is good it's going to switch over to it so our users will never have an interruption in service so I'll see you back here shortly all right great so our mutable deploy uh has completed um it's actually been quite a while since last time I was here uh because I'm actually recording this the next day but I can tell you that that IM deploy uh didn't take too long um so it definitely takes longer than uh the than the all at once deploy all at once is extremely fast where um these immutable deploys have to go through health checks um and then go through multiple checks before it determines that the new service is good and moves over um so you know that was a mutable deploy but what I want you to do is go back to Cloud9 and we are just going to undo those changes there because the next thing we're going to learn how to do is blue green deployment and I don't want these immutable deploy slowing down our development here so uh just to get rid of this immutable deploy stuff all we're going to do is remove that file there so I want you to type in RM and we're going to do Tilda here environments uh study sync uh EB extensions 0000 config deploy and then we're just going to add those changes I'm going to make sure that I'm in that study sync directory there because it looks like I was in the wrong uh wrong place there and we will do um uh get add all get commit hyphen M uh revert back to IM mutable uh deploys okay get push and just before we do anything else uh here I just want to go back um to uh the environment here and just show you under your configuration that it should have switched to mutable deploys so here you can see it's a mutable and the health checks are disabled um but anyway now that we uh have that set up all I'm going to do here is now that I've made these changes I did a push I'm just going to do a cap deploy or not cap deploy EB deploy I'm thinking of cap arono which is for Ruby on Rails it's not what we're doing here uh and and what we'll do is we'll just revert this back uh to all at once and this isn't going to take too long so just going go back here to my dashboard and we can see this is updating and I'll see you here uh when this is done and then we'll move on on to blue green deployments which should be super exciting all right so after a short wait there our um our mutable deploys are back to all at once deploys and we can just double check here under the configuration if we go down here uh we should see now it's that now it's all at once so let's move on to Blu green deploy so Blu green deploy is when you um you switch um environment so right now we have this environment here which we can consider our blue environment and the idea is we're going to spin up an identical environment called our green environment which will have our latest changes and once that environment is in good shape what we'll do is we will swap the URL of the environments now this option isn't available to us right now because there's nothing for it to swap to but once we have that um other environment that's how we will uh make that switch over so in order to do so we're going to go back to Cloud 9 and what we need to do is clone this environment make a copy of it and we could go in here and I think we could go right into here and then click actions and clone the environment but let's do it through um the CLI because again this is the developer associate this is the best way to learn so we're going to type in EB or EB clone and then it's going to uh prompt us for the name I think clone is okay for our case here so I'll hit enter uh we'll keep the C name the same and what it's going to do is um start up a new um environment there so we'll go back to study sync and I'm just going to give it a refresh and here we can see that this environment is spinning up so again we're going to have to wait a little bit here and I will see you back momentarily all right so after a short wait there because it's using alt once deployment uh we have our production clone environment up I know it doesn't look like it's running here but if we just go back here a second and I do a refresh here we can see now it's green so don't always trust the ads console always refresh and look around because sometimes things are ready and you're just waiting around for nothing uh so if we take a look here at this clone environment following this uh this C name here uh or the DNS uh what do they call it we'll just say URL this URL here we can see that it's running uh but we want to make sure that we have a new version here so this is version one uh so let's just make a superficial change client version two and then see how we can deploy to this new environment and then uh uh facilitate the switch so what I'm going to do is go back to Cloud9 and I'm going to make my way over to um the app and it's just making some complaints here so I'm going to close these tabs and what we want to do is we want to go ahead and open this appjs file and I'm going just change this to version two all right and so now that I've changed a version two I'm going to commit this to the repo so I'm going to go get add well I'll do get status we should always do that we can see the file we want to add get add all get commit change to version two uh we will do a get push get status all right great so our version two is there so how would we go about deploying to um the the green environment because we have these two environment so if we ran um EB deploy I think by default it's going to deploy to the original environment but if we want to specify the environment we want to deploy to which is the green environment we just have to provide its name so that's called study sync prod clone I'm pretty sure that's the name of it yep study sync prod clone and so this should now deploy uh the latest changes to this environment so we'll go ahead and press that there and it's going to start up again we'll just give it a second here we'll flip back we will go back to study sync U I'm just going to give it a refresh here because something should be changing probably the prod yep so it's updating so we will uh let that deploy there um we'll give that a little bit of time and I will see you back here when that's done and we'll just double check its version two and if that's all good we can do the swap all right so um pushing our changes or version two changes to the Clone is done if we go to this page and refresh you're going to notice that uh there hasn't been any change but it actually has worked uh this is just an issue with chrome because if you open up another browser and refresh in Firefox it says version two for the same URL so this is a chrome caching issue um I spent hours upon hours trying to uh solve this problem and not realizing it's Chrome so just be aware that anytime you're doing deployments to anything um and you're checking stuff always just rule out your browser CU sometimes it's not even AWS so you know if we want to get to see the latest version here I'm going to open up inspector make sure you're on network and have disabled cache do a refresh there and now it says version two this will not work unless you have this open this check boox and then you do the refresh all right um but yeah now that uh we have figured out how to deploy our second version with blue green deployment what we can uh go ahead and do is swap over the environment URL and so um we said that what we could do is go to here to actions and go to swap environment URL and we could go here and choose our other version and swap here however um since this is for the developer associate I really do want you to get as much experience with the uh CLI and I'm going to just keep on saying that and what we're going to do is go ahead and uh use Cloud9 and use the actual EB CLI to do that so the command here is EB um Swap and then we're going to say um it could if we hit enter now it would prompt us to ask you know the source and the destinition but I just want to be very explicit here and I'm going to just type in the source so I want to swap the prod with the Clone so we'll say study sync prod clone and this will do exactly the same thing as uh swapping the url url out here okay and I'm just going to hit enter um unrecognized argument here clone let me just make sure that I spelled that name correctly I'm just going to go over here and check study sync prod clone study sync prod clone um oh sorry you know what um I have to provide a flag here it is um hyphen hyen destination name and now we'll hit enter and this is going to trigger that swap action there and so we're just going to have to wait a little bit there it says it's completed the swap wow that was really fast so we'll go back here and um if we are to click on prod now what I want you to notice is this is the Clone URL here right and we are now in the Clone environment so this used to say clone but now it says prod it's taking the original C name from the other um environment if we go to the first environment here this one now has clone so that's the swap that occurred so that's how we know that it worked uh now that that swap has occurred what we want to do is just get rid of our old environment because we know our new environment is running uh with no problems here if we just go to prod like this it's wuning version two so we're all good and so what we need to do is go in here and then just uh Delete terminate this environment but let's do it from the uh CLI so we'll go back here Cloud9 and what we're going to do is type in EB terminate study sync prod and then it's going to just ask us to confirm it we'll hit enter and that's going to terminate now we're pretty much done with uh blue green deployment here and uh with that out of the way we can actually move on to learning how to deploy a single Docker container next so what we're going to need to do is Rip down everything because we do not need even this clone anymore so I'm going to also terminate this one but I'm going to do it through the console here and we need to type its environment name so I'm just going to copy it here paste that in make sure that's right and uh what we're going to do is we're going to wait for these to uh shut down here and when these are both terminated we'll move on to the next step which is deploying um a single container Docker environment to elastic bean stock all right so we are back here uh it went through the whole process here and uh built everything and it's saying that it's running on port 8080 what we're going to need to do is open a new uh tab here because we're going to need to get the IP address of this um Docker or this uh Cloud9 environment so go to a terminal here I'm going to type in curl HTTP colon1 169.254 169.254 uh latest metadata we'll hit enter make sure this works and then we'll type for SLU ipv4 and that's the IP address so what we can do with this just copy this here and go port 8080 and we will see if this works oh let's just click it too I uh open and there it is so this is running in a single Docker container the reason you'd want to dockerize your environment is because it allows you to ship um your your configuration uh with uh your codebase uh so you saw before we were restricted to version 10.18 whatever of no but now we are only restricted to whatever we provide with it so a lot more flexibility around that uh in order to prepare this for deployment we aren going to need this node modules anymore uh because we are using a Docker container and this is just going to do nothing so we'll go ahead and remove that so what I want you to do is just close this tab here I'm going to do control C to stop the docker container I'm going to go CD Tilda oh we're already in the right place but this is where we need to be uh and we're just going to do RM uh EB extension uh 002 and just remove that file so now that's been removed um we need to make an adjustment to this file here I'll just hit keep open this needs to be port 8080 because that's what we're setting in our application and uh let's go ahead and commit our changes here so configure EB for Docker and so now that our pushes have been changed what we can do is go ahead and do an EB create hyphen hyphen single so we don't launch a load balancer we are going to name this one uh different just so we can identify it so we're going to say study sync Docker we'll hit enter we'll hit no we don't want spot instances and we will make our way over to the um Cloud9 or Cloud9 over here do a refresh we'll give it a second here to start up there it goes we can see that's using the docker platform we'll do a refresh and we will just wait until this is done and see if it works so after a little while there our environment is now running here let's take a look to see if it's working and there you go we're running on Docker it was that easy uh the thing with elastic beant stock is that it did all the work for us we just had the docker file in here and when we uploaded it it did all the work it built the image for us uh but normally what you'd have to do is build the image yourself and then push it to an actual ual um Docker repository that could be Docker Hub or in the case of AWS you can use elastic container registry ECR and that's what we're going to do because um that's a more complex setup and the more common setup that people will be using because most people outgrow um this Docker file this simple setup here uh in order to do that what we're going to need to do is um create a new file called a Docker run ads. Json file and we're going to have to build an image and push it to ECR but before we do that let's just make a revision to our actual codebase here uh and we will go to app.js and we'll call this version three and the next thing we will do is we'll go ahead and build our Docker image so I'm going to type in Docker build hyphen T study sync period so this is going to build a Docker image and it's going to name it study sync so we'll just wait here a little bit and there it is it's done that was fast the next thing we need to do is we need to um authenticate to ECR and this is a very long command so we'll get to it it's ads ECR get login password um pipe Docker login hyphen hyen username AWS hyphen Hy password stn we're going to have to provide our account ID here um I don't know what my account ID is for this account we need to poke around we should be able to find it somewhere it's generally under my account settings I just don't want to uh show all of my billing information here so another easier way we'll just go make our way over to IM am I feel like that's always a place where we see our account number we should see it anywhere we'll just go into even the user here here's one our account number is everywhere I just need part of it there so I'm just going to paste it over here and then extract it out and then what we need to do is provide it as such and then we need to uh type in this URL so we need D kr. ECR and then we need the region we're operating in so us one dot Amazon aws.com if you're wondering how I got this whole link it's in the adus documentations for ECR so what this is going to do is log Us in and and generate us out a um a token so we can authenticate so it says there's an unknown flag named user so I'm just going to double check that there it's actually supposed to be user name so we'll go ahead here and type this in and here it says that it's created that credentials helper file so there you go so notice that it's created a file here called dot uh uh or a hidden file called Docker config.js and that's what's storing the token which is going to help us to authenticate so let's take a look at the actual images that are here and we can see that we have our images built here and we're going to get have to get this image ID next and what we need to do is tag this Docker image so I'm going to put in that uh image ID we need our account ID again here and it's actually the same link here so it' probably be easier if I just copy that out like that and then we need to specify the uh name and so now that it has been tagged what we can do is do Docker push and I believe it is the same URL here so let's copy this and it says here that the repository does not exist with this ID so um maybe what we should do is make our way over to um ECR and just maybe we need to make the repo beforehand I always forget this so I guess we'll find out I thought it would just create it for us um NOP I guess not so we'll just type in study Sync here I'll hit uh create repository then we'll make our way back to Cloud9 just hit up and there we go it's uploading our Docker image it's kind of like GitHub this is incredibly small uh Docker image so it's not taking too long which is really nice one advantage of using nodejs over other um languages and Frameworks so I'll just wait here a little bit and I'll see you back in a moment so our uh Docker image is now uh built and pushed to our ECR repo here so if we go in here we can see that we have it uh and the next thing is to prepare our actual um uh this next environment here and instead of working with this one because it's going to be um uh a lot of work here we're just going to make a new folder so once you go cdcd dot or just go actually here to CD Tilda SL environment we're going to make a new directory we're going to call it study sync external and what I want you to do is make a new file in here so we'll just CD into this and we're going to call it Docker run. abs. Jon if you've ever seen a task definition file it's extremely similar and uh in this um developer soci course we definitely cover how to deploy um with ECS and fargate so this will become extremely familiar to you shortly but what we need to do is open up that file uh I made it as a directory that was an accident I should have made that a file so I'm just going to remove that and instead of doing uh mkd I'm going to typee in touch okay and then we can just open up that file there and what we're going to do is um write some Json so the first thing we need to do is Define the docker version uh for EBS here so Docker run version just going to double check make sure that is correct yep that is right and we're going to specify version one version one is for um single uh single containers when you multicontainer you do version two then we need to specify the image and that's going to be uh the UR all we were seeing there earlier I feel like we could grab that from ECR yep it's pretty much the same thing here I just want this part of it I don't know if I need to put latest in there I until we put that in there we have to specify the ports so we'll go ahead here and do that so it knows what to map to and in that we will do this little bit of clean up here I'm just going to double check to make sure everything is right here sometimes it's easy to miss these commas um it looks all correct to me so we're in good shape uh the next thing we need to do is initialize a get repo here so we're going to do get AIT and uh we're going to copy over a couple files so we want to bring over our get ignore EB extensions file and our nvar doc config file I think so we will go ahead and do that um I'm just trying to think of the easiest way to do this probably just make the files again so I'm going to just type touch. GE ignore and then we will do uh we'll make a new directory called EB extensions and then we will touch um EB extensions 001 nvar config um and I think that's the only two files we need to move over so we will go to our old one here um and it has some elastic be stop uh stuff in here and we'll just take all of it that's totally fine and we will go to our new one here and paste that in and we said we need to set this as well so we'll go to our old one copy that paste that paste that there and now that we have those files in there I want you to do is go ahead and do a get status so we have three files that's great get add okay commit hyphen M Docker run and we need this we need a get repo because it's going to um create a new one when we run uh EB anit here in a moment so I'll just do get status make sure that all worked fine great and we will do EB in it so we're going to choose us East one so number one we are going to create a new application so press two we are going to stick with the name that we are given here so hit enter um we are definitely using Docker so we'll hit yes uh we want to use CIT sure so we'll hit Y um we need to select a repo we are making a new repo so press two enter the repo's name this is going to be called study sync external make sure you type it right uh we are going to want it to be Master Branch so hit enter um we don't need ssh in that's okay and so there we go um so now that that is created I'm just going to go double check and make sure that is the case so we'll make our way over to code commit and here we have our external repo so it's all in good shape so now that that's all set up we should be able to create a new environment so we'll do EB create hyphen hyphen single uh we will name it pretty much the same I'm just going to take off the dev part on it doesn't matter too much as long as you uh can remember what you said it to uh we'll say no for spot instances and what we're going to do here is just wait a little bit here it's nice to see the message just make sure that it's creating what we want to create um and this is yep a Docker image so hopefully this works uh first try I'm going to make my way back to Docker here we'll go over here and while that's going we can go ahead and terminate this one we don't need this one anymore and just to point out like look at this this doesn't contain any of our code so where is our code our code is part of the actual Docker container that's why we don't see it here because when we built it it copied it and put it into the actual container whereas in this setup the docker file is here and so we can work with our source code and have it all one place um so you just have to decide you know what workflow works best for you and you know if you can get away with just having a Docker file like that that's definitely better um and this is creating I'm just going to go back here and do a refresh I don't see this new environment yet should be called external right um study sync external oh here it is yeah because it's a completely new application that's totally fine so yeah I'll see you back here in a moment once this environment is done creating all right so our deploy is done but we have health degraded and it looks like we have an error here it turns out um elastic beanock can't authentic to ECR because we didn't give it permissions to do so whereas in Cloud 9 we had pulled that um the credentials and stored it in here so that we could read from ECR so what we need to do is update the instance profile um of the actual ec2 instance that runs uh here so what we'll need to do is make our way over to am so just type in IM am here we'll open this in a new tab on the left hand side we'll go to rolls uh we'll go to ec2 roll here for elastic beanock we're going to attach permissions we're going to type in Amazon ec2 uh readon container we'll attach that policy so this should allow us to uh gain access to ECR um and then what we'll do is go back to Cloud9 and we'll simply do EB deploy and so what that will do is it will just deploy again um but now it will also update the IM roll and it should have permissions this time around so uh we'll give it a second here to get started we'll make our way back over to here uh we'll do a refresh and we can see this is in progress and I'll see you back here momentarily and our deploy is done so I'm just going to close these additional tabs here and I'm just going to open up the new tab here and we are now seeing version three if you don't remember uh could be Chrome caching yet but uh there you go so we went through a lot of different variations here with elastic beant stock um and you know that is a lot of stuff but uh it is necessary to go all through these things so let's just go ahead and clean up what we have so I'm going to go back um all the way to applications and what we can do is go ahead and just delete these applications this should terminate all the environments so we hit delete and we're going to also delete um this application here we'll say delete if we click into this these should be terminating so our Cloud9 environment it's not a big issue it's going to shut down after 30 minutes when we're not in use for our um elastic containers we're going to go to uh code commit I don't think this is really an issue having these around so I'm I don't have much motivation to delete them and we might be using them for the um ECS and farget tutorials so we're going to leave these alone we're going to leave our code commit or our e or yeah code commit and ECR alone so we'll leave this alone as well um but yeah that's it for the um elastic beanock walk through so to really understand OS config we need to First understand two things and the first of those two is what is change management so change management in the context of cloud infrastructure is when we have a formal process to monitor changes enforce changes remediate changes and then the other thing we need to know is compliances code which is when we utilize programming to automate the monitoring and enforcing and remediating changes to stay compliant with a compliance program or expected configuration which brings us to ads config and adus config is a compliances code framework that allows us to manage change in your adus account on a per region basis all right so now let's take a look at use cases for config so when should you use adus config and I would say it's when you want a resource to stay configured in a specific way for compliance when you want to keep track of configuration changes to a resource or when you want a list of resources within a region or when you want to analyze potential security weaknesses uh and you need detailed historical information so just down below I just want to show you an example of an AR tal diagram so imagine you have a um a a rule and that rule is to make sure that your E2 instances configured it correct way you'll have a Lambda that will actually check to make sure that whatever your configuration is correct and then we have an example of another adus configuration rule that's checking an RDS instance and it's found a problem and so what it does is it remediates so it tries to correct the issue uh but we'll talk about remediation here in a bit but there you go so let's take a look at resource inventory this is the first feature you're going to encounter on Aus config because abis config is turned off by default and as soon as you turn it on it's going to discover all your resources and give you an inventory of all your resources within the region uh and this is totally free uh and this is a great way to find where your stuff is so here on the right hand side we have a a big list of resources you can see there's a 61 resources within our region here uh and you can use the resource inventory to figure out what you have running in an Abus region search existing or deleted resources uh recorded by Abus config find malicious ads resources that should not be running within your account and I have to emphasize that you have to turn this on per regen uh so you know that's something that's really important if you really want to have full visibility make sure this turned on in every region that you're operating in so now let's take a look at the most important component of AG config which are config rul rules and configure rules are just adus landus designed to check if a desired configuration is met on an adus resource and there are two types of configuration rules we have adabs manage rules and these rules are created by adabs so you just choose one that you want and you have custom rules so you can uh create and Associate a Lambda and Define it how you want it to trigger and so here are the two options there itus manage rules and create custom rules and we're going to look at both of them uh here in the next slides so now let's take a look at ads manage rules and these are predefined rules by ads with some level of customization and there are around 100 plus 8s manage rules to choose from so you got a lot of options here all you got to do is go uh into that uh manage rules box type in what you want so here I'm typing RDS and you can see there's a lot of rules that you can uh spin up there's one that says check whether the RDS instance has backed up enabled so that's kind of a cool thing uh you cannot change the trigger type but you can change some of the settings such as scope of changes or frequency when we're dealing with manage rules uh so here you can see the trigger type is commented out uh and we'll look at this uh in the custom rules so these will make a little bit more sense of the next one but I just wanted to point that out so now let's take a look at custom rules uh and with custom rules we're able to set the trigger type uh and so there's two types here configuration changes and periodic we're first looking at configuration changes and what we can do here is change the scope of when if evaluation occurs so when it will actually uh check the rule to see if things are compliant uh and so uh the default option is on all changes so when any resource is changed uh created or deleted this uh this will valuation will occur uh you can specify a very particular resource and you can uh add multiple resources uh on which ones they would happen on or you can specify tags okay and so you just provide a tag there the other kind of rule is um periodic and so you you can change the frequency of how often the evaluation occurs and so you can just drop this down and choose between 1 hour and 24 hours uh every time evaluation occurs when it's checking a resource uh it does cost money it is in the fraction of a cent but the thing is is that um evaluation could definitely be more cost effective than periodic especially if periodic it is a lower frequency um so that would be the reason why you'd want to do it also evaluation is a bit better because it's kind of reacting to changes on the resource so if an ec2 instance has been uh terminated then it's going to evaluate it right whereas with custom rules is always on a schedule so there you go take a look at the resource timeline and I really love this feature about abis config but the idea is when you click into a resource it's possible to see a timeline of changes and you can see two different kinds of timelines we can see a configuration timeline so when changes have been made to the resource and compliance timeline line so when a resource has become uncompliant with a rule and so down below here we can see that and uh we can literally see exactly when uh something was compliant and what it was not and then we can drill down and see what was actually changed this is super important uh for compliancy because if you want to prove that you're compliant at a certain time so that you are not liable this is what this feature is for so there you go let's take a look at remediation and and Remediation is the concept or the action of reversing stopping or correcting something I would say if you look at the word remediation just think remedy because that's where the word remediation comes from and that helps you make more sense of it you are applying a remedy to a problem or situation and so adus config allows you to remediate noncompliant resources that are evaluated by adus config rules and you configure a rule to remediate automatically or or manually trigger remediation so there are your two options and and so what you can do is you can choose a remediation action that should happen so if it was something that was happening automatically maybe the remediation is to delete the image take a backup uh enable cloud trail whatever that option is but that's a really cool feature to have so now let's take a look at aggregators and I want to pose a question to you what if I had multiple adus accounts and regions and I want to consolidate aggregate those adus configure information in a single account and that that's where aggregator comes into play because what it can do is it can actually collect all your aabus config data from multiple accounts and regions so all you do there is you specify the individual account IDs you can sa for the entire organization uh and then you just specify the regions so you can say I just want these particular regions or you can say include all future regions and aggregate will replicate the data from The Source uh account to the aggregate account so uh the data is going to be available in both places but it's going to uh be replicated into your aggregate account okay and the aggregator has three subcategories we have rules resources and authorization so looking at rules first rules represent your desired configuration settings and you will see rules across regions and Source accounts uh and so here is just an example of something that we could or this is an example of a rule I'm just showing you a manage rule here we already know what rules are just redefining it here then you have resources which are adus resources within your adus account so that could be easy 2 cadesa pipelines RDS databases and you will see resources across regions and Source accounts so that's really useful and then you have authorization and this grants permission to the aggregator account and the regions to collect adabs uh configuration uh uh and compliance data okay so uh there it's just saying who is authorized to share that information so there you go so let's take a look at conformance packs and these are collection of adus config rules and Remediation actions that can be easily deployed as a single entity in an account in a region or across an Abus organization and packs are created by authoring yaml templates that contains the list of adus config managed or custom rules and mediation actions and you deploy them via the itus console or a CLI and when they say yaml templates what they really mean is a cloud formation template I like how ads like will say something and you just find out it's something underneath um but that's great that they're CLA information templates CU they're easy to read uh and so it actually has sample performance pack templates which are actually really useful to use you can find it via the adus documentation but I just want to show you the names if you want to go uh look them up yourself so the first is Abus control tower detective guardrails conformance pack we have operational best practices for Amazon Dynamo DB we have operational best practices for Amazon S3 we have operational best practices for Abus best practices for the center of Internet Security nist CSF uh PCI DSS uh templates with remediation action and then you can uh it has an example of a custom conformance pack so there you go uh one of my favorite features of adus config and I hope you use it soon so let's take a look at Advanced queries for ad config and these allow you to write SQL queries to quickly find resources within your adus account and you can query for resources cross region or cross accounts so here on the left hand side uh what we have is a bunch of uh premade SQL uh queries so if you're ing what do you write well you can just go click on one of those and what it will do is open up uh this SQL editor uh that already is prepopulated you just hit run and there you can find your resources and you can export it as Json or CSC very very cool feature to have so there you go we're taking a look here at adus API so first let's answer what an API is so that stands for application programming interface and an API is software that allows two applications or services to talk to each other and the most common protocol that you can use is HTTP and so what does adabs use for their protocol well they use HTTP and so you can send HTTP uh requests at using a program such as Postman or or you could use Curl or wget and the idea is that you will get back a response of information about your Cloud resources so let's go take a look at an example of a request that we would send to adabs so here is an example and what's really important to make note of is the service endpoint so if you notice here I'm going to get my pen tool out so we can draw a little bit here um the idea is that we are using the HTTP protocol we're sending a get request so we're just looking to read information and then the idea is that every single service basically has their own service endpoint so monitoring probably goes to cloudwatch logs that's what I'm assuming and it's targeting it for us East one so there's that um the other thing is that you need to authenticate these requests or authorize them so what you'll need to do is generate a signed request so the idea here is you make a request with your a credentials and you get back a token and then you provide that token here uh in the authorization header the other part is that you need to supply an action and also parameters we don't show this exactly here because there's no room for it but the idea is that if we're going to go to monitoring we're going to want to Target something in particular uh say like list objects in S3 and then you might have additional information these are very obvious when we use the CLI or the SDK but uh here it's a little bit more obscure but I just want to make it very clear that the API is extremely important and it is uh the primary way that you interact with ads even if you don't think that you're using it you absolutely are so so the idea here is we have the API and we just mentioned the one use case that you can use uh to access it is by directly uh creating HTP requests this is not the normal way that you'll interact with it because it's a lot of work to do so adus has these other methods that you're going to use to interact with the API the first and the primary way is through the adus Management console that is that wizzy wig web interface that you do click offs through in order to interact with it and this is the primary way people interact with the API uh the other two ways um besides that one is the SDK so this is where you use your favorite programming language or the CLI this is where uh you use a shell program that you can pass it flags and get information back really quickly so if you want to be a good Cloud engineer I mean you can use itos Management console but you really should focus on your CLI skills and your SDK skills because those are going to be what you use uh day in and day out and the other part is that the adabs Management console can sometimes obscure uh the underlying apis and you don't really understand what you're interacting with so the closer you get to the API and you understand it uh the more successful you are going to be as a cloud engineer or devops engineer um so there you go hey this is Angie Brown and we are taking a look at the a CLI and I think that we should ask what a CLI is first and so a CLI stands for the command line interface which is for processing commands to a computer program in the form of lines of text so the idea is we have operating systems that Implement a command line interface uh in a shell so there are a few terms you're going to hear terminal console shell and I want you to know that these terms um they do mean a very specific thing but they get abused all the time so you're going to hear them in different contexts where they're going to mean different things but for what we're talking about this programmatic thing they're they mean very specific things so a terminal is the text only interface um it's basically the environment where you're going to be inputting and getting output okay uh then you have a console a console is a physical computer physic uh to physically input information into a terminal and you might be remembering that there's a thing called the adabs Management console and the reason why that's confusing is the console is sometimes used to describe web browsers that have web applications that interact with Pro programs okay so but the real definition is a console is a physical computer to physically input information into a terminal then you have the shell so shell is the command line program that uses uh that users interact with to in input commands popular shell programs are bash zsh Powershell there's another one called fish there's a few of them so when you look at this kind of graphic you usually think of terminal console shell um intermixed here so like when I say ter or console or shell we're generally thinking about this kind of visual where we have a a terminal so a text only interface and we are entering in um uh commands into a program so I mean this is Ms do so they probably have some name for their shell I don't know what their shells called probably MS DOS as well and so that is an example of that so again I want to point out that people often use the terms terminal shell and console um in different ways and erroneously but usually it's just to describe interacting with a shell so what is the ads command line interface well it allows users to programmatically interact with the a API via entering single or multiline commands into a shell or a terminal so here is an example of a uh command that's being entered into the a CLI and if you look here closely this part here the part where it says ads that is the command line interface okay that's the CLI and then it's up with commands and then sub commands and then it has parameters and it's all text right so the idea is that we input that and we get an output um the CLI is a Python exible program um clis can be written in whatever language um that people want to write them in but uh when they're packaged to you you might not realize that there's an underlying language it is good to know what the underlying language of the CLI is so that you have all the requirements to run it um so python is required um it has to be on your system to install the a CLI the CLI can be installed on Windows Mac or Linux or Unix uh the name of the CLI of the program is called AWS so there you go so access Keys is a key and secret required to have programmatic access to the AIS resources when interacting with the a API outside of the Management console so when we're talking about access keys we're talking about itus credentials and if we say it credentials we're talking about access Keys when we say access keys we're talking about both a key and secret is this confusing yes I know but I want to make sure that you understand that there is some variations in that terminology when you create a new user within your 's account you get to choose do they have pragmatic access if they do you're going to be able to generate out access keys for them when you generate out an access key you'll get a key and a secret technically the key part of it uh is not as uh sensitive as the uh secret but you should keep them both uh secret and treat them as super super sensitive never share your access keys to anyone only person that should use them is you okay never commit them to a code base if they go on GitHub or gitlab or somewhere else they can get exposed people can get in your account and that will be a big problem technically if you use GitHub or ads they have measurements which they will uh that they have there that will detect um if your key has been compromised and then they will lock down your account for you for your benefit but don't rely on that because you can still uh suffer damages to your uh your bill or to your resources if that stuff gets exposed you can have two uh access keys at a time um if you need more then you need to deactivate another one or delete them uh access Keys have whatever access a user has in the adabs resources so if you are an admid user your access keys will have admin user you cannot set separate permissions for individual access keys if you need that make a new user and then lock it down to exactly what you want it that kind of user is called a machine user where it doesn't represent a person it just represents um particular access that you want to get out so let's take a look at how we would actually save these access Keys locally well there's one way which is putting them in your credentials file this is a toml file which is expected to be in a very particular directory or folder uh on your operating system so if you're on Linux or Mac it's going to be located in your home directory that's what that Tilda is for um that's a shortcut to get to your home directory and then it would be in a folder called ads and then it would be a file called credentials the folder has a per in front of it because it's a hidden folder since this is secret stuff you don't want to make that easily accessible um this credentials file is uh pretty straightforward the idea is that you will always have a default section this is when you use the CLI or the SDK it will default to using this one if you need to have more profiles you can set them below with custom names and then the idea is that you will provide that profile uh when you specify that in your configuration but this is so you can access multiple credentials you could have a 100 in here and be able to quickly switch between uh projects or accounts to get the work that you need to get done um you don't have to make this file from scratch you just type in ads configure and then you'll be able to enter in your access key and secret and it will create this file with the default value there you can also provide other information like the default region and the default output format there is another file I can't remember what called called the configuration that goes along with it one's supposed to have the credentials in it the other one's supposed to have General configuration now this is one way of using your access Keys the other way is via environment variables and the SDK and the CLI will automatically pick up your environment variables I use this method a lot because I use cloud developer environments and I can't set this uh credentials file in it um and when you are working with applications you often are going to set environment variables cuz again you're not going to be able to set credential files so there are those two ways uh credential files is really great when you're in a local developer environment on your local computer it's just going to be dependent on your scenario but you're going to see me use Nars the most um so hopefully that is clear but uh yeah that is access Keys okay let's talk about API retries and exponential back off so when interact ING with apis over a network it's common for networking issues to occur for various reasons due the amount of devices a request has to pass through and points of failures you could have issues with DNS servers you could have issues with switches you could have issues with load balancers gateways there's all sorts of networking things that can happen uh so the thing is when working with apis you need to plan for possible Network failure by trying again basically sending the request another time it is an industrywide uh recommendation when working with apis to use exponential backoff before trying again what does that mean well the idea is that if we have a failure we'll try again in 1 second then the next time if it fails we'll try again in 2 seconds then 4 seconds then 8 seconds then 16 seconds then 32 seconds I'm not sure if you're seeing that pattern there but we got some math on the left hand side noce it's going by by a factor of two look at the exponential 2 to 2 2 ^ 3 2 ^ 4 2^ 5 okay um good clis and sdks already have this built in so quite often you don't have to do anything but to use the CLI or SDK but if you are not using them and you're directly interacting with the API via HTTP requests you'll have to code this in yourself and then often these cizer sdks will allow you to change the strategy for exponential back off so they might allow you to change um the initial value uh in terms of what it will back off on or other things like that okay Smithy 2.0 is iTab Us's open source interface definition language IDL for web services and it is a language for defining services and sdks Smithy and its server generator unlocks model First Development and forces you to find your interface first rather than let your API to become implicitly defined by your implementation choices so here is an example of this language and so um this is kind of like a mock example of how it would be used for S3 why am i showing you Smithy I just want you to know uh how these tools are built out and that um the uh the SDK the the python SDK uses something called service models and if and we'll look at that in other slide somewhere in this course but um it's basically will be a Json file and that Json file is based off what this language defines it to be will this ever show up on any exam absolutely not but why would I show it to you so you can understand how these tools are built um and maybe when if for whatever reason in the future you have to go digging in and change out a service model you might remember Smithy and then that will lead you on the right path okay service token service also known as STS is a web service that enables you to request temporary limited priv privilege credentials for IM users or Federated users so the STS is a global Service and all STS requests go to a single endpoint at ss. amazon.com you can use the following API actions to obtain STS so there is a bunch of um IM am API actions the most common being assume rooll and assume role with web identity when you look at trying to do cross account roles you're going to become extremely familiar with aume roll when you make a call to STS it's going to return back generally this information so it's going to give you back a temporary access key secret access key and its session token and its expiration so I just want to point out that STS is not hard but um it's definitely something you will want to remember because it is used quite often uh for a lot of scenarios so I just want to show you exactly how you would use STS so here's an example in Python the idea is that we are calling the STS client um and then from here we're going to assume a role which is one of the API actions that we can utilize and then here um it's going to return back the credentials and then we can just load the credentials back into the client and now any calls uh with the Bodo 3 the ads python SDK is going to use those temporary credentials um we also can use the CLI it's actually easier to use it with the CLI as we can just assume the role pass the rle AR that we want to assume get back credentials and then load those into our environment variables or our um CLI configuration file credentials file uh in order to do that so um it is a lot more secure to use use STS as opposed to using your Long Live credentials so there is a strategy where you could have a user and the user just assumes roles has permissions to assume roles but does not have uh permissions inherently to themselves and these shortlived credentials are going to expire after a certain amount of time and you're going to run into less issues there so there you go okay hey this is Angie Brown and in this fall along let's take a look at security token service also known as STS um so that we know how to generate out temporary credentials um I have this repository called it examples that I'm going to continue to use and where everything is going to be um I like to use git pod use whichever Cloud developer environment that you like to use but understand that my environment already has a lot of tools already loaded into it so if you don't have those tools you're going to have to install them yourself so we're waiting for that to spin up and now that we're in here um I'm going to go ahead and create a new folder here and this one is going to be for API and it'll be all the API stuff that I do uh I might make a video if that that is in the um that comes before this where this folder already exists so just understand when I'm making these videos they're not in a particular order I'm just shooting them getting them all done but hopefully you'll find where all the videos are anyway so I'm going to make a new folder in here and this is going to be for STS and uh you know we can authenticate with STS either using the CLI which is a very easy way uh but we can also um do this using the SDK but we'll just go ahead and create ourselves a new readme file so we can get started cuz there's a couple things that we're going to need before we even think about doing STS um the first thing is that we're going to need a uh a rule as we always need to assume a ro at least when we're going to use the assume rooll command for STDs um and so I think that will be the first thing we need to do so we'll say create a roll and maybe what we could do is use cloud formation for that because I always like to get us as much practice with everything else as we can so I'm going to go here and just say template. yaml and uh we'll go ahead and create ourselves um some cloud formation so we'll just go ahead and type in cloud formation and I'll just say uh template here as I want to just get the starting stuff for it over here I think it is and um looking for that introduction example working with templates here is template formats and so we always start with this template format version I'm going to say allow and then the next thing I want to do is put a description in here and say paste so what are we doing here uh create a rule for us to to assume H and create a resource will have access to okay so there is a couple things that um I want to create here the first is going to be the role so in order to create the RO we'll have to create that under a resource and you know again if you're not the famili with Cloud information don't worry just follow along the best you can as we'll get a lot of exposure throughout the course and there is a cloud information section where we will go into the stuff in detail so under resources we need to have something uh for a ro so I'm just type in cloud formation I am roll and hopefully we can get a nice and easy example here so on the right hand side I'm going to click examples and down below we have the Json example then we have the yaml example and so what I want to do is I'm just going to go ahead and copy this here this one looks good this says root rule but we're going to change that and I'm going to just paste that in here okay and so it says root roll but um we're creating a r to um be used for STS I'm just going to call this STS roll and carefully looking at this um let's do this for S S3 so we want to set up um our service to allow us to uh have access S3 and we're going to assume a role and then down below we need some permissions so in our permissions we'll just name the policy as uh S3 access and we're going to get access to a very specific S3 bucket and uh we'll allow it for all action so what I'm thinking here is um we need to place an S3 AR so I believe it's going to be ar colon 8os colon um S3 colon colon colon I'm just going to double check and look at what that format is so we'll say S3 Ron format format sorry and there should be an example here yeah looks like that and it is three uh three uh three colons and then the name of it so what I'll do here is just put a bucket name so the idea is that we're going to need to name a bucket I'm not sure what it is yet so I'm just going to put Sub in here for now and and I'm going to put up a parameter here and this parameter is going to be bucket name and uh I'm just going to say this is a type string okay and then I'm just going to give it a default value and I'm just going to say something Rand here I'm going to say STS fun and then I'm going to put ab and then a few numbers here on the end so we have a unique name and then using interpolation um I'll probably do doubles around this I think we want doubles around this or singles um I can just do this and then we get bucket name okay and so that should um allow us to get S3 access to this very specific bucket often when you're doing um uh permissions for S3 you might have to do both so like what we're doing the all action so we're going to give access to the bucket itself and then all the objects within the bucket so I'm just going to do that and that's going to uh be good for us and just carefully looking at this um yeah we can assume rooll here as our Act and then we have our S3 amazon.com so that looks all good to me okay so I guess the next thing is it would probably be useful if we were attempting to um uh get access to resources for a user that wouldn't normally have those permissions so the thing is is that like right now in my account I have loaded in uh administrator mode so even if I was to assume this role it's no different than me trying to access it because I already have full permission so to make full effect of this demo we should really create a new user that we're going to temporarily use in this uh here and that's going to allow us to simulate that so the first thing is uh create a user with no permissions so um we need to create a new user with no permissions and generate out um a access keys okay then we need to uh we need to create a role and a resource a a rooll that will access a new resource okay and so that's what we're doing via the cloud formation here and then we need to then um use new user credentials and assume roll so those are the three steps that we are going to attempt to do here um so let's go ahead and create our user so we are making Cloud information here and this looks like it's already set up so I think this is in good shape so next thing is to create a user we can obviously do that through the IM console I would like to see if we can make it through the CLI terminal here so I'm going to go ahead and type in it CLI am and in here we have create user so that's perfect we're going to go down to examples and so here we can create a user very quickly so I'm just going to go ahead and create a user this one is going to be called um what should we call this user um I'm going to call this one STS machine user I do this often uh is I'll have machine users which are for very specific use cases let's go see if we can go ahead and create that before we do let's uh check if there's any other stuff that we can add here because it' be really nice to um turn on programmatic access I don't think we do it here I think that's a separate command so let's go ahead and create this user we'll type in clear here and hit enter and so that's going to go ahead and create ourselves a user I'm going to go over to I and go take a look here had no issue with the hyphens there so here is our our our user STS machine user notice there are no permissions attached notice that we have yet to Grant it any um or generate out any access keys for it um so we will go back over to IM and let's see if we can get it credentials so I'm going to type in credentials here and see what I get oh it actually has access key so that seems to be more what we would want I'm going to go down to examples and it's as simple as that okay I keep expecting this to be a little bit more complex but it is not um I'm going to assume this is going to download this to uh somewhere here actually it outputs it here so one thing that we can do is we can make sure that we um uh get the output in a way that is more friendly so I'm going to just change this to table just say output table and then that way we can grab these values here so I'm going go ahead and copy this and and hit enter and so now we have these values of course you're seeing mine but do not share your values with anybody else as that is not a great idea um I want to uh temporarily apply these environment variables so I'm just thinking about this for a moment um so I don't want to use them right away um I'm using again uh environment variables because in Cloud developer environments they don't really like using uh credential files because when you stop the environment that file will be gone but for this case I think we can actually go ahead and and create an uh a credentials file and so that's something that I would like to do um so what I'm going to do is just type in ads um configure and I'm actually going to enter this stuff in in place here so we have this one here whoops right click copy just going to paste it here make sure I have it just to make this a lot easier okay and I'm going to go copy this and hit enter and then copy this and hit enter the default region we'll leave it alone output format we'll leave that alone and so that's all set up and I'm just going to document that so then we'll say um adab us configure copy the access key and secret here then edit um uh credentials file to change away from default profile okay so I'm just going to type in clear here and if I type in STS ads STS um get caller identity we can see who we are so notice here that I'm it as examples I'm not this user and um uh test who you are okay and so the idea here is that I want to just make sure I am who I think I am and so I'm going to go ahead and type in profile I'm going to say STS so what we'll do is we'll go and update that uh profile file and that lives at uh it should live at your root directory here so if I type in I think I can type open here and actually open it right in place if I go here and type ads and do credentials we should be able to see that go ahead and paste that in up here okay I we'll go ahead and hit enter we'll notice that we have those values I'm going to go ahead and change this to SCS and so now we have a profile called SCS um that we can try to work with so what we'll do is go ahead and copy this hit enter and notice now it's using that machine user okay this machine user again does not have permission for S3 so if I go here and type in a S3 uh LS and we do profile STS notice I don't have access to it right make sure you don't have access to s three okay so we should see something like this so the idea is that by creating this role then we can then assume it and be able to access that bucket um so what we'll need to do is create ourselves a new bin file or sorry new folder here called bin and in here I'm going to make a new file here called deploy and we're going to want this to be a executable bash script so somewhere in here I already have some existing code to get this Top Line I never remember what it is you think I'd know by now and so that's going to make this uh this whole script uh be interpreted as bash and so the idea is that now we can go ahead and uh set up our deployment of the script so we'll go back over to the CLI command and then from here I'm going to go to ads and we're going to search for cloud formation and then there should be a deploy deploy command here there is like a create stack one that's the old wave doing it they um uh made this one here and it's it's the recommended way to utilize it so I'm going down to examples here is an example here I'll go and paste it in and what we'll do is bring this down a line and then we'll use backslash to bring these onto new lines okay he and uh we don't need to pass any parameters we don't need any tags we will need a stack name so I'm going to just call this um my SS fun stack you know and then the template Json is going to be up One Directory so I think that we need to do that I always kind of forget um it's either going to work or it's not we'll just tweak it and so that should deploy it it might want permissions in order to deploy the stack there also might be something wrong with our CLA formation file but we'll find out here in a moment let's CD into the API STS directory here I'm going to type in clear and uh let's go ahead and see if we can well before we can execute our script we need to make it executable so we're going to chamod that with a u plus X on that that's going to um put this X on here so if we look at our I have to actually CD into the directory otherwise you're not going to see its contents but if we look at this here notice that there's an X here that's what we did we added that X to make it executable so now if we do period SL bin and then then do deploy it should be able to deploy I'm assuming that there's something wrong here um so it doesn't know where uh this file is so I assumed it was relative to this file but I think if we just take this out here it will know where to go you can hardcode or tell it to uh look relatively or absolute in this file sometimes I do that I'm just trying to keep it really simple here so we'll just hit enter but if we try to use this script in other places we run into issues this is an example of where we need additional permissions so this happens um quite often with um confirmation um deployments a lot of times I'll just leave these permissions in place so if we go over to capabilities this one has to be capable of some level of um stuff so there's it's either between capability or named am and I mean we are creating an IM rule so that kind of makes sense this what wants I am so what we'll do is we'll go here and copy this why doesn't seamlessly work I don't know I just I kind of forget I just kind of get used to just pasting this in and making sure it works works so we'll go ahead and paste that in here I'm going to type in clear we're going to hit up do bin deploy again and so hopefully it will uh it will deploy or will tell us what's wrong with our CL information script as we didn't validate or do anything else with it so as it's deploying let's make our way over to cloud formation okay and uh it's in progress we're going to give this a refresh here we'll click into it uh we'll go to resources often I will do this or I'll go to events and I'll I'll click here if there's a problem it will show up in here I think um clation is getting better at detecting root cause which is really nice but apparently my role was perfect I didn't have to tweak anything there so that's really nice um let's go and take a look at that role so if we go to resources here we should be able to click through to it and if we go here we can take a look at the trust relationship so this is what we wrote in here earlier so if we go back to our code I wasn't 100% confident but um oh it is this is the um this is the trust policy here so see here we are saying we want to trust that we have access to S3 and that we're allowed to assume uh assume Ro to S3 okay so what I want to do now is uh get access to that Ro um I guess the question is do I need permissions to assume roles because this says that we can assume R to S3 but my current user even make use of this and that's what we're going to find out let's go also check if that bucket exists oh you know we didn't even create a bucket here so we have a bucket name but we never created a bucket so let's go ahead and and create that so I'm going to type in um bucket uh cloudformation okay and we'll go over here and on the right hand side we'll go to examples and we'll copy this simple example and then we'll paste this above our rule all right and then from here I need to reference my bucket name so let say ref bucket name this one uses substitute because we are substituting with in a string uh we're using reference here I think buckets by default return um the bucket name we always have to check when we use a reference it is using the bucket name sometimes you have to use get uh get at for get attribute to get what you want in this case it's totally fine I don't want to retain the bucket I just want it gone when we're not using it so I took out the retention um there I'll type in clear and we'll go ahead and deploy again so this should start automatically deploying I normally will tell it to not automatically approve change sets as I like to go in here and approve them manually to make sure I know what is changing but for this small example it's not a big deal we'll go to stack info we can refresh watch it deploy we can go to events refresh watch it deploy buckets create usually pretty quickly here but I'll just pause here and I'll wait till it's done okay literally uh 1 minute later and it is completely done no issues whatsoever um so our bucket is uh now deployed if you had issues with your uh bucket just understand that these names are unique you might have to fiddle with this to get this to work correctly um it's interesting it's should be done here but for whatever reason it's just taking time to uh finish back over to here so notice there's a bit of a delay so we know that this bucket is up to date I don't think it's going to hurt there we go it just took a bit of time I'm going to go ahead and type in clear and so we have our bucket we have our roll okay coming back over to here and so the next thing we need to do I'm just going to put that command here so folks know how we did that chamod U plus X um bin deploy it was Bin deploy okay uh but anyway um now that we have it we need to assume our rule the easiest way is to use um the CLI and so I think that's what we'll do we'll go ahead and type in ads well we'll just scroll up here make sure we're on version two and go over to ads you'll notice that sometimes I use version one by accident but for the most part it always works I don't think I've ever ran into an issue with using version one but you really should use version two of the CLI but in here we'll have an assume roll um maybe not there and as per usual Sam is interrupting me as I'm recording not a big deal I just put her on uh do not disturb but um here there should be something for SS I'm searching SS and I'm not getting it maybe it's an isolate service let's go ahead type that it is it is okay so it's not part of of I am that's fine and we'll go to assume roll and we'll go down to examples and here is our example go ahead and copy it we'll go back over to here we'll paste that in and so in here we want to get the RN for for this from CLA formation so we'll go back over here go to our resources and we'll click through to the r and from here we can grab our R it's over here just click the copy button go all the way back here and we'll go ahead and paste this in so now we have our Arn uh we can name the session I'm just going to call this s S3 STS fun so we know what it is and so I'm hoping that this is going to work the other thing that matters is that we have to specify that we are using a profile to do this um that's very very important otherwise this is not going to work as expected I'm also going to double check no that that profile flag will work with assume I I believe we'll go ahead and copy this and what I want back is um is uh what do you call it um uh uh credentials right so because that's how this works is that we're going to say hey I want credentials give them back then I want to utilize them so the issue that we're having here is that it says we're not authorized to perform the assume role so we do have assume role in our template here for S3 but that's for this role to be able to access S3 not this user so in order for this user um to be able to um access um this Ro is that we need to have the assume uh the ability for it to assume roles and specifically for this AR so what I'm going to do is go back over here and I'm just going to uh give it permissions to do so so we're going to have to um basically give it permission um maybe there's an easy way we can add an inline policy so let's go take look at our commands and IM IM and see what we have available to us so we'll go here to ABS we'll type in IM am and then we'll go to here and I'm looking for a way of attaching an inline rule there must be some way inline or policy I'm just carefully looking here um maybe it's attach because we can do this via the um uh via the consoles attach an inline policy like if I go over here and I'm uh if I go over to my am user I can just add an inline policy right and this is the easiest way to do it um so I'm thinking that it's probably attached user policy attaches the specified managed policy to the specified user um I don't want to attach a specified policy I want to attach an inline policy so I'm just going to carefully look here and see what I can find so I'm not sure let's go ask chat GPT so I'll just say uh how do I attach an inline policy to a user using uh the a CLI let's see what we get so yes we of course have to create a policy document first then we need to attach the policy with put user policy okay I didn't see that there put user policy ads or updates an inline policy that's perfect so we'll go ahead and grab this we don't really need chat gpt's uh suggestion there we just stick with the code over here and go ahead and paste this in we'll type in sh here I'll go down below and so now we need to specify a policy I'm going to go ahead and add one into this folder uh file so we'll say um policy Json and so in here we need to uh create a policy that is going to allow us to assume that specific role and we need to specify this user this user is called what SCS machine user so paste that in here like that okay and so you know we're going to assume that user and then assume that policy but we need to write out what this policy is going to be um so let's go take a look and see if we can find that structure so say uh it policy IM structure and I do not have it memorized even though I've written policies forever and ever so what I'm looking for is resource policy maybe it's identity policy so we just want to assume well be able to assume a role we can just take a look at maybe what we have in our cloud formation to quickly find it I'm just trying to avoid using chat gbt as it's so easy to utilize it but um let's see what we have here because we always have to have a statement that's for sure a statement and in that statement we'll have an allow an effect allow and um we want to allow it to have the action of STS assume role I believe and then we want to specify the resource and the resource resource is going to be that specific AR so down below here yeah it says SCS assume roll assume roll and then this very specific resource right okay so that's the statement let's go take a look at a full example of an IM policy example policies any of them maybe there's one for STDs in here no there's not but we'll look at any of them I don't think it really matters which one um so we have our statement we probably want our version in here okay and so I think that is sufficient right like be able to assume the role of this uh resource and allow us to do that you can have um multiple statements so this I think can be an array as well what we'll do here is just type in clear down below and we'll go back to our read me and we want to go ahead and put this so I think we wrote it here fingers crossed that this works we'll hit enter and it looks like it's attached it let's go take a look at our user and confirm that we should always be confirming and so we have um our machine user and there it is notice I had to refresh a few times normally it of us shows things instantly but for whatever reason it wasn't there and so that looks fine is there an issue I don't know we'll only know when we try to attempt to access um the assum roll so let's go ahead and try this and again remember we're using a profile here so this user is very limited in terms of access so it says an error occurred when attempting to assume the role operation uh is not authorized to perform assume role on the resource here okay but we did attach to it this here so I'm wondering what else it wants so just give me a moment okay so I checked with chat gbt and it actually produced the exact same thing so it seems like we should be on the same line one thing I thought that is that maybe um you wouldn't be able to use an inline policy because you don't specify the trust relationship the policy will allow the specified user to assume the specified role provided uh provided that the trust relationship of the role also allows this user to assume and ensure that the R's trust policy includes a statement that allows the user of the user's account to assume the role okay so maybe that's where we're running to our issue so if we go back over to our Cloud information I mean that makes sense right if we go back over to our cloud formation this assume ru allows particular resources to assume it and so it's saying that s S3 is allowed to assume this but what we really need to specify here is actually um uh the user as the principal so I guess my thought here is that can we leave S3 amazon.com in here probably and we can specify multiple principles so let me just try something here um I'm just trying to think about this here because we can get the users AR right so we can go here and we get our our AR here and technically uh I am lets you put different principles here like rot C like root user users other stuff like that I just don't know if we need this that's what I'm not 100% sure about but we'll give it a go and we'll see what happens and we'll see if that works because now this is saying like these are the people that are allowed to assume the role and it says that um the SS machine user should be able to right and I guess this would make sense if we had a bucket we attach permissions to it we can't exactly do that um but anyway so it looks like this is changing so we'll go ahead and give this a refresh we have a failure so clearly I wrote the script wrong and we'll go take a look and see what it's complaining about events sorry events so it says here the resource Handler message is invalid so this is not possible at line 145 that's not really useful in terms of the formatting so not exactly sure what it's trying to say there what we can do is if we're trying to get this to work we can go ahead and try to open up that policy that that we have for that rule and we could probably just kind of fiddle with it so this rule is called S3 actually have to go to the rule because we're mucking with the um the trust policy here so we'll look for this one here what's that rule called my STS fund stack so we'll go here and type in my STS under roles my STS fun stack and we'll take a look at the trust policy and let's see if we can just edit it okay so we have AUM roll and we'll go ahead and add a principal and we'll choose the principal type as an I am user and so here is what it's expecting um I really don't like this because you have to supply those values yourself but I think the old editor was a lot nicer we'll go here and I will just place that in here and then we need to get the uh machine username which I already forgot which is this right here okay we'll say add principal oh and it adds it as AWS so maybe that's what we had to do I could have swore that you could add it directly but maybe that's not the case so I'm going to go back here and we are going to um bring this down into a new line so this one's going to just be ad ofs like that and so hopefully that will work we'll go back back down below hit bin deploy and we'll see if it takes that okay so we'll just wait here and see what happens I'm just going to keep refreshing I'll be back here when there's something okay so our update is complete and uh I mean we have our permission in place we'll go back and take a look and see if we can now assume that roll so if we run this one here we do copy and we hit enter paste it actually works so notice that we didn't need to um we didn't need to have this S3 Amazon aus.com because again the the person that is we're trusting to be able to have this role is this user unless we run into something that I'm not aware of but we have our credentials here and um we'll have to extract those out temporarily so what we'll do is go down below here and I only want to set them for the time being so we're going to go back and well first we'll copy these three here okay we'll paste them in and what I want to do here is I want to go back and open up our um credentials file that did not copy right just type clear here and I'll paste that here and so what we can do is we can now add uh our profile that is assumed okay and so that should give us some access to our S3 uh bucket that we had there so I'm going to go ahead and copy this and we'll make our way back over to here and we'll paste this in here and then we'll bring this all back to the wall there we go um this one's obviously going to be called Abus AIS key ID this one is going to be called this one and I'm not sure what it is for session to to so we'll go take a look here we'll say anab Nars and it should get us the most common ones we need so we go down here and I want the one for session and I believe it's just called yeah it's a session token that's what we need so we'll go back here and we'll type in ads session token in put equals here equals equals and this one doesn't have doubles around them so I'm just take the doubles out of here double quotations okay and so I'm hoping that this works I'm go ahead and type in clear and we will get rid of these and now what want to do is test this and make sure it works so go down here below and paste this in and this one is called assumed assumed we'll go ahead and see what we get and notice that it is now uh S3 STS fun that's the name we named our rule session so that's how we know that we are uh that and let's go ahead and see what we can do with our bucket so we'll do this and we'll do a test to list out buckets and see what we get uh we want to make sure this is the profile I hit enter so it says list bucket operations access denied because I don't think we gave it permissions to list buckets I or maybe we did I thought we gave it access to everything so we go here and um we have permissions for everything for all possible actions not even just anything but everything so like that'd be S3 colon asteris so maybe we do need to have the service here uh so that we can assume permissions for for that so let's go ahead and change this I did change this to S3 colon Aster because that is a little bit better that's not why it will work but it might be because we need to add the service in here and so I'm going to go ahead and try this again I don't know if it'll take effect for the existing permission so we might have to attempt to regenerate out the temporary permissions and try again I'm going to go back over to here I'm going to give this a refresh and we're waiting for this stack to finish I'll be back here in just a moment okay all right so our permissions have uh or should be updated we can double check that by going over to that role okay and we're going to look at our policy and that looks fine we'll look at its trust relationship and notice that now we have S3 amazon.com so let's go ahead and see if that was what we needed I'm going to go ahead and go back up and hit enter and notice it says list bucket operation access denied so the question is okay we have this in here we have this in here and the policy says we're allowed to access um this specific bucket but I wonder maybe if it doesn't work because list buckets is for listing all possible buckets so maybe something we can do to rule that out is to look at this exact bucket and see if we can actually list that specific one so if I go here and I have LS um we can go get that bucket name which is up here and we can see if we just list the contents of that bucket okay so we'll hit enter does that work and so that actually does work um it's showing nothing because there's nothing in that bucket so I think the reason we're getting this list buckets operation is that we actually need a wild card here um for all buckets like this so if we want this to work we'd have to actually do this for the list bucket action and what we'll do is we'll take out this uh here again because I just want to make sure that we understand that we don't necessarily need that so I'll go ahead and hit up so just understand this worked accessing that the bucket work just not list buckets so we'll go back up here and we'll go ahead and do a deploy and we'll go back over to here and we'll give this a refresh and I want to see if it's updating so it's in progress so we'll just wait a moment for this to finish okay all right that is up to date and we'll go back over here and we will try uh this again so we'll try to list there no issues with us removing the S3 amazon.com so we didn't need that let's go take a look if we can list all buckets now and we'll whoops that didn't exactly work how I wanted it to but we'll try this again LS okay I'll hit up again here and then just try this again so I just want to clear this part out here I'm not sure why this is mucking up but I'm just going to copy this uh from here type in clear sometimes that happens the uh terminal starts acting funny so now I'm able to list it so it was an issue with this here and again nothing with that um so yeah hopefully this is starting to make sense to you in terms of the Assumption roll could I made this a lot easier by doing click Ops in the console and just running as simple C command absolutely but you know when you're working with Cloud you need to have all these disciplines and so I just want to take you through as much as I can in terms of uh difficulty here uh but anyway we're done here so let's go ahead and clean up so to clean up there's a couple things we'll need to do the first is to get rid of that stack we can just go ahead here in cloud formation tear it down this is normally how I tear down Stacks I just delete them in the console and so that will delete and while that's deleting we need to get rid of um a couple things because we did create a few things that we don't need just here go here and say clean up say tear down your CL formation stack via the adus console Management console and then there is a couple things that we create I know there's a spelling mistake there I do not care we created a uh this user and so I'm not sure if we have to delete the policy cuz it's in line but let's go ahead and see how we can get rid of that user via the uh CLI so I'll say a CLI I am and we'll go here I'll make sure we're on version two I'm going to look for delete do we have delete user we do here so that is good we'll go down to examples and we should be able to delete it and uh you your policy or your credentials you can delete that out of your credentials if you like as well I don't really need to because this environment is going to refresh when I shut it down which is the advantage of having a cloud developer environment but I will show you how to get back into that file just in case you forgot and here it says cannot delete entity must delete the policies first so I guess we have to get rid of the policy um we'll go back here and there's probably like delete oops delete user policy go down below grab this here um don't remember what we called the policy but I'm sure we can find out very easily this policy was called this okay so we'll go ahead and delete that and then we'll go delete that um when calling delete user cannot delete entity must delete the access key first okay so now we got to delete the access key access key watch it'll be like you have to deactivate it first I'm sure it will say that but we'll see what we can do anyway um we need the access key value there so actually we have a reason to open up our credentials file I'm going to go up here and find that line that allowed us to open our credentials here it is and um we need this access key right here and again I wouldn't be surprised if it tells us to deactivate it first as it likes to do that quite often can try to copy and paste this again it just messed up there we go and then I need the username and we'll paste it in here as such okay we'll bring this over copy that hit enter so that's now gone actually was nice that it didn't tell us to deactivate it first a lot of times in the console tell us to do that and so we're able to clean up so there you go uh for our credentials file I'm just going to go ahead and clear this out um and I'm just going to commit my code here make sure I'm not committing anything sensitive double triple check here just make sure you did not put credentials in here by accident especially when you're working credentials double check here I mean this doesn't really matter it's not really sensitive information and it's also deleted so it doesn't matter I'll go ahead here and just say STDs example there you go and I'll see you in the next one ciao when you send an API request to ads you have to sign the requests because that's going to identify uh who sent the request datos so signatures do two things it prevents data tampering and it verifies the identity of the requester um just so you know when you're using the CLI or SDK requests are signed for you automatically the only time that you would be um uh signing requests manually is that if you're using the API uh directly through HTTP uh as I've said before I in general use you do not want to do that unless you have to for some um very specific use case because you cannot use the SDK or CLI um just so you know that not every single request requires signing so for Amazon S3 there is an ninous request like say when you have um a public bucket and you just want people to be able to read files from it um and there are some API operations uh such as assume rooll with web identity where you're just not going to be passing on sign requests because that is um SCS is for used uh for getting credentials so at that point you wouldn't have any credentials to sign with uh so to speak um here is an example of a signature being used and so this example is for um uh or within a query parameter there's a few different ways you can specify it uh but that's where it would go and so when you use S3 or other services and you expect the URLs you're definitely going to see the signatures there and that's what it is um it just has two different protocols for signing version two which is the old one nobody uses this anymore and then we have version 4 let's just take a quick look at um how to sign uh on version 4 so we just have a general idea how it works okay so this is the diagram that adus gives us for uh adus signature version 4 I'm just going to tell you I've never ever had any reason to ever do this so I don't have the Practical experience on this but I understand what's going on I just want to convey that to you so you have a general idea what is happening here so the idea is that you need to First assemble a string to sign so you're going to get the data that you want to pass along uh that it was is going to use to verify you um I'm sure there's very specific information they want you to provide there but the idea is you're assembling that string then you're going to uh sign uh use a signing key using hmac uh for the uh string to sign the key thing to remember here is that it's using a secret secret access key in version two they use maybe a combination of both I don't remember but the idea is that you're using your secret access key um in order to generate out that date key and then the date key is used in the the region key and then the region key I don't know if you see this but it's kind of cascading where it's like here to here here to here and then finally that one is used to sign the key and then down below you have your signing key and then your string to sign so uh your string design is really actually all the way down here it says step two but it really is uh here in step three uh and that's going to produce that final signature that you can use um depending on the type of request this process might vary a bit um in terms of how that works so it's based on your request type the most common one is in authorization headers it could also be in a post request or query parameters they more or less look the same in terms of um like if I showed you example photos but uh the one we saw in the last one was in the query parameters or I would say query string um but yeah hopefully that uh makes sense there again in terms of practicality you never have to do this but it's good to know that there's a signature there and when you look at um your L's you know why that thing is there okay to connect pratically to naus service you need to use an endpoint and an endpoint is the URL of the entry point of an adabs web service so these things are very important to know and they do have a default but when you understand what they are underneath you have more flexibility to change things out so uh there is a general format that service endpoints utilize they're going to vary per service because service teams have flexibility to deviate from the norm but more or less this is what you're going to see so it's going to look like this where you have a protocol I'm just going to get my pen out here so I can mark that a protocol a service code a region code and then the ads uh Amazon ads.com domain and for security uh generally TLS version 2 is expected or an older one might be supported I believe the latest version of TLS is is three so that's generally what should be used but there is some uh things to know there if we wanted to see this filled in here's an example of cloud formation so we have cloud formation and this is operating in you East 2 and we're using the protocol of https generally everything is using htps there could be a case where HTTP is allowed or maybe even other protocols um that I'm not listing here so there are generally four types of service endpoints there could be more than this but four is what I was able to boil it down to we have Global endpoints these are a services that use the same endpoint then you have Regional endpoints this is a services that require you to specify a very specific region you could have fips endpoints so some endpoints support fips for Enterprise use fips is a um a a method of using cryptographic keys that use envelope encryption then you have dual stack end points this is where they support both IPv6 and ipv4 a lot of services are moving towards dual stack because they want to utilize the IPv6 address range and that will probably be the future for most IP uh endpoints then there are types of service endpoints that be combined so you could have a regional plus fips plus dual stack uh so you can see there's a lot of variation there ads Services may have multiple different service endpoints and the a CLI and SDK will automatically use the default endpoint for each service any this region and I should say that you can change the endpoint out for the CLI um or SDK via a configuration option um but yeah so that is a general overview of surface endpoints now let's take a look at at the four major endpoint types okay so a regional endpoint requires a regional code and a regional a Services have Regional endpoints makes sense to me this is the general format we have the protocol the service code the the region code it's what we just saw in the last slide let's look at some examples to make sure it's very clear so here is an example where we have different services with different um uh service codes with different region codes and here are examples of regional Services ec2 S3 code build SC s Sage maker it's interesting because if you were to use S3 you would think that is would have a global end point because in the console in the top right corner it would say Global but the thing is is that all data for S3 is stored in a specific region so it has Regional endpoints some Regional services will accept a globall like endpoint that will route to the default region so ec2 has this for some reason um and that will route to us East one uh Us East one is basically the the most default region of all regions and that's where billing is done so it makes sense that it would go there okay a global endpoint does not specify a region and Global adaba Services have Global endpoints so looks very similar to the last but notice that it only has the service code here is an example of a bunch of global endpoints like R 53 am Global accelerator Cloud front there's not that many Global uh services so there's not that many uh Global end points in the documentation Global endpoints still indicate a region so that's kind of interesting uh cloudfront is always going to list Us East one at least in my observation and then R 53 is uh is going to use the same endpoint but it will list all the regions out to indicate that those end points will internally route to specific regions so I just wanted to point that out to clarify by why the documentation has regions in there but they're still called Global endpoints okay a fips endpoint is a regional or Global endpoint that supports fips fips endpoints are available for specific services and specific regions so here is an example of fips being used notice that there can be some variation so in the first case we have ec2 hyphen fips uh then the second one it says API hyphen fips do sagemaker for sag maker so you can see that there is some variation in terms of how fips will be used using a fips endpoint may be required by Enterprises that interact with the US government and possibly the Canadian government because it's a standard created for both of them fips stands for federal information processing standard uh publication 140 hyphen 2 it is a us and Canadian government standard that specif IES the security requirements for cryptographic modules that protect sensitive information fips is something you should absolutely know when working in the cloud because it is often used um so just make note of that and you'll see me talk about it multiple times uh throughout courses so let's say we wanted to go ahead and use a different endpoint we would just specify this flag hyphen hyphen endpoint hyphen URL it's for every single CLI command you can overwrite it so here we we could provide um fips uh for KMS when you're using the SDK you specify an environment variable so here you would use inab usor _ fips endpoint true and then it would start using the fips endpoint and figure that out for you okay let's talk about troubleshooting end points there are a few different uh networking uh programs that you can use specifically for Linux um and I suppose you know some of these are also available for Windows that you can use and that is tell net NS lookup and ping and I really recommend that you learn to use these things even in their most basic form because they're going to be so valuable to you because networking or Cloud networking is a very large component of working with the cloud uh and sometimes you'll want to use these things um not just for service endpoints but for other networking things okay so let's talk about tnet so tnet provides a command line interface for Comm communication with remote devices or servers um so the idea here is that we're we could use it to say can we establish a connection so you can see that I'm providing a service endpoint and then the next part is 443 um tet's really cool because it was used back in the day to connect to various online games like muds and so you can still do that today and we should take a look at tnet because it is pretty cool then we have NS lookup this query Internet domain name servers uh we can utilize this to say is there a routable path so notice that I type in NS lookup and we have s3. amazon.com we get back the server uh we get uh other information so we can see the path of what is happening then we have ping this tests and verifies if a particular destination address exists and if we can accept requests so basically we're saying we sent something did it send anything back it's called a ping so imagine it goes and it goes ping and you hear the Ping coming back so notice we type in ping s3. amazon.com and we get a ping and data starts flowing back so those are three uh networking uh programs that we can use and yeah get a little bit practice with that okay a dual stack and point supports both ipv4 and IPv6 traffic why do we have this well um all endpoints originally were ipv4 but ipv4 uh ran into an issue where we're running out of IP addresses and so IPv6 addresses that by having a larger address space so there's literally more addresses to go around in fact we can barely ever run out of them there and so basically dual stack will possibly be the future of all end points on AWS because it's not going to have an issue how you specify dual stack end points greatly varies for some weird reason for S3 there there's two different ways because you could either have virtual hosted style format or past style format but basically they have the dot dual stack somewhere there in the subdomain but then for E2 they have api. abos so I don't know if ads wants to move all their dual stack API addresses over to api. at some point but for whatever reason that's how you uh get dual stack for ec2 if you want to specify um a dual stack endpoint you use that endpoint or uh hyphen URL flag this works for any time you want to change any endpoint to anything else you specify that flag it's it's a global flag for all CLI commands um but you know I just want to show that example there there does not appear to be many dual stack end points at least while I'm looking at um the end points right now but I imagine that they will get them for everything eventually dual end uh endpoints do not seem to appear on global endpoints or maybe they're already happening underneath so I'm just indicating that when we look them up I'm not seeing the world dual stack or they're not really indicating so they might already be happening underneath okay some ad Services have feature specific endpoints with quite a bit of variation why it is because every team ATS gets to do whatever they want and you get weird results so when we look at rev D3 we have we have R3 we have R3 domains R3 resolver it could be that um these were added as uh features to a service later on and so they basically are um an umbrella of services that are similar uh for S3 we have S3 and then we have S3 access points and then there's the case where we have S3 control um again these were features that were very large and act as their own services but are under the S3 umbrella so their end points are a little bit different there then you have sagemaker and sagemaker has all these subdomains so they do a api. sagemaker runtime. sagemaker edge. sagemaker so just understand again we got teams at abs and they're just doing whatever they want to do and that's the result we get okay all right let's take a look at service endpoints um and I basically just want to show you how you can swap that out when you are working with let's say the CLI um I imagine the SDK there's probably a similar process for swapping it out but um and we could look that up but uh the idea is that if we were to go to to let's say S3 service endpoints okay we should get a list of different endpoints and we can swap them out if we look up the adus CLI we just say version two I'm so tired of getting version one there we go uh if we were to go to uh the ads logo here and go to command reference you're going to notice in here we have an endo URL this is a global option meaning that every single thing you use uh any CLI command you you'll have you'll be able to supply an endpoint URL to override where it is going to um so what we can do is we can uh swap it out for a different S one but I think what would be more fun is to try to use something like local stack so local stack is this um emulator that emulates the the adabs um API so that you can basically do local development without actually using adabs so MOX the adabs API as you can see we can try to swap out that endpoint so I think that would be a fun thing to utilize and we can load this in as a Docker container so let's go ahead and see how we can get started with this and if we're on Linux it looks like they have a quick command here to install so let's give this a go and see what we can do the quickest way to get started is to by using the local sax it allows you to start local Sac room crand please make sure that you have a working Docker environment um this has Docker in it if I type in Docker so understand that if you want to do this you're going to need Docker installed that is a big pain in local developer environments please utilize a um a cloud developer environment for this it'll make your life a lot easier like get pod or code spaces or maybe even Cloud9 but uh before we get started I have this little relaunch to update that's kind of buing me so I'm just going to go ahead and do that and be back in just a second all right there we go so um let's take a look at how we can install that my Chrome browser is still trying to get responsive here and I'm just going to close this tab out and let's just take a look here so the quickest way to get started for command please make sure you have Docker installed um okay so use the do c to manually start local stack container I'm just going to go with Linux here and uh we should be on X 8664 so that's what we're probably using so I'm going to go ahead and copy that apparently there's a home brew command as well I kind of like that I'm going to go ahead and use this because Homebrew is installed here we'll make a new folder and we'll just call this local stack and I'm just going to create a new readme file we'll go ahead and paste this in here so install local stack CLI and the folks from local stack are super nice they're always trying to get me to utilize their um service because you can use it for I mean parts of it for free uh when learning cloud it's just um some of their services are behind a pay wall and some aren't so it doesn't meet all use cases for learning that's why I don't uh try to use it more than I can here um so now we have local stack CLI installed the question is it running I'm not sure we'll type in local stack C I'm going to assume that's the command for it no um let's just follow along and see what we can do here so we have the stack installed okay what's the command I forget is it just local stack maybe it's just local stack local stack oh there we go okay great and so what I want to know is if it's running or not so they probably have like a status no but they have a oh yeah they do okay local stack status is this running it's checking and by the way if this is a Docker container we're in G pod we can go here on left hand side and if it was running it should probably show up here on the left hand tab so I don't think it's running so what I'm going to do is just kill that I did control C to do that we'll type in uh local stack start and we'll see if it starts up now so it will start up here and I'm just going to wait until it's ready okay all right so it says it's ready um now I've done this before and I actually have a template specifically for G pod and I'm wondering if we're going to run into issues because in here I noticed that I do a Docker Network create local stack and I'm not doing that um uh here but maybe it will just work without issue but I guess we'll find out there are ports that are supposed to be um open so there's a range of ports I'm not sure if we need to open those up here we might have to so I'm not 100% sure if this is going to work without doing that and the port range is kind of interesting because it's actually telling you I think these are the ports that um ads uses their range of ports right so I think they match a similar Port range maybe not I don't know but anyway um what I'm going to do is go ahead and see if this works so to utilize this I think I already have an example here um so I think it installs a CLI called adus local let's go ahead and take a look there and we'll go back to terminal in a separate tab here we'll just type in clear and can I type in Aus local here no can I type in local stack just making sure that it knows what it is okay that's fine um so that's okay I guess the question is can we access the local stack by changing the endpoint URL so we'll go back over to here and there should be a thing to point to the specific address so if we go back over to here wherever we were running it where are you local stack here it is it's running here on what port over here okay so the the idea is that we should be able to swap out the endpoint URL to uh query some stuffff so I'm going to just say we installed this whoops and we'll do um local stack start and then we need to see if we can do anything so we'll say ads S3 LS and then we'll say endpoint URL and then we should be able to supply that so again I'm not sure if that's going to work but we'll give it a go and see what happens I just need this in another Tab and we'll hit enter it says doesn't match either that or etc etc so that's not working exactly right um okay give me a second okay so uh last year I made like a full example of local stack which is uh interesting that I'm struggling with it now but I wrote tons and tons of U tutorials that probably nobody knows about if we go over here to setup let's go take a look here and see what we have um so yes install the CLI uh um this one creates a local stack here and then this one's referencing Local Host so I suppose what we could do is just specify Local Host instead and see if that helps so I can go ahead and hit up and we'll try Local Host and hit enter and that's still not working so the key difference between what I did here and this one is that it looks like I actually created a um a network called local sack there so I'm going to go ahead and do that as well so we go ahead and copy this type in clear hit enter and so now we'll go over here and stop local stack give it a moment to stop there we go it's stopped I'm going to type in clear and we'll go ahead and hit up and hit start and we'll try this again it should be faster because it doesn't need to pull the entire container there we go whereas the first time R it was a lot slower notice that we see like hyper corn error I don't know if that matters but we'll go back over to here and hit up and I want to try this again and see if it works now SSL validation failed for well the other thing is that we can't do it like that we'll have to do HTTP okay so now it's working with that issue did we have to create this I'm not sure it depends if we had that um that error before but we really should create in here a a a local a local stack Network here so I I'm going to leave that in there okay but it looks like it's working so the idea now is that we are connected uh not to the adus S3 but actually S3 that is um running pretend local in a Docker container over here so let's see if we can actually go ahead and and do some actions here so let's create some mock S3 buckets and what we can do here here just copy this and go down below and we'll just say make bucket S3 let's pick a name that we definitely know that would be taken like amazon.com let's see if we can do that and here it says SSL validation failed oh because I have the s in here again sorry take that out and so it made the bucket okay cool just understand that we probably shouldn't be able to create a bucket we type in adus S3 make bucket I type in S3 Co amazon.com so I'm not using the endpoint URL I'm using whatever credentials I have to go to my real adus account this should fail bucket already exists but notice that we had created it there and you might say well Andrew that's because we just created it here well if you don't trust me uh what we'll do is we'll do another one like adab us. amazon.com here notice that that doesn't work now let's put our endpoint back in we'll hit enter and we've created that bucket so we are using local stack all right so that's kind of cool and we can do things we can create objects here and and do stuff and uh again for free tier it's pretty good but again you'll hit some limitations for uh more advanced stuff um so yeah that endpoint is in place now let's say we don't want to set that every single time there's probably a CLI or sorry environment variable for that so we'll look it up say it us Nars here and there's probably one for endpoint so I can just scroll down here and just say endpoint here endpoint configuration setting are located in multiple places so we have end point URL um and looks like you can have one for specific Services I didn't even document that I didn't know that you could even do that spe uh specify a custom endpoint that is used for a specific service well that's cool so maybe we want only do this for S3 it didn't uh so we'll go ahead and do this we'll say S3 so we'll say only so like uh set an nbar just for S3 so go ahead and copy that we'll paste this in and we'll type in export and we'll give this a go and see what happens I'll type in clear and uh it might not like those periods so I'm just going to put double quotations around it when it comes to periods I'm a bit paranoid because they always seem to uh muck up our variables so I'm pasting that in that's only going to work in this one understand that it's not available in all these other tabs but what we'll do is I'll just make sure that this is actually set type in Aus endpoint here and it is set great so now we should be able to do ads S3 LS and notice it's only listing the ones from local stack and not from our real account which would actually I have a lot of buckets so I expect to see a lot a lot of stuff here um the other question is how would you set that in the SDK um so I imagine that that would be in the client so I'm going to check just for the Ruby version because I'm most familiar with Ruby version three and so in here all these should have like a client for configuration if I scroll on down here like notice we have like this update that's one way of doing it with is with the update and you can set it per service but in here there should be a way to configure an endpoint let just type an endpoint so can we override it this way h not sure it's not showing it to us it's either that or it has to be set via the environment variables right so maybe it's picking up that way um let's go ahead and ask chat GPT how can I override the end point uh in the itus SDK Ruby s version version three again it might pick up VI the N bars or there might be an explicit way of doing that yeah we already have it installed I really should have just told it to give us the code we'll just give it a moment to generate out okay all right so basically we can set it but it looks like we're setting it at the client level per service right so there's Global configuration and for whatever reason it's not showing us that there I would imagine that it could pick it up globally somewhere but supposedly not each SD is probably a bit different so let's say okay now how do we do this in Python I'm just trying to see if it has different options or if the sdks are always at the service level when when overwriting it so we'll see what it says here create a client with a custom endpoint so for this one you can overwrite it okay so I think that different sdks are going to have them set in different places for whatever reason the Ruby one does not allow you to set it in the main client or the main uh configuration here but you can set it uh per service which is totally fine and then for Bodo 3 it looks like you can globally set it so it's going to vary on on those cases if we want we could quickly do a um python test here so I'm just going to go here and make a new file and we'll just say s3. pi and then we'll have a requirements.txt so requirements.txt hopefully I spelled that correctly I think I did because it's picking stuff up and so I'm going to go here and just type in boto 3 and and then in here I'm going to CD into the local sack directory we'll do pip install um hyphen R requirements.txt I think that's what it is yeah it must be because it's working and then we'll go ahead over to chat GPT and grab their really simple example okay we'll paste that in there and so this is going to establish a connection to the S3 client let's go look up the python documentation python S3 or yeah s three reference and we'll go here I don't know what the latest version of Bodo is I don't use Botto as much as I use Ruby or other ones but we should be able to figure it out and let's say we want to go ahead and create a bucket here and we'll scroll on down and here we have an example so I'm going to just drag this over and we'll say S3 client do create undor bucket and we'll need a bucket name probably that's all we really need bucket my Pi bucket we want to get in this uh endpoint URL now it might actually already get picked up because it's set at least in this uh environment it's it's it's set in here so uh what I want to do is I just want to make sure I'm not picking it up locally I'm just going to switch to another tab because this one's not going to have it if I type in uh grep or sorry EnV grap adabs endpoint notice it doesn't pick it up okay so um we do want to go ahead and grab this endpoint uh information here like this and we'll go back over to our code we're going to replace this endpoint URL okay and uh we'll do google.com for fun so that should create that bucket and I'm going to CD into API local stack here and just do clear and I want to run this we'll type in pi and s3. Pi we'll enter oh sorry Python and it's saying an air curve we're calling the operation the unspecified location con is incompatible for the region specific endpoint that was requested to send to so I'm not exactly sure what the issue is I'm going to go ahead and do Us East one we'll try this again and it works work so maybe there's some limitations with um local sack and regions but now it's in Us East one let's go ahead and see if we actually were able to create that bucket using Code so we'll type in Aus S3 LS and then we want to specify that endpoint URL again so go ahead and just copy this hit enter and there you go so yeah hopefully that makes it really clear in terms of how that endp point URL thing works uh it's great that we can set it per service and obviously the way you said in the SDK can be a bit different I'm going to go ahead and just save my code here um if you are using a local developer environment you have to stop this you might want to also delete out the images because they take up space on your computer I don't need to do any of that I'm in a cloud developer environment so just say local stack service endpoint example or endpoint URL example and we are good to go and I will see you in the next one ciao so the idea is that we have service endpoints but in order to perform an action we need to specify that when we make an API request and so every single datus service has a list of possible API actions that we can perform I want to point out that sometimes they're called operations so if you hear API operations or operations think actions or if you hear actions think operations okay so when you specify a service endpoint you always have to specify an action and so here using a query string that's that question mark I specify that I want to run an instance for ec2 the naming of these actions are all over the place why because adab us has a lot of employees each team has their own um ability to decide what they want to name things and they don't always stick to the same thing for whatever reason so the most common ones you're going to see is describe create update delete so if we're using the CLI notice down below here we have easy to describe vpcs but notice that is for describing a bunch of vpcs and then we have describe VPC for an individual VPC so you get really strange things like that um where you might think maybe describe VPC should be list VCS nope it's describe vpcs then you have uncommon ones like list put get copy modify and there's definitely more and then they get uh even more uh very specific for the service so there's some where you disable enable stuff you have associate accept allow cancel so yeah they're all over the place but what I want to hit home is that if you know that something's in action it's going to be pretty much the same no matter how you're going to access it uh via uh different um developer tools so if you're using the CLI notice it says run hyphen instances now let's go take a look at what that looks like with the SDK well if we're using look at that looks like Ruby code uh the function is called run underscore instances or let's say we are giving permission to a specific action in your IM policies it's going to be ec2 so that is the um service code and then we have colon and then we have the action name and title case so just understand that when you're using IM am all those actions literally line up to actions and that's that way you can figure out what permissions you need to get access to perform specific actions or operations within an ad service so there you go all right so I just want to make it really clear uh the relationship between API actions and other parts of AWS I know we showed it in the lecture slides but let's make sure we really understand this clearly so let's say we're looking up any service like Dynamo DB and we can type in API actions here and if we go here there should be an API reference in the API reference it will actually provide us all of the possible actions that are available we also have I guess data types over here which um I suppose are data structures that are provided uh within an action but we're not worried about that right now we just want to talk about API actions so let's say we wanted to create ourselves a table right we could go into here and we can read about the specification these are the attributes that it expects so if we were to send an HTP request these are all the stuff that it would want um and it'll tell you what it requires and things like that now let's go take a look at what this looks like when we go to the adus CLI so we type in ad CLI version two since I really don't want version one and we click into here we'll go to the adus logo we're going to search for Dynamo DB and in here you'll notice the available commands more or less match up to actions what I say what the reason I say more or less is that um I don't know 100% if they do but they sure look like they do so let's just confirm that and see we have batch execute statement batch execute statement if we go all the way the bottom the last one is Wizard we go here and this one doesn't have a wizard so wizard is like a special one and we actually learned about wizard um in in the CLI uh in the API section here and weight is is a special sub command but if we go beyond that we say update time to live update time to live so it looks like they're more or less all the same actions are here they're just written in uh lowercase with hyphens right and if we go into again create item I would or create table sorry I would expect that a lot of this stuff is going to maybe map up to the syntax here so let's see if that's true so if we go here and and and look very closely I'm trying to find um uh billing billing mode so do we have bilding mode here we do we have building mode right there do we have uh Global secondary indexes we do we have Global secondary indexes now that's not not to say that this is one to one because there's going to be obviously conveniences in here to make it a lot easier there's going to be ones that are going to just exist in the SK live that are just not going to exist here so expect this one to be the most for Bose um does this map up to cloud formation let's go take a look so we'll say it cloud formation Dynamo DB create create table or'll just type in table here and if we look at this we see building mode we see global secondary indexes and if we click into that it says provision payer request if we were to read specifically about this it says provision pay pay per request so there seems to be a large or high correlation between the Dynamo DB syntax and here and then this one would be similar but maybe a little bit more higher level let's go take a look at the we'll look at ads um python so maybe Bodo 3 Dynamo DB create table let's see if we can find that reference this is probably not the latest version but that's okay we'll go down here and it looks kind of similar more or less similar Global secondary index and the structure is I mean these inputs are the same and so that's the same now let's go look at the API actions or sorry um uh IM policies right let's say we needed to Grant permissions so we go go here to policies Let's Pretend We're making a new policy we'll go here and we'll give it a moment to load and we're going to look for the Dynamo DB service and then we have allowed actions so if we were to expand all and look at these actions I bet they match one to one with all of these here so I bet I could search for this one here so we're searching for it it did not appear okay that's okay we'll continue on to the next one there it is Batch get item batch write item do we have create backup create backup so more or less they should all be there why isn't this one here it might have to do with the nature of what this is this operation allows you to perform batch reads or writes on the Dynamo DB yeah again I'm not sure why we don't have a direct permission for that it could be that because this is a batch command that it is doing something underneath um but I I don't really think so so yeah not sure why that one doesn't show up but uh or maybe it is there and it just doesn't show up in the UI here so that could be the other case that's the reason why we don't see it is that the UI doesn't have it but you could technically put it in via the actual Json when you write in which action you want to include so hopefully that really helps a lot uh in terms of uh making that relationship remember the databas management console is a UI that tries to make things easier so it is a much higher level abstraction of these API actions and so uh when you go to the Dynamo DB page and let's say you're trying to create a table here it might be performing multiple actions underneath right so it's not going to just hit that one API action it can probably hit multiple ones for you um and make make that process a lot easier so just understand when you use the Management console could be using multiple actions that's why I like to push and try to use the CLI and the API as much as possible so you understand all the underlying components but anyway hopefully that makes sense and there you go the ads CLI command reference is a website that contains detailed documentation on commands subc commands and their parameters and it's located here you are going to see me so many times pull up this website this this is the most important website for you to know because this is how we're going to work with ads in the most common way outside of the console and for your professional career you should be writing bash commands uh utilizing the ca commands constantly so the idea here is that we have the website and they have a command reference for every single service you will click into a service and in that service so for example this is code build it will have an action so our action is create project um and then you'll get all the information there so you'll be really familiar with this and understand that there is version two and there's version one they look really similar but you should always look at version two a lot of times in the documentation you'll see me on version one and I'll scroll up and I'll realize I'm on version one I'll click over to version two um why don't they just get rid of version one I don't know maybe for legacy reasons but just make sure you're always using the latest version of the command reference as that's probably going to um relate to the AI version as well okay hey this is angrew brown and in this video I just want to take a bit closer look at the um CI command reference so you've probably seen me open it up so many times in follow alongs and you might think why are we even bothering looking at this but let's just poke around and see if we can learn something new um and so normally I'll just type in a CLI when I want to find it but let's actually go find where eight of us would host it and probably on their developer tools pages so if we type in developer tools and we go over to um uh wherever that page is I'm not sure why I'm having such a hard time finding it right now development tools here we go developer tools uh you'll notice we have our SDK programming languages if we go down below we have command line tools and so there's actually more than one CLI tool and this might not even be all of them listed but we have the CLI power ch ec2 has their own tools for making Amis elastic beanock has their own ECS has their own amplify has their own Sam has their own copilot which is for um which is for containers has their own so there's a bunch of cies the one we're interested is this one and I'll just show you here ad a shell is still being listed here I talk about this when we talk about Auto prompt and how this is a feature that is four years old and never going to be done so they should really remove it off this page but whoever maintains these pages are not keeping them up to date but we can actually get to the GitHub repo and then open up the CLI command reference before we go over to here let's take a look at uh where the website is host or the CLI is hosted so all the code is here and for some reason adus does not does not keep their releases up to dat but if we go to tags it's a shame that they don't update releases so they clearly are making changes but they're not telling us what those changes are we can see all the changes that are occurring um two is for version two of the CLI one is for version one of the CLI um if they're doing updates to one I'm not sure if that's the case anymore but anyway let's go take a look at some of the code and the ad C is using the SDK underneath so a lot of the uh code that you might think is in here is not actually in here it's just leveraging this the SDK in some way Bodo 3 um on the left hand side you know we can go here and they have examples so we can actually see an example for every single CLI command if we're not sure on how to utilize it some are better than others we'll actually they'll show the outputs but most of the time we'll just get that in the CLI reference ther customizations I think that's where we start to see some actual useful useful information if you a programmer and you like digging deep in here so if I'm in code deploy uh maybe better would be cloud formation we have that deploy action and you can kind of see the options that are being passed through um you know I am of the belief that you should be very comfortable being able to open up any kind of source code even if you don't understand it uh to try to poke around to um uh get more knowledge or uh understanding of the tools you're utilizing because you never know when that's going to help you so that's really interesting there but let's just close that out and we'll go over here to the actual command line reference there are two versions version one version two obviously don't use version one uh so here if we click this we'll go to version two and we'll know that we're there because it says V2 right away we are on the index page and it's showing us these Global references I think if we were to click this table of contents it would bring us here so we actually we're just on this page on the command reference it says reference up here and we have some Global options such as debug endpoint URL output no paginate Query profile um all these things here and then down below we can see all the available services so whether you want to do from here or there it's up to you but um we have all of our possible Services we can go to anything in here and click into something like glacier and we can see all the available commands and then from there we can click into a specific command sometimes they will have examples they don't always have examples as you can see here and then uh sometimes they will show the output it's a shame that they don't always show the output it could be the case this command has no output but uh seeing that structure helps a lot then you'll see all the possible arguments you can provide as flags and sometimes they will you will have to read very carefully at the top the options because it might expl exp something in particular that uh you need to know maybe not options but up here in the description so you know if you're exploring something new and you don't know what it is take the time to read the top um I don't do this in the videos too often because I use this stuff so often I don't need to do that but here they're talking about path argument type and they're talking about that in great detail so if you want to know more about the S3 URI you could read about it here and this might not even show up in the docs it might only show up here in uh the the references here okay so yeah uh that's all I really wanted to show you um I noticed there's a user guide up here so I guess this talks more about the CLI and actually a lot of our content um uh We've basically comb through all this so there's no need to read through it because basically I've extracted out all the best parts for you but anyway yeah that's the CLI there for you so there you go let's talk about the a CLI return codes um if you've ever worked with HTTP you've probably seen things like 404 500 errors these are codes that get returned by the HTTP protocol and for terminal programs that you run in a shell they also return their own codes that tell you how the program did um and so using the a CLI we can actually observe what the um the return codes are for the program so we're using bash we can use a dollar sign uh question mark after we ran the command or Powershell it'll be a little bit different um a lot of times the programs will return uh information so they don't necessarily return an error code you have to reference it this way so that's why I'm specifying it let's just take a look at what these error codes mean specifically for adabs and the error codes for programs can mean different things for different programs right so this is what adabs says but these three are always usually the same so if you get a zero back that's a a 200 remember this is using HTTP as a protocol so it's the equivalent of that two 200 HTTP response which is good that's a good status code um this means that there was no error generated this is what you want if you get a one then you it says one or more uh of Amazon S3 transfer operations have failed so this is specifically for S3 if you get a two the command enter could couldn't be pared maybe it's missing an argument if you get a two and you're using S3 one or more files were marked for transfer were skipped during the transfer process if you get a 130 this command was interrupted by a Sig int or control c a control plus C is a very common way for us to quit out of a command that is running I do it all the time in uh a program you ever see me quit uh like stop a command that is running I'm usually hitting that um then we have 252 this command syntax is invalid is for when something's invalid unknown or incorrect parameters 253 is the system environment or configuration was invalid so you have bad it with credentials 254 was a command that was successfully sending request but the error returned by the API from uh was from the ad service 255 is the command failed but could not be from the a CLI or the ad service it is useful to be able to access return codes when you have ambiguous errors or you're not sure what happened you can check the return codes and maybe it helps you narrow it down um okay so there you go hey this is angrew brown and in this video what I want to do is see if we can get some return codes to come back from the ad CLI so I'm going to just open up Cloud shell here um I just recently had it open so it instantly opened you might have to wait a little bit or you might have to create um some kind of volume I think it's backed by EBS or S3 or something so you need to maybe create that first but anyway now that it spun up what we can do is is we can go ahead and try to use the CLI so if I type in ads just making sure the CLI is there we're in good shape let's see who we are so we're going to use the ads STS get caller identity to confirm who we think we are I'm I'm right now Andrew Brown which is totally great and if we want to get our eror code I made this in the slide where we have this Echo dollar sign um question mark for bash so let's go ahead and type that in so we'll type in Echo dollar sign question mark I said bash I meant to say question mark and we got an error code of zero and zero means that we're getting a status 200 and the code was returned by ad no errors generated let's see if we can get a different kind of error and um I think one way that we could emulate that is in our Cloud developer environment so I'm going to go ahead and just open up our existing environment and what I'm going to do is I'm going to uh purposely muck up my credentials and then see if we can get an error code back so uh we'll do that here in just a moment and probably the easiest way is to set up our uh a profile so that's what I'm going to do um so I'm going to go ahead and type in OS configure I'm going to purposely put in junk content so just do this and I'll do that and I will let it set to whatever and so now we've set up junk content I'm going to type in open and I'm going to open up the credentials file because I want to set it to a profile that's uh something other than what I'm currently using I just say uh wrong as it's supposed to be wrong so now if I type in ads SS get caller identity and I set the profile as wrong notice that it it entered Auto prompt because it clearly thinks that there's something wrong oh I think it's because I type the word identity wrong I always type that word wrong it looks like I added an n in the front there we'll try this again I notice that I get an error occurred invalid client token but let's go to take a look at what the error code is and the error code we get back is 254 so if we look down below here the command was successful sending a request air return by the API from the service so it was able to send the request but it was an issue with um uh things being invalid I'm actually surprised it didn't throw a 253 so the system environment configuration was invalid batter or missing his credentials that's actually what happened but what we got was the 253 let's go double check the return codes to make sure that uh what I wrote in that slide is not wrong pretty sure I'm I'm not wrong there but we'll go ahead and check and so here um we see some CI return codes okay but I'm pretty sure any of us has more documentation on it I'm surprised this is so limited this is version one let's go to version two do we get more there we go and so here it says the system was invalid um this one says nothing about credentials this one talks about credential so you can see that it's not me that is making the mistake here let's go ahead and just copy this link this was in my slide I grabbed it as a source and so this shows the error codes as well that's how you can see how I knew how to do for that and for this that's for Powershell that is for command prompt which didn't look at and so 253 is still talking about credentials but clearly we just have bad credentials so you know why ad Us ad us doesn't report the same thing as a codes I don't know why but anyway that's all I wanted to show you so there you go and I will see you in the next one okay ciao so the adus CLI has a special wizard command for very specific services that allow you to go through a wizard process um a few of them is adus configure and then they're always followed by the word wizard so we have one for Dynamo DB events I Lambda and basically it's like a really fast way to uh create things um so here's an example where we're trying to make a new Lambda and then it's going to provide us options and step through it um so that's really really cool um the only thing that I would say is that um uh Wizards seems like a functionality That was supposed to make things easier but nobody knows about this functionality and nobody uses it but I thought it was really cool and that's why I wanted to uh show it off you can actually make your own wizard commands uh but there's not a lot of documentation on it but I know how to do it because I actually went through it and figured it out um the uh now one thing I should point out is that the wizard will fail if you don't have a default region set so that is one caveat to it is this going to show up in the exams no but maybe one day you might want to use it for fun um so I just wanted to show it here okay for hey this is Angie Brown in this video I want to show you the CLI wizard which I think is a um very underrated tool and actually has some utility and you should know what it is so I've opened up Cloud shell here and if you just hit the expand button you'll get this over here notice there's some artifacts that's because I was running this tool just prior to this video and I can't get rid of that artifact but one thing we could do is uh and I'll just break down the slide that we have for configure Dynamo DB events I am I am and Lambda the ones I like is the I am one and the configure one so with configure we can type in a configure Wizard and one that's really useful is the assume Ro so if you need to assume a role very quickly you could type that and then just enter a profile in so you know my uh my assumption my assume and you could choose the exact uh rule if you already have it you can say where the source credential is coming from and so that is one useful wizard the other one is the I am wizard new rule and we'll go ahead and hit enter and so you could of course enter stuff in so my rule my roll and then you could choose where the trust uh the trust entity is from and then the service and then choose what policies you want attach so I really like that so that's really cool um you can create your own Wizards I think I wrote a tutorial on it I might be the only person that that even knows how to do it I'm going see if I can find the code really quickly here just give me a moment um I think I have a separate Library here called C CLI labs and yeah actually I wrote a tutorial on it uh not that anybody cares but um so here if we want to create our own wizard it looks like yeah it's not documented but you can create your own Wizards all you need to do is Sim link a folder in the Wizards directory So within the C tool so this is where it's installed if you follow through to here there's a Wizard's directory then we just need to follow the structure of the files so here is an example of um a structure and they don't document it but you can see like you have to make a plan then you have your preview uh and stuff like this and this thing totally works I have it here at a Labs a CLI wizard um so if you ever wanted to create your own and let's say you just didn't ever have access to um uh the console and you're doing something extremely routine and you want to have some flexibility you can do that if you if you would like to but uh that's all I wanted to show you and there you go the adus CLI allows you to specify the output of the adus CLI commands using the output flag and there are a few different formats that are supported Jason yaml yaml stream text table I'm going to tell you right now it's not for every single command some commands only have some outputs uh some of them have all of them so it's really going to be dependent on it so I'm going to show a bunch of examples and they're all different commands because it's hard to find one that supports all of them uh as a good example so here we are describing uh instances and we're saying give us the output in yaml format then here we're listing out buckets and we're getting them in Json format and then here we are listing tables in Dynamo DB and we're saying provided in this tab format uh that table format is kind of like an asky table it's not like a CSV or tsv or Excel file then you have a text which is just raw text output and if you want you can set uh the default output via the configuration file or the environment variable adus default output you will definitely be changing your outputs uh manually for a very a variety of reasons because often you want to pipe that data or save that data and and pass it on to a future command and so there you go hey this is angre brown and what I want to do in this video is to show you the different output formats so uh there's a lot of different ways that we can do this um I guess that we could do this via the uh Cloud shell here so I'm going to go ahead and just open that up and what we'll do here is we want to be able to see something so um for these output formats I'm thinking maybe something in VPC might be useful so I'm going to go ahead and just type in AWS I think I have already enabled here no I don't have Auto prompt on here so we'll just look for for the auto prompt Auto prompt uh nvar so we can turn that on adli if we go here we are looking for this here okay and so I just type in uh export here and then paste that in and say on partial we'll type in clear and now we'll type in ads and so now I get this prompt and so the idea is that we should be able to get um different outputs now if I open this up here for a moment into cloudshell then we get this nice bigger window and that's a lot easier to work with because I want to see we can use the output panel which kind of gives you a preview of your output let's go ahead and take a look at VPC here we're getting some weird artifacts from the the time we resize so I'm just going to clear that out type in clear and then do this again and so I'm going to type in in VPC or sorry ec2 cuz that's where VPC is under and then I'm going to say uh describe vpcs okay and the idea here is that if I hit F5 I don't know if it'll work here I'm going to try to hit F5 F5 F5 there we go I hit F5 sorry F5 I can actually see what the output's going to be and look by default it's going to Output uh to Json okay so if I if I just do output here I could change this to table and that's what it would look like if it was a table I could change this to be text and that's what it would look like if it was text I could go here and say yaml and that's what it would look like with yaml I don't know what stream would be I guess it's if you need to stream it um I don't know if there's any other formats let's take a look here so output um it was CI formats let's just see if there's any other formats table text yam Jason yeah that's pretty much it and of course you can override it um by setting um an environment variable and putting your configuration file like see you see it's in the config file nothing super exciting but uh yeah I mean you just play around with it and you try to figure out what output works for you um Json obviously is the most useful sometimes it's nice to have text text is really good if you're going to pair down the information so like obviously this isn't very useful but uh if we just go back to Output uh maybe we want to change our query and we only want to select um let's say uh the state we could do query and then I could say vpcs and then I could do state now that wouldn't work we'd have to do period square or Square State there we go and so now we're getting array of States okay and then I would just say output and then we would say uh text instead or we could do table but for text it's usually good for text when you want to get a single value that's what you would do right um so you know hopefully that gives you a a clear idea on how that works I'm going to just get out of there by hitting F2 um or sorry F5 to hide that panel but uh yeah I don't use these very often in G pod but I love these panels when working with the CLI because it makes it very clear what you're going to uh get returned to but uh yeah we'll see you in the next one okay ciao pagination is used when you have too much data and you got to break it up into separate sections which we call pages and then you specify the page of data you want um for the a CLI we have two ways of doing this server side and client side for server side there will be serers side Flags these flags are only available for data that can be paginated so it might not be for every uh command um it could be for all of them I'm not sure but um anyway the point is that some sometimes you will be utilizing pagination um so let's take a look at the flags the first is no paginate so should be pretty clear it's not going to paginate it's just going to return uh the default amount of information and it's not going to return uh pagination stuff then we have uh page size so we can say give us 100 at a time we have Max items so we can say 100 at a time then we have the starting token that's how it knows where to start so if you're using these commands I'm assuming it's going to return back a token and then you will supply that token so you know what page you want to get uh for client side pagination um it's just query so query has its own language called jme s path I don't remember what that stands for but um within that if you have a query you can select a range of items from the data is returned and in that sense that is pagination um so there you go okay all right so let's take a look here at how to actually do pagination uh with the CLI so I'm over here at the command reference uh and if we go into here you'll notice that there's an option to have no paginate and you can apply this basically to all commands um basically all commands do pagination but they don't always necessarily give you all of these parameters it depends if you are working with a lowlevel or highlevel API so for instance if we go over to S3 which is um a highle one and we go into something like LS we do have page size but we don't have Max items we don't have starting token so it still does pagination but it's a little bit more limited but if we were to go into S3 API which is a lowlevel uh CLI command and we go over into um what is it called uh list objects we should get all of the uh parameters or flags that we would like to work with these three here so let's go put this into practice I'm going to want to use something that actually will have something we can paginate without us having to create a bucket and fill it with stuff because I'm kind of getting tired of doing that so let's take a look at maybe subnets as we should be able to um uh be able to list out a bunch of subnets so there's a describe command for subnets and so that will return back subnets so it's very straightforward I'm going to go back over the CI I'm actually going to switch over to us East one because I know that one has a bunch of subnets and I want to get enough back and I'm going to go ahead and and open up Cloud shell okay now Cloud shell I might have activated um previously uh Auto prompt but if it's not we'll find out here in just a moment so I'm going to go ahead and type in ads and I'm waiting to see if Auto prompt shows up it does not so I'm going to go ahead and type in ads CLI Auto prompt and you know what I was just thinking is that it'd be really smart if the um the wizard allows you to set auto prompt I'm really conf uh curious if it can do that CU I'm really getting tired of looking up the um uh that option there additional C parameters oh that that was nothing that didn't help at all okay but any if you're listening create a wizard to to set the auto prompt because that would be super nice um so I just look up the nvar here because I don't remember what it is and the value is this one here so we go back over to here and we'll say export and we'll say on partial so now if I type A S I get the prompt perfect so we'll type in ec2 and then I want to get describe uh subnets let's take a look at its output I'm hitting F5 for output okay and I'm going to navigate down to this screen by clicking on F2 or pressing F2 2 and I'm using J to go down or maybe K to go down I always forget which which way it is and it is not using jrk I'm actually using the arrow keys that's interesting because normally it will use the Vim keys but that's totally fine if it doesn't but that's down below here we have next token so if there is an X token it's going to appear down below but we have the subnets above I'm going to go ahead and just um go back up to the other panel by hitting F2 on my keyboard and so what we can do is just query this and just get the subnets so I'm gonna go here and say subnets Square braces and then subnet um ID okay and then we'll hit enter and we can see we have 1 2 3 4 5 six seven seven subnets so let's go back and hit up on our keyboard and go to ads and um we can pull up previous commands they don't appear outside of this but if we hit controlr we can get our previous commands here and I'm just going to go and hit uh tab or yeah there we go to bring that back and so now what we can do is we can say let's return um fewer items so if I go here and say Max items I can say give me only one okay it'll return one we also have the other option which is for page size and this one's a little bit confusing but what it what it does is trying to get to our previous one here here there we go uh what page size does is it will call it US based on the size of documents you want back so it will still return everything but it will say Okay chunk this in the size you want so if I say one then it'll say it'll make 1 2 3 four 5 six seven API calls and then Stitch the data together and return it back to me so if I hit enter it says I can't do that because that has a minimum value of five so different CLI commands are going to have different minimum and maximums for these things we'll go ahead and hit uh enter on ABS I'm hit controlr I'm going to go back to page size I'm going to just change this to five and notice it's still returning all of the data but underneath it's it's calling five of these and then two of these okay and the reason you might want to do that is if you have a lot of data it might um uh just not work because one of them fails right so by doing this you're going to have less issues but it is making more API call so you have to make that trade uh for what you want there and some some CI commands are basically it's the cost per request so not all services but some of them pagination could cost you more but you're more reliably to get your information let's go ahead and type in ads again here and I'm going to hit controlr and what I want to look look at is the max items again so we have one but the difference here is that uh what I want to do is I actually want to um get the next item so so I'm going to just go ahead and hit enter and let's make note that this is the one that is currently here just so we don't lose it I'm just going to paste it up here and so this is number one so I want to get number two and the way we can do that is go back here hit controlr and I'm going to go back to Max items but this time I'm actually going to query for the other uh thing here which is the next was next token if we' enter it returns us the next token and so I can use this token in our next uh command to get the next item so if I go here I'm going to type in starting token and then we'll go ahead and copy this I'm just going to right click copy right click paste and then hit enter and so notice that uh we should get the next item um well we got the next token it it has to do with our query so I'll go back and try that again one more time and I want to go here that's fine I suppose I don't know if once we use a token if we can use it again but we'll we'll try here and I'm just going to do subnets um Square period subnet ID hit enter okay and so notice that this is this one and this is the next one so let's just take a look and see all of them and just make sure we can confirm that's the order so we'll go here and we'll go back down to this one here okay and so this was this was the first one so that matches up and that was the second one and so there you go that is pagination so yep we'll see you in the next one okay ciao what is command Flags well shell programs will often support command Flags to pass options or parameters and programs may call their Flags by different names such as options par or Fields I will generally call them Flags but sometimes I will call them other things you need to know based on the context of what you're working with what what people are referring to so just understand there's some variation there uh so here I have a bunch of examples of flags and let's take a look at all the different kinds of variations these are just common uh programs that you will encounter while using Linux so Flags uh start with the usual hyphen hyphen and they'll usually expect an input uh program might allow either a space uh or an equals between the flag and input so just note that here we have a space and then here it's an equals and sometimes you can do both um Flags might expect to come first or last so notice here that above here it comes after and then here it becomes before and some programs will complain uh that it has to be in a specific order or could not matter whatsoever ever it could be in the front or behind a specific area Flags can have shorthand Flags noted by the single hyphen so um notice we have output here this hyphen o means the exact same thing as the one above here uh there could be a capitalized shorthand flag that does something different so this hyphen capitalo does not do the same thing as the one above uh Flags might not require or expect an input so here it's just hyphen G no input required you can have multiple Flags where the order doesn't ma matter so notice here we have LS hyphen L and then which is the shorthand and then hyphen hyphen all you could flip them in the opposite order and it would still work some programs may let you combine short hand Flags together so uh this command hyphen L is the same thing as this one above here so yeah there's a lot of variation here it can trip up begin um where you think you're doing things right but you just didn't realize that there was some hidden restriction for that specific program but there you go so in the context of a CLI command Flags is how options and parameters are supplied to an API action when using the adus CLI so looking at how you run able commands you have your command your subcommand and then it says option and parameters that is your command Flags so command flags for the a CLI are expected to always be ritten after the sub command so hopefully that is clear so where it says MB you put it right after there it us does not have shorthand command Flags the order of the flags does not matter they simply must appear after the sub command some flags do not expect an input uh you can use either an equals or a space between the flag and value and sometimes you need to wrap the value of the flag and quotations to avoid input side effects because if you encounter a period or some other interesting character it might break the command thinking that it's now doing something else and so often you'll see me just wrapping things in double quotations to avoid side effects um with command flags for a CLI you can use either a space or an equals using an equal is required if you have an input that starts with a hyphen so here's an example of an uh of a uh an input where the value actually has a hyphen in the name which is really unusual U so that can happen so uh yeah there you go that is command flags for the a CLI there is a subcommand called the wake subcommand and it does exactly as described it's going to wait for a particular condition until that condition is satisfied this is useful for long running API actions such as stopping an instance or deleting a volume and this sub commmand is only supported for very specific anus API commands such as ec2 so in this example uh we are uh waiting for a VPC to become available after its creation and so that weight command is there surprisingly that doesn't take long to happen so it's not the best use case for this but it is an example of how you could use it and so here are some weight commands we have here you're going to notice most of the stuff is related to VPC or ec2 or uh images that are being created um so hopefully that is very clear but uh you know if I was to pick out something here maybe it would be when an uh it could be when an instance exists so that could be a interesting one uh to utilize I would assume that's for ec2 but uh yeah there you go hey this is Angie Brown and this fall along we're going to take a look at utilizing the weight command to see when a resource is ready um we're going to launch an ec2 instance so this is a little bit more involved than uh some of our other ones but that's okay we're going to just get to it and I have uh this adus examples repo I think it's going to be a lot easier doing that in here as opposed to doing this in Cloud shell so I'm going to go ahead and open up my cloud developer environment which is G pod use what you like to use but that's what I'm using because it's easy for me to use so um this is going to open up and I have an API directory here I'm going to make a new folder called wait and in here I'm going to make a new read me because there's a couple things we need to set up the first is we need to create an ec2 instance and then we need uh to watch for ec2 instance to be ready okay so if we go over here um and I just went over to ec2 so we go to ec2 here uh they're going to have weight command down below and so let's take a look at what we could watch for um we have instance running or status okay status okay is usually a pretty good one to have so I'm going to go down to examples and here we have one where we're just applying the instance ID so I'm going to go here and do this okay and we're going to want the instance ID I'm just going to pretend that we already have it and put this here like that and we'll put this on one line okay and then I want to write a have a CLI command that's going to just return back exactly the instance ID so um if I go here on the right hand side I should have Auto prompt turn on I do excellent and then we can take a look here and maybe start an ec2 instance so we'll say ec2 launch uh provision I kind of forget what it is I'm not sure why I can't remember it's such a common command but what we'll do is just take a look here I always find it easier when I can see all the commands in front of me we'll go here and it's launch right is it not launch no wow I I really can't remember well let's go ask tach BT what is the CLI command for ads to launch any dc2 instance run instances of course okay all right we'll go back here uh and we'll just maybe use um problem do that so we have run instances okay and it should tell us some of the requirements it needs it's not really doing that we'll definitely need an image type or an image ID so let's just go back over to yeah it's giving me a bunch of stuff that's fine um but we'll go into run instances and just use the documentation because this one's a little bit more involved and they might also have an example for us they do that's perfect so I'm going to go ahead and grab this one here and there's a couple things that we're going to need to change I'm just going to get out of this down here below and we'll paste this in here and say allow whoops try this again allow and uh we're going to need the amid we're going to have to choose a type T2 micro's fine and then there's a key pair um I'm not interested in having a key uh a key pair but uh we might need to specify that I need to get the uh image ID for our current uh area so that's something I would like to have um we should be able to list out our Ami but it's a lot of work because we'll have to search for it it's a lot easier to get in the console so that's what I'm going to do I'm just going to pretend I'm going to launch an ec2 instance via the console here and then we're going to grab that Ami so I'm it says I'm in North Virginia but uh that computer or this environment is here in Central 1 so match to whatever you need to match for Amis are region specific so you can't just take this code that I that I have and use it okay you have to go and open this up and grab it all right so here um we have Amazon Linux 2023 and I just want to grab its Ami so it's right there and you can really tell that uh iTab best does not uh the people that make this are not the people that are using it via the C because if they did um they would Design This in a different way but anyway this is the Ami that we want here and I'm going to go back over over to here and again this is going to be different based on your regions so make sure you do not just take this or type this in it will not work um so image ID instance type do we need the key name because I don't really want to have a key pair so we'll have that what I want to know is the output from this so I'm going to go over here and type in ads um run instances I'm just going to inom uh have an incomplete typing here I'm going to see if I can pull up uh the output so I'm going to hit F5 for that cancel sorry function F5 so notice that I'm trying to hit it but it's not working here shift F5 so um how do I use F5 keys in terminal for vs code you can press contrl F5 yeah so this is the thing is like I can't get the output here which is kind of frustrating so what I'll do is just open up Cloud shell really quickly so that I can quickly write my query um so I think I saw have this enabled from before A I do excellent I'm going type in ec2 run instances and then we will type in query and then I can kind of narrow down what it is that I want so put squares and then what I'm looking for is the instance ID so this is what I want uh in terms of output so I go ahead and copy that uh the other thing is that I want this to return back um text so say output text and so now what we can do is we can then assign this output which is the instance ID to our environment variable here all right and that's going to make our lives a lot easier okay so will this work I don't know um because we don't have that key name in there but we'll find out here in just a moment no default VPC for this user group name is only supported for E2 classic and default VPC so I guess we have to specify a little bit more information um I guess that example wasn't that great but this might be for C version one so let's go up to version two and make sure we're we're using the latest that might be the reason why no it's not different but that's fine so we'll just look through here and see where we configure that VPC VPC VPC ID that's the output I want to know where to configure that um yeah like it says default V default VPC it's why would I have to specify anything should not pick up the default so it says group name is only supported for ec2 classic and default P oh output not found okay sorry I'm going to go ahead and do this did I launch an ec2 instance and I just didn't realize it let's go take a look here no all right we'll try this again Copy Type in clear and we'll paste this in hit enter um an error occurred VPC ID not specified so I guess we do have to specify a VPC ID it's just really hard to find that flag um let's go ahead and type uh type it in here and maybe it'll be easier to find this way instances okay it's not there so where does it get configured let me go find it okay so uh you're not watching me just struggle try to find it all right so looking at the examples I'm just scrolling down a little bit more and it looks like there's a few more so to launch instance into a non default subnet um I mean I'd love to use the default subnet I'm not sure why it wouldn't just use it so that's kind of interesting let's go back over to here read the read the here again um VPC ID not specified no default default VPC for this user M all right well I guess we could specify subnet and see if that resolves the issue so I'm going to go ahead and put this in here and we'll go into um vpcs I guess and I'm in Canada Central 1 so I got to just make sure I know where I am and it could be that I don't I don't have the default VPC anymore well it says it's okay so this is my problem you it probably worked for you and the reason why it's not working for me is I don't have a default VPC so if I go here because I must have made one before and then it got deleted so what I'm going to do is I'm going to switch over to us East one sorry and uh this is probably fine okay so my other issue is that this is specific to the region I'm in so I'm going to have to go here and go back to um ec2 and go grab that other one there and if we go into why is there an instance running in here my server how old is This Server uh yeah I don't know why this one's running here I'm just going to kill it me just spending money not even knowing what this is so anyway I'm shutting that down here but um uh what I'm going to do here is yeah again I don't know why this is here anyway but um uh what I want to do is pretend I'm launching an instance and I want to again go ahead and grab this Ami okay we'll go back over to here and we'll paste this in and notice that it changed because we're in a different region and then I'm going to try this again and paste this in and headit enter and it says unknown option T2 micro um I mean T2 micro is a type is it still available here I thought it would be go down below T2 micro is right there what's it talking about so let me go take a look here to instance type I mean we really should use T3 micros because they're newer mhm T2 micro yeah I mean that's what it provided us but uh if it doesn't like that happy to change it to a T3 oh you know what it is it's because there's not a space here so that'll probably fix that issue but I'm going to switch it to T3 um T2 T3 depends on what you want to do I think t3s are are free so free tier adab us ec2 go over to here um t3s in regions in which T2 is unavailable so just in case that's the case I'm going to switch back to T2 if this is the future you might not even have T2 you'll have to do that so I'll just change it to this we'll hit enter and it says invalid Ami ID not found when calling the Run instances the image ID does not exist really oh you know what we have to specify our region here so let's say region Us East one CU I'm I'm defaulted to ca Central that's why we're having this issue we'll go ahead and hit enter uh are you sure about that cuz I'm in the right region and it's right here um I guess I could wrap it like this and we'll try this again copy enter oh you know what there's a space after here so it probably didn't execute an us one you got to be really careful with these commands and there we go so now if we type in Echo instance ID we now have that so what we can do is just run this command next and see when it's ready the only thing is I have to make sure I specify the oh the region here it's not going to work so i' say Us East one I'm trying to go as quick as I can all right and type in clear here paste this in and hit enter and so now it should wait right okay so nothing super exciting here as it's not showing us any output I don't think it's supposed to let's go take a look here we'll go check out weight here and we'll look for status okay the following weight example pauses and resumes running only after it's confed confirmed the status of the specified instance is okay all right so we'll go over here and watch our E2 instance spin up so what we're looking for is that status okay to appear and it has initializing right now and notice that it's still in that mode so I'm not sure if it's going to print something out to us or if it's going to to just return the terminal back to us um notice it doesn't show any output so maybe there is no output and it's just going to stop here but this will take a few minutes so I'm just going to pause here I'll be back in a moment okay all right so let's take a look here and see if it has its status check so it does if we go back over to the CLI notice that is returned so where would this be useful I would imagine that if you if you're running let's say A A bash script and uh you had a series of commands you'd want to have your like this command here and then you'd want to hold this one before you ran the next one it would hold up the script so that' probably be a really good use case for this uh weight command um though we didn't really demonstrate that in a bash script but uh you know in the future well I guess that would be what we would want to do with that but uh let's make sure we get rid of the cc2 instance I'm just going to do it the easy way and say terminate instance there you go and we are done we'll see you in the next one okay ciao ads CLI aliases allow you to shorten commonly used commands similar to how aliases work for bash terminal so here is an example of um setting up CLI aliases the way you would configure this is you'd have to create a new file called alias in your home directory in that hidden folder called ABS in a subfolder called CLI uh all commands appear under the top level group if there's other levels of groups I have no idea what you can do with them but if you carefully look at this example notice that we are creating alas called Amazon Linux Amis and basically it's describing images on ec2 with all of these parameters so now what I can do is I can type in ads and then Amazon Linux Amis and so we should just say that the a CLI will just pick up the file you don't need to load it refresh it or activate it the reason I have to state that is because when you use bash profile a lot of times when you update aliases and there you have to refresh it but for this you don't have to do anything in particular uh you cannot override existing command Flags so for example here we have a a new Alias called hello and we have easy to describe hyphen vpcs and the output is Json and so the idea is that when we go ahead and actually execute this command notice that we put output table but notice that the result is Json because here we put an output Json you can't override it down below when you actually execute it but you can append new commands so let's say we have easy to describe bbcs what we can do now is add output table to it and that's going to allow us to have some customization so you have to consider uh what you want to put in the alsis and what you want to leave out if you want to uh tweak it later later on the these aliases come in two formats the first is simple Alias this is a oneline command that maps to an alias so um here we have uh VPC W2 and so I guess the W2 is to say list all vpcs in West Us West 2 and we have it as a oneline command this one here also is a oneline command but notice I'm using the backslash to break it up into a multiline command but we still consider this a oneline command so just be aware that you have a a bit more flexibility there so single line um uh there notice that you exclude ads in in front of when writing aliases so that's another thing I should point out you don't need to put ads in there whatsoever um the Alias commands again can be multiline as uh I've shown here with the back slashes the other format allows for a lot more flexibility this is where you have a bash Alias and so the Alias is going to trigger a bass scrip for more complex logic so here um we have a command called bucket and then the idea is we are wrapping it in this uh F or this function here and then with inside of this it's actually going to go ahead and execute some stuff here so that is how you know where the uh script content starts and so here we can run ads bucket my bucket us east1 and so what we're able to do is we're actually able to provide uh parameters as uh the input here so normally we wouldn't be able to do this but now now with this we can do that so it gives you more flexibility so the idea is that Bach aases are pretty much for provide providing um parameters and other one is just you have straightforward single commands so hopefully that makes sense in terms of uh how much I actually use the uh use this in uh my dayto day I really don't use it ever um it does seem like a cool feature but for whatever reason I just find it easier to make bash scripts so you decide what works for you if you're working on a team uh having an alias file could be very useful to share share with your team so it's not a bad idea but again in practice I don't find myself using aliases too often hey this is Angie Brown in this follow along I want to take a look at uh implementing aliases so aliases uh in theory seems like a great feature I don't feel like I use it that much I'm going to go ahead and create a new folder and we're going to call it aliases okay and then we'll create a new file in here and then we're just going to call this file Alias and the idea is that we can write in some aliases so imagine that we are using a CLI and there's a large command that we want to run again and again and again and again um maybe we're trying to describe something in um uh let's say let's say for vpcs and then in here we want to maybe filter some information no maybe not vpcs but uh we'll say subnets instead and I'm just see what options we have for filtering so h let's go take a look here and say a CLI describe subnets okay we'll go to version two let's take a look at what some filters we might have on this um we might want to filter based on state right so we might say here name state values and then we only want ones that are available okay they're all available so that's not going to be that much of an interesting of a filter um then let's say we want to change how the output look so we say query and we're going to say subnets Square braces and then maybe we just want their subnet ID and then let's say we want the output as a table okay so we hit enter that looks great but uh and maybe we also want to do specifi a region but I'll leave that out for now and so what I can do is I'll copy this command okay and we'll bring it over to our Alias file here I'm going to paste it in and we have to have this top level part in for all of them we're going to take out the word eight of us because we do not need it and we're going to put an equals here and we're describing subnet so I could call this as um dis desk sub okay or D sub all right and we'll save that and so the idea is that if we take this file and place it in our um our adabs CLI Alias folder then we're going to be able to uh use it right away so what I'm going to do is just make a command here to like copy our Alias file to the correct location so it is right here so I'm just drag it out and paste that in here we'll say uh we'll just say copy this this to ads CLI Alias okay okay so we'll just see if that will copy and then I want to make sure that file is there so I'm just going to go ahead and Cat it let's make sure that file exists so we say copy your file make sure it exists and I'm going to go ahead and type in clear down below here so now we should be able to execute this Alias let's see what happens we type in ads duub and it is executing this command up here okay now I need to point out that if you want to do uh you want these on uh break these up you can do this as well um there is of course the ability to set up um bash scripts I'm not going to do that I think this is sufficient um are alas is cool kind of yeah um do they are they better than bash scripts no not really so most of the times I just end up using bash scripts but it's nice that they have this feature um and just want you to know about it if you in case wanted to use it for some use case but uh it definitely won't show up an exam but there you go see you in the next one ciao itus allows you uh for specific subcommands to filter the data server side and this is really useful if you working with large data sets uh where by filtering the data server side it will speed up the request because it's only going to return uh the data you need basically a smaller data set um to specify filtering for the CLI it's confusing because the flag will vary it could be filter filters filter expression um also the inputs it expects so notice that it's uh it's expecting a list here and then it's a structured list of adjacent object so that's what it is for uh describe uh instances here but understand that this uh uh type can be different um and the parameters are going to be very specific to the uh service so uh just understand that the idea is that you're just getting exactly what you want back so here um we're saying that we do want our instance type name and it has to equal value of running so it's only returning back states that are running here and that we want an instance type with a T2 micro so yeah just understand there's a lot of variation here and you have to carefully read the a CLI but filtering is a super powerful uh feature to pair down your information before you get it back to okay hey this is angre brown and this fall along I want to take a look at uh utilizing filters so if we go over to the it CLI version 2 maybe there's something here that we could filter out and figure out where we could use this uh so what I'm going to do is go all the way down to maybe um vpcs again so we'll say ec2 sorry ec2 and uh or maybe we'll do subnets we usually get more of those and in here there will be an option for filters and so we have all these filters that we can choose from and generally the filters will follow this format it might not for some cases so you have to watch out for that uh but I think for the most part it will show up for that that um I'm not sure if we can supply a file to the Json it'd be kind of nice to see if we could do that but let's go ahead and make a new folder in our Cloud developer environment um so this one's going to be called filters and we'll say a new read me here and what I want to do is um get a bunch of subnets so we're going to say ads ec2 describe subnets and we're going to see what we get so yeah we're getting some data back here which is great okay um and so what we'll want to do is apply some filters so what could we get in particular and this is only returning one uh subnet because I'm running right now in C Central 1 and I'm just going to move this over to us East 1 because then I'll get a lot more e it's not that there aren't more in Canada it's just that I deleted my uh default subnet somehow so we go here and okay now we're getting good data okay great so for filters what can we filter on um there's State there's availability uh I would I think that I'd like to filter on maybe availability zone so before we do that I'm just going to query this so we can um see what kind of data we get back let's try this again here enter um and so we have availability Zone here so I'm just going to write a query so I can see what we're looking at so dot subnets or sorry not do subnets I think I'm running uh JQ we'll say subnets and then we'll do squares and then we'll say availability Zone and so I think that this should get us back all the availability zones okay great um and so we have none that are repeated here so so um you know that's one way we can apply the filter so let's go ahead and say filter and we have to provide a name and then we have the option of availability Zone okay and then we can say values and then let's just grab this one and so if we do that we should only get one or two back right two back because there's only two there so we go ahead and paste that there and we'll give this a go copy enter great so now we're just getting back those I'm just going to do this and just just put this on a separate line okay and so now we get more information here so the thing is is that we could also um we could also get uh other ones so let's say we wanted to get 1 a we could go here and then Supply this like this I'm going to put this back on here cuz it's a little bit easier to read when we have that query on the end there so we'll copy this paste that in so now know we're filter on a and then the B so that's as simple as a filters are um you can technically filter client side that's what query allows you to do but filter is great when you want to minimize the amount of data being returned and the syntax is usually pretty straightforward compared to query so uh that's all there really is to it so that's all I want you to know and we'll see you in the next one okay ciao querying allows you to filter Json client Side by using the JM path query language and so the idea here is that when you are going to use a sub command and it returns back data whether it's Jason or yaml or or or whatever you can U parse it so the idea is that this what is what you would nor normally get back uh if you did not use Query but let's say we only wanted to return exactly the instance ID which is over here and we want or sorry not that one but this one right here and we want the first five of them so the idea is that we're going to select inside of reservation we're going to then say give me back all of everything in reservation then go inside of instances which is right here grab everything and then select all the instance ID and then limit it to 0 to you have to pipe it to the next command so these are technically two separate queries and then the idea is we're going to get that result this query flag that you see right here it is available basically for almost all commands I say almost all because sometimes I think that it might not be available um and uh it's a super powerful feature we're not talking much about it here but when we look at the qu language we'll understand it a lot more okay hey this is Andrew Brown in this follow along I want to take a look at uh working with queries so queries is going to use that JSM path language uh for we'll just type in JP because JP um Library will get us to that uh page there it's really not working here today type in JP GitHub and it uses this JM path language okay so it's using this language um I'm really wondering who made this I just kind of wonder if like this was made by somebody adabs because I kind of wonder why is using it and you know what looks like they they do so that kind of explains why inabus uses it I always kind of wondered that e um because when I look at that here I just saw Seattle and that's the headquarters of AWS and AWS uses it in their query language and I'm not sure why they chose to use this over using um uh the JQ syntax but maybe maybe this is a kind of a spinoff of an Amazon employee uh employees project which looks like to be the case so anyway uh what I want to do here is just go to the CLI and remember that we have turned on here the environment variable set for on paral so if I type in ads I'm going to get um the auto prompt and so with the auto prompt we can go ahead and start exploring the query so I'm going to go ahead and describe vpcs if we do query here we can supply a query based on the uh JM path language uh you'll notice that it kind of starts to autocomplete for you which is really nice I can type in vpcs here after that we don't really get a whole lot of information but um we can get some here I'm not sure if I can do this VPC IDs I'm trying to utilize the uh the language of um JQ because I know JQ is so much better than uh JP because it's such a more robust length language but we'll go here and give this a go and I'm just seeing if I can select two attributes at the same time no okay no again I'm not the best with it but my point is that when you want to use this language all you have to do right is go here and use this tool and it'll make it so much easier to write it and then you don't really have to learn the language in detail describe vpcs quer three bpcs but you know what I think it's this when you want to select multiples you put it in a block like this and VPC ID there we go okay and so that's how we could uh get that format there um but uh we'll just go back up there and we we lost the query language there because it's stuck in the um CLI tool here so I'm just going to hit that so I can just write that in here so make a new folder here I'm just going to call this one uh query and we'll go ahead and write this out so we know exactly what we did so we have this one here there some examples um yeah so I mean that's the best example we could provide there but uh yeah it's pretty straightforward again not so much to talk about when you want a more complex query you could probably ask Chachi PT we might want to try that for fun so write uh a a CLI command and have a query that is going to um return back all vpcs uh in the data structure CER um bpc ID and then inside of it subnets okay let's see if I can even do that so let's see if it can do it so not exactly what I wanted but um again it has to do with the limitations of the query language um yeah so could do the subnets right so just understand that uh what you could do with JQ like JQ I could get all the subnets and make a very complex query this stuff is going to be really simple so often you're going to want to pipe things over to JQ and then do more complex things if you need uh more robust data structures but that's all I wanted to show you for query and we do use it quite a bit so we don't really have to cover it that much in detail here we'll see you in the next one okay ciao this data structure is going to be one of the most important day structures that you're going to be working with a lot so it's good to know what it is Json which stands for JavaScript object notation is a lightweight data interchange format it is easy for humans to read and write it is easy for machines to parse and generate it is based on a subset of JavaScript so here is an example of Json Json comes in two different kinds of structures uh you can see it as a collection of name and value pairs that is what we're looking at here on the left hand side and where we have um a a name or key and then its value which can also be another object so I'm just going to clear that out there so in other languages uh these are realized as an object record struct dictionary hash table keyed list or associative array so you know if again like if you use Ruby they we call them hashes if you're in Python we call them dictionaries so you've seen this probably before if you've done programming in another language uh but it's not the exact same thing but it's similar another you can do is have an ordered list of values so so basically a Json can represent uh something that looks like an array a vector list or a sequence So within here you can see that we have an array right here this could be the top level object so you could start as an object um like this collection key value pair or this ordered list Json is a text format that is that is completely language independent so that is why it's so popular in devops uh with programmers and things like that it's also just easy to read and work with uh but there you go what is yaml well yaml is a markup language that uses an indent based syntax notice I have quotations around markup language as uh it can provide a lot more flexibility than just being a markup language so here's an example of yaml but the interesting thing is that yaml uh easily turns into Json so uh if you're looking at the same equivalent this what this is what it is if you're noticing that the yaml one looks a lot more readable that's intentional yaml actually is a superet of Json so Json is considered valid in a yaml file you could take Json and literally put it in there and it will work uh yaml rhymes with camel uh that's the way you pronounce it and yaml is very useful as a semistructured data uh data document just as Json is a semistructured data uh document if you don't know what semistructured means it is data that contains Fields the fields don't have to be the same in every identity uh and you only to find the fields when on a need be basis uh yaml files can be either written with a yaml extension or a yml extension some programs enforce yaml as a convention that's what I try to do uh the best I can when we're working with cloud services some of them will want one or the other so you're just going to have to work with that I think the creator also said that they wish that they had uh said that it was in yaml so there was never that confusion um so hopefully that makes sense so why is it it called yaml well it actually stands for yet another markup language and the naming is a joke because so many other markup languages exist and it was intended to replace existing markup languages like HTML XML sgml um so yaml is seen by many as a better alternative to Json when required to manually write out information whereas Json is really great when you are generating out code programmatically from a program that data set yaml was again renamed to yaml ain't a markup language uh because they're trying to make an emphasis that it's not a markup language so yaml uh is used uh is using the the AML in the name and this again is another kind of joke that um programmers or program langu language creators like to make uh this is what we would call Meta as it is something that is selfreferencing or selfaware um and the reason why they made this change is they wanted to distinguish it as no longer being a markup document but being more of a data document and that's why we said earlier in quotations why it was called a markup language okay let's talk about a very very very useful uh command line tool called JQ and this is used to parse format and transform Jason's response uh from other commands and the best way of thinking about JQ is that it is a filter so imagine a coffee filter and then you get out coffee if that's what you're into I don't drink coffee but that's the best example I could think of um so the idea is that we have an input so here is some Json and so we want to pass that over to JQ so notice here that we use an echo uh that's a simple way we could also pass a file to JQ but the idea is it's going to go to JQ and then we have this filter language uh in order to select what it is that we want to select um and then we're going to get the result out as Mars because we said period so select the object then select where it says hello and then select the first array um and so JQ is uh one Tool uh that we can utilize and it's something that I use quite often it's not ITA specific it's just extremely useful command light tool that you should absolutely know how to use so there you go hey this is Andrew Brown this fall along I want to show you JQ so JQ is a library that is for parsing uh Json data it's extremely useful and often comes preinstalled on operating systems so uh In This Cloud developer environment for git pod it actually already is preinstalled um and so what I want to do is get you some handson experience with JQ so going to make a new folder here called JQ and I'm making a new file called readme.md I actually have this other repo where I have a tutorial on it so I'm just going to go ahead and copy all this contents here we'll paste it into here now if you are watching this video you can go to it examples repo and grab this stuff and start using JQ if you don't have JQ installed you can install a variety of ways so maybe you need to do a pseudo app get install JQ for Linux go look at the instructions on how to install it for your machine I'm sure somewhere here it explains that um but anyway let's go ahead and start utilizing this so describe epcs is a really great example as that's going to return back some uh Json data and so notice we have Json data here the First Advantage here is that if we do JQ and put a period which is selecting the root document that's how we select the root document JQ is right away we're going to get um this nice formatting okay so that is one really nice Advantage here uh just by passing it along but let's start selecting things so what we can do is Select into the VPC and grab information there so we go ahead and copy that and I'm going to just go ahead and paste that in okay and so now notice that it's selecting within the VPC and we have this Square brace U what would happen if we did not include the period would that work and the reason why I include that is that maybe something like JP would not require you to have a period you could start selecting it but in JQ you have to explicitly put that period in the front just select at the root of the document so understand that you can't just not have a period there um let's select within that array so I'm going to go back up to this command and we're going to see that we have an array here and let's put some Square braces on the end and notice that it's now selecting within the array and so now it's curlies and so with that what we can do is we can say do cider block and um it didn't exactly work in this case because the reason why it's not working is that we need to select within the square braces because remember we do this right it's Square you can't call cider block on an array it's within the object so if we go back up to here and then we typed in cider block notice that we can get the value but let's say we have Json and we don't know if it's going to consistently return back data so maybe it does return an array or sometimes it returns an object we're just pretending that we're not doing describe vbcs here and just in general uh sometimes we have some variability with our Json what we can do is we can put a question mark and the idea is that if it encounters an error it's just going to ignore it so cider block is something that should not work right but notice it doesn't throw an error so uh you know if encountering an error just ignore it okay and this is really useful if you don't want your Bas scripts to exit out and that could be a case that you want to occur uh this will result in an error because we are not selecting from the root of the Json this will select uh vpcs key from the root of the Json object this will select the array of vpcs returning back um returning back Json objects this will error out because we are attempting to select an attribute on an array and not the array objects okay so then down below here this is obviously we just tried that and that worked so that's how we would get that right now I'm in an area that only has a single VPC so I could switch to some other region um or I could just create more so I'm just going to go here to C Central one I'm just going to create a couple more vpcs and yeah that's okay I don't care it's just erring out we'll click here and I'm going to say create VPC and just say my VPC to and I don't know if that one was used but 17231 00 create doesn't like that um oh sorry 16 we create that VPC we will try this again we'll say create VPC uh my vbc 04 I don't care what it is we'll say 120000 16 I'm go ahead and do that and so now I have a couple more VP C's okay and what I'm going to do here is just type in clear and I'm going to hit up and then now notice that I'm getting more data back okay so um just understand that when you're doing Square braces it's selecting the objects within the array so it's not a single object it's an array of objects but it's it's trying to select from those objects as opposed uh of of on the array itself let's say we just wanted to get back the first one then we could Supply a one okay and so this one is get back uh attribute CER block from first VPC get back all attribute CER cider blocks from the array of objects here this would be an index so this is between one and two so get back um the VPC within the range of 1 and two and return the cider block so if we could do this one here and copy and hit paste we get an error um cannot index array with cider block string um okay so that throws an error and the reason why is because if we just do this let's go above here for a second what do we get we get an array okay so just understand when you get that back we're going to have to put those squares on here so if we put these squares on here okay come on trying to copy and paste here but we put these uh these squares on the end here notice that now it's selecting the ray of objects okay we go back up again we take this off now it's an array now of course it's only returning one because we said between the range of one and two but it's still an array of objects so now if we try cider Block it's going to return back that one so select within within the range of one and two we actually already did that before down here select within the range of one and two and technically this one is the one that's actually working here okay so we go down here and so this one's going to return an error so this will return an error because the range will return an array and we already know that we need to get the array of objects not the array itself now what if we did a range of one to one what would we get back you think the first object but we actually get nothing if we did two to two what would we get back nothing so this will return nothing because it's an index that is equal to zero right one to one so that's kind of interesting then we can select a range of so let's say this is not a range but this is going to get get the first and second VPC so that's another thing we can do this one's really useful I think we'll hit enter times it's pasting weird stuff in here we'll hit enter and so we get the first and second VPC and we'll take a look here notice that it's returning an array of objects and not the array so if we go ahead and try this this should work right so we go down below copy hit enter and we're getting the uh the two there um let's say we wanted to get um another tribute we want to also get the VPC ID what we could try to do is we could try to string a bunch of um commands together and so what you can do is you can put a comma and that will select two things at the same time so we copy this type in clear what I'm hoping to get here is the first cider block and the first VPC ID so that looks like it's working now what if I got want to get for the first and the second one let's go ahead and try that what do we get we get the first and the second so that's really nice okay what what if we want to um just get it for the first one I mean that looks like the same as this line is just the same line repeat it twice it is so we'll just take that one out um another thing you can do is you can uh take let's say an array and then you can pipe it to a sub command and then just select from there so the idea is that you you take this you pass it here and now this sub object becomes the root the array of objects is the root so you you'd be selecting period from that so what we can do is copy this paste in and so this is very similar to this command here right so this and this right I'm just going to copy this one here this right and this are the same thing so why would you want to do this because you can have more flexibility when you're piping over to here and you can really um make the um expression a lot easier it really depends on what kind of expression you're writing so some expressions are easier to write when you're piping them here some simply cannot be done with piping them to a subquery and so just understand that uh you'll learn that as you try to write more complex JQ if you need to if you don't need to that's okay you can get by with just basic JQ but let's continue on here so we uh we started the piping here okay so there's this one and this one looks exactly the same so I'm just going to take that out so let's try to select the CER block and v v VP uh VPC ID so we'll try this okay so that's very similar to um not sure what it is but very similar kind of similar to this one here it's it's selecting from all of them notice we're getting back text let's say we want to return it back in adjacent structure well I think we can put these curlies on here and then provide the key and ID let's take a look and see what we get and it didn't copy properly just type in clear notice I'm just struggling with copy paste here we enter um and it complains um VPC 0 is not defined at the top level okay so the issue here is we still have to have that period right so we'll go ahead and do the next line this will result in an error we'll go ahead and copy the next one some of these are intentionally supposed to run into errors so now we get this back in adjacent structur so format the first VPC uh in this format and also notice here we put a zero in here and up here we've been doing ones and twos so just understand which one's been selecting and that's something we should be paying attention to so let's go back over to here copy this one right we did enter and notice we get this first second third so what happens if we do one okay that's actually the second one this starts at zero okay so we wrote first and that's not true so I'm just looking here so this is actually gets the second one and then this one gets the second and third one all right so that's a good indicator here um let's say we wanted to get back um adjacent object for the the first and second V VPC ID in that structure that we want a lot of times you're going to want to turn that back back into Json right go ahead and hit enter and there is our structure so that's looking really good um this line down below is identical again I'm not sure why I have identical lines in here but what we'll do here is just take this out uh this one here is just showing you that you can take uh a JQ object or like adjacent object and pass it here and parse it that one might not work so maybe we have to wrap this in double quotations let's try this should work it's in here no it's not working we'll take it out of here but I could have swore that would work um what do we have here we have 01 and so now it looks like it might just take in the actual key name let's see what happens if we do this what do we get okay so notice it's picking up the keys so here we're providing them and here we're just saying use the key the key names it has um then we have this one again we're just piping here notice this syntax is a lot cleaner right so this was a little bit more verbose this one looks to be the same as this one here so we'll give this one a go will this work because it doesn't have a period in front of it okay so we didit enter notice we got an error so that's going to have an error because there's no root on it no root being selected so we'll go get the next one hit enter okay so we get back that and then we have a much more complex example so I and I believe I actually used this for something um like in production and so that's why I brought this example here the idea is that let's say we want to export these two environment variables okay so now we have a username and password and the idea is that we're providing an argument to JQ saying set this as user set this as password and then uh make this Json um this Json here so we'll go ahead and try that out we'll hit enter so now we get back at Json object and so this is a great way to take environment variables and then turn it into ajason file which you might need for some kind of configuration file and then the next thing we can do is we can actually dump this out to um an actual Json file so I'm just going to make sure that I'm in the right directory here I'm just going to go over back to JQ there we go and I'm going to run this with it enter and so now if we go here we have a creds do Json and we have a user and a password we can c that out to make sure that it works okay I'm going to delete this because sometimes G GitHub will think that this is like credentials and they might like complain to us so I'm going to take that out but you know hopefully that gives you kind of an idea of how JQ works if that doesn't make total sense you can read up on the documentation here as they have a bunch of examples what I was doing is I was going through and figuring out all the things here you can read them if you want there's a bunch of builtin functions and stuff that we weren't using so there's definitely a lot of stuff that you can do so like let's say we wanted to floor a number notice you could do JQ floor so there's a bunch of stuff that we could do um other than that but basically this is the basic filtering that we were doing which was uh selecting stuff and so hopefully that gives you an idea about how how all this stuff works but uh yeah there you go so the JP library is a command line tool that filters Json data if that s similar to JQ that's because it is and it was intended to be easier to use than JQ because it utilizes the JM SE path query language whereas JQ has its own uh language so here's an example of using JP and you can kind of get an idea of the JM path query language and the way you would install this tool is actually through the node package manager if it's distributed as any other language I'm not sure but you know I've said this before that uh command line tools uh can be written in very specific languages and so you have to have that language installed to use it but most most people uh these days have no GS so it's not a big deal but I find that most operating systems will have JQ installed by default and not JP um but you know one advantages of JP is that because it uses the JM path uh syntax language or query language this is the same language that the query parameter uses when you're using aable CLI so if you know how to use it for this you can use it for that so that's why some people prefer to use JQ because if they know their JM path uh very well then they can translate that knowledge over quite well um I'm going to just tell you I don't find JP easier to use in JQ and I find JQ to be a lot more robust and I definitely prefer it especially since it's always there and and ready to go in most operating systems but uh you decide what you want to use but you will often see uh scripts using one or the other or or interchangeable so I want to make you aware of both of them okay okay hey this is angrew brown and in this video what I want to do is take a look at JP so JP is similar to JQ it's supposed to be easier to use and it uses the JSM path language um now the example I showed was installing via the mpm install uh originally I remember this being a JavaScript library but looks like now it's written in go so um unless I'm mistaken I thought it was a JavaScript library but I guess could be go as well it has Brew instructions here um so I think that we could go ahead and utilize that there so I'm going to go and try to do that maybe the reason they made it Json was so that it would be more portable to other systems as JavaScript will require you to install mpm or note package manager so we'll go ahead and install that here I'm going to type in JP see if it works right away we get a permission deny it is it is uh stored here so it knows where it is so I'm going to do an LS typ in LA in this bin directory we'll take a look and see what kind of permissions we have so here is JP and it shows it shows that it has um this for git pod so I'm not 100% sure why I can't execute it so just give me a moment to figure that out okay so I'm not exactly sure on how to fix that but what we can do is maybe just install the other one so what I'm going to do is just stop my environment and I'm going to re open it up and this time I'm going to go install the one via node as I know that one will install again the only disadvantage is that you'd have to have node for it and this one obviously is a little bit more flexible there's nothing talking about pseudo here um I suppose that we could also install via a different mean so we could try this as well so let's try this before we completely give up here and and go switch over to node make a new filer here called JP and I'm going to go ahead and grab this line here say allow and hit enter and so now let's type in JP and so now this is less of an issue so definitely installing this via um the binary directly is going to give give us less issues but this is obviously if you're using Linux that was for Linux 64 um so now what we can do is go ahead and try to use this so you know here is the syntax language and you're going to notice that it's not the same it's not the same as um JQ so what we can do is bring up our JQ example and see if we can do things that are similar all right and the first thing I want to do is try to pipe something over to JP so we'll say JP and we'll see what we get we'll first that's our data good and we'll copy this and enter and saying provide at least one argument Okay so um I want to provide it a file we'll go to tutorials here and again just trying to look how to pipe to it so maybe we could I don't know it should just work okay what if I do period here no so just give me a second okay all right so going down here you'll notice that for the usage like they have Echo and you can pipe to it so that's interesting um maybe it's that we just need to provide some kind of key in here so maybe this is fine we just have to say something like vpcs there we go and so now we're getting data back so we're going to go ahead and grab this okay so this will result in an error we need to select something notice that we don't need to put a period in the front there I wonder if we could put a period to reference the root let's go ahead and do that and it doesn't like that okay so again different language not going to follow the same rules but it's not like super hard to uh utilize uh and it's also a very limited language so it's not particularly difficult but you know maybe we want to um uh select some information here so let's go back to the home and take a look at this example so here it's showing locations and saying select all the locations that are are Washington State sort at at not sure what at is and then uh it's joining them together as the final result here so maybe we could try a bit of that out so let's take a look at our data structure and try to figure what we could do with this so maybe we want to filter uh based on whether it's default or not um so I did create a few vpcs in here and they all should not be default so we should get three back okay so I'm going to copy this and go down here and we'll look at the syntax and so we have this so grab this here paste it in this side and so I'm saying is default and then we're saying false and because it's a Boolean I'm going to assume that it takes that enter and it doesn't like that there so I'm going to go back and take a look here carefully I mean looks fine to me what I'll do is I'll wrap this in quotations sometimes this might help so I'm just going to tell you I don't like JQ or JP I always find it a little bit tricky to use and I don't always get the results that I want let's go back here and make sure that this false is a it is a Boolean so I mean that should work okay we're not really getting back stuff back um we go back here and take a look here again I might want to try to select a tribute on it maybe it's just returning the data in an odd way so we go here and say cider block so that is no good let's try this instead that is no good um what if we try a wild card in here there we go so I can get a wild card of stuff but my filtration didn't work as as I was hoping it would not to say that it won't work but it's just um you know maybe the issue is that it doesn't like the title case here or maybe it doesn't like the operator um so we'll go back over to here looks like we can join it it and turn it into something so let's try that and so we'll make a pipe here do curlies and we'll say blocks and it looks like it has a builtin function we didn't use any functions in JQ but uh JQ definitely has the same stuff and we'll go here and do this okay I guess the at sign probably refers to the the current instance so we could probably put this here and do at sign there we go and so we're getting kind of an array of blocks so you know hopefully that gives you kind of an idea how JQ Works um a lot of times when you're working with JQ is going to be with uh or sorry Js Js uh jmes path is when you're doing queries and of course when we're doing queries right we could do the same thing here and see what happens so let's go take a look and see if we can literally use the same syntax here but using the query command um so we'll say ec2 describe of vpcs what I'm going to do is just bring this up a lot more because I want to see if I can open up the outputs panel I believe that's F5 and yeah we can't do that in git pod which is kind of frustrating so I'm going to go over here and open this up in cloud shell and let's see if we can uh get that uh get that preview there that we want I'm going to open this up in another tab so we have a lot more room to work with here and just hit X here let that load up so I'm going to copy this part and we'll try that out really quickly okay good and uh I want this open up in uh using the auto prompt mode so I'm going to go over to my git pod gamble file and just quickly grab that nvar because I don't remember what it is it's this and we'll go over to Cloud shell paste this in and I'll just say on I'm just going to turn it on always so that it's a little bit easier to work with here not what I normally do but we'll go ahead and grab this query and hit enter and so it actually returned the data so I don't think this actually worked I'm going type in export on this type adabs again there we go so easy to describe vpcs VPC that's right okay I just keep expecting it to show me more parameters e TR again here vpcs I'm going to hit F5 to open up the outputs and then we're going to open up query and the idea is that we want to see if uh we can use that JSM jsme path language here so yeah again generally don't like writing uh jsme path but when it autocompletes here it's less of an issue so we'll say we want to select all of them and I want say cider block and then I want to pipe it over here and we're going to join it and oh I think we need our curlies right so we're going to say blocks like this and then a comma and then that and then close it let's take a look if we got that syntax right yeah this one's using single quotations something I'll definitely need to do is wrap this in doubles because it might be running into problems here and just notice that it can't uh figure it out as we would hope it would let's go ahead hit enter and see what happens but notice that it produces the same thing so we could do this right or we could do this so this is these two commands are identical Okay so but you know hopefully that gives you an idea um if you want to learn more about J uh JP you absolutely can a lot of times you can just tell chat gbt or larger language model Say Hey I want to have this can you write this query for me and that's usually good enough and you don't have to really learn the full details of those query languages but you just want you to be aware of that functionality there but we'll see you in the next one okay ciao so yq is a wrapper of the JQ Library for Json to allow us to filter yaml in various ways uh so just like JQ this is specifically for yq and it is super awesome um the thing is I believe that it can also output to Json so notice here it's taking in a yaml file and then you can say hey output this um to Json now yq is not a cloud specific thing it's just a very useful tool that I think that you should know as uh working in the cloud you're often working with Jason file so we should know these very common tools uh yq is useful when you need to transform yaml data from one structure to another it's uh it uses the same JQ language to filter information uh yq has yaml specific utilities uh and it is a good tool to validate how an input yaml is interpreted it is also a great tool to see the Json cled of your yaml file so you might have a yaml file and need to quickly convert it to uh to Json and so this this would be a tool that you could use to install it you can install it uh via code directly from GitHub um if you're using a package manager most of them have it so like here we have Brew install yq snap install yq uh I would I would also think that if you're using like app get uh install probably works there as well let's take a look at some common or notable flag features um so one thing is you can print out the output with colors and do pretty print so to make let's say you have a a yaml file you just want to see it printed out in a nicer format you can do that you can take Json as an input and produce Json as an output so even if you are uh working with Json you can work with it in both directions you can output to a CSV or tsv so that's really useful when you have AML file you can also uh take XML input and produce Json output you can process front matter from yaml so front matter is a special syntax that uh is used um mostly for static websites so it's interesting that it and process that it can split multidocument yaml files into several files um and it has shell completion that can be turned on for easier use it can control outputed space tab size I mean you really should keep it at two but it can do that so a lot of cool functionality that you get when using yq um there are generally three ways that you can input a file into yq just the file name the filter along with the file name or you can pipe the input from another bash command so just matching the these up cuz my graphic showed up in the wrong order but here just directly there or you filtered uh filter along the way or you can pipe it into it um piping is a common use case when you get Json output Json from another program loading AML file is common if there is a configuration file part of an application codebase you need to filter and yq appears to have an embedded version of JQ so when you install yq you don't have to go ahead install JQ separately so hopefully that has encouraged you to go take a look at yq uh as a extremely useful tool to add to your devops toolkit hey this is Angie Brown in this video I want to take a quick look at yq so yq is a library that is similar to JQ it doesn't have the same functionality as JQ but it supports uh most of its commands what I really like it for is converting one file type to another or quickly working with yaml files so what we can do is go ahead and install this and uh I'm not sure why it doesn't show the installation instructions here I'll go to version three and I know that they show up right away um I thought they did go back over here click on yq that's fine I believe it's Brew install yq I'm going to go ahead and just make a new folder here called yq okay and we'll make a new file readme.md we'll say Brew install yq and so yq is installed so we should be able to uh take a file and use similar syntaxes so if we copied this first line here from JQ from our JQ video and we just try to pipe it to yq would it work let's hope so now it might not work right off the bat because we might have to specify the outputs and things like that we go ahead there and we're starting to get some formatting again I don't really want that I wanted it to be yaml so I'm going to say it might be reading it as a single string as opposed to actually Json so I'm going to say parse Json and now we're getting back yaml and we can also say like we don't necessarily want it to be um the output to be that we maybe say CSV let's see if we can do that no it won't work for that but let's say Json so we're taking a Json and exporting out back Json uh I wonder if we could do XML yep so we get back as XML if we want uh and you could parse XML and get back Jason or yaml but you usually you're going to want yaml here all right and I suppose the idea is that we could probably select things so let's see if we can do that so we'll try vpcs so be trying some of our syntax here uh we have to have that period in the front remember this is jq's language okay so notice now it doesn't select that so now we can do this and say um Square brace cider block okay and you get the idea it's very similar um it's very similar to JQ but uh you know you might run into some issues and that's why um I would probably recommend that you use uh JQ when you uh for most use cases and use yq when you're working with ANL files or you have simpler queries because if we go down this list here I'm pretty certain like these ones aren't going to work um or if they if they don't maybe there's some kind of change that we have to make so let's just try this one here I'm very confident that like this one won't work here go here and select this yeah it says input syntax error on cider block so you know again still nice tool and I want you to know about it and uh you might see me use it a few times CU I I really like yaml but uh there you go so when you're working with the a CLI sometimes you're going to have parameters or command flags that expect file names as arguments and some parameters will accept either inline text or alternatively a file path so here are examples of um different ways that we can specify a file so notice that we're giving it a relative path uh as we just have the protocol and then it's followed with the filter then we have an absolute path notice that we have triple um forward slash and that third one indicates that we are getting an absolute path and then sometimes you will need to um provide quotations around it or single quotes because we might have a space in the path um some more examples here is that you can have explicit relative so we can say uh I mean this is relative the same thing with this one up here but the idea is that this one is explicit because period indicates the relative path we can specify the home directory which is represented by a Tilda you can go back a directory with two dots uh for Windows command prompts the slashes for the directories need to go the other way because windows for whatever reason or MS DOS or dos or whatever command prompt uh it likes to go uh as backlashes as as opposed to forward slashes um there are commands that can take binary data and so you might see the protocol have a file B in front of it uh instead and then sometimes you need to encode your files into base 64 and so that might be something that is necessary but that's what this B is for it's going to um uh say that this is a binary file and I don't have this Illustrated here but there are cases that um you will have where it will want to file but it doesn't want you to have any file protocol you'll literally put the name why is it uh inconsistent like that it just has to do with it us having a bunch of teams and each team can do whatever the heck they want so sometimes they will have a flag that can accept it with a protocol without a protocol or it can expect it can take a string or a file or sometimes it'll have separate flags for the raw content um or specifically to reference a file so just understand it's going to vary a lot there but that you do or you can pass parameter files and we will do that uh through um our follow along content okay let's quickly talk about base 64 encoding because when you're working with clis sdks things like that sometimes they want Bas 64 encoded files generally you can use um a programming library to do the work for you but we should generally have an idea how the encoding works I'm not going to explain everything behind it but I'm going to give you generally the stepbystep process in in terms of how we turn let's say a piece of text into Bas 64 so base 64 is a binary to text encoding scheme that represents B your data generally as asky could you have a different encoding absolutely but we're just going to stick with asky here uh so imagine we have the word man and we're going to convert this to basic4 so we're going to represent all of the letters as their binary equivalents and so there'll be like a table that will indicate that at index 77 the number 77 represented binary will map to M so the idea is we're going to take all of these um all the data from our our binary um all of our binary information from our text represen in8 bits and make it a stream so I have a space here but just imagine that this is a single stream right then what we'll do is instead of reading it eight bits at a time we're going to read it six bits at a time and then we're going to use this base 64 table to map uh each six bit to a letter so um if we just get rid of this here for a moment I'm just going to clear up my screen here so the idea is that we're matching the first six 1 2 3 four five six notice that's the same here okay this is six and this is six and then down below um the this six the first six uh bits represents 19 because if we go over here to our table it looks exactly the same and that's going to be T and so we get T wfu reading through this stream okay and so basically we're going to end up with t wfu and so that's going to be the same thing as our man text if we were to decode it um so hopefully that kind of makes sense in terms of that process um why we need b64 encoding you know etc etc not going to explain that here but I just want you to understand that there's a process happening and that you can use um programming languages or programs to do that basic 64 encoding for you okay hey this is Angie Brown and in this fall along I want to take a look at base 64 encoding uh and do a little a little bit of programming with it so we have an idea how we can base 64 encode things ourselves so I'm going to make a new folder in our API directory called base 64 and what I want to do here is just create a new file let's try to do this with Ruby first as uh that's a common way that I know how to uh work with B 64 encoding so I'm just going to call this H we'll call it main. RB we don't need to name it anything fancy and if we look up base 64 encoding uh for Ruby it is super super easy to work with um so we go here it's not the latest version of Ruby the latest version is like Ruby 3 or something but um Ruby doesn't change as much as versions like python does so it's not a big deal so the idea is that we can do encoding and decoding so I'm going to put these here and uh the idea is that we have send reinforcements and so that's going to produce an encoding and then we can decode that again so we'll just say puts ENC and then puts plane and we'll see what we get so this will be puts uh encoded encoded and then decoded and this is uh part of the standard Library so there's nothing fancy that we have to do to be able to do this okay I imagine it's going to be very similar for python uh so just say text text right and I'm going to CD into uh this Bas 64 type in clear and and we will go ahead and try this out so we'll say Ruby main. RB we don't have to do bundle EXA because we're not using that um but notice that we can code it so we take this and it turns into this and then we can decode it as such so that's kind of interesting we can take a look at python so we'll say python base 64 and it's probably as simple as that one so go here um yeah it looks like that so looks like we can copy that let's give that a try for fun so we'll say U main. pi and we'll paste this in very very similar as you can see whoops um is that how that looked so these ones are outputs okay so that's encoded and then we can go ahead and grab this one here paste it in and say print encoded and then we'll print decoded and see if that works so Python main.py and there you go so it does the exact same thing um it might be interesting to try out the exact same uh text and see what we get typ in clear and uh let's just go ahead and see if they actually are the same so yep they are identical as you can see there I assume b stands for binary I'm not exactly sure for um for python there but I mean that's as complicated as p as encoding gets there might also be a c command so uh how to encode decode B 64 uh Linux command there could be something here oh yeah base 64 yeah yeah yeah of course so uh this might already be preinstalled I think it is we can type in man B 64 if we want to find out more about it so it tells us how it can work we have a decode uh it looks like it probably automatically encodes so I would think that uh the way this would work uh we'll just make a new file here readme.md I imagine we can just pipe something to it so we say send reinforcements grab the same thing paste that in here oops and then we'll say base 64 and I'm just trying to copy this okay looks the same to me um no it's not so notice this one has it's very similar uh but it's a little bit different what happens if we do single cotations because if you use doubles there could be um a line break in there and that could be affecting it let's go ahead and try this okay let's go back and run the Ruby one and so there's definitely a difference here this one has CW and this one has two equals and this one has an o so there's probably some kind of little difference ignore garbage wrap so there's probably some reason for it again I don't know why but um let's see if we can do some uh decoding so I'm go here and say Echo I'm going to copy this and then we'll say base 64 decode and it it it decodes no problem what if we were to take the Ruby one and try that just curious what would we get would we get the same thing we get the same thing so for whatever reason there's a slight variation there it's still working correctly I don't know why one has double equals and the other one does not but that's all I really want to show you for Bas 64 understand that the ADI will need to Tak in Bas 64 and sometimes AI will automatically do the base 64 conversion for you um it really depends on what you're doing but uh same thing with the SDK it might it may may or may not do it but uh there you go okay see you the next one ciao let's talk about quoting in strings when using a so when you're inputting parameter values like Json objects quoting will help avoid weird termination or incomplete interrupting of commands so you can use single quotes so here's an example where we have a key name and we have a key pair notice that there are spaces in the name for the key name those spaces could mess up or uh prematurely terminate the line because it'll get confused you can also uh use doubles within singles um so this is an example uh where you can do that and the advantage is that you don't have to then escap if you had doubles then you have to escape the doubles in the interior so you don't have to do that just use the singles on the outside then you can also use just plain double quotations um on the outside if you want you cannot use singles in doubles so here's an example where we have doubles and then we have singles on the inside why that is I have no idea I think because when it tries to Parts the Json Json expects to have doubles and so it just doesn't work there so um it says invalid Json expecting property name and Clos in double quotes so yeah it's a Json thing it just wants it to be formatted with doubles on the interior the examples above are for Linux Unix for Powershell and Windows command prompts can be different so just understand that it is going to be based on that system parameters I'm specifying for Linux uh and and Unix because that is what is most commonly used okay when you're using AB CLI you have command flags and those command Flags actually map to a parameter type and that's going to determine what kind of value is expected to be accepted by that command flag so we have strings we have time stamps we have lists we have booleans we have integers we have binaries um or blobs and so when you are looking at the um the Abus command line reference the the website they'll tell you what it takes so notice it says timestamp and so it will even tell you the format okay so let's look at some very simple examples some of these are very obvious like Boolean we don't have to explain what that is or integer um but let's just take a look at a few like string Tim stamp and list so for Strings um they can contain alpha numeric characters symbols white spaces uh when using white spaces you need to wrap them in quotes for timestamps uh the the uh we can provide a bunch of different formats but they have to be in the iso 8601 standard so these are the formats that are accepted often it will just show you in examples you just use the example that is provided but notice that there's a bit more flexibility there um like some people might like using the Unix epox time it's up to you for lists this is where you could be applying U multiple um uh values like strings uh to a flag and notice here that um that you're using the space to denote that there are two things here it's not a comma it's a space so there of course there are more parameters they're pretty straightforward but I just want you to be aware of those parameter types and notice that when you are entering stuff in don't assume everything is just a string okay this one's a really cool command and I wish I would remember to use it a lot more and it is the skeleton uh command so uh the generate CLI skeleton allows you to generate out a Json or yaml file that can then be modified with inputs and used to mass import configuration for a future command so just going to get my pen out here so the thing is there's two variants to this command but here this is our input so this is what we are inputting and then we're saying uh output it to E2 run instance. Json and then we have a a line right below and that's just going to be for the yaml equivalent because sometimes you can output um uh you can input Json and sometimes you can input yaml the idea is that you're going to get this Json file out and the idea is that then we can use this later on to load up a CLI command we'll see that in the next slide not this one here but the commands are commonly used with CLI input Json CLI input yaml but not all commands except both Json or yaml input so while you can generate out yaml you may need to convert it to Json for specific subcommand so just understand that there is some variation there okay so in the last slide we're talking about generating out um a skeleton file and so the question is how do we actually utilize that uh in F commands and that's where we use the input flag so the idea here is that we're going to pass um that file back to the CLI and the idea is that we don't have to supply all of those parameters because it's going to get autofilled by that file so you have to check every single sub command because uh you have to see if they actually allow you to um Mass uh set values and so it will have a flag like CLI input Jason or CLI input yaml it might have both it might only have one for yaml or one for Json so just double check the docs but this is a super powerful feature that again I really wish I'd use it a lot more okay hey this is Angie Brown and in this fall along I want to take a look at how we can utilize the a CLI skeleton templates and the inputs um in order to um make it easier to work with larger configuration files so I'm opening up my cloud developer environment and I'm just going to make a new folder in API here called skeleton okay and what we'll do is just create ourselves a readme file so we can just start to make things a bit easier and so the idea is that we want to find something in the a CLI that's going to require some um interesting configuration so there's not a whole lot to do with um S3 but maybe an ec2 instance will give us a lot more configuration options and things we can play around with so I'm going to make our way over to ec2 and I'm starting to remember remember that what we want to utilize is run instances so let's take a look if it even supports it so what I'm looking for here is the generate CLI skeleton so if it has that then we can utilize it it can take an input of Jason or yaml so that's really nice so what we'll do is go down below and just read about it so prints adjacent skeleton to standard output without sending an API request so the idea is that we can kind of start writing what we want and then we can get a skeleton and then fill in it fill in our options so I'm going to go down to uh examples let's see what we can pull out um so this we don't really need I'm going to go down a little bit more um this one's a little bit more specific now we'll go with the more basic one actually now that I've thought about it going paste this in here we'll say allow and um let say generate out skeleton for ec2 and what I'm going to do here is I'm going to go with region Us East one because last time I checked my CA Central One the default region was deleted it shouldn't be but it is and it's causing me some issues and I don't feel like figuring out how to recreate it right now I'm sure we'll cover that in the VPC section so I just want to make my life uh really easy here today and so in the North Virginia I need to go get that Ami so if we go over to ec2 and we attempt to launch an instance we can then go ahead and grab um the Ami code and so I just want it for this one here the Amazon 2023 which is totally fine we'll grab that code we'll bring it over here and again this is the future and it says 2024 whatever just grab what you need to grab okay um and I'm going to go ahead and paste that on here understand that these Ami codes are region specific so if we're running this in Us East one we have to get this in Us East one otherwise this code will be different even if it is the same software image in in a sense the code will be different per region so we have some information here let's go ahead and see what happens if we add that skeleton so I'm going to look for skeleton again here okay and it looks like you can take some parameters so prints Json skeleton standard if provided no value uh or the input prints a sample Json that can be used as an argument um so it sounds like you can provide it a string so I'm not 100% sure um how you provide that value as we're not exactly seeing any examples maybe there is one if we just copy this and search a bit no but we'll give it a go and see what happens so we'll go here down to line and we'll paste this in and we'll put that there on the end actually we can just bring this over a bit more and so the idea is that when we run this uh we should get a Json Fallout and it might print right to the screen so we might have to do a little bit work before we do it actually I'm going to clear that out and just make sure I'm in the skeleton directory just in case it does dump something I want it to dump in its current directory and remember that I already have loaded in my credentials here so I don't have to constantly do it so it is dumping it to the screen here so if we really want to make use of this we'll have to I guess go here and just say skeleton Json okay I'm going to type clear and we'll copy this again and we'll try this again hit enter and so let's make sure that our file is populated and there we go personally I like Json so this is a great opportunity for us to convert Jason to yaml um do we have yql installed yq no so let's go ahead and install that so just look for yq install Debian I'm imagine we can probably use snap or something else for that Brew we can do Brew let's go ahead and try that Brew is okay um and it's installed by default on um uh it's installed by default on this here but it doesn't want the at sign 3 it just wants the normal one and it looks like maybe there's a version four now so so maybe it didn't work because it's at 3 and it's really at 4 so maybe this documentation is oh is now deprecated okay so why why is it there must have dropped down so we really wanted four but that's okay we installed and it it brought in four but what I want to do is convert a Json file to yaml so we can go here and uh you know there should be some way to do it I'm pretty sure we have it in in the slides let me just go look in the slides and see if I can find it quickly okay all right so I had a look and I think that we can figure this out so there is this command here we'll go ahead and grab this so we can partially solve this and so the idea is that we want to ingest um that file once we have a working out copy and paste it into the read me but the idea is that we're saying get it uh be able to parse the format Json we'll hit enter and there it's returning back um yaml so that's exactly what we want so just say convert Jason to yaml and you know we could have done is probably piped it in this way but we'll just run it separately here because you know we've already done this here and the idea is that I want to Output this into a skeleton yaml file see if that works okay uh output is now Auto normally I IQ would output Json but for backwards compatibility yaml has been set so please use hyphen o for explicit output so maybe we should be setting that so that there is uh less less uh chance of it mucking up uh we'll go over here and take a look and it did produce out yaml um I'm going to run that again because I want to see it without the errors so we'll go ahead and delete this permanently we'll grab this and we'll hit enter and so now we get yaml without any complaints and the reason I converted to yaml is because it makes it super easy to replace or substitute these values okay so now we can go in and just fill this in based on what we want um we are getting a lot of information and it seems like a lot to fill in so I wonder if we can just conditionally fill in what we want to fill in because I don't want to do all of this right like I don't want to figure out what all these device mappings are but um you can see what we have here also notice that it didn't pick up the values we passed in so we did have some Flags here and so these flags didn't really matter so what I'm going to do is just um cut this out of here for a moment and really the command should just be this because obviously it didn't take any of the uh information we told it to do so we'll go ahead and I'm just going to delete these again and just make sure this works as expected so we'll hit enter and we get our Json here and I would really like to get it in in one go so I wonder if we could um pipe this in one go so I'm just looking here how we could do that um I imagine what we could do I don't know this will work but I'm going to try it but I'm going to do this and then I'm going to try this skeleton yaml sometimes you got to play around this stuff to figure out if it works and we'll do this and let's see if this just works in one go no it didn't like that so there's probably some way to figure it out again it's not that big of a deal we could probably ask chbt or some L llm to figure it out for us but you know I'm not I don't care that much okay so I'm going to go ahead here and just delete these out and we'll run this individually so we'll run this one and then we will run this one okay and so now we have our yaml file and if I close this out and just go here we can go ahead and start filling this in so one thing we want to have here I don't think you can fill region in here no but we can fill out let's say the type so we know we want this to be a T2 micro and we do know we want the image ID to be this so here's the question if we have all this other stuff is it going to pick this up and these will default or will this all complain and that's what we're going to find out right um so what we'll do is we'll just say load yaml input file and I'm just going to go ahead and delete the Json skeleton because I don't want to use that here and we'll go over here and we'll take a look so we have this reads A String um so I don't know if we have to specify the file protocol here but I guess we'll find out here in a moment and one thing I definitely want to do say region Us East one and then for the input here I'll try file and we will say skeleton yaml we'll see if that works okay Windows too small uh doesn't like something we'll go ahead and hit enter parameter validation failed for this so I think what's happening here is that if something isn't set here it's complaining so we would really have to fill all this out so I think what we'll do is we'll just take out what we want so let's say we only want to have these two values I wonder if it'll just take that let's see if that will work let's try that again and it looks like it did create it because it's returning back Json that's usually a good indicator that something has been created for instance we'll go over here and if I refresh this uh um I'm getting stuff back monitoring disabled Etc okay so I mean like I'm getting information back but I don't see an instance here maybe oh maybe we have this uh here if we turn that off there we go sometimes you got to be careful those filters might be confusing and so it's initializing it here so that is definitely working it is a T2 micro so we can fill that in partially I'm going to go ahead and just terminate that because we've basically we're able to make use of what we wanted there so I'll go ahead and I'm just going to save this say skeleton and input example so that is kind of nice but uh yeah there you go okay so when we are writing uh CLI commands a lot of times you have to supply them uh Json so for example we have an option here and we have to pass it U adjacent object and that's a lot of work to type out luckily iTab us has a shorthand syntax that makes it a lot easier and so uh here we can emit all the double quotations the colon the brackets the quotations and it becomes this um here's another example where um you know we have have adjacent object and then we change it out uh to this or here we have an example of an array uh which is simplified as such uh and then here is a more complex example where we have an array of objects so um you know I tried to use the shorthand syntax as much as I can because it makes my life a lot easier it can be hard to figure out what the shorthand syntax is but it is pretty robust um and I think you'll enjoy it and we're just going to end up using it quite often as we are writing CLI commands all right so I just want to talk about Powershell adabs to the a so you will be using Powershell and Powershell will output um uh uh text files or Json files and then you want to utilize them later on maybe by a Linux system but there's one little caveat you have to consider by default Windows Powershell outputs text as utf16 which is going to conflict with utf8 encoding used by Json files and many Linux systems so when you are using Powershell you're going to have to specify that hyphen encoding flag and and provide asky and then you're going to have uh Less Problems why not specify utf8 I don't know but they say use the asky one and you're going to have a lot less trouble um so again this is a very Niche case but for those windows users out there that are going to be using Powershell I want you to know this okay when we're working with a CLI we're going to get output from it and we're going to want to pass it um to Future commands hence chaining now we might do this in a single command pass one program to another or we might store for later and use it for later and that's what I'm saying when I say chaining so with Nars what you can do is you can do have a dollar sign in parentheses and wrap your command in it so I'm just going to show you very clearly so we have dollar sign parenthesis and then uh we should have a closing parentheses it's missing the graphic but imagine that I actually have it in there and then the idea is that uh the output here which is describing VPC and it's outputting to text is going to um uh set this environment variable to be VPC ID okay and then when we want to use it later on we can just reference it in this command doll sign VPC ID and that's kind of a way of chaining your commands together by outputting the results to Temporary environment vers ibles um so it's going to be easier for future commands so there you go so in the last example we were showing chaining and I say chaining in quotations because it's not true chaining uh with Nars and so you might say hey I really want to chain commands together in a single command I don't want to store it as a variable and load it later uh well normally you would use a pipe um but it's just not going to work um in the case for the ab to line so we're going to have to use a program called XRS in order to do that I believe it's a builtin uh program in most most Linux operating systems and when you use a program that cannot utilize uh piping naturally you end up using XRS um I'm not that great with XRS I'm just telling you that's what you can use and I made sure that I I figured an example that would work so for uh here we have uh um ec2 and we're describing the vpcs we are looking for the first VPC ID and we want to pass it uh to subnets and so the idea is that we can use XRS and then we can um specify uh that value so we say U curlies here and then those curlies here is going to load that VPC ID right there so XRS which stands for extended arguments is a is a Unix command used to build and execute commands from standard input XRS is used to build commands so if a if a command needs arguments passed instead of an input on S St St in uh or standard in you can use XRS using a simple pipe so hopefully that is clear um that XRS is how you're going to um chain things together in single commands Okay hey this is angrew brown this follow I want to explore looking at chaining um uh a CLI commands together so that we can work with them more efficiently so what I'll do is create a new folder and again I'm in this repo the itus examples and um again this folder is going to be called chaining and we'll look at two chaining examples the first will be with just using environment variable and then the other one will'll be using xargs so we'll just say chain chaining with Nars so let's say that we need to get a VPC in order to create a subnet that was the example we used in the uh slide so let's go ahead and just do that so what I'm going to do here is I'm going to type in VPC ID and I'm going to write export because I plan on exporting this uh variable and I'm going to write in adabs ec2 describe vpcs and then we're going to apply a filter here and so we might want to say um we might want to get back a very specific VPC so let's go take a look here at uh what we have for vpcs and um I'm going to go over to I'll say North Virginia okay and we'll go over to vpcs the reason I'm saying CA Central is because right now I don't have a default VPC and so I want to make things a little bit easier for you as well if we go into here um this should be my default here yes it is and so the idea is I want to return back this one here okay so the idea is that if we go and look up describe vpcs it was C version two then we should be able to look at some filters that we can U utiliz to select it um so we could select it based on the cider range so here if we look at it it is 172 31 06 that's normally what the default is at least that's what I believe it to be but maybe we could also just select it based on it whether it's the default one I think that's what I would like to do so what we'll do here is go back over to our filters I'm going to type in filters here and and we provide a name so I'm going to assume that this is going to be is default what what's interesting here is that I don't see a value it's supposed to provide so normally you would see something like State and then you provided a value but here it's not suggesting we provide a value unless we're supposed to provide it yes or no but it's not saying right so the uh filters are case sensitive you specify value for a filter the values are joined as as as the case here so let's go find out if this actually works so um I'm going to just go ahead and copy these two lines here and paste them in another thing that we need to indicate is the region so I'm going to go ahead here and just explicitly set the region because again uh this is just an issue with my CA Central one that I don't have a default VPC at least right not right now so we go ahead and hit enter and it's returned no data so clearly there is something that I'm missing here I'm going to just take the filter out here for a moment and we'll test with that okay we do get back stuff eventually I just want to select the VPC ID so what I'll do is I'll write a query to filter out that information so I'm going to say vpcs and the thing is you can only have one uh default region so it's safe to say select zero the first one and we'll say VPC ID okay and we'll go ahead and try this and so there it is it's selecting it but I I wanted to select um well it's selecting the first it's finding but not necessarily the default one because if we go back over to here notice this one says 00 e6b this one's saying 00 e6b but we want the default one so we need to get this filter to work um it didn't show us what we needed to provide as a value but let's just go ahead and type in values um yes and see if that resolves it I'm not sure if those are K sensitive it might be uh the values are K sensitive okay so what I'm thinking is that this over here says yes in uh that casing and so I'm going to match it and see if that makes a difference or does what we needed to do and we'll go ahead and see if that works I'm just going to put that there and this is totally a guess I'm not sure if that will work and we get back at null um so you know I'm not 100% sure with that so what I'm going to do I'm going just bring this onto a single line I'm just going to copy this whole thing and I'm just going to um put an x on the end I'm doing that on purpose because I wanted to open up the CLI I'm going to go to the a CLI tab make sure I'm there and we'll bring this up so now what I'll do is I'll just back out a bit I'm going to go here and type in name equals is default what I was hoping is is that it would um start to show up here here but it's not so I thought maybe it would autocomplete it here so that doesn't work um let's try a few other values so maybe we'll try yes in lowercase try this again here I'm not sure why I copied some weird thing in the front of it that didn't work what if we try true okay so it's true again notice that it doesn't tell me what it is but I had to infer that's what it was it's probably using a Boolean I'm not sure if it tells us doesn't tell us what these are so again we had to guess to figure that one out but that's bringing back the value we want another thing we should probably do is is specify that the output is text we'll go ahead here because I don't want those double quotations around it there we go so notice that this had double quotations and now it doesn't so that's something that uh might matter when we're working with this so what we'll do is make this a little bit easier to read so I'm going to go here and put a forward slash or backs slash and then a back slash and then a back slash and then another backs slash okay and uh then the idea here is that this should assign it to the VPC ID so I'll type in clear here and what I'll do is I'm going to copy this and paste this in here and so the idea is that now I should be able to print it out and see its contents so we go down here and we'll say Echo and dollar sign PC ID okay we'll paste that in and so we can get that value so now what we can do is go ahead and create our subnet so we'll say it us ec2 create subnet uh VPC the reason I know what I'm typing is because again it's just in my slide so I'm just kind of referencing that if we didn't know and you've seen me do this enough that I would go here and I would look it up and I would say okay where is this create subnet and then we go down and we'd look at for an example code and we have an example code there it's basically the exact same thing we're using so I'm going to go ahead and just grab it but we'll go ahead and paste this in and so here now what we want to do is just Supply the VPC ID um we have the cider block so the cider block does matter and it has to basically have the same um IP address as the one up here so this is where a case where we might actually want to pull another value which is um grabbing the uh cider range so what I'm going to do is go back up to here and grab this statement and I actually just want to go down and say paste and this one will be for grabbing that cider range so we'll take out um this part so we'll still get back the Json object I'm just going to switch this back to Json so I can see what I'm doing we'll go ahead and hit enter and what I'm looking for is that cider block so there it is and it's also up here as well so we can grab either one all I want is the front of it off so I just want this part of it so what we'll do is we will say cider block and I'm going to change this back to text type in clear we're going to grab this and we'll hit enter okay and so we need some way of dividing this string all right so we don't want the um the last characters on it so there's probably some kind of C command we can use I use chat to help us figure this out really quickly so we'll say using the CL or the um Linux uh Linux command programs take this string and remove the for sl16 or it will say save the string and replace the for sl16 with a for slash 24 and there's something probably out there like s might work for us the trick with it is that SED always does something different um okay use a Linux command to do this to do the replacement I'm not sure maybe it's this over here sometimes it has that and so this one's using S so just understand that s is going to vary in syntax based on the um distribution or Linux that you're using so we could use this and it could totally fail we'll go ahead and test it and it worked we're really lucky that that worked I'm just going to tell you right now you could write this and then you could deploy to another server and it doesn't works so sometimes the syntax here in these programs vary based on Linux distribution so be very careful about that what I want to do is make sure that this works by um like with chaining it so we'll go ahead and put this on the end and I'm just going to go and try uh this okay we'll paste this in and then we'll type in clear I'm going to go ahead and copy this and oops and we'll go ahead and copy this again and press enter and so now we're getting the result that we want we might already have a um a cider block for that so that might uh not work in our favor but we'll we'll work with this here in a moment so we'll just say cider block okay okay and I'm not sure why that is highlighting like that it's probably fine I'm going to go ahead and copy this and we'll give it a try so again copy paste enter and it does not like the export here because we're missing the equals okay I knew there's something wrong there we'll try this one more time okay and then we will just do an echo on the cider block as well great but the issue is that we have z0 right so um again if you haven't done subnets in bpcs yet the way it works is that you are going to have uh subnets within this VPC so if we go over to subnets I'm just going to grab this here we go over to subnets wherever that is and we are going to look at the cider uh ipv addresses you're seeing 20 20 20 20 20 so the thing is is that the next one uh we can't use zero because it's already being in use the other thing is that this one was divided with 24 I'm going to stick with 20 as that's the way it's been subdivided um I guess the question is is there any more room for anymore I'm not 100% sure um there is a way of doing cider math I'm not the best at it but we'll go ahead and take a look here and the idea here is that we would have an IP address this is just a free website that everybody knows when working with cider ranges so we'll just mimic this and so we'll say 172 310 that doesn't really matter but I'm just doing this so that our example matches for everybody 172 what was it 30 72 31 0 0 and so if we type in 20 notice that this is how many we have and so if we do 6 we have 65,000 so what I want to know is do we have any more room for any more cider blocks okay so we have 65,536 addresses and if we go to 20 we have a number of 4,8 4,096 and this one I believe has six subnets already so we go here we'll count them 1 2 3 four five six so there's six of them so what I'll do is I'll go and take a look and say okay there are times six so what is that value times six please and so we have this much used okay so clearly we have more room for more cider blocks so we can definitely put another one here the next question is how is it incrementing and based on what um how you divide it is going to be based on what this increments at so if this was um um uh sl20 you know it's adding this amount of range if it was 24 it's going to be fewer and the idea is that uh the numbers for these side ranges are just going to be different so what is the next one well the next one I usually just look at it and I can just see what it's incrementing by so it's going 0 16 32 48 64 80 so it's adding 16 every time so the next one's going to be 8 80 + 16 which will be 96 so I know that we can change this into 96 so this one should be Z what was it 960 and so let's see if we can modify this to do that um so I'm just carefully looking at the value here we have 1624 and we want this to be a zero here okay and then a period and then a zero and then on this side of it we're going to do 960 and hopefully that will just work work um this is using regular Expressions so you'd have to understand how regular Expressions work and regular expressions are different based on the OS that's why I was talking about the variation for SD so what we'll do is go ahead and try this and see if it actually prints out what we want it to print out okay and then we'll Echo this out and now we have 172 31 96020 so that should be a valid cider block that we can use and we'll go ahead and paste it on in here like this okay we'll take type in clear um there's tag specifications that's good we probably could have say ipv4 only because that's what it might already be I'm not sure if we go back here as it can have ipv4 and IPv6 this one only has ipv4 so we're going to match it based on that and let's go ahead and see if we can actually create this now and so it has a problem we've done something wrong not sure what we'll go ahead and hit enter and see what it's complet about and it says um it's an issue with the tech parameter which is interesting because they're the ones who supplied it to us we didn't we didn't change that um so that's interesting yeah I'm not sure exactly what it doesn't like there maybe this needs to be wrapped in single quotations sometimes this stuff messes up if it's not wrapped so I'm can go ahead and try that we'll do double sorry we'll do doubles I just forget if it's doubles or singles I think it's singles to be honest let's go ahead and try this and see if that fixes our issue um it's saying this VPC VPC does not exist the issue here is that we don't have the region specified so this might not be an issue for you but it's an issue for me because my default is not usc1 I'm in CA Central we'll go ahead and hit enter and there we go so it should have created that subnet I'm going to refresh here and we should see it now now so there it is right here at the top and it provide that name I'm going to go ahead and just delete out that subnet and so we've successfully changed using environment variables and I do this a lot when writing bash scripts now we can also use um chaining with xargs and I don't use xargs that often because I just find it hard and a lot of times I don't find much benefit making a single line but sometimes you might need to provide a single line especially if you're going to provide it to a customer and you want it to be one and done but uh what we can do is we can use it to describe vpcs and then describe the subnets within a VPC so what we'll what we'll go ahead is start typing some commands we'll say ads ec2 describe vpcs and we'll query it and I only want to get the first VPC so this won't necessarily be the default one but it'll be the one that I want um I'm going to just change this to be region Us East one and then I want to Output the text so let's see if this works and did I WR from that from that from scratch yes I'm once you start working with it you start remembering the commands you're working with often so here is that one and what I want to do is I want to describe VPC so that'll be a ec2 describe subnets I said vpcs I'm going to say subnets and um if we go ahead here and copy I mean that should be a thing so there's probably a filter to filter based on vpcs let's go take a look there all the way to the top we'll go to ec2 we'll say describe subnets we'll go down below here and I'm going to go to filters to see if there is one if there wasn't we could probably use a query to abstract it and so there should be something for VPC ID here some carefully looking and there it is on the end so there it is and we'll go ahead and put that in there so filters I'm going to put this in double quot just so that there's no um issues and we'll say values and the idea is that we want to supply it here right so let's go ahead and just grab it and just make sure that this works and then we will then figure out how to write the X ARG so say copy paste and I'm going to grab this here paste it in right there not exactly where I want it to be okay okay and I'm going to go ahead and copy this and paste and hit enter and so it's not returning any subnets um that's not what I wanted it's supposed to return subnets so I'm just kind of wondering why that isn't working um good question let's go take a look at the describe subnets and see if there's any examples cuz maybe I wrote it incorrectly doesn't look like I have it looks like I've written it correctly um maybe the issue oh you know what it's the region again sorry I can't find it in a region that doesn't exist that makes sense so go ahead and copy that we'll paste it in we'll hit enter and so now we're getting back to subnets I only want the subnet information so I'm going to just query that out looking at that structure we can select all subnets I'm going to say subnets and I want all of them um and in side of that I only want to grab two things I want to grab the cider block and the subnet ID so what I'll do is I'll do period and then what you can do is do Square braces like this cider block subnet ID and so that should select them within the interior how do I know that I don't know I've just written enough of this to kind of remember it and um let's go ahead and execute that type this clear again I thought that was right oh maybe I'm just missing this here copy paste enter there we go I'm just going to change this to have an output of table so this one only has a single subnet so we could change this out to be the other one so maybe instead of this um we could get the one that is the default VPC so we can go here and just grab this here and paste this in Above So now well sorry I don't want it in there there I'm not creating a subnet I want it in here sorry and it's just copying what I don't want to copy I'm really not that bad at copy and pasting it's just again because I'm working within uh a cloud developer environment and it's just finicky so we'll go down here and paste this here and the idea is that I want to um filter out specifically for the other one because I'll get more subnets that way and then we'll get better data so go ahead and grab this one so we'll grab this VPC and we will paste it on in like that we'll make sure that this works okay um again I'm going to put a table on that because that output's a bit messy there we go that looks really good so now the next thing is to bring these together so here are our two commands and then down below I want to stitch them together so that's where we're going to use xar so the idea here is that we are going to have a pipe and we'll bring this onto a new line so it's a little bit easier to work with I'm going to type in X args and we're going to provide the input and the input's going to be curlies and so wherever these curries appear is where it's going to substitute and where I wanted to substitute is over here in values and so I'm hoping that this is going to work let's go ahead and copy this I'm going type in clear and fingers cross as this works there we go so that is a way you can chain it together I know it looks like a huge mess but it's as simple as doing xrx I with that uh curlies and do this I'm sure there are more options but again I don't work with xargs enough to know them but I know this one uh and this is uh pretty useful just in this limited way so yep that's all there is to it um and I'll see you in the next one okay ciao adus has two different configurations in toml for format uh for configuring the a CLI or I suppose this could be used for anything that's using API like the SDK so we have uh the credentials file which is used to store sensitive credentials such as your access key and and secret then you have config which is used to store generic configurations such as preferred region um now even though they say that you can technically store um the region or other information in the credentials um but you're supposed to put it in the config so just understand that you can have config in credentials but you really should separate them out uh you can store all the same configurations either files the credentials will take presidence over config um there is a lot of configuration uh options here so I'm just going to quickly list them out as you can see them all um what's really important are the ones in red as these are the ones you're always going to be using which is the um the the key and secret uh over here and then we have the output and the regions so again those are the most common ones that you'll use but you can see there are a few other options there is also S3 specific settings and I'm just going to bring them up quickly here um so they look like this where you can set um you know things like the concurrent requests and the max bandwidth uh some apply to all S3 commands in both the S3 API and S3 namespaces if you don't know what those are it'll make more sense when we go to the S3 section um databas S3 transfer commands copy sync move remove have additional sets you can use to control S3 transfers but that's the only caveat that I know for um configuration but again just remember uh these four and you're going to be in really good shape okay so the adus config files support the ability to have multiple profiles profiles allow you to switch between different configurations quickly for different environments and basically when we say different environments that could be with different permissions or different accounts so imagine we want to run the command iTab us ec2 describe hyphen instances well what it's going to do is it's going to look for that credentials file or your configuration file it's going to look for the default profile and that is what it's going to use as the uh key and secret for that so you can choose not to have a default profile and actually leaving out the default profile is a good practice because then you are always explicitly saying I want to use this profile and you don't accidentally use whatever the default is if you want to specify which profile you're using there's this hyphen hyphen profile flag so you can choose what it is uh if you don't want to uh do that and you want to set it via an environment variable you can go ahead and Export it and set it that way um in local developer environments setting credential files and using name profiles happen frequently um in a lot of the follow alongs I'm using a cloud developer environment and they don't like you using credential file there because you're not supposed to store them on the machines so uh you're not going to see me utilizing this much but in local development you'll probably use it all the time on your own machine and I just want to point that out okay so the a CLI has multiple configure commands to make it easy uh to configure your configuration files um and this is generally the recommended way to generate out these files uh you can create them by hand but you know you could use the wizard here as well so the idea idea is you're going to type in abis configure it's going to prompt you for everything that you need to enter the access key the secret the default region um and your default output format you can set a value for a specific setting by using the set command so there you setting the region for the profile Dev you can unset a value by just providing a blank string you can print out a specific value if you want to see what your current configuration has you can import databas credentials generated uh that were generated out from the console um and this is a way for you to basically import without ever having to open the file and expose it this could be very useful if let's say you are I don't know a content creator and you don't want to show off those credentials and you want to import them so that's kind of an example that I can utilize it for um you can list out your configurations and you can list out your Prof files you can also log in uh via the um or you can configure uh configure login for single signon I don't think I've ever done this before because I usually avoid single sign on because I have to make a lot of videos that don't utilize it and it's kind of a pain to um turn on single sign on functionality and then turn it off uh I imagine that this is allowing you to bring your directory service in AOS or using maybe the IM identity Center but I just want you to be aware of that you can configure it with single on and large Enterprises would probably appreciate knowing that so that's why I'm sharing that here okay another capability of ads CLI is ability to use single sign on and so you can do this by just configuring the single sign on option by typing adus configure SSO and then that way um you're going to have a much easier time working with the c so there you go it's that simple when an API request fails due to possible network issues AB CLI will automatically retry using exponential backoff we absolutely did explain this in another slide the concept of exponential back off that good clis and sdks have them built in and guess what both the CIS and the sdks for us have them built in but they provide you some configuration options if you want to change how it works and so there's three modes there's the Legacy retry mode the standard retry mode and the Adaptive retry mode so for legacy we have a default of four uh for Max retries attempts a total of five call attempts and an exponential backoff by a base of factor of two then you have standard so that is um a default of two for Max retry attempts a total of three call attempts an exponential back off by a base factor of two for a Max back off time of 20 seconds supports uh uh supports retries on various errors and exceptions over Legacy so there's a little bit better support there then we have adaptive retry mode experimental retry mode that includes all features of standard mode and intelligently modifies the rate limit variables based on the stand on the type of exception and error pass attempts so you can change this in your configuration file to have um the retry mode as standard and then you can change the max attempts so would you ever need to to touch this um I think in one use case I did change this and it's because I was uh writing a program that was calling lots of a CLI commands and so I had to increase the Max uh attempts but uh for the most part you don't have to touch this ever but it's good to know that you have uh some level of flexibility there okay so when we start utilizing ec2 you're going to find out that it's going to be already loaded with credentials um and the A C is already going to start working on very specific um types of um instances so like if you open up maybe let's say Amazon Linux 2023 uh the CLI will start working right away why is that well has to do with the metadata so in an ec2 instance the metadata for the CI can directly use temporary credentials so when an IM roll is attached to the instance the a CLI automatically and securely retrieves the credentials from the instance metadata so if you go into here we have credential Source notice that it's not listing out a a key and a secret it's saying go get it from the ec2 instance metadata you can turn this feature off uh but it is a really good security feature um and you know if it doesn't make sense don't worry when you start working at ec2 you can always come back here to kind of remember uh this functionality okay let's say you are a large Enterprise and in order for um you to reach out to the internet or to ads you have to go through a proxy server so how would the a CLI know how to communicate with that proxy server um or know where that proxy server is to get out to ads well uh the ADI allows you to set a few nbars in order to do so again this is going to be really more for Enterprises but I want you to know that you can utilize a proxy the first is by setting HPS proxy to the address of your proxy uh if you were uh needing to authenticate then of course you could pass in your um username password whatever you need to do uh if you're processing on an ec2 instance uh then you need to exempt the address used to access the instance metadata so that is uh one other thing that you'll need to do um but again this is really for Enterprise use cases but uh or people that are need utilizing a proxy but there you go so we can configure the a CLI to use an IM roll instead of just directly using access keys with permissions attached to perform operations the way that's going to look like is we'd set up a profile we're going to have a profile Arn and so this is to the role that we should assume it's it's Arn Amazon resource name of the IM Ro and then the source profile so this will be another profile in the credentials file that's going to have the access keys and and you might think well if that has access Keys wouldn't it have permissions well you're really just giving it permissions to assume a role and so it's not exactly the same um now if you're using ECT or ECS you'll probably see this kind of rule AR being used because I think by default when they create the credentials file it's going to use a a credential source and the idea is that instead of pulling the credentials from The Source profile which would be in the same file it's going to pull from other places like the environment variables like the ec2 instance metadata uh or the ECS uh container um could you Source the credentials outside of ec2 and ECS absolutely you could um I would assume that you would not supply The Source profile what I don't know is why didn't they take Source profile and put it under credential source so that we'd only have two flags have no idea why but why would you want to do this um well there's a few reasons you're going to get enhanced security because you're decoupling the role from the machine user when I say machine user I just mean a user that is used to perform things that um a service should do and is not necessarily attached to a particular person um it will produce temporary credentials so when you have an access key and secret those are Long Live credentials and so if you are assuming a role it produces uh shortlived credentials to access those rules permissions so you still have longterm um longterm Keys being used but the permissions underneath of it is going to be short term and that's going to be a lot more secure if you're going to be doing cross account access you have to assume roles in another account anyway and so this is going to make um utilizing them a lot easier I think that you can assume rules of the a CLI commands by running STS um but this kind of Skips one step and makes it a lot easier um in practicality how often do I use this very rarely but I probably should use it more I just kind of forget about it um but anyway I just want you to know because when you go into uh ec2 or ECS you're going to open up those credential files and you're going to see that credential source and this RAR you're going to be like what the heck is it this is what it is okay so this is something interesting that you can do with the a CLI and that is you can use credentials process which allows you to use a bash command as the input for your access Keys um so the idea is that uh we have a script here called Adis creds custom it's just a random script and the idea is that it needs to Output this Json and it will then go get the access key in secret um and so this is a programmatic way for you to um assign uh access Keys okay itus does not recommend that you use an external program to gener credentials because it could pose a security risk but there are use cases where you might want to do this so I just want to point out that this functionality exists you're probably not going to use it um but yeah there you go so the ad CLI has an official Docker image so you can execute a CLI commands in an isolated container um and the place you're going to find that is in the docker Hub uh at amazonus CI and the way you're going to execute it is you just run the docker container and Supply the command here on the end when you run this for the first time and this is just in general how Docker works when you don't have a container that you're using it's going to download it once but then it will run the command so here I'm just putting the version flag and I'm trying to see what version of the CLI is it's like hey you don't have it let's go download it and then it runs it but again you're only going to see that's the first time you ever run this command uh for your Docker image um so that's just me pointing that out there um you can use the official uh Docker container in order to run any kind of CLI command you want um you could do this if you don't want to install python or you want everything to be in isolate for whatever reason uh so there's various reasons as to why or maybe you are just working in an environment that uses Docker containers so you just want to know that this functionality exists so here's an example where we are running the CLI command and we want to do easy to describe vpcs but here are some challenges is that we're running everything in an isolated environment so how are we going to do things like pass credentials or download files um in an is environment and well I actually went ahead and I solved that so the first is we can pass credentials um by specifying a file so notice here oh sorry um we're not specifying file but we're actually mapping the the the the root volume so here we're saying let's get the um ads directory and we're going to map it to root. ads and that's going to allow the CLI command to automatically load your credentials files okay okay so would passing Nars work such as hyphen e postgress user they should but um I didn't have a whole lot of luck when trying it out so you know if you want to fiddle around with it and figure it out you can absolutely do so um but I didn't have much luck if you want to download files uh we can map a volume of the the files to download to the current working directory again this is just Docker stuff um using this hyphen V which is the mapping um but you know again if you're not that familiar with Docker um just understand that you have that functionality there but yeah that is US using a via the docker image hey this is Angie Brown and this follow along I want to take a look at running the it via Docker so I already have my git pod environment open here and in here I'm going to create a new folder called donker uh we actually we really don't need to make much here but I'm going to make one anyway so that we can kind of just write it out and have a reference to it um but the idea is that we should be able to run the a CLI within a Docker container and that Docker container is available or Docker image is available on the docker Hub so if I go over here you can see it is hosted here and we do have some Docker CLI usage down below so that's a really good command that we can go ahead and grab so going go ahead and grab this one here okay and so what this will do is if we run this down here and I already have just so you know I already have Docker installed here and if you're doing this in your local developer environment you'd have to install doer Docker is a big pain in the butt so I really do recommend to do this in a cloud developer environment but anyway um what we'll do is we'll copy this command here and we're going to paste it in and what you'll notice is that it's going to say that it did not find the image and it's going to go ahead and download the image and so that image will be stored on this machine now once it finishes and then it will run the command which is the hyphen hyphen version okay so we'll give it a moment there and it should show us it running so there it is 2154 that's telling us the version and so we'll go here and just say install Docker container and show us the version just say attempt to list out S3 buckets okay and so we try to do this if we copy this here and then we put our command on the end it shouldn't work because this is running in an isolated environment um we have Docker installed here but it probably won't show because it's so darn quick but um the issue here is that when we attempt to run this line here it ran the last line with the version not the one that I wanted um it shouldn't work because it does not have credentials within that container um and so it's not going to be able to anything so we're going to have to have a way to uh give it access access to that and the way we would do that is by mounting a local volume and so a common way would be mounting Theus directory um when I use cloud developer environments I don't really use credentials um but in this case we will use it so what I'll do is go over to IM and I'm going to create out new credentials just to make this super easy and I don't want to have to rotate out my current credentials so we'll go here into this user count and I'm going to create uh another access key in secret so just choose any option first options fine we'll say create access key and so we have access key in secret so I'm going to go ahead and type in ads configure and we'll paste in our key and then we'll copy the secret I'll paste in the secret we'll hit enter enter that's all good and so now what I want to do is open up um here the adabs credentials file actually we don't have to because it'll be the default value but I'll just show you where it is we'll go here and notice that it set these credentials of the default so now it should utilizes credentials however um this environment uh uses itus environment variable so it actually is using the other key but that doesn't matter we just wanted to work for the docker container so what I'm going to do here is just make this a bit easier by uh bring these down in a separate line so we can bring this down on a line whoops we'll say now uh connect with credentials and I'm just going to bring this line down here and this can be here and then we'll say hyphen V and the idea is we're going to mount those directories so we'll have Tilda ads and then we're going to want to map this to for SL root. ads okay so the idea is we're taking this one here and then putting it to the root 8 ofs because I believe that root 8 ofs is where it will look for it because um I don't think like user profiles and things like that that's why we don't map it uh to the um we don't do like Tilda here we do root instead so we have the command there and um I'm just going to go ahead here and do this like that okay and you know we could also Source this from uh ECR so right now this is coming from Docker Hub but I kind of would prefer to get it from ECR uh which is from ads that way we know we're always up to the latest uh one and that's actually what I end up using here in the slides if the the slides would like to move here today uh there we go nope I'm trying to drag the slides onto the screen here but they're really refusing to move just give me a second okay now I can uh drag these slides on here but notice I'm using it from the repository of um uh ads is repository for ECR elastic container uh uh registry as opposed to Docker Hub so you can do from either or I again I prefer to get it froms but I think what's going to happen here is that when we run this it's going to download it again because now we're downloading from a different location so notice it doesn't know where it is it's going to go ahead and download it so we'll just wait a little while here and notice that it runs and it's showing me my buckets so if you don't have any buckets you'll you won't see anything in here but at least you won't get an error but the point is this is how we would run it authenticated so that's all there really is to it uh if you're running this locally um you are going to want to uh maybe clean up that image afterwards you might have to type in Docker images and you might want to delete these because these store files on your computer so that's up to you um I think it's Docker remove could be images remove Amazon a Docker images remove Amazon abcl no Docker images help um how do we remove Docker images since I can't remember R RM hyphen f so I suppose that's what one way we could do it so we could try that now I'm in a cloud developer environment so I don't really have to worry about this but again if you care about it uh I'm just showing you here let's see if we can do this um uh probably I for the name try this there we go and so I think we're able to remove that we list images again here and so that's how you can delete those repos and get back that space on your computer cuz those Docker images do take up a lot of room but we'll see in the next one okay ciao so if we want to go ahead and install the a CLI there are um instructions that a US provide you based on what operating system you're using whether it's Linux Mac OS or windows so here's an example for Linux but really make sure you check the documentation for the installation uh if you want to update here's an example of uh performing an update for Linux I just want to make the point that if you do install the CLI please do not use thirdparty installation tools because they are not maintained by AWS even if they are and they say official uh what I found is that when you've installed them they they will not be the most up toate version of the CLI and then you're going to get errors that's going to stress you out out so here this says official but it's not actually officially maintained by ads so this one is very misleading um so I just really want to point that out there um and again this is dependent on what C tool you're using but for ads do not install third party tools with Brew or any other package manager install it directly from the code okay all right let's talk about version history for the a CLI and when I say version history I mean the version of the CLI that you're using there are two versions the latest one and the previous one if this is the future there could be a third one I can't imagine that they would do that because um it takes a lot of work to create new versions so I can't expect there is a version three uh there is no reason to use the old version of thei so make sure you're always using version two if you type The adabs Hyphen hyen version flag it's going to tell you what version you're on so at this time I'm using 2.9.0 I'm sure there's a newer one since the time I've made the slide um also notice that it's going to specify the python version you're going to be using you should really be using Python 3 you could have side effects if you're using python 2 um some systems still utilize python 2 for particular reasons but basically everything is Python 3 and the a CLI is written in Python so that is important there okay if you want to make utilizing the aw CLI even more secure you can pair it with MFA um so it'll create a temporary session it's going to really uh be dependent on if you're going to be using Hardware MFA or virtual MFA uh we're not talking too much about MFA right now I'm I'm sure somewhere in the course we talk uh more explicitly about MFA um but anyway the way it's going to work is that you're going to have IM roll and you're going to modify that trust relationship to expect that there to be multiactor authentication present and so if your user is assuming that rule then that means you're going to make sure that you're going to need to have that there so um here in our profile you can see we have the rule Arn right and so that is the role that we were going to assume the source profile there C user is what has the access key in Secrets which is going to assume that role which is going to check that trust policy to see that MFA is required we have um a flag there called MFA serial and that is going to point to the MFA device whether it's Hardware or virtual so when you go and attempt to use the command it's going to then prompt you for the MFA code so here you can see I'm using the profile it says enter MFA code uh for this particular user and so you know in your in this case this is a virtual one so in my phone I would have it there I enter the six numbers in and then I would get in there okay so for the C we can set up environment variables or what I like to call nirs to provide another way to specify configuration and credentials and this is very useful for scripting uh or temporarily setting uh a named profile as a default so you are going to see me use environment variables a lot with the a a lot in general because when you're working with code it is super useful in terms of presidence um or what takes priority a c parameters are first environment variables are second and configuration files are third so the idea is that the higher up on the list they're going to override anything that's down below common environment variables for the a that you absolutely will use on a regular basis is the adus access key ID the adus secret access key the default region and the region and the profile those are the main ones and often you will see me Googling what they are cuz I can never ever ever remember no matter how many times I entered them in these two top ones because they are horribly named uh so whoever did that I'm not happy with you but it is what it is there is a lot of environment variables let's quickly go through them and see if there's any interesting ones there throughout uh this section of the course we do cover these for their specific ones but we'll just cover them all in general really quickly here so what we have is the ca bundle this is for certificate bundles using HPS certificate validation the auto prompt we definitely like to use this one this is a super useful one to turn on that uh that wizzywig um so it makes it easier to write CLI commands there's for file encodings uh if we need to change the location of the config file if we need to uh uh provide additional directories uh to check for um modules I believe that's for service models I said modules I meant to say models then there is the default output so this would be changing from text to Json to table then there is the r AR we use that when we are uh trying to assume a role um we have rle session name if you want to name your session your temporary session then we have session token this is for temporary STS then we have shared credentials file this is changing the location of the credentials file we have web identity token file this is used for off 2.0 um we have metad dat disabled this is for easy to instance metadata when we want to disable the usage of it or the lookup of it um for credentials I suppose uh we have metadata service number attempts so how many times you're going to attempt to retrieve credentials VI an E2 instance metadata service timeout so number of seconds before connection the instance metadata service should time out Max attempts um pager that's for pagination and then we have the retry mode so yeah we cover these all over the place within this um within the section so you don't really have to memorize it but again just doing a quick overview so you are aware of those environment variables okay adus has multiple competing projects that provide autocompletion while typing out ads CLI commands the first is ADS completer uh this was the original Auto completer for ads probably intended for CLI version one don't use it it's Legacy it is super limited it was great back in the day but it is served its time let's move on then came about adaba shell and this looked really cool it was an interactive shell with the a CLI but this project hasn't been active since 2020 and it looks like its functionality has been rolling directly into the CLI um uh there are bugs that are going back to years that they're just never going to solve um so I do not recommend to use this tool despite it being listed as a um a thing in the a marketing page uh maybe they've fixed it but when I went there they were heavily promoting this even though it was clearly old and not recommended for use and the one you should use is called the ads CI Auto prompt so this uses CLI version 2 um and it gives you very similar functional functionality to the aess shell and this is the way that I'm recommending that you use Auto prompt and we're going to focus on all the functionality of it because it's such a useful tool in the next upcoming slides okay a CLI Auto prompt is a powerful interactive Shell built into the a CLI to assist in writing CLI commands this tool is so awesome it's going to make your life so much easier and you don't have to be a super expert at writing CI commands because it's going to show you a lot of the information in place this thing can do fuzzy Search Command completion parameter completion resource completion shorthand completion file completion region completion profile completion documentation panel projected output panel command history two different modes of activation and works anywhere the install it's super useful you should absolutely make use of this as much as you can when I'm using a local developer environment where I have my terminal on my Mac or Linux machine uh I can use it to its full potential when you're in a cloud developer environment or the cloud shell those panels that give you useful information don't work uh well and it's not the fault of the tool it's the fault of the browser uh environment so just understand that you might not see me using those out output panels and the documentation panels often in video tutorials but locally when I'm when I can use the tools that I want to use uh I can make use of that stuff stuff very effectively so for the documentation panel the idea is that if you want to see a command or subcommand documentation and you have that panel open it's going to show you the information okay so here's a command there's a subcommand if you want to bring it up it's F3 and as you type it's going to then start showing you that information contextually change you can navigate between panels by pressing F2 so you can actually go down into the documentation panel and then start scrolling up and down by using the vi directions that's Vim key key commands k for Down K for up so you can basically navigate all the documentation without leaving the console which is nice um so for configuration there are multiple ways to enable auto prompt you can put a flag so that it'll only execute for that single command Auto prompt is not turned on by default so you have to always provide some kind of flag um if you don't want to do that every single time you uh enter in a adus command you can export an environment variable to the um your current uh shell just understand that it's only going to be for the uh current uh the current shell if you open up another tab it's not going to necessarily work so you'd have to set that Nar in your bash profile so it'll persist all uh in all future profiles or runtime environments that are open in your terminal you can also set this in your configuration file I always use the environment variable that's just the way that I do it there's a couple different modes that you can utilize there's full full mode so every time you uh you execute an aable C command it's going to open up uh whereas in partial mode it's only going to execute Auto prompt if you let's say mess up the command or you don't completely fill it in uh the reason why I want you to use partial mode is that when we are writing bash scripts it will still try to open up in full mode and this is really annoying because you don't want this to happen uh so it's going to mess up our coding workflow so set to on partial we have an output panel this is going to show us the schema for a sub command so let's say we're trying to create a table we want to see what results we get back in terms of Json we can see that if we press F5 it's going to open up um that panel so that we can start seeing that information and I just want you to notice or notice that it's not real data it's just skeleton data because you'd have to send the command to get the data back um it does work with the query option so as you are writing out your query uh it'll actually even just you JP and then it'll filter out and show you exactly what you'll get back it's a great way to uh quickly write uh query commands it works with different output types so you could use it with tables again I would show this in my video tutorials but I can't because I'm using Cloud developer environment and this window doesn't open very well in git pod and things like that uh for command history you can press controlr this is a normal thing that you can do uh in your terminal to go back to previous commands but when you're in the auto prompt you can do that and it'll give you your previous commands so this will show uh commands that were successful or unsuccessful so you can run them again they will see like commands that that you do not enter uh activate the auto promp mode will not show up in the history so if Auto promp was not activated it's not going to be in that history however outside of Auto prompt uh if you do enter commands in that didn't that were never entered in Auto prompt you can search for them using controlr and do a fuzzy search this is not a functionality of AI this is is basic Linux batch terminal commands command history but just understand that if you run a command in the command or in the auto prompt it's not going to show outside uh and vice versa so that's just one limitation but that'll make sense as we start to work with uh the ca you'll see what I'm talking about sorry long video uh we have command completion so notice it kind of autocompletes the commands and subcommands we have uh parameter completion and it will even indicate to you which ones are required they'll show them first okay so notice here it says required so you'll know exactly what to fill in that you have to fill in we have resource completion um this is a hit or miss but the idea is that it will actually go to your A's account so and will suggest names so here I have Dynamo DB describe table and it's showing me table names same thing with VPC IDs I've not found this to always work as as as expected but um is cool when it does work we have shorthand completion so will it will autocomplete shorthand Jason Sy sytax so that is really useful uh when you are uh trying to provide structure there so notice here we putting filters and saying name equals and it's showing you the shorthand that you should write um so that's really nice uh for file completion it will autocomplete file names uh locally so you can basically just tab your way through it has region uh autocompletion here so the idea is you can provide the region and then it will show it here it may not work until you type in other required parameters so just understand that here we have make bucket and we're providing the bucket but it's not showing it here and here's an example that it refuses to show because of the order that it's happening in we have profile completion so it will show you possible profiles you can enter in it has fuzzy searching so what that means is that you don't have to perfectly type with it in you could type in something like um notice we typed the word East we didn't start with ap and it still selected all the easts here and the East here so that's really nice and notice here I'm typing MCA and it knows to do me and then the C so that is really nice so there you go that is auto prompt please use it it is super awesome okay hey this is Angie Brown and we are taking a look at AWS virtual private Cloud also known as VPC and this is a logically isolated virtual Network so adus VPC resembles a traditional Network that you'd operate in your own Data Center um but uh a lot more easily done than I would say than a real data center as you're not managing all that Hardware infrastructure underneath and some things are simplified or abstracted away to make your life a lot easier this is a diagram I created to represent a very simple VPC I want to point out that these diagrams can get very complex because there's a lot of components that go into VPC depending on what you want to do and you might only have room to represent certain things so just understand that you're going to see lot of variations here where some things are emitted because they're assumed and other times uh they're adding more detail for specific reasons so the VPC in here uh has again lots of different components but generally when you're using VPC uh the two main reasons why is that you're launching a virtual machine because that is the lowest kind of compute that you're going to get here so here it's represented here as an ec2 instance um or a virtual network card which are often attached to um a computes such as ec2 and so they're not always represented I would say in um in these diagrams but uh those are uh network cards are very important because even if you're not using compute you might need a network card to bridge some kind of other compute over into your VPC whether that's Lambda or uh ECS or any of these other managed compute services or something like RDS um but generally it's for ec2 instances but we'll get all into those components here shortly um ads VPC is tightly coupled with e 2 so all VPC CLI commands are under the adus ec2 so just understand why that is because you know you can't separate VPC from ec2 even though they're treated in the console as two separate things okay let's take a look here at core components for VPC it's absolutely not all of them as there's just so much going on in VPC but these are what I consider core that you absolutely should know um because they're just used so often the first being internet gateway uh often initialized as igw this is a gateway that connects your VPC out to the Internet so if you don't have igw it's going to be hard to get internet then we have virtual private Gateway um this is a VPN Gateway it's a way to connect your VPC to a private external network really useful if you are you have your own data center or office that you need to connect securely to your VPC we have rotate this determines where to routee traffic within a VPC we have Nat Gateway this allows private instances so virtual machines to connect to services outside the vbc um I should note that Nat Gateway is specifically for um ipv4 since IPv6 you don't necessarily um need uh Network addressing because all addresses are public then we have uh knackles this is network access control lists this acts as a stateless virtual fire wall for compute within a VPC and it operates at the subnet level with allow and deny rules then we have security groups which doesn't really have its own icon uh this acts as a stateful virtual firewall for compute within a uh VPC it operates at the instance level with allow rules we have public subnets so this sub these are subnets that allow to have public IP addresses then we have private subnets these are subnets that disallow instances to have public IP addresses we have VPC endpoints these privately connect to Support Services we have uh VPC peering this is connecting vpcs to other vpcs again there's a lot of stuff here so you know I didn't cover in here like um uh Ingress there's like a ESS only internet gateway which is for IPv6 um there is uh a lot like Transit Transit pering or Transit Gateway which is another form of VPC peing so again a lot of stuff there but we do cover them in the course just I'm just trying to frontload some stuff to you here uh technically knackles and SGS are uh ec2 Network components but we talked about before how uh VPC is so so core to ec2 they're kind of interchangeable um because ec2s uh launch within vpcs but uh there you go okay let's take a look at some of the key features of VPC um this is not everything about VPC is just to help load our minds about specific things about vpcs before we start working with them so first thing is that vpcs are region specific they do not span regions you can use VPC pairing to connect to vpcs across regions so that is your way of working around that there is other tools out there besides just the standard VPC pairing you can create up to five epcs per region you can increase the amount of epcs based on uh increasing service limits every region comes with a default VPC you can delete this I don't recommend that you do that um and adus does recommend that you use the default VPC whereas other providers uh the recommendation is a little bit different you can have up to 200 subnets per VPC you can have up to five ipv4 cider blocks per VPC through service limits you can increase this to 50 this is the same thing for IPv6 cider blocks um most components don't cost anything so vpcs rot cables knackles internet gateway Security Group subnets VPC peering some things do cost money so uh KN Gateway VPC end points VPN Gateway customer Gateway ipv4 addresses which is a new thing um all ipv4 addresses have a cost so kind of motivates you to use IPv6 elastic IPS I should kind of emphasize here that you know if you have data going across region it's not the VPC pairing turning that feature on uh that will cost you money but going between regions might cost you money um for DNS host names um this is a feature for ec2 instances where you can have a DNS host name on them and you can optionally turn this off I like to point that one out because um sometimes it catches me off guard but there's a lot of stuff that goes into VPC this is just key stuff that I want you to remember okay everyone it's Andrew Brown and we are taking a look at vpcs um before we start utilizing the C to create Resources with it I think it is worth our time to do click offs with vpcs as there are a lot of components and really the easiest way to make vbcs is through click Ops so it's generally how I would do it but um I wanted to take both approaches here what we'll do is go to the top here and type in VPC we'll make our way over to the VPC Management console you're going to see a lot of stuff here on the left hand side this just keeps growing and growing hopefully the UI does not change on us anytime soon but this is what it looks like currently and um you know some of these things are vbc and some of these things are not necessarily uh VPC but they they interact with VPC so just understand that uh if you click through something like Cloud when it's going to take you somewhere else um and this will take you to the network manager or for example we have something like the DNS firewall wherever that is here on the Le hand side and this is technically a rough D3 product so it remains in VPC oh here we go so now it says R 53 notice how it switches is over kind it's still in here but it's not there so understand that uh VPC is its own thing but it's also ec2 and it's can appear as other services anyway what I want to accomplish here is creating a new vbc but before we do that let's go take a look at our current vpcs and you'll notice I have two here if you are uh looking at yours and you've never done anything in your account you should only have one if you scroll over to the right it should be the default vbc I think that it would be this 172 31006 no there's no IPv6 cider block by default as that is just how the default ones are and this one here was actually I I was trying to delete it before I started this video but I can't because this one is a shared vbc from another account so we'll get to that later on but uh you know if I go to something else let's say I went to uh Europe over in Frankfurt Germany I should just see one VPC there it is and notice it's 172 uh 3100 for sl16 I'm going to go back to ca Central 1 since I'm in Canada and that's where I like to operate but uh let's go ahead and create ourselves a new VPC so there is obviously a few ways to do this and we'll go ahead and create VPC and we'll be presented with two options you can create only the VPC or VPC and more and so VPC and more is going to produce everything you need so it's going to create the vbc subnets route tables the internet gateway um um in this case it's showing um if you scroll on down here it's adding a VPC end point so I just say none so notice that vanish there but this gives you a really good visual of a uh basic VPC and how to get started now I don't think we can do 106 because if we go back over to here I think that one already exists it does so I think we would end up with a conflict here and I don't think we could do uh this one but what I'll do is I'll just copy this and paste it in let's see what happens if we try to create it so I'm going to go down below and you know again we can adjust this to say we have three azs I'm going to do one for now and we could say we have zero private subnets and one public subnet to keep this simple this is actually really nice we'll go ahead and say create the VPC and we'll see if it actually works it actually uh created it which I'm really surprised and we go back here we have two that have the same uh CED block range so I'm actually surprised that I was able to do that that but I should point out that you should not have uh vpcs that overlap in their CER ranges um so we're not going to want to have that okay so we'll go ahead and delete it I really thought we were going to hit like something that says it wouldn't work but notice when you go ahead and delete it actually tries to remove all the associated resources so we'll go ahead and type in delete all right um so the next thing I want to do is I want to go ahead and create a VPC from scratch and again you know we're not supposed to use the this EX existing address but I'm going to to keep things really simple so I'm going to go ahead and type in my VPC and we'll go and mirror that IP address that's 172 3100 172 3100 sl16 I think that's what it is let's just do the standard size for it if you're not familiar with cider block there's a website called sd. XYZ it's been around forever and if you put in the number here on the end it will show you uh the size of your um of how you're splitting it so if we have 16 we're going to end up with 65532 IP addresses there's actually less than this because there are some reserved IP addresses but that's pretty much what there is there but if we scroll on down uh we can go ahead and create our VPC we have some options uh whether we want to have an IPv6 cider block still going to say no it could be ipam allocator Amazon provided cider block but I'll just say no for now for tency we want have a uh default I didn't even know they had dedicated I would assume that would cost additional money uh select dedicated ensure that the instance launch of the vbc are run on dedicated uh tency instances regardless of the tency attributes so I guess that would have to do with um dedicated ec2 instances maybe you need a specialized vbc for that the reason I don't know is because I rarely ever launch dedicated instances we'll go ahead and create our VPC and we've created created it but notice that we do not have any components underneath so we're going to have to go ahead and create them so let's go and create our subnet and we'll select our VPC my VPC we'll just say my Subnet 01 and we'll choose any a you could just say no preference it would randomly choose we'll we'll say uh that there and these buttons weren't here before so that's kind of interesting but the cider The Cider block is going to be very similar um so we say 17 2. 31.0 do0 sl20 I think again I don't know what these buttons are but that's nice that they're there but the idea is that when you create your um there we go when you do this Ah that's really nice okay so the thing is we put in our default number and before when you would put in a cider range you'd actually have to calculate what the next number was so if you had 24 and you said give me the next number in range you could just do that I'm thinking here yes it increments so you don't have to do the math um and here if you want to change the cider block size so if I do 20 notice it's incrementing differently if I go up it increments differently if I go this way okay so just understand that there is some logic there and I do have a section on um uh in one of my courses I have a section on doing like cider cider math so we do cover that there but to be honest I never remember it myself and I usually just look at um other referenced uh stuff but apparently we can keep it really simple I'll say 20 let's go take a look at what the size 20 is here so if we do 20 that gives us 4,000 if we did 24 that would give us 256 so reality we'd probably want to be 20 because if we're splitting up that 65,000 um we don't want to have super small subnets but we don't want to have subnets that are super giant either so 20 seems good and it even tells you over here how many there are so that is very useful as well so I'll go ahead and create this subnet actually before we do it looks like we can add two subnets at the same time let's go do that so here will be my second subnet 02 we'll let it choose wherever it wants to go but uh now what we can do is just copy this one here okay and then we can just hit up or sorry right and then we get the next one and we don't have conflicts we'll go ahead and create that so now we have our subnets if we go back to our VPC and we go down here and we go to our resource map now we have our subnets we need to have our route table configured it comes with a route table by default but we can explicitly associate these subnets with the route table so what we'll do is go over to Route tables here and I'm looking for that new one uh we have a lot more than I'm expecting we have some shared ones so really it's just these thre that we're focused on we know that it's not the default one and this one has two explicitly associated with it so I don't think it's this one I think it's this one here and the way we can match that up is by looking at the VPC so if I just click through here this VPC is not showing its name for some weird reason but if we carefully look here 0 C9 and we go back here 084 so it isn't what we thought it would be if we want to really quickly find which one it is what we could also do is just go back to our vpcs and grab the VPC ID and I think if we were to paste it in here like this yeah bpc click that first and do this hit an enter it would sort it for that one but it's not um let's go back over here and take a look this one's 09 F22 so just carefully looking here at our route tables 09 F2 are we not in the same uh region why why can't I seem to find this uh rad table we'll copy this here weird maybe we need to refresh this page oops refresh okay oh okay there we go so for whatever reason um even though we're clicking around it apparently had some cache client information so now we refresh it it's a lot easier to go over here and take a look and we see my my VPC nice and clear we're going to go over to to our um uh our subnet Association we want to see if there're already subnets and so you'll see that these ones are implicitly Associated because this is the default subnet but if we wanted to we could explicitly um associate them and you really should when you can okay and we go back over to our VPC here our resource map I mean nothing has changed there so that's still pretty straightforward notice that we don't have any outbound connection so what we'd have to do is update our route table to uh do that and notice if we click this it says one route including local so I'll go back and click into our route table here and what we'll do is We'll add a new route so we'll go ahead and say edit routes and I'll add a new route and we want to add one to the internet gateway but here's the thing we don't have an internet gateway so before we can actually do that we'll have to create an igw so on the left hand side we're looking for internet gateway and we'll create one we'll say my igw we'll go ahead and create that so we have it and I'll go back over to vpcs and we'll select our VPC and um close these other tabs here and what I want to do is go back into that route table okay and we will edit the routes and we'll add the route and we want this to go to 000000 so it goes everywhere and then we can drop this down to internet game Gateway and I mean there should be an igw for us to choose actually before that will work we'd actually have to associate the igw with our VPC so we'll go back over here to our igw and notice that it's detached so we'll go ahead and checkbox this and we'll have to attach this VPC we'll drop it down to my VPC and then we'll go back over to our routes and if we drop this down does it show up no no so we'll just remove it and we will try to add that again h no it's still not showing up just because we'll have to go back and reenter it because you know sometimes these pages are are uh just whatever client data they pulled at the time you clicked it so if you go back sometimes they will actually then show up so we'll go back into here and there it is okay great and so we want this to be 000040 so it's goes to everywhere and so now we have our uh VPC and um it's using the same one as our default one which again I'm really surprised we're able to do that without conflicts but uh yeah that is really straightforward so let's go ahead and delete that and we'll say delete and so now what I want to do and notice it's deleting all the resources we didn't have to IND individually delete them but now what I want to do is I want to go ahead and uh set up a VPC using the a CLI so what we'll do is we'll make our way over to GitHub okay and um we have that ad examples repo and I'm going to launch this up in G pod so I I'm going to hit the open Button if you want to use code spaces you can if you want to download the repo or do this locally but you will have to configure a CLI we talk about this again and again over in lab so if you didn't do the setup in the beginning of this course go ahead and do that so that uh you know you're not getting confused here but we'll go ahead and launch up our git pod environment and this already comes preinstalled with a CLI it's going to go ahead and install it which is configured in the git pod yaml so it's starting up these tasks on the right hand side you can see I have other things configured I'm going to make my way over to the adab CLI I'm going to just type in clear I'm going to type in adabs STS get caller identity it's been a while since I made a video not to you it seems like I just made it yesterday but I'm to see um what account this is for so this is for 982 38 so I'm going to go back over here 982 38 so I believe it is this account and I think I'm I'm able to make stuff so what we'll do is make a new folder on the left hand side um called uh VPC and in here we'll just make a new file new folder here called basics and I'm going to create a new file here I'm going to make it a bash script so this will be uh VPC Dosh or we'll just call it um sorry we'll um I'm just trying to decide here how I want to do that yeah we'll just make it a a executable file so we'll just say VPC and so I want to turn this into a bash script so we'll drop this down here and I'm just looking for any other file that has a bash script in it like this one nope maybe in our bin here we go so I just want to copy this here paste allow and so now what I want to do is just um write some VPC commands so the first thing we want to do is we want to we want to create uh our VPC we want to um create an igw we want to attach an igw we'll need to uh like a route table will get created by default when we create the VPC but we want to explicitly associate uh subnet whoops subnet going say create a new subnet associate a subnet add a route to our uh add a route for our RT which stand for Route table to our igw and I think that's pretty much everything we need to do that so let's go ahead and figure that out um I'm going to go ahead and just launch up the adabs CLI here notice that it's launching this mode because I have environment variable set um here so if you don't have it set set it if you don't know how look it up and so we want to do something with VPC now everything's under e 2 uh so we'll have to type that create VPC so I just want to go up here and start typing this up so we'll say adab us ec2 create VPC and if we go down below it should tell us what we need so it doesn't say it's required but we definitely need to have a cider block let's go ahead and add that in there so we'll say cider block and for this we'll say 172 1.0.0 sl16 uh because there's periods and other things I'm just going to wrap this in quotations so that we don't run into any trouble here there could be some other options that we might want to set going to click down and just press down here and take a look nothing super exciting very straightforward in terms of its configuration I'm not sure if it would automatically set the DNS host name so we might have to configure things after the fact we'll go ahead here and uh say region we'll say ca Central one all right so that'll be step one okay um so yeah that that's fine so what I'll do is I'll just go ahead and copy this command and we'll make this into a script I'm just going to make sure it works so we get some feedback here I'm going to go back and give this a refresh and it did create it it would be nice if this was tag so we can see which one we're working with because I don't want to accidentally delete the uh default one I also called this 172 1 0 0 which is fine because then it doesn't conflict 1 2 3 4 1 2 3 four yeah I'm just making sure I have the right amount of numbers so that is fine I don't even know if 17210 is a real number but it works I was able to put it in there so it's fine but uh it has no conflict so that is that's okay but I would like to tag this so what we'll do is look this up in the CLI and we are going to look for tags so there's some way to tag stuff tags tag specification down below here so it's going to be something like this okay I'm going to paste that in like that and it's going to expect some kind of format resource type string Tags I'm not sure why we need a resource type the type of resource on tag creation the tags to apply to Resource when the resource is being created so this is if you want tags to propagate whenever you create resources and that's not a bad idea but what I really want is actually just tagging which isn't exactly the same thing so let's just search for tags a bit more and you know I just don't have confidence that this is how you how you apply tag so let's just carefully look here maybe there's like a way of naming it as opposed to tagging all all shows is tag specification so I'm not confident in this so what I'm going to do is go over to chat GPT create a VPC with a tag name of hello and let's see what it produces maybe we're using the old CI and that's our problem we'll go here still nothing new tags so is it tag specification that's why I want to know just give me the you know what it's cuz I didn't specify via the ads come on tell me something here okay let's just say just tell me the command don't explain anything thank you so it is tag specification I really thought they would have been something on the end like tags VPC okay interesting so we go here and it makes sense to put this in uh single quotations here the name will be uh my vbc three to make this a little bit easier to work with I'm going to put in some backs slashes I usually like the region to be last I'll go ahead and do that it already is defaulted to region CA Central 1 but um I'm just being very explicit here just in case of course if you're following along you got to make sure you switch over to ca Central 1 but I think at this point you probably know that but what we'll do is we'll go ahead and um I mean I'm going to want some uh output returned here because there might be some things that I want like the actual VPC uh the VPC ID might also want the cider block here I'm not 100% sure but I'll definitely want the VPC ID so what we'll do here is just query this and we'll just say VPC and I think what we can do is do zero and then do VPC ID I'm just taking a guess here if it messes up we can just fiddle with it a bit and I'm just hitting Q on the bottom here to to quit out of the the uh C but the CLI for me is Frozen for some weird reason there we go and so we'll try this again enter and it's hanging not exactly sure why it's hanging I mean I don't think we deleted the old one did we no but if we go back here yeah it's not uh it's not being responsive so I'll just close that out and we should really delete the other one before we proceed here so the other one was the the one this this one here right so we'll go ahead and delete that delete and I'm going to go back over to here and I'm going to copy this again now I think that it said vpcs and not VPC let's just double check the output here all the way the bottom there should be some output maybe or it will explain no it's just PC okay um all right well let's see what we get then we'll copy this and we'll paste it in here now because we're in a different tab we're not going to get that auto complete which is fine it is not cooperating here today right click paste hit enter and we get null back so it doesn't mean that it didn't work it just means it didn't like our query yeah so we didn't like our query we'll go back here ahead and uh try this again so um we'll go and delete this one and it didn't like our query so what we can do are going to need that flag I'm going to go back over to here and just grab it paste it in just say uh how do we assign them again I should know this export it's uh going to be export there we go and so just say Ad ofs VPC or sorry ec2 VPC cider block 172.2 z.0 sl16 query VPC oh maybe just that okay because it only returns one thing I thought it came back in an array and so I I assumed that we'd have to do that but maybe it's not an array it's just this which is fine if that that's what it is that's great okay and so we're getting back kind of what we want it's wrapping it in quotations because it's returning back Json so I'm just going to go here and just uh say output as text and so that should uh fix that we'll go give this a refresh very good we'll go ahead and copy this we'll paste it in we'll hit enter and so we get back that the reason I wanted to do this was so that we could assign it to variable this this will capture it and assign it so we'll just say uh bpc ID like this you could do export you don't necessarily have to I don't think but I'm going to go ahead and try this here so we'll go and refresh this because we creating a script right so we want this to um continue on and actually if this is a script we we wouldn't put export in here we would just we would just have this now that I think about it and then we could Echo this down below so we'll just say dollar sign VPC ID sometimes in videos I will just write these in read me files sometimes I prefer to make scripts I'm just trying to give you um more experience in different ways so you'll see alternating in different videos how how I'm teaching and so it didn't return anything but that doesn't mean that nothing was assigned but uh in our script it probably would have echoed that out so I'm not too concerned about it I do believe that is working we can go ahead and uh just quickly test this by running our script we'll have to chamod it I don't think we did that earlier so we'll go here and say VPC Basics um VPC I'm actually going to change this to say create VPC and then we'll have one which will be like really doesn't like that I rename that shouldn't be that hard to rename come on you can do it maybe there's issues with um uh G pod right now because it's normally not this uh problematic for renaming stuff while that's uh thinking really hard I'm going to go ahead and say create VPC yeah there's uh something wrong going on here I think it's just my environment so what I'm going to do this is a bit of a pain but I think I'm going to have to just stop the video here or pause the video and come back to it in a moment okay till this is resolving I just waited a while here and it's finally resolved itself again I'm not sure what the hanging was for but it is back now so that's okay but we create um anyway we uh assigned that VPC ID but I want to have another script here I'm going to call this one um delete VPC and the idea is that we will create it in one script and tear it down the other one again I'm not sure why this is hanging so badly but um I guess we'll wait here a little bit okay again waited a long time but it did eventually create a file I'm not sure why but all the uh the windows are becoming very unresponsive there we go is this one still broken yeah I don't know uh give me a second yep um not exactly sure what's wrong here but what I'm going to go ahead to do is just add everything commit files and what I'll have to do is just uh stop and restart this workpace not sure why it's having uh troubles but right now I'm not technically using terraform so I'm just going to well I kind of want to leave it installed I guess I don't need Powershell so byebye Powershell for now and hopefully that gives me less issues this is what it's like it's h you'll be working and then you'll run into issues but I'll be back here in just a moment I'm just going to uh stop and relaunch a new one okay all right I just uh restarted it here but anyway we'll continue on and so the idea is that if we create it then we should be able to uh quickly tear down uh tear tear it down but I'll go ahead and just chamod it's uh doing some funny loading here we'll chamod that other file here so VPC Basics delete VPC um so now they're both executable but the idea is that if we want to create something we also want to be able to delete it so we'll just say delete VPC and this one probably works backwards because as you are um deleting stuff you want to do in the reverse order I believe as opposed to creating um but we'll go down here and just type in AWS VPC or ec2 VPC VPC delete VPC and see what options we need to provide um oh sorry delete VPC and it just wants the VPC ID okay so we'll grab this and we'll paste it in as such VPC ID um there is a way to to uh bass script that takes one argument and assigns it to an nbar in the script okay I'll just get some basic code here I usually copy from something else but um I mean technically that would work okay uh if uh if statement if it's not present and that's actually how I would do it I would set it like that for sure so I'm going to copy this example here because I'm happy with that one so just above here in our script I'll go paste that in we'll need this first line here and so this is going to get assigned to VPC ID okay so now the idea is if we go back over to here we don't have any vpcs that's totally fine I'm going to go back and just cancel out of this and I want to run that script so just to make my life easier I'm going to go into that Basics folder we're going to run that script and so that should create the vbc notice it printed it out so if I go back over to here and give this a refresh we can see that it created it if I want to go ahead and delete this I'll do delete VPC and do that and if we go back over to here it is now gone so our crate works are delete Works let's continue on with this script um so we'll want to create an igw I imagine that's going to be very similar so we'll say ABS ec2 igw or create igw or internet gateway it does not require anything so we'll go ahead and just do that um I would like to associate it so we'll have to have there's probably an associate one again just guessing here but associate internet nope not not what I was hoping for so Associates or maybe it's attach because you can attach it right attach there it is internet gateway and it will want to have the igw internet gateway ID and then we'll want to have the VPC ID I imagine it's actually autoc comp completing which is nice but that's not what I need right now so just do that and we'll want the VPC ID so I'll go ahead and grab this here because we know we're going to need to attach it from here we're going to want to have the um the actual VPC or the the internet gateway ID I'm not exactly sure what we would be so we'll go ahead and take a look here attach uh we could also just grab the command here as well and just utilize it down here below create internet gateway we'll say query internet gateway and then we want the ID like that so that is how we're going to grab that there we'll see output text this will be our igw ID and we'll Echo that out as well so that should be good um then we should be able to attach it so we'll say igw ID and we'll say VPC ID we'll go back over to here and uh before we delete the vbc we need to detach the igw and we need to delete the igw fun stuff eh but this is how you get good you have to put this time in here and uh do that this is uh not listening to me I can't uh get this to get out of here oh boy this one works nope they all just hang what the heck like every second this one's fine this one's fine utilities is not fine this one's hanging as long as one of them is working and no like notice I'm erasing it it keeps coming back so again not sure what's going on here but what I'm going to do is I think I'm going to have to launch this in my local um editor and so there's an option in G pod where I can just say open in uh vs code so I'm going to go ahead and do that right now okay all right well I would say this is being extremely unreliable here today so what I'm going to do is actually use code spaces I usually don't do that but uh you know sometimes you just got to do what you need to do to get things to work get commit uh notice I can barely type and get anything done here today get commit hyphen M um add files but that's the thing about Cloud there's always other options and you can work around them luckily I've gotten really good at code spaces as of recently because I did a GitHub certification course um and I had to go deep dive on code spaces but what I'm going to go ahead to do is say create a code space they do have a free tier so it's not that major of an issue but I'm going to have to do some configuration to uh get this environment working it more acts like a virtual machine where G pod when you shut things down they uh it's like turning off a container or just it just stops after a while where this one you have to be a little bit more careful about shutting it down so you're not over utilizing it but you know if you didn't attach your credit card to anything then it's not going to cost you anything so it's not that big of a deal so I'm opening this up in code spaces and what I need to have installed is the ad CLI and if I want to utilize ad CLI I need to configure a Dev a Dev container Json so what I can do is go down here in the bottom left corner open up command pallet I'm going to look up uh Dev container configuration file and I'm going to create a new one because I don't have one yet and in here um um I mean all I really want is AWS so it doesn't really matter what it launches but here it says simple Alpine container want use the default one so I'm going to say default Linux Universal so that is probably the one that is being used right now and what I want in here is the Inaba CLI and we'll go checkbox that on and at some point I might want terraform but right now the C is fine for me it says we've noticed you changed the dev container rebuild the container to apply that now so go up to command pallet and I'm just going to type in build and here we'll do a code space rebuild I'll will give it a moment to rebuild okay it's rebuilding right there we go yep uh rebuilding creates recreates the code space let's go ahead and do that see you back here in just a second okay all right uh so there we go after that rebuild we should have the C I'll go down below here and type in adabs we'll see if that shows up the only thing is that we are not connected to um uh to anything right now so I'm going to have to go over here and you've seen me do this a bunch of times in Labs here but I have to go over to IM and what I'll need to do is I'll need to create new credentials so under my users here and I'm doing iTab examples for my security credentials I'm going to leave this other key around because I don't want to have to go back and uh rotate it right now we'll go ahead and hit next and we'll create a new access key and the way we set them in here is it's called secrets so if I go back over here type in Secrets manage user Secrets we'll go ahead and click that and we can add stuff we can manage it on GitHub as well it's a bit easier to manage it on GitHub as opposed to entering in that little thing you can see I have some stuff in here from November which is very old and probably not the credentials that I'm looking for right now but we can create a new secret and I want an access key ID and I'll grab this one paste that in I'm going to select the repos I'm saying examples there it is good so that's that first value um I don't want to show the second one here so I'm just going to pause the video and just assume that you have to follow through as well the same way I'm also just going to add in a default region here so I don't have to worry about um constantly choosing CA Central 1 here add that secret now even though we have um oh it says your Cod spaces Secrets have changed reload to apply so we'll click reload to apply I'm going to wait and see if anything happens here I keep expecting something to happen sometimes it's a bit delayed in code spaces you have to be very patient there we go see y you had to wait you know G pod really great product when it works um when it doesn't for whatever reason then it's very frustrating but it hasn't really happened to be a lot it's just for whatever reason this week I just been having a lot of trouble with it so but uh you know code spaces is a good backup if code spaces didn't work we could use Cloud9 there's just tons of options for us to utilize I don't want to use that down below here but this one tends to forget your configuration so it's a little bit frustrating to have to go in here and keep bumping up fonts and doing all that stuff but I'm just going to increase this here and we'll find a terminal size here terminal bring this up to 20 so we can see what we're doing there we go and we're back to business so we'll go ahead under our wow S3 we did a lot uh we'll go into vpcs and so we'll continue on with our creating our basic stuff um we did not create yet but we want that igw and that VPC ID so let's go ahead and see what happens if we run this here and I'm noticing that this is unresponsive oh it's typing though oh you know why I was trying to hit up as if there was like a previous command to type and there isn't um but let's make sure that we can actually do something here so I'm say get caller identity okay so there we go we are authenticated and I'm going to CD into that folder VPC Basics and we will create the VPC and we have a spelling mistake here so we'll have this output another thing that we can do is if one of these commands fail we can set e and that should um quit on the I think it's set e which will quit on the first instance of something what does set e do in a bash script I think that's what it is I just always forget execute yeah that exactly that's what we want so now that that's in our script we'll go back over to codes spaces here the thing I don't like about code spaces the icon looks the same thing as your repo so it does get a bit confusing but um if we go back over to our VPC ID or VPC here it is we'll go ahead and delete that good and we'll try this again to say create VPC line 21 line 21 oh we need a fort slash on the end there so let's let's go check our internet gateway now and notice we have it over here it's not supposed to be there we'll just delete that and so maybe this time it will work we'll try this again create VPC very good so it created both those there if we go back over to our VPC here and give this a refresh um under VPC sorry and we have to apparently you could just name them the same there's no conflict so one of these is our new one one one of these is our old one resource map so that one has no connection that one over here has it so this is our latest one but we really want to get rid of both of these so I'll just individually delete these right now delete delete fun stuff eh and we'll say delete since it's so easy to delete via the uh uh console I think we'll just focus on the creation actions and then we'll do all the delete afterwards but we've attached our internet gateway we need to create a subnet um I would really like to have that partial command on so we'll look it up say CLI on partial because I always forget what the environment variable is here it is allow what does it say um export on partial is that what it is maybe it's all lowercase clear AWS bc2 and we want to create a subnet so create subnet and it will want a VPC ID so go here and say ads ec2 VPC create subnet VPC ID that'll be our VPC ID here um it's not specifying anything else but definitely you would have to provide it a cider block which is weird because it it would be required says this parameter is required on IPv6 not exactly so we know that this one is um this one here so we grab that then we'll go down below and we'll paste this in that's our cider block and we will uh what will we do here we have cider block 172 one0 16 and that technically will work uh we want this to be 20 though and so that's fine so we'll see if that works create create the subnet good and we can query that and say query subnet subnet ID if we want that I'm not sure if that's what we need but we might subnet ID I keep thinking I have Vim commands in here I don't don't install Vim unless you know how to use them but I'm just going to install it so that I can uh move a lot quicker in here so that would give us the subnet ID whoops and so that should get us the subnet ID we'll just say Echo subnet ID probably want to explicitly associate it we'll look that up um a CLI associate subnet to route table which is going to be over here Associates a subnet in your VPC we'll go down examples that one looks pretty good we'll go back over here P it in so we have route table and subnet we would need to know what our route table is um we don't really know because it was cated with the vbc we might be able to get from there but I think we might want to describe or um get route table from subnet or from VPC I'm GNA be lazy here um using the a CLI get the route table get the default route table for the VPC for the provided VPC just show me the command don't explain describe route tables it's applying a filter and yeah that makes sense so go ahead and copy this and so what we would need to do is we need to get the RT ID route table ID and we're going to apply our filter this filter is a little bit more complex than usual um I'm going to say output text just because I don't want it to be wrapped in quotations and the other part here is it says VPC ID that's fine we need to actually provide the VPC ID here so say dollar sign VPC ID that should get interpolated it says association. main because that's how you get the default table so that should technically work and we're not asking for the main VPC we're asking for the main route table the default route table so yeah that should assign it over here and hopefully that prints out and then the idea is we just do RTI and then here we do subnet ID so that should associate that route table and then the last thing would be creating a route but let's make sure this works first we'll go back over here to our vpcs give it a refresh we'll go ahead and delete our new VPC and we'll hit up BBC igw ID subnet um a bunch of stuff back so we came pretty close to getting what we want um this which one messed up here I mean this one should have been wrapped here so it's not a big deal we'll just say subnet ID unknown command for this and it could be that uh we need to apply a query on this so it might be printing out all the information notice I said text and it came out like this so that's probably our issue is that it doesn't um grab just the route table ID if we go back here uh only query the route table ID from describing the route table please hopefully doesn't describe anything to us good thank you and this is what we're missing here so go back over here and we will paste that in so I'm pretty confident that's going to fix that issue there we'll go back and remove this again super fun but you know what all this practice makes perfect you want to be a cloud engineer you got to know how to do this should be follow along here don't just watch we'll go ahead and create that again VPC igw subnet ID RTI Associated good good good good um we need to uh still do a little bit more like another thing that that might not be obvious is that notice that DNS host name is not enabled which means that um our vpcs when they're launched they're not going to get host names which is like a friendly um it's like it's like getting a domain name for your uh your instance and so you might want to have that enabled and so that's something that we might want to turn on I know that's like through config here so say uh we'll just say configure VPC to have DNS host names turned on so tell do that and that's going to be the modify BBC attribute I can't even be bothered to look that up I know this is correct so we'll do that and we'll go here turn on DNS poost names so that's one configuration we'd have to be uh concerned about bpc ID okay uh another one is that this um these route tables or subnets I should say I'm not sure which ones are ours but it doesn't matter which one we look into I'm going to refresh this for a moment if we look into it's down here actually if we we look into any of these subnets there should be an option to uh to whether it will assign an IP address or not that's what I'm looking for right now at least that's what it over here Auto assign a customer owned ipv4 because if it doesn't have this Auto assign this is technically a a private subnet okay that's there's no difference between a public and private subnet with the exception that um a private subnet it's assumed that you don't have public IP addresses so by not Auto assigning it that' be the case but you technically could have uh those there but just say configure subnet to have Auto sign ipv4 address and so now it's modify subnet attribute very similar to what we saw there before so go ahead and copy that back over to here and this will be associate subnet I just first I want to create it then we'll just say Auto assign ipv4 here and we do have the subnet ID so I'll go ahead and paste that in here undo as it was not listening to me the last thing is we need to add a route to our route table and again I'm going to just use TBT for this so add a route to our route table to our igw please give me the CLI command thank you create a route to our route table to our destination cider block excellent that looks good to me no issues there the worst case if it didn't work we just look up the docs we know how to do that by now and I'll go ahead and paste that on in here so here we have the route table ID so we'll go ahead and type in dollar sign RTI ID um here we have the Gateway so we'll say igw ID I believe we have that that's the destinition so that looks good to me so this should essentially set up everything that we want okay so what I'm going to do is go back over to here into our vpcs I'm going to make sure we delete this as such delete and we'll go ahead and do that we'll create the VPC okay it looks like it did everything we'll go back here and give it a refresh we'll inspect it so the first thing it has DNS host names enabled excellent we'll go to our resource map looks good looks good we'll check our route table look it says one subnet Association two routes including local the IG igw is connected let's take a look at our subnet says no IPv6 we'll go into subnets and it is somewhere over here fresh it's down below Auto assign excellent so that is the proper way to set it up let's look at deleting yeah you thought we are done no no we have to do all of this here so we'll go over to delete um and I'll speed it up a bit so we'll say I want uh C commands for detach detach and igw delete a subnet delete a route table delete a um attach delete NW delete avbc so go ahead and hit these commands and hopefully it will just produce them for us okay I'm just going to copy these and try to get them in the correct order so we do want to detach for sure I believe we have to do that beforehand otherwise it'll complain we want to delete our route table at some point I would do that um after we get rid of the igw I'm not sure if we have to delete the routes first we definitely want to delete our VPC so we've already done that we need to delete our subnet we'll go back over here so delete subnet I think that's everything detach delete delete subnet delete route table delete VPC and um so this needs to take more than a single parameter and it's going to need um the internet gateway the subnet the route table of VPC so we have have the VPC ID this will be igw ID we'll fix the numbers here in just a moment okay and we have the route table ID so we just have to kind of figure out what order we want this to be in so I would say VPC igw route table subnet is the order so be dollar sign one we know like what numbers 1 2 3 four so VPC ID is first igw second um we can't delete a route table first we'll have to do that last we'll say subnet and then route table RT okay so that's going to be the order of this so this is 1 one 22 33 44 all right we'll say dollar sign igw ID we'll say VPC ID subnet ID RT ID okay and so I think this this should work right we'll go ahead and hit up oh I'm creating what why and I didn't enter anything in okay we'll go back over to here we'll see that we have more than one VPC again I'm really surprised that you can create more than one I I really thought at one point that they uh complained about that but maybe they changed it so we'll try this one more time we'll say create VPC you know we could have done at the end of the script we could have printed it out so it's a lot easier to work with so let's say delete VPC and so we need the VPC ID first the igw second the net third it's actually in the correct order that we need it to to happen in and then this okay or at least how we specified it let's see if that works enter so it says when calling the delete route table operation the route table has dependencies and cannot be deleted so we have to disassociate it before that happens another thing is that I just want to make my life easier and so at the end I just want to print out this command so that I don't have to continually uh print out this command here so say Echo and this way it'll be dollar sign VPC ID dollar sign igw ID dollar sign subnet ID dollar sign uh RTI right so we'll go here and we'll print out that command for later on but um disassociate a subnet to an RT to a route table via a CLI okay cuz that's something that we will need to do I didn't think we'd have to do that and we will go back over to here add a route we want to delete delete um dis associate subnet don't worry if you don't spell it right no one's going to look just you're going to notice I did that and we need the RT Association really come on okay we'll go back here um I wasn't done with this script get the um associate route table ID using the VPC ID uh using the ADI okay sometimes you have to do intermediate queries to get stuff it's kind of annoying but this is how it goes if you know the route table ID yes I do I know the subnet as well can we narrow it down more y because it seems like we have to use Query and filter for that and uh it's a lot easier to let a program do this we go back over to here and we'll paste that in here and so this will be the rtdb ID I guess or we'll just call it the assos ID so ID that could have gone wrong real quick there but it didn't and and uh we'll just put parentheses and you know I'm reading all the stuff we're doing so I don't just paste commands in I'm always looking at what they're doing so if you don't understand them make sure you know before you run them just don't blindly copy paste stuff okay we have the associ subnet ID here it's going to want the subnet ID right here that's fine I think that should work um and then same thing here subnet ID shouldn't it want the route table ID interesting if we go back over here like this one like you explicitly specified so why wouldn't we do that here I'm not sure why it didn't do that sometimes it also makes stuff up so this might not even be correct if we don't look up the documentation but we know what the RT is so we'll just say RTI ID I'm going to just assume that I can do this I'm going to try that out just going to double check that and then we'll paste that in here but this could totally muck up on us so we'll find out I'm going to say Echo assos ID dollar sign assos ID good so maybe this will work I'll go back here see if I have anything I don't that's fine we'll type in clear create VPC hoping that last command helps us out good and now we'll go see if we can delete it we'll try this again another thing we could do is for that reassociation we could print it out and just pass it along here it says unexpected token AWS it's because we're missing the dollar sign here we'll go back and we'll delete we'll say delete and we will go back over here give it a moment did it delete yes we'll create it again there's no way that you would not know how uh vpcs work after spending all this time with me just making them 100 times a uh calling the create internet gateway operation max number of Internet gateways it seems like it might not be deleting all the other ones let's go here and look we have detached ones but it's not necessarily deleting them so maybe when you actually um delete it via the VP like the CLI it wasn't getting or the console here wasn't getting rid of them which I'm really surprised not a big deal these don't like it's not like they cost anything having them hang around but uh we do have to do a little bit work to get rid of this here we'll go over here did it actually create some stuff it did so we'll try this again great okay cool let's try this enter now if we're having a hard time with that disassociate and there we go we already having a problem with it if we're having an issue with it we could always uh fiddle with that a bit line 42 so yeah our query didn't seem to work what I'm going to do just to make our life a little bit easier I'm going to go back to this script because I we can fiddle with this all day and try to grab that but the other thing is that we could probably just get it from here because I imagine that this command probably um has that associate ID and then we can technically be done sometimes you just got to do that in your life you know you got to make your life easy for yourself all it does is return an association ID so um if we do output here output text and query this the soci ID not the best way but it will work just going to put that on the end uh we'll put it in the proper orders this will be subet ID and then this asso ID which I think is silly that we have to include it but let's just do it anyway and so we have this here we'll bring this down this will be our assos ID this will be number four and number four we'll do number five and we'll do number five we'll go all the way down to the bottom I'm just going to make sure yep that match is good and we should probably Echo out that asso ID if we're Noto shate subnet here and we'll go back over to here we'll give this a refresh the only way we would have known that that uh internet gateway was not getting deleted was the fact that we did this so many times and that's why it's so important when you're learning cloud to repeat the thing you're doing a 100 times over um so that you get that kind of uh differences there a lot of people they just go through it they do it once and they would never know like this was never deleted so make sure you repeat repeat repeat um just going to close that Tab out we don't need it anymore let's go back over to here I'm just going to hit clear on that contrl c and let's go ahead and create I think we printed it out on the bottom yes we did good I wanted to do cloud formation this video but I figured uh we don't need to do that again we' already given people enough pain here we are running into an issue on the output on line 59 it's cuz we're missing uh this here query associate ID oh yeah that's all that's missing okay um so it did not work still so close internet gateway this time it actually got rid of it so you know just double checking go ahead and create the VPC good now let's see if we can get rid of it all now this word manually doing so something could happen here or not work the route table has dependencies and cannot be deleted so we go down here we disassociated it maybe it's talking about the routes let's go in here and find out so if we go in here and take a look at our rad table um it's not exactly saying which one it is let's pull this up that's the main this say to routes it's not that one I don't think it's that one vpcs it deleted the VPC but it said it didn't work so did it not actually delete it is it possible that the VPC deleted it let's go ahead and just comment that out so let's say we try this again we just got to pay attention to our route tables because here we have more than what we should have at least I think so but we'll go back over to our VPC we'll try this one more time again I think we're done but I just want to confirm that we know what we're looking at okay excellent we'll go back over here I want to confirm with the row tables what we see there it is enter no errors this time but it looks like the VPC carried away the rest way uh the rest of it with it so there you go I think that's sufficient for vpcs for the basics um and we'll see the next one okay ciao oh actually before we go I just want to make sure you're not wasting any money so um normally like I don't not worried about git pod but with this you definitely want to stop your workspace so what I'm going to do so I'm going to go up here to to the command prompt and I'm going to say stop or we'll say code spaces maybe code spaces and we can stop this before we do I want to commit my code I just kind of want to keep everything I have here so configure Dev container and add VPC Basics okay let's just say okay that's fine push push push and we'll go back over to here and we'll just say code spaces I'm looking for stop stop current workspace so we'll stop that and that way you aren't um wasting your your free credits for your workspace okay we'll see you in the next one ciao itus has a default VPC in every region so you can immediately deploy instances so here is an example from the console as it looks like today as I'm sure they'll change it sometime in the future but uh this is for the Ireland region and if you notice the default VPC uh uh flag says it's yes um so that's pretty straightforward a default VPC is configured by default with the following an ipv for cider block at the address 172 3100 for6 I believe it's the same for every single region if it's not it's not that big of a deal but I'm almost certain that it is it has that for6 on the end indicating the amount of ipv addresses that are available to you so there's about 65,500 understand that this number is is not exact because there are reserved IP addresses that we'll talk about at some other point but generally that's how many you have it's going to come with um uh for every single availability Zone there will be a subnet so if you're in USC to one which has six azs then there will be six subnets and so uh those will be the size of Ford sl20 and each of those subnets if you're not familiar with Ford sl20 will be about 4,000 ipv4 addresses we have an internet gateway we get a default Security Group we get a default knle uh we'll have default DHCP option sets which you never have to play around with but it it is an option we have route table that routs up to the internet gateway you can delete a default VPC you shouldn't but we'll talk about that here in a moment and iTab us recommends that you use the default VPC other cloud service providers May recommend against using default VPC such as gcp they don't want you to use it you're supposed to delete it um or not utilize it but for ads there's no reason to delete your default VPC um if for whatever reason you delete your default VPC by accident which happens a lot or intentional you may want to recreate the default VPC and you can run the following a CLI command which is create default default VPC and you'll get some output indicating that it's been created notice that it uses the 172 3100 I'm not sure what would happen if you were to create uh like delete the VPC and then use that address and try to recreate it I'm not sure if it would conflict on that so that's an interesting use case but it's not something it would ever show up in the exam just something I'm thinking about this command is only available via the CLI so if you delete your VPC you got to be comfortable using the CLI here some other things I found out while U playing around and discovering some stuff you can you cannot restore a previous default VPC that you've deleted you cannot mark an existing nondefault VPC as a default VPC if you already have a default VPC in the region you cannot create another one there can only be one just like the movie Highlander but there you go and we'll see in the next one let us talk about deleting a VPC because you're going to find that when you go attempt to delete a VPC there are a bunch of other VPC components that have to be deleted before you can proceed so there are security groups and knackles there are subnets there are route tables there are Gateway endpoints internet gateways egress Only Internet gateways if you're using them and so you'd have to run all of these commands before you can delete your VPC so you can see there's a lot of work to do that however when you delete a VPC in theas Management console it will automatically attempt to delete the resources for you so um yes you are supposed to delete these all manually but uh this is one of the conveniences of the console and when I can I always try to delete the VPC in the console okay the default route or catchall route represents all possible IP addresses and you can think of this route as giving access from anywhere or to the internet without restriction um there is a ipv4 4 version of this default route and an IPv6 version of this route since IPv6 is becoming more pre prant prevalent however you want to say that word more common um Cloud providers are charging for ipv4 forcing you to consider IPv6 so it's important to learn both of them so for ipv4 it looks like this 0.0.0.0 and for IPv6 it looks like this colon col0 why is the IPv6 just colon colon well if you've ever seen an IPv6 address you know they are super long and so they came up with the shorthand which is the double colon to represent all those zeros and hyphens uh when you specify um let's say a route in your route table for the igw you're going to be putting 0.0 0.040 to get out to the internet uh when we specify it in our security group we would do it in our inbound rules we're allowing all traffic from the internet to access our public resources so those are just two examples uh when you are creating routes you will have to create a route for the ipv4 and the IPv6 for both of these use cases where whether it's a route table or Security Group so just be aware of that um but there you go okay hey this is Angie Brown in this video we are going to look at uh deleting and recreating the default VPC because this is something that I've done so many times and I'm glad that I know how to bring it back um so what I'm going to do is make my way over to VPC get there however you want search for it or click it if it's in a quick link there go over to vpcs and I'm looking for the default VPC so over here we have a default one and I'm going to go ahead and delete that vbc says warning if you delete the default vbc you can't launch instances in this region unless you specify a subnet in another VPC or create a new default VPC so something I didn't really know is that uh if you didn't have it you have to explicitly select another vbc I guess that makes sense because it wouldn't have anything to associate directly but now it is gone so what are we going to do if we need a new one what we'll do is we'll go ahead and open up um our console here right so if you don't have Cloud shell in your region you're going to have to connect the a CLI via G pod or code spaces or Cloud9 however you want or your local machine with your credentials I use cloud shell to bring back that default VPC so what we'll do and notice it is gone so it's gone we'll type in adabs vc2 create default VPC and we'll specify the region CA centor one it should know what's what region we're in but just in case it doesn't we'll do that and so that should bring it back we'll give this a refresh and there we go we have back our default VPC so nothing too complicated there um and that's pretty much all I really wanted to show you in this video so we will see you in the next one okay so we're talking about share vpcs but this is actually for a very specific service which um allows you to share a lot of things other than vpcs and technically we're not sharing vpcs we sharing subnets but I wanted to give this a a good feature in the course because it is such a useful uh service so adus resource access manager also known as RAM allows you to share resources across your adus accounts so vpcs can be shared with other it accounts within the same region to centrally manage resources in a single VPC using a shared VPC allows you to reduce the number of epcs that you create and manage separate accounts for billing and access control if you want to utilize this feature you got to turn it on using the ram API for your ab organization you share vpcs by sharing subnets I know confusing but that's how it works you can share only nondefault subnets which I think that's the case um I accidentally deleted my default sub uh VPC so I'm not 100% sure but that's what the docs say is that it's non default subnets you need to create a resource share in Ram so what you're sharing you need to create a share shared principles in Ram so who you plan to share with and uh once you have shared that vbc if you go to the account that you're sharing with it will just look like a normal VPC it'll appear there and if you go into the VPC it'll actually tell you who the owner is so you have a clear idea how that works so there you go hey everyone it's Andrew Brown this follow along we're going to take a look at how to set up a shared VPC using the IUS um resource manager also known as RAM you might have already seen in a previous video um that I was indirectly showing that I had one shared because I had tested this out earlier so I do have to get rid of this one but this is what it looks like when you are sharing a VPC just looks like a normal VPC but um what I'll do is I'll make my way over to a resource manager I'll tear down this old one and will make a new one from scratch and uh I just have to kind of look at how I did this before as I don't fully remember but um I know I shared it in here um probably have to be in the exact same region for it to work so I'm going to go to ca Central 1 yeah there we go there it is and you know again I like to do everything via the a CLI but I think for this one I'll probably end up um it says it's deleted I'll probably end up um uh doing everything through the console just because this is this this isn't something you do very often and uh I don't feel like it it benefits from utilizing CLI so notice that other one vanished so the idea is that we have two accounts this is my uh one of my other accounts and I want to create a vbc in this account and then share it to this one so the way I'm going to do that is I'm going to make my way over to um uh VPC in this other account and we'll create ourselves a new VPC so we'll go here here to vpcs and we'll you can see I already had one here before so I'll just go ahead and delete this one before we proceed delete okay not sure what this one is I'm not sure if this one's in use but I'm going to ignore it for now and I'm going to go ahead and create myself a new VPC here and we'll say VPC and more I'm going to scroll on down because I do not want to have a VPC endpoint and I'm going to keep it to uh zero subnets it's interesting you can't choose one why can't I just choose one I'm not sure why totally fine May oh you know I think it's reliant on the amount of azs you choose I'm going to choose one AZ and keep this set up really simple inste I have 10 0 06 um oh look at that you can toggle through different numbers that's cool okay we'll start at I don't know trying to figure out something that I can use here we'll just do again I don't know if this is awful looking but I'm going to make it 12006 if it works it works who cares just drop this down see if there's anything additional I want to add I'm going to add a new tag here and I just want to make sure I have a name for this I'm going to say name and we'll just say Ram bpc example so that should name it is this project VPC here oh maybe because it's autogenerating the name so we'll go down here below sorry we'll go up here we'll just say project this is be a ram example VPC and we'll go down down below here I'm just to collapse that and create this VPC doesn't look like it will let me remove that tag great and so we'll create the vbc that we plan to share now that we uh have created that vbc we'll go up to actions here and I don't see an option to share here so what we'll do is go over to RAM and we'll share resource we'll just say Ram example share and then it wants us to choose the resource we want to share well in this case we want to share a subnet because you don't share the vpcs you share the subnets Apparently and here's the one that we want to share so we have it selected which is good and we'll checkbox it on down here below next and so it's asking for permissions yes and then there's the principles this is who's going to be allowed to use it so we're going to specify the other adus account here copy that and go over here and we'll add that and then we'll add the actual principle we'll go to it create the resource and so now that should allow us uh to see that resource so if we go back over to our other account here and we give this a refresh we now have two vpcs so the idea is that um some things are shared but what would happen if we launched an ec2 instance let's go ahead and try that out so there's going to be a couple things yeah we'll we'll create an ec2 instance in this new VPC and with the security group and see what else we can do with it so go ahead and I mean first thing I want to do is create a new Security Group just separately and I'll just call this uh RAM SG because it's going to be shared for that purpose as SG to show sharing with ram and we'll drop this down and we want to choose our 12.0.0 do6 for this I don't care about any of the rules I just really want to create a security grip for me to utilize when I launch an instance we'll go ahead and create our instance launch it I'm just say my Ram ec2 we'll choose Amazon 2 the architecture 64 bit is fine eligible tier sounds great to me I don't want to add any key pairs I don't care I want to select an existing Security Group which will be uh the one I created which apparently is not there because maybe I didn't create it for the correct Security Group very easy for that to happen so we'll go ahead and just delete that this we'll create this again my Ram SG my SG for my Ram shared VPC 12006 I did I did select that so it must be that um I have the wrong VPC selected here so I'm just looking for where the VPC selection is it's your under Network we'll edit it there's only one subnet so that's pretty straightforward and so now we can select our Ram uh at SG go to Advanced details here all looks fine we'll go ahead and launch this instance all right so this is going to launch and what I'm really interested is seeing if we can see that ec2 instance over here because we shared the VPC but we didn't necessarily share the um uh the security or the instance itself s so I'm just curious and you know what I did I did it in the wrong order so I meant to create it the other way around but that's okay because maybe this one can see it on this one so I I guess it doesn't really matter but this is not coming from the reference account it's going the other way so let's just see what we can see so if we go into here yeah we have the vbc we'll go over to ec2 sorry and so the question will be really can we see resources in that other account and we'll refresh that we'll go back over here and take a look and look and see uh how our instances are going so this is initializing so what we'll do is we'll wait for this to complete and then we'll see if it actually appears in the uh the original account okay all right so our instance is running and so we can see it of course in the account that we spun it up in but can we see it in the adjacent account and we cannot so just because we have a resource that is spun up doesn't necessarily mean that we can uh utilize it because I would imagine that we'd actually have to share that resource back the other way now the question is what would happen if we launched an ec2 instance from this one would it show up over here and again I don't think it will but let's go ahead and try anyway so we'll call this um and these are separate accounts but uh my Ram ec22 we'll just say original uh or or like original as in it's not the sharing account it's the account that is doing the sharing if that makes sense and so I'm going to scroll on down here and see what options I have so we have the network uh can I select an existing security group so that other one exists in the other location I guess I'll create in the default one for now we'll we'll launch that and we'll proceed without a key pair okay and let me launch and so we will see what happens here in just a moment I'm just going to double check making sure that's it's going into the correct VPC as it's very easy to accidentally put it in the wrong VPC is that the new one I created let's just double check because I don't want to go to default I want to go to the uh new one and this launched into the default VPC so this one is totally useless so I'm going to go ahead and terminate this instance very easy to make that mistake try this again so my Ram original ec2 we'll go down below here and we'll choose Network and we'll switch over to that one there I don't know if there is one so oh so now we can select it okay so the other one that wasn't created by us but by the other account is accessible here which is good wait a second wait a second I'm getting mixed up here real bad sorry so this is the account that actually created it this is the one that we shared it to which is the original one which doesn't have one so we actually have to launch an instance over here sorry my Ram ec2 original confusing I know I'm sorry and what we'll do is choose this this one here and we can see the other one okay that's good remember we didn't make in this one it's from the other one um the other thing is I don't want to have key pairs because I don't care about that right now so just say proceed without and we'll launch this instance it says the subnet belongs to different accounts specify security group that you own and try again so we couldn't do it because it is in a security group that we don't necessarily own so go back to editing this instance I going to create a new Security Group we'll just call it my Ram SG original here I don't care about any of the settings down below I just want to make sure that it's being created with this one here we'll go ahead and launch that instance and so the question will be will we see that resource in the other account or will it have to be explicitly shared my assumption is that it would have to be explicitly shared for us to see it so this one is launching I'm going to go back over to here and take a look okay we'll wait for it to be 100% running and then we'll make that determination okay all right so remember we launched it in our original account that's doing the sharing and the question is do we see it over here and we don't so just understand that just because we've shared our subnets does not necessarily mean we can see the resources between them I think that if we wanted to uh see them we'd have to individually share them so let's go take a look at Ram quickly and see if we could actually share that instance over to here so we'd see two on that side what I'm going to do is just to make things a little bit less confusing I'm going to go ahead and terminate this other one so we say terminate instance that is fine we'll go back over to here and we'll create a new resource just say Ram share instance it's possible that we could have added it into uh this group I don't do a lot of um sharing so it's possible that we could modify this and just add an instance to it so we'll say instance PC2 instance scroll on down here let's check this again so we've shared the subnet but we can't seat from the other side interesting okay just give me a moment okay see now I'm looking at the permissions for the subnet and it shows that we have permissions to start and describe sure we have like describe yeah we have described instances in here so for whatever reason we don't see the uh the ec2 instance but maybe that's a different story if we use the C okay so that that could be something so just go back here and let's just take a look at our ec2 and we do have an instance running here so we definitely can see an instance and this is launched in that shared VPC I believe so just carefully looking here for that VPC yeah it's in the it's in the shared VPC and so in this account we'll go back over to here give us a refresh and we don't see it so what I'm going to do is I'm going to open up um command prompt or command prompt terminal we're not on Windows here terminal and uh what we'll do is we'll just describe our instances and see if we can see anything so we'll say it E2 describe instances and see if we get back any results so we do get something that doesn't necessarily mean that's what we want uh cuz that's that's a terminated one here this one's terminated and this might be easier if it's a table so just say table and so what I'm looking for is I want to see if we can see the instance from the other other direction here okay because right now we have uh these two which is not very useful for us I'm going to just expand this I just click this button over here to open in a new tab and what I'm going to do is just carefully look at um this and see if we can see from the other direction we have down here my Ram ec2 this one says terminated the one we're looking for is called my Ram ec2 original another thing we could do is maybe we go chat GPT and say um because I it's just hard to look at that uh data but'll say um for adabs CLI produce a command that shows the name of that gets that that Returns the list of ec2 tag uh names only uh don't describe just give me the command okay so I just don't feel like figuring how to write that query because it's quite the headache there we go and hopefully that will help us see what there is in this list let's go back over to here says we lost the connection oh it's because it's in this tab and now it's over here it's getting mixed up we'll go ahead and paste that in here and hit enter and so we have my Ram eect original and so it's over here my Ram e to original so my my thought process here is that the reason we can't see it is that um I would think that it's because well first of all let's just rule out before I say anything else I just want to make sure that that other one's not a terminated one that just happens to have the same name but we'll go ahead and enter here and that's going to return the first one so this one says terminated Go Down based on the name my Ram E2 original so yeah so for whatever reason I can't seem to see it from the other direction I'm not sure why I would think that you would be able to if you launch something in one you be able to see in the other but there's something that is eluding me here so I'm you know again not exactly sure give me two more seconds to see if I can figure it out if not we'll just call it wraps here okay all right so I did a bit of research and the reason why we cannot see the uh instance is because we need would need a cross account roll so just because we are sharing resources and launching them a cross doesn't not necessarily mean that we can we have the uh the ability to have visibility to see those resources despite what we saw in the um the rules about like describing instances and things like that um and I guess that makes sense because if you had an explicit cross account role then I guess that' give you access to it um the main reason for sharing across um resources like having a centralized vbc is maybe you want the um billing to uh take effect in another account or other reasons like that but um anyway yeah it is what it is I was hoping that Ram was a little bit more impressive and that it kind of you know served the purpose of a cross account role without you having to configure one or it would configure one underneath but nope that's not the case but we do know how to share a VPC now so that's kind of cool we'll go ahead and we will terminate this instance we're just doing cleanup here and uh this is the count that I don't think that we have an instance in here but let's just make sure if there is one they're they're all terminated great we are sharing from this account so we'll go back over to RAM and we'll go ahead and tear this down there we go that's all it takes and we'll see you in the next one okay ciao network access controls also known as knackles act as a stateless virtual firewall at the subnet level and when we say that they're stateless this means that they have both allow and deny rules if that doesn't make sense right now don't worry as we will talk in more detail about stateless and stateful firewalls here shortly um when you uh create a VPC it will always create a default knle for you that's very important to remember knackles contain two different sets of rules they have both inbound rules and outbound rules um so inbound is ress so traffic that is entering the Knack and outbound rules is egress it's traffic exiting The Knack so there is an example here on the inbounds and outbounds tabs where you see rules with numbers we'll explain the numbers here shortly uh subnets are associated with knackles a subnet can only belong to a single Knack so there's also a tab for subnet Association um the key difference between knackles versus security groups is that knackles have both allow and deny rules where um security groups only have allow rules and with naacal you could block a single IP address and this is something you could not do with security groups U we will make emphasis of that last example because that is the best way for me to remember the difference between the two uh if we take a look here at the actual rules for uh knackles the first thing is you're going to have a rule number this is something that security groups do not have and only knackles do the number it will determine the order of evaluation from lowest to highest it's important to understand that that low is a higher priority than high the recommended way of working with these numbers is to work in either 10 or 100 increments the reason why is that if for whatever reason you needed a number in between you've given your room uh yourself some room to work with but you'll see a lot of times people doing tens or 100s and the highest number I I still think is that uh 32766 so you have a lot of room to work with you can set a type of traffic or set a custom protocol or Port range this is going to be the same as security groups you can either allow or deny traffic for this rule which security groups do not have security groups only have um allow rules so you wouldn't even specify it a use case here uh would be let's say we need to determine there is a malicious actor at a specific IP address that's trying to access our instance so we can block their IP address um so just carefully looking at this diagram the idea is that we can create a deny rule at a very specific um ipv4 address and we have a deny rule uh just specifically for Port 22 so they can't SSH which we are describing there so hopefully that is very clear as a clear example um if we're creating knackles and we're going to use the CLI as examples here the idea is that we would provide the VPC um we would have to create a a knle entry rule so very similar to what we saw in the UI screenshot where you are providing a cider block uh if it if the rules allow action its protocol the rule number the direction Ingress being uh inbound traffic uh we associate the knle to a subnet so that is going to be another thing that we're going to want to do but there you go that is knackles hey everybody it's Andrew Brown and this fall along I want to take a look at working with knackles uh they're not too difficult to work with but uh what we'll do is make our way over to VPC and what I want to do is I want to go ahead and um I want to maybe use create a new yeah we'll make a new VPC for this so create a new VPC I'm going to go all the way down the bottom make sure there's no VPC end points and I only want to work with one subnet to make my life a lot easier we'll have zero private subnets here we'll confirm what it is that we're creating that looks good enough to me we'll go ahead and name this knle example and we'll create our VPC and I don't care about uh providing feedback but we'll create that go view the VPC and there we have it so now what we can do is go ahead and create ourselves a knack I would like to do that via the um CLI so what I'm going to do is just go and open up uh our GitHub repo I'm going to go to a examples and I really just want to provide some documentation I don't really need to use the um I'm just going to use the CLI that's built in here and if I press period on my keyboard in this GitHub repo what it's going to do is open this up in .d is uh basically a vs code editor without any compute attach and that's going to allow us to update our docs just going to go and see if I can change the theme so it is a little bit easier on my eyes there we go and so in vpcs I'm going to make a new folder in here call it knackles knle I'm going to make a new file we'll just say uh readme.md because what I want to do is go ahead and create myself a knle and um I would like to do that via the a CLI so we'll type in a CLI knle and see what we get so maybe under here and I'll go to version two and we'll go to examples and so here we have one I'm just going to paste this in and I'll go to the right here sure if I can get Vim extensions in here so my life's a little bit easier thank you okay and so here this will just be don't don't install Vim unless you know how to use Vim okay it's just more for keyboard shortcuts and so here what I'll have is uh this command now we need to grab the VPC ID for our current or this one here um I'm going to go ahead and ask chat PT to grab it for us I'm trying not to just copy and paste and put anything in here so grab or uh using the a CLI grab us the a uh the VPC ID for the VPC that is named what was the name of this VPC Knack knal example VPC and one thing I didn't check while I was creating this was what is the I uh The Cider range but is that 10 so I think we're okay the 10 006 um but but anyway yeah that's the um we say get VPC ID and we'll go back over to here I'll just say named that using the a CLI do not just just provide the command don't describe as I don't need an explanation and this is just going to save us a bit of time we could write this by hand but you know it's just faster to grab it here're not learning a whole lot by writing it out by hand okay and so we have our filter we have our query and I think we could just actually do this as well um because there's only one thing returned so that should work as well we're saying filter for the tag name values of knle VPC so if I run this here open this up here I guess we didn't really have to do it because we just copy it in place I don't know why I was thinking I was making like a bass script for some darn reason it's probably because like last time I made a bass scrip so but it doesn't hurt doing this anyway none it didn't even work anyway we'll grab the vbc ID here whatever and uh we'll just not do that part there but I'm going just go ahead and replace this here of course you'll replace it with what you have just say create knle and it might be as simple as that so let's go ahead and try that out paste enter great so we have created ourself a knack um notice that it comes up with some entries so we have here egress true for zero deny forever everywhere and we could probably just look at this at the knle level here so if we go over to our uh knackles and we go down you can see that we already have some default knackles that we have the knle that we just created so we have the default one then we have this one here if we click into our default one we'll take a look at its inbound rules we have 100 all traffic all all all z0 allow and then uh deny for everything else then we have have outbound it's the same procedure so basically um what it's saying is like we're allowing traffic from everywhere and if we had any other uh catchalls then then they would be denied so what I want to do is launch up a um uh an ec2 instance within our VPC here so it would be really nice if we could just start having a script that we can continually use and I'm very tempted to do that so I'm going just make a new folder here called ec2 and I'm just call this um well I don't know since we are kind of working with VPC I just don't want to do that just yet so I'm going to go here I'm just going to make a new cloudformation template called template. yaml and what I want to do is launch up an ec2 instance let's say ec2 instance cloud formation and we'll scroll on down and they they probably will have some kind of example here for us here's a simple one and I'll just carefully look at this and configure it so we'll just paste that in we need to have resources up here um we need a little bit more than that for our template there should be something here like best practices or or working with templates template formats here we go so I want this top line here we'll grab actually these two and we'll go back over to here and we'll paste these in this will just be uh launch a simple ec2 uh for use with testing bpcs so we have uh a sdc2 instance we need to provide an image ID it's obviously going to be dependent on uh where you are so uh get me the Ami image for Amazon Linux 2 uh for CA Central 1 using the a CLI command okay we'll see if it can do that but the idea is that um when you want to get the Ami you would go over to ec2 and you're going to go and launch that ec2 and in here we're looking for in here this this thing right here okay the Ami value this is scope based on the region so if you go to a different region it will actually change even though it's the same image um because there could be slight variation based on compliance or something I'm not really sure but uh or maybe configuration in terms of like Default Time Zone and stuff on that machine but I'm hoping that we can just use something like this so we'll copy this code in here we'll go back over to um our console and I want to see if we can just grab that programmatically because that's really how we should grab it to be honest and it's just reconnecting here so I'll just hit q and then we'll paste that in here here we'll save paste and we'll hit enter so we're getting stuff back it's a little bit of a mess um so I'll just take out the text output on that okay so what does this thing even do let's go back and take a look here just say uh get Ami for Amazon Antics 2 and this thing is a mess but we will figure it out here so we got o owners it's all basically a filter and then we'll go here say see Central one okay um and I guess there's more than one so we are looking for a very particular one um I should have been more specific this one here is hvm 2023 so we just paste this in here let's just take a look so this one's saying the value is amz2 Ami HMV which is correct I mean this seems okay but why is there so many images then that's what I want to know so we'll go back over to here and uh I mean we have some quite a few here but which one actually matches the one that we want 0 b47 is that one even in here 04 whoops can I search 04 B Be and boy it doesn't even show up there this has always been kind of hard for me to do programmatically I'm not sure why even with chat gpg's help it I just never seem to ever get the results I want here but um and we have the query here so what I'll do just going to take this part out for a second bring this down onto a new line take this out for just a moment I think this is a valuable line for us to uh spend some time getting so let's just spend a little bit time here and we'll hit enter paste paste enter okay um so we are getting some images here and so I'm just carefully looking what it is it's X 8664 Linux Unix okay good so what the heck is the difference here I mean this one doesn't have a hyphen this one does maybe there's these are just different variants yeah I'm just carefully looking here just give me a second and I'll uh speed us through this okay anyway uh it's not helping too much but the thing is is that I think that all of these are technically um free tier and um I think they're pretty much all the same I think it's just we have ones with different dates and so I think we're just seeing ones that are newer or older than others so maybe we go back here cuz it couldn't do free tier but we'll go back here and just say get me the uh return only one with the late with the most recent creation date okay so maybe that will help a bit here because I don't need all of them I just want the one that is the latest and this might not be perfect to be honest but um we'll we'll try our best here okay where case we'll just copy and paste it but you know we should try this because this is how I would want to do it and this is how you should want to do it too programmatically so go here um and let's hope that we just get one back here so we'll go try this again paste it in paste enter I mean that's seems fine it's not the exact same one but uh maybe I could just see a little bit more more confidence that I see a bit more information on it just going to go here and paste it down and we'll just take out the query part no but that quer is how we get all the information about it right so maybe I'll just take off the image ID part of it there we go and then let's see if it'll just describe this one in particular paste enter enter I don't want the output to be text because I can't read it that way we'll say output table and so this creation date here 2024 very new um well I don't see anything wrong with using this one so yeah I guess that's fine we'll go back over here I just do want I just want the image ID to be honest so we'll just go here and do that um just say grab the latest Ami Amazon Linux 2 instance um sorry Ami ID well whatever you we know what it we know what it's doing so we'll go back over here and we'll just go back and paste in this command again and I'll hit enter so I'm going to use this as the actual one that I want to spin up um this here so we'll just grab that and we can put it in as a parameters we'll just say parameters here parameters I'll just say image ID and I'm going to say default and paste that in there then we can reference it down here so just say ref image ID um I don't want to download a key pair so I'm going to see if I can take that out I also don't want to specify the device block size so I'm going to try to take that out as well I'm hoping that's all I need to do but I need to specify what uh where this ec2 instance is going to go so in here we'll go back to the ec2 instance and let's just take a look at what's required so we'll go here and says uh I mean if we don't specify anything it will automatically go into the default um VPC so I'm just searching here for VPC just carefully looking if it isn't like that should be able to specify I don't know why this one's so hard um CL foration template to launch a a basic ec2 instance in a specific VPC I thought they'd be like here's your VPC but maybe we have to choose a subnet or something else it is a subnet ID okay okay so we go back here subnet it's right here uh the idea of the subnet to launch the instance in so we'll go ahead and copy this ref subnet ID subnet ID I'm just default it to a value we can go back over to here and we'll go to subnets and we only have a single subnet for it and that one was 10 Etc just going to go ahead and refresh this because it's not appearing in our list there we go and its ID here is this so we'll go back over here and paste that in so now we have an image ID and a subnet ID um I mean Security Group would probably be good as well Security Group we have two options Security Group IDs the name of the security group uh for non default vpcs you must use Security Group ID so we'll go ahead and copy this one okay and we'll go down here and the idea is that we would provide that ID but we need to actually create ourselves a security group so that's what we'll do next I'll just uh go to new tab here we'll just say Security Group uh cloud formation I know a lot of work just for knackles but you know again this is stuff that you would generally have to do so let's do everything and and get cross domain skills here uh that's an example of a security group so we'll go ahead and grab that and we'll paste that on in here so we'll say uh SG we'll keep nice and simple and then the idea is that I want to be able to reference this Security Group it might be the case that um it might not return that by default so we'll have to double check here when you pass logical ID you're returning the resource ID and then down here the group ID for the specified group is that going to be the same I'm not sure it's not really indicating that so I'm going to go back here and say get ATT and then I'm going to say SG do um group ID and then here we're supposed to specify the VPC so that's really interesting so we'll go have to go back up here and then our parameters we'll just say uh VPC ID and I'll go back here and just say VPC VPC ID have to go grab that as well so go back to your vpcs and we'll grab this value we'll say default I think we also have to say type up here so say string you think I know writing so much Cloud information but when you're not doing something for two seconds you're going to forget so we'll do that this has Ingress egress to everywhere that's totally fine um says front Port two Port which is fine um I kind of would prefer it to go everywhere so I'm just thinking about this for a second let me think okay all right um I mean like I'd like to keep these wide open so that I can just do whatever I need to do so let's go up to uh the syntax over here erress from Port if the protocol CCP UTP is the start of the port range oh just tell me how to do it um security group that allows any inbound or outbound traffic as a template okay so I just don't know how to do ranges I can't remember how to do them so I don't want to spend forever looking for it so let's just get Chach PD to help us out here be back here when it's done generating okay all right so we can just use negative one for everywhere really okay if it works it works I'm I'm fine with that so let's go ahead and copy that did not know that and uh did it say that in the docs use negative one to specify all protocols okay all right looks weird but apparently it's okay the reason I want to keep it open is that we don't want to have the the security group restricting Port access when we're trying to test restrictions on Port access on the knle and so that's the reason why um I'm doing that but anyway so we have our SGS getting reference there I think that that we kind of have everything um we didn't choose the size of the instance that's kind of important so there'd be instance type right so we'll go here and we'll say T2 micro we can go up the top here and just say instance type as well type string default T2 micro that should still be free tier and uh I mean that looks like everything that we probably need so we have this template and what we can do there's a lot of ways to launch templates um and you'll see us do it in a variety of ways but today I'm going to keep it nice and simple and we're going to make our way over to cloud formation we'll just upload the template directly because that is something that we can do right so what I'm going to do here is create a new one a new standard one and I'll need to download this template so I'm going to go back over to here I'm going to download this template to my computer all right and just give me a moment to go get it all right and so over here we can bring in that template so template is ready upload from a file I think I can drag it onto that area if I drag it right here yeah I can and hopefully it works we'll go next it says the security group ID's resource def uh definition is malformed so what that's saying is that um where we reference this it doesn't like it which to me it looks fine so what does it not like you know what it's supposed to be indented under the properties here like this that's probably why we'll save that again and we'll download it again I'll delete the old one before I download this one you know again I'm just showing you variations in different ways so in sometimes we will take a cloudformation template and use the CLI and all sorts of ways invalid template property called SG the reason why is that we didn't indent this under the resources so it doesn't know what it is so we'll save that again you get really good at spotting your problems if you keep working with this stuff over and over again so even though I make mistakes I can easily spot them we'll go ahead and hit next uh unresolve resource dependency image ID in the resource block of the template image ID so maybe there's a spelling mistake so we'll go back over to here and I think that there is we'll just put that as a lowercase ID we really should be consistent with these so I'm just going to lowercase that I thought it wasn't case sensitive so I'm actually surprised that even matters um unless it was saying something else here so yeah I'm not 100% sure what the problem is here but we'll go ahead and try this again we'll download it again and we'll go back over to here we'll try to upload the template we will close this we'll hit next and we'll just say my ec2 you can see we have some parameters that will be filled in we'll hit next we will go ahead and attempt to create this and we'll wait for that to create something that we should probably have done is we probably should have created an IM roll because if we don't create an IM roll we're not going to have a means to log into the uh to the resource so I'm going to go here and just say uh in cloud formation launch an ec2 instance that has an IM Ro that grants the uc2 instance access to use sessions manager and I have code somewhere for this but we're going to just have it generate this out and we'll evaluate if that's any good but I just realized this template's going to be kind of useless because we need to actually log into the instance to see if the internet is blocked or not right otherwise there's no point and we are getting I roll it looks like it's the permissions are way more than I expect it to be that doesn't look right please use a managed roll instead of creating your own it should not be doing that and this is what I mean like you can't just trust uh chat BT to to make stuff because it'll just dump out a bunch of junk if it ever decides to stop uh talking you can stop talking now I can't hit the stop button because if I do it it won't remember the context of what it's written there we go please don't describe anything just give me the code okay so that that's the one I want adabs SS manag instance core now what's important to know that some of them are old and some of them are new so I'm going to go and just double check that over in IM am and by the way this uh this one we we're running here it did create but I'm going to go ahead and delete it because it's kind of useless to us right now and I want to go to roles and I want to see what that is so let's just type in SSM as there are uh there's rules in here or policies I should say that uh there's old ones and new ones and you only want to use the new ones so if I go here we have Amazon ec2 roll for SSM it doesn't say this one is old let's go back over to here to chat GPT and see what it says so is this one here let take a look uh this one is try this again SSM so this one's the deprecated one and it says to use that one okay so it did use the new one which is good and so all I really wanted was this part here okay so we'll go back over to our template and then if we go here again we can grab this and we don't again need a key name because we're going to use sessions manager to log into this instance um this is supposed to be under our actual E2 instance let go back here and this looks okay to me great so what we'll do is we'll go ahead and download this download and we'll go back over to cloud formation and we're going to go ahead and launch this so I'm just going to drag this on back on over I I download it more than once I just don't want to get confused and launch an old one so I'm just going to download it one more time here you can't see it but uh it's just some old conflicts and I'll drag this over here we'll go ahead and hit next and we'll just say uh ec2 instance ec2 for because whatever we name this is going to be before knle so knle ec2 knle ec2 example so we're going to want to make sure we name that that uh will help us later on go ahead and hit next and we'll have to acknowledge that so it has IM capabilities when you create instance profiles it even said that in there you have to checkbox that so we'll give that a little bit of time well that was fast the role with the name Amazon SS manag score cannot be found are you kidding me it's right here SSM let's go back over to here it is right there you're crazy let's click click into that that's its Arn over there we'll go back over to our cloud formation template here it wants the rules so let's go look up this one since I don't seem to trust that AWS CFN and we'll go go here and uh here we have rolles so it says the name of the RO to associate with instant profile only one rle can be assigned to ac2 instance at a time oh the r okay sorry so we have a policy that's called that but we don't necessarily have a role so we'd actually have to create the RO let's go back over here and take a look so maybe we're missing something from our chat GPT code so here we have this Ro but it's not showing that Ro go back here roles all right so we got to go back here and say this should use a this should create a new rule called which uses the managed policy called and that's uh I think it my instructions weren't the best because I told it to use a manag rule but it can't because there is no managed role it can use a manage can use a managed policy and so we'll have to create our our our Ro here there we go that's what I wanted so we'll go back to wherever that is here it is and we'll just click above here as such or paste in here above and then we can just reference that here there we go so now we should be in good shape all right so I'll delete the old template off my uh my downloads I'll download this new one we'll go back over here if you don't like this kind of levels of repetition you're not going to like Cloud cuz this is 100% how cloud is every day we'll go here and upload this template oops there we go and we'll say next uh this is ec2 knle ec2 test we'll go ahead and hit next and next and we'll checkbox this and we'll go submit and so now it should launch probably what would have been really good is if we um set up Amazon or an Apachi server um using user data so that's something that I think would have been useful so launch or install an Apache server via user data uh okay let's see if it'll just add that that to the instance all right I again just give me the code for the CN the CFN don't don't explain I have it somewhere again I'm not going to go dig it up when it's so easy to grab this way there we go there's that user data and and that yeah that looks correct to me so that's all I really want is this part here and we'll go back over to here and we'll just go down here and paste that in so the thing is is that a user data can either take a gamble file or it can take a like a very specialized gamble file or a Bas script and so the idea is that via yum it'll install httpd and I guess that is the main doing it because there's aachi 2 or there's hcbd I mean that looks right to me so that should work but um this instance is launching and that's okay like if this launches we can launch it again once it's done so we'll wait for this to finish and uh if it it's successful then we'll go launch this again uh with the user data script okay all right so that's deployed there let's go take a look at our ec2 instance um we just go over here and take a look and there's our instance it's running so that's really good um and let's just make sure that we can actually get into that instance it'd be nice if it was named so that we know exactly which instance is ours want to connect this to connect to an instance sessions manager it requires an IM profile you can create an instance profile and assign it to your instance um didn't we do that if we go here here we like we stopped and relaunched it because it didn't have it and so we do have an IM roll here so why is it giving us that message can I connect to it why not we don't have a public IP address oh okay why not um maybe we need to specify that when we launch the ec2 instance I thought it would Auto assign one it might matter like for our VPC because um we created our VPC but I'm not sure if it auto assigns for that subnet let's just go take a look so if we go into our VPC here and then into here and into our resource map and we click into our subnet we'll open this up um we'll go to Route tables here we sorry I'm looking for auto assign and so Auto assigns actually turned off so if we go and change that edit subnet settings okay so by default non non default subnets have an ipv4 public address attribute set to false I guess this kind of makes sense because if ipv4 addresses cost money every time you launch one up you'd be encouraging people to have spend and so I guess the idea here is that if it doesn't assign an ipv4 then that would be less of an issue okay and I guess it would probably be good if we used IPv6 because there would be no cost to us I'm just not very good with IPv6 to be honest um but I guess during this refresh I should get a lot better for everybody here so let's go back to our ec2 for a moment because I just want to know if it's Auto assigning anything to it it and that's that's where we're running into an issue here because if this is using ipv4 and let's say we wanted to use IPv6 we'd have to then convert our VPC into an IPv6 one so I think for the time being we'll just turn we'll just turn this into an ipv4 one and just understand there is a cost with ipv4 I can't avoid it anymore um there's probably some kind of free tier I'm not really sure but um just like ipv4 costs because there's um there's a new cost C now with awsa so it says uh here will be charged 0000 five% per IP per hour for all public IP addresses attached so if we had an IP address for 730 hours that's $3.65 okay so you know understand that there's that and I can't really work around that if I want to teach you Cloud so we'll go here and we'll want to have an ipv4 app address assigned so somewhere in here we should be able to set that um somewhere so I just type in ipv4 doesn't say where assign an ipv4 address uh public address to the instance there we go and um oh it's down under network interfaces so if we have a network interface is that what we have to do to to do that I guess I'm just so used to it automatically assigning it that I would never normally have to set up a network interface but I mean if that works that's really good but wouldn't this already have a network card so if we go down here to networking it has a network interface and so I guess my thought is like are we just enabling it on this one yeah because notice this doesn't have a public ib4 address if we click in it here I I'm not I don't want to do it here because we need to do in our cloudformation template but could we assign that address there I mean that's what I think that that that it's suggesting that we can do so what I'm going to do here is I'm going to go I know it gets more and more complex as we go but whatever this is how it is um so I'm going to assume that it's going this is going to be the default device that's attached and here I guess we need to specify the subnet again subnet ID and I'm not sure why we'd have to set the group set on this is that really the the easiest way to to set it because that's I thought there'd be like a simple flag for that let's just carefully look for network interface then if you use this property to point to network interface you must terminate the original interface before attaching a new one to allow the update to succeed that's fine um what's this group set the ID of the security groups of the network interface oh okay that's pretty straightforward um I thought that was something else so we'll go here and just say get ATT sg. group ID it's interesting like we set it here but we also set it there so I'm surprised like I'd be surprised I would think that you could admit those and have them uh here elsewhere but the question is is this going to spin up another uh eni or are we just going to have a single eii that's what we're going to find out here I'm go ahead and delete my template and uh it was kind of suggesting that if we're going to fiddle with um the nic's that or the Enis that it has to tear down the old one and make a new one I'm going to hope that it does that in the cloudformation template we're going to go ahead and download this we're going to go back to um here and what we'll do is actually update our stack and so we'll replace the current template by by uploading our file drag it over and we'll go next uh every ref must have a single string value doesn't it where does it not oh right here I made a mistake there we go um and so I'll just download that again we'll go back over to here I'll just give this a refresh because I don't trust AWS all the time and we'll click update we'll replace the current template we'll go over here and we'll do that okay and we have an error so it says network interface an instance uh level subnet ID must not be specified on the same request so that's kind of like what I was wondering so I'm thinking that maybe we just comment these out like this because why would you need them twice and that was kind of what I brought up there a second ago so we'll go ahead and delete it download again we'll go over here we'll go back uh we'll give this a refresh and we'll update our stack again we'll replace the current template we'll upload again we'll drag this on over we'll hit next we'll hit next we'll go all the way down the bottom we'll hit next we'll acknowledge good we'll submit and maybe this time we'll have more luck all this just to test out knackles and all we're going to do with knackles is just make sure that when we um utilize the knle that we can block a spe our specific IP address on a specific Port so that we can show that it's working because um knackles have the ability to block whereas security groups do not and I'll be back here when this is done or if it fails okay all right so that looks like it updated we're going to make our way back over to uh ec2 and what we're really interested in is that attached instance again this is just because it was had made that change and so we had to do a bit extra work here so we go over to networking and we go down below um sorry let's refresh this because it might be looking at older dat I really don't trust this page and we'll try this again and we'll go back over to networking and we'll scroll on down and so we do have R Andi here and we do have a public IP address now okay great so that was really important because if we don't have that I don't think we can connect to this so now notice we have no connection issue so we'll go ahead and connect it we really needed that to work because otherwise we would have to spin up a VPC endpoint or G something VPC endpoint yeah and those cost money we don't want to do that so this will be pseudo Su hyphen ec2 user because the default user for Amazon look 2 is zc2 and by default you're not that user I'll just show you if you type in who am I you don't want to be the SSM user you want to be the ec2 user so you're going to say pseudo suyen space ec2 user and now we're logged in as that user we type in who am I so that's who we are now this is supposed to be running a server I'm not sure if we have curl installed or or wget we do have WG so we'll type in Local Host uh Port 80 and uh notice it downloaded index HTML page so that means that that uh that Apachi server is running if we wanted to make sure it was running we could type in psox grep maybe Apachi um and there it is it's running here htpd so we can see that it is currently running if we used pseudo system CTL status htpd it could probably tell us if it's running and it is surprised I remember that command off the top of my head um but anyway the web server is running and if what we want to do is see uh it working from the outside we'll grab that that IP address and we'll just paste it in somewhere here okay and so this is resolving and we have this so now the idea is that knackles operate the subnet level and we should be able to adjust our knle to have a rule that's going to block for our IP address um so what I want to do is get my IP address I'm going to say what is my IP address what's my IP address and we'll go here and it says that I'm at 17441 1083 and there's an IPv6 and what I want to do is update that knack to specifically block on that so back over in our script here we've created our knle but let's go add a rule so what I want to do is say we'll just grab this here I'll go over chat gbt um add a rule to my knle that blocks inbound traffic from this IP address for ads using the ad CLI don't explain don't explain just show the code okay and let's see if it can figure out that big mess of what I wrote and that looks pretty much right so we'll grab that we'll go back over to our document say uh add entry we'll take a look at what we have and now we know negative one means from anywhere let's see if we can make sense of this so we need our Knack a ACL ID here this is going to be for Ingress it's going to have a uh a rule number of 100 so the thing is the lower the number the higher priority and so I'm thinking that we should set this as 90 cuz we should do increments of 10 and this is saying from anywhere any protocol so either TCP or UDP and it's blocking the entire range of ports not just ad but all ports and um it's not allowing uh from here and for 32 is a way of specifying an individual IP address and we know that because if we go over to cider and we were to put in 32 you'll notice it says one okay so we'll go back over to here and we'll bring this up and um we'll type in we'll bring in knle and we'll paste that in and we'll copy this and what we'll do is we'll go back over to here and we will add our rule programmatically we'll paste it in we'll say paste enter enter we'll go check our knle make sure that that has been set type in knackles here oh we're already under knackles um so if we click into this one here this is our new one and this one isn't um associated with any specific we'll have to refresh this it's not we'll have to hit enter to do that um inbound rule oh did we do it on the default one we did okay I didn't want to add it to this one so we'll go ahead and remove that and we'll go back and we'll try it on the one that we actually wanted to do it on it was this one here so I meant to grab this one I mean it would still work with the default one but I don't want to fiddle with the default one I want to uh apply to this one here so we'll paste that here like that we'll go ahead and copy this and we'll try this again we'll type in clear just to make things a little bit easier we'll paste this in we'll say paste we'll hit enter and so now if we check our knackles inbound I have to give it a hard refresh because it's uh UI is the worst and we'll click on this and go inbound and so there's our rule we have a deny 90 now the other interesting thing is that it actually denies on everything so I thought this one would would have had a a um an inbound of of 100 and it doesn't so it just looks like uh because when we created a new one it just doesn't have that so we would have to add that in ourselves I'll just manually add it in we don't have to do everything programmatically but we should do our best when we can here 100 all traffic I'll just say allow okay because that's basic basically what it would be like and same thing here we'll just say add a new rule and we'll just say 100 all traffic allow so that kind of looks like more of a normal Knack and what we need to do is we need to associate um the specific subnets with this one so we'll take this one here and so now uh this this one is associated of here if you go down here it's no longer associated with the default one and we could leave the default one alone because then we don't have to fiddle with it so if we go back over to here and we refresh it should never resolve because we've blocked it right so it should just keep moving here and it should eventually time out so um I just want to prove that it's going to time out so I'm just going to pause the video and can come back here when it's done and if yours is hanging you don't have to wait forever okay all right and so notice that it's saying the site took too long to respond it's because we blocked it and so if we go back into our inbound rule here and we edit our Rule and we remove this rule we save it and we go back and we refresh it now resolves it's taking a little bit of time but it will resolve okay reload come on you can do it uh we'll go back here let's take take a look here yeah that's this should be fine and down go back back here give it a refresh well we did remove the rule let's just refresh that maybe we didn't no no we did inbound Force destination and you know if you never trust it like I don't always trust it here but if you're having issues you can always open another browser to find out so just give me a moment all right it's not resolving here either I'm a bit confused um because we added that rule now it just decided not to work so just give me a moment all right so what I did is I just uh double clicked into here and I noticed it was actually trying to do https so I changed it back to that and now it's it's doing that so just to make sure this wasn't a fluke I'm going to go back and add this rule in again we'll add a new rule we'll call it 90 we'll say on all traffic and we will deny and so the idea is well really we just want to do that single IP so what's my IP address uh so actually you know what sometimes you can drop down and choose your no you can't here and so I'm just going to grab this again okay I'm going to try this again here for sl32 we'll go refresh this and so now it should not work notice it's not resolving excellent and if we go back to our inbound rules and we edit and we remove the rule we save it and we refresh now it instantly works okay so it's working as expected and that's all I really wanted to show you for knackles it can also work for outbound traffic it's a little bit harder because it's hard to say like let's block the IP address of um Google um like let's say we try to download a page there or we'd have to launch up another ec2 instance in another subnet and that that's where it gets to be a lot more work so I think that this is good enough and it satisfies us testing knackles hopefully you didn't mind the huge deter but we did learn a lot and um I think it was valuable to go off the path there a bit I'm going to commit these files and just say um VPC knle here and we'll just commit I'll make our way back over to here I'm going to delete this and um I don't want to be running this instance anymore so it is shutting that one down so that's good so once that's done we'll then tear down the rest so just give it a moment okay all right so what we'll do is go ahead and type in VPC and uh from here what we'll do is go ahead and delete the VPC and so that should delete yeah all the knackles that we have associated with it looks like it anyway if it doesn't we should just double check see if we have any extra internet gateways I don't know why we keep ending up with some extra igws but let's just delete them sometimes delete them sometimes they don't I don't I don't know who cares and uh we'll make our way over to knackles nope just one knle okay great so we are in good shape and I will see you in the next one all right ciaoo security groups often referred to as SGS act as a stateless virtual firewall at the instance level the key difference between knackles and security groups is that security groups are at the instance level and they are stateful uh if you were to go into an ec2 instance you could see a security group often associated with one I'm not even sure that you can even launch one without a security group I don't think that's an option you will see other services like um RDS instances or fargate containers have security groups understand that underneath they are utilizing ec2 instances there are virtual machines even if they're running containers and so um understand that that is kind of the relationship that is going on here and the other thing that I don't really list here but it's really about the the uh the network cards the um uh I forget what they call them in ads even though we have slides for them but uh the Knicks um they're the ones that have to be really associated with security groups but uh yeah hopefully that makes sense um just like knackles they have outbound inbound rules so nothing exciting there SC groups can contain multiple instances at different subnet levels so SC groups are not bound by subnets but they are bound by vpcs and so are knackles knackles don't span um multiple um they don't span multiple vpcs okay if we're comparing Security Group rules against knackles it looks pretty much the same the key difference here is is that there is no option to say allow or deny because we'll learn here in a second here that SEC groups always only have allow rules when you're creating them the other thing you're going to notice is that there is no numbering system to set the Precedence of rules because it's not needed because there's only allow rules uh you can choose a preset traffic type this is just going to fill in the protocol and Port range for very common uh configurations for the Destination type we have uh quite a few things we can do here we have an ipv4 cider block an IP V6 cider block another security group or a manage prefix list I've actually never used this one before and so I ended up making extra slides just so we could cover this particular interesting use case there are only allow rules for SEC security groups there are no deny rules all traffic is blocked by default let's take a look at some use cases the first is allowing IP addresses and of course we can do ipv4 and IPv6 if you want to specify a very specific IP address you'd have the 4/32 on the end of the IP address IP uh IP address to specify a range of one IP address if it's IPv6 you're going to be using for1 128 you can allow another Security Group to have access to another Security Group this is a very common uh thing you're going to see oh I have a pen I never I never use the pointer but um the idea here is that if you have a database you may want to only allow uh the web application access and that's a lot better than saying IP addresses because you could be adding tons of compute instances that have to have access there and so it makes more sense to allow a security group um so hopefully that one is clear there we can have nested security groups I don't do this very often but the idea is that if something is in the same Security Group it's going to automatically be able to talk to each other doesn't have to go through allow and deny rules because they're in the same one but what you could do is if you wanted additional security around your compute let's say we could give it a security group and this one then has specific rules for um traffic that is coming inbound from the internet so there are um some interesting configurations the last one of course being the netive one being the most interesting if we are using the CLI to create the security groups uh it's as simple as this we're going to specify the V VPC ID we create that then we need to add a rule um to the security group and then we need to associate the ec2 instance to the security group to uh complete that process there there are some limits that you should know about security groups you can have up to 10,000 security groups in a region the default is 2,500 but through a service limit we can increase that you can have 60 inbound rules and 60 upbound rules per Security Group I don't believe there is a service limit limit to increase that you can have 16 security groups per elastic network interface the Enis I couldn't remember the name earlier but theis is again the most important thing to remember um the default is a five apparently security grips do not filter traffic uh destined to and from the following so if it is from DNS it's not going to filter if it's DHCP if it's from ec2 instance metadata if it's from an ECS task metadata endpoint uh if it's a license activation for uh Windows instance it's not going to if it's from Amazon time sync service it's not going to if it's a reserved IP address used by the default VPC route it's not going to so that is security groups a nutshell and there you go hey this is Andre Brown and this fall along we're going to take a look at security groups and we don't really have to touch too much on these at this point because later on many times over we're going to be fiddling with security groups so getting the basics is just what is most important here um and we did lay some ground workor in those knackles so I'd like to reuse some of that code so I'm going back to our repo over here and I'm going to hit period to open up we can uh do a little bit of coding not too much but you know get our stuff in place but we're definitely going to be reutilizing that template that we produced uh in the last video for knackles so in here I'm just going to make a new folder we're going to call it SG for security group and we're going to make a new file and I'm just going to call this one readme.md and the idea is that we're going to want to create a security group um and so what I want to do here is just create a security group that is going to I don't know just just create a security group so we'll say to chat GPT create a security group for a specific um well actually you know what I don't think we need to even do that because in our template prior here we actually do create a security group and we can tweak it as such so maybe what would be better is if we copy this template we'll make a new file here and we'll call this um template. yaml and I really don't have much to put here for now but the idea that I'm thinking is that uh what we can do is kind of just play around with this and adjust the requirements so that we can whether we want to block or allow uh traffic I should say allow because that's the only thing that security groups can do is uh create rules that will um add additional rules let's go ahead and uh get this ready to go so we don't have our VPC from before so what we'll need to do is create ourselves a new VPC so I'm going to make my way over to VPC and I'm in CA Central 1 and it does matter because the Ami is tied to the region so if you're in different region you need need a different um Ami code so we'll say VPC and more I'm going to have one a so we have one public subnet no private subnet we'll go down here and take off the VPC end point this looks good to me I call this one SG project good we'll go down below create the VPC we'll wait just a moment for that to create doesn't take too long we'll view our VPC and so now now what we can do is copy this VPC id address and I'm going to drop this into here the other thing I'm going to need is the subnet ID so we'll go back over uh to here and I'm looking for our new subnet this one is um give this a refresh here it is down below so we'll just checkbox into that so we can get it from the copy paste and I'll paste in that subnet ID here and so we should all be set up and as per usual we'll go ahead and download this so off screen here I'm just getting my downloads folder in place and we'll make our way over to cloud formation again in other videos I show you how to create cloudformation deployment scripts but we're just keeping it simple here and having some variation so yeah we'll go ahead and create a new stack I'm going to upload a template file I'm going to drag the one over that we had right there not trying to open it just trying to drag it it's getting confused my double it keeps double clicking by accident that's why it's doing that we'll go ahead and hit next and we'll call this one um uh SG example we'll go ahead and hit next and we'll go down below and hit next and we will go ahead and say I acknowledge and we'll submit and I'll see you back here in a moment when this deploy is done or if it fails and it looks like we've encountered a failure so I'll go over here and take a look uh into uh any of our parameters it should tell us somewhere usually under events failed the SSM roll SG roll back requested user uh this does does not exist so it's suggesting that I did not copy the VPC ID which I'm pretty sure that I did but if we did not let's go back here and double check so we'll go into here over into VPC and we'll go to VPC here and this is our new one so I'm going to go ahead and copy that bpc ID again maybe I it cut off when I copied and pasted it I'm not sure and we'll go ahead and paste that in definitely definitely does exist the other thought is like am I deploying this in the correct region so if I go back over here yeah I am the VPC value does not exist 03181 and we'll go back over here 03 no so it's using uh an older template so I it's probably because I never actually downloaded it so we'll go ahead here and just hit download that's the reason why gotta be really careful when you're doing this stuff when you create a stack for the first time and it fails then you have to delete it if it's a stack that you have created successfully and updated you don't have to delete it what we'll do is try this again so we'll go ahead and say create create new this darn button there we go just banging on my mouse here to fix that issue but we'll drag this in here hit next we'll say SG example go all the way down below hit next next acknowledge submit okay so that's going to go ahead and launch that um instance and hopefully it'll work this time so I'll see you back in just a moment okay all right so this uh ec2 instance is ready and the idea is that if we go over to ec2 we should have that uh Apachi server running so let's go see if that is the case remember that we are using ipv4 so it is utilizing up an ipv4 address which does have spend so we don't want to keep this up for very long I'm going to go up here and just paste in that IP address and there it is so that's working as per usual so no problems there um you know but the thing is is that we do have this working but we want to see if we can uh change our security group rules to do something different so something very simple would be to only allow access for particular areas so let's go ahead here and adjust our security group rule so we'll go ahead and just click into our security group and we'll edit the inbound rules and what we'll do is we'll just first remove our rule here and see if we can save it it seems that we have to um provide it for a particular area so if we go my IP that will grab our current IP and what I'm going to do is just change it to something El like SSH because the idea is that if we switch it to SSH this should not work because SSH is on Port 22 so notice that it's going to hang okay so notice it's hanging which makes sense we go back to our inbound uh our inbound rule and we change this over to http which will set it to Port 80 as TCP we save it and we go back here and refresh it should no longer hang might have to double click just make sure that it's not using https because we're not using that protocol and notice that it instantly loads so this is definitely working we'll click back into our inbound rules I just want to point out that you have um other options here so notice when I click into this we have our CER blocks but then there's security groups and there's those prefix lists so if we want to restrict on these we can absolutely do that when we get into something more complex like having a um an ec2 instance that talks an RDS instance this is where we would reference another security group or if we have a load balancer I'm going to leave that for later because we already did knackles for quite a bit of time and um there was a lot of set up there we don't want to make things too complex here right now but anyway that is security groups in a nutshell so let's go back over to um cloud formation and what we'll do is we'll just terminate script so go here and it was SG example which is the old one so we go ahead and delete that okay and so that should delete without issue there and uh we are done okay see you in the next one ciao all right let's compare stateful versus stateless and I'm going to tell you this is something I always forget you know I'm 10 years in with adus at least and I always get these backwards and I have no idea why I think it's cuz I'm from a developer background so this it networking stuff is not top of mine so if you get it backwards don't feel bad about it but let's do our best to make it really clear so we can have the best chance of remember this when we sit the exams so stateless firewalls like a snackles are not aware of the state of the request so so that's why they're called stateless they don't know anything about it so in both directions they'll treat you like a stranger and stop you both ways to do a rule check now I I use the word stop to say they will check to whether allow or deny that's not a block that's just a stop so understand that I have this nice little graphic and we have ourselves an ec2 instance and this ec2 instance wants to go talk out to the Internet so it's going to make a request and we'll call this request a so we can track the progress of the request and it's originating from the server that's the easy to instance the virtual machine over here and it's going to go over to The Knack and The Knack is going to say stop we need to decide whether you're allowed to uh like leave or not allow or deny and so it either passes that rule or it doesn't but let's imagine that it it is allowed to go out now on the internet side it's going to send a response it knows that the origin is still the server it's over over here on the right hand side okay but it's uh coming back the other way okay it's coming back with the response from that that specific request and the knle is going to say stop we don't know who you are and we're like well we're from the originating server they're like we don't care we're going to check you anyone coming or leaving is going to get checked so that's the example there now imagine uh a request is coming from the internet and it's trying to reach the ec2 instance it's going to get stopped and then the ec2 instance is going to send back something to the uh to that request as a response and it's going to get stopped so it just stops all directions so hopefully that is crystal clear and that's what makes it stateless let's take a look at staple firewalls like adab us security groups they are aware of the state of the requests for SGS they allow all outbound requests and and responses for the requests that originated outbound are allowed back through so I know that was a lot to say there but we'll we'll break it down here and understand that different virtual firewalls work differently so don't look at this and think all staple firewalls work this way it's just the way uh Security Group firewalls work okay so the idea here is very similar we have request a so request a is going to be the one in yellow here so this is originating from the server and it's going to go out and as we said all outbound requests are allowed so it's going to go and go out this way right out to the internet and the internet's going to return a response and the response is allowed back because it remembers you know that the response came from the ec2 instance so it's allowed to go out all right so this one always if it goes out it gets to come back in all right now let's look at the other direction so we have request B coming from the internet just going to uh clean up here this one here has to get stopped and it says are you allowed or not to enter uh enter through the security group if it is allowed then the response automatically is allowed to go back out okay so hopefully that is very clear um and you know that's the best I can do there but again understand that scur groups are uh the way they handle it is that particular thing and if if you want to know this little icon here actually doesn't exist I made it because security groups don't have an icon but I wanted to make a good representation so if you carefully look at the icon look at the knle the knle has these two lines and it has these arrows pointing in and so it's saying that they're going to stop them both ways and then the one I made down here shows it only checks on entering uh for this one specific use case Okay so there you go uh let's take a look at uh no that's it yep see you in the next one rout tables are used to determine where Network traffic is directed so here is an example of the architectural diagram and we're representing the route table and a lot of these VPC um uh diagrams a lot of times the route table will be left out and the router and often times if the route table is in the diagram the router is just not present in the diagram but I'm showing it to you so you know there is something there uh the router is not something that you have direct access to so that's why it's never shown because there's no need to but I just want you to know that there obviously is some kind of component because of Route table needs a router to Route things right um often not often actually but the the initialization for Route tables is RT and I'd say I rarely if ever see that used but I have seen it before so I'm I'm listing it there just in case a route table contains routes you can notice that we have a destination and a Target so those are two things we'll talk about here shortly um the most important thing is that route tables uh are related to subnets so each Subnet in your VPC must be implicitly or explicitly associated with a route table a subnet can only uh be associated with one route table at a time but you can associate M multiple uh subnets with the same route table so route table is really associated with subnets here and that's how you're going to tell what to get access outbound or or go somewhere else we have the main route table this is the default route table created alongside your vbc which cannot be deleted a subnet that is not explicitly associated with the route table will use the main route table this is good but can also be bad if you have a resource that you don't want to go out to the internet which the main route table will be associated with you can see which is your main route table in the list of Route tables because it'll be a um column called Main and have one called yes so here is a diagram of the main route table being used and it's going out to an internet gateway and it's associated with some public subnets the default public subnets that are created with the um that are part of the default VPC in your area there are use cases for having custom route tables uh a great use case would be if you need to um pick very specific subnets uh and they are going to go out to very specific things so like a very common use case would be out to VPN or something that's not an internet gateway because it's a private connection or there's a security uh secur security through party Appliance or something so just understand that the use case for custom route tables really comes into play when you're doing things other than an internet gateway um if that makes sense for Destination this is where the route will go here you can specify the IPv6 cider block or the manag prefix list so if we were going out to the internet for ipv4 it would look like that if we're going out to the internet for IPv6 it would look like that if you wanted to specify a manage prefix list which I believe would mean any uh this would allow for any anything to go to a Dynamo DB um uh table that is in Us East one in any account not just yours that's with man's pref prefix list do uh that's something that you could do there the other thing I want to cover is targets so earlier in the slides I'm just going to go back here for a second but we talked about or we said that there's destination and Target this is a bit confusing confusing because I kind of wish it was like source and destination but I'm sure there's a reason for it um the networking folks can tell us why but let's cover what targets can be used because it is important to know the targets not all of them for the exam but it just gives you a better understanding of the utility of Route tables when you can see them all here and it was a bit of work to round them up for you here because they don't make it straightforward in the docs to get them all so Target will have a local this is the default local route you cannot delete this route it will always be present this is a this is so that your subnets can route to things if you didn't have a local route your subnets would not connect to any anything else and so we have to have um a local Target you have your internet gateway um so this is for Ingress and egress traffic for ipv4 and IPv6 you have a virtual private gway this is for uh private connections to on on premise Network you have knate Knack gateways this is for egress connections for private instances out to the internet for ipv4 specifically ipv4 the reason it's not for IPv6 is because IPv6 doesn't need um Network address translation which we'll repeat many times in this course because IPv6 um addresses are public whereas ipv4 are not generally known as the public so uh Network add translations required for that we have an egress only internet gateway so this is interesting because IPv6 all the addresses are technically public they actually have to have this special um way of if you have private instances to go out to the internet but not allow uh Public Access and so we have this egress only internet gateway which we talk about at some point in this course you can specify a specific instance um that is allowed to send traffic out via Route so a very specific ec2 instance or a network interface an eni uh for a specific elastic network interface you have carrier gateways this is out to anabis partner Telecom carrier networks via inabus wavel like this is what I have to assume because um carrier gateways is associated with adabs wavelength which is for those Telecom carrier Networks we have core Network this is out to a manag wide area networking so that's w w and this is specifically via iTab us Cloud Wan we have Gateway load balancer endpoints um this is out of a Gateway load balancer a g wlb and gwbs are for thirdparty virtual appliances this is when you want to bring your own uh virtual Appliance um or maybe it's probably like a list of very specific ones but the idea is for for additional security uh stuff in your network we have Outpost local Gateway so this is out uh out from a I keep saying to an but it's actually from a Outpost a physical server rack with an service in your own data center a peering connection so out to um a another VPC and then we have Transit Gateway so T J tgw out to a Transit hub for connecting multiple vpcs and on premise Network now the reason I really want to list these over is uh in this list here is because I want to repeat this stuff again and again and again because there's so many little weird networking services and any opportunity I can repeat it when it's being used somewhere I'm hoping that you'll remember what the stuff is you'll definitely see a lot of the stuff uh when we cover gateways but yeah that is Route tables so there you go hey this is angre brown in this fall along we want to take a closer look at Route tables and a really good example for Route tables when we uh when we need to set up something uh that is a private or secure connection um but for this video I just want to Simply look at Route tables with you so you can see the options that are possible to set um we don't have to do anything super complex here we'll leave that for other videos but anyway we'll go up to the top here we'll create ourselves a new rate Tate route table and so I'm going to just create another one off of my um um my default one I actually do have this other VPC in the last video which I forgot to clean up but they do not cost anything so it's not a big deal so I'm just going to clean this one up right now and I'm just going to go over to internet gateway and clean that up if there are any detached ones there it's not so we'll go back over to our route tables so what I want to do is go ahead and create a new route table I'll just say my custom route table and then from here we can go ahead and select uh our default VPC and we'll create that route table so now we have a route table and uh of course you cannot switch which is the main route table it's just you have a route table here and we do have a few options but uh what's really important is about subnet Association so if you want to you can explicitly associate specific subnets with a route table and that that ensures that anything launched into those will always go to this one I'm just going to take that off because I don't necessarily want to do that I was just doing that uh to show you if we go into our routes you'll notice that we have a local route you cannot delete the local route it will not let you do that just to uh show to you what we'll do is just give this a refresh so that that comes back I don't remember this's preview button being here so it's interesting that that's there but what we probably would want to do is R out to an internet gateway could be an example so here's an internet gateway let's click that preview and see what it shows us it shows us what it will create so that's okay but just make note of how much stuff there is that we can choose and a lot of these things are going to be for hybrid or more complex setups um but uh you know we need to do um private IPv6 then we'll be using egress only internet gateway so again we'll get more experience with route tables but I just wanted to show you how to you create a rot table and some of the options there and I think that satisfies it so we'll go ahead and just delete this rot table and I'll see you in the next one okay ciao hey this is Angie Brown and we are going to cover what is a Gateway because there is a lot of things called gateways in ad and so I figured it would be very important to kind of uh glue this knowledge Al together before we jump into all the types of gateways so what is a gateway a Gateway in the context of cloud services is a networking service which sits between two different networks it's a Gateway imagine a door opening and closing right it's a gate so gateways often act as reverse proxies firewalls or load balancers and so if you see a load balancer it is a gateway um and vice versa so just understand that these terms are interchangeable let's go over all the types of gateways that I could find on adus there's probably more but um some are very specific networking components for VPC some are gateways in name in terms of service but they technically are gateways and uh in general and other things are gateways for very specific services but are not part of VPC so let's cover them the first is internet gateway also known as igw this is for inbound and outbound public uh traffic for ipv4 and IPv6 so it's a a pretty good one uh and you'll be definitely using this there's egress only internet gateway this is for outbound private traffic for IPv6 the reason why we need this is because the only way we can have um a private subnet ensure that it doesn't have public internet is is to use this method because IPv6 addresses by default are public so the only way to prevent them is basically to block Public Access in this uh this special way with this uh this egress only internet gateway we have carrier Gateway this is for connecting to adus partner Telecom networks this is specifically for adus wavelength we have knat gateways these are outbound private traffic for ipv4 because ipv4 not all addresses are known you have to do some kind of translation and map them um so that they can uh internally route to to private networks it's the only way that they can work there are other Gat stuff in ABS so before natat gateways there were natat instances and there are gats that you can launch up from the um the adus store uh so there are a few options there we have virtual private Gateway this is the endpoint into an ad account from a VPN connection so this is for private secure connections or we say secure uh private connections but not necessarily secure uh you need another layer for that we have customer Gateway this is the endpoint into your on premise account for a VPN connection so when you're doing vpns or other things you're going to often use a customer Gateway you'll use it probably for more than one thing but you have customer Gateway and virtual private Gateway go hand inand they're used together quite often we have Gateway load balancer G wlb I've never used this but I understand what it is um it is a layer three Network layer load bouncer to run and scale third part virtual applications such as firewalls or IDs and IPS if you've heard of guard Duty for adab us imagine you wanted to use a third party one that had more controls this is what you'd have to use so that anything that's going to reach your uh uh ec2 instances are going to pass through this very particular product that you prefer to use and uh I'm I'm going to assume that there's a Marketplace for them or maybe there's a way to hook up custom ones if we do actually look at this more detail we'll we'll know when we get to that point we have direct connect Gateway this is an endpoint connection to a fiber optic connection to a cocation data center we have inabus backup Gateway this is a very specific Gateway for the Abus Backup Service you have iot device Gateway this is a very specific gateway to um iot data for the iot services for adus you have adus Transit Gateway this is a very broad service that gives you a hub and spoke model to uh to I say simply here but it should be to simplify y to simplify VPC peering it's an alternate way of doing VPC pairing that is much easier if you have a lot of vpcs you have Amazon API Gateway this abstracts API endpoints to Services we have adab storage Gateway this is syncing caching or extending local storage to cloud storage so this is a particular service and it has products underneath but they're all gateways some cloud services call their gateways load balancers and vice versa they're technically all gateways the the the difference between load balancers and gateways can can also mean the direction so one is a reverse proxy right the other On's not um but uh you know just understand broadly what gateways are they gatekeep data or like the flow of traffic between two networks okay so there you go all right let's take a look here at internet gateway also known as igw which allows both inbound and outbound traffic to your VPC operating on both ipv4 and IPv6 so here is our diagram where we have our igw and what's important to know is what we need in the route table so even if we have an igw that's associated with our VPC we have to have these very particular routes so the idea is that you need to have one for ipv4 and IPv6 and the target is going to be for the igw um these you see here the 0.0.0.0 sl0 and the colon colon for you should remember because we had a slide on it dedicated to it these are the catchall routes this is what you want when you want to say allow anything um or anything and everywhere okay we always use the uh these with igw to get out to the internet you'll always see those no matter what uh features of igw it works uh for both IPv6 and ipv4 addresses it provides a Target in your VPC route tables for internet routable traffic it performs Network address translation Nats for instances that have been assigned public ipv4 addresses a Nat is not needed for IPv6 addresses because IPv6 addresses are all public default vpcs come with an igw for nondefault vpcs you must manually create your igw and Associate it and create your route tables it is that simple there's not much more to say about igws there you go hey this is Andrew Brown and this fall along we're going to take a look here at internet gateway uh we've already kind of looked at it in other videos but we will just take a simple closer look and we will have opportunity to play around with uh internet gateway a little bit more um but let's just go ahead and set one up so what I'm going to do is go over to VPC so get there however you need to get there I'm going to type it in the top and from here I'm going to go ahead and create myself a new internet gateway notice that there's already one attached to this one so if I create one my igw um I'm not going to be able to attach it to the other one because it's already attached to an existing vbc so notice that I'd have to create a new vbc so we'll go ahead and do that um I'm just going to create the VPC in this case and just say my VPC because I don't need all that other stuff and we'll just do 10.0.0.0 for6 and we can select IPv6 if we would like to so we haven't been doing that as of yet but maybe we'll do that in a future video and so now we have our vbc if we go to our internet gateway we can attach the internet gateway so this is our new one we'll go ahead and attach it we'll select that one there um I'm not sure why it has this here but uh I guess it'd be nice if you wanted to know the exact command so that's fine as well that's something you see more about uh in Google they'll give you quick commands and it's interesting that adus was showing that there but that is now attached what's really important is um the route out to the internet gateway so we have a VPC and we have internet gateway but we might not necessar they have a subnet uh for that new table so I'm going to give this a refresh so notice we don't have a subnet for our new VPC cuz we manually created it so let's just say my um oh sorry subnet I meant to say route table so if we go over here give this a refresh yeah we did get a default one main um yep okay so we can go ahead and edit this one and so we'll edit the routes notice there isn't a route out to the Internet so we would have to go add internet gateway this would be for 0000 but the thing is that we' probably also want this for IPv6 in the future so you'd set it to colon colon for size 006 okay and that would be what you'd want to do so that was pretty straightforward there's not much else to talk about right now about that so we'll go ahead and delete this and we'll see if it actually deletes all the stuff so it'll delete the igw as well it appears so that's good we'll go ahead and type delete sometimes it doesn't delete it I don't know why so I just double check here yeah there you go ciao let us take a look here at ESS Only Internet gateways um it's not official but I'm going to assume that it's EO igw um I think I might have seen it a couple times but um that's that's I think a good way to distinguish it also this service does not have an icon I created this one but I think it best represents what's going on here which is the idea that we don't want to allow um Public Access it's only private access so things should not be able to uh from the internet come into the uh into things so anyway let's read the description here so uh the ew are speci specifically for IPv6 when you want to allow outbound traffic to the internet prevent inbound traffic from the internet I think my icon best represents that so the idea is that You' still have to add your route so you want a route out to the internet for igws so that should be very clear IPv6 addresses are all public so they don't require Network address translation however igws would not restrict the inbound traffic so the EO igw will ensure your instances are private by D by denying inbound traffic and again it's as simple as that hey this is Angie Brown and this fall along we're going to be doing some IPv6 by setting up an egress only internet gateway I'm just tell you right now that I don't do a whole lot of IPv6 and specifically AWS because AWS does not have a great support for IPv6 at the time of this video I do want to get you some exposure to IPv6 but it took me so much work to figure this out and I had to pull my cofounder to get something working um and so I do have instructions it's on our platform but I'm just going to be honest with you I'm going to be blasting through this stuff by placing these commands in and I want to focus on the things that you have have to set uh for this stuff to work now one interesting thing is that um uh sessions manager does not natively support IPv6 so in order to utilize sessions manager we'll have to do natat translation from ipv uh 6 to ipv4 and so we will have to launch a Nat Gateway if you're worried about costs just follow uh follow or watch the video uh if you're not too worried about uh spending a few pennies or a dollar um then go ahead and and follow along but what I'm going to do is go ahead and launch up up um uh itus Cloud shell and I'm just going to verbatim follow these instructions because I swear to you it's really easy to mck this one up okay so the first thing that we want to do I'm going close this tab out here is we're going to want to uh bring in the cider block but also if you said Andrew why didn't you put this in the repo I don't know um it's on our website and that's free as well so if you need to get to just sign up and get to the free access to get these instructions but uh again I just want you to watch and see what I'm doing here so the first thing we're going to do is we're going to set up a um a new vbc and notice that we're saying Amazon provided ipvc 6 cider block so we're telling it as we create it to set it up for IPv6 uh we could easily do this via the wizard as well so if we were to create the VPC um we have this option to use the Amazon provided IPv6 cider block so that's all we are doing here we'll go ahead and hit enter and so now we have our iv6 cider block the next thing we're going to need uh need to do is is um is wait for that value to be assigned so if you look here notice it says associating so it's not 100% done right off the bat so we'll go back over to here we'll give it a refresh and now notice that we have our cider our cider IPv6 so that is good the next thing we're going to need to do is create a subnet um because we just created a VPC there's nothing else here so I'm going to go ahead and grab the next command uh that Bo has here and what we need to do is provide the VPC ID so that is going to be this value here good I'll paste that in and then the next thing is going to be this iv6 cider block so I'm not sure if it's for 64 but we'll grab whatever it is this one says 56 so the reason he's he says 64 is because he's choosing a smaller uh cider block than the whole thing you wouldn't obviously want to make it 64 you want to make your subnets not use fully fully everything so I'll just switch this out to be 64 um and this first one for whatever reason is going to oh no sorry so this one is going to create a subnet but notice that it's also going to set a setting an IPv6 cider block but also a regular cider block so it we're going to have to support both ipv4 to IPv6 because that's going to be the only way that we can get into the instance and be able to check to make sure that egress only is working uh it would be nice to do an IPv6 only setup but it's just very hard to do that so we'll go ahead and just uh take take that off there the next thing we're going to need to do is enable dns64 so we'll type in a ec2 modify subnet attribute subnet ID and then we'll provide the new subnet ID right here and so what that's going to do by enabling uh dns64 is that it will allow us to do that natat translation from um IPv6 to ipv4 uh okay and that's very important um so we'll go ahead and type in ads ec2 create egress only internet gateway as we are ready to create that internet gateway one thing I like about the wizard uh is that it will actually ask you to associate it at the time of creation it does so here as well so we don't have to have attach as a separate step um it looks like I typed egress in correctly so I'll go ahead and just type in egress only here and so now we've created our internet uh egress only internet gateway we are also going to need a regular uh internet gateway so I'm going to go ahead and do that but this one will actually have to be attached so we'll we'll grab the igw here and we'll grab the VPC here and so now we have attached our internet gateway we're going to also need an elastic IP I think I might have not uh deallocated one from prior very bad habit I I apologize for anybody that uh gets caught caught off guard with these these things here but uh it's hard to stay on top of them but we we'll use the um the CLI to create one so we'll go ahead and allocate an address um the next thing we need to do is create our Gat Gateway so we'll go ahead and do that so so we will need to provide our subnet this subnet needs to be in I guess the one subnet that we created so I'm just going to scroll up here till I can find that subnet really wish we didn't have that problem there and so this subnet here it's ID is what it's this here it's also up here as well scroll all the way back down to the bottom I'm going to paste that in I need the allocation ID which is right here and we'll go ahead and B I didn't hit enter and so now we should have launched our net Gateway let's go take a look and see if that actually is the case so we go over to ec2 instances there might be a little bit more that we need to do to get the KN gateway to go because um I've done gateways before and you have to do a little bit more we do it in this course right so if we go here and we go into that Gateway the idea is that um maybe click into it remember there being a bit more that we had to set up Maybe not maybe that's all it was I'm thinking maybe like um oh the routing yes okay so it is the next step that uh Bo has here and we'll go ahead and do that so we'll just go ahead and open that in a separate page it's a lot less of a headache if it's separate here um so the next thing is we need to uh create our route table so we'll go ahead and do that I've already lost what the VPC is so I'll go back over here I can't tell which is the new one so I'll just say this is our uh IPv6 one so I'm not confused go ahead and grab that VPC ID we'll paste that on in there and so now we have created our route table we'll need to create some routes and so the route that we're going to create here is for this route table so we'll have to go ahead and grab that route table ID it notice that the destination is colon colon for. Zer that that means it'll go to anywhere and that is the IPv6 version notice that it says egress only internet gateway so it's going to R out to egress only internet gateway I'm assuming we're going to also have to do this for um our regular internet gateway here in just a moment but we'll go over to here and we'll grab that and we'll say enter okay and then we have our next route so create an ipv4 outbound service route to use the natat so we'll paste this in and we still have the same route table here and I'm not sure if that is correct so 64 col and FF 9B wouldn't it be based on whatever we assign so I'm not exactly sure what to set for this so what I'm going to do because I I don't know what to put there for The ipv Cider block destination I'm going to just manually create it via the route table so I'm not confused we have three route tables we just created one which is this one here so just say new new like new route table new RT so I'm not confused um and we go into here this one has zero out to the egress only one this one has that there associate the route table the new subnet so I'm going to associate I'm just doing this out of order but I'm going to associate the subnets because this makes more sense to me and I'm going to edit the route and then what I need to add here is um create an ipv4 out only service bound to natat so I'm going to assume that we need to do natat Gateway and then this one is going to be says 96 I don't understand why this is this here give me back give me a second I'll be back in a moment all right so I just called Bo who uh wrote the instructions he's my cofounder and better at networking and so what he explained to me was that this special address is specifically for ipv uh to be able to route to to ipv4 uh so like asking chat gbt here it says refers to the IPv6 address block that is used from IPv6 to ip4 address translation also known as ipv4 embedded IPv6 addresses okay that explains why when I did this it never worked but uh what we'll do is we'll go back over to um uh to my instructions here and I guess what Bo had was actually okay so I'm going to go and uh paste this in again and just stick with what he had um but yeah it looked funny to me because i' had never seen that before kid not a networking person a developer person over here and I guess we could just create it here as well um I feel like just doing that here actually to be honest and we'll go select that Nat so now we have that natat route um we need a route out to the internet 4 internet gateway so I'll edit the route here and we'll add another one this will be for 0000 and we'll say internet gateway and we'll go ahead and drop this down and we'll create it um and so the thing is is that yes internet gateway allows for routing for ipv4 to IPv6 but just notice that we aren't assigning a colon colon forze zero to the internet gateway so basically all IPv6 traffic is going to utilize that egress only internet gateway the next thing we need to do is to um set up sessions manager so well we already have that as a roll I'm not going to go ahead and make a new one but I will show you what it looks like just in case for whatever reason you're skipping through videos and you have no idea how to create that role but the idea is that you're creating a role with a very particular policy and the policy is very easy to find because you just type in SSM and this is the old one I believe or maybe it's not but um the one that you should use is called core core SSM manag instance core and so this is the role that you should use that's going to allow you to get access to sessions manager all right but anyway um I guess we're ready to launch up a new ec2 instance I'm just again checking through the instructions here of what Boo has but he's launching up a T3 micro with the um uh Amazon Linux 2023 I can tell because I've seen this so many time for C Central and he's attaching that instance profile and we have a network interface card with IPv6 and we're providing the subnet so I'm going to do this through the UI okay and we'll go ahead and choose instance here and we will launch a new instance we'll just say uh IPv6 um server and so we're choosing Amazon 2023 I'm going to switch over to T3 micro because T2 micros are so old and I do not want a key pair I'm going to change this to be our new VPC I'm going to launch in our only single subnet I'm going to let it assign an IPv6 address I'm also going to let it assign an ipv4 address because that seems like a good idea for me he's launching with the default uh Security Group here so if he's not choosing Security Group I'm just going to utilize the the default one as well there is some network configuration going on here so we did choose IPv6 and so my assumption is that because we um because we chose IPv6 we should not necessarily have to do anything additional here say device zero IPv6 one subnet so yeah I don't think that I need to do that I don't want to fiddle with these so I'll just collapse that and leave that alone and what we'll do is go ahead and launch this instance so I think for me what the reason I just couldn't get this to work was that I just didn't have a way to get in and I didn't think about using that Gateway uh like that address translation which was what B figured out here but let's see if this launches up and if we can actually get into the instance and if we can then let's see if we can actually ping out and get our IPv6 address I'm very confident that the ipv4 will part will work but will the IPv6 part work I don't know all right so let's see if we can get into that instance now so let's go ahead and try that we'll go ahead and connect um we'll give this a hard refresh because I do not trust the screen okay well now this doesn't have a public IP address so darn it it it doesn't have one I'm going to have to go ahead and create an elastic IP so I'm going to go ahead and create one and we'll say allocate and I'm going to have to go back to here give this a refresh here and I'm going to need to grab the one that's not associated here and Associate to here and I'll all associate that I'm going to go back over to our ec2 instances is I'm going to have to restart the private server because or the IPv6 server and the reason why that I need to do that is that if it tried to go talk out to the sessions manager agent it can fail and then it just will stop trying to reach sessions manager so by giving it a reboot we'll give it a better chance to establish a connection we give us a nice hard refresh here and uh nothing yet we'll go back to our instance give this a refresh here and see if it has an ipv4 address it does public ipv4 address come on any day now um we'll try this again give this a nice hard Refresh on the uc2 instance I don't always TR trust it so I'm going to go over to systems manager and if you go to like uh your fleet manager under here then it's a little bit better we have to configure this and so usually that it would show up here so that's not going too well let's go back over to E2 instance here let's take a look um I have a public IP address it's public it has a private ipv address it has private ipv4 I could have swore I told it to Auto assign an ipv4 on launch but maybe I did not so I'm going to go ahead and just tear this down and try one more time so let say launch instance IPv6 address 2 and we will go down here to T3 micro and I'm going to choose no key pair I'm going to go to edit I'm going to choose our other um VPC I'm going to Auto assign an IP address for both I could have swore I selected that we'll drop drop this down we'll say defaults uh we'll leave Advanced networking alone we'll drop this down we'll choose the IM roll to be SSM yeah the SSM rule from earlier and I think that's everything that we need I'm going to go ahead and launch this instance all right and I'm going to wait for it to become available okay all right so let's take a look at that instance and uh see if it actually has addresses so I'm going to refresh this I'm still not noticing a DNS um so maybe that is not turned on uh we did create our stuff here I mean I don't know if it really needs DNS to be turned on but it does have a public IP address and it does have a private IP address I'm going to go ahead and try to connect this again and now it decides to work for whatever reason um so that is good so do pseudo hyphen Su ec2 user so remember this does not have an outbound connection via ipv4 it's only IPv6 so if we type in curl ifconfig domme which is a website that returns back your IP address we get IPv6 and so we have accomplished ESS only internet gateway IPv6 which is great so we are done here let's go ahead and terminate the instance we'll go over to Nat Gateway and get rid of that now so we'll go here and terminate our Nat Gateway we'll wait here a little while and then we'll get rid of our other things um we can get rid of our elastic IPS while we're waiting so I'm just going to say release we got to wait so we'll wait a little bit okay all right so I've been waiting a while I didn't check if those things were uh uh gone but I'm going to go ahead and see if I can release so I can release one why can I release this one release um will not longer be associated with your account you can no longer do that that's totally fine we'll go ahead and do that so those are both gone we'll make our way over to our VPC and we'll delete our VPC and I will see you in the next one ciao so elastic IPS in ads are addresses that are just simply static ip4 addresses that means that you'll always have the same address no matter what and so the idea is that we have a few different use cases the first use case will be when you restart an ec2 instance and its IP address will change this happens every time you restart it you might end up with a new IP address or if let's say you have a workload and you stop it or so you terminate you spin up a new one it will absolutely have a new IP address this could be an issue if you have something that relies on the on the hardcoded IP address um it will break so you know most systems try to stay away from uh relying on IP addresses but when you're integrating with things externally from ad or you are working with Legacy systems sometimes there's no way around it okay uh for use case number two when an ec2 instance fails you might want to use the same IP address uh in the case of a failover instance so you have more than one instance and you just want that IP address to carry over so elastic IP address allows you to do that because you can remap it to another instance or a network interface card elastic IPS are region specific so you can't use them across regions elastic IPS are drawn from the Amazon Pool of ip4 addresses so as long as they have one you can get one elastic IPS are charged a dollar for each that is allocated and unassociated because they want you to use them not to just um allocate them and never use them so they incentivize that um incentivize you returning them back to the pool when unused by providing uh by charging for it elastic IPS are subject to the new um ipv4 charge which is for any public IP address that you're using using this is the whole thing about there's a limited amount of ipv4 addresses so they're kind of at a premium and you should try to use IPv6 where you can U assuming that they they work as expected elastic IPS can be Associated or unassociated to an EC ec2 instance or an eni elastic network interface card um which really in theory they're both just going to the eni because the ec2 instance has a network attached card um to it and they're always going and pointing to that but understand that you can point it to either the Nick or the E2 instance if you notice that I keep using different words it's because Ani is a Nick and it is a network attached card so there's a lot of ways that we can say that the same thing 10 times over so I try to utilize more language there so you're in better shape to understand that um you might be wondering well this is for ipv4 what do we do for IPv6 well IPv6 doesn't really require elastic IP because addresses are already globally unique um is there static IP addresses for IPv6 I guess so but anyway my research pointed that we don't need them uh for ipv IPv6 and so this is really more about ipv4 so elastic IP does not support IPv6 for obvious reasons let's go take a look at some CLI commands so we can just better understand the workflow when working with these elastic uh IPS the first is allocating your uh your EIP um there is one option there where you can specify the network board group where you can you can pick very specific azs local zones or wavelength zones I didn't even know that so this is something that doesn't show up in the UI but you can really narrow down exactly um what kind of uh Network you're trying to Target so I guess it works outside of just your plain old vpcs we have uh we have to associate our elastic IP address so we need to associate the elastic IP to an instance you could also use the network interface ID to associate to a network interface and now thinking about it I'm not even sure if the UI allows you to directly associate to a network interface card so maybe the only way to do it is through thisi command I'm not sure we can disassociate our eips that's pretty straightforward and then we can deallocate or release our um our EIP back to the pool there's a little bit more so this is where we get kind of into like more interesting things that you can do with elastic IPS but I never ever do them because they're more exotic but you could have a reassociate ation so you can tell an address to always attempt to reassociate with the same instance or network interface in the case of a failure or restart so that is an example of having that failover example um you can tell it to explicitly not to reassociate which I'm not sure why that's an option but anyway that is an explicit option you can have because I think by default it already says not to do it um so there's that you can recover so I didn't know this I only found it out through the CLI but you can attempt to if available recover a specific address during allocation so if you had like your favorite address that you used before you could try to get it back or maybe you uh someone on your team accidentally put it back in the pool and you want to get it back because things are hardcoded so as long as no one else has taken it you can probably get it back if you're lucky uh you can also bring your own ipv4 address I this is something I really didn't know until again going through the CLI but you can allocate one from a pool so there is some way to import a public ibv Port pool uh ipv4 pool into uh as a separate step um so that's really interesting but uh yeah so there you go that's elastic IP hey this is Andie Brown this fall along we're going to take a look at elastic IPS this stuff is extremely straightforward the idea is we're just going to allocate an ipv4 um static IP address and assign it to an ec2 instance so the first thing we're going to need to do is launch up a new ec2 instance I know we've created a lot of templates and other scripts what I want to do here is just go ahead and launch one uh using the console so that we get a little bit of console experience Management console to be specific and so here we'll use Amazon Linux 2 we're going to scroll on down we have T2 micro I'm going to switch to T3 micro since it seems to give me uh less trouble especially in the ca Central One region and um I don't need a key pair so that is totally fine and the other thing here is I want to have a security group and I want this Security Group to have internet access we checkbox htps and HTTP I really only need HTTP here we're going to go down over to Advanced details and what I want to do here is change the user data script so somewhere here ah here it is user data and the idea is that we can run a bass script over here and um if we go into our VPC and we look at our Basics example no it wasn't that one uh knackles maybe there it is in our template yaml we created ourselves this user data script so if we copy this and bring it on over here we can just place that in there I'm just going to bring this to the wall so that uh it's a little bit easier for us to work with the idea is that this should run that script and install our website site so looks good to me again just double triple checking that everything is against the wall here um we don't need to say it's B 64 encoded because I think it will it'll do that as it launches up but this looks like it's in good shape so let's go ahead and launch that instance and um I don't want to uh keep pair we don't need to log in this should just work and launch up if it doesn't we'll just tear it down and try again I don't think we named it so we'll just go up here and just say my EIP example just so we're not confused in terms of what this is doing go down here say launch instance and so now it is launching the instance and I'll see you back here when this is done running we just have to wait a little bit shouldn't take too long okay okay so let's give this a refresh and see if the status checks have passed and hopefully our website just works we're going to go ahead and copy the ip4 address and paste it into our browser and we're going to wait here and there we can see our actual address so the idea is that if we create an elastic IP which we can do here in the ec2 console then we can attach it uh to our ec2 instance so here says allocate elastic IP for ipv4 for C Central one that looks good to me we're going to go ahead and allocate it apparently there are Global stat IP addresses I didn't even know that I guess if you're using the global accelerator there's a high chance you probably have to use that anyway but anyway we have our ipv4 address so the idea is that we want to go ahead and Associate this IP address so I'm going to drop down choose our running instance and down below allow the elastic IP address to be reassociated specify whether the elastic IP address would be reassociated with a different resource if it's already in use so we could say yes so that it could be uh reass Associated notice we could either select the network interface or the instance which underneath is going to select that network interface anyway but it seems like there is a slight difference so if you associate the EIP with the instance um the previously EIP address will be disassociated so it might be a better idea to associate it with the uh instance in this case we're going to look at that record looks like it's already Associated so the idea is if we grab this IP address paste it on up here our website should work the same way and it does so that's perfect let's go ahead and disassociate this we'll do that it's now disassociated we'll refresh our website should hang because the idea is that it's not going anywhere and it's not going to resolve so that is the exact Behavior we expect we're going to double click this make sure it's just HTTP it will resolve faster if we do that sometimes if it's htps and you don't have htps support it'll just spin forever but anyway um we know that this is never going to resolve because it's no longer there and so that's all we had to show for our EIP let's make sure that we um release it because if we do not release it we're going to have issues notice there was some options to uh transfer so maybe that's case if you need to transfer between uh different instances we're going to go back over to instances we are going to go ahead and terminate our instance so there you go we'll see you in the next one ciao just making sure it's shutting down hey it's Andrew Brown and we are taking a look at manage prefix lists because I kept seeing this over and over again I'm like what is this thing and now I know what it is so I'm going to share it with you um it is a collection of cider blocks to make it easier to configure route taes and security groups so adus provides by default in each Reg a set of manag prefix lists you can use and you can uh if you click into one you will see VAR specific uh IP addresses with CER ranges a prefix list can either be a collection of ipv4 or IPv6 cannot be a mix of the two you can create your own prefix list with a collection of cider blocks uh an inabus owned prefix list will be updated by ads automatically and this makes sense because if a list contains all of the dynamodb instances in CA Central 1 and they're changing them then they should maintain them of course so some inab US own manage prefixes list only appear in specific regions so cloudfront cloudfront for example is a global Service but it its Origins default is in Us East one so you'll only see cloudfront in Us East one because when I was in C Central 1 I wasn't seeing that service uh prefix list and I knew it existed same thing with S3 EX it's currently only available in Us East one so it doesn't appear anywhere else so there is the ones that I'm talking about that you can only see in USC to one um a managed prefix list uh could be used to only allow cloudfront distributions to Origins to have inbound access to security so the idea here is that when you assign a source for an inbound or outbound rule you can select a prefix supplying its prefix list ID to specify the manage prefix list and then the idea here is that only cloudfront is going to be able uh to transverse that security group based on that setting I need to point out that if you choose to use an adus own manage prefix list you're including the cider block ranges for all adus accounts so in this use case you would be allowing from any adus customers cloudfront distribution origin but it is a little bit more secure than say the entire public so how you want to use this is up to you um but yeah that's as simple as it is there you go in this video let's go take a look at manage prefix lists so um I believe this would be under VPC if it's not it'll be under ec2 and we'll take a look and see if we can find it so here it is on the left hand side manage prefix list and you'll notice that there is a bunch of different prefix lists that are produced by eight of us already so we have bpc latus health checks ground station dynamodb S3 and the idea is that um by allowing uh these we can allow traffic uh to uh flow between these if we are using security groups I'm sure there's other use cases for managed prefix lists which I probably detail in the lecture content but where it's going to actually be something that is actionable right away is probably going to be in Us East one because there are very specific manage prefix lists um in this one that we could utilize so if we go over here for a moment you're going to notice that we have uh ones for and I'm just looking for it we should see cloudfront so if we wanted to only allow stuff from cloudfront we could utilize that so let's go take a look at what that would look like in terms of Security Group we're not going to do a fullblown example because it's just not worth our time but I just want to show you we could say only allow um Cloud front and yeah we'll just have it for this VPC here and the idea is that we can go ahead and from this list choose existing prefix list so notice it's showing everything from us to one um I don't know if you could use this across in different regions that might be something that might be interesting to test because what if you wanted to allow cloudfront from another origin so we'll go ahead and create this and we'll just put the same description here Okay so we've created that we give this a nice refresh and oh we set it for the outbound rule it would have made more sense to set it up for the inbound rule but here we say the destination is any of those outbound addresses what I want to do is just to a quick sanity check um before we do anything else I'm going to just edit the outbound rule I want to copy this value here because I want to see what if we can access that in uh a cross region I don't think we could but um it be interesting to just give it a test and find out so I'm going to go ahead back over to ca Central 1 I'm going to go and create myself a new security group and call my um cross account um prefix list and we'll go down below here and I want to see if I can paste that in and so it looks like I I can uh if I clear this out does it actually provide the one it does so so even though that list exists in Us East one it is accessible here in uh other regions so that is very useful I think that's enough of a demonstration to show how manage prefix lists work you can make your own I don't have much of a use case for making my own or showing showing you those ones but the idea is that if you wanted to I suppose we could take a look really quickly you can make your own manage prefix list and then from here you would specify ipv4 addresses so my prefix list and we say my prefix list and I'm just seeing yeah we'll say ipv4 oh Max entries the number um number of entries we'll say five and the idea is I guess you'd have to enter them in right so you say this one that one again I don't do this very often so I'm just putting in whatever just to show that we can make our own but you know if you had a bunch of uh specific IP addresses you could put those in there that you trust um that might be for legacy systems that you want to uh quickly use but uh yeah that's pretty much prefix in a nutshell nothing too complicated but we will see you in the next one okay ciao IPv6 was developed to provide a solution for the eventual exhaustion of an IP V4 address and iTab Services Provide support for IPv6 but it configuration access will vary per service if you've not seen what an IPv6 address looks like it looks like that you can see it's really long and it serves that purpose to make sure that there is an endless supply of addresses a service will be configured for either and we're talking in the context of IB IPv6 support but it either have support specifically for a dedicated uh a dedicated way for iv6 only or it will be dual stack that doesn't mean that there aren't endpoints for ipv4 only and then dual stack or then IPv6 I'm just talking about the IPv6 support um additional to what we normally have a service endpoint is the way to access IPv6 and so we'll either have public endpoint support and or P private endpoint support So note that all enable Services support ipv4 not all services or their resources have IPv6 turned on by default so they might be some work that you'll need to do here okay so something that I think is very important to know about IPv6 is what does the migration path look like when we are utilizing a VPC uh a VPC that only is using ipv4 so you can migrate it over to dual stack but there are some steps to it the first step to move over to dual stack is you'll need to add a new IPv6 cider block to the VPC you need to need to create or associate an IPv6 subnets ipv4 subnets can can't be migrated so you're just going to have um subnets for ipv4 and subnets for IPv6 you'll need to update the route table for IPv6 to I uh to the igw so it's adding that colon colon sl0 out to the internet you'll need to upgrade Security Group rules to include the IPv6 addresses as well you'll need to migrate ec2 instant types if it does not support IPv6 so most ec2 instances support IPv6 but there could be ones that do not such as Legacy ones or specialized ones you cannot disable ipv ipv4 support for your VPC and subnets this is the default IP addressing system for VPC and ec2 um but you can make them dual stack so there you go hey this is Angie Brown in this fall along let's take a look at how we could migrate a ipv4 VPC over to IPv6 it seems like it shouldn't be too complicated here so we'll go ahead and create ourselves a new VPC and uh we'll just say we'll just actually make a lot of stuff here and we'll have it with one subnet so we'll choose one availability Zone here and we'll say none and the idea is that we don't want to set it with IPv6 because we want to show that migration so we'll just say ipv 6 migration and we'll go ahead and create this VPC you're going see it's going to create a bunch of of stuff for us so how would we have to prepare this for IPv6 well the first thing is we would need to add a cider range for um IPv6 I'm going to go over here and we'll add a new ipv cider range I'm going to allow to allocate from Amazon provided so now we have IPv6 I don't think you can add multiples or you shouldn't be able to but we'll try it anyway apparently you can but we only really need one so we'll go ahead and do that notice that now has an IPv6 range here so that is really good the next thing is we'll need to update the routes table so we'll go here we give this a refresh we'll find our new routes table which is over here I guess we must have forgot to tell it not to make a private subnet which is totally fine but what we can do is go to our routes and we'd have to edit our routes and make sure that we have um a IP address for local IPv6 over to our internet gateway so we'll go ahead and select this and create it so that's part one um the other thing is our subnets our subnets might need some configuration in order to support them generally it I at one point they said like you could only have it ipv4 IPv6 but let's go take a look here and see edit IPv6 addresses so I'm going to goe ahead and add an IPv6 cider block and we go head down and well this is VPC details was I editing the VPC or the subnet IPv6 yes well it says it's on under the subnet so we'll add it here I'm going to go down one so we're not using up all of them and supposedly that's all we had to do noce dns64 is not turned on that's something else you might be interested in but the idea is that you if you uh need to turn off ip4 I think you'd have to create a new one so and also we might want to Auto assign the IPv6 and we could enable that there enable dns64 to allow IPv6 only Services into the VPC to communicate uh to communicate with ipv4 only services so I guess it doesn't necessarily mean that everything has to be I ib4 in order to turn that on but uh that's an option that we might want to want to do and that's pretty much it so um yeah we'll go ahead and just delete our VPC in practicality working with IPv6 is extremely difficult I am not from a networking background so I'm not the best person to show that but I know enough that we could configure that and if I really wanted to I could spend hours fiddling with IPv6 but you know I think that uh iTab us just does not have a good offering for IPv6 um at this time and I just generally would not recommend it it and even networking folks complain about it so there you go okay CIA Network address usage also known as na is a metric applied to resources in your virtual Network to help you plan for and monitor the size of your VPC Nows help you ensure you don't run out of room your VPC by having this rough calculation that you are approaching Max um I've never had to use Nows but I imagine for Enterprise Architects that uh this could be a concern when you have a lot of resources so this definitely could be uh something useful for Enterprise architects now is represent re uh specific resources as Dow units we have a few examples here so we have each so uh we'll just go through the list here each ipv4 address I'll get the pen out so we can keep track of where we are each IP ipv4 address uh assigned to an eni additional Enis attached to the2 instances prefix assigned to a network interface you can see a network load balancer has six Nows a VPC endpoint praisy a Transit Gateway attachment a Lambda function a knat Gateway an EFS attached to an ec2 instance I'm not sure why a Lambda function is so heavy duty but maybe when you associate it with it there's a lot of stuff that's being added to do that um so extremely heavy but that is what it is um a VPC or peered VPC can handle a certain amount of Nows so understand this is also um going to change based on peering so for the now limit you have 64,000 Nows what if you could also pronounce it no I don't know naus maybe I just be saying naus instead it just sounds weird every time I say Nows um if you are using um you can increase that quota right up to uh a quarter of a million then if we're looking at um uh appearing and by the way this is shared for all the all the all the vpcs you're pairing with you're looking at 128,000 so looks like more but I understand that it's going to uh be a lot more limited if you are peering quite a bit um so we have 512 Nows this supplies to all vpcs in the pier so that's what I'm talking about here um you can use Nows to manually calculate when planning a VPC you can use cloudwatch NAU metric to automatically keep track of naus you need to enable na monitoring on a VPC collect metrics for cloudwatch and that would be these particular metrics so we have the network address usage over here uh the usage period and the resource uh resources counted or resource count um you can enable this uh via this flag in fact I'm not sure if you can even do this through the CLI you might or sorry through the UI you might only be able to do this through the CLI you can set a cloudwatch alarm to monitor those Nows and then uh you can see that specific metric tying back to there okay so there you go ciao adus Direct Connect is the adus solution for establishing dedicated network connections from on premise locations datab bus and we have this beautiful diagram to show what that connection looks like uh and the whole point of direct connect is for establishing very fast network connection options and we have two options here a lower bandwidth option with 50 megabytes per second up to 500 megabytes per second I'm saying megabytes per second it's I think it's megabits per second sorry I'm not uh I'm not from an IT networking background so my networking knowledge is never top of Mind here but I'll do my best to get us through this here we have higher bandwidth so we have one gig gigabits per second 10 gigabits per second and 100 gigabits per second I didn't even realize it got up to 100 gigabits which um last time there was only one and 10 so clearly we've seen an increase here um it this the whole point of it was direct connection is to help reduce Network cost increase bandwidth through throughput great for hi trffic networks provides a more consistent Network experience than a typical internet based connection so if you need reliability and security because you're running extremely um uh latency sensitive workloads then this could matter to you of course this is an Enterprise solution so expect to pay Enterprise costs to use it and it's actually kind of hard to learn Because unless you are working with an Enterprise I'm not sure how you would get handson experience you utilizing this so understand that we're doing everything from a theoretical uh um perspective okay so for connection requirements your network is collocated with an existing itus Direct Connect location you're working with an itus Direct Connect partner who is a member of the uh the adus partner Network or also known as the APN you are working with an independent service provider to connect to the adus direct connect if you're not familiar with the word collocated that's talking about basically a data center that you are renting space bam with from um people that want to have private Cloud networks or their own private Cloud they they often will uh use a co location because then they don't have to worry about uh the infrastructure and the folks they just pay for the stuff and rent it we have Network requirements so this is going to be dependent on how much throughput you need but you'll see we have 1 gbit 10 gigabits and 100 gbits and there's this really cool looking thing and then the the cord goes into there uh you know again not from an IT Network working background so this all looks like spaghetti to me but it makes sense in terms of how that would work uh the idea is you have to autonegotiate for a port that so Auto negoti auto negotiation for a port must be disabled for connection with a port speed of more than 1 gabbit per second for one gabit uh per second speed Auto negotiate may or may not need to be disabled uh you might need uh to have this 8021 QV L encapsulation to be supported across the entire connection including intermediate devices your device must support uh B bgp and bgp md5 authentication you can optionally configure bir directional for direction BFD on your network I'm just including these little things in here for the the it networking folk this is not going to show up on your exam or if it is it it' only be for the advanced networking certification but understand for me again a lot of this just looks like spaghetti so you know it is what it is uh for direct connect it supports both ipv4 and IPv6 an Ethernet frame size of 1522 or 9,23 bytes and that's kind of the breakdown of the bytes so you know if you want to know there it is for Direct Connect maintenance it will perod iTab us will periodically perform maintenance on the hardware Fleet there are two types of Maintenance planned so the idea here is that they scheduled in advance uh to improve the availability and delivery of the new features uh they have three notifications that are provided at a 15 and 10 calendar days advance so you are very certain to know of a planned outage there's emergency this is just when something goes wrong as they say due to unexpected failure um for impacted customers there are notified 60 minutes prior to maintenance how good iTab us is with that level support I don't know um but I hope at the Enterprise level they they stick to that as best they can there are three factors to pricing the first is capacity that's the port size so this is the maximum rate the data can be transferred through the network connection uh the larger the port size the greater the cost so you can see that we go from here all the way up and I assume the 10 the 100 gigabits per seconds would be on there as well um uh there is the port hours so this is for the amount of direct connect Port is provisioned for regardless of the data transfer we have two connection types we have dedicated and hosted I don't know why usually my animations will'll do this per per bullet point but I guess I just didn't set the animation right here but for dedicated this is for phys IAL connections between your network port and Network Port and this is build per hour then you have a hosted which is get all this stuff on the screen here hosted which is um logical connections between adus partner and the adus network Port building subject to the adus direct connect delivery partner so if you're working directly with ads it seems like um it's a lot more straightforward if it's not with ads and a partner bilding is going to be um variable the last one is data transfer out um so uh this is charged based on the outbound traffic sent to direct connect to destinations outside of ads and data transfer at uh into the network is free data transfer within the same region will not result in a cost because sometimes it can um in terms of what you should know about Direct Connect well definitely know that uh about the pricing factors this one uh could show up on your exam all the other particular networking details not so much um but just understand it its purpose it's delivery and the amount of G it bit fits for a second it can deliver okay bpcm points allow you to privately connect your VPC to other adabs at Services as well as thirdparty Services think of a secret tunnel where you don't have to leave the adus network and so this is kind of the diagram here I'm going to get my um I guess my laser pointer out for this but imagine you have your ec2 instance and normally if it wants to talk to S3 it would go over the internet so would leave the network and then make its way over to S3 but with a VPC endpoint instead it stays within the network and uh that way it doesn't have to tr transverse the internet which provides a lot of benefit which we'll talk about here in the moment so the benefits are eliminates the need for an internet gateway a net device a VPN connection it was direct connect instances in the VPC do not require a public IP Fe for address public one still needs an IP vp4 address traffic between your VPC and other services do not leave the adus network horizontally scaled redundant and highly available for VPC components allow secure uh communication between instances and services without adding availability risk or bandwidth constraints to your traffic so there's that there are three variants of uh VPC um or three types of VPC endpoints we have interface endpoints Gateway endpoints and we have our Gateway load balancer end points um I put ipv 4 here does it support IPv6 I don't know um but you know I don't think it's going to show up on the exam so just understand what VPC end points do okay so private link is one of those services that just confuses the heck out of me because you cannot just type in uh the search bar in the console in databas Management console private link and have a clear understanding of this um offering um it is a bit fragmented in the console and uh the documentation does not help uh as much otherwise but I do my best here to try to help make it make sense so adus private link is a broadest service that allows you to securely connect your VPC to supported adus Services adus Services hosted in other itus accounts supported itus Marketplace uh partner services without the need of an igw a VPN or an adus direct connect connection now you can use something like an adus direct connection in combination with private link but I'm just saying that you don't have to have one um and so here's a setup of using private link this one specifically if you are either connecting to an existing service provided by somebody else that's third party um or if you are wanting to create your own service that that can be connected via your I suppose your own itus account to that service from other itas accounts through private link and so there's two components I want you to know about the first is the interface endpoint that is what's going to establish an endpoint so that you can go talk to private link and on the other side of it we have a service endpoint and this is if you are creating your own service or if you are connecting to another service there will be a service endpoint there for you there's a couple of components here I don't have text on the slide but I'll tell you about it but generally when you're using private link you're going to see a um uh eni an elastic network interface called card so that is the way it's going to work and it's deployed in a VPC if you are connecting to a service hosting a service there's going to be a load balcer in play here so here is an example of an NLB a network load balancer that would be part of that puzzle because it needs to distribute the load to the applications that you're going to be uh connecting to so you would have to um select if you set up your own service endpoint to select a load balancer that you have configured um there's a thing called private link ready partner service this allows you to access SAS products privately as if they're running in your own uh VPC because when you go and you uh use private link you'll have the option to go to the marketplace which um is where it's very easy to connect to thirdparty uh services but there could be thirdparty services that aren't in the marketplace but uh you can establish connection and so that is that one there but you'll see the word private link repeated a few times in the VPC section here so understand that it's for something things and uh others do not utilize it and the only one that actually does not utilize it is Gateway endpoints but uh yeah there you go interface endpoints are Ean with a private IP address and they serve as an entry point for traffic going to a supported service this is all via private link I didn't write private link here but it is a thing of private link and so here's example of a diagram that is utilizing private link we have the um interface endpoint over here you can see that there's an eni and it's going out to very specific itus services that it supports um so you access Services hosted on itus easily and securely by keeping your network traffic within the adus network remember the whole point of these uh end points is to keep traffic internally with an AWS and interface endpoint supports the following a services so there's quite a few there might even be more now but the point is it supports a broad range of services I don't think it's super important to remember exactly which ones but API gateways always seems to be on the top of that list there um the thing is interface endpoints are not free as long as they are provisioned they are costing you money um and I mean I think these costs are up to date but uh the point is is that for each endpoint you are deploying you are looking at at least $7 a month and the thing is is it's per 10 point per a so this cost can stack up very quickly if you are um if you have a a uh if you have to connect to multiple availability zones so you know I try to avoid using interface endpoints as much as I can and this is more of like an Enterprise um product so just understand that there is that cost there okay hey this is angre brown and this fall we're going to take a look at um interface endpoints so interface endpoints allow us to establish uh via our VPC um a uh a secure connection to resources within the adus network a great example of using um interface endpoints is imagining that we launch our ec2 instance into a private subnet and we want to use sessions manager in order to connect to it uh for sessions manager uh to work uh the E2 instance needs to be able to reach the sessions manager um ipv4 endpoint a n Gateway could be used but also um an interface endpoint could be used so that it can securely talk to sessions manager so I think that would be a really good example for us to uh do here and I think maybe this would be a great uh great time to launch up our our instance here in a private subnet so um I think that we can basically use this Knack one that we have from before as it is totally fine the only thing is that we're going to have to configure it to be a little bit different and choose a a private subnet and so that's what we're going to do here uh for this one we'll actually just launch it in the private into one of the private default subnets I believe that we do have a private one so let's go back in into VPC and take a look here if we go here to subnets or are these all public um I mean we have three so that makes me think that we do not have a private subnet so what I'm going to do is actually create a new VB for this case and we'll do this and we'll just say um interface endpoint interface I'll just say int endpoint example and we'll give the cider block of 10 10 10 10 10 Z 416 and we'll stick with no IPv6 we'll just do ipv4 I'm going to have one availability Zone I want zero public subnets I want to have one private subnet I do not want an net Gateway in this case and I actually do want an endpoint but not a VPC endpoint so we'll leave that alone for now and I'll go all the way down below and we'll go ahead and create this VPC once this VPC is created we'll view the VPC and um now the VPC is created we can go ahead and launch our instance so we do have this script here and I think this one is sufficient enough to utilize we'll have to swap out some of these default values because uh they're not going to work because these are for a VPC that no longer exists but we can swap that out pretty easily so what I want to do is just download this file and I'm going to go off screen here to my downloads folder just give me just a moment and by the way if you saw me having issues with clicking in other videos um that Mouse is completely broken and bo uh brought over one of his cheaper Logitech mouses so um you won't hear any more complaining from me from now on in the video so hopefully that uh that's a bonus here but again looking for that downloads directory I'm not sure why I'm having such a hard time here but I found my template yaml and what I'll do is open up a new tab and we're going to make our way over to cloud formation and we'll go ahead and create a new stack I'm going to upload from a template and we're going to drag that on here and what we're going to have to really do is be very careful to actually um uh replace these parameters right so this will be U my uh interface int endpoint ec2 so we know this is four and we need to go ahead and grab our new subnet so we have a private subnet somewhere here it's going to just be this one let's just make sure that it actually is private it should be we told it to be and the way we know is that it's not going to Auto assign ip4 so just understand that you can launch instances and they could be technically private but uh the other thing that makes this private is that it should not have a route out to the internet um and so that's probably something also that we might want to take a look at but I'm first I'm just going to go ahead and copy that subnet ID and we're going to go ahead and paste it on in there the other part of it is we want the VPC so it'd be nice if the VPC was right here it is right there so I'm just going to cheat and grab it from here and we'll replace that down below and uh T3 micro seem to give us less issues I'm going to go ahead and switch over to T3 micro because if it goes in that CA Central 1D it's going to give us problems and I don't want problems here today the other thing is I'm just thinking about this for moment yeah let's take a look at the route tables because again I just want to make sure that it doesn't have a route out to the Internet so checkbox on that and notice that we have a route and there is no internet gateway okay so there's no internet gateway there's it's not going to make it out to the internet and because it's not Auto signed an ipv4 address we'll treat it that as a private subnet we go ahead and hit next go all the way the bottom hit next we probably have to checkbox this I am uh permissions thing we'll go ahead and hit submit and we'll wait for our deploy to finish this is supposed to launch up a web application we have all the ports open so everything is accessible um and it should already have the SSM R attached as this script here it does it has that Amazon SSM manage instance course so the goal here isev launch up this cc2 instance and we're expecting that when we try to use sessions manager it's not going to work okay so I'll see you back here when it's done spinning up just hit that filter there if you're having having those issues and we'll we'll be back here why is it not uh we're not seeing anything we're we're maybe it's still launching hold on here did CL information mess up we see Central One usually it would show us this by now oh okay it's creating the instance profile but yeah it will take a bit of time here to create the instance profile then the instance will spin up but we'll wait patiently and see you back here shortly okay all right so after waiting a little while looks like our ec2 instance should be up and running we'll make our way over to the ec2 instance itself um and just double check everything is a okay there it is down below here and so we should not be able to establish a connection because if we go here it says we cannot connect your instance so what I'm hoping for is by creating a uh endpoint interface that we will be able to resolve that issue so let's make our way over to VPC as that is where these are created and on the left hand side we should have something that says bpc end points or end points if you can't find it just type in end points it should be here on the leftand side if it's not it could be under the ec2 console I can't always remember where things are you know what I think I just saw it a moment ago end points right here now end points Services if you want to create your own Service as an endpoint but we just want to create an endpoint here today and the idea is that we want to use this so that we can establish connection now we notice that there's already one for ec2 instance connect um you know but my understanding is that we could use this with sessions manager an elastic network interface that allows you to connect the resource in a private subnet other endpoint Services um so there should be go a service should be an endpoint here for sessions manager and that's what we're looking for Kendra notebook Studio access analyzer ACM airflow app config not exactly what I want oh there's more tabs that's why okay I was going to be like if that's all there is that doesn't sound right and so the idea is that these are all services that are being served up by ads basically they're just their end points and what I'm looking for in here is sessions manager or um systems manager it could be the same endpoint we'll just type in session is's that systems manager the reason it wasn't that one if I type in sessions again because it says qldb so qldb is that service um the quad uh quader Ledger we type in SSM that's going to make things a bit easier so I think that it's probably this one here um because it's the systems man uh simple systems manager that's where sessions manager is we'll go ahead and create ourselves a BBC in our new one before we do I just want to show before we do that how many network interfaces we actually have in that um in in well this particular region which will be for that vbc but if we go under network interfaces you'll see that we have one right and so this one is the one that is attached to our ec2 instance we'll go ahead here and um just say full access here here so if we wanted to we could apply policy to restrict it more the idea is that we are provisioning that um that new endpoint and so that endpoint says that it's here is it ready it says it's pending so we're going to have to wait for that to become available now it is we're going to go back here see that there if there's a new network interface I don't see one but that doesn't necessarily mean that it's not there so the thing is is that if it is something that's being managed bys sometimes they don't show you uh the components because they don't want you fiddling with it right so it probably is there but what we'll have to do is to really find out is to go ahead and uh try to connect our instance but uh we'll go back here and hit uh connect I'm doing a hard refresh because I might not be aware of it and so it's still not working um I'm not convinced that our configuration's wrong I wonder if we have to stop and start our instance again so that it becomes aware of that there so what I'm going to do is just give this a quick reboot okay and I'm hoping by rebooting this that might resolve our issue here not yet just give me a moment okay you know one thing that uh I might not have been paying attention to was when we create that endpoint which VPC did we created it for and we did create it for this one so that one is the correct VPC let's go over to our ec2 instance and it must be in the same one because otherwise how would it not have internet access and it is and we'll go back over here sometimes you got to fiddle with this to get it to work so we'll go ahead and try that again and it's thinking still don't see it so just give me a moment and let me see if I can do some research okay another thing that we should probably check is to make sure they're in the same subnet so this one is in which particular subnet let's take a look here because if they're in different subnets um it's not going to work because normally you'll need a interface endpoint per availability zone so they have to be in the same a I should say not necessarily the same subnet and so for here I'm looking for its subnet probably go to the networking tab we'll save us a bunch more time here CA Central 1A so we go back to our end point and does it tell us which subnet this is in subnets we have no subnets attached so this is where we're running into our issue we'll go ahead here and select our first one and there actually only is one so that's pretty straightforward designate IP addresses sure oh no no no we'll just let it choose whatever it wants um there could be another option with like DNS I'm just seeing if there is anything else here I don't think so so maybe that's all that we were missing let's go back over to here and we'll try to connect again also has this finished deploying it's pending so we'll we'll give it a second here to finish okay probably what it's doing now is now I'm thinking about it it's probably Now setting up the eni so let's go take a look and see if there actually is going to be another eni there is okay great so it was my mistake I thought that when we first launched it up it would automatically add an ini but it looks like we have to manually add um the I don't remember it being like that before so I'm not sure if it's changed but this is the way we have to do it now I guess the other thing is that we might need to consider is the security group so uh we have a very um promiscuous security group that allows for everywhere but this would be a case where if they're both in the default then they would share the same thing or you might have to add this and say allow from default so you know we don't need to do this but but what we could do is go to edit here and we could add a rule explicitly say let's add the uh the default one and um that would be here so we just say all traffic but again we're already adding all traffic anywhere so that's not necessary but just as an example of uh what you could do let's go back over here and see if it's available it says it's available now we're going to go back over to um the instance and we're going to go ahead and connect come on it should connect it's it's uh definitely in better shape so it makes me think that I'm still missing something so let me take a look okay okay so one thing I did is I uh rebooted this but it still is not working and so one thought I had was let's take a look here and at the end point so the policies show that it has full access because that is something that could be the other thing is that it has to do with security groups now the thing is is that we are allowing access from our uc2 instance from default but does default allow us access the other way and so maybe this is where our issue is and what we need to do is update our security group to either use the same uh this endpoint to use the same security group or to um or to allow this to work with or this attach the default one on here because I believe we can attach multiple security groups let's go ahead and try to change security groups and so what I'm thinking is we'll just associate the default one because if we do that for the ec2 instance then in theory that would be another thing that it needs to have um to do so I'm hoping that that is the case fingers crossed here we're going to go ahead and try to connect it still doesn't work wow um and this is a really common thing to be honest uh I I always uh struggle with sometimes getting sessions manager to connect in private ones but it just takes fiddling until you get it so I guess just give me another moment okay another thing I'm I'm thinking of trying is just taking down the entire uh instance and bringing it back up because I'm just not sure if it's aware that can reach out uh to that instance and the other thing that we can do is just check and make sure that this instance has the role it does so it really really should just work this is frustrating ipv address good details good same same everything private DNS enabled sure it says private DNS as well okay um but my other thought is that I need this to maybe use the default Security Group so what I'm going to do here I can even do this I'm going to grab this and I don't know because it we'd have to then go edit our script I'm just trying not to have to do any coding which might be impossible in this case so I'll go ahead and hit period we'll just go ahead and open this doesn't seem like there's any way around it here today and we'll create ourselves a new file or a new folder we'll call this interface endpoint and I'll just go ahead and make a new template file in here and the template file we were working with was this snackle one so go ahead and copy that and so yeah my thought here is that we need to allow Ingress for the default security group or I guess we could just use the same one here um another thought is what I could do maybe before we do that we should try to do something with this actual end point here um I'm going to go back to the end points now I can just like change it so it uses the same Security Group manage security groups or just add both of them them and we'll go back here give this a refresh we'll give this a hard refresh we'll try to connect come on like it definitely should work here after creating new instance of sessions manner tab Etc doesn't give you the option connect create inance profile that already exists with session that already exists no error but still can't connect error about missing SSM agent well what error are we getting to connect to the instance the session manager requires an IM am instance profile you can create an instance profile and assign it to your instance using that if you still can't connect to your instance or receiving an error including an SSM agent see troubleshooting what if I click on this does this do anything for us or or is this junk I think this is just junk this is just junk um no error but still can't connect quick connect in configurations host management okay let's let's try what it's telling us to do here said host management under here on the left hand side side maybe because this is where sessions manager is on left hand side said host management not sure why it said that um that was a session created the other day so that's not new preferences no sessions what can can we actually select it from here the SSM agent on the instance supports sessions manager but the instance is not configured for use with sessions manager verify the IM profile attached to the instance includes the required permissions but the instance is not configured for use with sessions manager well it definitely is because it has that rule attach again we can go check it again and just maybe make sure I didn't uh do the wrong one here but I'm pretty certain that this is correct we'll go here we'll checkbox this we'll go to down below to our role we'll check our role our role has Amazon SSM managed instance core that is exactly what it needs there should be nothing fancy in the trust relationship for that to work so that is all fine let's go back to this these instructions which are useless in the systems manager homepage choose the hamburger menu to open the navigation page and go to Quick setup in configurations list choose host management uhhuh so that's this here on the left hand side hosts that is not what I wanted it to do hosts management that is not here here they are full of baloney let's just click back here um we'll carefully follow their silly instructions so it says quick start hamburger menu tell me if you see a hamburger menu I sure don't maybe we have to hit the get started here host management this quick setup will uh type makes it easy to configure instances with the adist agents but it's already preinstalled we don't need the agent installed update systems manager agent every two weeks launch Amazon ec2 launch agent update the ec2 launch agent once every 30 days I don't feel like we need to do any of this so this is not useful advice and also the instances aren't even showing up here manual I guess we saying all we'll go ahead and create now I got host management which I don't really want to have but I guess I have it it says it's running a deployment we'll refresh here says three pending I don't even see host management here on the left hand side so I'm not exactly sure what this is supposed to do say four pending we'll give it a bit of time okay it says it succeeded I I don't understand why it says one success four pending so I guess it must be going through more things I'd like to know what those things are three pending I guess we'll just wait okay I think um I might know what the issue is now this is doing other stuff and honestly I don't think this stuff is going to be that useful it can keep doing what it wants to do here and we'll just kind of ignore it um but I think the issue is that we don't have enough end points so we did add an endpoint interface endpoint for um sessions manager or sorry uh simple systems messenger but there is other stuff that we might need to communicate to like ec2 messages or SSM messages and so maybe by adding those two other interface endpoints we we be able to establish a connection so what we'll do is we'll go over to VPC but now you can see like if we need to have three just for a single a that's going to get really expensive really quick but we'll find out here by doing it so go ahead and create a new endpoint we'll say at adabs we're going to type in SSM and we're going to choose messages and we'll choose our VPC and we'll choose our first one here and we'll choose the same subnet and we'll just check checkbox on both of these so our life is a lot easier we'll go ahead and create that endpoint and then we'll go and create another endpoint and I'm going to go down here and type in SSM and there should be one for ec2 or it could be like ec2 messages there we go and we'll go ahead and create that endpoint sorry there we go and we'll choose our VPC and we will checkbox this and drop this down and we'll checkbox on these and we'll create the end point so now we have three end points so maybe with all those darn end points it'll work we can go back here and take a look at what this is doing whole bunch of nothing I mean he told us what it was going to do was going to try to uh update the SSM agent hopefully it's not mucking up with her instance we'll go here we'll attempt to connect and it's still complaining we'll give this a nice refresh here come on come on let us in this is crazy now the other thing is that we should be able to see this in U managed it's like a thing for seeing uh ones that are being managed um that would probably be under used I thought it used to be called managed instances maybe fleet manager that's where it is now they always change things on me so it's a little bit hard to locate it you have unmanaged instances so I'll go here configure default host is what we've been trying to do not sure if this will ever get configured here today we go back to sessions manager try this from this direction well we don't want to start this uh this session that's an old session still can't do it what a pain just awful let me try some other things okay now it is suggesting that these are being managed so maybe we can go ahead here and just launch from here connect start terminal session not connected is it can it connect from here oh boy um another thing we can do is take a look at that Security Group again maybe there's something wrong with that back to our default one for our new one so this is which one can't really tell which is which even if like just one thing we want to check here it's all it's for everything right all and all well that's out out to the internet outbound rules okay um I'm going to go back to this one here I mean we checkbox this this is this is all for all for everywhere that's outbound to be the same P so this one says from anywhere and also from anywhere interesting let's go back to our end points something's going to click at some point here we'll go to our endo Point Let's double check our security groups yep yes yep go back to our instance connect Frick I don't know okay um if we go back over to sessions manager one thing I'm really curious about so I was just thinking of another case is like is sessions manager always in Us East one because if it is that could also be our problem not that I don't know how you end up creating end points that were not there but let's go to assistance manager is this regionally specific it is okay we go back to our end points see essential one yeah yeah yeah apparently there's another Endo that it could possibly be so like if we go back to our end points here for a moment um it's suggesting um in repost I have it on my other computer here but for the SSM message this one is specifically for um this one is specifically for if you is endo is required if you're connecting to the instance through a secure data Channel using sessions manager I mean I thought that's kind of what we were doing but maybe that's not exactly what we're doing another one that we didn't include was just plain old ec2 um it says here if you're using sessions manager to create a f vsss enabled snapshot which I don't think we are you need to ensure you have an endpoint to the ec2 service and then for the ec2 messages use this endpoint to make calls from SSM agent for systems manager service and then we have the endpoint uh for the systems manager service so we have all these end points and I the other thing I thought is like if we had to reboot it that might resolve our issue um but it has not done that so far so I think um what I'm going to do is go back and adjust the script because I think maybe by having the same uh default VPC our life might be a little bit easier so I'm going to go back and adjust this so instead of having uh this Security Group here I'm just going to take this off here I'm just going to look for SG and um for this we just want to specify the group ID and that is the default one I'm not really sure which one is the freaking default because I can't tell but we'll go back over to here uh to the security group and we'll just grab this one and I'll go back over to our code and I'll paste that on in here as such oh that's the reason why it was it was spinning up a public IP address but we definitely do not need one so I'll take that off I'm going to save that and what I'll do while I'm here is I might as well just update some of these value so I don't have to uh do this later we'll go here and grab this one and paste it in and then we'll go grab the subnet and grab this in my only thought is like when we reboot it the I mean the the SSM agent should try to establish a reconnection but I just want to rule that out and get get rid of our security group so that we don't have to worry about that um we don't need a user data script here so I'll take that out because that's also this would never work because it doesn't have an outbound connection so could this also be the issue the fact that we have that user data in there and it could be maybe causing the user data to fail but I don't see why because the agent should start up on its own and it did say the agent was installed so I'm not exactly sure why that would be an issue um I can switch this over to T3 micro so that is now updated and what we'll do is we'll go ahead and delete our ec2 instance before we do we might need to uh disassociate these security goups here so let's get rid of this one and we'll go to this one here here to this Security Group and we'll manage it and we'll get rid of this one and then we'll go to this one here and get rid of this Security Group and we'll do the same thing I'm going to go back over to cloud formation and we'll go ahead and delete this stack all right that's interesting that when we set up the host management that it has it over here in the quick setup so we can take a look at this which is good because then we can tear it down later on if we don't want it um I wasn't exactly sure what it was setting up but apparently it was setting up a bunch of associations and a document I know these things uh are issue I don't think they cost anything so I'm not too worried about it but we will tear it down later on and so we'll let that delete and then the idea is we'll download our template and we will uh try this again and hopefully we might have better success okay so um we're going to go back over to our stacks and I need to download this file here so we'll go ahead and download that and we'll make our way back over to here and we will create our c a new stack and we'll upload that file just dragging on over going to go ahead and hit next and this will be easy2 for what are we doing interface end points and all our values are updated so we're in good shape we're going to go ahead and hit next and go all the way down the bottom hit next and we'll say I acknowledge and then hopefully this time we'll be in better shape Okay so we've deployed this and we'll take a look at the cct2 instance and confirm that it is in the same uh default uh Security Group and make sure that that security group would allow communication with each other one thing that we did change was to make sure it did not sign a public IP address because we don't need one um so that should be fine it is going to default so that is good and last time I checked anything within defaults allowed to uh communicate with each other so that should be fine as well let's go over here and take a look and see if we can establish our connection can we really does it work it looks like it works and this is how we'd log into ec2 we do pseudo you to switch users otherwise we are other user we want and so we were able to get into it um so yeah there we go I guess it required a bit of finessing um if you were ever to do this for real uh you would obviously want to set up all of these end points in your cloudform mission templates because configuring this stuff manually is a pain and there's always some uncertainty as to what is working and what is not working so uh you know something that we should have done is gone here and actually added the um endpoints here and and tell it to add dependence uh for this so I think that would be the only thing that um I would do a little bit differently here if we wanted to we could tell chat GPT and say hey write that stuff in for us and make sure that template uh gets deployed um I'm not going to do that if you want to do that you we can call that your homework the uh so let's just go ahead and clean up I really thought I would have to start over this video but hey you know sometimes things turn out better than you hope for we'll go ahead and well I won't terminate from here because that's not necessary but what I'll do is go over to cloud formation and then we will go ahead and tear down this stack we'll wait for that to finish and then we'll tear down the quick setup we'll try to actually just tear down them both at the same time once that's done we have to get rid of our um our end points and as well as our VPC so I'll be back in just a moment actually just thinking about it we don't have to wait for these to go down I'm going to go ahead over to VPC and just delete those interface end points you really don't want those sticking around because they'll add cost but you know hopefully it made it very clear that it wasn't just one end point but we had to have three and wow that just adds up in cost uh we'll go ahead and delete these vpcs we'll say delete and their eni should supposedly get deleted with them we'll only know when everything is teared down this is still deleting so we'll just wait a little bit here okay all right so now we can go over to our VPC and if there was any I think it would actually complain and say hey uh you have some eyes you got to get rid of these looks like it's going to delete our resources here excellent going to go double check our Enis um which is under ec2 over here I'll also just check security groups I don't like having security groups lingering around so if we can let's just get rid of some of these other ones they add up really quickly so anytime you can get rid of security groups you're not using do so there's no eni so this is perfect okay great so we'll see you in the next one and uh hopefully you learned a lot Gateway load balancer endpoints are powered via private link and allows you to distribute traffic to a fleet of network virtual Appliance so kind of similar to our previous diagrams but uh this one is a little bit more funky because the idea is that you have an eni which acts as your uh your endpoint for the Gateway load balancer the idea is you have traffic coming in from the internet possibly and then you are sending it over to the Gateway load bouncer this says endpoint but really it's just a load bouncer I I just uh forgot to take the word endpoint out of there and then it's going to go to your virtual appliances I wrote security appliances because that's commonly what you're going to have here and the idea is that traffic is going to flow back and it's going to then go to your uh workload and so the idea is that you're adding a layer of um security or filter before stuff reaches your stuff and allows you to use third party the appliances whether you deploy your own or whether you use um one that is provided by a third party provider so deploy scale and manage firewalls intrusion detection prevention systems deep packet inspection systems you can obtain virtual appliances as a service from the a partner Network the APN or the Adas Marketplace you can send traffic to the glb by making making a simple configuration update in your VPC route cables because you will obviously have to um tell it to route to the Gateway load balancer but there you go a Gateway endpoint provides reliable connectivity to Amazon S3 and Dynamo dbe without requiring an internet gateway or a natat device for your VPC I've seen these called VPC Gateway endpoints I've seen these called Gateway VPC endpoints and also just Gateway endpoints and they have nothing to do with Gateway load balance or end points they're their own thing um very straightforward you can use them they can connect you to dynamodb or S3 just those two Services um they do not use private link they have no additional charge and they only work for Dynamo DB and S3 so uh they're a very cost effective because they're free uh way of keeping a secure connection for those particular Services um to create a Gateway endpoint you must specify the VPC in which you want to create the endpoint and the service to which you want to establish the connection they're super straightforward um but there you go hey this is Angie Brown in this video we're going to take a look at a bbcn points and so the idea is that it allows us to establish a connection um just like an interface endpoint would except it does not require any I don't think it uses Ani but there's no additional cost to utilize it so if we launch an ec2 instance in in order to actually test it we'd have to be able to log into it but we'd also have to deny access to the internet so we're ensuring that it is using the internal network uh as a guarantee um we could do that we just did that with interface endpoint um and we could just take that one a bit further but I don't want to do that I want to try to find something that's a little bit easier and so what I'm thinking is that maybe we can deploy a Lambda um because you can deploy them into VPC and um have it try to read what's in S3 bucket and I think that might be uh easier to do because then all we have to do is tell the Lambda to um log out what it sees in the bucket and if that doesn't work we can quickly iterate over uh that to find out so let's make our way over to Lambda we're going to do all of this um you know via the console just because we have a lot of moving Parts here we're going to go ahead and create ourselves a new function um we have a lot of options for creating a function I'm going to go with Ruby because it's supery easy to use and uh well I guess you have the run time for custom stuff which is nice but we'll just say my uh simple Lambda or we could say this for VPC endpoint uh example probably be a better example we do armor X6 x68 if we have the option choose arm it's a lot more efficient these days and it doesn't seem to have any issues ever running on AWS so we should do that we don't need to enable function URL we do need to enable VPC because we need to connect it to a very specific VPC I think in this case we should make a new VPC as everything's all about VPC so let's make a new one and we'll call this new VPC um uh bpc endpoint example and we'll stick with 10 Z6 that's fine we want to have one a we want to have no public subnets we'll have one private subnet and this actually can create for us uh a vbc endpoint but let's leave that for us so let's go ahead and say none and we'll create this VPC that will create that VPC which is great the next thing I want to do is I want to go ahead and actually create the VPC endpoint we can create this under endpoints as this both creates interface endpoints and VPC endpoints so I mean at least it should and what I'm looking for here is uh I saying VPC endpoint we're creating a Gateway endpoint okay a Gateway endpoint and I'm looking for that I guess it would probably show up here under the type so we go to type here and then we filter by Gateway here we have one for S3 and we will go ahead and choose our new VPC that we created we'll associate it with our route table um we have two rout tables here I'm not sure why we have two because if we chose our vbc it should be choosing this one I guess what it did is it created a route table for um the specific private one because that's how the wizard would create it so I suppose that's fine but um I guess that's what we'll do is we'll do that we'll go down below here full access seems fine to me we'll go ahead and create that endpoint so now we have our VPC endpoint nothing else has to occur it should just work so now what we need to do is go back to our Lambda and we'll go ahead and choose our new VPC here good we will choose our subnet which is the only one here we recommend that you choose at least two subnets we don't want two subnets because we are not uh worried about high availability we'll choose the default security goup I suppose again this doesn't go to the Internet so the default is totally fine it's going to allow anything that is in that anything outbound so this should be fine we'll go ahead and create this function and what we're going to need is some Ruby code so just say write an ads Ruby Lambda that will print out or print out the contents of a specific S3 bucket so we'll go ahead and let that do that while that's going let's go ahead and create our S3 bucket that we're going to need and we'll go ahead and create ourselves an S3 bucket so we say VPC or uh well Gateway it's Gateway end points we'll say Gateway end points example we'll go down below here and we will go and create that bucket we might not get that name well we did but you know change it to whatever you need to have as your name and we'll go back over to chat gbt we'll take a look at the code um this is the latest it looks correct to me we'll go ahead and copy this I'm going to go ahead and open this up just so that we have this example code we can all share it together here just so that you know if you are us utilizing you're getting different code you're going to have less problems so we'll go ahead and say new folder and this will be for um Gateway endpoint and I'll just make a new lambda. Ruby RB save that review it so require that yes that is correct the Lambda Handler is fine we might need to change this based on what configure but that's fine we would want to set up a client um I'm operating CA Central one so we can change that out as such you know we usually don't want to hard code values but we're just going to in this case when we cover Lambda we'll actually do it a bit more proper okay we'll go ahead and paste that in here we have our bucket name I just yeah that's fine uh fetch a list of objects that looks correct to me if it's empty no objects found else print it out um the puts will actually get printed to um our logs so that is really good in the case of Errors let's set the errors the LDA function uh needs to return a value even if it's not being used so that is supposedly fine that looks good to me so I'm happy with this code I'm going to go ahead and copy it we're going to go back over to here well before we do let's just make sure that uh it's named correctly yes it is so that is fine this one's not requiring the Json um it is a builtin library but let's just explicitly require it just in case because Chad te might not have known better go ahead and copy this and we'll go back and paste this in and I mean that looks fine to me Lambda function Lambda Handler and yeah this one's called Lambda Lambda function Lambda Handler so as long as those match then this is fine um so I would suspect this to work the other thing is that we do need permission as well to actually do this but we'll go ahead and upload this this is fine so we'll say deploy but we do need to have a policy to actually access this stuff here so what I'm looking for is our configuration and under this we have permissions and we have access to cloudwatch logs that is not going to be enough for us so we'll go ahead and edit this and vbc endpoint example roll Etc so we'll click into this because we're going to have to configure it to have S3 access and in here I'm going to just add an inline policy I want to allow it to have access to S3 I want to allow it to list buckets I always put that one in because half the time you need it and we need get object and so that should be sufficient those two there I'm going to click on the first one and we have our bucket name over here so I'll copy it in that is our bucket name We'll add the AR we'll go to this one add the bucket name it's any object We'll add the Arn we'll hit next and I'm just going to say S3 simple S3 access we'll go ahead and create that policy I thought it was going to make it in line but uh I guess it is it just named it and so this should allow us to read so that was very important that we have that I'm going to go back over to here and so our function is deployed it should be deployed within that VPC if we go down below here we can see that it is in the VPC um so that seems fine so now my guess is that it should just work so let's go ahead and actually trigger this so trying to figure out how do we do this it's been a while we'll go back to our code I just need to run it I'm not sure why I can't remember I mean I guess it would just be test right because if we not doing anything else it would be test that's what it is so here um we can set it any payload it doesn't really matter because we're not actually going to uh do anything so we'll just say my test and we'll go ahead and save that and so now we'll just hit test and so now it's going to execute it and it says it listed the object successfully there are no objects so I mean that makes sense what we should do is try to add something here um so I need some kind of image chat gbt and say uh create me an image of aw Amazon web services on a skateboard if we can do that we'll give it a moment to generate an image all right so hopefully the uh the weight is worth it but here is our image so kind of looks like Amazon sure why not and so I'm going to just go ahead and download that and uh rename the file so it's a little bit easier to uh work with here did it download the file it says web oh I hate web peas um you know what I don't care I'm just going to upload it as it is it's a great name anyway so we'll grab this and upload it say upload and so what I want to do is um go back once it's uploaded we'll go back over to here and we'll test it again and so what I'm looking for is that object to get listed and here it is it's getting listed so that's great so the question is what would happen if I removed uh the Gateway endpoint because if the Gateway endpoint is removed this should fail right because now we can't we don't have direct access as we don't have an outbound connection to the internet um so what we'll do is go over to VPC and what I'm looking for here is end points right and we'll go ahead and delete this endpoint and we'll delete so that is now deleted which is great we'll go ahead and uh test this again and so the idea is that I expect it not to work okay error message it timed out because there's no endpoint there's no outbound connection it can't hit uh the S3 public endpoint and so that shows that we were able to do what we wanted to do um I'm going to go and just get get rid of this file here and we'll do some cleanups we'll just delete the file usually it's faster to delete the single file maybe not um but we will go over to our buckets and I want to delete this bucket and then I want to go back to vpcs and I'll go ahead and delete these vpcs we cannot oh sorry H yeah we have to get rid of our Lambda first so we'll go back over to Lambda and we'll delete our Lambda I'm not sure if that would be instantaneous because deleting a function Perman removes the code yes yes yes I understand I already have the code it's saved so that's good we'll go back over to our vpcs we'll go ahead and delete this VPC thei is still in use it shouldn't be delete it we'll go back over we'll give this a hard refresh and try this again delete delete there we go and we will see you in the next one ciao let us compare VPC end points to make sure that we clearly understand the difference between these three as they are a little bit confusing and so we'll look across these feature uh categories and hopefully will solidify the difference here so the first we'll talk about type so interface endpoints are reliant on elastic network interfaces are Enis Gateway endpoints are gateways a Gateway with a VPC and then Gateway uh load balancer is a type of interface endpoint so it is a um a subset of uh the interface endpoint over here what is the use case for interface endpoints well they Pro uh provide private connections to ad Services partnered services and other vpcs without public IPS for Gateway end points their private connections to either S3 or Dynamo DB from the vbc and at this point there is no other services that Gateway endpoints work with the Gateway load balancer endpoint will Route traffic to thirdparty virtual appliances like firewalls in another VPC for service integration this is utilizing ads private link for Gateway endpoints it is utilizing S3 and Dynamo DB for Gateway lad balancer endpoints it is utilizing Adit private link as well as the Gateway load balancer to um perform this endpoint connection for supported Services it supports a lot of stuff as we had shown in the prior list when we covered interface endpoints Gateway endpoint again S3 Dynamo DB for Gateway load balancer endpoint it is for thirdparty virtual appliances um so it's going to vary based on the appliance if that makes sense for pricing this is going to be per hour when provision and data process so interface endpoints you really have to think about it um and Factor it in your cost because this can add up very quickly if you uh are running multiaz and and and have a lot of stuff going on here for Gateway endpoints they're free um for Gateway load balance or endpoint understand what it's using under the hood it's using it private link so it's going to be subject to very similar pricing for routing mechanism it's using DN DNS interception and routing for Gateway endpoints it routes table entries for specific destinations for Gateway load balcer it's using Gateway load balancer to do the routing for traffic direction it's bidirectional for interface end points for Gateway endpoint it's unidirectional and Gateway load balcer for the most part is usually unidirectional but there can be some variation there again remember what is the underlying stuff for Gateway load balancer endpoint because it it is just a subset of interface end points but there you go VPC flow logs allow you to capture IP traffic information through your VPC so it's very simple you just have to turn this feature on um at the VPC level but you can scope it a few different ways so you can have it for vpcs for specific subnets um for specific Enis for specific Transit gateways or Transit gate way attachments you can monitor traffic for things that are being accepted or things that are being rejected or all traffic you can log the deliveries to Amazon S3 bucket cloudwatch logs Kinesis data fire hose if you're wondering what um an entry looks like within a VPC flow log it has this entry format which would be represented as something like this to make it even more clear we will just break down each line item here so so the first thing is version this is the flow log version we have the account ID of the account that is um uh writing the log we have an interface ID for the network interface we have the source address which is either ipv4 or IPv6 we have destination address we have Source Port we have destination Port we have the protocol we have um packets so the number of packets being transferred we have the number of bytes being transferred the uh start time in Unix seconds the start or the end time in Unix seconds the type of action we have a few here that's the accept the reject then we have our log status such as okay or no data or skip data and so I think that is I think there just one little uh part on the end they're missing but that's it okay hey this is angre brown and in this video we're going to be looking at VPC flow logs uh so this is pretty straightforward though there is a from what I remember a delay from the time that we will actually receive our logs so that is something we'll have to do is wait about 5 to 10 minutes for it to appear in whichever uh destination we decide to send it to but uh to do this I'm going to go ahead and create myself a new VPC and just so that we are working with something new here so yeah well this will just be our flow logs example and we'll work with 106 as per usual that looks fine to me I'm going to use one availability zone I'm going to have one public subnet no private subnets and I do not want to have a VPC endpoint we'll go ahead and create ourselves this network and the next thing I'm going to want to do is just open this up in github.io but uh if you just want to access the files and not actually writing this code that's totally fine as well but we've been utilizing um a template over and over again for launching ec2 instance I think the one where it was very useful was under Knack because it was just a barebones web server because the idea is that I want to go and hit uh traffic at this instance uh because we're trying to capture um the VPC flow log data so I'm going to end up using this template so what I'll do I just have to find my downloads folder off screen here I'm just deleting my older template. yaml I'm going to go ahead and download this one here I'm going to go back over to here and I want to just open a new tab because it seems to have uh some of the values I want so I just don't want to get rid of those right away we'll make our way over to cloud formation you can also just type in CFN that's the short for for cloud formation and I'm going to go ahead and upload or create this stack so we say upload a file we'll drag drag this one on over there we go I'm going to go ahead ahead and hit next and here this is going to be VPC or so ec2 instance ec2 for flow logs there are a few things we want to change I mean I'm liking T3 micros let's switch over to that um for VPC and subnet we'll need to choose that so there is our VPC so want to copy that there and uh we'll want to have our Subnet in Canada for some reason we always seem to have troubles in 1D but with uh T3 micro I don't think that will be an issue here so go ahead and paste that in place double check that they are correct because if you're using my values it's not going to work we'll go ahead and hit next and I'm going to scroll on down hit next and we'll go ahead and checkbox I acknowledge and we'll submit and we'll wait for that to spin up as that is going what I want to do is make my way over to my VPC and enable flow logs so if we just checkbox that on and go to actions um notice it says create a flow log we could also have this tab down below here I'm going to just create it here and we have a few options so we'll just say my flow log and we can either accept reject or all so I want everything the maximum interval of time duration which a flow package is captured and aggregated I would assume the more frequent the the more expensive it get gets I don't want to wait 10 minutes so I'm going to choose 1 minute we have a few places that we can publish to cloudwatch logs Amazon S3 bucket data fire hose um all great options I feel like cloudwatch logs is going to be the easiest so what we'll do is provide a um a destination log group so the name of an existing log group or the name of log group that you created for this full log so I'm surprised they don't have like an easy uh oneclick button to do this but what we'll do is go over to cloudwatch logs and I'm going to make a new log group I'm not really sure how many log groups I actually have in here I might have next to none and again we're in CA Central we have a couple from when we were running our lamba earlier but I'm going to go ahead and just delete these just because uh these add up very quickly I just don't want to see anything in here and we can use a forward slash to denote what it is I'm going to say uh VPC flow log uh example as my name here and we could say this will expire after a day I'm going to end up deleting this afterwards but just in case I don't want to keep data lying around here as that stuff does add up over time so there is our log group we need an IM roll to publish to uh cloudwatch logs I apparently have a bunch of junk rolls in here um over time that I really should delete out but let's go ahead and set up permissions hopefully it'll just Auto prompt or fill stuff in doesn't look like it we have we have a bunch of junk rolls in here I'm just deciding if I want to get rid of some of these wonder if I can just delete these obviously don't delete your rules if you're using them for stuff but I have a lot of junk in here and I just want to uh clear it out just give me a second I'll be back in just a moment I'm G to let this delete all right just waited a few minutes refresh here um well didn't delete all the roles but uh well I guess it did the best it could anyway I mean I guess these are service link ones so maybe that's the reason why they all say service link rules but I need to go ahead and create myself a new role um this is so that VPC flow log can access cloudwatch log so trusted entity type um allow services like to perform actions in this account so I guess this would be VPC and yeah I'm not actually sure for this uh let's go here you must associate an IMR with your flow log that has sufficient permissions to publish flow logs to specified log group okay what is it um publishing a cloud watch logs we'll go down here here it is so an IM roll that is associated with your flow log must have sufficient permissions uh specified for the cloudwatch logs theal must have the following stuff and here it says services PC flow logs so I think the thing is if we go back here we really need to make sure this is flow logs flow logs bpc I just do a custom trust policy because I don't trust it um because I wasn't seeing where we could choose that so it's a lot easier if you just have it here you can just copy and paste it I try not to use the gooey as much as I can so now we are choosing vbc flow logs which is great and so that's and that's all really this is about is setting up that trusted identity we'll go to the next one and so now we're looking at our policy um I suppose we could just go create our own in this case so we're going to go over to roles here or policies back to IM policies we'll create a policy and the service I really do not like uh this I'm going to go over to Json and just make my life a lot easier and we'll just copy what we need here the thing is we only need this for a very specific uh resource so we really should uh provide that exact resource here so just add it on the right hand side and here we can specify for a specific log group um and if we go back over to our actual log group we go to cloudwatch logs when you can you should always try to narrow things down the exactly uh what it is using but but uh we'll go over to log groups and uh here it is if we click into it there might be like an AR somewhere that we can utilize here it is I'll grab that we'll go back over to here and is that what we're looking for yeah it is now do we need to have the colon uh as on the I'm not sure but we'll give that a go and see if that works we'll hit next we don't want all of them we just want the individual one so I'll just fix that we'll go next and this one will be bpc flow logs Cloud watch example example for bpc flow logs to cloudwatch permissions we'll go down below we'll hit create policy and so that will create our policy we'll go back over to our roll wherever that is and we're adding the policy and I'm refreshing we can drop it down and say customer manage so that we can quickly find the ones that we've created and I'm looking for that VPC flow lock flow log cloudwatch example down below we'll hit next and this one's going to be uh VPC flow log example permission to write to write to cloudwatch logs okay we'll go down and hit create roll let so we created that rule we go back to our VPC flow log we'll refresh it we'll drop this down and we're looking for VPC flow log examples uh we can choose either custom format or the default format I'm going to stick with that we're going to go ahead and hit create the flow log and so it's my assumption that that now is working and this should be happening every minute so hopefully that is the case I'm going to make my way over to um cloudwatch logs I'm going to open this up and actually there's a live tail function so this is something that we could utilize to see this in uh in real time I actually haven't used it because when this existed was not when I last was recording about cloudwatch logs and I just don't normally need to utilize this one but sounds really good um I mean I just want everything that's coming through here so we'll apply those filters and hopefully that is enough so it's saying waiting for logs to match the filters and the only filter we put in here was the fact that we it has to come from here um so what I'll do is I'll go over to our uh VPC or sorry over to the ec2 console because I want to go see what we've launched we'll choose instances um and I want to open this address this public IP 4 address hopefully this works says it cannot be reached I'm not really sure about that we'll just take out the HTTP there we go and so that is now triggering and so what I'm doing is I'm just hitting it a bunch of times the reason I'm doing that is that I want to send information through VPC flow logs if we go back over here aha there we go we got data so here it is logging stuff uh which is very fun um and if we expand this we can see we have it's not the most interesting but the idea is that we have accept Okay um thei that it's going to so yeah pretty straight forward so I I would call this a success so let's go ahead and tear this all down so the first thing is we want to get rid of our ec2 instance um we'll actually get rid of it through cloud formation because that is actually how we provisioned it and I think we already have a tab open so I'll utilize that we'll delete that the next thing I want to do is I want to tear down this VPC um I'm not sure if we can do that directly but I'll get rid of the VPC flow log I'm not sure if um the flow log would be automatically deleted by bpc but we'll just manually delete it because we're going to have to wait for that instance to spin down anyway so go over here and we go to flow logs and I'm going to go ahead and delete my flow logs so just say delete we'll make our way over to cloudwatch logs and now that that VPC flow log is gone they're really trying to compete with Azure these days uh they never had such a complex way of getting into your log so that's interesting um and we'll go ahead and delete this delete our log group there we go good so that one's gone I'm assuming that our Cloud information instance should be gone by now it's still waiting to delete we'll just wait a moment for that to delete okay all right that ec2 has now been torn down and so uh we can now just go ahead and get rid of our VPC that we created just for this tutorial and we'll say uh Delete good and there you go that is VPC flow logs we'll see you in the next one okay ciao VPN lets you establish a secure and private tunnel from your network or device to the ad Global Network this is the slide that I used in the cloud partitioner so not super exciting but it does point out the two things that we need to learn about which we are definitely going to cover the first is the adab the sight to sight VPN which securely connects on premise Network or Branch offices sites to VPC and the inis client VPN this securely connect IND users to the ads or on premise networks uh one thing I'd like to cover is IPC because there's not really anywhere else to talk about it so I threw it in here so IPC which stands for Internet Protocol security is a secure network protocol Suite that authenticates and encrypts p uh the packets of data to provide secure encrypted communication between two computers over the IP network is used in vpns so if you're using vpns you're going to get that secure connection and the reason why you need VPN is to get that secure connection so if you're using Direct Connect which is a private connection it's not necessarily secure and so you need that additional layer uh to get that if that makes sense but let's get into these two site to site VPN and itus client VPN okay itus sight to site VPN allows you to connect your VPC to your on premise Network and so here is an example of a sight to sight VPN you might see um some of those is a little bit different but more or less it is pretty much the same thing but let's take a look at the components that are involved um the first is the VPN connection so this is the secure connection between the VPC and the on premise equipment you have your VPN tunnel which is not Illustrated here but we'll see it in another slide as I I did visualize it in another diagram uh this is the encrypted connection uh for your data we have the customer Gateway the cgw which provides information to ads about your customer Gateway device we have your customer Gateway device itself this is the physical device or software application on your side of the sight to sight vpm connection we have the Target gateway this is a generic term for the VPN endpoint on the Amazon side to the sight to site VPN connection so I suppose I'm just trying to look at our diagram because I don't see the Target gateway in the diagram but um uh I suppose it's what you're targeting maybe that's within the virtual private Gateway again not the best at networking but I know enough of this to get you through it uh we have the virtual private Gateway vgw uh this is the VPN npoint on the Amazon side of your sight tosite VPN connection that can be attached to a single VPC then we have Transit Gateway which is not not in here but the thing is is that Transit Gateway is a newer um uh service and can be used uh to uh utiliz in a VPN and we will we'll take a look at that but let's just describe what Transit Gateway is it's a Transit Hub that can used to interconnect multiple multiple vpcs and on premise networks and as a VPN endpoint for the Amazon side of the site to site V VPN connection and I think one of the reasons why you'd want to use a Transit Gateway as opposed to um a VPN Gateway is because uh VPN gateways only support ipv4 whereas Transit gateways support both ipv4 IPv6 but let's go look at more specific details and this is going to be a very boring slide but we'll get into the ng grees of the features set for V uh for site to site VPN we have uh internet key exchange version two as uh the method of I guess encryption or whatever we have natat TR traversal we have 4 byte ASN in the range of what is provided there we have two byte ASN for the cgw um if you don't know what ASN is we describe it in one of the slides um we have cloudwatch metrics we have the reusable IP addresses of your customer gateways you have additional encryption options if you so choose to want to utilize them we have custom private ASN for the Amazon site for the uh BPG session we support for IPv6 traffic for the VPN con connections on a Transit Gateway so that's that one I should have highlighted in red because I think that's a very important indicator as to why you would use one over the other uh you can optionally enable acceleration for your sight to site VPN connection via the aabus global accelerator you can attach a sight tosite vpm to aabus Cloud when you can attach a sight to sight VPN to ABS Transit Gateway um in terms of pricing you're going to be paying for each connection per hour uh the data transfer out from the Amazon easy2 instance to the internet is another cost that you could be incurring for limitations uh it does not support IPv6 traffic uh for virtual private Gateway if you want IPv6 you're going to use Transit Gateway and edus VPN connection does not support path MTU Discovery uh it's recommended that you do not use nonoverlapping cider blocks for your network so for those who are not big into networking um you don't really need to remember all this stuff the only thing that that I think really matters is that IP IPv6 difference and the pricing here but for those that are into networking hopefully this uh was a value to you so there you go let's talk about the virtual private Gateway component which is important when you're setting up a sight tosite VPN so uh it is a VPN endpoint and it's on the Amazon side and it can be attached to a single VPC as you can see here when you create your vgw and I find the initialism always really hard to say there because it says virtual private Gateway so you think it'd be vpg but it's not it's vgw you need to assign an Amazon an uh autonomous that's a hard word to say system number or an ASN or a custom ASN and if you don't set one you'll always use the Amazon one which is 64512 okay uh once you create your vgw you cannot change the ASN so you're going to be locked in with what it is and you might be asking what is this autonomous system number well an ASN is a unique identifier that is globally allocated to each autonomous system that participates in the internet so it's a way for your device to be detected has something to do with um border Gateway border big Gateway border Gateway protocol okay so um again not super great at networking but I know that is part of it and uh that's all we really need to know about vgw okay a customer Gateway is a resource that you create in ANS that represents the customer Gateway device in your on premise Network so imagine you have your on premise Network there is your customer Gateway and then we have a gray box to represent the customer Gateway device which is going to the actual physical uh machines you want to connect to your network so when you configure your cgw you're going to set the BPG ASN for your customer Gateway device your IP address for your customer Gateway external device the private certificate provision by adab certificate manager ACM um you will also need to provide additional configuration to your custom Gateway device which will establish the connection uh between ads and your on premise Network so here is that other diagram I was talking about where we weren't representing tunnel earlier well here it is and so if there's a device failure within adabs your VPN connection automatically fails over to the second tunnel so your access isn't interrupted from from time to time it also performs routine maintenance on the VPN connection so it's just to try to represent those physical routers on the Abus side and the fact that there's two established tunnels um and that there is that redundancy there and just so you're aware of that um it was provides sample configuration files for various customer Gateway devices so if you have a piece of this software um in uh sitting in your uh on premise Network then the idea is that you can just download it and quickly configure it if you have a device that is not in this list you can con possibly configure your device if it meets particular requirements so it has to have internet key exchange uh and you have to associate that of course IP SEC security Association you'll have to have a tunnel interface and you might need to um configure border Gateway protocol I'm not sure what I said BP bgp was before it's funny because I've covered BPG and uh so I generally know what it is but off the top of my head I can never clearly talk about it but yeah what I want you understand from customer Gateway is that you're you you're hooking up but there's going to be a separate step where you have to configure the device um would we ever do this in a lab I don't know it seems pretty complex but um you know that's something you can do there anyway we'll see you in the next one ciao a Transit Gateway is a Transit Hub that you can use to interconnect your vpcs and your on premise Network this is just a very straightforward slide to be able to show you a diagram I couldn't be bothered to make one I just grabbed it from the documentation but I just wanted you to know that you can substitute your virtual private Gateway for a Transit Gateway Transit gateways there's a lot more to them than just the single slide and we have a separate section on Transit Gateway as it is a hub and spoke way of connecting multiple vpcs to um uh one one way of communicating but the reason I want to give this extra emphasis is that Transit gateways can support support both ipv4 traffic and IPv6 traffic inside the VPN tunnels so if you need IPv6 you're going to use Transit Gateway so yeah there's my extra emphasis there for you okay ads client VPN is a fully managed clientbased VPN service that enables you to securely access edus resources and resources in your own on premise network uh so the main reason you'd be using a client VPN is that you are just a laptop and you're floating around in the world and you need to uh securely connect and so generally what will happen is that you'll end up installing something like op openvpn client which is a VPN client that's going to allow you to establish a connection to the adabas client VPN um and that way you'll be able to securely connect but let's just uh go through some of the things that you can do with the aess client VPN you can use certificate based authentication also known as Mutual authentication um to uh uh authenticate you can use active directory Authentication via itus directory service you can do Federation authentication which would allow for single sign on Via saml uh it uses a single tunnel so there's no redundancy here so you know hopefully you don't lose your connection uh it uses security groups for granular control it can use uh active directory groups for granular control again depends on how you authenticating uh and if you are bringing in your directory service it has a self serve portal to download uh the itus VPN desktop client which I think last time I used this that didn't exist so that's kind of interesting the a VPN client has two roles there's the administrator responsible for setting up and configuring the services then you have clients the person who connects to the client VPN do uh endpoint to establish a VPN session not sure why the end is missing there but let's just add it in so it is a little bit nicer there there we go uh you could use the ads client VPN to securely connect to an RDS instance that is only in a private subnet that would be a really good use case for using a client VPN because if uh you didn't what you'd have to do to connect to let's say an RDS and a private subnet is you'd have to spin up another ec2 instance in the same subnet and then you'd have to no public subnet and then you'd have to then connect to it or whatever but anyway I just want to make the point that using AIS client VPN is great for that use case there but uh this is one of those things that is is better showing than reading about so we'll do that in a lab okay before we talk about uh natat gateways and N instances we need to know what network address translation is so it is a method of mapping an IP address space into another by modifying the network address information in the IP header of packets while there are in transit across a traffic routing device so an example for AWS would be imagine you have an ec2 instance that's using ipv4 it has an internal address something like 10 10 10 254 but that's a address that makes sense in a local network but out to the internet that makes no sense so you have to map it um to an address that uh can be uh readable back to that private instance and so here you can see it's going to be 1726 131 254 and so that's what the N is doing it's it is U mapping uh those internal IPS those private IPS to public IPS there's an another use for Nat and that's when you have um uh two private networks and they happen to have the same IP address where you would have conflicts because of course there' be no way to wrote to both of them so uh the the natat would be able to resolve the differences between that and give them different IP addresses so hopefully that is very clear what network address translation is Nats really make sense for ipv4 IPv6 is not an issue because you have an in inexhaustible list of uh addresses okay n Gateway is a fully managed net service to allow instances in your private subnet to establish outbound connections and as you can see here we have a net gayway in a public subnet but technically uh they can be deployed in a private subnet and that wasn't always the case but uh they've changed it since the last time I've use Nat Gateway a KN Gateway is redundant within a single subnet meaning you'll need a Nat Gateway per per subnet and uh the issue with this is it can add up costs pretty quickly because as long as you have a net Gateway deployed it's going to cost you money even if you're not using it so um if you have a workload let's say in three subnets then you're going to be paying for three Net gateways and for the uh that's 730 hours for each of those it's going to add up to $98 and that doesn't even take into account uh the data processing cost for data passing through it so nway can be expensive and um is something that companies are always having a hard time with especially at the Enterprise level because it's just when you want everything to secure you got to have Knack gateways for ipv4 so um that is a challenge for uh some folks to mitigate uh Knack Gateway has two connection modes public which is the default uh this is where instances are in private subnets and can connect to the internet through a public net Gateway as demonstrated in our diagram there uh they cannot receive unsolicited inbound connections from the internet which is what we want you must associate an elastic IP address so that is an additional cost because that EIP has an ipv4 cost um so small but it is a cost that you have to uh consider as well um then there's private I don't remember private existing before so I feel like this is new um instances in private subnets can connect to other vpcs or you're on premise Network through a private net Gateway you can Route traffic from the knat Gateway through a Transit Gateway or a virtual private Gateway you cannot associate an elastic IP address with a private n Gateway because it's private um if it's public then you can have a public elastic IP address it makes sense there's probably a little bit more Nuance to this but it's not going to show up on your exam so we're just keeping it uh straightforward here there is uh these things called dns64 and N64 because IPv6 is just the future I figured that we should throw this stuff in here even if it doesn't show up in the exam so uh this is straight from the documentation let's just read through and understand what's going on here so a naaa supports a network ad address translation from IPv6 to ipv4 popularly known as natat 64 um and you know we said that uh natat can't do uh address translation for um IPv6 which is true but it can do IPv6 to ipv4 so just understand that distinction there so what is dns64 your IPv6 only workloads running in your VPC can only send and receive IPv6 Network packets without dns64 a DNS query for an ipv4 only service will yield an ipv4 destination address in response and your IPv6 only service cannot communicate with it to bridge this communication capap gap you can enable dns64 for a subnet and it applies to all the adus resources within that subnet as far as I understand there's no additional cost to this because I couldn't find any what is Nat 64 Nat 64 enables your IPv6 only Service uh in Amazon vbc to communicate with an ipv4 only service within the same VB vbc or connected to vbcs in your on premise networks or over the Internet the last one sounds extremely useful but yeah there you go hey this is Angie Brown and this fall along we want to take a look here at Nat gateways now these things are a little bit expensive so if you don't want to spin one up that is totally fine um but I'm going to go ahead and do that here so we know how they work the purpose of a na Gateway is to allow uh private instances within avbc to communicate outbound to the internet because if they're in a private subnet they're not going to be able to do so so that's what we're going to do in this video and uh we're going to have to go ahead as per usual and make ourselves a VPC so let's go ahead and do that before we do that I'm just going to pop over and take a look at what the requirements are for n Gateway I assume that we can just make one per subnet but you know sometimes with these uh networking Services they they might ask for more than one a or something even though I know that we should only be using one a I'm just double cheing here yeah we'll be choosing a single subnet we'll also have to create our own elastic IP so let's go ahead and create our C are VPC so we'll just say here uh natat example it will s with a 106 we'll have 1 AZ no public subnets one private subnet we're not going to set up a net Gateway here we're going to say none on this and we'll create this VPC so that'll be step one the other thing we need to do is create ourselves an elastic IP address so we'll go ahead over to ec2 as I believe that's where they are and on the left hand side we're going to look for elastic IPS and from here we'll allocate a new elastic IP from the ipv4 pool and we'll go ahead and allocate that so now we have our elastic IP if we go over to our natat Gateway we'll say myn Gateway 01 we'll choose the single private subnet that we have which is 1 a um the connection type select connective type for The Knack Gateway um wouldn't it be public it would be public yes because you canat stuff uh privately as well we covered that in the slides we'll go ahead and choose our elastic IP we'll look at some additional configurations that all looks fine we'll go ahead and create that knat Gateway that's going to spin up that knat Gateway as that is spinning up let's go ahead and get our ec2 instance going so I'm going to make my way over to GitHub as per usual and um we need to have a bit of code so we'll go to ad's examples and I'm just thinking about this for a moment because the reason we want the N Gateway is so that we can have an outbound connection to it um if we launch up our usual um server that we usually do H period here um it does allow us to have SSM roll so maybe what we can do like to test that this works is that as long as we can establish a connection with sessions manager then we know that it's working because then we can actually get into the instance um we don't need it to spin up a web server so what I'm going to do is just make a new folder here and we'll just call Nat Gateway I I imagine we already have this code somewhere else in another example probably with interface endpoint let's go take a look at that template because all I need is um the SSM roll I don't need the web server so we'll copy this one instead and we'll make our way over to that folder that I just created for Nat Gateway does anybody see it there it is I'm sure you seeing it but I can't hear you so it's not going to help too much we'll go ahead and paste that in and so we have a few things here the T3 micro we'll have to choose our VPC so I'll go here and grab that as I still have it open so I'll just paste that in there we need the subnet ID which we have right here grab that loving this new mouse even though it's not the nicest Mouse clicks really loud though um and so we have our T3 micro the image is set up the subnet set up the VPC setup setup we'll scroll on down here we are specifying a very specific Security Group um this made sense for that other example because we were bringing in the default Security Group I suppose we could do that for this one as well I'm not really too worried about it so maybe that's what we'll do so I'm just going to go into this VPC here and checkbox it does it have like default Security Group here no um I'll copy the VPC ID and I'll make my way over to ec2 and we'll go find out what that um Security Group is so if we go to our security groups wherever that is here security groups security groups here we are I am searching for the default one which is this one here let's just click into it and it's allowing any inbound traffic from anything that's within the security group and any outbound traffic to anywhere um yeah so that should work I think that we don't have to do anything fancy here so I'm going to grab that Security Group ID and we'll make our way back over to here and I'm going to paste this in place going delete it just make sure that I actually got the new one good and I'm going to go ahead and download this template I'm going to open up my downloads directory off screen here I'm deleting my old template yamel file that we've been downloading a bunch of times I'm going go download this template I'm going to make my way over to cloud formation I'm going to try to leave some of these tabs open here Cloud information I'm going to want to upload this so say create the stack upload a file dragging that template file over we'll hit next because we already prefilled these in earlier we don't have to worry about that we'll say Nat ec2 example and we'll launch this one up and we will drop this down and so what I'm thinking is that if the KN Gateway is deployed then the agent can go out and talk to the um thing we be able to connect to it so before we do that let's just make sure this is deployed uh is this fully deployed oh it failed to deploy well good thing we didn't launch that cloudformation template yet so what is its problem does it tell us why failed okay great why did it fail we'd love to know why why is a great great answer as to why something failed it doesn't say all right Nat Gateway failed to launch there may have been an airor when it was created for more information uh see the net Gateway creation fails check sure I'd love to see that where is that um a that fail zom usually about there was an error with the N Gateway created sub subnet has insufficient uh free addresses to create a n Gateway the network has no internet gateway attached oh I guess we probably would need to have an uh internet gateway that makes sense let's go ahead and do that so um all right well let's just do that and see if that helps because I'm just trying to think here like we'd have to update our route table but our our route table is for the private subnet and our private subnet should not be going upbound to the the internet also don't we need to have a public subnet because in order to deploy the net Gateway it would have to deploy in the public subnet and have a route to the internet and then our private subnet would yeah I think that's that's what we're talking about here so what I'm going to do let's go back to our VPC and we'll just tear this down I really messed that one up and we'll go ahead and um you can see I actually have more than one here I'm going to delete this also it looks like I have two vbcs I didn't delete the last one I was trying to do a VPN client and it failed as a tutorial and I didn't delete that earlier what what we'll do is go ahead and create a new VPC we'll say uh VP or like natat gate natat Gateway example and we will choose 10.0.0.0 sl16 as our range actually we'll do VPC and more so this just saves us some time uh Nat Gateway I'm going to have one public subnet one private subnet I'm not going to set up a n gayway because I again want to set up manually we'll have no VPC end point we'll create this VPC and that's going to spin that all up now the problem is with our Cloud foration template this is now old so we'll have to swap out these values here I'm just going to do that right here okay and I'm just going to grab this sub net though that was the subnet and then we'll grab this VPC here actually which is which is which I don't know now I had two tabs open we'll go back to uh your vpcs and we'll grab this one I don't feel like if this messes up I don't feel like having to do this multiple times I'm just going to paste it in here for now and this time we will go back here and I want to choose the private side n so this is the private one right here okay and I'll also do that over here as well just grab this one okay um so now we have those set up we'll go ahead and attempt the N Gateway again I'm going to just see if I can delete this one I think it's going to delete after an hour they said so I guess I'll have to spin up a new one myn Gateway 2 and we'll choose this time our public subnet which is here and this will be public which makes sense we'll drop this down and use this elastic IP we'll go ahead and say create net Gateway I know there's internet gateway because if we chose the public subnet it would have already added that as a a routing mechanism um so I'm hoping that that is going to put us in good shape here let's go back to n gateways and just carefully monitor this until it deploys be back here in just a moment okay all right let's take a look here we'll give it a refresh and it's now available so we should be in good shape to launch our ec2 instance because if our public subnet has our KN Gateway and it has a red up to the internet then deploying our um uh this this fellow here into our private subnet should uh put us in good shape going just double check make sure we're using that private subnet and not that public one okay good we'll go ahead and hit next down below next and we will say I acknowledge and we'll submit this and we'll wait for this to deploy but hopefully we don't run into any issues see you back here in just a moment and our ec2 instance failed for some reason it says uh Security Group does not exist oh yeah because we um updated that value in here and it's hardcoded so that's where we're running into that issue I'm just going to extract that out and call it a group ID so I don't have this problem again group group group ID and then I'll go down below here we'll just change this out to ref group ID and um I mean if we didn't specify one it should automatically utilize it but I think we should be very explicit here in terms of what we use so that way I have less problems I'm going to go over to Security Group groups I guess it's under ec2 right security groups which is which nobody knows is this the uh newer one nope it's this one so copy this I don't know why they don't put it right there make it so much easier to see the name of the security group um or the VPC so you could find that match there we'll copy this one I'm going to go back over to my template go all the way to the top here I'm going to paste this into here and I will delete my local template yaml file we'll go ahead and download this one I'll make my way back over to cloud formation and we're going to go ahead and delete this stack because this one is junk now we'll refresh this and we'll create ourselves a new stack go back to Stacks here create a new stack we'll upload the template we'll drag this one on over we'll go ahead and hit next and we'll say um ec2 what what are we doing in this one we're doing something uh Nat gayway Nat gayway yeah Nat gayway and this time we don't have to update anything because we did that before we'll acknowledge and so hopefully this time it's going to work so I'll see you back here in just a moment okay okay so our net Gateway is now deployed so the question is um are we able to connect conect to it or was there something else that we needed to do we're going to find out here in just a moment by switching over to ec2 and establishing or seeing we can establish a connection here so I'm just going to take off that so we're just seeing all of them we'll checkbox this one hit connect please just work it says to connect to an instance sessions manager Sy systems manager requires an IM am instance profile we absolutely have that if you still can't connect etc etc so maybe there's something that is missing here um it's still initialized so maybe we should just give it a moment to finish and then we'll check okay all right let's see if those status checks have passed they have let's go ahead and see if we can establish a connection and we still cannot so there's something that is missing that it wants what that could be I'm not sure let's take a look at our subnets and see if we can figure that out so I'm going to make my way over to VPC and we will go first check the VPC configuration we should have DNS host names on so we have res resolution of DNS and host names let's go take a look at our subnets for our private the question is is it resolving DS host names for some reason I think that there would be a setting in here maybe this is fine resource uh resource names DNS a record not 100% sure here let's go ahead and check edit subnet settings I think we need that okay I mean these look fine to me let's go back to our ec2 instance and take a look at its configuration I'm going to go to the networking tab it has a uh a DNS for private IP so everything looks to be okay here um we do have that IP address but it's associated with the net Gateway so has nothing to do with our instance so that all looks okay could it be our route tables I mean we didn't configure it it was autoc configured for us so I would expect we have this going to internet gateway and this allows for any traffic local and then this one here we have routes for local do we have to add a route for the that Gateway that's what I'm thinking here because this is local but how would it know to talk to each other and I'm pretty sure we can had a route for that Gateway so that's probably what our issue is we're missing our route so let me go figure out where this route's supposed to be yeah so I think what we're missing is here we're supposed to put 000000 and we're supposed to select our net Gateway and that way our private subnet can uh go out to anywhere which would then go to our public subnet I suppose so let's see if that helps that out we might have to relaunch the instance because if the agent had attempted to connect and failed I don't know if it's going to continuously try to connect but we can go ahead and try to establish our connection here I'm going to give this a hard refresh and see what happens so no good what I'll do is go over to cloud formation uh well first we can just try restarting the instance not that that always works but we'll go ahead and give this a reboot so we're going to go here and do this actually I'll give it a full stop to confirm that you want to stop this instance after you stop the instance you're no longer charge yeah I know that we'll go do that so I'll stop that and then we'll start it up here in just a moment all right so that's been stopped let's go ahead and restart that instance and so what I'm hoping for here is that when it launches up that uh SSM agent is going to establish that connection and then it's going to allow us to use sessions manager and assuming our routes are configured correctly then that will put us in great shape but we'll have to wait here and find out in a moment okay okay so let's take a look here and see if we get our status checks we do now does our connection work please just work and we have no uh warning so that is a very great indicator that this is working sessions manager you have to play around with it a bit and just understand what that agent is doing underneath it's trying to establish the connection if it fails to do so uh for any configuration you got to take it on down but there we've established our connection proving that our natat Gateway is working so that's all I really wanted to do here um and we were very successful there of course we missed that one single route so it's good that we uh figured that out we going to go ahead and terminate this instance we going to make our way over to VPC and while terminating the instance isn't going to do much because we actually have to tear down the CLA formation stack so I'm going to go ahead and delete this stack hopefully it doesn't have trouble doing that cuz we terminated that over there uh while that is happening I'm going to make my way back over to VPC we don't need that Gat anymore um so we'll go look for the Gat Gateway wherever it is here it is there we go we'll go ahead and tear this down so delete the N Gateway I don't know if it will instantly delete but it will probably delete after a bit of time notice this one's going to hang around for a little while at least hour hour before it goes away but uh yeah we'll just hang tight here and um wait a little bit and then once those things are torn down then we'll get rid of that bpc be back here in just a moment okay all right so our stack has now been torn down let's go take a look and look at that Gateway that's been deleted so we're in good shape let's make our way over to our VPC and we'll go ahead and delete this VPC it should delete all the resources we wanted to delete so it is going ahead and doing that I going to check internet gateway sometimes they are hanging around here and I don't want extra ones but looks like everything tore down so yeah that is natat Gateway of course Nat Gateway can get uh very expensive very quickly but um it is fully managed so that is really nice but we'll see you in the next one okay ciao so a Nat instance is an IT of us managed am uh to launch a gat onto an individual ECT to instance um and so this was what existed before Nat gateways and uh they worked fine the only thing was that they didn't have any builtin scaling so whereas natat gateways will automatically um increase or decrease the amount of a compute underneath to handle the workload or the throughput that instances you are actually having to manage uh the C2 instances and that could cause disruptions and it requires a lot more additional maintenance but it was very cost effective so this was still seen as a a cost effective alternative to natat Gateway um and I mean that's the diagram very straightforward here uh you can deploy your public I guess you can deploy your private subnet if you wanted as as well but there it is in the public subnet and you know people were still using this even though n Gateway existed because of cost however itaba stopped um supporting their natat Ami and they said they're they they were going to stop development in 2018 and it's it's reached the end of uh support in 2023 so there's really no reason to use this anymore uh luckily via the marketplace a Marketplace you can get community Amis or even probably paid Amis uh like managed Amis for thirdparty natat software so if you're looking for an alternative to Nat gway that could be more cost effective or more feature then uh you know look into the marketplace so there you go hey this is Angie Brown and this fall along we're going to take a look at setting up a natat instance which is different from a natat Gateway since Nat Gateway is a manage service natat instance is the idea of setting up your own uh Gat and there is this nice article on kibasa it is uh outdated since it's from 2019 so there are some tweaks we have to do to get something similar working but I originally thought that I would go to the marketplace and just launch up a natat instance um turns out there really wasn't any Community Amis or at least that I could find that would have been suitable for that instance or if there were paid ones um I just did not want to proceed forward so really all we need to do to have a ad instance is we need to launch an E2 instance and configure IP tables to act as a net and do Network address translation and that's what we are going to do I'm going to tell you this is my fifth time trying to record this it is that much of a headache so just carefully follow through and hopefully we'll make it through it I had to pull my cofounder to figure fig it out like um and it's not to say this stuff is hard but like when you're dealing with networking especially at the OS level um it's very easy to muck things up and miss something especially if you're like me and you're a developer not necessarily um from the the networking background but anyway we'll go ahead here and just say Nat instance uh as we create our new VPC 106 is fine 1 a uh we'll set as that I'm going to drop this down make sure it's 1 a central one public subnet subnet one private subnet no Gateway no VPC endp Point make sure both DNS options are enabled that will go ahead and create that our VPC is now ready so the next thing we need to do is we need to go and launch two ec2 instances the first is going to be um just a regular old ec2 instance that we're going to place in our um actually I was just thinking about this we should launch the N instance first and hook it up and then the other E2 instance because if we want to use sessions managers to connect to our ec2 private instance if it goes and tries to talk to the agent and there is no way to route to it it's just going to fail so we should really set up ourn instance first you can see previous attempts here so we'll go ahead and type in natat instance at the top so we know what we're doing uh we can go ahead and choose um Amazon Linux 2023 note that this the version is here um if we drop down we can use Arm if we want I'm going to stick with x86 here just because that's what my cofounder was utilizing and I just want to get through this unscaved we're going to go down below I'm going to switch this over to a T3 micro because T2 is so darn old at this point I don't need a key pair to log in here I'm going to update the network make sure that I'm choosing 106 for our net instance um this one is supposed to be the public subnet because the net instance has to go out to the internet um and that is the whole point there um I'm going to assign it an an IP address because it's absolutely going to need one we are going to create a new security group I might already have one so if I do I just want to delete it first so that it's out of my way so I don't have like naming conflicts and stuff like that we go over here um yeah I have one here I'm just going to go ahead and delete that and we'll go back over to here this will be our Nat instance SG just so I don't have a conflict I'm just going to put two on the end there I'm going to remove this rule here I'm going to add a new Ingress rule I'm going to just say for everywhere for all traffic um for advanced details this is very important as we need to configure our user data now we could log into this instance and do this I have it here it is very similar uh to that article that I linked um this is exactly how Bo did it so I'm going to follow his exact instructions but I do know that this this one could be um separated into into a single line but I'm going to go ahead and just copy this here you know what screw it I'm going to go I'm going to deviate here and I like this line better because it does it in one line right so in here you can see we have we Echo we're just putting in that file then we do pseudo but like I feel like we should just do this um so that's what I'm going to do instead so I'm just going to tweak that but this line is almost the same as this article the difference is that this one says ethernet zero and this one is um en5 so I'm thinking that that was pretty much the reason why mine wasn't working uh in my previous attempts we'll do pseudo yum IP tables install this this CTL here um and then this last one which is updating the IP tables to have uh natat post routing en5 from anywhere it's going to masquerade it that's how you do do that so hopefully all those lines are fine and we'll go ahead and launch this instance from anywhere here sorry so yeah I think that is fine we'll go ahead and launch that instance and we'll have to wait for that to complete the launch but while we're waiting we can update the uh the security here to have or sorry the networking to say from anywhere because if we don't do this that's not what I wanted we'll go back here again under networking we need to update this so that it allows us to receive traffic from anywhere and not just from our our our own IP address and because that's what anat does right and we need to wait for those checks to pass otherwise we cannot um set up the route table we might be able to do it let's go find out but from the uh private route table we need to uh Route traffic to that internet gateway or that sorry natat that natat instance so we go to our route table and we go into our private one here and we go to routes and edit it the question is can we do that or will it wait for that since to launch up we can excellent from 00000000 excellent so the idea now is that if theat is ready we should in theory be able to go ahead and um launch that so you know I'm going to again wait for those checks to pass and once they're done uh we'll launch other instance but while we're waiting we might as well just get that instance ready to go so we'll just say ec2 as we're just doing everything through the terminal here today to make our live super easy say launch instance this one's going to be our private server it'll be Amazon link 2 I'm going to just switch it to T3 micro T3 micro here it is and I don't want to keep pair we're going to change our Network we're going to drop this down to 106 this one has to be in the private subnet um I'm going to use the exact same uh firewall Security Group because it's is going to just allow access from everywhere and so everything else seems fine um and the other thing is that we need to just make sure we give that instance roll that ec2 SSM roll and in fact I actually don't know if I did that for this one so I'm going to go back here and take a look and I didn't attach the IM roll so I'm going to go ahead and do that now instance and I'm going to go ahead and think it's it's in settings security IM roll modify this and I'm going to drop down choose ec2 SSM roll if you're wondering what's in that we've created that Ro in other ones it's just containing the core managed SSM policy so we've made it so many times I'm not going to go and make it a tenth time here so just go look that up if you're not sure um and so when you update an imal you actually do have to reboot it so even though this did um do that I'm going to have to reboot it I wish I had made sure we set that initially but we're going to have to wait for this reboot to finish I really don't trust these reboots so I'm going to stop the instance completely okay I'm going to wait for it to stop I'm going to start it back up okay all right so we're going to start this um instance back up we're going to refresh this here sorry oh it's still stopping okay I guess we'll wait for it to fully stop usually it doesn't take that long for uh instances to stop I'm not sure why it's taking so long all right let's see if it stopped now there we go I'm going to start that back up and so it should in theory run the um uh run the script again again not sure why it's there we go it's starting up so there's that um the thing is is that we did shut it down and start it back up so I don't trust the uh VPC because it's not using a static IP address um this is probably where' be a good idea to actually attach an EIP so I'm going to actually go ahead and do that so going to so ate an ipv or an IP address and we'll go ahead and Associate that associate that with the instance okay and that way it will follow it around no matter where it goes um I'm going to go back over to VPC I'm just going to double triple check the um route tables as I really do want these to be set up correctly and we don't have any issues but I'm pretty sure it would already know to point to the right place so just in case it's not I'm going to go ahead and just reso appreciate it I really don't trust it uh we'll go back and refresh this so the idea is that once this instance is up then we can go ahead and launch our other instance so I'm waiting for those two status checks to pass while that's going let's go back over here and this one is ready to go um so we will just patiently wait for this uh these two status checks to pass okay all right we'll give this a nice refresh here and we have two out of two status checks passing so this n instance should be in good shape we can also connect to it before we again before we launch other instance and just take a quick look here um I don't really remember what I was using for debugging but I think that it wouldn't hurt for us to poke around and make sure our configuration is what we think it is so we'll go ahead and type this in pseudo Su so there's that Cy CTL command and in here we can go ahead and just say display all the variables and one that we did set in here was for um ipv4 I think that's what it was it was IP for uh forward or sorry we'll just say IP forward and see what we get and notice that it's set to zero so even though uh we use this command which which um I utilized here let's go ahead and copy that again I'll paste it in and hit enter pseudo and you know what this is probably why this command didn't work is because I didn't have pseudo in here that's probably why so we'll just say save that so it's a good thing we double check and you know I I would always say that is it's a good habit to always double triple check your um uh your work there so what we'll do is we'll go ahead and um yeah do this again and so now we can see that is set to one the other thing is the IP table so that's something I'm going to want to check um I believe we can do IP tables hyphen LV VL does that would say show all IP tables rules because I always forget what it is yeah it's like hyphen L hyphen V to list all rules okay so there's this paste it in and we'll try this again if it doesn't say just type in pseudo pseudo IP tables so it's not showing wh just take that part out of it nope that's not working that's not working are we sure IP tables is there pseudo IP tables so this is where I don't trust that refresh and that was my major concern was that um this might not be set up so I'm just going to run all this manually because I don't trust any of it pseudo yum hyphen y install IP tables and so that's going to go install tables and then we'll grab this last command here the pseudo IP tables n it did enter so it says it's installed and then we'll try this command we'll try that again yeah I just want to list all of them so we'll grab this I mean we only need IP pv4 rules to be honest so so we can just go ahead and try that again and it's not very verbose so we'll go ahead and say be a little B more verbose and I want everything so we'll do n says that's an illegal operation I mean heck heck they're using it there's The Hyphen l so let's go ahead and try hyphen l this is looking more normal to what I expect what I'm looking for is this command that we set up and I don't think Nat will show up unless we do hyphen tnat so this is another unusual thing that we'd have to do this and just type in clear here again so we do that I think that um I'm not seeing anything do you know what I mean so like I'm expecting to see that that command there but it's definitely not showing up so I'm going to go ahead and copy this command again and paste it in just make sure that it is actually indeed running it notice that it's doing this so it's making me think that I actually didn't execute that command I think it's just because I have a exclamation mark on the end there we'll go back and try this again and so now we have it in here okay so you know I always think that it's very useful to go in and double triple check stuff before we run it so now we should be set up for natting I'm going to go over to our E2 instance and um this one should be set up to launch so we have dc2 SSM roll we have our um same Security Group so it should be fine it allows it from everywhere we are going into the private subnet which is correct we're not launching with a key pair um and so we should be in good shape so let's go ahead and launch this instance and I'm going to go back over to here and I'm going to wait for our private server to completely launch up and then we'll see if we can connect okay all right so our private server is running so let's go ahead and see if we can establish a connection because this is how we're going to know whether this is going to work and as you can see I cannot establish a connection so I'll be back okay all right so I literally called Boo and we went we stepped through everything and then all I did was I did a hard Refresh on this page that's all I did and then I went to to our private sub uh server and I went to connect and then the sesss manager didn't have the error so this is a thing that sesss manager sometimes does and you have to give it a hard Refresh on the page and it's so frustrating but at least I got in without issue now so we go to here and say ec2 instance we should be able to do a w get on if curl or if config domme maybe and I'm just going to go ahead and just change it so it's like hyphen hyphen here and just say hyphen o um because what I'm looking for is just well do we have curl installed yeah we do we we'll curl this instead I have config config me because all I want to do is get back the IP address to show that I'm making it out to the Internet so there you go um maybe not as painful for you but extremely painful for me to do this lab um but I thought it was very useful to know how to do this because you know natat gateways are expensive and if you can utilize a net instance even in a limited case I think it is worth your time well let's go ahead and shut down these servers and once those are shut down we'll then go delete our VPC okay all right so those are shutting down and by the way I still have this session open it's killed now but um if you if you do an if config you'll notice that this ISS en5 and so that was one of the changes from Amazon Linux 2 to Amazon Linux 2023 was that it's no longer uh e z it's en5 for whatever reason any way I'm going to go over to um our VPC now and I think we can go ahead and clean this up delete and we'll delete this and we are now all in great shape I might just make sure that security group is gone because we might have manually created it it is gone as well so that is good but yeah we'll see you in the next one ciao jump boxes are security hardened virtual machines that provide secure access to private sub NS and so this sounds very similar to a uh the adus client VPN uh and it kind of is but um it's a little bit different the idea is that instead of you allowing a secure connection that is verified on your machine uh you have this connection that you don't necessarily trust and it's on the responsibility of this uh virtual machine to do the security hardening before you establish a connection so if you didn't have a Bastion you could technically use a client VPN um and there are some other options that you can utilize besides that I can't remember off the top of my head but just understand there is some variations and some of these Services kind of overlap uh but they just push the level of security in different places um but anyway so ec2 instances are security hardened uh you access private subnet resources via SSH or r r CP um RCP would be if you're using Windows known as jump boxes because you're jumping from one box to another known as Bastion since it's something that gives protection uh against attacks that's what a Bastion is it's something to do with probably like old timey forts um gats cannot and should not be used uh with Bastion so uh I mean we see a knat Gateway here that's why I put in the diagram to understand that the ec2 instance you're connecting to can have a a n Gateway but you wouldn't have one on the actual Bastion right because the Bastion is not supposed to uh go go talk out to the Internet it's supposed to talk within your network now Gateway instances are only intended for ec2 instances to gain outbound access to the internet for such things as security updates so itos does not have a builtin jumpbox service um for those who coming over from Azure they're probably used to be able to spin up a Bastion over there either windows or otherwise um databas Marketplace will have Community or third party jump boxes so we can definitely try to utilize one there System Manager sessions manager can replace the need for Bastion so you know that is another use case and this is what I like to do is have people use sessions manager whenever they can but there are cases where a jump box is better because uh maybe it's for auditing purposes or the level of um uh control you want to have in terms of how they connect but uh yeah there you go hey this is Andrew Brown and this fall along we're going to take a look at jump boxes or bastions so this shouldn't be too difficult it's been a while since I've actually launched one but I remember there being one from the ec2 marketplace that we could utilize so hopefully that will uh work great in our uh scenario here so let's make our way over to ec2 and we're not going to launch instance just yet we're going to look at the marketplace and see what options we have available to us we browse for more Amis oh I'm just going to type in jumpbox and see what we get so jumpbox and and we'll give it a moment to populate so in the marketplace guacamole is the one that I use so I would probably go ahead and use this again um it does have a trial so that is totally fine with me so let's go take a look here and we have one for arm 64 or the regular I'm going to go into this one pricing here is 08 an hour if you do not want to have any kind of expense I totally understand that and you can just completely watch here but I just want to show you how we can go ahead and start using one um I'm going to go take a look at the usage as it's been again a little while since I've done this but I'll be back here in just a moment all right so I think I've read enough but the idea is that we could launch an instance here we are going to need um a specialized instance profile so let's go ahead and launch that so this is a CL foration stack that they have we'll go ahead and hit next and we'll take a look at what it is that is being spun up here before we actually you launch it so we'll go next I acknowledge and it doesn't show me here let's go ahead and create a change set so change set will let us see what it is and I'm just waiting for this to become available so it's going to create an instance profile and I am roll so that seems good enough so we'll go ahead and execute that and so we're going to end up here with a um uh this by the way I didn't tear these down in one of my previous ones so you know if you see anything that is kicking around that you need to get rid of make sure you do that I do my best to try to get everything but not always the best at grabbing all that stuff so that is spinning up um I don't think we need to really spin up a a new VPC for this but um H maybe we will I I haven't decided yet I'm thinking about it because the whole point of a jump box is to get access to something like a private instance so maybe we should spin up a VPC while we're waiting here so I'm going to go ahead over to VPC we're going to create a new VPC I'm going to say this is for jumpbox 10 006 is fine by me 1 AZ one public subnet one private subnet no net Gateway uh no VPC end points we'll go ahead and create this and so now that spun up we created our instance profile it says here enable cross account access the guaca agent is able to discover instances in the account other than the ad account it was launched and it will query the VPC for Transit Gateway attachments or peing connections if the VPC is connected in a VPC in a different account etc etc so it seems like there is a bunch of options here to enable cross Discovery for E2 instances create the following IM roll so thing is like we don't need to do anything cross account as we're doing everything in one place so I don't think that is necessary so I'm going to just skip that we're just waiting for this IM or instance profile to create this takes a little bit of time so we'll wait till it's done okay all right so our guacamole instance profile is ready let's go over and launch an ec2 instance uh so I'll go ahead and select it and we'll say subscribe on instance launch so when I launch it it'll actually subscribe for it I'm going to go ahead here and just type in Jump box as that is what we are launching I think it chooses exactly what we what we need which is the case here yes um it chooses T2 small again if you're not comfortable doing this don't worry about it you do not have to um uh do that one thing I'm not sure is do we have to set a key pair it did not specify anywhere it says instance should be fully booted within five minutes access the browser via Etc so I yeah that's what I remember I don't remember us having to SSH into it to connect to the underlying operating system of ec2 instance connect SSH with the username of ec2 user so you know just in the rare case that I actually do have to get access to this I suppose that we could um download a key pair and use it could we also use sessions manager probably I mean it does log in Via ec2 so I kind of feel like giving it access just in case so I'm going to go over to this instance Ro if it doesn't have sessions manager I'm going to add it to it and it does have a rule for SSM it looks like the older one so technically it should still work but that one's not supposed to be used anymore so for whatever reason they seem to not have updated that so it must not be causing any issues so I'm just going to proceed without a key pair because I'm confident that we can get into that uh what really matters is where we are deploying this so I want to place it into our new VPC and specifically into our public subnet so that looks fine to me I'm going to leave all the other settings alone that looks fine I'm going to go ahead and launch this instance and we're going to wait for that to be ready okay maybe while we're waiting what we can do is go ahead and uh launch our other instance I suppose so no we'll wait okay all right so uh this instance is now launched let's see if we can go ahead and get into it it is in a public subnet and I'm assuming that we actually do have a public IP address I'm not seeing one here which is kind of annoying um because it's not Auto assigning it of course so maybe we can go fix that do we have to restart it to get it can I just get a public IP address here allow secondary private IP address no I just want a public one um yeah so I I usually don't ever have to assign it after to the fact so I'm never really certain uh it's really that change with the uh it not assigning it but um just give me a moment and let me figure it out so it looks like that is not possible so I'm just going to have to generate out an elastic IP address because there's no other way around it um otherwise I'd have to tear that down and sand it up again I'm not doing that so we'll go ahead and generate out an elastic IP great practice for getting elastic IPS I suppose I'm going to associate that with our only instance that is launched which is over here so go ahead and do that so now that is assigned I'm going to go back over to instances and take a look here the status tcks have passed let's go visit the actual public IP address here so we'll open that up uh we'll Advance anyway uh despite it not being technically safe and here we have our okay I get it stop that there we go now we can enter in our username and password so if we go back to instruction it should have some default values it might show in the um Network console where it is but the default user is guac admin so we'll paste that in here and then for the password is the default password of the instance ID interesting so we'll go over here and grab the instance ID I'll just copy and paste it from there and we'll log in and so notice that we have no connections let's go ahead and create cre a connection so nothing interesting to do there we'd have to enter in a connection URI so yeah don't exactly remember how to use this but uh give me a second I I'll figure out the next step for us okay all right so I didn't get a whole lot of help but um the thing is is that apparently you can connect or sorry uh cc2 instances but I'm not sure how it's going to know how to do that if it doesn't have like a key or anything to there you see two instances and I definitely made a video on this and I can't find how I did this before but I'm thinking what we can do is launch another ec2 instance and see what happens also guacamole is apparently a Apachi project so we could have probably launched it on a T2 micro on a free tier if we wanted to configure it but that would have been a lot more work um what I'm going to do is go over to our um GitHub page I'm just going to launch up an ec2 instance and the other one I just want to see if guacamole can at least see it but I I would still think like wouldn't it need a key pair like how else would it uh how else would it work with it what going to do is go over to um press period here and we'll go into VPC and we'll grab that Knack one because that one has been very reliable for us uh this one here yep I'm going to go ahead and copy that and well actually I just want to download it so I'm going to download that to my downloads folder if I have an older one I'm going to go ahead and delete it we'll go ahead and download that that's now downloaded which is good think I might have some uh ones I need to fix from earlier commit have a couple templates from before I want to save but anyway we'll go back over to cloud formation the hard part is like there's no information online and I'm I'm really surprised with what year it is and how much the stuff has been around that there's just nobody making videos on this stuff and you have to piece it together or even documentation out there I should say but anyway what we'll do is we'll provide uh these values in here so we'll make our way over to VPC and we'll grab our VPC that we want to have this launch in and we'll also grab its subnet I want to place this in the private subnet so we'll go ahead and do that um it depends if they're in the same a or not let me just double check here C1 a yeah they are so I think we can launch this in the private subnet we'll go ahead and do that uh my PC2 don't really care what we call this here next and we go down below here and we'll launch this and so we'll wait for that to get up and running okay just reading a bit more here it says and I guess we're maybe using a variation of guacamole because it says gu AWS is capable of scanning the vpcs for instances using the adabs API and we'll add them to the list for connections so hopefully that is what is going to work in our case here the other thing is that they would need to possibly be in the same Security Group so once this is spun up I suppose I can add it into it uh while we're waiting I mean I don't think we have to wait forever for this but I think for our instances uh specifically our jumpbox here we could update our Security Group and add this other one if we have an option here quick security groups go back here maybe it's up here security change security groups and I'm going to go add the other one in here because if it's in the same network then that's going to make things a lot easier for them to communicate and we're waiting for that to initialize we'll go over here and I'm not sure what we need to do to refresh the connection but we'll give it a moment okay if we go back over to here um this instance is now spun up go over over to guacamole I'm not seeing anything I'm going to try logging out I'm going to try to relog back in and see if that makes any kind of difference here go back to the setup instructions here I'm just thinking that if I attempt to log in uh twice that that might fix the issue because maybe it might refresh the list as I log in that's kind of my guess we'll go here and grab the instance ID as the password so I don't see anything let's go over to settings connections groups users history anything how did I do this before that's that's what I can't remember it's like they just don't have any other additional information I mean we can choose how we want to connect so like we could go here and do SSH and and and specify it that way I don't have any SSH information because I I would have to had downloaded the key pair when I launched that up which I did not do but we we do have a bunch of options here yeah so what I'm thinking is maybe I'll tear down this ec2 instance because it's not going to ever appear here I'm going to go ahead and launch a new ec2 instance and what we can do is we can just add the connection manually so we can go here and just add it in if this will uh click off it's not the best interface I might have to just refresh this and so maybe what we can do is just use SS to get in so what I'll do is I'll launch a new instance this time I'm going to launch in the same public subnet I just want to see if it will even appear so my ec2 instance and we'll choose Amazon 2 which is totally fine um I want a T3 micro just that's what I like to have so we'll say T3 micro we will have a key pair so I'm going to go ahead and do that says not recommended oh sorry we'll create a new key pair so before jumpbox we'll download that pen I'm going to go over to uh here and change our VPC to be the same VPC I want to launch it this time the public subnet I'm going to have it utilize the same subnet as the guacamole host and we'll put it in default at the same time as well why not that seems fine to me I'm going to add a rule we created this in another video but I I made a rule so that you can have an instance profile under ec2 SSM rule if we need to SSH into this instance for whatever reason so go ahead and we'll down uh uh uh sorry uh launch that instance so we'll wait for that to launch and then we will see if guacamole picks it up and if it doesn't we'll just then go ahead and create this connection here okay all right so I think we waited long enough for that to spin up it should be up here now um yeah we have our ec2 instance and I mean they should be in the same place we'll give this refresh but notice that we don't see it so what I'm going to do is add the connection and we did we did uh download the um SSH key so we can definitely utilize that so what I'm going to do is go over to connections I'm going to add a new connection this will be my ec2 instance and in here we'll drop this down to whoops this darn click thing you can't like there we go can't get out of there maximum number connections I don't care I'm going to let it assume the default here um the host for the proxy parameters I'm not sure what I need to do here um I'd assume that we would be placing the stuff in here so this would be ec2 user the port would be 22 the host name is going to be the IP address so um since they're on the internal Network I suppose I could just provide the internal IP address it'll just uh let me see what I'm looking at here private IP yep and then there would be the private key so we downloaded that I'm just opening it up in something like VSS code open with no that's not what I want more apps just drag it over to vs code here try this again open it up thank you so here we have a private Keys we'll go ahead and copy that we'll go back over to here we're going to paste it into here uh there is no pass phrase so we don't have to worry about that everything else there's a lot of stuff in here I don't think I need to set any of this stuff so I'm going to leave it alone uh yeah I think this is fine public host key base 64 no private key let's go ahead and save this and I'm going to go back over to here and see see if I can have establish a connection with what I have I guess I must have instantly connected oh cool okay so uh let's say who am I there we go so I I'm inside the ec2 instance that's pretty cool um could we also try to do RDP maybe we should give it a go um it might fail but let's try it anyway because I'm I'm having good Su success there if you're worried about spend do not do anything you can just follow along here but we're going to say my Windows Server it's been a little bit uh a little bit a while since I've done a Windows Server here but on AWS in particular and I lo lost what I was at so I'll have to launch it again so I'll say my Windows Server my Windows server and we'll choose windows and it says 202 base we have free uh free tier eligibility that sounds great I cannot imagine it running T2 micro so Microsoft Windows Server which instance size AWS I know off the top of my head what it is for Amazon Linux or sorry on Azure but for adabs I'm kind of forgetting just give me a moment to find out I just have a hard time believing that we can run on a micro so I think what I'll do is I'll put it on a I'm just trying to decide I feel like I could probably get away with a T3 medium so I'm going to run it on a T3 medium go back here I used to know because I used to do this all the time on adabs but now that I'm really good at Azure I usually run my windows workloads over there but we're going to choose T3 micro or sorry medium this is definitely not free tier so do not use it if uh if you're worried about spend tier um this would use RDP as the protocol to connect so it's not necessarily going to do that here so we don't need an SSH key um I'm going to use an existing Security Group here I'm going to use the default one oh well sorry we'll switch over to the other network first and I'm going to use the existing ones this is going to be the guac moly and the default one and we will scroll on down I'll go ahead and hit launch I do not want to well hold on here no yeah yeah we can't get an RDP value here so we'll go ahead and just uh download this we'll say launch instance and we'll wait for this instance to launch it takes a little bit of time but I'll see you back here in just a moment okay you know what I actually do think we need the pem so I'm actually going to go ahead and tear this down I'm just so used to having like a one click to connect to RDP that I was wasn't considering that maybe it does use the same type of uh key so I'm going to go ahead and relaunch this and we'll just say my Windows Server 2 just in case it conflicts with the one that is spinning down I'm going to choose Windows I'm going to go with T3 micro or T3 medium sorry T3 medium I'm going to choose the existing keypad the jumpbox one I'm going to switch our Network out I'm going to go to our other one I want to make sure it's in the public subnet um I'm going to use an existing Security Group so the default and guacamole I'm going to go down here and launch the instance now could we have placed that other one in the private subnet absolutely and it would have worked but um I just wanted to rule out other other issues there and so that's why we did not launch that there so this will now wait for it to spin up and I again I think that we need to use it because I'm pretty sure there is a pem key involved with um RDP but we'll see here in a moment while we're waiting I'm going to just go take a look at the connection information um I think I just closed it in guacamole so we'll have to reopen that a lot of junk tabs so we'll just close a few out here and I want to go back to our jump box and we'll open up this address again here and we're already in here so we can see that we have this connection it's a recent connection I'm going to go over to settings and we're going to go to connections and we'll add a new connection here and this time in instead of this root thing is annoying uh we'll choose RDP which just say my windows connection down below we have a few options here so yeah not 100% sure what if chose already be encryption not sure what to do for this but we'll give it a moment for it to spin up and we'll figure it out okay while it's been launching I just been reading about connecting to your windows instance so we'll go down here and look at RDP so it says the default username of the administrator account depends on the language of the operating system contained in the Ami to assertion the correct username identify the language of your Ami it's going to be English okay so I'm not worried about it so it's going to be administrator we'll go back over to here and so I'm guessing what it's suggesting is the username is going to be this uh what is the port for RDP RDP Port I used to not it off the top of my head it's 3389 so we'll go ahead and grab that and we'll paste it in there the IP address is going to be whatever it's been up with um well actually it'll have a private IP address so we don't have to worry about a public IP address in this case which is down here apparently it's already running so I'm going to grab that private IP address and paste it in here what the password is I don't know I I keep expecting to having to put the key pair somewhere it could be dependent on how we have to establish our connection but but I'm not actually sure the default username of the admin account depends on the language of the OS to insert it that Etc if you've joined your instance to domain you can connect to your instance using the domain credentials you defined in adabs directory service on the remote desktop login screen instead of using local computer name and generate password use the fully qualified username of the administrator okay mhm H uh let's go over here and try to establish our usual connection so we hit connect we have the rdb client here administrator yep so we already know that you can connect to your windows instance using the remote desktop client of your choice by downloading running uh running the RDP shortcut and that's normally what I'm used to utilizing is that shortcut file I'm going to download I'm just curious what the contents of that file is so I'm going to drag that over into um vs code and so here it just shows our full address autoconnect username so nothing very interesting here very simple straightforward stuff but what would the password be so here it shows password oh here it is get password either upload the private uh key file or copy and paste the contents below so we have our private key we can just drag it on in here um or you know I'll just get the contents of it I already have it open over here so just grab it all copy and then we'll paste it in it's good thing we have the uh the key there and so that is our password we'll copy that we'll go back over to guacamole we're going to paste it into here I don't know what I'm supposed to put as the domain I'm going to just save it and see if it works save it does seem like we would need more stuff though going to go back here and we'll go over to our active sessions or however we want to get back home and is this enough to connect I feel like it's not I feel like I'm missing something so noticing that nothing is happening yeah there we go so it's not working so let me go figure it out I feel like we have to fill in the domain and some other things it does say here if you've joined your instance to a domain which we never did you can connect your instance using the domain credentials which we don't have you have to find in the a directory service which we did not launch on the remote desktop login screen instead of using local computer name and generated password use the fully qualified username of the administrator so that's something we don't really have to worry about um so let's go back over to here to guacamole and I want to edit this connection I don't want to actually uh get into that connection I'm going to go back to my settings I'm going to play around with this here off screen so you don't have to worry about it okay and I'll be back in just a moment well I looked it up and apparently everything that I've been setting is actually exactly what somebody else would set so um yeah not 100% sure why it's not working we do have it set to RDP here but I just saw somebody else do it and they filled it out exact the same way but they weren't doing guac they were just or uh go guacamole's AWS version of it they were just doing regular stuff um I don't know if we'd have to set encryption for this we could try that doesn't seem like we should have to do that we'll try connecting to this again here you know it's not a big deal we did achieve the Linux side of it but it would be really nice to be able to establish a Windows connection one other thing we could do is we could try logging into um it through here by well we need our password so I'm just going to copy that and place place this in a new file here for a second new text file but uh you know my my one thought here is we could try to use RDP to connect to it you'd have to have the RDP client installed I'm pretty sure that I do or if it's Windows it comes preinstalled I think so we'll go ahead and connect now the only issue with this is that this is going out to a private uh subnet so it has no way of actually um connecting to it so that's going to be a problem for it so for this to work I would have to assign it um an IP address and I can do that really quickly here I'm going to just go give it an elastic IP very quickly go ahead and do that and then I want to associate it with our Windows server and so now if I go back to our instances and I attempt to connect to this down here it should now have a public DNS which is great apparently we're going to have to keep doing that every time we want to get our password here we'll copy that just copying that off screen we'll paste it in and we'll decrypt it just making sure that's the same one as before it looks like it is good okay and I'm going to just download this again again I don't think it's changed well actually might have because the other one was uh a private IP address so I'm just going to double click and see if it can establish a connection because I can just rule out that uh issue that it's not guacamole it's something that we've done here the remote desktop cannot uh connect to the remote computer for the several reasons but let me just think about this for a moment so wouldn't the RDP instance have the private the public key that um that we upload yeah it would so it should work but anyway I don't think it's guacamole I just think that we just in general can't connect to it so something is wrong and it's not 100% sure what it is so just give me a moment to figure that out okay one thing I'm wondering is if we actually have RDP uh access because this instance here has a security group that might not allow us to do that here it's for 443 2280 um you know what that could be the issue because we don't actually have the ability to um have access to that Port so maybe there was no issue with guacamole and it was our security group this entire time so what I'm going to do is edit our inbound rules I'm going to add a rule we're going to go look for RDP it should be in here somewhere there we go and I'm just going to say from anywhere and we'll save that rule let's go back back and try this again maybe it will work reconnect nope but since it's part of this one it should expect to uh be able to get um a connection from anywhere now so let's go back over to um our instance where we were trying to connect to it I'm just going to click it again that file that rdb file okay and so now it's actually prompting us so I think that's a good indicator that we can at least connect to it let's go ahead and try this paste okay making sure our password works so it's connecting so that means um again this isn't through guacamole this is just me using uh Remote Desktop Client which again is preinstall on Windows if you're on Mac you got to download it so we'll go ahead and just close this because I'm already aware that that connection is working I'm more interested as to why it's not working with guacamole so I'm just going to go back into our connection we did play around with it and I did set um I want to edit the connection I don't want to reconnect I did set in here this and maybe there is no encryption security mode so we'll go back and save that and we'll establish our connection again or or attempt to here reconnect and it still does not like it we'll go back to our connection information so the port is correct we have the correct um IP address I don't think we should have to use a public IP address but I'm just going to rule it out really quickly and just make sure that we can establish a connection to rule out like inter inter network issues I'm just going to give this a refresh here we'll grab this one we ideally do not want to use the public one but uh whatever is going to help it work we'll go ahead and save this we'll go back here we'll attempt to connect again it does not work even with the public address so to me it has to be some kind of setting in here that we're we're missing that we're not aware of um I don't think we need to set a domain because it's not connected to domain I don't know if we need a security mode because it did not specify it so I'm not sure what it wants as the security mode I'm going to go see if I can figure it out look I'm gonna be honest with you I don't think it's worth our time to figure it out um I could probably ask bo bo would definitely be able to figure it out because he's really good at Windows servers and so it has to be some configuration in here that we're missing um as far as I can tell we are doing it correctly and I mean we're obviously not there's something missing but there's no information out there and we'll be sending hours on this but we did establish a connection through SSH so we generally understand how this is supposed to work so what we'll do is go ahead and just spin these things down we'll be all done uh with this here and we've have done our best that we could okay and we'll terminate these instances here so we'll let that all terminate and once that is done uh we'll go and we will then tear down the VPC okay so we'll just wait here till they're all stopped all right okay so go ahead and give this a refresh we're going to go ahead and tear down our VPC and uh yeah that was basically jump boxes the best we could do it here today um and you know other other clouds make it a little bit easier um both Google and uh Microsoft you know they have managed managed providers or services for this but it us uses an open source one here but of course sessions manager allows you to skip that and using RDP via the Windows desktop server is usually the way you want to connect anyway but having a jump box you can have a lot of additional uh security parameters added to it so you know depends on what your use case is but anyway uh before we go let's not forget about those elastic IP addresses it's very easy to forget about those so I'm going to go ahead and release those and I might as well just get rid of the key pairs as well because I just want to not have a big mess of stuff so I have some key pairs here that I'm definitely not using we'll go ahead and delete them and we'll see you in the next one ciao Amazon VPC is a fully managed application networking service that you use to connect secure and monitor the services for your application and if that doesn't make any sense I have a more uh human readable uh description which is easily turn your adist resources into services for a microservice architecture this is a GameChanger because if you've ever wanted to utilize microservice architecture but you found uh doing Mutual authentication uh very difficult there setting up kubernetes very difficult you kind of get this um uh fully managed version by AWS uh that can get you kind of comfortable with the idea of microservices so this works with a single VPC works with multiple vpcs you can use it across accounts you can be used with multiple accounts it performs n for ipv4 IPv6 and overlapping networks now you've heard me say before that IPv6 doesn't need a natat but it does need a natat if you are uh taking traffic that is from ipv4 to IPv6 or vice versa so that's what that natat is doing it integrates with IM IM we have weight routing for traffic such as blue green or car Canary style deploys it uses HTP uh https for the service to service communication and this is where we hit our limitations because when you look at microservice architectures they like to communicate with things like uh gcpr grcp always forget the initialism RCP grcp grpc there we go um it supports custom domains which is very useful you can bring your own SSL and TLS certificate this also really replaces um probably it probably leverages underneath but like service mesh so ad service mesh was just not a very good service there was service mesh in cloud map when I used to uh when we used to do um those kind of microservice architectures when we did the cloud project boot camp and this was not out yet and I wish it was because it would have made our lives a lot easier here's an example of the components so you have your service network That's The Logical container for all your stuff um it can be associated with multiple vpcs as far as I understand um you have listeners so this is starting to look a lot like um what do you call it uh load balancers but just understand these listeners and these rules are not the same thing as uh application load balancer so you have a protocol a port to listen to uh on what you want to listen to you can have up to two listeners per service a listener contains writing rules and has a default route so very similar to application load balancer you have a Target group this is a collection of resources for a very specified type so they can all bc2 or all IP addresses or all Lambda functions or all albs it says K pods in the documentation but when you go and choose the options it's not there so I imagine you are choosing K pods based on IP addresses or something else so there's something else going on there and hopefully you can see that I'm trying to represent each Target group as very unique things here um and then there's service directory so a central registry for all VPC lce services that you own and are shared with your account through the adabs resource access manager so adabs Ram I don't really have a dedicated section of ram in this uh in this course but uh you know we'll just keep coming back to Ram because other services seem to keep utilizing it in interesting ways but uh there you go hey this is Andrew Brown this fall along work we going to take a look at VPC lattice which allows you to basically turn your services into microservices I say services but I just mean any kind of resource that it can Target um be become a service let's see if we can set this up won't be something complicated but it'll be something that we can start working with right away I'm going go ahead and type in VPC lce and we'll make our way over here and the idea is that it will create us a service network so we'll go ahead and create ourselves a service network I'm going to call this um my lattice and that seems fine here we can choose any existing vbc lot of services I don't have any yet so we'll go ahead and go down here I'm going to go ahead and add a vbc I want to work with the default one here today so that will be the 172 3100 for network access um we just not authenticate so basically things can easily resolve for resource sharing PPC last integrates with uh it was ramped and enable resource sharing across your accounts I'm not really worried about that today we'll go ahead and create our service network and see if this is uh pretty easy to start working with so is this up yet we have our vbc association here so that's good um so I guess the next thing is we need to do is associate some services so in order to do that we'll have to go ahead and create a service so we'll have a service name we'll just say web server and this will be a simple web server we can set up a custom domain I'm not worried about that here today we're going to say none for the service we don't need to have any kind of off type here um and I'm going to ignore the resource sharing for now we need to define a listener this will be on htttp Port 80 um and we'll need a Target group we don't have one so we'll have to create one surprised they don't let you do that right in place um I know says that everything's always like simple and then better later on but I think should do a better job of uh making their services a bit easier to use because they're such a large company there's really no excuse for it we're going to make our way over to lattice here and we need to define a Target group so we'll go ahead and do that I'm going to want to Target instances I'll just say web server and we'll have htps this will be for Port 80 because we're not going to be doing anything secure um I don't care about which protocol it is it's using the default subnet and it is going to do a health check at uh HTTP for Port 80 so that looks fine to me we'll go ahead and hit next it wants to associate some instances so I suppose we should spin up one I'll go ahead and open up a new tab we'll make our way over to GitHub and I'll go over to ad's examples again never used lce before but I've used um things very similar to it uh so for me it's just like launching up the application load balancer working with service mesh so somewhere here we have um an instance that we can use probably the knle one's going to be a good one because it has the ability to log in as well as it sets up a web server for us so I'm going to go ahead and download this template before I do if I have any other template files I'm going to delete them out of the way so we'll go ahead and download this so that's now downloading great I want to go back over to cloud formation and we'll go and upload or create this new stack so I'm going to go ahead and drag this one on over turns out I do have another template somewhere I'm not sure where it is but that's totally fine and we'll go ahead and hit next this will be my lattice service and there's a few things we want to change here I want this to be T3 micro I want to make sure we choose the default subnet default VPC so we'll make our way back over here in the new tab over to um to here to VPC we'll go ahead and to go to vpcs and we'll grab the default VPC which is the 172 we'll grab that we'll make our way back we'll paste this in and I'm going to go back over to subnets I'm looking for the 172 one so we have 172 zero this one seems like a good one here I'll grab that one and we'll go back to here and paste this one in paste and we'll go next I'm going to go all the way down the bottom hit next I'm going to go all the way down to the bottom and hit acknowledge we're going to go ahead and deploy this E2 instance we'll wait till it's done all right all right let's see if this is done um we'll go ahead and give that a refresh so it is done uh I want to go over that instance just make sure that it is a good working condition does it need to have a public IP address I'm not sure but let's let's go ahead and take a look and make sure this works don't see a website yet let's go make sure we switch this over to htcp does this resolve I think it's going to resolve it's still working on those status checks so try this again we'll go check our security groups everything is open so there shouldn't be an issue here maybe we're just being impatient so we'll wait for the status checks to complete all right oh you know what within a second it resolves so that is good um yeah I'm not worried about those status checks let's go over to our Target group we're going to uh refresh here we're going to register this one here um available targets we'll do that we'll create the target group give it a moment here to create the target group not sure why it's so hard to make one but we'll wait all right so it created it there um down below we have a register Target says the uh the health check has not been used the target group is not configured to receive traffic interesting traffic from the service what the heck does that mean the target group is not configured to receive traffic from the service okay how is that possible oh you know what it's because uh we need to associate it over here that makes sense um so down below we'll give this a refresh interesting they don't have a little icon here but we'll click that um any additional rules no I'm not interested in that right now we'll go ahead and hit next we'll choose our VPC lattice Network we'll go ahead and hit next review it all we'll go ahead and create that VPC latus um service we'll give it a moment and so my thought is is that we're going to have some kind of way of accessing um this service over some kind of DNS address let's go back over to our Target group and see if it goes healthy or not it's healthy okay so we're in good shape then um let's go back to our L of service and we'll check box this on and so my expect uh my expectation here is that we have some kind of um service address because you know that has to be a way for us to Route stuff I guess the question is like why would it be external is it only for internal Mutual um communication because if that's the case we're going to have to do a lot more to set that up you see a lot of service defines routing mechanism uh receives from services to its assoc Associated ones because what I'm thinking is that the idea is that we can just then um use web server and then whatever it is to communicate with those Individual Services I'm going to go down over to service networks access there's a domain name over here okay so again I I thought maybe there might be a domain name or there might not be so if we go here would it automatically route to that how would it know to route to that uh we'll go back over to our service network and also if we had a service then what would its domain be would it be like a subdomain on the end on the front of it I'm going to go into uh this last Network yeah there's a domain name over here so we go back here and take a look is there a difference or is it the same domain well we'll try this one I think it's the same though all right so let me see if we can actually access access anything from here just give me a moment yeah so I I think it is exactly what I think it is which is all these services are within the service network and so that address that we're seeing is really something that would be used internally so to in order to test this I think what we'd have to to do is have another service and then we'd have to I guess ping the other one so technically what we could do is um spin up a second Cloud information template and test it I suppose it'd be nice if we had a custom domain as our domain is really really long there um but uh let's go ahead and try try something else here so what I'll do is I will go ahead and try to spin up another service again this might not work but uh I figured we'll try try to give it a go anyway and to have another service I need to make my way over to cloud formation so I'm going to go create a new stack I'm going to bring over um this template here I'm going to go next I'm going to say um service 2 we'll make this T3 micro I'm going to have to make my way over to VPC we're going to have to grab our VPC ID which is here and we're going to have to go back over to our subnets I think we were using this one before yeah we were good and I'm going to go ahead and hit next and we'll say next next and we'll acknowledge that and so we will have this one deployed here in just a moment okay all right so our second service is now spun up let's go ahead and um go to the next step which would be uh making that other service available so we would have to go over to vbc lce and we'll add another service and I mean it is another web server we'll just say create and we'll say web server 2 and then down below um we'll keep on going down be really nice if we could have a custom domain but um could we do if you supply custom domain you must configure DS routing after your service is created yes you don't have to fiddle with more stuff to do that but um work with the domains that we already have we'll go ahead and hit next uh we'll go ahead and hit add for listeners we need to create ourselves a new Target group just specifically for this um service we'll go ahead and create a new Target group this will be web server 2 and we'll go to http uh and we'll say 80 we'll go on down below here we'll hit next um and we need to choose a second service which is over here we can tell based on its Security Group not based on its name that's for sure and we'll go ahead and oh hold on we'll have to include the pending Below on Port 80 create that Target group and now that Target group exists so we'll go back over to our VPC we'll give this a refresh we'll choose our other web server we'll hit next we'll say on Port 80 um we'll go next we'll choose our VPC latus we'll hit next and we'll create the latus VPC so now my thoughts are we have these two services and the idea is that we should be able to use that internal domain in order to Ping each other or at least get the website so my expectation is that um uh The Association address or whatever it was there is going to be different so yeah we go routing here doesn't really mean much to me let's go over to um lot of services here and so these I would expect these are different and it looks like they are because I'm looking at these and they absolutely look different uh let's go back to um rc2 instances we're going to have to log into one of them and I'm hoping that we can just do a simple W get to one of them and it'll just work fingers crossed so we have two of these here this is going to be for uh well this is the VPC I wanted to go to ec2 sorry there's four instances as you can see B is uh working on something else for me here but uh what I'm looking for is Security Group here cuz it's going to tell me this is service 2 so this will be service two and this will this will obviously be service one and so now the idea is that I will log into this one pseudo hyen ec2 user and so now I'm logged in and we're going to make our way back over to VPC latus and we're logged into server web server one so the idea is we want to grab web server 2's uh address there this is kind of mucking up so I'm sorry for clicking all over the place here we'll go back to our service network and we'll click into my lattice and what I'm looking for is that domain name of the second one down here great so we'll go back over to here and we'll type in w get hyphen qo hyen paste and so that's what I'm thinking that this should work and it does so we just uh did some service to service communication in uh in a real use case it' be way more complex than this because we would have uh I don't know if it has Mutual TLS it wouldn't because I guess it's using htps but the idea is that there would be some additional steps uh for security and um we'd have a custom domain and there'd be more uh interesting services like having a database but this is a lot nicer than uh cloud map and service network um uh that I used to use with adab so I really I really do like like that uh this offering is here but we are now done so we have achieved what we want to do I figur the best way to tear this down is to First tear down the services and then we'll tear down the target groups and slowly work our way backwards so we'll go here and say delete the service confirm and uh something we have to do first has one or more Associated networks okay so we'll have to go to the network and remove the associations first delete Service Association confirm this is not going to be fun to clean up confirm and then we'll go down here and delete this one confirm good great and then we'll make our way over to uh our services and we'll tear down these Services delete confirm delete confirm so that will be good here in just a moment okay so those are now gone we can uh get rid of our Target groups delete confirm it's registered with uh listeners so we'll go back to our service network and we'll go to our listeners I suppose wherever those are um I mean that would have been through our service associations maybe the issue is that we we have resources running why can't I delete that will it tell me again confirm you cannot delete Target Group after you deregistered all of it uh uh delete this target Group after you deregister all of its Targets target target groups have targets within it okay so we'll go here here and we'll just deregister this and then we'll go ahead and delete it we'll make our way over to here and we'll deregister it and we'll delete it confirm and so that is now gone we'll go over to our service network and hopefully it'll just let us delete it yeah of course make it making us do everything by hand such a great service but like they've made this a little bit easier it's a little bit silly AWS and uh refresh this great can I now delete this refresh delete the service network confirm and I'll make my way over to ec2 we will actually we'll make our way over to cloud formation we'll delete this stack and we'll go over here and delete this stack and we'll wait for that to complete all right so those are torn down and I guess that's basically everything as we didn't create a VPC in this one and I'll see you the next one ciao a Transit Gateway is a network Transit Hub that you can use to interconnect your virtual private cloud or VPC and on premise networks so Transit Gateway leverages a resource manager it operates as a Virtual Router at the regional level you can attach up to 5,000 vpcs to each Gateway and uh for each uh connection you're making to each VPC it's going to spin up an eii that's how it's going to do that VPC to Transit Gateway communication I think that's kind of important because that could factor into costs or hitting particular limitations if you um are restricted on the amount of Enis you have in your VPC each attachment can handle up to 50 gigabits per second of traffic you can attach your ads VPN connections to a Transit Gateway you can attach Direct Connect connections to a Transit Gateway you can attach thirdparty virtual appliances via transit Gateway attachments and this can be a source and destination for packets so it's by directional you can peer connections with other Transit gateways you can do a lot with Transit gateways and I do have a diagram it's in the next slide I could not fit it on this slide here uh at all but I just want to point out third party virtual appliances we talk about these when we talk about um Gateway load balancer the idea is that you have let's say a virtual machine it runs some kind of um security security service and all your traffic passes through it to you know filter out traffic or do whatever kind of monitering your detection so um the point is that Transit Gateway can serve that as well but you can also use Gateway load balance um in different scenarios but here is the diagram I could not be bothered to make my own I grabbed this again from the documentation as it seemed to cover all the cases but the idea is that imagine you have Transit Gateway here and you're able to connect um multiple vpcs so that they can talk to each other the great Advantage is that it's a hub and spoke model so you can just connect as much as you want and they can um uh talk through there you can obviously connect it this Direct connect as it's showing there you can use a Transit Gateway peering to um appearer to another Transit Gateway so you can connect uh a bunch of vbcs between two Transit gateways you can see that it's going to this virtual Appliance here so that's a possible connection you'd use that um it doesn't show it here but there's that uh Transit Gateway um attachment which would be intermediate and then they're kind of representing the uh the cloud Wan right CU we said that cloud Wan can uh get under there where you bring your you connect your on premise Network through a VPC to your Transit Gateway so a lot going on there but my point is is that Transit Gateway is a lot more flexible than your standard pairing um but yeah you have vpcs then you have your VPC pairing then you have Transit Gateway and then you have VPC latus which is more specialized so you can see that there are a lot of cloud services that do similar things you just got to choose what's going to work for you okay hey this is angrew brown and this fall along we're going to look at inabus Transit Gateway as a way of doing uh perer connections it has a distinct advantage over uh regular VPC pairing in that it can use a hub and spoke model which gives you a lot of flexibility for um connecting vpcs it's not limited to just four connected to a single VPC you can have as many as you want uh uh in in theory uh because the number is very high but there is a cost for Transit Gateway as we're seeing 0.05 for Transit Gateway attachments that's if you want to do virtual appliances um and there's an additional charge here for um 1 cent is that 1 cent it looks like 10 cents but it's 1 cent 1 cent or 0.5 for attachment so um that can add up very quickly so you have to really consider if you want uh or or would benefit from utilizing trans gateways but um let's go ahead and see how this works I do have General instructions here looks very similar similar to appearing we're going to create a Transit Gateway attach your vpcs to your Transit Gateway add routes between them I'm going to do everything through click Ops here because this is a newer service and um I honestly haven't done much with it so let's go ahead and do this I'm sure it'll be very easy what I want to do is create two vpcs so that's what I'm going to go ahead and do and I'm going to say create a vbc here this will be a and I want to have for this I'm just thinking about this for a moment um we'll just have we'll have no private subnets we'll just have a one public subnet for each of them we'll say none here I'm going to go ahead and create this one we'll view our VPC we'll go ahead and create ourselves our B VPC and so we'll say 12 do well hold on we'll go to VPC more first but we'll say 12.0.0 2016 and we'll say 1 a no private subnets none uh nonone VPC endpoints we'll go ahead and create that I hope I didn't create the other one there uh the uh VPC endpoint for other one but now we have two vpcs and the idea is that we want to peer these together if we just refresh this here um this is actually B so I'll just rename this to B so I don't get confused there we go and so um the idea is that I I want to deploy an instance in one but for now I'll just set up the the transit Gateway between them so I'm looking for that down below here and I believe that it would be this so Transit Gateway and we'll go ahead and create our Transit Gateway so just say my Transit Gateway and uh we'll let it use whatever the default ASN is I'm assuming if we leave it empty it will default to something so it says you can use the default ASN or provide a private ASN so we'll let it use the default there we have a bunch of options here seems like something to me transit Gateway cider blocks say it's optional you can specify a 24 cider block or larger um you can associate any public private range um not exactly what I need to do here so I'm just going to leave it alone and a optionally set it here Auto accept shared attachments um we're not doing attachments per se we I mean we're going to definitely attach few BCS but I don't think it's the same thing as other stuff here so we'll go ahead and create that Transit Gateway we're going to wait for that to be created we'll be back here in just a moment all right so after waiting just a few minutes our Transit Gateway is ready so we're supposed to be able to attach vpcs to it um so let's take a look here and see what we can do we can modify the transit Gateway share it but what we really need to do is actually attach those uh vpcs let's go back and take a look at the instructions here attach your vpcs um oh we do use a Transit Gateway attachment okay so we'll open the VPC navigate the transit Gateway attachments and attach them that way all right so on the left hand side we'll go down to Transit Gateway attachments and we'll create the transit Gateway so my attachment a this will be for oh we attach other things yeah we know that because we saw in the documentation earlier or in our slide sorry so we have a um and that seems fine to me so we'll go ahead and create that one thing I didn't realize is that they're probably in different subnets which I don't think really matters we will then go ahead and create another attachment so this would be um my attachment B and we'll drop this down we'll choose B in this case and they just happen to be in the same subnet so that's great so now those are connected I think that we needed to do something else for um routes so a route table includes Dynamic static routes Etc configure a route table that has a destination for nonlocal routes and Target the transit Gateway attach attachment ID so we'll go over to our route tables and in here we have this one supposed to be B but that's just what happens when we uh name things a certain way we'll go into this route and the idea is we want to route from everywhere to our Transit Gateway Transit Gateway and why am I not allowed to select one here I can give this a hard refresh so I'm not 100% sure as to why it's not showing up as they said that we should be able to just add it Transit Gateway there it is if you don't trust just always uh you can go ahead and do that we'll save that there are errors indicating the route z0 already exists we'll go back and um I guess the idea is that we're not supposed to put 0 here we're supposed to put whatever the destination we want it to be so that's going to be whatever the transit gateway address is I believe so here it says that Transit Gateway destination enter the destination range address range so we'll go back over to our Transit Gateway and its IP address range is what does it tell us m what would we enter in for that so you know I would I would assume that we would have entered in uh the IP address of the trans G we just give me a moment okay all right so one I thought I had is if we go back over to our attachments I wonder if we can attach something like an igw now I don't think you can but uh I don't really remember what the attachments were so let's go take a look here we'll create a Transit Gateway and no we don't have that so I was thinking that that that might have been an option so let me just keep looking okay all right so I'm not 100% sure but I'm going to go ahead and take a guess here and do my best and so what I think is that we're going to edit these route tables and we're going to just Target the other address so this one's 10 we'll say or 12 we'll say uh 10 here 10 16 we'll go ahead and try that again never said I was a wizard at networking but uh you know it was long as we can get through this here we'll be in good shape we'll go ahead and edit this route table 12.0.0 do0 for6 and this one will go out to our internet gateway and so I'm thinking that maybe that's how we communicate with each other by saying hey uh we're looking at the destination here in the internet gateway and it's going to Route it uh to the correct uh VPC I was thinking that because it was a cider block that maybe you specify the CER block of the um uh the VPC and then you can treat everything as the same network I'm not 100% sure on that and I'm not sure if all you even test that to find out but uh I think that's sufficient we'll go over to GitHub okay and we'll go over to our repo as per usual so I want to go to itus examples and we need um we need two different um uh instances and I think what we can do is go over to period connection because these are exactly what we need so I'm just going to off screen here open up my downloads folder if that's okay with everybody and once I have that going I'm going to just hit period here I'm going to download these two templates and um what we'll do is go over here to ec2 or already to cloud formation apparently I have a ec2 from earlier unless this is Boo let me just double check here as this time might indicate something B was doing this is from 2017 is this cc2 instance still here I just couldn't delete the security group because maybe it was in use or something I'm just going to clean these up I'll be back here in just a moment okay anyway sorry about that but we'll go ahead and create our stack here and so we'll have to have one for a and one for B so for a we will upload obviously the a template see you see 2A Transit um this stuff will have to be updated so that is a minor thing that we'll have to take care of here make our way over to our bpcs and then I'll also need the subnet next next I acknowledge submit and we'll go ahead here and do the next one and we'll go ahead and hit next uh before I do that I'm just going to put put Transit on the end here and we'll create that one and so we'll just wait for those instances to spin up okay all right so those are both deployed and one of these are going to have our um our server installed other one's not so I think a since has instance profile is going to be the one with um the server that we're going to log into so we'll make our way over to C2 and I'm just hoping that this works that' be great if it just does and so one is a and one is B which is which uh that is the question so here does it show our resources here um this instance is a okay so that is um login like server and then this one is uh website and we'll go over here this one we'll see if we can connect to it hopefully we can without issue it's always great when we can connect to it we'll say pseudo suyen ec2 user and we'll type w get we learned in another video it was like q0 is what we want to type Q for quiet zero to display and then uh the trick of it was that we needed to have a hyphen in between here so go here hyphen q0 and we'll hit enter and so we are hitting that uh public IP address the question is can we hit the private one because if the private one works then we know that the transit Gateway is working that's how we would know so do go ahead and do this we have to type it right it's not going to work um I have to make it a zero or sorry an O and not a zero so it's not uh working and so that means that there must be something wrong with my Transit Gateway so just give me a moment to figure this out all right all right um oh sorry yep still working on it apparently there might be something called rep propagation that we need is set um so let me see if I can figure that out let's make our way over to Transit Gateway again so we'll go back over to our uh VPC section here VPC and left hand side I'm looking for Transit Gateway I'm going to go ahead and see if I can modify this or just see if there's anything for propagation we have propagation route table default rout propagation route table so it seems like it would be propagating um I can't imagine there's any other setting in here automatically accept cross account attachments we aren't doing that so we don't need to do that let's go over to our prop our route table propagations if we click into this yeah I'm not really sure why so let me just debug it a bit further you know one thought I had is maybe it's an issue with our um security groups now I I can't imagine that would be the case because I would think that would allow traffic from anywhere let's just go take a look and double check our ec2 instances for our our um security groups and see what is allowed uh to be occurring here so if we go here and take a look at our server and we take a look at networking and uh or sorry security security we're looking at inbound rules and says allow from anywhere from anywhere so it's completely wide open uh there should be no limitations here we'll go over to our website we'll go to security and this one is saying inbound from anywhere and from anywhere just because I'm not 100% confident with this one I'm going to go ahead and edit it yeah it's from anywhere so there's no reason for this not to work and they both have uh internet gateway so they're both going out to the Internet so that's really interesting I'm going to go back and log into the the2 instance and just see if we get some kind of different result maybe it will just suddenly work for no particular reason but it's suggests that it autor propagates so I'm not exactly sure what else it would want pseudo suyen ec2 user um and we'll go ahead here and we'll type something um W get we'll go ahead and try that again from the other perspective here I'm going to grab this public IP address which we know already works but I'm going to go ahead and try it again W w0 or o sorry hyphen and then paste this in the link or the IP address or sorry Q qo There we go and now we'll just switch that out so this will be 12311 12.03 11 double check that that is correct we'll hit enter and it's not resolving so I guess there is something wrong here I'll just keep looking okay um I mean we are using ipv4 so this could be an opportunity for us to use the reachability analyzer I'm not sure if it'll actually work in this here use case but I'm going to go ahead and try it anyway and maybe it might tell us something so what we'll do is we will go to um network manager and then from here we have the reachability access analyzer and I'm going to go ahead and create a new analyze path we just say Transit Gateway and I'm going to go ahead and choose so we have our source like where is it coming from and the idea is that we want to reach one instance to another so I would say this instance uh this from the server and then our path is going to be from we could choose attachment I'm going to try this first and see if that works because what I'm interested in is whether cuz like if we could choose the other instance I'm almost certain that it would say like yes we can reach it because it might go over out through the internet to find out right so I guess we'll do a simple one here and just say instance to instance and see what happens but I suspect that should work and if that doesn't work we'll try and say go to attachment Gateway or if it doesn't give us any useful information we'll we'll try that that next type of analysis okay all right let's take a look here at our reach ability status and see what it might show us what's useful is it will show us a path so here we start with an instance uh 12 0 13 11 so this would actually be oh have I been getting this backwards the entire time so we have website here this one's 12 this one's 106 17 wait a second wait a second um we can connect to this one but we can't connect to that one what what's going on here is this the hold on here is this one with the website no have I just been doing it backwards this entire time so at least we know what's going on because it says server 12 12311 and that's what we've been putting in as our address so maybe our problem isn't the fact that it's not working the problem is it's using error maybe so we'll go ahead here and just type in clear and I'm going to just try this again okay this is server jeez and this is [Laughter] website yep and uh figures cross this just works I'll be happy if it does I don't care if I was wrong I just care that I figured it out enter and we didn't get any errors per se fill connection we're getting Port 80 I'm going to go back over to Here For a Moment let's grab this public IP address just open it up another tab does this resolve into a website which one's the one with the website is it suggesting that the same server is the website because if it is that's really silly um I'm going to go ahead and open this again all right so the instance that I can log into is the same one that has the the instance profile no no so this one is the website okay so this one is the website that is confusing so this is the website which is on 12311 and this is the server so I'll change this back to normal here I'm going to close these out make sure I'm not confused we're going to test this one more time I'm going to connect to the server I thought maybe it wasn't because the other one wasn't showing the um the gray check mark but uh maybe again I'm just I'm just confused here today we're going to go ahead and copy the private IP address we're going to do pseudo Su hyphen ec2 user I'm going to go ahead do w w connect hyphen Q capital O hyphen space paste enter and it doesn't seem like it's going to resolve which is totally fine so let's go back over to the access analyzer and so we have the outbound header starting from the server and this one is this one here is is um 10 so I'm just reading this wrong it actually is going the destination is where we wanted to go it's giving full port range which is totally fine 65535 35 shouldn't we specify that port to be very specific seems like a very narrow Port so I'm not 100% sure about that it goes to the eni which goes to the security group which goes to the ACL which goes through um public which goes out to the ACL which goes to the eni and it's actually passing through the transit Gateway which is great unidirectional path analyst only the results include forward path analyst from the source and destination there might be a blocking configuration in the reverse path that could not be analyzed so what does that mean that it doesn't know if it worked and it didn't necessarily failed or it's not sure well here the thing is it's trying to make it to that instance right so we'll go back over to here and we'll just take a look here and read thoroughly the result included forward path analyst analysis from source and destination there might be a blocking configuration the reverse path which could not be analyzed which means what would happen if it went back the other way would it work um that's a good question I don't know I'm going to go set this up again as I really want to make sure that when we analyze this path oops uh we just be a bit more thorough in terms of what we can configure here so my Transit Gateway test two and so here I'm going to choose the instance it actually did go through as as I hoped it did and I'm going to very specific I want Port 80 Port 80 you can option specify the packet header details of the traffic to evaluate the network reachability destination path this one's going to be instance and again it's going to be Port 80 Port 80 I feel like we could configure these but I don't I don't want to fiddle with those it is TCP so if it knows that the protocols TCP it should expect it to return that in check we'll go ahead and try this again and uh the destination here is the website we'll go ahead and type create at analysis path and we'll wait for this to complete okay so we ran that and let's just go take a look here and see what it looks like so on the outbound header we can see the port range is now 8080 it's going through the eni it's going through the security group it's going through the ACL it's going through the uh this one through that ACL dni the transit Gateway and then we have our inbound header which is from 12 13 uh 1132 which is correct as it came through uh that one over there and here we see 8080 TCP Etc so I mean it looks like it's reaching it but it's not returning anything so that's a bit tricky because we know that when we use the VPC appearing it definitely worked so I'll just look some things up okay one other thing that I'm noticing here is it says no route to destination so route table does not have an applicable route to the tgw so suggesting in this one here it doesn't have a route are they sure about that so we'll go over to here to this table we'll check table a we'll look at routes oh well maybe that is our problem okay I'm going to edit this I think maybe because it's so similar that's our problem let's go ahead and try this was that it let's go the other way and make sure that's the case as well tgw good let's go back to reachability analyzer going to close this session out because we haven't used it for a while um we'll go back here and I'm going to try this one again so I'm going to go back I'm going to analyze the path uh confirm it and so maybe this time it will work going to refresh here so just wait a moment okay all right so this one wasn't reachable let's go take a look here it might actually still work but um now we're starting to see something here so the route table route from 12006 to the route table cannot transmit the packet because the packet destination address matches a more specific route with a destination cider of 12016 so hey at least we're getting somewhere but uh I'm not exactly sure what the issue is maybe it wants a smaller cider block uh as the range so let me see what I can can do I'm going to try to make a change and then if I get some success I'll show you well wait hold on let's just go to the route tables here for a moment and this one says the igw is here and local is 12006 so this one is going in the opposite direction so that is right let's go this way is this one going in the correct direction uh it's still wrong wow I'm so bad at routing so okay igw 000000 that makes sense right we go to this one and now it's even more mucked up because the transit Gateway is supposed to be this is supposed to be flipped around so this one should be the internet gateway okay and then this one should be the transit Gateway we'll save it and we'll go back over to our reachability analyzer into our path and just so I'm not getting mixed up I'm just going to delete these older ones because I'm not using these anymore and uh we'll give this a refresh here and so I'll click into this one and I'll analyze path and then hopefully this time I'll get a um a better result we'll find out here in just a moment okay all right so after waiting a little while and by the way make sure you hit the refresh because it doesn't always tell you it now says it's reachable so uh we can go here and everything looks really good now it does still have that same warning which is totally fine but now we can see a direct path of analysis so what I'll do and I just got popups uh appearing here so I'm just going to get this to not be so annoying there we go that's better um let's go back into our web server as we did before or sorry our just server server not our web server and we'll attempt to see if we can reach um the actual website that is running that basic page so here Will type in pseudo suyen ec2 user and I'm going to go ahead and type in w get q0 and paste in this link here and there we go so now Transit Gateway is working and on a side note we got to utilize uh reachability analyzer so let's go ahead and tear down these servers and clean up as we are 100% done here so we'll go to our instances and I'm going to terminate um two of them I'll leave this third one alone because I assume boo is trying to solve one of our Cloud networking challenges This Cloud networking stuff has been extremely difficult for me because I'm just not great at troubleshooting networking issues but you can see we can get through it so that's all that really matters and um I don't know why I uh terminated the instances is we need to just delete the cloud formation Stacks so we'll go ahead and delete those and I mean since the instances are stopped we should be able to go and tear down the VP or the VPC Transit Gateway I'm not sure if it would remove everything but let's see if it is smart enough to do that it probably will tell us to remove everything individually yeah so we have to get rid of the attachments first and we'll get rid of this attachment okay so those are deleting and we'll just wait a little while here okay all right let's see if these have deleted and they are now deleted so we'll go back over to our our Transit Gateway uh we'll go ahead and delete the transit Gateway see if that works excellent we'll wait for that to delete all right so the transit Gateway is deleted we're going to make our way back over to our uh vpcs and see if we can go ahead and delete those so we'll get rid of a and say delete and we'll get rid of B and say delete and there you go that is Transit Gateway traffic Maring sends a copy Network traffic from a source I to a Target eni or to a Target UDP enabled Network load bouncer or Gateway load bouncer so the whole point of this is that you can use traffic mirroring to send a copy of your traffic to something like a security monitor Appliance so imagine you have traffic coming in it goes to the eni and you say okay now send it to the other eni and then we can analyze that traffic you're going to notice that traffic miring is going to attach a VX Len header if you want to know more about that it's in the docs but all we need to know is that it's attaching an additional header for more information for each packet it's mirrored once um for traffic mirroring it allows you to apply filters so we can filter based on a bunch of parameters a mirror a traffic mirror sessions need to be created to mirror the traffic so that's these are the components you need to actually do this you need to create a mirror Source a mirror Target and a mirror filter and that's about it so there you go so CNS firewall shows up under the VPC uh portal but it is really a row 53 um and specifically row 53 resolver uh service but it is a DNS firewall that protects against DNS exfiltration of your data if it can be used for something else I'm not really sure but uh as far as I understand it's for ex uh protecting against exfiltration so what is DNS exfiltration well it is a method used by hackers to steal data from an IT system or network by exploiting the domain name system system protocol by hiding malicious code within DNS packets the best way I can kind of describe it and again not the best at DNS knowledge uh but boo is really good at it and uh he just doesn't go on video he's not going to help us with it but DNS firewalls allow you to monitor to control the domains of your applic what your applications can query so DNS firewalls let you uh set domain list it allows has allow rules to only allow trusted domains and deny rules uh to prevent domains with known Bad actors you can centralize your DNS firewall policies via Theus firewall manager so let me just get my pen tool out to explain this so the idea is that imagine you have um data coming in uh packets coming in to your your ec2 instance and somehow they figured out how to hide malicious code into the DNS packets and this is not really an issue of stuff coming in it's it's more of an issue if it makes it to the end server and starts to execute code and then what it wants to do is send code or like bad stuff back out through your DNS resolver and so the idea is that if your DNS resolver has a DNS fir wall it's going to have a layer protection uh to see like hey my server is trying to send out weird stuff I and it's going to this bad place therefore I should stop it and so that is basically what it's doing so there you go hey this is angre brown and we are taking a look at ad of us Network firewall which is a stateful manage Network firewall and also an intrusion detection intrusion protection systems for vpcs it can filter outbound and inbound traffic at the perimeter of your vpcs it can filter for your igws for your nap gateways for your VPN traffic for Amazon Direct Connect traffic and the way it is working is that it's utilizing this uh open Source uh software underneath called Sirata I have no idea if that's how you how you pronounce it so if anyone knows better please tell me but uh sirotta is this opensource um IDs IPS and so the idea is that adabs is leveraging that underneath and so you can use compatible rules um so either they're us using that software or they're using something similar to it or emulating it I'm not exactly sure um for ads Network firewall it has multiple use cases you can use it as a pass traffic through only from known service service domains or IP address endpoints you can use custom list of bad domain to limit the types of domain names that your app can access you can perform deep packet inspection on traffic entering or leaving the VPC uh you can use stateful protect protocol detection to filter protocols like htps or an independent Port is used so there you go VPC pairing allows you to connect one VPC with another over a direct Network route using a private IP address so imagine imagine you have two vpcs you'll want them to be able to uh share traffic between them as if they're one VPC and that's what VPC pering connection is going to allow us to do so VPC pering connections is not a Gateway it's not a VPN connection um it does not rely on a separate piece of physical Hardware there's no single point of failure for communication or a bandwidth bottleneck so it's very simple way to set up a VPC pairing and so VPC pairing can be between an ipv4 or IPv6 address instances on on peered vpcs behave just like they're on the same network uh they connect vpcs across the same or different it accounts and regions piring uses a star configuration so you have one Central VPC and four other vpcs um you can see that it is a little bit more limited uh then Transit Gateway which uh is like VPC appearing on steroids as you can add like something like 50,000 or crazy amount of VCS but for this it's very simple there's no transitive peering so peering must take place between vpcs again if you want Transit appearing use Transit Gateway uh it needs uh yeah it needs a onetoone connect to the immediate VPC um for Transit appearing use a Transit Gateway as I said here uh it has no overlapping cider block so you don't want to have it's not going to work if you have overlapping ones uh data transfer across the a or across region incurs charges so if it's within the same AZ no no charges but in all those other cases there's going to be a charge the way you're going to configure vbc you're going to create the peing connection you're going to accept the peing connection you're going to enable the routing of traffic between the two vpcs they say that this is optional but to me it seems like you would really want to add these routes into your route table so that they can uh accept traffic from each other or uh be not just accept but actually just be able to do it one other thing that we want to point out is that if you can up dat um you can update the inbound or outbound rule of your VPC security groups to reference security groups and peered vpcs because if you're going to have two perer vpcs referencing a security group another one is going to be a very common use case and so I wanted to include that in here so here we describing Security Group reference from another one so there's a very dedicated uh command line to go find that and so that way you're able to uh grab the ID so that you can uh utilize it but there you go hey this is Andrew Brown and this fall along we are going to peer two vpcs together and do some uh communication between them uh so let's go ahead and make our way over to VPC and what we're going to want to do is create ourselves two new VPC I'm going to create one uh called I we'll go over to VPC and more we'll say uh VPC a or just a so because it'll already have VPC in the name and 10 16 seems fine to me we'll have one AZ one public subnet and no private subnets here no KN gateways and no VPC endpoint we'll go ahead and create that one so that will create very quickly we're going to go over and create another VPC and this time it's going to be B I'm going to just change this to B12 um so we have a different number here we're going to say one availability Zone uh one public subnet no private subnets no net gateways no VPC and points and so now uh we have those there too so we go ahead and view the VPC so that's really good the uh next thing I'll do is just refresh here so we need to create a peering connection between them we can absolutely just make it over here but I want us to get more practice with the uh console so let's go ahead and do that here I'm going just type in clear as you can see I was trying this before we have inabus ec2 create VPC peering connection and we'll provide the VPC ID I'm just going to open this up in a new tab so I can see what I'm doing and we'll make our way I'll close it out in this tab I don't need in both but I will go into vpcs here and we'll grab vpca and we'll go back over here and I'll paste that in then we'll do a pier vbc ID and we'll go ahead and paste that in and we'll hit enter and so now we've created our pering connection if we go and refresh here we should have a pering connection it has yet to be accepted so that's what we're going to need to do we're going to need this pering connection ID in order to do so so I've copied it we're going to type in a a ec2 accept VPC pering connection it say vbc pering connection ID and we'll paste that in and we'll hit enter great and we'll go ahead and do that we'll give this a nice refresh and now we are active so we're good we're going to need um routes each way so um we'll have to update our route tables I'm just going to do it uh uh find them over here to make it a little bit easier so for Route table uh a we want to Route it to B and that should be pretty straightforward so we'll go ahead here and type in a ec2 create route route table and paste it in we need to provide uh the destination CER block which is going to be the direction from the other one so if a is 10 0000 this one's going to be 120000 so we'll go ahead and put in 12. 0.0.0 sl16 for the entire size of the network so basically from anywhere and we're also going to need our VPC appearing connection ID here so go ahead and copy this again and I'm just going to paste it in below we'll hit enter and so that establish it establishes it in One Direction I'm going to go ahead and put in 10.0.0 16 and we need to replace uh this with the other rout table so this is the other one and we'll paste it in we'll hit enter and so that is now been established let's go take a look at our route tables and see if they are configured correctly we go to our routes this one is going out to 12 16 so that is good this one here is going out to the the connection so they are now connected um in order to do communication we're going to need um an instance in each one and the idea is that we can uh uh we can attempt to connect to the other one over the network so that's what I would like to do so what I'm thinking here is we should launch two ec2 instances one can have a sessions manager and the other one uh we'll have to launch manually so what I'll do here is go over to GitHub and we'll make our way over to our adus examples repo as we've been using this entire time I'm hitting period to open up or you know what I'm just going to make two files so it's a bit easier for us to work with so I'm going to go ahead here and make a new folder we'll call this appearing connection because I'm actually going to set up two templates this will be template a and then we'll have template B and we'll have these do something a little bit different so I'll grab this from knle because this one is a good base one and I'm going to paste this in here and so a is going to be the one that we're going to utilize to log into for um SSM so for this the SSM rule is very important for us to attach but we're not going to install anything on this one so we'll delete this part out here for the security group we can just keep it open that's totally fine um so I'm I'm fine with that and yeah we don't need this because we know we're using the network interface card here so this looks fine I'm going to just change this over to T3 micro and we'll go back over to here to our subnets and this will look for subnet a so I'm going to copy that ID here and bring it over here and paste it in I'm going to then go over to our vpcs a copy it and I'm going to go ahead and paste it in and we'll save it there we'll copy this B and here I'm not going to even have an SSM roll we don't need to SSH or get into it um but we do need a running a web sering the idea is that we can try to hit that IP address and if it will return that value will be in good shape we're going to just keep it open to everywhere and so for this one we're going to update this and we'll go ahead and we'll paste this in and we'll go ahead and replace this and we'll save that so now we have a and b so hopefully that is very clear we'll go ahead and download each of those and the idea is that um they should be able to talk to each other because you know they should be able to um because they're perod right but what what we'll do is just go to our downloads directory I know you can't see this I'm just doing this off screen and we'll make our way over to ec2 in a new tab uh well I keep saying ec2 I mean cloud formation and so we'll have to create ourselves a stack and uh before we do I think I think I replaced all these yeah these have all been replaced and so we will upload stack a uh uc2 a and I'm going to go back over to here create a new stack and I'm going to grab B unresolved ec2 instance profile so this has a slight little issue here as we do not have the instance profile so we'll go ahead and just remove that and I'll just have to redownload that file and just delete it locally so we'll try this again B and we'll go next see ec2 B and we'll say next next submit and so those are creating we're going to wait for those to create I'll be back here in just a moment okay okay so we are back both seem to be uh deployed we'll make our way over to ec2 and the goal here is to log into um one of our instances and then ping the other one and see if we can get any information about it now before we actually log the other one it would have helped if I named these a bit differently but um I guess we'll just click here and figure out which one is which so this one is probably not the web server is this one the web server let's find out uh not that one it has to be this one oh come on like uh the thing is the way we'll know is if we go down here the network is 12 so this one has to be the web server I'm going to open this one up again and we'll just take that out so yeah that is the web server and I'm just going to rename that so I know that web server and in a sense this is our jump box well it's not really our jump box I'm just going to call this um uh you know just ec2 instance whatever uh so what we'll do is go ahead and connect to this and we'll connect with sessions manager and we'll just switch over to the prop user sud Su hyphen ec2 user we'll type in clear so the idea is that I should be able to grab this IP address and I'm just going to do W get hyphen o or is it zero hyphen o zero paste I'm thinking of curl but if I do this it'll actually download the page which not what I want um I just want to print it out um so I'm just going to go ask chat GPT really quickly here um using WG download and display like WG display a remote HTML page I don't want to download I want to see it right away I imagine we could pipe it over to cat or something yeah okay on line it with wget oh wait hold on keep in mind uh there's The Hyphen p no we don't need that let's just try it ourselves uh I didn't really tell us what I wanted but maybe we can just pipe this to cat no that doesn't help um in one line using WG print out the contents of the uh uh website I would have thought yeah I thought there was a hyphen command quiet mode it okay so maybe it was zero and not o but we'll try this again curl definitely has that but it does something else we'll try I did O there let's try zero says it's not a thing capital O missing URL it's definitely not missing oh I have to put a Hy in between them is that why there we go and then if we put a queue on that there we are okay so we're able to access it that doesn't mean we're peering because this is going out to the public IP address right so one way we can determine if this is actually working is by hitting the private IP address because the private IP address is only available to those that are within the network so we go to our web server and we grab our private IP address I wonder if we can just go ahead and hit that and notice that it's still working so just to prove that it it really really is working what I want to do is go over to vpcb and we're going to remove the route to the Internet so you absolutely know that that is working so we go here and update the routes and edit it I'm going to remove that internet gateway we'll save it I'm going to go back over to here I notice that it is still working if we tried the top one notice that it does not work so that's how we know that that connection is peered one thing that uh I wasn't paying attention to was whether they were in the same availability zone or not so we go over our subnets that really matters for cost because if they're not in the same then it's going to cost something and I mean for this example I I should expect you not to see really any kind of expense because we're really sending so few requests but you know I just want to remind people about that and so what we can do is just give this a refresh the problem is we used the wizard so we didn't get to pick where it was so this one is in a cac1 a and so they just happen to be in the same availability zone so that means for my case it's going to be nothing but again it might not do that with the wizard it could randomly choose a one it's possible uh one other thing that I didn't mention in the actual uh slides but if you have two period connections and let's say that um you have a a web server in B and you have a internet gateway in a but no internet gateway in your vpcb um that web server can't get out to the internet because VPC peering is about connecting two networks together and even if you have an internet gateway or a n Gateway in the other one does not mean that those resources will route out to there so just understand that uh uh the connections are basically private between the two vpcs so that's all I really wanted to do here so we'll go ahead and make our way over to ec2 and I just want to terminate both of those once those are terminated we'll come back here and we will then tear down the VPC I think we could probably start doing that now a little bit because we do have to delete the VPC uh pering connection down here below so go ahead and we'll delete that pering connection says yeah we'll delete the routes it doesn't matter okay and we'll just wait for these instances to tear down I'll be back in just a moment okay all right so now we'll go ahead and tear down that VPC we created or vpcs I should say because there's more than one and yeah we'll make our way over to VPC we'll get rid of B which is easy to do do here and we'll get rid of a there we go and I will see you in the next one ciao hey this is Andrew Brown and we are taking a look at cloudfront but before we do let's describe what a CDN is so a CDN is a distributed network of servers that deliver web pages and contents to users based on their geographical location and the origin of the web page and a cont delivery server so it does a few different things but more or less it is a global cache um so here is a diagram of what it looks like we'll explain those components here in a moment but cloudfront is a CDN that can be used to deliver static content Dynamic content streaming videos and websockets Amazon cloudfront can be fronted with the adus WAFF for oasp I think I spelled that wrong there because it says o swap but it's supposed to be uh sorry about that that's supposed to be wsap as you can see I can't even draw with this pen for protection uh Amazon cloudfront can stream videos on demand using the ISS Microsoft smooth streaming for core components the way it works is you have an origin this is lo the location where all of the original files are located so this could be S3 bucket ec2 instance and elb roughy 3 we have Edge locations these are the compute located strategically close to the end users it these are caching data so they're holding on to a cache as close as they can to the edge location there are Regional caches these are intermediate caches in between um uh AWS and the edge location and this is to optimize um the cache for for geographical broad locations then you have your distribution and it's the big gray box if you didn't notice this is a collection of edge locations and Regional caches that Define how content should behave so there you go Lambda Edge are Lambda functions to override the behavior of requests and responses for cloudfront um so here is a diagram of what that looks like and so we have four possible functions we can um override via Lambda Edge the first is the viewer request when cloudfront receives a request from the viewer origin request before cloudfront forwards a request to the origin origin response when cloudfront receives a response from the origin your response before cloudfront Returns the response to the viewer supported languages here is Python and node.js functions are deployed at the regional Edge caches this matters because you'll understand uh latency uh for these things okay um let's look at some use cases so for viewer requests we could redirect HTTP to HPS inspect cookies for user authentication modify headers for AB testing for view response use cases we can add security headers set cookies for client side tracking customize error messages for origin request use cases we rewrite URLs for SEO or routing inject headers for origin authentication selective content serving based on user agent for origin response use cases we modify headers to control caching update URLs and htl HTML versioning customize Air responses for origin uh some of these things don't make sense like I don't know if you need to re redirect HTP to HPS because cloud from can do that but you can still use these functions to do that let's look at some examples here uh each for one so here for viewer request we are redirecting HTTP to HPS okay so I'm just trying to get you a bit familiar in terms of what this looks like so here we have the request header we see that it is HTTP so we tell it 301 and we change uh that value there now we have adding security headers and actually emitted one because there's just too many headers in here but the idea is that we are adding uh specific uh headers here okay and then returning the response here we are modifying the origin requests to serve different versions based on the user agent this is actually is very useful so here we are looking at our request and then from here we get the user agent and we see if it's mobile if it's mobile we'll give the mobile URL if not it'll be the desktop and then we have the origin response so here we're changing a 404 to a 200 which is useful but again this is functionality that cloudfront can automatically do for you the idea is if we get a 404 return a 200 give an okay and give it a custom page and some content type there so there you go let's take a look here at Cloud front functions these are lightweight Edge Lo Edge functions for highs scale latency sensitive CD and customization cloudfront functions are cheaper faster but more limited than the Lambda Edge functions so there are two function for cloud from functions you get the view request and the viewer response the only supported language at this time is ecmascript 5.1 JavaScript um these functions are deployed at the edge location so they're much closer uh to the end user use cases here would be cach key normalization header manipulation status code modification body generation your all redirects or rewrites request authorization so there is overlap for these things okay with Lambda edges uh use cases but here's a view request redirect HTP HPS we saw this before but I'm just showing you what it looks like in that ecma script 5.1 JavaScript and then here's the viewer response and we are just adding some security headers so there you go all right let's compare directly clam functions and Lambda Edge because they're a little bit confusing uh and they seem like they do the same thing but they serve different purposes uh so CLR functions and Lambda Edge across the side here so for CLR functions we have ecmascript 5.1 JavaScript for Lambda Edge we got nodejs and python for event sources it's just request and response for the viewer uh for Lambda Edge it's both for viewer and origin request and response the scale here is uh 10 million requests per seconds or more whereas Lambda Edge is 10,000 requests per second per region the function duration is sub millisecond for cloud uh front functions for Lambda edge up to 5 seconds or up to 30 seconds so you can see Lambda Edge is much slower uh for maximum memory we have 2 megabytes for cloud front functions for Lambda Edge we have considerably more um memory that we can use uh for the max code size for cloud from functions it's super tiny 10 kilobytes for Lambda Edge they can be 1 Megabyte for the viewer or 50 megabytes for the origin uh Network file system Network file system body request access we do not have that for cloud from functions but we do for Lambda Edge uh for geolocation data uh geolocation data device building and testing entirely within cloudfront and function logging and metrics are all yeses for cloudfront functions for Lambda Edge um for geolocation and device data it's not for the viewer but it is for the origin you cannot do build and test entirely within cloudfront uh you do get logging and metrics for cloudro functions there is a free tier available it's charged per request for Lambda Edge there is no free tier it's charge per request and function duration Lambda Edge is more expensive okay I'm not listing the prices in here but I'm just telling you it's more expensive um so generally you want to use CL from functions when you can for performance everything else unless there are uh specific reasons this really will help visualize where these things are cloud from functions are at Edge locations CL CLR Lambda edges are at Regional Edge caches and this diagram doesn't show it but technically the viewer request would be on this side here so we see origin request you have viewer request it's just not in the diagram and I didn't want to make a new one so this is what it looks like okay all right let's talk about origin so cloudfront origin is the source where cloudfront will send requests and when you create your distribution you have to specify the origin so when you're utilizing the CLI you're going to pass a configuration file to it for the distribution and there's there's going to be a section for Origins and uh that's what we are going to uh set so the main thing is that you're giving a domain name so this is where the address of the origin that you want to point to you can also Supply a path um so that could be something prepended to the end of the URL like for slash whatever you want um depending on what your origin is there is some configuration that can go uh that can uh be set on it so for S3 as the origin you have the S3 origin config um for custom origin uh config which will be for all the other um possible uh possible Origins uh the configuration is going to vary there so hopefully that is pretty straightforward when you're utilizing um cloudfront um via the adus Management console this configuration is a bit abstracted away from you so it's nice to see that here so we can see exactly what is going on okay all right let's take a look here at origin groups so this allows you to group a primary and secondary origin together as a destination for requests enabling enabling High availability and fault tolerance so here is the example of origin groups just as we saw Origins where we configured it at the time when we created our distribution uh it's in the same configuration file you'd have origin groups uh so An Origin group includes two Origins you have a primary origin where the request will be sent and the secondary origin a failover origin that will receive traffic over the primary origin if the failover criteria is met the failover criteria is based uh is based chosen uh based chosen on the HTTP status code I feel like I'm missing a word there so choose your status code that you want to check on in the example the right hand side it's on a 500 clar front will only fail over over on get head options requests it will not fil over on put post patch and delete you can use different Custom Air Pages for each origin uh in these cases but yeah the whole point of origin groups I always thought it was just a group a bunch together but it really is just a failover to another origin so there you go before we talk about caching policy let's make sure we understand what the cach key is so this is a unique identifier for every object and the cash and it determines whether a viewer request results in a cash hit so a cash hit occurs when the viewer request generates the same cash key as a prior request and the object for the cash key is in the edge location's cash a higher cash hit ratio more people hitting the cash will result in better performance so a cash policy is used to determine the cash key inabus has has managed cash policies for use so we have amplify uh caching disabled caching optimized caching optimized for uncompressed objects Elemental media package so I'm assuming the top one and the bottom one are specific to a amplify and Elemental media package um but the the the three in the middle are good to know let's open up just the uh the one they highlighted in red and so you can see where the ttls are set for it you can see that it is not including the headers or cookies or query strings it it doesn't care about compression so you get an idea of what can be set in there I think that's all of the options that can be set in a cash policy um so we want to update our cash policy we just uh provide it here and so in this example we are setting a manag cash policy right so all these managed cash policies have cash policy IDs you don't give it the name you give it the the the ID and that's how you set it now you can create a custom cach policy um and so we'll have settings for TTL so you have your minimum your maximum your default uh if all the values are zero then you basically have disabled the cach okay uh you have cash settings so uh you could be uh having a cat policy for headers for cookies for Cy strings for compression support so yeah there it is okay we are taking a look here at the origin request policy so let's talk about origin requests this is when there is a cash Miss and cloudfront must reach out to the origin to retrieve the data Okay so origin request always returns the following a UR path a request body if it's present and specific HTTP headers um origin request doesn't include these by default the URL query string cookies or all other HTTP headers um but if you are using the Legacy caching which we don't really talk about at all but it's a older older mode of setting um caching in cloudfront it usually will always forward all the headers um cash key information is set by the cash policy and are automatically included in the origin requests okay and so we're going to have the URL query string the HSP headers and the cookies here okay so origin request policy is to uh is to retrieve additional information at the origin about a request and also maintain a good cash hit ratio use the origin request policy to include information you want in the request but not in the cash key it was says the following managed or or origin request policies not orange but or origin uh we have all viewer all viewer and cloudfront headers all viewers accept host headers cores custom origin Kors S3 origin Elemental media tailor personalized manifest user agent refers headers often when I set up cloudfront I'll have to set up cores S3 origin um all of these require reading okay they're not fun to summarize but uh you should read through them before you set your origin request policy to determine which one you want to use you can create your own custom policy so for header Behavior we can we have none so exclude all viewer request headers except those specified in the cach policy we have Whit list we have all viewers so just include all the headers um we have all view and Whit list cloudfront so include all viewer request headers and additional specified cloudfront Hunters all accept we have cookie Behavior so we have none Whit list all all accept Okay pretty straightforward query string Behavior none white list all all accept so there you go all right so now we have the response header policy yes cloudfront is just full of policies it drives me crazy allows you to modify the H headers in the response that the viewers will see so that's what this does it it changes the headers and the response headers which cannot be removed by the policy so you cannot remove these ones okay there's a lot there but yeah you cannot tamper with those change them um it us has the following managed response header policies that you can utilize so we have cores and security headers cores with preflight cores with preflight security header policy security header policies simple cores if you're dealing with cores you're definitely going to be setting something here often cores with preflight or simple cores you can create a custom header policy of course so example here adds content security policy header to restrict resources to the browser uh that loads for a page enables the X content types options header this is what this thing is doing by the way uh it sets the xframe option to deny specifies refer policy uh of no refer configure strict Transportation Security to enforce HPS so you can kind of get an idea what's going on here so yeah I'm going to go through here a little bit but yeah that's what you can do with it but pretty straightforward you can change the headers okay all right so origin axis identity is a legacy method to ensure that an S3 bucket is only accessed through cloudfront and not directly via the Internet so here is kind of our diagram here and the idea is that you can see that if we try to attempt to access the bucket based on it let's say it's a static website and it has the static website endpoint we should not be able to reach it um because the bucket policy would not allow anything other than from our oi um the oi uh is basically a special cloudfront user and it will be granted access in the S3 bucket policy so it ends up having this really long arm so if you look at the principle we set ads and it says cloudfront user it's the only thing that this cloudfront user is used for is for this and then we have to write Cloud front origin access identity and then the ID here um the limitations it's not supporting uh granual policy configurations does not sign HTP and HPS requests um that needs to utilize signature version 4 and uh there are regions where it's expected so like you basically are forced to um use signature version 4 so ois are just not going to work does not integrate with SS KMS only uh it's only supported in existing Abus regions and and regions before December 2022 okay so there are limitations uh and that all has to do with signature version 4 but uh there's a newer solution called origin access controls which we'll look here in a moment that it's just easier to use um and what you should use going forward okay let us take a look here at origin access controls to ensure that an S3 bucket is always accessed through cloudfront and not directly accessed via the Internet so same description as oi um same place as where you think that the oi would be but it says OAC uh it looks different because when you implement the bucket policy now what we're doing is we are saying the principle is cloudfront okay and then we provide the source R to the distribution so that is how it's configured differently uh the advantage of OAC over oi enhanced security practices because it utilizes shortterm credentials resourcebased policies frequent credential rotations because they're short term protects against confused Deputy problem attacks if you're not familiar with that that's where an entity that doesn't have permission to perform an action can coarse a more privileged entity to perform the action now that's the description of what a deputy problem is in the case of cloudfront I'm not exactly sure what Deputy problem like where and how it would uh cause that but apparently it protects against it uh we have comprehensive HTTP HTTP method support so get put post patch delete options and head um supports downloading and uploading S3 objects encrypted with SS KMS supports accessing s S3 in all itus regions because it's using that signature version 4 which is the key thing it's it's doing the signing and I didn't write it in there but it is definitely doing that so there you go so cloudfront origin Shield is an additional layer in the cloudfront caching infrastructure that helps to minimize your Origins load improve its availability and reduce its operating costs the following benefits is better cash hit ratio reduced origin load better Network performance okay so without origin Shield this is what it looks like you have your Regional casses and your Edge locations and with origin Shield um the idea is that you have a bunch of stuff pointing to origin Shield okay so it's like another layer of caching um and it definitely will increase performance so it's as simple as that okay let's take a look here at geographical restrictions so this prevents users from specific geographical locations from accessing your content served by cloudfront cloudfront returns an HPS status code 403 Forbidden to the user uh in the case that it is uh restricting your location uh the way it works is you are going to uh configure your Geo uh restriction and either it's you're going to Whit list it or Blacklist it uh and you're going to provide items so here we're giving the code so US and Canada saying that these are allowed to access um our distribution so there you go let us take a look here at caching query string parameters so what is a query string a query string is part of the URL that assigns values to uh specified parameters that then can be parsed by the web app or browser client so if you're looking at the URL um the query string is this thing that can be added at any time to a fixed URL it has a question mark with the key equals value and the aand to do multiple ones there cloudfront can forward query strings to An Origin using an origin request policy cloudfront cach key can be based off query parameters so you can use it as part of your cash key and your cash policy the reason you might want to utilize um this in your cash key would be something such as serving content in different languages so in your query string you'd have language and then give the the code there cloudfront does not by default cach or for the query string so you need to uh update those policies if you want to utilize it okay now we're taking a look here at caching cookie based contents what is a cookie a cookie for a web browser is a key pair data set from a website and stored on the user's computer by the user's web browser while the user is browsing so here is an example of us setting a cookie on a web browser client um the idea here is that uh when you're looking at the header data because the server is always going to be uh returning the cookie data it'll be in the set cookie and here you'll see um our data so you see we can see first name Andrew last name Brown as well SS additional information about the cookie like the expiry uh its path its domain uh and things like that so cloudfront can forward Origins uh or cookies to An Origin via the origin request policy and CLR can cash based on the uh cookie and which would affect the cash key there via the cash policy you could use the cookie values to cach different pages just like our language example we could just give a thing that says language equals um English or for different countries whatever you want um I think the last one I did I don't think I did it based on like English I said like country I don't know but anyway the point is is that you can use cookies in that way just like um query strings uh these are not cash by default so you have to set them in your origin request policy or your cash policy okay all right let's take a look here at content type so cloudfront servers don't determine the MIM type for the objects that are served so what is a mime or media type a media type also known as the multipurpose internet male extension or MIM type indicates the nature and format of a document file and assortment of bytes web browsers use M type header and not the file extension to determine how to process a URL of a file if the M type is not correctly set then the files may not work as expected because it doesn't know what to do with this mismatch file examples of M types would be plain texts HTML documents CSS JavaScript jpegs pngs gifs uh svgs Json uh XML PDF there's a much larger list than this this is just the most common ones uh so when you upload a file to your origin we recommend you set that content type header so it knows what it's serving so hopefully that makes sense but there you go let's talk about invalidating the cach so there's a thing called a TTL which is time to live and basically when a file reaches passes time to live then the file is ready to be removed so the next time there's a request it's not going to take that file it's going to go to origin however let's say you don't want to wait till the TTL finishes and you need to invalidate them right away well you can do that um for the entire cach or specific files with the create invalidation API action but I should tell you that you should really invalidate exactly what you need because um when you think about the caching it's making copies at all these Edge locations and uh Regional uh uh uh Regional cache locations and so if you clear everything then it's going to have to replicate all of the files everywhere and that's going to be more expensive as opposed to just replacing the files that you need to invalidate so you can invalidate the all uh by doing create invalidation and then specifying forward slash you got that for slash in there um Aster okay or uh you can invalidate very specific files now if you have a lot of files uh you're not going to be doing it this way you're going to be providing a batch file and this will be a Json file where you have the items and you have to specify the amount of items in there and then you get a color reference which is just like a unique value um that you have to change every time uh when you do that so the way I do it is always the batch of file way um but yeah yeah there we go okay so let's talk about cash busting with digest so first of all what is a cash digest a cash digest is a value that's appended to the end of a file name with the intention to bust a cash uh cash busting is a way to prevent browsers from caching your content uh or you know it's just like if you have a new file you want them to pick up that new file but you don't have to invalidate the cash um for that file okay so here's an example that is implemented in Ruby because you basically have to code it that's the only way you're going to be able to do this uh so we here we create a function and the way it works is it's creating a hash based on if the contents of the file has changed so in this case what we're doing I'm just get my pen tool out here so you can see what's going on but we're reading we have a file path we read the contents of it and then we use shot 56 and we create a a hex digest okay so we're saying based on the content create a a a digest and that digest is always that hex or hash is always going to be the same thing if the content is the same if the content changes even one character it's going to spit out something else okay and then so the idea here is what we're going to do is we're going to go down below here and we're going to get the base name and we're just going to take the first eight letters you could take the whole digest if you want but there are limitations sometimes on file sizes or sorry file names like the URL so we take the the first eight letters and the idea is that the next time uh somebody uh like a server or website retrieves the style file it's going to pull um the latest file and it'll have a different um a different hash the cach Ming uh using cash digest hash is a common strategy for static websites or assets it absolutely is Javascript bundles like web haacker can automate this process by creating a digest hash but the thing is is that the file the the HTML file has to use the latest one here okay so the other advantage of having this uh digest is that if there are older files that somebody is still accessing they usually are still in the cache so you you will have multiple versions of style so in some cases allows multiple versions to work at the same time uh gracefully as you transition to newer ones but anyway there you go cloudfront can be configured to return a specific object as the default root object so the default of the default root object is index HTML so if you were to access this website or the cloudfront distribution uh end point and it was pointing to an S3 bucket or wherever it's going to assume that you want to return back the index HTML okay the URL will still look like this but the file that will be returned is the index HTML you can change this to something else so let's say you wanted to serve up world.jpg you could do that if a user requests a subdirectory of your your cloud from distribution it will not return the default root object um the reason I have to specify that is because um older systems I can't remember what they're called but back in the day um I can't remember what it's called but whatever was used to manage websites a really long time ago its functionality the way it would would work is it would always go back to the root um but claw front does not do that okay but there you go you can configure Cloud front to require that view ERS use htps so that connections are encrypted when cloudfront communicates with viewers uh you can require htps between viewers and cloudfront so when using the cloudfront domain name change of your uh protocol policy setting for one or more cache behaviors to require HPS communication so very simple in this case when using cloudfront with a custom domain Creator import your certificate with the uh uh ad Cloud certification manager I wrote AWS here I actually wrot I meant to write ACM because that's its acronym A CM apologies uh uh you can require HBS to a custom origin so change the origin protocol policy install the certificate on your origin server you can require HBS to uh Amazon S3 origin when An Origin points to a bucket change the viewer protocol policy to require https when An Origin uh points to a bucket website endpoint then htps can't be required so generally you always want to point to the bucket not the end point uh to help wart SSL renegotiating type ATT tax CLR does not support renegotiation of viewer and origin requests cloudfront sign URLs allow users to access private content via temporarily generated URLs if you think this is similar to S3 uh presign URLs I'm going to tell you these ones are so much harder to use uh Club front sign URLs can uh use either canned or custom policy so a canned policy just contains the path the URL and when it should expire so it's very simple a custom policy has more restrictions than can be applied to the signl a sign policy document must be signed using a cloudfront key pair this is a key pair that you uh generate out via the ab Management console the key pair can be generated via cloudfront console so a cloudfront sign URL has to be generated programmatically since you need to sign the policy document using the key pair and so yeah this is a a very complex example this one's using python here um but the idea is you can see the can policy in here and we are going to utilize the key here and then we sign it and we return the sign URL um what would be the use case for sign URL well if again if you have content that is maybe paid content then you provide the URL you generate it up by your server and then the person can only access it for a certain amount of time so like let's say you have a PDF that people want to purchase right um the ideas that you generated out there and then after Spire there's no likely someone's going to share it with somebody else so there you go cloudfront sign cookies allow users access to private content via cookies sign cookies use the same Syntax for canned or custom policy documents that uh sign URLs use sign cookie policy documents must be signed using the cloudro keypad just like sign URLs sign cookies make sense when you need to let users access multiple files imagine a confer distribution that serves videos and these videos should only be accessible to paid users in fact we used to use this on the exam proo platform because we used to serve our videos via cloudfront but we found it was very expensive and not easy to utilize so we stopped doing it but that that's what we utilize with signed cookies you need to create three cookies that will be served by your web application CLR policy CLR signature clar front key pair ID so those are the three values you need um here's an example of sign cookies you're signing the cookie no the following cookies will be set down below so those are the three and that is the python implementation of it but there you go in Cloud front you can set custom error pages so this is pretty straightforward you can change the path to a custom error page for your origin you can change the Response Code returned um so I'm just going to go here and bring in my pen and so the idea here is that if you got a 404 then you can say say um then return this custom page here or you can say we have the error code 500 return the custom custom error page 500 I don't know why I didn't show this as an example but what I would do and I I mean I didn't show the code but I I talk about it here which is Custom Air pages are useful when you want to serve a single page application via S3 so you could take a 404 and then say return a 200 and always have it return index HTML and uh I do that often when we want to have Spas stack website hosting for S3 that's how you would do it with like with a react uh app or angular or whatever so there you go hey this is Andre Brown and we are taking a look at sqs but before we talk about what it is let's just Define a few things so what is a messaging system this is used to provide asynchronous communication and he couples processes via messages and events from a sender and receiver or you could say a producer and a consumer what is a queuing system a queing system is a messaging system that generally will delete messages once they're consumed as simple communication it's not real time you have to pull it's not reactive so what is sqs well it's a fully managed queuing service that enables you to decouple and scale microservices distributed systems and serverless applications the use case here would be you need to queue up transaction emails to be sent so things like for background jobs and things like that there are two types of cues we have standard this allows you to process a nearly unlimited amount of messages but messages are out of order you have fifo so first and first out allows you to process a certain amount of messages but guarantees order of messages and no duplicate messages for message size it's between one byte and 256 Kil kilobits for larger messages you need to use a library for message retention it's based on how long sqs will hold onto a message in the queue before dropping it from the dropping means deleting so if it's if it's dropped it's gone message retention is by default 4 days message retention can be adjusted for a minimum of 60 seconds to a Max of 14 days you can encrypt your cues using ssse or KMS so KMS just allows you to customize your customer key there um a use case here would be and I'm not sure why the graphics not showing up but we'll see in a moment here but the app publishes a message of the queue another app pulls the queue and finds the message and does something let's just go here and it's out of order here but we'll we'll just get the pen tool out here so anyway the app publishes messages to the queue which is here right so from the phone to the queue another app pulls the queue and finds the message okay so that's what it's doing here another app reports that that they completed their task so it goes back um and marks the message for completion then the original app pulls the queue and sees the message is no longer in the queue so both apps are using the ads SDK to push messages and the pull Que in this example uh could you use the CLI sure but generally when you're using sqs you are pratically working with it okay so there you go if you want to send large messages to an sqsq you're going to need the sqs extended client library for Java and python if this sounds familiar we had something very similar for SNS um so it's pretty much the same process here it's hosted on GitHub for ad's lab GitHub page um and the idea here is that we are going to have to utilize um an S3 bucket uh these libraries are useful for messages that are larger than 256 uh kilobytes and can go up to two gigabytes both libraries save the actual payload to S bucket and publish the reference of the stored S3 object to the SNS topic some interesting things to note and this is going to be for the same one as well uh the idea is that you are telling the uh telling the topic that it's going to be handling a large payload for this specific bucket you can even tell it to to leave it on so that every time you're utilizing the SDK it's always going to send it to S3 otherwise it's just going to do it for that single request so there you go so one of the two types of cues that sqs has is the sqs standard Q this allows you to nearly unlimited number of transactions per second um but as you can see they're not in any particular order the adus sqs standard Q guarantees that a message will be delivered at least once more than one copy of a message could potentially be delivered out of order you need to ensure your consumers can process messages that arrive more than once and out of order provides best effort ordering that uh that helps ensure a message is generally delivered in the same order that it was sent so here's an example of us using a standard q and we are sending a message to the que using the adabs SDK this is a ruby example pretty simple going to get my pen tool out here so we can just make sure we're on the same page here we have a message body we sent it here we have our standard Q we're sending it out there by default the the standard Q is the default okay um talking a bit more about sending messages we can also send batch messages as we learn with SNS it is costsaving when we do batching I assume it's the same for sqs okay but there you go the other type of queue that we can create is the first in first out queue also known as fifo this guarantees the order of messages when being consumed it's limited to 300 TR transactions per second messages have a unique uh D duplication ID to ensure there are no duplicate messages in the queue since there are no duplicate uh fifo insurers exactly one processing messages are ordered based on the message group ID to ensure order is preserved each producers should use their own unique message group ID to request or pull messages your consumer has to specify a message group ID it's possible to read up to 10 messages at a time the sqs fifo uh manages data in partitions across multiple AES but it's managed by you uh like on behalf of You by AWS each partition supports 3 ,000 messages per second with batching or up to 300 messages per second for send receive and delete operations you can't convert an existing standard queue into a fifo q so here's an example uh as you can see we're sending a message to the fif q but there are a few little differences let me get my pen here to show you the first is we are setting a d duplication ID here notice we can just set it to whatever we want we have a message group ID uh as well so those are the two key differences when you were sending messages when I was doing the lab I forgot how fifo worked and you'll see me going what the heck why doesn't this work and then I was like all right we have to set these two values um you can set High through putt um so this is a thing that you can enable on fifo it allows you allows for 3,000 messages per second with batching so 10 times greater than sqs fifo Q with uh when it compared to when it doesn't not have high throughput so you can turn this on by setting the fifo throughput limit the message group I'm going to assume this costs extra money um but yeah if you need more throughput you can uh get it okay hey this is angre brown and this video I want to take a look at sqs which is a simple queuing service so what I'm going to do is open up our G pod environment as per usual uh and get something provisioning here so we'll give this just a moment to get spun up uh and sqs is not two different ult to utilize normally when I use it uh because I like Ruby on Rails quite a bit they will already have an adapter to utilize it seamlessly which I think is really nice um but you can also interact with it directly so yeah a lot of web Frameworks will have a abstraction of it but uh working with the queue directly is not too hard we'll go ahead and make a new folder here I'm going to call this one sqs we're going to make a new folder here called basic uh I might change that in a moment because there are two types of cues that we can have standard and first and first out so I'm going to just change this over to standard as we will work with that first um yeah I'm just going to rename this here standard and uh you know there's a few ways we can build it I wonder if we can do the server list so I'm going to look up the Sam C or Sam uh CFN templates here because this might be one way that we can create our Q so if we go over here you'll see that we have um it should be somewhere here q q q sometimes they rename these things a bit differently so I'm just looking here application connector function graphql API HTP API layer layer version simple State machine uh so I'm not seeing in here I really thought it was part of Sam so Sam sqs so no I guess we just create a queue the regular way and that's totally fine so what I'm going to do here is I'm going to go ahead and create a new sqsq just type in CFN sqs and we'll go down to examples here and it's really easy to uh create AQ so we'll go down well they got a lot of code here but this is the simplest way to create a queue right here we'll make a new file in here we'll call this one uh sqs or sorry template template. yaml that we'll paste this in we'll need the uh standard starting line here I'm going to go into any place that I have a um cloud formation template here's one okay I'm going to paste this in as such okay excellent so we have our sample Q I'm just going to call this one standard Q because that's more of what it is I think actually if we we might be able to emit the name let's go take a look if that's possible if we can I'd rather any of us name it uh Q name it's not required but we're going to have to have something in here so it knows what this thing is so I'm going to go down here if set to True create an f f uh ffq so what I'm going to do is actually emit this one and I'm going to rather just say false so we know this is a standard Q let's go read through some of these other options so we can see what we have so we have content based duplication delayed seconds maximum message size message retention period okay so we have a lot of stuff here um I don't really want to tweak anything right now I think that's totally fine so we could always do that in a another video to optimize uh Q settings but this should be enough to uh create our que so I'm going to say um a simple sqsq all right and uh let's go ahead and get this ready for deployment so I'm going to go ahead and make a new folder here call it bin and we'll make a new file here called deploy I'm going to go grab a deployment script from somewhere so I think in Kinesis we had uh we have them everywhere but I think in this one we have our deploy script looks like I just have them loose in here which is totally fine I guess we could just not make it in a bin folder it really depends if we're going to have more than one action yeah you know I'm going to actually have it here in the bin directory this time around and uh this is going to have to go up to directories because that's where the template actually is this one's going to be called um sqsq standard Q press go standard deploy sqs standard remember we have no execute chain set so we'll have to confirm this I'm going to just double check and make sure that I have an active um go to my ads am or ad C tab here make sure that I have an active key so I do this is excellent so let's go ahead and attempt to deploy this before we do we're going to have to CD into that directory sqs standard and we'll chamod the bin deploy here we'll go ahead and attempt a bin deploy so this is kind of messing up this happens with G pod sometimes I have no fix for it it might be one of my extensions I'm just going to give this a nice reload give me just a second and we should be back here there we go we'll go ahead and do a bin deploy it's saying it cannot find the template well I assume that it would have to look relative to where it is so maybe like this would that help no does not like that so sometimes what I'll do is I'll reference uh templates I remember having reference code somewhere around here it was for I think Lambda so I'm just going to go into Lambda and steal some code in our Lambda section here and our bin probably any of these will have it so yeah here I could say the root path which would expand here and then I can get the template here as such and the way this should work is it should like wherever we execute is what this should be set as and then we can get that relative to there and so that should uh fix our issue there so we'll go ahead and try that so hopefully that fixes our issue typ in clear we'll go ahead and try this again invalid template path standard template yaml uh the key difference here is this is y y ml we called it y ml for samel because they tend to name them that way which is kind of annoying we'll wait for our change set we'll make our way over to cloud formation here okay and we have our sqs standard queue we'll go to change set we'll click into it and we have our standard CU so we'll go ahead and execute that change set so this should not take too long as cues create very quickly and we have a problem surprisingly consider how simple this is I didn't think we'd have an issue here unknown attribute uh ffq really maybe we didn't put this under the correct location um I'm really surprised that's wrong let's go ahead and take a look here did we uh provide the type we did yes and by the way while we're here we might as well go ahead and grab this URL and put it in here we have a good reference of where we are but we'll go back and take a look so fifo Q if set to True create a fifo q i copy that again paste that I mean that I I don't know how more uncomplicated that could get here but let's carefully read this maybe resource Handler return message unknown attribute FIFA Q oh did we name our Q this is something I no yeah it's right here weird give me just a second okay all right so some people are suggesting that FIFA cues are only available in two regions but we're actually just doing a standard one so I'm not exactly sure why that's the issue and also we're in CA Central one here and there's no restriction that I'm seeing so I I don't believe this I don't think this is true um but yeah give me a moment to continue looking not exactly sure what the issue is I've moved it up one here and what I'm going to do is attempt to delete the stack and try this again we can always just remove it and have the name but it's bizarre that I'm not able to set that and uh see that's a pretty straightforward uh template it's really bizarre that that would not be working we'll go ahead and give this another try here we'll give this a refresh uh we'll go into chain sets we will accept execute the chain set and we'll see if this fails again we should not have to wait very long as this is normally very quick still the same issue this is bizarre What if I I make this true okay does it only work in the true case because how am I going to make it fif Q if it does not let me specify it and we could try changing the region to see if that makes any difference but it'd be very confusing if if this shows it here but we can't do it in the CLI the same way or the CLA formation so give this a moment to delete should not take that long all right so I've deleted that stack we'll give this another attempt here while setting it it's true we don't really want to create a first in first out q but I'm just trying to figure out the parameters as to why this is messing up um because it is kind of bizarre having such a simple configuration decide not to work so we'll go ahead and execute this and if it does work then maybe it's just the fact that um it doesn't like it when it's in a false state for that which I don't know why that would be but uh we'll see what we come up with here so we'll give it a moment here to hopefully uh tell us what we'll do here all right so take a look here and it works in this case so that is bizarre so I'm thinking What's Happening Here is that it just does not like it if it's set to fall so what we'll do is we'll take this out but I'm going to need to have some other value as I don't believe we can just create one without properties I mean we could try and see what happens um I usually always assume there has to be a single property so I'm going to go ahead and delete this queue and then we'll go ahead and try to deploy it without uh any properties all right so uh I've deleted that stack we're going to go ahead and deploy it again and this time we're going to have it without the properties and see if it will uh take it I assume again there has to be at least one property but you know maybe it's just been a while since I've attempted to deploy without that we'll go ahead and go to our change set and we will execute that change set and we'll give it a moment to finish the deploy okay all right so our sqs standard queue is uh spun up and so that's surprising that that one property was so confusing but that's okay we figured it out we'll go into here and we'll take a look at our Que remember we can send receive messages within the queue here uh normally like when you uh set up sqs with something like uh shukin on Ruby on Rails it just kind of works and you don't really have to fiddle with it too much but you know there are stuff that we can do so we could add a Lambda trigger we can pipe it into event Bridge we can have a dead letter Q um apparently we can have SNS subscription so there's a lot of direct Integrations but we want to interact with this directly so let's go ahead and send a message to the que go in here just say hello world hello world and I don't want to delay it I just want to go ahead and send it then the idea is that we see the messages received we can pull for messages so that we can uh see the message we can click into it and we can see that message okay so pretty straightforward I'm going to go ahead and just delete it because we have now received that message but let's programmatically actually do that because that's one of the ways you're going to be doing that so let's go ahead and take a look at the a CLI here and actually you know what I think in this case we should programmatically do it because that is the most common way you're going to interact with the que uh we'll use the clii first and then we'll do some coding okay so I'm going to go ahead and type in a CLI um we'll go ahead into here and I just want to go to AWS and we'll look up sqs sqs and we'll go into here and there we have crate Q which we don't need um so we have send message we send uh Mass messages which is fine what I'm looking for is the poll so somewhere in here yeah receive message here it is okay so we have a send and receive we'll go down below here we'll copy this and we'll make a new file here we'll call this one send and we'll paste this in I'm going go back over to our uh deploy script here just grab this first line we'll just add a quick comment here paste send we're going to bring these down onto new lines so this is a little bit easier to look at um okay and we're going to need a q URL so I'm just going to go ahead and type in Q URL up here and then I'll just get our Q URL so we'll go to cues and we want the URL there we go we'll go ahead and paste that in there as such we'll bring this down onto a new line so we'll say dollar sign Q URL hello Mars I'm not going to delay the message um message attributes let's go take a look at what we can do for that so tributes I'm not really sure what we'd want to do for attributes so a list of attributes that need to be returned along with each message so we can tell it to return all or specific things I'm not sure what the default is but I think um what's is attribute names is that what we're doing here it says message attributes where is it message attributes why is this slightly different than what we have I mean we copied it from the oh this is the receive example that's why I'm getting confused uh so we'll grab the send one we'll take a look at the message attributes here each message attribute consists of name type and value okay the user specified message attributes so can this be whatever whatever we want to pass along let's go take a look here at the documentation let you include structured metadata such as time sense geospatial data signatures identifiers with messages using message attributes you can have up to 10 message attributes so can we make this whatever we want it seems like we can so yeah maybe we'll do that for fun I'm going to go ahead here and make a new file just call this message attributes Json and this file will be message attributes we'll actually rename that to send message I suppose just make our lives easier send message this is up a directory or it should be at least um I'll leave it in place because I have a feeling that it'll probably work in place or what we could do is we could just grab the root path here and just absolutely set it then there's no confusion here we'll grab this one here say send message Json we'll say uh message attributes path and will be message attributes have file in front of it here file call SL slash message attributes path we'll put a dollar sign in front of that and so that should in Theory do what we want it to do so we'll go ahead and type in clear here and I'm got to go ahead and chamod this file bin send and then we'll attempt the send here probably should spell send correctly so it works not sure what its issue is it doesn't like something here um I mean this looks fine it was examples SQ standard that looks fine this is really long so I'm not sure if that that's the problem we'll go ahead and hit enter and see what it says expected equals oh well there's nothing in this file so that might be something we might want to do here um so we have each message consists of name type and value okay I'm going to assume this is an array we'll have name type and value just say fruit actually the type's probably going to be a u a specific thing so the use your specified attribute for the string data value attribute has the same restrictions on the content type message that's fine um oh okay and then the value actually has to be what it is so if it's a string if it's a string then we have to actually specify string value let's go go down below and here take a look so oh this is really not the structure that I thought it was okay so really is like this so we'd say string and I guess that's cilles too string value Apple let me just carefully read this so I can make more sense of it I mean that certainly is confusing but I think we can name this whatever we want so I can just put here fruit and let's see if this actually works so we'll go ahead and try this again B send we have a problem here uh Missing required parameter in the message fruits data type it'd be really nice if they oh they show it right down here I was going to complain saying like hey they're not making it clear but it was there and I'm the one at fault here so let's try this again we'll go ahead and hit send and it says the type of the message user attribute fruit is invalid you must use only the following supported type prefixes uhhuh okay um string what a weird syntax okay well give me a second here I wish this was more clear so here's an example of one I mean like that kind of looks like what we're trying to do here so I'm not exactly sure what its issue is I'm going to go ahead and type in Fruit U maybe it wants a capital on the S that's might be the issue here there we go might be K sensitive there we go so let's go back over to our q and uh we'll go to send and receive we'll go down and pull for the message we'll go into the message we're looking at the tributes and there's our tribute so obviously we can set different types of things here uh billions numbers all sorts of things but we'll go ahead and delete this message let's go ahead and now receive the message so basically what we're just doing with the polling which is very similar we'll go ahead and make a new file here we'll call this one receive so may I spell receive right there I always seem to spell that wrong we'll go ahead and copy this I'm sure it's going to be very similar receive and um we'll go into the receive example down below we'll go ahead and grab this one and we'll just paste it on below here so yeah we have our curl attribute names going to say all message attribute names all Max number message 10 and so we'll type in Q URL okay so we'll go ahead and traod this file and so now we've received the message now that does not necessarily mean it removed it from the queue let's go take a look here and well hold on did we did we not delete it from the queue so how is there anything in here let's go here and take a look uh pull for messages no there's something here okay so you can see that we're pulling I'm still pulling here I'm going to see what happens if I I think we can both pull at the same time right yeah we can so if we look at this and compare them uh now we have two in here receive message oh you know what the problem is I'm doing it twice that's I was wondering like why it was working even though there wasn't anything there but we'll go ahead and try this again and notice we're only getting one here I'm going to just stop pulling for a second clear content um so we'll pull messages again and so we're getting three we go back over to here why do we only get one back that's interesting like the other one gets three this one gets one uh let's go take a look at receive you think because we say max number of messages that we get the maximum that there are the maximum number of messages to return Amazon SQL never returns more messages uh than this value however fewer messages might return ballot is 1 to 10 all right whatever but anyway at least we're getting a message I'm not sure why we don't see all of them I think this will be less of an issue when we're are uh using the code uh but anyway we have this information so I'm I'm guessing that we have to use another SQL command to or AI command to actually read the message so we receive the message okay how do we actually look at it purage message deletes available message in the quebe specified by the Q okay that's fine that empti the whole q but I want to be able to read the message so I'm guessing maybe it's like get Q attributes example yeah I didn't think this would be so hard let me go figure that out one second all right carefully reading the uh CLI it looks like the output it would actually output the body here um but it must be encoding it for a reason so there must be some other setting that we are forgetting to set here or there has to be something uh set out longer for timeout maybe it's visibility timeout since we're not actually setting that um that the message receives are hidden from the subsequent required request no I don't think that's that's it um let's go over here and just take a look at the attributes so we have all and all for message attributes so I mean that's pretty straightforward the name of the message attributes where n is the index and we set all here you send an attribute and receive you can return all the tributes specifying all or in your request you can also use all the message tribut starting with a prefix of bar okay uh still I don't see the information here so yeah there has to be a reason why this is happening um I'm going to pull all messages I'm going to delete them again let's just delete them all delete any messages in here let's just delete them stop pulling give this a nice hard refresh here and let's go back to our original send message number of messages so let's go ahead and maybe read a bit more about this because maybe there's something in here that we're supposed to be setting that's causing our issue here we'll go back down to example I'm going to remove something delayed seconds so let's go ahead and just read about that the length of time and seconds for which uh uh to delay a specific message I mean that's what I thought it was so that's not exactly what I want uh we'll go back over to here let's just see what we set for the actual message you know what um it doesn't even look like we set a body so maybe that's why I'm not getting a body back that would explain a lot we'll go ahead and do this uh does it have to be B 64 encoded it doesn't say that it has to be so we'll go ahead and just put in hello world as the first thing message attribute names as all attributes names as all so we'll go ahead and try this so we'll try the send first and it doesn't like something oh you know what it's supposed to have two hyphens in the front here we'll try that again unknown option message body it's right here are you crazy oh this is in the receive so I'm in the wrong area that's why that uh that would definitely mess it up I guess I just thought that that one was open but it wasn't oh no it's going here oh you know what okay um this is our receive which is our send and this is our receive so these are backwards I'll go take this one here put that one come on copy and paste give me a break here today cut paste cut paste and what's this DOC for the send message I'm going to cut this and paste this here yeah if you get things backwards it's definitely not going to work um so for this we don't need the uh tribute path because we're not actually using that here okay so let's go ahead and try this again so go bin send so that makes more sense that's what we were actually seeing there and so we'll do B receive and there's something wrong here on the receive oh well this message body is not supposed to be here let's take that out okay and so now it looks a lot more normal so we are getting our body hello Mars we're getting um attribute information I feel like to get the attributes maybe there actually are here yeah there there it is so we can actually get that data out of there that is great I don't believe does that remove it from the queue I don't think so let's go pull the message because if we received it I would assume that it would just pop off the queue right nope there it is okay so um I think to get rid of it we have to do that as a separate command so we'll go back up here sqs and let's just carefully look here so we have send message receive message delete message so once we receive it then we would want to delete it and so here we would just provide provide the receipt handle so if we look at this here there it is right there so I'm not going to bother programmatically doing that I don't think that is important but what I would like to do is write a bit of code here so that we can go ahead and um uh send and receive the queue because that is going to be the real way that we're going to do this um I'm going to use Ruby because it's super easy to do that so we'll go ahead here and just type in um uh we'll just make a folder called Ruby here and I'm going make two files we're going to have a send. RB and a receive. RB just understand that Ruby's already preinstalled on this machine so you might have to install that I'm using version 3.2.3 I'm going to go bundle a it here so we're going to initialize our bundler and then here I'm going to add a gem here so gem I'll add Ox because we'll definitely need that usually I add noiri I just can't spell it STK sqs we'll go ahead and type in bundle install to install those gems and then what we'll do is go over to our send and we'll just oops our send and we'll say require sqs it's probably to be like send and pull not necessarily send and receive we'll type in sqs Ruby SDK version 3 and we'll go over here so here we have our sqs uh configuration we're going to need this first Let's us see what we need uh this is going to be picked up by uh the local local environment variables that are set so I don't need to worry about that and in here we want to send a message see if there's like a poll there isn't is there a receive there is so it's very similar I guess we'll have our send and our receive so here will be our send I'm going to go ahead and just copy this here and so we'll need our Q URL so I'm going to hard code this you could set it as an environment variable I'm just not going to do that here today I don't care and we'll go into our send we'll go ahead and copy this paste whoops uh Q URL that will be Q URL hello Ruby delay in seconds and now we have some message attributes so that's kind of nice that we have that there uh I guess we didn't explore system attributes I don't care what system attributes we'll take those out uh I don't care about these we'll just go ahead here and say fruit apple all right looks pretty darn similar um now what we'll do is type in bundle exec so we're running this in the in the context of bundler so it picks up the um so it picks up the uh gems we installed the libraries it has an issue here we're missing a comma there probably shouldn't be a trailing comma here it Ruby does not like that like other languages do it's not tolerant about those could not load such file sqs um oh you know what this should be ads SDK sqs that's why okay still has an issue here what's our problem canot load such file no that's the old one isn't it yeah so it's here missing required parameter message attributes data type oh yeah sorry so we'll need to have the data type in here data type string and we'll go ahead and try this so that should have uh sent a message I'm not going to pull it here I'm just going to go ahead and write the other implementation I'm pretty confident that that worked correctly we're going to go ahead and just copy whoops copy these lines here and we'll go back over to our receive and paste this in here and we'll go over to our receive code which is here and so we'll have our ql all tributes seems great to me um I'm pretty sure we can set this as all as well so I'm going to do that I take that out we can leave that all alone there is only one message to be received anyway and so I think this is sufficient we'll go ahead and give this a go the only thing is that we're not constantly logging anything out so I'm going to just make one little change here I'm going to go to our gem file I'm going to add pry and I'm going to require it up here so we can kind of um programmatically debug this we'll say binding pry and what we'll do is just do a bundle install here bundle install and we'll go ahead and try this again it has an issue with this one message attribute names um I thought we could set this as all we go over here see we were able to set it like that so why can't we do that there I think before it said we could do like a wild card so maybe we'll just do that instead maybe it wants uh this I kind of remember reading that no it doesn't like that we'll just take it out Max oh this is missing a comma here maybe that was working before we're just missing that comma so the idea is that we have that binding prize so we can type in the response and see what we get back and so in the structured data we have our messages so if I type in ARR messages we can see our messages and if we want to delete the receipt handle then we can get that there so we'll go ahead and just do this whoops try that again rest oh you know what it's an array right so we say like first message here we probably want to iterate through it if we did this so in this what we would do is we'd say resp messages each do message and then here I can say messes message do um receipt handle okay so we could put that there's obviously other stuff in here so we have the body so we could say puts message body it's just body puts message attributes because this is a weird structure I'm going to do inspect it's like a way of taking objects and just kind of expanding them into something readable in Ruby we'll go ahead and type in exit here type in clear we'll go ahead and execute this again um so notice up here that it's printing so it's doing exactly what I wanted to do the idea is that we want to get this message receipt handle we're going to use that to delete it so we're going to just take this and just say handle here assign it to a variable called handle and then what we can do is then go ahead and delete this out of the que so we'll go back uh to our Ruby code it'll probably be like delete message and then we'll bring this here we'll paste it in and so we'll have the Q URL again and then this time we'll have the handle okay so we'll exit we don't have any binding prize so it should just proceed forward it uh probably has a spelling mistake we'll go ahead and copy that paste we'll try this again still doesn't like it oh we have to put message in front of it of course we'll try this again we spell Q wrong now I'll try it great and so it's deleted the message if we try it again we should get nothing back try this again so notice we're getting nothing back because it deleted it we read it from the queue but again there are um uh more convenient ways of using it so you know Ruby on Rails we have something called show ruken it's very hard for me to spell and the idea is that a lot of like a lot of web Frameworks you'll just install it and it'll work with the existing system in your web framework um and uh you don't have to worry about it so much but this was the example for the standard Q which it's doesn't matter what order we receive messages in it just happens uh however it's going to happen so you know we could probably simulate that before we move on here which might be a good idea and the way we do that is we just need to create more messages here so what I'll do is go ahead and just create a bunch of messages here delay this like this will be one second two seconds I we can send these all at the same time I don't want to flood the Q so I'm just going to go like hello one hello 2 hello wait why aren't these delaying hold on here one two 3 four five okay then we'll match these four 3 2 one excellent we're going to go ahead and execute this we could have done this programmatically but I I figur just we'll just do it this way here bin and we'll say send oh sorry Ben send there we go all right so that will send the messages we'll just wait 10 seconds and then we'll uh check the receive and what we're looking for is to see the order that's coming back here all right so let's go ahead and try to receive the messages I remember we did this before we were only getting one which was a b confusing but maybe we'll have to adjust it to find out what we want to find out is what message we receive first notice that it says two okay three five they're not in order and we only got three which is fine but um uh my point is is that the order which we seeing these in is not uh no no noce it's one four okay we'll try this again now we're getting nothing all right but everything has been there we go now we get two four five all right so again notice the uh the difference in consistency in terms of what we're receiving so I think that pretty much demonstrates everything I wanted to show with sqs standard there's of course a lot of features of sqs cues like de letter cues Integrations with other stuff that's for a a different video but this satisfies uh basic uses of standard and I'll see you shortly as we attempt F fiio's or f f fifo first in first out in the next one okay ciao oh uh yeah just before we go make sure you just tear down your your stack there okay so yeah but see you in the next one okay hey this is Andrew and we're back for another sqs video this time we're going to work with first and first out really it's going to be very similar to our last one but what we should see in terms of difference is we should uh always consistently have the first one in uh to be the same okay so what we'll do is make a new folder here called fifo um and we'll bring up not everything but we'll bring over lots of stuff so we'll bring over the uh Bin directory so that'll be one we'll bring over from our last repo and we'll bring over the template and send messages okay we will change this now so that it actually does a fif Q so it's like fifo Q I believe true so bring that properties and so that will set up our fifo Q um our bin script we have a deploy I'm just going to change this to sqs fifo Let's CD into this other directory and we'll go ahead and deploy that we'll make our way over to sqs cues refresh refresh refresh refresh oh not cues cloud formation I'm being silly here cloud formation and here it is we'll go over to change sets and we will execute this change set and so we'll wait for this to create okay so our squs fifo Q is uh ready to go what I'm going to do here is um uh send those messages so we do have this queued up to already send a bunch of messages which is perfect the only difference is that our sqsq has changed its endpoint so what we'll do is make our way over to sqs and we'll go grab that URL so we'll go to here and we'll grab this URL and we will make sure we are in our fifo one here so I'm just double clicking on that just making sure that is the case we'll paste it in here and paste it in our receive okay uh might as well take a look here at the que looks pretty much the same as the other one we'll go ahead and Trigger the send first so we'll do that uh must contain a message group ID so I guess in this case we have to have a message group ID all right so what do we have to do for message group IDs so say message group ID uh the parameter applies only to First and first out the tag that specifies that a message belongs to a specific message group the message that belongs to the message are processed by fifo Manor um so I think we can name it whatever we want so I guess I'll just call it message group they say ID is that what they said id yeah message group ID hello so we'll go ahead and paste this into the other ones hopefully mostly in the same spot doesn't really matter that much we'll go ahead and send again unknown option message group ID what do you mean oh okay this one's missing a I of course it's and now it's deleting my lines of course it is um we'll try this again this time trying not to make mistakes okay we'll try this again I'm just control seeing to get out of there um should either have a Content based duplication enabled or message duplication ID provided explicitly Okay so we'll go here and I guess that's another thing we have to set uh d d duplication here I'm just going to see if there's anything else besides this ID no just here the token used for duplication message ID if a message is it with a particular message is sent successfully any message sent with the same ID are are successful aren't successful so I'm guessing that this has to be unique that's pretty straightforward so we'll go ahead and set this value here one all right and this will be two and this will be three and this will be four four and then we have five so there's our five try this again fingers crossed okay so delay seconds is not permitted here we'll just take it out delay seconds I did not think it would be this different and again the reason I like I don't notice these things is that when you're working again with like something shro uh shukin which by the way is like the uh it's that thing from Street Fighter when um Ryu shoots that fireball that's what it's called but these Frameworks will abstract this stuff away so I don't tend to remember these uh these things okay so now we're getting messages sent excellent we'll go ahead and hopefully the receive will just work as is and so notice we have one we have two we have three we have four we have five it's all all in order excellent um we're done here I think the last thing I want to do is just well actually we don't even have to purge the queue we can just tear it down because we are done so we'll go ahead here and we'll just say uh fif Q sqs excellent we'll commit those changes we'll go back to cloud formation we will go ahead and tear this down and I will see you in the next one okay ciao let's take a look here at attribute based access controls it which is an authorization process that defines permissions based on tags that are attached to users and adus resources so sqs supports AAC by allowing you to control access to your sqs Q's based on tags and aliases that are associated with your sqsq here's an example um of denying a production resource so you can see here let me just get my um my pen tool out here is that we are saying sqs resource conditions only those uh anyone that's on prod they're not going to have access to this possible condition tags would be resource tags request tags tag keys so there you go so we saw aback but did you know that uh you can have access policies to Grant other principal permissions to ssq just like how you have an S3 bucket and you can set a bucket policy an access policy is basically the same thing it's like a sqs Q policy if you will but it's called access policy so here's an example of letting an SNS topic send messages to the que and I almost feel like this was a scenario in one of the the labs but I didn't exactly know it at the time but but here it is now um we have common actions for send message receive message delete message it's just a it's just um uh like an IM policy so you know it is what you'd expect it to be here okay but there you go let's take a look here at message metadata this allows you to attach metadata to your sqs uh messages very similar to what SNS allows you to do so here we are using uh the CLI and the idea here is that we are passing message attributes and that is our metadata down below possible logical data types for your metadata here are strings numbers binaries you can also uh have custom the idea here with custom is that you're taking an existing one binary and saying binary. binary. PNG but yeah extra metadata there okay all right let's take a look here at visibility timeout this is a period of time messages will be invisible after they are read or consumed by an app to avoid being read and processed by other applications the default is 30 seconds the minimum is 0 seconds and the maximum is up to 12 hours you can set the uh visit ibility timeout uh via the attributes there so we have visibility timeout there a message is hidden only after it is consumed from a que pretty straightforward so there you go let's take a look here at delay cues so delay cues lets you postpone the delivery of new messages to Consumers for a number of seconds when your app needs more time so any message that you send to a delay Cube remain invisible to Consumers for the duration of that delay period the default is 0 seconds the max value is 15 minutes San qes uh for them it will be per Q delay setting will only apply on new messages for fifo it will apply to all messages in the queue uh you can set the delay seconds just similar to the visibility timeout by setting it on the tributes a message is hidden when it's first added to the queue let us take a look here at message timers so this lets you specify an initial invisibility period for an individual message when sending to the queue um so the idea here is that we are going to set the second delay on the send message okay uh there's a little bit confusion here because it would seem like this is the same thing as delay q but it's not and um here the key difference is that it's on individualized I messages okay so yes it's the same but one's at the whole Q level and the other one is uh is per message okay so um for message timers And Delay cues they're both called delay seconds because they're both doing delays okay fif cues don't support timers on individual messages so that's the one exception all right temporary cues are high throughput costeffective application manag temporary cues when using common message patterns such as request response temporary cue client written in Java lets you create lightweight cues that are deleted automatically when they are no longer in use this is really interesting um that I I did not know that sqs could do but using uh and that's just like a little part of the Java code and by the way this is not easy to implement even though it us has examples for it but the idea of these benefit temper cues is it serves as a lightweight Communication channel for specific threads or processes can be created and deleted without inuring additional costs are API compatible static uh normal Amazon sqsq so basically what this is it's like um a like a like a like a mock version in a sense of cloud sqs but it's in your app okay um and so yeah it's just it's just that okay all right let's compare short versus long pulling so pulling is the method by which we retrieve messages from the queue so short pulling is our default this returns messages immediately even if the message queue being pulled is empty for long pulling this is where we wait until messages arrive in the queue or till long poll timeout expires you're using short polling when you need a message right away you're using long polling when you need to save money by reducing how often you pull the way you set this is by setting the weight time time seconds okay so short polling is if it's set to zero if it's greater than zero then you're using long pulling and the max you can set for long pulling is 20 seconds so there you go before we jump into uh SNS let's make sure we understand what pubsub is so Pub sub is the publish subscribed pattern commonly implemented in messaging systems in Pub sub systems the cender of the messages the public Publishers does not send their messages directly to receivers uh they instead send their messages to an event bus and the event bus categorizes their messages into groups then receivers of messages which we can call subscribers subscribe to these groups whenever new messages appear within the subscription the messages are immediately delivered to them so here's an example a simple example of an event bus with Publishers and subscribers sometimes these are called other things like sources and destinations and you'll see some variations uh for these terms a Publishers have no knowledge of who their subscribers are subscribers do not pull for messages messages are instead automatically immediately pushed to subscribers messages and events are interchangeable terms in Pub sub the use case here could be something like a realtime chat system or a web hook system so there you go hey this is Andrew Brown and we are taking a look at SNS so SNS stands for simple not notification service it's a highly available durable secure and fully managed Pub sub messaging service that enables you to decouple microservices distributed systems and serverless applications here is the general uh uh view of it we have Publishers who will send the messages the topics which are logical access points for communication and the subscribers uh sometimes we'll see Publishers called sources and the subscribers destinations but this is pretty much the setup here so there you go all right let's take a look at what kind of sources we can use so inabus Services can publish events to SNS via the topics SNS topic standard we'll talk about topic standard and uh fifo shortly most of the services can publish to SNS topics fifo um and so you can see we have a lot of AWS services that can integrate with SNS so just understand uh the scope to which SNS can integrate okay all right so SNS has destinations these are the subscribers who can receive messages and AD to categorize these in two ways we have application to application also known as a2a which I think they made up to be honest um supports subscribers from uh itus Services of web apps so things uh going to data firose Lambda functions sqsq https endpoints it was event Fork pipelines then you have application to person or a2p supports subscribers from Human entities so mobile apps mobile phones email addresses chat Bots or pager Duty so hopefully that gives you an idea and again source is publisher destination is the receiver okay or subscriber all right let's take a look here at topics so topics allow you to group multiple subscriptions together a topic is able to deliver to multiple protocols at once uh when topics deliver messages to subscribers it will automatically format your message according to the subscribers chosen protocol you can also encrypt topics via KMS Publishers don't care about the subscribers protocol subscribers listen for incoming messages and here's how we would actually uh create uh cues or sorry I should not cues uh uh topics sorry um there are two types of SNS topics we have standard and first in first out uh the main difference is first in first out guarantees the order of messages so if you want to create a first and first out topic it's very similar except you're going to set the attributes to be top uh fifo topic equals true and apparently you have to have the name of the um topic to have period fifo on the end so that's how that's going to work let's do a comparison between standard and first in first out so for throughput uh standard as high throughput for first and first out it has lower throughput compared to standard for delivery it it it for standard it's at least once and there's possible duplicates for first and first out you only get one and there are no duplicates for ordering there's no ordering guarantee for first and first out messages are delivered in the order they are sent within a message group that's why it's called First and first out the use case here would be for sandard where the volume messages is high and the exact order ordering is not critical so alerts notifications for first and first out this is where the ordering is crucial so banking transactions order data process processing for messaging groups um we don't have those for standard for first and first out supports messaging groups allowing multiple ordered streams within the same topic so there you go let's take a look here at publishing messages and the most common way to work with SNS is by having your app programmatically published messages to SNS topic this is pretty much the main way you're going to work with it so here we have a ruby example I'm just going to get my pen tool out here so we can take a quick look here we're establishing a client we're including the SDK up here for SNS we have our topic R we have our message and we're just going to publish it to that topic r with a message it's pretty straightforward uh if you have larger messages uh you'll have to use a a library called the SNS extended client library for Java or python originally this was only for Java obviously not everybody wants use Java so now it's available also in Python which can be found on adabs Labs Amazon on SNS python extended client lib on GitHub so here's an example of it uh there's a lot more a lot more going on here I would say um because you'll have to do this through S3 but the way it works is these libraries are useful for messages that are larger than the current uh 256 K kilobyte message up to a maximum of 2 gigabytes so understand that the standard messaging is two 256 kilobytes kilobytes yeah S I was going to say bits uh both libraries saved the actual payload to the S3 bucket and publish the reference of the stored S3 object to the SNS topic um we have message attributes so this allows you to provide structured metadata items uh about the message so here's an example of this publishing a message now utilizing the CLI as opposed of the SDK and so we have message attributes that we passed along here um so the supported data types here are string string array number binary uh so yeah just additional information we can pass along we can do batch processing so here we are publishing multiple messages at the same time and actually we're doing up to 10 which is what we can do sending messages and batches can help reduce the SNS cost by factor of 10 because you can send up to 10 at a time together uh if you want to send different messages to different kinds of subscribers set message structure to Json and Supply a Json file as the message so the idea here is that we might have different types of protocols that we're trying to service but we want it to be different for each one so this would be the default this is what email would see this is what SMS would see this is what sqs would see so there's variation there but there you go all right let's take a look here at subscription so to receive messages from a topic you need to create a subscription but a subscription can only subscribe to one protocol and one topic you can make lots of subcriptions but you are limited in that regard uh when you choose to cre subcription you're going to choose a protocol and we have quite a few options we have HBS for web hooks email which is good for internal email notifications even though it only supports plain text email Json so send your Json via email Amazon sqs adus Lambda or I think it's called Amazon Lambda I always forget if it's adus or Amazon Lambda these days SMS which is a text message uh or platform application endpoints so you can push it out uh to mobile devices so there you go hey this is Andrew Brown and in this video we're going to take a look at SNS uh so SNS is very uh straightforward to use it's a simple notification service um it does have a lot of different kinds of endpoints that we can utilize but we're going to do this programmatically as per usual I already have my environment opened up I'm using git pod you use whatever you want you will have to install the clii uh set your credentials set them as environment variables because that will make your life a lot easier I'm going to make a new folder here called SNS and so we will go and I guess use CL information because this is so simple that there's no reason for us not to use it I do believe that we can set up SNS via um uh Sam as well so if we look up Sam SNS here I believe it's part of this and uh I mean we could set it as an event source for a function so that's something that we could do so I'm kind of tempted to utilize that so maybe what we'll do is we will um make a a function that is triggered by SNS I think that would be kind of interesting because under Event Source we can um uh specify that so let's go take a look here and see if they already have an example that'd be really nice if they did SNS not exactly but let's go ahead and just first create our Q the standard way and then we'll consider converting this over to um uh Sam Sam C templates which we cover in the Lambda section if you're not don't know what I'm talking about right now we'll go ahead and type in template yaml as per usual I'm going to just grab something I got too many tabs here from previous I'm going to just close them all and I just need something from a previous project I'm just looking for the first two lines here get us started excellent this will be um SNS basic and we'll type in resources as per usual and we'll go here and grab our SNS topic this one looks a little bit more complex than it has to be so we have our SNS topic here so uh SNS topic we can specify subscriptions I think that these can also be specified separately if I recall so we can uh have them in line or we can um have a subcription there so that's kind of interesting just pull this down and uh so here we might want a topic name but I think all we need to do to create a queue is this that's it okay so let's go give that a try I'm going to go ahead and make a new file new file here this will just be deploy and um I'm going to go ahead and just grab maybe this one here that looks fine we'll paste this in here this will be SNS Basics deploy SNS I'm going to go ahead type in clear and we'll go back a a level SNS basic deploy we'll do a chmod on that I don't think it knows what it is we'll first CD into the SNS basic directory then we'll chmod it so we'll say U plus X SNS uh oh sorry deploy and then we'll just trigger our deploy here but I think that's all we need in order to um actually create that so hopefully that is the case we'll go over to Cloud information here and we'll go to here go to change set and we will execute this change set there we go and we'll just wait a moment for that to take place that was very quick so it has created the snsq we'll go over to SNS and we'll take a look in place messaging and Replay for uh fifo topics that's cool there's fifo topics now nice they're always adding stuff on me here so we have subscriptions but what we have is our topics so the idea is we have a topic here and subscriptions is um if we go here you can see that we can choose uh what the protocol is so we can send to Lambda email HTTP we have a lot of options here so it might be interesting to uh hook this up to Lambda and so um what I'm thinking is that we'll do what I was describing which is we'll hook it up to Sam because that's how we would do it so uh Sam stands for seral application model it's a way for us to um uh quickly write uh mostly lambdas with ease so in here under Lambda we'll go get our inline one because that's a very simple one and we'll go to this template we'll just merge it over into our current one so even though we have that we'll add in the transform that's going to make this so that this works as a Sam template and then we'll bring in our inline Lambda and I'm actually just going to call this Lambda because I I I can't stand seeing inline Lambda constantly and so that should be sufficient for that to work the uh other part is we want to uh specify the Event Source so I'm going to just bring this line over as well as I would like to have the serverless function I usually put it right here I'm not sure why I put it up there that one time we'll click through to that so we're looking for Sam resource function and here it says there's an Event Source I believe Event Source so events and Event Source okay so we'll go here and we'll specify our events and we have a lot of event sources the one we're looking for is um SNS so we'll click into that so here we have an example of one um do we have to make it and then reference it is that what they're telling us to do here I guess so all right so I'm just going to go ahead and copy this quickly oops and we'll go down here for a second I thought we would actually specified in line there but that's fine uh we'll go back over to uh we'll go back one so we'll go back again Event Source so properties type we'll go down to examples so we have our type paste that in there and then our properties which is whatever it does okay I mean do we have an SNS example I don't understand like why it was showing it at separately here then there function events vent source and then we click here does that just take us to that other page okay so it does oh oh okay so this is actually under events okay great I get it all right um so basically yeah I thought that's what originally was supposed to be so we'll bring our events down here below the code and then we'll just indent it like this and so the idea is that uh we can now reference our SNS uh I want to point this out because I didn't really talk about this when we were doing lambdas but you can just put any kind of regular clir code here but the transformation allows you to bring in other stuff okay like the uh the servess function stuff so here we have our SNS basic uh sorry that's the description SNS topic here and so we need to reference the um the the handle or the uh the topic so we'll go look this up again which is right here just going to grab this so we can find it later if we need it and uh we'll go to return values and so I want to know if the reference is going to be the Ron and it is so that is excellent so we'll go down to here ref and this is just SNS topic SNS topic sqs subscription no we do not want an sqs subscription that seems a bit silly but let's go read and what and see what options we have for this SNS filter policy the filter policy Jason assigned to the subscription so I guess if we wanted to narrow down exactly um what gets triggered because you might not want it to trigger on every single case that's what I'm assuming this yeah filter unwanted messages okay not worried about that today filter policy scope not interested in that uh redrive policy whether Des send it to a dead letter q for sqs not interested in that the region uh I want the default here set the property to uh to enable batch SNS topic notifications into sqs okay so I guess the idea is that if you had a lot of of stuff coming to S SNS can send it to sqs that's actually quite clever I like that idea but we don't need any of this stuff so I mean it's interesting that this has a filter so that's kind of cool but I'm going to go ahead and just take the rest of this out and so this uh should set up a Lambda the idea is that when we trigger this uh SNS topic or wait a second this actually this wouldn't trigger it would it well let's deploy it and find out I suppose is this the Event Source right so this should be no this should be what triggers it we're not going to know until we deploy it so um that's set up I wonder if we need anything else in here we chose the inline one for uh for Lambda and so this doesn't have a Sam config so that's totally fine we are going to need to install um the uh Sam CI if you have not done so so far and I have a bin script in here uh Bin whoops bin it a Sam C you can just open that up if you're not sure how that works very straightforward we just took it from the docs and and just got it to work so it's just that so uh we have that um I'm going to CD back into I never actually C into in the first place we're now in the SNS basic and I think all we all we need to do now is do Sam build so we're going to actually uh build it while I'm here I'm just going to check does this actually specify um no I don't think it matters okay I just think like do we need an S3 bucket cuz normally you have to specify one but I don't think so in this case so we'll go I actually just going to double check the deploy script for the Lambda here because I just can't remember if there was some additional stuff in here so we have our build yeah and then we have our deploy oh you know what we just ran Sam build so we actually should bring these scripts over that's what I'm going to do so we'll go ahead and we'll make a new folder in here and I'll just call this B B I'm going to move our deploy script in here and actually want to replace this one with uh this deploy script as this one is more capable for Sam Noti is using Sam to deploy as opposed to this other way so before we proceed forward I want to go ahead and tear down the um uh the clo formation template because we need to deploy it with Sam and not clo formation directly otherwise it's going to get confused or at least I think it'll get confused so we'll go back here and this looks fine I'm going to call this one just uh SNS basic uh Lambda yeah we don't need a configuration here today and I'll bring in the bin directory here so we make a new file here called or build build and that's in our Lambda here as well so bring that over copy paste we will go ahead and chamod those directories chamod U Plus X this is mucking up I'm just going to do a refresh here because G pod is uh messing up on me here all right we are back uh we'll go ahead and chamad U plus X bin all like that so that is good uh we'll do bin build template file not found in the basic template yam all right we'll just rename this to yml try this again and the reason why we named it yml is because Sam generally favors yml and then everything else is yl um and we could probably rename it to Y AML but I just don't want any weird side effects that we have to fight through so I just know out of habit to always make it yml for that one we'll go into here um this should automatically execute because we don't have to worry about the chain set because Sam is ignoring it and we'll just see if this actually creates okay all right so this deployed let's go take a look at SNS and see if it reflects anything in the UI I think this is going to be mostly on um other side here but we go here we have a subscription it's confirmed so we didn't have to confirm it we'll click through to it and it goes to Lambda so let's go take a look at our Lambda and see if it's the source I believe it's the source and if we go here and click through yes so it is the source that means that this should trigger it if we were to send something to this Q um so let's go ahead and do that so we're going to go back to SNS okay and the idea is that we want to trigger this so let me think about this let's go over to the itus CLI SNS yeah and so there should be a publish command send a message to the SNS topic and so this is what we're looking for and um actually what's interesting is we could probably pass the first name and last name here I wonder if how that would work but we'll we'll give that a go okay yeah so we'll just copy this code over here we'll make a new um uh file here we'll call it publish okay we'll just paste our code in here we'll need to grab the first two lines I there's only one command so we don't really need to set hyphen e but I'll put it in there anyway I'm go ahead and chamod that shamod uh Bin basic whoops bin uh publish sorry publish and we do need to supply the message um I mean technically what we want to pass along is Json I'm not sure if we can see what it requires here so the message you want to send if you're publishing a topic you go to different messages set the value for message structure to parameter okay where's that we'll go ahead and do this valid value would be Json well that's definitely something we want to do here and we'll say Json and it'd be nice if we could just write it in line I'm not sure if we're allowed to do that I'm just going to try name name like let it say first name Andrew Brown last name Brown so I'm trying to use the short syntax without having to specify file we could specify file and that's not a big deal either but we'll just try this first and see what happens so we'll go bin publish uh oh well we need the the topic AR that's going to uh be required so we'll go back over to here we'll grab the AR and we'll make our way back I'm just going to refresh make sure that this is the yeah it says lamb in the name so that must be the one we're using close out these windows here so I'm have less of a mess I'm going B this above and'll just say topic iron okay publish message structure Json body failed to parse okay um so what we can do is again I'm just trying to not have to reference a f because I have to bring in more code if I do that so what we'll do is just make this full Json just take a moment here and we will need to escape it so we'll escape this one and this one and this one all the all these here okay we'll try this again B publish no default entry in the in the Json message body no default entry okay give me a second here all right so asking ttpt for the structure it's starting to make sense to me that uh there's something that's very structured here depending on what it is so maybe what we should do is look up the documentation here message structure Json SNS and see if there's one specifically for Lambda so that's what I'm looking for here um not exactly what I wanted a TOA that's a new initial initialization they just keep making stuff up he let me go look for it see if I can find the docs okay all right so the docs weren't very helpful but uh chat GPT is suggesting that we can just provide the message directly um I guess we'll give it a go because be nice if that just worked I'm really surprised that they're not uh oh they're using single quotations I thought you couldn't do that we'll just try it the question is what will the event data look like on the Lambda side like will it be uh structured a little bit different we don't know let's go ahead and try to publish this okay so that worked so it sent it off let's make our way over to our Lambda the idea is to see whether it's been triggered tried or not the way we're going to find out here is by going over to Monitor and we'll open up our logs and what we're looking for is something to log because if it logs that means it's been called so give it a moment here and we go down below and we have a log I think that's recent I don't like 24hour time I can never read it properly it says um first name message equals hello so maybe there's an issue with our code with our Lambda so let's go take a look at that under the actual template yaml file here uh I don't think so it looks fine to me let's go back and read the error uh key name key error name first name so I guess it's suggesting that this event does not exist so what we can do to find this out is um if I update this inline it's not going to update so normally you'd update it but uh we'll just have to do it manually here in the Lambda function so what I'm going to do is just redeploy this um by I'm just going to deploy the code here in line but we're going to print out the contents of events so we can actually see what's happening here so I'm hoping that we just do this we'll just print it print event like this and I'll say deploy okay so now we'll go back here and we'll try this again we'll send our message and we'll go back over to uh cloudwatch here I'll just click back one and we'll give this a refresh notice we have a new log because it was a new deploy and so now we can see the Event Source so because it's going to be a little bit different I think and just copy this so I don't have to read it uh in that in that mess just going to make a new scrap file here so I can see what I'm looking at here um maybe we can format this so format Json I'll just ask to format this Json we could have provided to JQ but it's just easier to put it in here so we'll just let that format it really quickly all right there we go so what I'm looking for is to grab this information so we'll have to modify our code a little bit to get this to work so we'll go back over to our template here and the first thing is it has to grab the records so we're going to go here and say event records and then the next part it grabs is going to be SNS think that is correct yep and so then in here what I want is the message okay and so uh we'll say Json convert uh string to I guess dictionary Python and we'll find out what method it is it looks like it's just loads so just import that it's a um standard Library so it should be we should be able to include this without having any have requirements.txt we'll go ahead and grab the loads here and I'm just going to go ahead and just say m for message and this will be the message here as a message string just to represent better what it is and then over here I'm just going to put M&M and so I'm just hoping that this works and I haven't introduced any mistakes I'm going to go ahead and copy this we'll go back over to here paste it in here it doesn't like that contrl V it's so stupid that does that you'll have to hit shift tab to fix the indentation there we'll go ahead and deploy that we'll trigger this again and see if this works so we'll go back over to uh here another topic we want to go over to cloudwatch go back up a level every time we deploy it's changing it so it's not going to be in the same log group and uh here we have a slightly different error so we're getting in better shape here list indices must be integers or slices so there must be like array of Records um so I probably wasn't paying attention to that if we go back over to chat gbt here yeah it's an array and so I'm looking for the first record probably what we'd want to do is iterate through that if there was multiple records being delivered uh but I'm just going to keep it simple and make this zero for now I'm just writing it here because we're going to uh want to save this code for later and I'll go ahead and just do this as such and we'll deploy that again it we'll wait for the deploy to finish excellent we'll go back over to here we'll go ahead and publish this I'm going to make my way back over to here and I'm going to refresh I'll take do we have a new one uh I'm not confident with this so I'm going to go ahead and delete these I don't trust it there it is sometimes it comes in a bit late and I just deleted the older one and what I'm looking for what say log this for me why is it saying that oh is that like a yeah we have that there but I want to see what the return message is so can we actually observe this in here I mean must be working so that's that's good the only thing is that uh that return message is kind of useless because it's not returning to anything um so I think in this case we will probably want to change this and we'll just make this so that it's printing out our message there's nothing to two return so I'm just going to have an empty return on this and we will deploy that hopefully it doesn't mind that I'm going to go ahead and just bring that code over here we'll go ahead and save that I'm going to go ahead and send a new message we're going to go back over to cloudwatch logs oh it did not just execute that fast did it yes it did okay excellent and so now our um our stuff is working so you know that is the basics of SNS there's of course more that we can do with it but I think that is sufficient for now we'll go ahead over to um cloud formation we'll just tear down our stack there we go I'm just going to commit my code let's just say SNS basic and I will see you in the next one okay ciao SNS filter policy allows you to filter a subset of messages only to be delivered so the idea is we're going to set our subscription attributes and in here you can see that we are setting up some filters um so here we're saying anything but for ordered canel and we are specifying there you see price USD numeric over 100 so assuming $100 there uh if you want your message body you just have to set the attribute value as message body um so just understand that we can do that there the filter policy scope here is message attributes so filter based on message attributes or message body filter based on the message body we have filtration options for and or uh U key matching numeric value matching numeric value anything but uh with ranges with single value exact matching with string value anything but matching string matching using a prefix with anything but operator string value equals ignore case string value IP address matching uh string value prefix matching string value suffix matching so you have a lot of types of matches I'm not sure why it's like or logic or or operator um but they're repeated twice but I'm sure there's a reason for it but anyway there we go message data protection safeguards the data that's published to your SNS topic by using uh data protection policies to audit Mass redact or block the sensitive information that moves between apps and a Services it scans for PP uh personally identifiable information piis I was want to say PPI but it's piis protected health information so phis via data identifiers you can choose to use predefined data identifiers like name address credit card numbers or you can create your own data identifiers uh it supports the following actions it can audit so audit up to 99% of the data publish then send findings to cloudwatch says3 data fire host why won't do 100% I don't know but apparently 99% is what it can do de identify so mask or redact data deny so block data from being sent message me message data protection can help reduce Financial legal regulatory risks by complying with privacy regulations such as Hippa gdpr PCI fed ramp message data protection is only supported for standard SNS topics so let's take a look at how this is implemented the idea is we'll put a data protection policy in our policy document here uh you can see that we have an operation we're performing which is the D identify and we are telling to mask uh mask the characters with um a pound here and you can see the data identifier we're using is the predefined one where we are saying let's mask credit cards uh using pound so there you go SNS has a feature called raw message delivery which avoids having Amazon data firehose sqs HPS endpoints process the Json formatting of messages so the idea is that when you set the subcription attributes you can set the attribute name as raw message delivery and set that value raw message delivery to True uh data fire hose and sqs metadata is stripped from published messages and the message is sent as is for HTP endpoints the HTP header x uh Amazon SNS raw delivery value is set to True indicating the message should uh should not be formatted so so for uh web applications you have to pick up on that header to know uh the state of it okay SNS delivery policy defines how SNS retries to delivery of messages when serers side errors occur each delivery protocol has its own delivery policy so when the message policy is exhausted SNS stops retrying the delivery and discards the message unless a dead letter Q is attached to the subscription so here we have um a bunch of places uh where we're working with SNS and uh it's going to be a little bit different for some of these so uh for the retry phase it will try three times without delay uh for the other services it will try zero times without delay for pre backoff phase it's two times uh 1 second apart and for the other services here it's two times for 10 seconds apart for the backoff phase it's 10 times with the exponential back off from 1 second to 20 seconds for the other it's 10 times with exponential back off from 10 seconds to 600 seconds for the Post backoff phase it's 100,000 times for 20 seconds apart and then for the other ones it's 38 times uh 600 which is 600 seconds so 10 minutes apart I want to point out here that uh these appear to be a2a actually never notice this till now but these are a2a and these are a 2 PS to person so I mean what y I'd say so for all those so that makes sense to me uh the data policies cannot be changed with the exception of HPS So This Is How They are um so you can set your own delivery policy for htps endpoints as we just said that was the exception was htps endpoints so if we want to do that here uh we we set a delivery policy here and so there's our example there are four options for backup functions so we have uh ER I never can say that word arithm like you know adding exponential geometric and linear that's set here and that changes like how it backs off okay so there you go all right let's talk about deader Q for SNS we talk about dead letter qes more in sqs but uh SQ or SNS has the option to send to a dead letter Q uh it will send failed message attempts to sqsq when you are uh configuring this the standard SNS topic sends to the standard sqsq if you have a first in first out SNS topic it has to send to the first and first out sqsq to configure an SNS topic to send to a dql you're going to use uh the set subcription ATT tributes so you're going to set a redrive policy because that's what you're actually doing and you're going to specify the Arn for the dead letter q a q policy will need to be configured to allow SNS to send events to the s s uh sqsq um which it's says there so yeah there you go let us take a look here at application as subscribers send push notification messages directly to apps on mobile devices so just notice there on the subscribers we're looking at the mobile device let me just get my pen out here all right and notice that we have all these different types of um protocols for different things whether it's Amazon Apple by Firebase Microsoft or Windows uh push notification messages sent to a mobile endpoint can appear in the mobile app as a message alert badge update or sound alert uh the option of the protocol is platform application endpoint so there you go hey this is Andrew Brown and we are taking a look at EFS so EFS is a file storage service for ec2 instances it has the storage capacity that grows up to pedabytes and shrinks automatically based on the data stored so it's considered elastic multiple ec2 instances in the same VPC can mount a single EFS which is super powerful uh volumes must be in the same bpc ec2 instances install uh install the NFS version 4.1 client and can mount the EFS volume EFS is using the network file system version 4 Protocol EFS creates multiple Mount Targets in all your VPC subnets uh uh uh your spend is based on Space use starting at 30 cents gigabytes per month it's going to obviously vary based on where you live and so here is a good screenshot of the those Mount targets um I'm using an older screenshot because I don't like the newer representation in the new UI so I'm just showing you this but the idea is that you have multiple Mount targets I did not tell you this or write it in the slides here but you can mount EFS to other services such as Lambda such as fargate so it is a useful service not just for ec2 but most compute Services uh for AWS so um that is something that is super useful so there you go the Amazon EFS utils package is an open source collection of Amazon EFS tools also known as the EFS client you can find it here at adabs EFS utils on github.com EFS client enables the ability to use Amazon cloudwatch to monitor an EFS file system Mount status it needs to install Amazon EFS client on the ec2 instance prior to mounting an EFS file system it includes the Amazon EFS Mount helper which makes it easier to mount EFS file systems uh Mount helper is a program that you use when you uh when is used when you mount a specific type of file system and so there was a whole different way of doing this before and so now this is a much easier way to mount uh this is the way you'd install on Amazon Linux 2 the docs seem to be really focused on Amazon Linux 2 not so much on 2023 so when we do the lab we'll obviously use the latest and see what we run into ifs Mount helper provides the following options it has mounting on supported ec2 instances mounting with IM authorization mounting with Amazon EPS access points mounting with on premise L Linux client autom mounting EFS file systems when an ec2 instance reboots mounting a file system when creating an ec2 instance can mount either Linux or Mac so it's super robust there's no reason not to use the EFS Mount helper part of EFS client Amazon EFS does not support mounting from Amazon to window in instances before EFS Mount helper standard Linux NFS client was recommended for mounting that's how you had to do it um Mount helper defines a new network file uh system type called EFS which is fully compatible with the standard mount mount command in Linux Mount helper also supports automating mounting as uh an EFS at the instance boot via Etc fstab configuration net Dev option uh is used to identify the NFS when mounting your file system automatically if emitted ec2 might stop responding so that is a configuration you might care about here is how you would mount it so I'm just going to get my pen tool so we can get a little bit closer here to see what we're doing so here we're saying EFS we're using TLS and then we're providing either the file system DNS name here the file system ID which I believe is this and the mount Target IP address which is this there's a variant of this in the docs but this is pretty much it and this actually is for mounting multiple IP addresses so there's one where anyway this is the one that you should usefs Management console will provide the code via the attach button so you don't have to figure this stuff out they make it pretty easy for you um EFS Mount helper will uh will use the following Mount options so we have version 4.1 used when mounting re to Linux 4.0 when you are mounting to supported Mac instances the r size is whatever that number is that's the maximum number of bytes okay we have the W size we have hard so set the recovery behavior of NFS client after an NFS request times out so the NFS requests are retried indefinitely we have the timeout set the timeout value of ANS client used to wait for response before it retries this is set to 60 seconds it has two numbers of uh times it will retry a request before it attempts to recover we have no resort so it tells the NS client to use a nonprivileged uh TCP uh Source uh Port when a network connection is re established we have Mount port at 204 uh 2049 for Mac instances and I didn't mean to go to the next slide but there you go okay ciao hey this is Andrew Brown in this video we're going to take a look at EFS so elastic file service um I always forget what the S stands for if it's storage or service oh they got archive now they're always just adding stuff to this uh this thing here but uh anyway it's it's not too difficult to uh create this but I want to create one using cloud formation as I think that would be probably uh the best case when we're working in production so I'm over here in our repo this one is at ad examples if you're just not sure where this is I've launched this up in G pod you use what you want to use I'm going to use this and it's already configured for my adabs account just going to go check here get caller identity um I'm going to make one change before I proceed here I'm just going to switch this back over to C Central One sometimes I'm in US e one depends on the use case but I'm going to CD back here and make a new folder called EFS and CD into that and we'll just make another folder here called basic and I'll CD into that and this is where we're going to get set up now I'm going to want a deployment script so I was just doing EBS a little bit while ago there so I'm going to just grab this deployment script I'm to copy it actually I'll just grab these two templates here even though I'm not launching an um an EBS volume I'm just going to go ahead here and and go rightclick paste okay and so now I have these two here I'm just going to tweak this this will be EFS basic and this is in CA Central 1 where I like to work um and we have an ec2 instance here which is not a bad idea um I really just want to focus on EFS separately just because I think there's a few things we have to configure let's go ahead and type in EFS uh cloud formation and see what we need to get working here so what do we have if we scroll here on the left hand side we have an access point and a file system so and then a mount Target so we have three different things here so I'm going to grab the first one I have a feeling we're going to have to get all of these here and for the time being I'm just going to go ahead and clear this out uh whoops and so this is going to be that's where that's up here this is going to be our file system and then we want the mount Target and then then we want the access point we have three different things here see I got some junk here from before just go over here okay so let's focus on the file system first okay creates a new empty file system for EFS you must create a mount Target to mount your EFS onto an Amazon ec2 instance fair enough so we'll go down examples and there's a lot of stuff going on here it' be nice if it had most of the code for us um so let's just take a look at what we have so we have um an access point yeah yeah yeah I want to see the file system so here's the file system going to go ahead and copy this and this will be here and so we'll paste that in um I don't want a backup policy encrypted sounds like a good idea I don't necessarily want a life cycle policy and I don't know about performance mode Let's go take a look at these performance mode we have come on performance mode here we are the performance mode is general purpose is what I want so I'm just going to take this out so it goes to the well I'll just replace it we'll just be explicit so we'll just make it nice and cheap I don't think I want a life cycle policy I'm just making sure that I don't need to specify it it's not required excellent so we'll go ahead and remove that I don't care about file system tags um file system policy we probably need so we'll go up to here property syntax uh here it is it's not required so uh for the EFS file system a file system policy use to control NFS access to the EFS file system so I'm not really sure if we need this at this point um but it's saying who has access to it so you're seeing very specific principles being like this is allowed to access it and uh you know if we're going to launch up an ec2 instance we might want to specify that here saying like this particular one I'm not sure at this time I don't think that we do so I'm just going to go ahead and comment this out for now we'll come back to this um we have the KMS key I wonder if I have to specify that if that's encrypted if it is I don't really feel like configuring this ailon value if it's true when creating the file system you have the option to use the the KMS ID key or use the default KMS key I'll stick with the default that seems fine to me here so we have our file system I'm just going to call this one file system to make our lives a little bit easier we'll go back down examples and we have some other things here I don't know why they have the Json in here but we have our file system excellent um that's the key we don't care about that and it said that we had to have a mount Target and so the mount Target is going to specify where it's deployed um I'm not sure how many Mount targets we have to have so we'll just look it up EFS Mount Target okay so after you create the EFS system you can create a mount Target for EFS file system use Regional classes you can create Mount Target in each a um I only really want to do one so let's see if we can just get get away with one so I'm going to go ahead and copy this and we'll go back over here and I'm going to go and specify our Mount Target here I'm just going to say Mount Target and here it's actually specifying an ec2 subnet well that's not what we want because that's not an EFS Mount Target that's just a subnet this is really what we want is this code here okay that was uh deploying the subnet so we'll look at this and here it's referencing the file system ID um so we do call this file system so that's fine just going to shorten this to make it a little bit nicer and then we have our subnet ID I think that we specify subnet ID up here yep so we'll do this um Security Group we probably want to set so we'll come back to that in a moment we didn't specify a subnet here do we have to for one zone file systems no what about the VPC how does it know where it's deploying we must have to tell the VPC where it goes right nope okay all right that's fine so I have our Mount Target now let's take a look at the access point creates an EFS access point access point is an application specific view that allows the operating system and user groups and file system Etc to do that uh so we'll go down to examples here so we have a access point root directory something like that yeah I mean it looks like something let me just read this a bit more I don't think we have to have an access point so until we actually need one I just can't remember uh we'll just leave that out for the moment but we have our Mount Target group we're going to need a security group here so I'm going to go and just uh split this across I'm going to go back to EBS and I want to go into basic and we'll go into template here and I want to grab the security group so go ahead and copy this I'll paste this in um this is allowing Ingress and egress from anywhere is that a good idea it's probably fine uh for our example um we might just want to create a security group and then add this one to the uh load balancer if we want to do that or we could just say anything from uh this particular one I'm not exactly sure how we specify that in one go here if we can uh tell it how to do that but uh I'm just going to bring these onto the correct lines I like to put them always above type here so I'm just thinking about this let me just go look this up and I'm just wondering can we specify our own Security Group because we have ID the default vbc the name of the Security Group you must specify either the security group ID or the security group name you can't specify both uh yeah cuz I'm just thinking like how would you tell this to specify its own name I'm not sure so I'm going to leave this alone and just leave it open up to the Internet is it best practice probably not but um I just want to make sure that we can get this to work so let's go ahead and set up this file system I think that this has everything that we need uh except that this is not specified correctly so I'm going go ahead just type in uh SG here whoops so we'll just say ref SG I always think that it's like something else here because we go to this one yeah it's like group ID so what is this one expecting because that kind of matters I'm going to go back over to our Mount Target because it might be the ID or might be the name I wish they were very consistent about this but they're not up to five Security Group IDs so it's going to be group ID then and there's no coal in here that doesn't make any sense and it looks like it's supposed to be an array so I'm G to go ahead and do that and I'm going to CD into oh we're already there basic I'm going to go ahead and try to deploy this and we'll see what happens um unresolved SG group ID well it's right here buddy and this one has it like that oh it's get at that's why so we try this again we'll go ahead and deploy uh we're going to go over to here and I'll go over to cloud formation change sets and we'll execute change sets Okay and we'll wait for that to deploy yep so while we're waiting for this deploy I'm just going to take a look here and see what we have because the security group is created um actually we'll wait for this to create just in case there's an issue I don't want to create the uc2 instance and have a problem so we'll just wait okay all right so that is now provisioned what I'm going to do is go over to ec2 and um we'll just create one manually it's not uh that big of a deal we don't to do everything uh via cloud formation but I go here and create a new instance I'm going to launch it um I'm going to launch it so that it's in the same subnet as the other one so I just want to double check and make sure what subnet it's in so go back to EFS here and we go into here I'm looking for uh they change the UI on me from last time I was in here um here where is it Regional okay but we launched it in a specific subnet well we made a we made a mount point right or a mount uh where is it they used to show you here somewhere see Central one okay let's go back to our uh code I'm really confused because we have a mount Target right and it's in a particular subnet well I mean we can just look up what subnet that it is I just want to guarantee that this is going to work correctly but it's saying it's regional so maybe it works everywhere normally like when you launch it via the um EFS via the click Ops it'll actually have it in every single region the um the mount targets and that's why I'm getting confused but we what we'll do is go ahead and just do this and this one is CA Central 1B so I'm just going to launch my ec2 instance in C Central 1B and we'll go here we'll say launch instances EFS example Amazon link 2023 T3 micro I don't want to keep hair I want to be in 1B just in case 1B excellent I'll leave everything else alone I'll drop this down here I'm going to create or add the SSM roll here excellent we'll go ahead and launch this in instance now what I'm curious about it does it have uh the commands for us so that we can quickly connect to it I I remember before they did uh but they might have changed it since last time I used it yeah here we go we have instructions so Mount via DNS or IP address so these are the instructions that we're looking for and so this should in theory work so I'm going to go back over to EFS here make a new folder or file remy. MD and I'm going to copy this one and paste and this one and paste so mounting obviously you can't use mine yours is going to be a little bit different than mine we'll go back over to our ec2 instance we'll take a look at it we'll wait for it to uh pass status checks okay all right let's take a look here give this a refresh and it is now running I'm going to go ahead and connect to our instance here and we'll go over and do pseudo Su hyphen ec2 user there we go um and I want to go back over to here and grab these so we'll try the first one which is the Mount line we'll paste that hit enter it says pseudo Mount T well wouldn't we have to create the directory first otherwise how would it mount it um and it doesn't show that here pseudo EFS Mount helper all right give me a second so here's a question is this tool installed um so let's go take a look and see if this works normally normally for Amazon links 2 this is the instructions and so I'm not sure what it will be for Amazon 2023 as they don't keep things super up to date here uh in the instructions we'll go ahead here and so it is installing it so it wasn't installed before and maybe that's the reason why it wasn't working so we'll go back over to here but you know I would think that we'd have to create a folder like EFS first so it knows what to mount to but we'll go ahead and paste this in and it says ifs Mount point doesn't exist um so that makes me think again that we have to specify where it's mounting so I think the last part is is actually the directory that we want it to Mount to um that's probably what it is I would have thought that it would be somewhere in our Mount directories like we place it in here so what I'm going to do is I'm going to make a new directory called Mount EFS go pseudo here see if I can do this here and we'll hit up again and then I'll just do forward slash Mount FS let's see if that makes it happy there we go so I think that's what we need to do so I'm just going to go ahead and write this in our instruction so uh what did we do first we installed that tool which was so setting up pseudo yum install hyphen y Amazon EFS utils and then the other part of that was pseudo mkd Mount EFS and we'll just change this to be mounts I'm going to assume that um this one is supposed to be Mount EFS as well I'm not sure why the line here is twice cuz this one says nfs4 and this one says EFS so why do we have two of those here did we just copy both of them oh using the mount helper okay so whether we want to use NFS the old way or the mount helper I see okay so we don't need both those lines this is like the older way is what I'm kind of used to uh but yeah this one is a lot more simplified but I'm going to go back over to here and what I want to do is CD into the um Mount directory LS and I'm going to just touch a file say touch hello.txt we'll give the pseudo okay and so what I'm going to do now is I'm just going to terminate this instance okay so the idea is that if this works then the instance is going to uh stick like the um EFS is going to persist the data and if I launch a new instance okay and then Mount that one then it should be able to access that file so we'll just say my FS2 this will be Amazon 2 I'm going to make this a T3 micro we'll go down below here we do not need a keep key pair I'm going to choose we said 1B there we go that's is fine we'll go all the way down to the bottom we will choose here SSM roll we'll go ahead and launch this instance we will not launch this with the key pair and we'll wait for this instance to launch and then we will mount it there and if it connects then we know that we're in good shape okay all right so our other instance is running we'll go ahead and connect to that one um instance is not connected to sessions manager yes you are I gave you a permission to do so there we go you have to know what you do so that uh just confidence in what you configured it as is what I'm trying to say there and I know that I hooked it up for sessions manager so we're connect to this again uh and so we have our instructions over here so I'm going to go ahead and install this so the mount Helper and then we will create that folder there we will Mount the directory which will take a little bit of time here there we go we'll say Mount EFS LS and there's the file so there you go that's what we wanted to do there uh did we use um EFS in the most secure way possible no but do we know how to utilize it now yes so that is our basic EFS example let's go over to cloud formation tear that down all right and I will see you in the next one okay ciao Amazon FSS that's so hard to say FSX allows you to deploy scale feature Rich high performance file systems in the cloud FFX fxs supports a variety of file system protocols I'm not going to reshoot the video it's just really hard to say okay so the point is that there are different types of file system protocols you can utilize with this it used to just have like one and then it added two and now there's four let's take a look at these four that we have the first is for net app ontap so this is a proprietary Enterprise storage platform known for handling pedabytes of data we have open ZFS this is an open Source storage platform originally developed by Sun Microsystems we have Windows file server so this is for Windows servers supporting native Windows features for Windows Developers for file stage we have uh luster which is an open source file system for parallel Computing so those are the four for the exams you're basically focusing on wfs um but ontap is something that is used quite a bit with Enterprises so that is a good one in practicality to learn but there you go SX for Windows file server is a fully managed shared storage built on Windows Server it has native support for Windows File system SMB native of Windows compatibility Enterprise performance and features consistent sub millisecond latencies tools that Windows developers administrator use today it can continue to work unchanged offers storage backed by SSD H HDD or both I think it's hard dis Drive yeah HDD FSX integrates with your Microsoft active directory Amazon FSX can be used for business apps Home Direct home directories web serving content management data analytics software build setups media processing workloads to run FSX you'll need an ec2 instance a workspace instance an appstream 2.0 or VMware Cloud on it should be an asct on that because I don't know the state of VMware these days as they were acquired from by broad cam Broadband whatever they're called um the the reason you should know this one is because for storage Gateway um file storage so you have file storage for w FSX for wfs and so that's that integration there so I'm just trying to get you a bit more familiar with this it's probably not easy to configure it's a Windows file server or file storage so um with Windows totally out of scope here going into detail here but we should uh overall know what these things do okay Amazon file cach is a highspeed cach for data sets stored anywhere and a accelerate Cloud bursting workloads this thing kept coming up in different services and so I figured we should just have a slide on it to cover it so we have a general idea of what it is is uh Amazon vcash is found under the Amazon FSX uh Management console I was really having a hard time finding this and then I found out that it was um somewhere somewhere else which was in FFX so we'll continue on here serves as a temporary high performance storage location for data that is stored in on premise file systems adus file systems Amazon S3 buckets makes disperse data sets available to file based apps on ads with a unified View at high speeds sub millisecond latencies and high throughput and fall Cas is accessible to ec2 ECS and eks compatible with most popular Linux based Amis integrates with adus batch via adus ec2 launch templates integrates with adus thinkbox deadline thinkbox deadline is for Creative Studios that that do scale rendering workloads there you go okay hey this is Andrew Brown this video we're going to take a look at FSS uh this is not a service I have much experience with but we're going to go ahead and just jump into it because I can't imagine it's too hard the idea is that we are going to create a file system uh and then I suppose that we want to utilize this probably with some ec2 instance compute so we have a bunch of options here so we'll create a file system uh we don't want to do on Tab because that's proprietary we don't want to do windows file server that's too hard we have luster and we have open ZFS um so I'm thinking open ZFS is probably the best option because it's going to be the simplest and cost effective I like how they tell us down here below this one's for HBC so this is the only really one I want to showcase here um so we'll go ahead and hit next we'll create standard before we proceed I'm just going to look up the cost for FSX pricing because don't know if it's expensive or cheap um I'm going to assume that this one has to be cost effective because it is open source and so we see 045 yeah it looks pretty pretty okay here if you are not comfortable with it don't go ahead and deploy this um but I'm going to go and do this and I'm going to get rid of it afterwards if you're not comfortable so just say my FS for file system and I just want to deploy this in a single a uh we have the storage capacity the the minimum is 64 gabyt jeez well that's what it's going to be not a big deal but still that's larger than I wanted to utilize here we have the three iops per gigabits um or user provisioned the lowest is to so I guess we'll just leave it where it is we have throughput capacity so the sustain speed at which the file server hosting or file system can serve data I'll leave it alone as it is uh we'll deploy in the default vbc knows today I'm in North Virginia normally I'm in C Central 1 I just forgot to switch over we have our encryption we have our data compression type which I don't know yeah sure that sounds fine uh tags sure why not uh we have backup and mainten okay sounds good we'll go ahead and hit next and just checking here so single a z in our VPC it's just telling us what we can do editable after creation what can we change and not change so that seems fine to me we'll go ahead and create this file system okay I have no idea how long this is going to take so again you know if you're not comfortable this and you're worried about cost just watch and we'll see what happens okay but I'm going to wait for this to complete all right looks like our file system is up um now what we'll do is go launch an ec2 instance so that we can actually mount it I'm going to launch up an Amazon 2023 instance as I believe that will'll probably have NFS installed and we can just mount it using NFS is my current assumption I'm just going to use the uh it was console here to do it here today so just say my FSX instance not that FSX is running on this we just want to know how we're going to connect to this I'm switching this over to a T3 a T3 micro as t2s are getting kind of old here at this point I do not want to launch with a key pair I'm going to go down below and go to advaned details and I'm going to choose an instance profile I don't know if it matters but I want to launch this in the same availability Zone this says that it's in a single a z which a is it in it's in 1D so I'm going to go here and just make sure that I'm launching in 1D as well just to make my life uh super here easy here today so I'm looking for one D and I think this one is public yep yeah they're all public I don't know why I I think they might not be anyway so we have that selected we'll go ahead and launch this instance and I do not want to launch with the key pair okay we'll wait for this instance to become available okay wherever it is one this one right here okay so we'll just wait all right and I just found the instructions and what I'm going to do is just make a new folder in here while we're waiting here make uh I'm just going to CD back for a second say mkd uh FSX and then I'll go into here and we'll just say mkd open ZFS and I just want to get those Mount instructions all ready to go while the ec2 instance is spinning up so we are saving some time here if I can find the folder as there is so much going on here today we'll just say read me MD I'm going to go ahead and grab this here and I need to grab this line you can also use the IP address of the file system instead kind of would prefer to do that if it has an IP V4 address um and so here it's we're just taking a look here so version 4.2 NFS and then we need uh the IP address let's go back and take a look and see if this one is ready and we have a public IP address I'm just going to go ahead and Swap this out uh as such okay we do need to create ourselves a folder so let's go into the cct2 instance and connect I'm not sure if we need any ports open I don't think so ec2 user and I'm going to just paste this in here as such let's go ahead and see if we can mount this wait a second wait a second this is the ec2 instance Port that makes no sense we have to get the one from here that makes more sense right so if that's the case we'll get the DNS name here and I don't see an IP address I guess you could follow through thei to get it um let's just go take a look here the fs DNS name yeah yeah okay so sorry my bad we'll go ahead and paste this in here as such and we'll see if this just works it'd be nice if this just works okay can we CD into FSX and I'll just touch hello.txt LS La there we go so I've mounted it I've ridden to it um I'm not sure if there's anything else that's interesting to look at here so we have our file system yeah pretty straightforward yeah there you go not too complicated let's go ahead and shut this down if we really wanted to do a good test what we could do is launch up other easy to instances and attach them I don't think we need to do that here today I think we satisfied the most basic thing we needed to do so nice to see that we have this attach option though oh it gives you instructions well that's nice wasn't that hard to begin with but we'll go ahead and delete delete our file system here I do not want to backup and I acknowledge that I'm deleting this yep go ahead here and go ahead and delete this file system I acknowledge yes I want to get rid of this deleting there we go and I will see you in the next one okay ciao hey it's Andie Brown and we are taking a look at Amazon detective which analyzes investigates and quickly identifies the root cause of security findings or suspicious activities you might be asking is this an S uh and according to best they would not classify it as one you'd still work with an S along with that um this is a really cool service um I wish I made more slides on this because it's just so cool all the information you can uh extract out of your account and find where the root causes are we do have a lab so you will see that there um but the point is is that one key thing it can do is it quickly identify Trends so here's an example of uh looking at I am principles either roller user or ec2 instances and you can see uh you know the activity of API calls being made so you can quickly identify where something can go wrong so that is something that's interesting uh if you click into a principle or you do instance it'll tell you generally where the API calls are being made on a global map you'll get a summary list of how many times specific API calls have been made uh you can see how much groups of API calls have increased in volume um you can launch investigations on specific i principles and see if they're use utilizing specific tactics and this feature is really cool because they use a behavior graph and so they can basically make like um a path of attack like uh and show you generally if they think that the uh I am principal is doing that so it'll make more sense when we look at the lab um but this is a really really cool service hey this is Andre Brown in this video we're going to take a look at Amazon detective so if we want to find the service we have to spell it right but this one can investigate and analyze potential security issue so uh that sounds very useful here before we end up utilizing it let's go take a look and check its pricing just so be doesn't get mad at me so Amazon detective is price based on the volume of data ingested um no additional charge for 30 days $ per gigabyte so that sounds pretty good um I'm in an account that has uh running workloads in here and I have cloud trail turned on um so this should be something that I can utilize to find this you don't have to do this because you're not going to have anything substantial to show here but if you uh go down below you can see that we have stuff here now I didn't activate this I'm going to go over to another region I just want to see maybe I turned it on before yeah so it seems like what I did is I probably went get started and yeah we created an administrator account an attached ion policy and that's how we got about to that I'm not going to go through all that I want to go back and see the results that we have and so um go down below you can see roles and users with the most API calls so we can see that brown laptop that's me here um I've been making some calls as of late and then we have this stuff here so this is probably really useful if you're trying to narrow down where something might be happening at a at a particular time um then we have ec2 instances with most TR most traffic we don't have any containers running in this account so nothing showing up here uh and nothing on the big map which seems really cool let's go ahead and see what we can do if we run an investigation so choose recommended resources um there are none under here so choose an aist role or user that appears in recent findings and finding groups to investigate so it's not showing anything so we'll go over to here and I'll choose a user and I'm going to choose myself so I'm going to go over here and go grab the R so just go over to users okay and so for this user I'm going to grab their AR and I'm going to go back over to detective and paste it in and we can change this to be I don't know till here till then and we'll run that investigation um the window must be between 3 hours and 30 days long okay so maybe I went too far out try this again there we go and we will wait for a report I'm not sure how long it takes but I imagine all it's doing is looking at cloud trail events um so hopefully that doesn't take too long oh there we go that was fast we'll take a look here it says low so here we have high severity medium severity low severity so for Discovery okay we click into this is trying to understand and learn about environment initial access that adversary is trying to get into someone else's Network persistance the adversary is trying to maintain their foothold impact their adversary is trying to manipulate interrupt or destroy your systems and data wow uh the adversary is trying to gain higher level permission so I'm guessing what it's telling us is that um based on the level permissions I have and the behavior that I have that um I this account is is a high risk account or high impact account makes sense it's an admin account um and so that could be the reason why there is an issue here but now it's showing us as to why so you can see that it can execute change sets start instances stop instances so like if you were running instances or you had confirmation saacks this user could tear them down which they have permission to do we have persistence so we here we can see that I can create access keys so maybe uh you would not Grant the user to to create access Keys you'd let somebody another account do that and that would uh reduce those problems there and I can create users and everything privilege excal uh um escalation I could promote somebody else to have permissions or even myself here because I can add a user to a group for Discovery here um it's suggesting here like what stuff can I see so like if I was trying to collect information you can see that I have um the ability to look at a lot of stuff um and then we have login so let's say we didn't need to log into the UI we could turn that off so that pretty cool I like that um it gives you kind of an idea as to um what you're looking at here so yeah I'd say this pretty cool tool um and then here we have finding group so summary now powered by AI so I think this is the user we just ran here unusual behavior for the I am user oo cool looks like we getting some AI here so the the brown lap laptop has newly observed issues API callings the Google Cloud platform in the geolocation of Dallas us that is not unusual because I did tried to do a lab um to connect to gcp um that or we might have been moving data over to there so that's totally fine uh we'll go down below here and notice that it would try to determine if we have any observed tactics so this is pretty cool because this kind of gives you an idea of the level of problems that you could have here uh like with a malicious actor um then we have here these are I guess things that I'm exploring so uhuh Amazon s block access what's granted from s bucket terraform project I mean we did the boot camp quite quite a while ago there but uh that's interesting then we go down below here so Amazon s3u Anonymous access was granted for the uh bucket that makes sense for what we're doing that for yeah I mean it's pretty cool like in terms of a visual but um I guess says wow I don't think wow stands for uh what we think it stands for but um week over week I think that's what it probably means like week over week but it looks like wow there's a problem but anyway there is Amazon detective and uh hopefully you found that interesting ciao hey this is Andy Brown we're taking a look at ads batch which plans schedules and executes your batch Computing workloads across the full range of AD compute Services it can utilize spot instances to save money uh we have a few components here we have jobs job definitions job cues the job scheduler by default the schedule uses first and first out you can run batch on ec2 fargate eks it can handle an array of jobs it can handle multinode parallel jobs it can handle GPU jobs and you can have GP uh job dependencies now can I just say something about the service because I really wanted to do a lab on this uh because the service has been around for a long time and I just they never required you to ever make a lab and so I thought okay in this refresh I'm going to make a lab on this and I could not believe how junk of a service this thing is um I could not get it to uh to work there was no information on on how to how to get it to work um I'm sure I could have got it to work if I went out to adab best support but um we will learn how it works we have a lab here even though we don't succeed at the end but we go through all the uh components of it um and so you can see for yourself the frustration I'm having with the service but we will understand it for um the exams okay hey this is Andrew Brown and in this video we're going to take a look at batch so um I'm going to go over to my GitHub repo over here in ad examples I'm going to open this up in git pod you can use cod spaces Cloud9 whatever you want but I'm going to open open this up uh this will already be connected to um my adus account here using adus credentials and the idea is that we want to go ahead and create a batch um a a batch job and so it shouldn't be too hard we'll have to define a job which we could do it over here uh and fill it out but I would rather programmatically do it since I think that is um the better way to do it there's also a wizard um but again you know we'll just try to programmatically do it because I think there's going to be a lot more value um if we go and perform it uh in that way there so I'm over here um in on my computer and uh what I want to do is go to the batch directory because I kind of prepped a bit of this I didn't actually uh do it end to end because I wanted to see if we have any issues but the idea is that we need to register a job and then submit our job and to register a job we need to create a container and that means the container will need to be pushed to ECR so what we're going to do here is first test our app so I have this Square do application and all it does um is it uh performs uh a square function okay so something very simple and it will have to take an input um so I want to just go here for a second and look at our Docker file as this is not providing any kind of uh input parameters so I'm wondering if this is actually going to even work um because it says if then it'll provide the usage so what we'll do is we'll CD into our batch app and first we'll make sure that it works I'm going to type in Python and then type in square pi and see if it works and notice that it's asking for number so I'm going to go ahead up and put three so now we're getting this the the square of three is nine so what we'll do is go back to our Docker file I'm going to tweak this I'm going to put the number uh three here and so the idea is that this is what this Docker file should do um so I want to go ahead and build this image and so I know in our ECR section we probably covered this so we going to go over to our ECR section and also we need to log in as well so we're going to have to um uh uh provide the login so somewhere here we have our our login information um so this one is for Central one which is totally fine this is the same thing you can get over at ECR so if you were to go make a repone ECR which we haven't done so yet but we need to do that like if I go over to here and I'm in CA Central 1 which is where I want to be what we'll do is go ahead and create a new repo this one's going to be called Square and it'll be private because I don't care about it being public we'll create the repo and if we go into square you'll see here that it's just the same command here that we just ran okay so we are now logged in so we can technically um do that what I want to do is build our app and so this line here will be sufficient for us to do that so I'm going to go back to our read me here and we'll just say build image and I'm going to go ahead and copy this line here and paste it in here and I don't want to be the app director I want to CD back out one and I'm going to go ahead and grab this here hit enter it says open Docker file no such file or directory so why is it having an issue well there's a Docker file right here um oh you know what it is it's because we're expecting the app to be in the app directory here but we actually just want to sa period that's actually what we want to do so we'll try this again and we'll go ahead andit enter and so that should build this container and I believe that we should be able to run it here on the right hand side so we'll wait for that to build all right so we have a minor issue here where it is saying well what is it saying um so we have work directory slapp on line 7 so change the working directory um we don't actually have a directory called that so we might actually have to create that before uh we do that so I'm just go ahead and go say run mkd slapp and maybe that will resolve our issue I'm going to try this again and see what happens it still has an issue because it already exists okay so then we'll try it again I suppose and it says an issue here failed to solve mkd well I remove the mkd right working directory SLA failed to solve mkd not a directory uhuh okay well what if we were to take this in placees up here okay not sure why we're having such a hard time with this there we go and it might just be that it created this and it didn't uh the order the order might have mattered because maybe it cre created this file as opposed to a folder and now that it is a folder then it will place it in the corresponding location not exactly sure but uh that's totally fine so what I'm going to do here is go over to here and I'm going to see if we can run this and so what I'm looking for is to see what it outputs so we don't see anything that's totally fine but if we go ahead and there should be a way to look at its logs may be inspect here no and I'm just going to bring this on over here get this out of the way there should be oh wait to see logs used to be a way to see logs at least terminate will etc etc so the question is is it working and that is what I'm not 100% sure about I guess the other question is why would python output to here so let's just say python you know what there is a command that uh that changes the buffer and I remember doing this for one of our apps I think it was an ECR and if we go into the darker file here I think it was this line here so I'm going to go ahead and copy this I'm going to paste this here I'm going to see if that changes the output at all and I'm going to go back over to Docker and I'm going to right click and run and we'll see what happens this time so no changes but the question is like how do I see the logging so uh Docker image uh C logging because I thought inspect that's what it would do so we have Docker logs that is the command that we could use so we're going to go back over to here say Docker logs and then it will want um the particular container so this container is called what app NP and so we'll say Docker list Docker PS Docker images okay and so we'll try this again Docker logs and I'll try to uh provide the image ID see if that makes any difference because it can only run I can only get it from an active container right so the point is is that in order for us to know this is working we need to see logging um and that's where I'm having a bit of frustration let's go back over to our app here and yeah just print it out so that's what I want to observe so give me a moment to figure that out so one thing that was suggested was maybe we could give it the hyphen U flag so if we go back over to here um I don't know that'll make a difference but there is a flag called hyphen U and we'll see what that does so we're going to try this again we'll say um we'll have to I supposed build it again right Docker build and then we'll go back over to here and I'll try run and I still don't see any output I'm not suggesting that it's not working because we could go interactive probably and now it's outputting okay so I mean I I think that's sufficient as long as we know that it is outputting it's working then I'm happy with it um I just don't necessarily know how to do that on a oneoff here right now so uh what we'll do is we'll just proceed forward so now that we have that part the next thing is to upload um our app here so I'm going to go ahead and follow the commands as expected so we'll do square and then I'll grab the next one hit enter and I'll grab the next one and we will tag it and I'll grab the next one and we will push it and so now we'll have our container in ECR so we'll just give that a moment to push all right so that is now pushed I'm going to go back over uh tdcr here and so we should now have a repo that'll give this a nice refresh there it is uh what I need to do is grab um the UR and I believe that what it wants here in the next command is our URI so what I'm going to do is replace this here and hopefully just as that that will work you don't want to have the double e on there though that was kind of a copy paste issue so we'll go ahead and take that out and I'm going to close this out we'll go ahead and copy this we'll type in clear we'll hit enter and it says when calling the register eror executing vcp is required so I figured there should be more in the configuration here than this it seems a pretty uh pretty low as I remember having to do more so we're going to bring this down here let's bring this up and we'll have to look up how these definition files are written so we'll just look up uh container container definition uh batch example and do we have an example here I mean it has a lot of stuff in here here I just want the Json which is kind of annoying maybe I can see a Ruby version probably not they are not making this one easy for us so I know I have it in my slid so give me a moment I'm going to pull up the slides so we can reference them yeah you know what I can't seem to find look at this I only have one slide I remember making more than one slide for this batch but I guess I don't have the example code so we're going to have to figure it out by hand here so it says register job definition uh vcpus is required Etc so I guess we'll just have to go down here and see what it wants because even going here it doesn't say vcpu it just says CPU V CPU so so this is not the best we'll go to inab CLI and we'll say register batch we'll see if we can find an example here so this one here has vcpus and memory and so this one might be a little bit more uh easier for us to work with that's probably all we need I'm hoping anyway so go ahead and do this and we'll go down here um and I'm going to go ahead and copy this and clear oops clear copy enter and there's a syntax here somewhere oh you know what it is I'm missing the double quotation here so we'll go ahead and copy this and we'll paste this in here so now we have our job definition so excellent we have that we'll go back over to batch and we'll see if it exists and I'm going to go over to job definitions and so we have our our Square job here and so that looks pretty good so far um so yeah pretty much defines it so now the next question is can we just run this and will it work and so here it's saying we have a job we have a definition and we have a que um I didn't create a que do I have to create a queue first because I haven't made one so I'm expecting that this will fail we'll hit enter yeah so it wants a q so we'll have to go create a q next so we'll go all the way to top here and while we're here we might as well just copy these so that uh you can find them easier easier later on so that is the that one there all the way to the top um I'm going to go back to batch batch batch batch where are you oh here it is batch and we'll say create job Q we'll go to examples and here's one which I wish it was a little bit easier than that but I guess it's not so say create Q um and it has CI input but it just been nice if they actually just not done it that way and they just had all the options here so going copy this here because I think I might want to just manually specify I don't really want to supply it like this and I'm going to go back over to here whoops go all the way to the top and so we have state we have job Q name so that'll be the first thing so it would be uh my job or what did we call it up here or down here we called it my job Q so my job Q that's part one take that out of here and then we have a state so I'll just put State enabled priority 10 okay so that's there we have compute environment order so we'll go grab that next like what lazy person ads did not make proper examples I don't know why like yes this technically works but why you know what I mean um so we have create job Q my job Q enable 10 compute environment order and I'm just looking for the shorter form of it so this is the shorter form and so here it's suggesting like what it would prioritize it on now I don't really care what it runs on as long as it runs map to a job queue the job schedule uses the parameter term and compute um all the compute environments must either be E2 or spot or fargate or fargate spot so I don't really want to run spot I'd rather just run fargate here it says confs must be in the valid State before you can associate them to the job all compute envir environments must be either ec2 spot or fargate spot the Amazon resource name of the compute environment so here we're actually providing the AR of the compute environment okay this is M4 spot that can't make any sense give me a second okay so over in cloud formation I think it's cloud formation it's telling us a bit more about it so it says compute environment you can Define manage or unmanage compute environments manage compute environments use E2 or fargate which makes sense unmanage can only use C2 resources um so I mean that's telling a little bit more so we go down to compute resources here this is not very uh descriptive it is not providing us good information here the Amazon resource name of the computer environment okay let me see if I can get chat BT to try to help us out here all right so it looks like what we need to do is actually create our computer environment and then they we're specifying it there so that's starting to make a lot more sense um so I'm going to go ahead and grab this here and I'm going to go back to our um our example here I'm going to grab this and since there's only one thing I'm just going to make this one and so now we have my compute environment the order is one so I think that is good there but now we also have to create a compute environment and I mean if this works that'd be great but I feel like there's a little bit more to it than this so we'll go here and the question I have is does this service Ro already exist that'd be nice so now this manage thing is making sense we have minimum vcpus One Max vcpu 64 I don't think so how about one uh instance types uh I don't want an M4 that's for sure so let's go ahead and go to compute environments and see what options we have so let's say it's far and we do have this a service uh roll for batch is that we're using a service roll for batch service r no that is not what ours is called and I'm trying to select it but it's not making easy for me here I'm going to make a new tab I guess and go over to am whenever the IM am tab wants to open here all right there we go so uh we'll make our way over to im as I was saying before and what I want to do is find uh this Ro see exists so inabus service R for batch and we'll go into uh roles Abus service roll for batch notice that it doesn't exist yet because I think it would probably create one when we do this one so I guess I'll just create a compute environment here compute EnV we'll say fargate and I'll say Max CP uh vcpus one we'll say next and the rest of this is fine next um we'll create that compute envirment and so maybe we don't really need to uh you know what I'm saying maybe we could just use this one and we can just skip the step because it looked like it had a lot of steps here so I think what I'll do is just skip that one we'll go down to RQ here and then it said that it wanted from our compute environment the r so that would make me think that it would want the Arn here right like that because it said AR in the docs let's go ahead and see what happens if we try this and it does not like that so what if we try this instead okay we'll copy this we'll paste it in here and enter and it still doesn't like it so we'll go back here I guess and read the docs here so this was for creating the job queue here it says the Amazon resource name on for the environment okay well what we could do if it doesn't like this we could just um give it its parameters here Compu environment is yeah just the IR so I'll grab it here like this and we will paste this in as such and we'll do this and we'll say one and then what I'll do is I'll go back and grab the full AR because again it's saying that it wants the AR and we'll paste that in like that we'll go ahead and copy this and we'll hit enter um I think it's not supposed to be back ticks here so I'll just fix that it's very easy to get the backticks mixed up I'm just so used to pressing back tick we'll hit enter still doesn't like it the following arguments are required compute environment order so I think the issue here is the fact that I just forgot a hyphen there we might not even had to change that to Json we'll try this again we'll hit enter and so now it's created our Q excellent let's go see if we can create our job now it says the job Q is attached to computer environment that cannot run jobs with capabilities of ec2 um the job Q is attached to a computer environment that can cannot run jobs with the capacity of ec2 so there's something it doesn't like here so we'll go back it's kind of frustrating we'll go take a look at our job queue which we have one it says ECS fargate type which is what I thought that we are running here um we'll go back over to this and here we have our job CU and then we have our job definition so what is the problem and this one is a container so that should run on ECS so that should not be our issue here and our compute our compute environment is fargate so that should not be our issue either I guess the other thing is do we have to Define I mean we could try it here so say my job and we can try it here and see if we get the same problem notice it shows no job queue why not we'll go back over to here says it's valid says it's enabled says it works with ECS fargate we'll submit our job again and nothing so there's clearly something wrong with our job queue let's go over here I'm going to disable this disable this queue and delete it we'll create it by hand so we'll say fargate my job queue um I don't want a scheduling policy we're choosing our compute environment that we want to utilize which is fargate here which is great we have Priority One everything else seems fine we'll create this my job Q we'll create this my job Q2 we'll create it again there we go we'll go over to here it's still deling the old one let's go ahead and create a job my job wow the service is not a fun one and it doesn't show up so give me a second okay so no answer here whatsoever so I'm thinking that maybe it's something wrong with our compute environment so let's go over to our compute environment we'll go ahead and hit oh I didn't mean not I did not mean to disable that we'll go ahead and we will hit edit so we have one vcpu which is totally fine it's enabled we have our v v uh our uh VPC selected our subnet selected so everything seems fine here there's nothing that would indicate to me that this is wrong okay it is updating right now now it's back to valid we have no minimum or desire it's just maximum is set to one I wonder if that's messing it up we don't have options for the other values here so that's interesting says ECS fargate type let's click here and take a look again and this says fargate so do we need this no that does not matter we have our compute environment we don't need to do anything else here so it seems like we have everything that we need to create this I guess we'll try again and we'll drop it down and yeah it just does not let us select the job queue I'll go research a bit more okay so yeah no no solution in sight here yet I'm going to go back over to our uh job queue click do it I'm just making sure that everything's enabled and working so this says the queue is healthy it's enabled looks like it's ready to go it has this compute environment here so clearly we can utilize this one this one says it's healthy like are we just being impatient here and we just have to wait for it to work still nothing to select I'm going give this a hard refresh and yeah so I'm going to say this is quits here because clearly it batch is the problem not me there's no information out there as to why this is happening I'm not going to go pay ads support to utilize your service but anyway you have an idea of how this would work so the idea is that we should register a job create the compute environment create the queue submit the job and it should just work um but what we'll do here because this one is total bust I'm going to go ahead and tear this down and it just sucks when I have a lab and I feel like we're so close to making it work but there's nothing I can do here but uh I think this is kind of betab best's fault here for this shoddy service so we'll go here and disable this and delete that and so everything is cleaned up I will see in the next one okay ciao itms firewall manager allows you to centrally configure and manage firewall rules across accounts and applications enable Services uh that can be managed on under the service would be Aus wff WF classic Shield Advance Security groups network access controls ads Network firewall Amazon refid 3 resolver DNS firewall and no there's not a double A in here that's just my mistake I apologize cross that not um and third party firewall services and the interesting thing about the service is that when you go to configure any of these uh it's completely different so like this is an example for security groups and it shows you some policy rules that you can select but this thing is wildly different based on each service that you are creating a um a rule for the service is uh I would describe as expensive at least I don't want to spin one up uh in the lab because uh B would be like why are you spending $50 for uh you know such a simple rule but we do go and look at how they vary and things like that even though we don't actually create a rule in the lab there are some prerequisites here so your account must be a member of the organization your account must be the adus firewall manager administrator you must have adus config enabled to uh for your accounts in a region um and you might need to have Ram enabled as well for specific uh security firewall Services I suspect that when you use a service it's creating it config rules I'm noticing that there's a lot of Security Services where ADS will just leverage ads config underneath and make a layer on top of it and then charge an ungodly amount uh so I almost feel like you could just make it config rules if you really wanted to um uh enforce compliance for specific configurations but this is a service so yeah there you go hey this is Andrew Brown in this video I want to just take a quick look at inis firewall manager um so it firewall manager is just a way where we can um centralize all of our security management for our firewalls which is pretty straightforward um there are some requirements you must uh your account must be a member of an adabs organization your account must be uh must be the adabs firewall manager administrator I'm pretty sure that if you're an admin you have access to everything so that should not be much of an issue here you must have it uh it was config enabled for your accounts and regions um so there are quite a few requirements so what I'm going to do is I'm going to log into my other account where I actually have it was config uh configured because I don't actually have it configured in this account and uh we'll take a look at this again so just give me two seconds all right so we're logged in to another account where I actually have stuff running and we'll go ahead and type in firewall again and we'll make our way over to it firewall manager so if we want to uh turn this on there's a lot of options here now the other thing is that I don't really want to turn this on because if you look at the price here it's $100 per month all right so we're just going to dip our toe in and and take a look at what we can without causing any spend and if I have a $100 spend boo is going to get mad at me so we'll go ahead and create our Administration account again we haven't created a policy yet so that is now um there so it might take some time to propagate the changes in your org check the settings page in a few minutes uh for that okay well we have settings over here and I guess we're waiting for the administrator count and now it has appeared so that is good um it config should be enabled in this account it was config can be region specific so I am in North uh Us East one so this should not be much of an issue but normally when you have a config you can turn it on if you don't have it turned on so if I was to go to another region let's say uh Paris right if we go here um normally there's like a oneclick setup to get started there's also the um a cloudformation yaml template you can download which I kind of prefer I mean they might both do the same thing I'm not sure but um yeah so we'll go back to getting started and so we'll create the policy and again I don't want to create one because I don't want the the the cost but I just want to show you what options we have so notice that we have it was W WF classic uh security groups Network firewalls let's go ahead and choose Security Group and we have security group policy types so apply security groups to specified accounts that sounds good to me we'll hit next and then we can name the policy so my policy expensive my expensive policy and then we have some rules so identify report when a security groups created by this policy becomes noncompliant dis associate any other security groups from the resources with the policy scope distribute tags and primary security groups to the security group policy etc etc then down below we have policy action identify resources that don't comply with the policy rules so I'm seeing lots of synergy here with itus config because it must be using that for compliance uh notice that we're not seeing any security groups here I'm just going to go ahead and select them all there we go and we'll go next and now we have policy scope so include everything in this account yep and then we have our resource types and things like that so that looks good automatically remove protections from resources that leave policy scope we can leave that alone oh I guess we have to select something so we'll just say ec2 here and we'll go next and then we have our policy tags all right so we've defined our rules so I imagine that these rules would create a config um stuff but again I don't want to create this because it's really expensive but that gives you an idea of what you can do imagine if we went here and choosed a different service we might get different things let's go here and yeah it looks very different I wasn't expecting to be very different yeah so it looks like every single Service uh every single service it has completely different look looking policies so we can go ahead look at w here now and yeah just totally different okay so that's cool um but uh yeah hopefully that gives you kind of a a peak into the firewall manager and I'll see you in the next one okay bye hey this is Andrew Brown and we are taking a look at the inabus transfer for SFTP so the inabus transfer family offers fully managed support for the transfer files over SFTP as2 ftps and FTP into and out of Amazon S3 and EFS so what are all these protocols well the first one is file transfer protocol this is an early Network protocol without encryption used for transferring files over a network then we have the secure version of it which uses SSH then we have ftps which extends FTP with the support of TLS um then you have as2 and so yeah there's a lot of protocols the common ports for these protocols would be Port 20 and Port 21 uh for SFTP this is Port 22 ftps Port 990 as2 443 so yeah there's a lot of stuff going on here um I used to use FTP quite a bit before the days of uh GitHub repos or logging to server basically all you had access was to SFTP or FTP because you're using shared hosts but these days not used so much but uh depends on the organization um there's also the transfer family managed file transfer workflows so this is a fully manage serverless fire transfer workflow service to set up run and automate monitor processing for files uploaded to the a transfer family workflow is allow you to perform the following uh following after file is uploaded so copy file tag file delete file custom file processing step decrypt file I'm just going to tell you right now that the inabus transfer family is really really really hard to set up um in fact we you we end up having to launch a simple ad um um what do you call it uh directory this actually might be one of the hardest uh Labs that uh that I had to do and I had to get Boo's help to do it just because we are going to have to touch Windows servers so um you know I didn't I kept the slides really like here cuz we're going to spend a lot of time in the labs but basically this service is for using FTP so it's pretty straightforward but we'll jump into the lab and you know if you're not confident just watch it because it is really hard okay hey this is Andrew Brown in this video we're going to be doing two things we're going to be um utilizing Amazon transfer family so that we can utilize FTP or whichever protocol that uh this tutorial is using the reason I don't no is because I originally attempted this fall along and I really got hung up on uh setting up active directory or the simple active directory so I had to pull in my cofounder B who's really good at Windows servers and an active directory and so I'm following through his stuff but I'll contextualize uh what's here because I've done enough of it myself to understand what's going on but when it comes to the windows configuration I am terrible at it so let's go ahead and uh jump into this the first thing we're going to do is open up Cloud shell as we're going to enter be entering everything into a particular location I NOS that he's doing this in Us East one I'm going to do it in Us East one and the primary reason why I'm going to do that there is because I don't believe that simple ad is accessible in any other place so like if we go over to the service directory um service directory service here and we drop this down you'll notice that there is no simple ad but if we go over to us East one I believe now that will appear there we go so we are going to be able to make the directory there I'm going to see if that directory still exists um it does not I'm also just going to double check and see if I need to clean anything up before we jump into this because I just don't want this to be a mess and I'm just again checking if there's any instances here nope everything seems to be good I'm going to just clean up my security groups and then we'll get to this okay all right I've deleted as many of my security groups as I can just to make things nice and tidy but let's go ahead and open up uh the cloud shell console and I want to have this in another tab just so I have a lot of room and we're going to be creating a VPC by default so just follow along with all the CLI commands here um and it might help for us to turn on um Auto prompt here so I'm just going to type in a CLI Auto prompt so we can get the environment variable that we need to set here uh I typed it wrong C Auto prompt V and we'll go ahead and copy this and we'll go paste it in we'll say on partial enter okay great so now if we just type in ads do we get our prompt no uh oh yeah we have to have a command so just type it ec2 here and it's interesting that it's not uh Auto prompting normally what it would do is if we set that auto prompt it would go um unless we have spelled it wrong I'm going to go ahead and try this again we'll just say export and I'm just going to put double quotations around this as the auto prompt should be working there we go and that's just going to make our life a lot easier because we just need to do a lot of CI commands here today so we're going to use the uh uh the VPC and I'm going to double check to see what VPC I have so that my life is a little bit easier here so we'll go over to um VPC and we'll do 10.0.0 for6 I just want to make sure there are no conflicts and apparently we already have an existing one I would like to get rid of this one so I'm going to go ahead and try delete it it says you have an instance that is here so I'm just going to get rid of these instances okay all right so now I've taken care of that other vbc it's always great to double check to make sure you don't have conflicts but we'll go back here and follow through as our instructions so the first thing is we'll need to provide the cider block and this one's going to be 10416 and then we'll specify the region is Us East one which is not necessary since we're already there but I'm going to put it in there anyway uh to be very explicit we're going to need a subnet so we'll type in ads I'll hit enter so we can get into that auto auto prompt mode you'll notice sometimes I do it sometimes I don't it just depends on how I'm feeling at the time of and if we go here it should it should autocomplete here I mean it should be able to it doesn't always but I'm going to go ahead and just copy this would have been nice if it did and uh we'll do a CER block here of 10. 0.1.0 I'm not sure why Bo made that 1.0 as opposed to just z0 but that's okay I'll just stick with what he has here because this is so dark hard that I do not want to even have any trouble um and for some reason we're specifying the availability Zone I guess it doesn't hurt we'll say US1 az1 I'm not used to typing usc1 availabil ones so looks a bit funny to me and it says that it's uh it's wrong here so let me just go back and enter in this mode here and if I hit I think controlr we can get back to our last command and I'm going to just careful look here just show you that I'm not crazy well you know what actually I did do it wrong it's az1 AZ one like this okay so I I I was wrong and so now we've created our subnet um the next thing we're going to need to do is create a private subnet so we're going to go ahead and type in AOS and we'll do control R to bring up our previous commands we'll go here and the key difference here is that we're going to just swap out one for two so we have two there's nothing that indicates that this is private or public um so what I'm going to do just to make our lives a little bit easier I'm going to go over to VPC just going to name some things here manually so we don't get confused we'll just say um what are we doing we are doing both ad and we're doing uh ft FTP so we go ahead and save that and in our subnets we'll go over here and so now we should be able to refresh this here and easily see what these subnets are one is going to be public so we'll mark this as public and one is going to be private okay let's go back over to cloudshell and continue on um I'm just double checking here apparently we need a second subnet there's probably a reason for it so like if you're doing FTP we might need to have two different um subnets in two different zones and it looks like the third one's going to be public as well so that's probably has to do with um transfer family because I almost remember having to select to before so we'll go ahead and just select this and it'll make sense as we work through this so don't worry if it it doesn't make sense right now but uh we'll just have to follow these steps because again this is just really really hard to uh do so we'll say cider block 10 03 0424 we'll say region Us East one and I'm going to say availability zone I'm hitting tab so I don't have to keep typing the whole thing us e and just get rid of this up here USC A2 and we'll hit enter and I forgot to um put the one in there so I'll just goit up so notice that we're putting it in Us East one availability zone two so there are two different availability zones still has an issue with the way we're typing it we'll go back and hit up and um US1 hyphen A2 oh id id so apparently there's a distinction between those two there we go I'm not I'm surprised boo did not just do availability Zone and choose the the moral normal looking ones I'm not used to setting them as IDs but that's totally fine it's it's still working so that's that's okay and apparently we need another one two so two public two private okay fair enough so I'll do control R here and we'll go up and then this one's going to be four and by the way make sure you change these numbers out because you can create subnets with the same ones and it won't complain which is a weird defect I I think but we'll go ahead and hit enter and let's go back over to our subnets and just make sure we really know what these are so we have public um this is in subnet what is he this is in B sorry wait which one is this one in this is in B this is in B this is in C this is in C okay so let say sub public B not sure why we're not doing a doesn't really matter but and then this one will be um 03 will be our just double checking here our public subnet public C and then 04 will be our private C just so we're not getting mixed up later on because it's so easy to get turned around and so now that we have our subnets the next thing we'll need to do is create an internet gateway uh again not sure why we're doing all these indiv but that's okay my cofounder he like uses the C exclusively so he might not be even aware that there's a wizard that we use in every single step and it would save us so much time but we'll just manually do everything it doesn't hurt uh getting good practice like this so go ahead and type in ec2 and we'll type in attach attach internet gateway internet gateway ID and it's Auto aut completing uh it's whatever the new one is so that's going to be the longer one so I'm going to hit down and select that one so that's finally actually Auto completed which is nice then we need our VPC ID and which one is it it is the longer one again so we'll go ahead and do that of course you have to really check and make sure you're choosing the right one um okay and so we have US1 again we don't have to specify that but we're going to and that will attach the internet gateway so that now should be attached we'll create our route table and I don't know why sometimes it autocompletes sometimes it does not which is kind of frustrating but we'll go ahead and paste this in manually oh you know why it's because we're not actually in the tool we would have to hit enter on that one we'll go ahead and create our route table there we go and now we need to add some routes I'm going to go ahead and add a route first I'm just going to type this in so we get auto complete uh we'll say create route table or create route create route yeah create route and I'm just going to hit enter here just to save us some time assuming that the auto complete works and it is not it's all right we need to specify our uh cider destination block which will be 000000 sl0 we're just routing out to the internet that's all we're doing here Gateway ID um and that will be our internet gateway which is here and then we'll say region Us East one gray we'll go ahead and enter and so now that is associated or attached I should or the route is out to the internet we need to uh we should explicitly associate our our um our subnets to the route tables that's we're do now so we have that command we'll just say route table ID and this one here is the last one and then we will select our subnet so we need our first Subnet and I'll go back here so we need public B and public B and and and public c those are the two that we need so here I'll go here I'm just going to grab it because it's going to be too hard to grab it any other way and we'll say region Us East one and I was trying to hit up there but I guess it didn't autocomplete how I was hoping so I'm just going to go ahead and copy this and I'll paste this in here and I'm going to go over to our public SE what's even more confusing is in this tutorial uh that my cofounder wrote he called it subnet A B C D and so I've I've called them U more reflective of what they are and so I'm hoping I don't get mixed up here so we'll go down and we'll take a look at our actual VPC here and we'll go our resource map out we have two route tables we have two public going here and private going there and so that makes sense so this one's not attached to any internet gateway so that's basically our private rout table and this is our public route table and we can even just indicate that more clearly here if it helps um so we have this one this one has a route to nothing so that's our default table hit refresh it's not showing all our tables is it let me go refresh this here and if I go into here this is our public yeah this is our public RT and let's take a look at what the other one is so this one is up here and this will be our private RT okay because this one should not have a route out to the internet which is fine and then the other ones are just automatically associated with it and we could give them an explicit Association which I think probably would make more sense so I'm going to go ahead and do that it's not part of the instructions but I'm going to go do that right now so that uh I'm not confused the next thing we're going to do is create a um active directory I'll show you in the console but we we have to do this everything through the CLI because that's our instructions but we'll just peek into it to see what that looks like so we'll go over to um service directory or directory service however it wants to be called and if you drop down here you we have SIMPLE ad and so here you have some options and the idea is you go here next again we're not going to do this for real but the idea is that we want to choose a small siiz directory we choose a directory DNS name uh this could basically be anything it doesn't have to be a real a real domain and then your net bios is like this short name that you use when you log in then you set an Administration password a confirmation password then you choose your vpcs and subnets you have to choose um two subnets and then you are uh good to go this takes quite a while to spin up so we will be patiently waiting for that uh you do get a free trial for the first time you run it um I've already ran it multiple times so I imagine I'm out of trial if you're concerned about costs go look up the cost it's not hard just type in simple ID pricing and do the diligence there yourself uh so let's go ahead and create this direction so I'm going to type in adabs DS and create directory and then we'll start entering all the stuff in so the first will be the name and I'm going to call this ad exampro doco it's not uncommon to have a subdomain for your active directory uh I'm going to put a password here I'm going to make my password capital T testing 123 uh exclamation mark Bo has uh put in a Star Trek reference in the documentation here he wrote terac nor which is a Star Trek reference I'm going to keep it simple here for folks we're going to select the size which will be small we'll choose region USC to one we'll select the VPC settings or assign the VPC settings and so here we need to assign our VPC ID and we'll go over to uh here we'll just scroll up a little bit should have a VPC here somewhere there we go there it is we'll copy this and we'll paste it in and then we need to select subnet he calls it subnet b and d and I believe those are our private ones D is private I'm going to just double check here I'm just uh yeah it is okay so we'll go back over to our VPC we'll grab it over here it's a bit easier to see what we're doing if we do that and we'll go back to our subnets and we want the private ones so I I call Private B and private C to reflect where they are we'll go ahead and copy subnet the subnet here and this will be um we'll go down here and we'll we'll choose I lost my spot here we go uh subnet IDs and then we'll paste that one in put a comma we'll go grab the other one so we just grabbed um the first one and then we'll grab the second private one and we'll paste it in and we'll go ahead and hit enter and it did not like it so I'm going to go ahead and hit up sorry command R sorry sorry contrl C we're going to hit up hit enter Then command R and I'm going to go hit up again and so I typed password wrong okay password we'll go ahead and enter unknown options subnet IDs I might have forgot a comma or something let me just see here and we'll do command R we'll hit up and that's my issue is that I don't have a comma between this otherwise the short syntax isn't going to work correctly and we'll hit enter holy smokes this is uh this is tricky um again this is not how I would write it uh you know but I I don't have a choice CU it's so hard to set up I really have to stick to our instructions as per per it setting so here it's was complaining that saying the following arguments are required we have password there so what I'm going to do just to make my life easier so I need to copy or put quotations around this stuff here um maybe the password's not long enough so I'm going to make it 456 I'm times that's a complaint and the size is Right everything else is right let's hit enter unknown option subnet IDs all right let's look at this command this is ridiculous we'll go over here we'll go down to Output Global options no nope doesn't tell you it so we got password we put that in we have have size we have that in there and then there should be VPC settings in here and let's go to version two make sure we are using the right one okay here I have VPC settings subnet IDs did we put a space in between is that why it's complaining well this one didn't have it oh boy try this again command R we'll go down so this one has the password corrected it has the comma here but it needs a comma here so finicky I'll hit enter and there we go we finally have our directory now that doesn't mean that it's necessarily available let's go over to active directory or directory Services again and if we go to directories notice that it's creating so we're going to have to wait for that to create and this will just be some time we'll have to wait okay so I'll see you back here in a bit okay all right let's take a look here and give this a refresh and so now it is active we can continue on and so what we'll be doing is setting up the virtual server for the Amazon family transfer I'm going to open up a new tab and again just kind of peek in at what that looks like um so we'll make our way over to adus transfer family you can search to the top or just click through and so here the idea is that we need to create ourselves a server we'll choose our prot call we'll have to choose an endpoint domain Etc um so if I just go ahead here you could see that we would choose a directory service and we go next and then we would uh choose something particular here or maybe it's in vpcs because it's internal all right so you know you get the kind of idea here let's go back over here and enter the stuff in so we'll type in adabs transfer create server and I'll go ahead and hit enter here and so we're going to need an what no I didn't I didn't think that would work okay go back hold on that wasn't supposed to work we'll go ahead and delete that that was silly I wanted it to enter into that other mode so I'll go back here I'll just type ad us transfer this time create server rarely ever does that ever work so I was really surprised endpoint type is is going to be VPC the endpoint provider sorry identity provider identity uh provider type this is going to be adabs directory service we're going to choose our region to be USC to one so we don't uh don't muck that up even though it'd be very hard for us to muck that up we'll go endpoint details and so in here we're providing the VPC ID so we'll go ahead and copy this and then the subn the syntax is identical to this one so I'm going to save myself some trouble and copy this one but the question is which subnets are we utilizing here because we need a and c and these ones are the private ones which are B and D well I say a and c and b and d That's be's naming but I think these are the uh private ones and these are the public ones so we can't match this other one exactly we're going to have to just make sure we extract out the correct values here and so we'll put a comma and then subnet IDs and then it's equals and we'll go and grab the public subnets so we'll go over here make a new tab we'll go over to VPC and we'll go over to subnets and we'll grab our first public subnet which is good and we'll put a comma here and then we'll grab our second public subnet okay um are we done we need to provide our identity provider details so that's another thing that we need to provide here so this one is going to be the directory ID that we need to provide and so that's going to be this right here and we'll go ahead and enter and there we go so now our virtual server is configured let's go back and take a look give this a refresh here so we have it it says it's online VPC Point type domain is Amazon S3 um I'm going to assume that's okay we'll go into yeah because that's where we want it to be and the thing is we could send this to e uh EFS or or S3 really depends on where we wanted to go because we didn't go through the usual um interface that's why it's a little bit confusing it's not that confusing but it's not as clear okay apparently there's a alternate port for 2222 which is interesting um so we'll just continue on so we'll go back to cloudshell and just give me a second okay so it looks like we need to stop the server um not sure why but uh I think it has to do with some other configuration be didn't write exactly why but I'm just going to follow his steps and stop the server that doesn't necessarily mean we're deleting it we're just uh turning it off for the uh for the moment and we do have a lot to configure so that kind of makes sense um it stop server we need to specify transfer here now we're going to allocate uh to uh elastic I addresses for bc2 say region Us East one so there will be one and then we'll head up again and then we'll have two need to assign these eips to the transfer service so that was what we'll do next so we're going to update the server that's why we shut it off because we are associating these static IPS so type in server ID and we'll go here and copy and paste we'll say endpoint details and then we will uh I'm going to just hit enter here so that I get the auto complete which is a lot nicer there we go we'll say um address allocation IDs and so it's going to be this one here P that in and we'll grab the second second one here put a comma here paste this in square brace and then we'll just again just stick stick with our region make sure we are creating this in the correct location okay and so those are now Associated I wonder if we can see this anywhere so I'm just going to look here before we do anything else um private IPS public we give this a nice refresh here uh hard refresh here we can see that it's offline right now I see public IP addresses maybe there these weren't there before so maybe that's what we had done and now we need to restart our server or just start it up I should say transfer start server and we'll just grab the server ID here and hit enter um probably have to uh yeah put the flag in front of it here server ID and we'll go back to our transfer service and we will make sure that is is starting up so it is currently starting as we're waiting we need to create ourselves an ec2 instance profile uh so we can do that via the uh via via the CLI as well so I'm just going to make a new file I'm going to type the word vi now we're entering Vim mode which is very particular so be very careful what you press here and so I'm going to hit I for insert mode I'm going to right click and paste and notice what we're pasting we'll say paste and notice we're in insert mode I'm going to hit escape and now we're entering normal mode Escape just click if it loses focus and I'm doing colon see down below it's down below here WQ I use Vim every day uh it doesn't want to save this file as we didn't specify a file name so I'm going to go colon Q exclamation mark to quit out of this I type in V I'm going just say policy here policy. uh Json okay and so again we're going to enter insert mode by hitting I we're going to right click paste okay we're going to don't worry that it's mess the formatting is messed up just click into here hit Escape colon WQ that formatting definitely does not matter we'll go ahead andate cre ourselves a new role we'll call this ec2 sessions session manager R now we obviously don't need this because we've created roles in the prior but again just to stick with this tutorial we're going to have to really just follow everything just in case we'll go ahead and do this um it has an issue here it us I am create role rle name looks correct assume assum rooll document um our assum roll Ro policy document is what we're supposed to write here and we'll hit enter so doesn't like it we'll go back and go up we have to spell assume correct or won't work this thing is uh not really cooperating here too much today assume Ro policy document sorry hit enter there we go so that is now created um so we now have our um IM like our instance profile or to have our IM roll so we'll have access to sessions manager next thing we need to do is attach SSM Amazon ad access policy to the new role so we just created this Ro so I guess is not the same as our other one because apparently we are attaching additional things here in fact we're going to attach a couple things we'll say attach roll policy we'll say roll name ec2 sessions manager session manager here we got to spell it right session manager rooll policy AR and this one's going to be ar colon ads colon IM am colon policy colon Amazon SS directory service access it'll all be named the same for everybody cuz it's it's a manag one by adabs uh doesn't like our AR so AR adabs I AWS policy that's what it's supposed to be still doesn't like it got arm adabs I am let's see here arm adabs I am adabs policy Amazon oh ssmd okay ssmd okay I don't want to goof around with this all day so I'm going to make my way over to IM and we'll just go grab it manually and we're attaching a policy so we'll go here and we're looking for Amazon SSD M directory there it is we'll click into it and we'll grab the Arn here I have no idea what I'm spelling wrong here so we'll go up we'll paste that in here double double okay I just want to show that I don't have it wrong look ar inabus I am he's missing the double in here so it's wrong all right I'm gonna go ahead ahead and hit enter great so we've attached that one I'm going to hit up we need um the Amazon manage since core session so go ahead and do that actually that was just the trust policy that we did up here we didn't even actually attach any roles so we'll go over to our policies here and I'm going to look type in core and we'll grab uh this one here and we'll grab its AR we'll make our way back over to here and we'll paste it in and we'll hit enter so now we've attached that other policy apparently we're going to create an inline policy as well this just keeps going and going okay so um we'll type in VI and I'll type in join Json and I'm going to copy this code here you're going to have to write it out by hand unfortunately we're going to hit insert mode I'm going to right click and paste this in again I just want to point out that this stuff here is on the exam Pro platform and it's free all you have to do is sign up for free and and you'll get that in line and you'll be able to access that uh information okay otherwise it's going to be very hard if you are uh trying to follow along here we going to go ahead and paste this in and yes that is ugly I do not care I'm going to do colon WQ notice that we're adding SSM create Association SSM update Association SSM L instance Association for resource all I'm going to go ahead and say write and quit I'm sorry I'm still in insert mode we'll hit escape and then colon write and quit I'm going to just type in clear I'm going to do LS here and so we can see we have the join file I'm going to now join this here or not join it but add it as an inline policy so say put rule policy rle name ec2 session manager role policy name simple a d join policy document this is going to be file jooin Json region USC to one we'll hit enter so now we've attached the ability to use um uh to associate the join okay now we need to create our instance profile so that's why we created this was so that we could set up this when we launch up our um our ec2 instance it's going to be running Windows ec2 session manager profile we'll hit enter and it says it already exists okay um give me a second here because I haven't created it yet so it's saying another one already exists you know what I think it is I think it's a Bo made one and never deleted it and it's kind of hard to delete um instance profile so what I'm going to do is just delete it and then I'll be back here in one second all right so I deleted it and I'll go back up and I will try this now again you wouldn't have to delete it was just be the fact that we ran this lab uh previously uh I profiles are interesting because you can't really access them via the console but uh you'll only see them through here so it makes sense why Bo did not delete it after he did this uh the next thing we'll need to do is add we created our our instance profile now we need to add yeah we created the instance profile but the roll is not attached to it so that's what we're doing now and that's just a separate step say ec2 I think like when we did the uh ec2 stuff I uh I didn't actually uh go through these commands so it's actually great that we're doing this now so we're going to go attach that role cannot be found yeah yeah yeah ec2 session manager manager rooll there we go there we are and so now we need to create the security group that we're going to use so we'll say ads C to create Security Group and we'll say uh group name Windows I'll type it the same Windows sg1 description Windows serve SG VPC ID I love it if it auto complete here I'm going to hit enter and we'll try this again and see if it auto completes no of course it's not going to autocomplete it's weird that it like sometimes works and sometimes doesn't very inconsistent sometimes the order matters is what I found and we're going to go down here and grab this we'll go back here and paste this in here so we have VPC ID um and we'll just say region us east1 so now we are creating the uh this here we'll go ahead and check your current Wan IP for what it says check your current W IP for What on where one second I'll be back all right so Wan means Lo like like I know what Wan stands for but for I'm not a I'm not a networking folk so I he was saying like get your current IP address and we don't have to curl we can just go to this website and grab it so just say if config me and the reason uh we're doing this is that we just want to know what our current IP address so that we are only letting our machine here on Windows or Mac whatever being able to um gain access so we're really just wanting that address we'll go over here I'm going to type in ads and I was telling B I was like Bo if you ever do this again just make this a cloud formation template because it's so painful now he's showing me a photo of his Tomatoes it's so painful to uh we're both growing tomatoes I I do Hydroponics and his are bigger than mine and I I set up both with the bucket so something's wrong with my setup and I'm supposed to be the person that knows what's going on here anyway um let's get back to it here so we have our IP address let's go ahead and work through this again it doesn't hurt like it's great to get good practice here I just don't like writing so much here authorize group Ingress group ID and this will be the security group we just created I guess wherever that is we made a security group somewhere here I guess I'll go over here and go to SG and this is the one we created it's over here it's where there like window security group name and then name yeah whatever we'll grab that there oh sorry we want the ID we want the ID there we go okay so we'll go back over to here and we'll paste that in here and then we'll say protocol TCP cider and it says my way IP so just means your ipv4 address for your local machine because that's the whole point of ftps we want a FTP from somewhere I'll paste that in I'm not sure why this is C uh di it's supposed to say cider and we'll put for 32 so it's only the single IP we're going to give the port range of uh 3 389 and actually this is so that we can do rdf uh RDP so um I guess be actually wants us to RDP based on this we might end up adding another security rule using our um our IP address I thought this was for the FTP but I guess it's actually for the RDP which is fine and I'll go ahead and hit enter here um inab us E2 authorize group Ingress it doesn't like something I wish it just tell me what's wrong enter again so maybe I spelled this wrong in the front here authorize Security Group and I'll go down here and I'm just looking here um because here it's going describe the security group so I'm not sure why we're bothering to describe it so this says group name default get the default Security Group ID and associate with your VPC sure so I'm just going to save myself some trouble and grab it by hand here so I just want the default one for uh this one here uh because we have two I'm going this refresh because we made a VPC and I just want the default the default VPC for this so I'm going to just grab this VPC ID I'm going to go back to our security groups and then I'm just going to go ahead and and filter like this okay so now we're going to grab this default one so we're not getting mixed up here and we are going to add uh something very similar so do controlr whoops yeah this one's fine and authorize group Ingress and this is going to be very similar except um let me just check here oh no we're actually doing on the default one okay so we're going to do this and so this will be I mean okay it says from anywhere so we'll just do that 0 00 z0 and we'll just give it Port 22 what doesn't like okay spelled it wrong still authorize security group Ingress this is not even the hard part this is not the hard part the hard part is actually the Windows Server okay we need to launch a Windows server and look Bo couldn't do it in the uh the CLI so he started doing the screenshots and so what's interesting is that like Bo is really good at directory services and the thing is you can provision um a Windows server and then use Powershell to quickly install and configure stuff but apparently with simple ID you're not allowed to do that you have to do everything through uh click Ops and it is super painful but we'll go ahead and work our way through this um we'll go over to ec2 we're really halfway through this by the way but at least we got all of our VPC stuff set up before we do that let's just double check our VPC stuff so that we're not uh uh struggling here with misconfigurations so I just want to double check this stuff before we go ahead okay I'd almost uh be good if I had like an architectural diagram of what we're actually doing we'll go into here let's just take a look we have our resource map we have our two subnets here going into our private one public private public private excellent okay so that is set up right we'll go to our route tables here um and this looks correct here this looks correct here good we'll go down to our security groups and we'll go into the windows one and we'll check our inbound rules RDP 3389 our own IP address obviously you don't have the same as mine so do not follow one to one like that there we'll go here and this has inbound traffic for SSH for everywhere and it's saying for the same Security Group um yeah that make sense yeah okay great so that means that we're in good shape here let's go over and set up our ec2 instance I'm going to go launch a new one and I don't know why he called it but he called it this and we're g to just follow along wind Dum he called it one I'm G to call mine two so I'm not mix uh get mixed up with his old one we'll go to Windows here and we're going to stick with Windows Server 2022 base um I don't see him changing the the away from T2 micro I just can't imagine using T2 micro for this fine let's use it he didn't change it I won't change it but every other video You See Me launch a Windows machine I always increase it I'm used to like using Azure where Azure just will not work on uh lower powered machines maybe it'll work here it'll just be really slow uh which is fine I don't see him downloading the key pair does he download the key pair here I don't see it it's not in the instructions but I'm going to do it before we do that I'm just going to go and clean up my key pairs so that uh we're not getting so confused with all these key pairs let's go over here and I'm going to go over to my key pairs here you don't have to do this I'm just doing this very quickly if I can find pairs there we go and I have a bunch everything in here is junk so I'm going to go ahead and delete them there we go and I'm just going to go ahead and create a new ke pair right here we just say windows FTP that's going to download that file we'll probably end up having to use it I'll refresh this here we'll drop down and we'll choose Windows FTP we're going to have to update our Network here we're going to be going over to um the that new one we created that 10.06 uh for the subnet says one USC 1B 10 Z10 so we'll yeah the first one that's fine we definitely want to enable uh assign public IP address we're going to uh select the exist existing Windows Server we could have like just made this stuff as we did this as opposed to like the big complicated mess of doing the C but whatever uh we'll go down below here and this is where it gets interesting where we are going to join Jo our domain there's some manual way of doing it but this is the easiest way to do it that allows uh your active directory to uh work with users groups and other domain objects uh there we're going to drop this down and choose our new ec2 sessions manager profile which has uh instance core manager and SSM directory service which we absolutely do need so those two things are in there I'm going to go ahead and launch this instance and I'll see you back here when this is ready it'll just take a little bit of time okay so click through wait for the two status checks to pass okay all right so let's see if our uh Windows server is ready to go here it is started so we'll have to connect to uh it via RDP um so the way we are going to do that and Baker didn't really show me how here but I can assume that we're going to download an RDP file to do that so we'll go ahead and go to the rdb client this might not work but you know I don't know what else to do here so we'll go ahead and say Remote Desktop Client and we'll select this here and I'm going to double click the file and we'll say connect if you are on Mac you have to install this if you're on Windows it's not the same story um I'm not specifying the domain here so I'm not sure how I would do that just give me a moment all right so I just hit different user here and B is suggesting that we could place this in here it's weird the instructions will say things like you have to use the net bios here again I'm not uh the best at this kind of stuff so I'm just going going to go ahead and type this in here and it was capital T testing 1 2 3 4 5 6 exclamation mark and hopefully this works it did not work I'll be back in just a moment okay all right B saying I did the the Slash the wrong way which is a common problem with uh stuff we'll try this again back slash administrator capital T testing 1 2 3 4 5 6 exclamation mark says caps keys on so I'll have to turn that off try that again capital T testing 1 2 3 4 5 six exclamation mark okay there we go yeah that's where I I would get stuck quite often um again not the best at this this is the part where I find things are a little bit difficult but uh we'll let this start up and give this a moment all right so now what we'll do just again be patient here because it does take time wait till the uh the background appears here and you'll notice that it's it's ready when you can click into here and it becomes responsive if you click just click once and be very patient there we go see don't want to click a bunch of times you're going to have a bad time again just wait for it to load okay and let's see if we can type in here so I'm going to type in features and we're going go ahead and turn Windows features on and off I don't know any of this stuff I'm just following the instructions I I've done stuff like this before but again my memory is not the best on this so I'm just hitting uh just waiting here and waiting for this to populate again we didn't give it a lot of um memory or Ram I probably would have again used more but I'm just following what's there looks like things are still loading we're just going to give it a bit of time here okay all right so I think everything's loaded now I'm going to go down here to Features we'll just type in the search features there we go we'll click into Windows features on and off um this has brought up this this wizard it looks like we actually got through here before so I'm just going to hit cancel and um we'll take a look here and we'll click next as we're not told to do anything here we'll set up R based yeah select R based and then we need to go ahead and select our server and there's stuff in the doc so when I was doing this before it was telling us all these instructions as well so it's very similar uh so we'll go here select the server yep it's already selected we'll get H next and then this is where we have to choose the roles we need to install um and this is where I kind of found it a bit finicky but the idea here is that we need to um let me see where this is we're are going to expand this and then this is this where it's under no give me a moment to find it okay oh you know what it's the next page here there we go and so then under here we have something that's for ad for active directory or maybe under roll stuff is not easy to find out um remote remote remote where are you here it is remote admin ation server tools there we go and then we need to expand rule because that's what we just set up for then we want to checkbox on 0 tools we're going to expand that and this looks a little bit different from mine oh no that's right I'm going to expand this this and we don't need snapin so we'll take that off we'll expand this and we want those two selected so we're in good shape and we also need the DNS server tools selected as well we'll go ahead and hit next there's no indication that we have to restart anything so I'll go ahead and hit install and we'll just wait here for this to finish okay also note that the uh the bar here doesn't go all the way to the end so sometimes it looks like it's still going but it's actually done okay all right so after wait here it looks like it's now uh installed we'll go ahead and hit close um and next thing we need to do is look up active directory users and computers so we'll go down here below my microphone keeps moving on me here so sorry for being a little bit slow here we go ahead and type in active directory users and computers we'll give it a moment here to open it yep it's open now great so we're going to add a new group under the users oh you that's what it says so it doesn't show us where to click but I'll I'll take a look here and I mean this is organizational units probably over here and like it doesn't say just quickly look here create a new group in the current container okay and then up here it says within ad Us ad code. users so I'm going to assume I'm going to click here first all right and then I'm going to go over here and just look for group there we go and that looks exactly the same we'll call it FTP users and this is going to be Global and security we'll go ahead and hit okay so now we have FTP users group come down below we're going to add a new user I'm going to assume to that group otherwise what why else we'll be doing that um can we double click into that group can maybe we just need to add a user let's just right click here new user there we go and so here this will just be test a and again I would name this a bit nicer but I'm just following what uh B has written here so we have test a that looks good we'll hit next and we'll need to put a password in here I'm going to put capital T T testing 1 2 3 4 5 6 exclamation mark capital T testing 1 2 3 4 5 6 exclamation mark must change a password next login no thank you user cannot change password no that's fine password never expires sure account is disabled no we'll go ahead and hit next and so now we have this user um that user is not necessarily part of that group so we'll have to add that user to the newly created group I'm going to assume we just right click here and we're going to say I rather just go here to add to group no that's not what I want uh let's double click it there we go and we have the members Tab and we'll add that member um enter the object name it' be nice if we could just do this easily but we'll say a d. exam pr.co SL like I I don't know what's the examples can I see the examples here like be is really assuming that I know more azra ad than or ad sorry active directory in general I do not and this is not even showing me the examples so this is not helpful whatsoever give me a second here my computer's just kind of uh going slow now and because I clicked that it tried opening up the edge browser and so now I'm trying to close it because it's really slow as I don't know what format it needs to be in here but um yeah again just trying to close this so I'll just it'll take time here okay all right the uh Windows edge browser just refuses to close so I'm just in a bit of a predicament but the idea is that we need to get uh this user into here I wish we could just drag it in here somehow it would have been really nice if um maybe we could just move it no one second all right so what b was telling me was that if we double click into here and then we add usually you just put your username in here like test a and then if you hit check names if it underlines and it actually finds the user so here it is actually finding it there is also another way that we could do it if we double click into here and we go members of we can add the domain and just say FTP and hit check names and notice it autocompletes to user so I'll do it this way and so that way it's now added to that group and we can double check that by just double clicking into our FTP users if we go to members we can see that uh they're now a member okay so that was a little bit hard but we got it uh get the Sid of the newly created group for the PO shell on the server so looks like we now have to open up um I really wish this stupid thing we get out of here start with out your data okay can I click that just get out of here get out of here Microsoft Edge I hate hate you Microsoft Edge go away I'm going to just try to close it there we go finally okay so um it's weird that you can't close it if because it wants you to set it up so frustrating uh so what I'm going to do is I'm going to open up um uh Powershell and it's important that we use administrator version so I'm going to go ahead and right click go didn't put this in the instructions but I'm going to assume it's it is for the administrator version because it always almost always is and we'll give this a moment here and so we're going to type in some commands here when this decides to load I I just gave it a space to to wake it up so typing WM I I have no idea what that stands for group where so we're basically querying something here and we're saying name like and then single quotations FTP users so looks like we're actually looking for for um that object let's see if that works and it appears that it's returning back data so that's good um great wh why did we want to do that get the Sid of the newly created group from the Parell the server and so the Sid is over here which is apparently number two oh no this is the Sid right here so I guess we're going to need this for later so I'm going to go ahead and just copy well can I copy this contrl C and I'm just going to go into something like vs code here and see if it lets me uh paste it did so you'll have to use the control C command because right click I I wouldn't trust it but I have that copied here uh off screen so we can come back to that later um now apparently we need to go ahead and create ourselves an S3 bucket um I don't want to get rid of this I'm just going to minimize this because I don't know if we have to come back into it later and we're going to go back over to our Cloud shell and we'll let it reestablish connection we're going to go ahead and create ourselves an S3 bucket so I'm going to say create bucket region Us East one um we have to give it a unique name so it doesn't have to be the same name but we'll just say this uh my active or FTP bucket AB for my initials and some random numbers um we have to specify bucket here if we want this to work there we go so now we have a bucket we're going to need to give it some permissions so um oh man it's just like so many steps I do not like this um all right uh I'm G to go and just make this manually because I just cannot do any more CI commands it's just so darn slow yeah I've already told B I was like boo why the heck did you not make this the CL formation template um I'm going to go ahead and create a new policy here or new role and I want this to be for the uh transfer service okay we're going to go next we're going to uh give it some permissions in order to do so we're going to have to create a new policy so we'll go off the screen here and make a policy separately because apparently you can't just uh do things super easily here and we'll create ourselves a new policy I'm going to go straight to Json I don't want to goof around with the UI here if it ever decides to load today please thank you we'll go to Json I'm going to copy it in here and we're going to review it so we have list objects and buckets and then we have the S31 here notice there's slightly different we're going to have to go get our bucket name which this is mine yours is going to be uniquely different for mine we're not going to have the same one paste this in we'll paste this one in here and uh yeah that looks like something so I think that is okay we'll go ahead and hit next I'll call this um FTP AIS policy we'll create that policy get rid of the space there it might muck that up it already exists so I'll call it two because Bo did not delete all the stuff that I need to do the the lab as he did not clean clean it all up so I have two of them we'll go back over to here I'm going to type in our policy name FTP access I'm going to give this a refresh I'm going to choose our number two yours could be without number two it doesn't matter it's the same policy but I need to make this from scratch and I have a feeling that this name's already going to be taken as well so go ahead and do that and yes it is so I'll put a two on the end you're not going to have that problem it's just unique to me and so now that we have our IM am policy me just check here um let's go look here so look we created so we created this one we did this right we did that good and then created another create another policy with what in it create policy fdp policy S3 policy oh okay sorry we're just creating that one then we're attaching okay great and now we're going to enter our Sid here great you can tell how much I like this tutorial so we'll go over to transfer family and we are going into the server and I guess we're adding access probably yeah there we go and so now we got to grab that Sid from earlier we'll paste it in and I guess that's why we needed the roll so we'll drop this down refresh it typing FTP user I'm using two so that I'm not just taking Boo's old one uh we're going to select a a policy from IM am um apparently this is FTP so we'll type in FTP access policy too that's why we created the separate one okay that makes sense uh then we're choosing our home directory so users login directory well where was this hold on give me a second okay oh this is the uh the this is the the bucket that we have to select okay great so we'll go here my FTP bucket and we go ahead and add it cool now we need to test our connection that sounds really good I'd love to do that because then we're pretty much done so get the endpoint your all associated with your your transfer family here it is okay and then and we need an FTP client I actually don't have one installed so I'm going to go get fire Zilla Mac has some really nice ones but this one is pretty straightforward so I'm going to go ahead and download it it's been years since I've used FTP years um used to use it on a regular basis but once that's done go ahead and install and I'll be back here in a moment when this is downloaded and installed okay all right so this is now uh download I'm going to go ahead and install it all right and we'll go through here just in case uh I need to show any additional things that might be annoying we'll go ahead headit NE hit next install retry I'm not sure why this messed up I'm going to accept the agreement again what the heck is going on I hit cancel holy smokes give me a second here let me just wait for it to uh yes I want to get out of here it is not playing nice here today get out of here why is it the 32bit one is there no 64bit one let's go back 64bit why did it say 32 bit this one says win 64 let's try this again I'm double clicking going say yes we'll say I agree we'll give it a moment to load the data it might be because I double clicked and there was two that were running at the same time I'm not sure sure so give it a moment okay all right let's go ahead and now it says for all users which is a little bit different I'm going to go ahead and hit next and hopefully this time the install goes without issue there we go we'll start it up excellent and we'll hit Okay so this should be pretty straightforward I bet the endpoint is going to go into a location Bo does not show us any of the inputs not helpful but that's okay um because I've done FTP for many years so I can figure that out here what we'll do is we will go back over to it transfer family and this is going to be our endpoint says it's internet facing so it should be reaching out to the internet that should be our host uh we set a username that I don't remember and we set a password I don't remember I remember it's testing 1 2 3 4 56 exclamation mark and our user is test a like what is it he doesn't he doesn't write it anywhere we'll go ahead and put this in here test. a we'll say Quick Connect say okay like which is which is it you know what I mean like is it with the full name or I guess we'll go back here and take a look while we're waiting as I don't see it establishing a connection so we'll go here expand this go to users and I'll go to test a and what is their username I don't know address account what is it username test. a okay the question is do we have to do at sign added exam. because that's usually what you'll have to do when you enter in uh accounts here so I'm just trying to get this out of the way oh yeah I'm trying to drag this whole thing out of the way here I'm going to try this again I'm going to say at sign add . exampro doco we'll try to this should be on Port 22 we'll try to kick quick connect we'll say okay we're g to get in we're in we're in wow okay great so now um I just need some kind of image to work with so we'll go over to chat PT and we'll say make an image of a dog that is uh slam dunking a basketball into a basket allall net there we go watch it's going to give us a web P web PG image I'll just wait a moment here to generate my image out you can obviously get any image you want we're just trying to get something to upload here not the best image but we'll we'll take it and I'm just dragging it to my desktop here and now I'm going to bring up filezilla and I'm going to go ahead and just double click into here and I'm going to drag over this graphic here and hopefully it's going to upload so give it a moment to upload and now the transfer is complete so now I can go ahead and uh confirm this in S3 is it worth our time to do this probably not is there is there another way that could have been easier probably if we Ed like a Lambda authorizer we could have we could have just like hardcoded a value but I really wanted to have an opportunity for us to use AC directory with something else and join it to something so that we could take take off those boxes and get some experience um but yeah we have our file so we are in good shape so now it's the painful part of tearing everything down let's close out file Zilla since we are uh done with it here so we'll go ahead and just close this out here um I know somewhere we have our window our window open here we'll close that out we'll say okay and we'll start to tear everything down so we'll go over to our instances I think that's the first place is tear down the instance we'll go here and say terminate and we'll terminate that instance and we'll let that uh terminate so I'll just be back here in just a moment when that's done terminating okay all right so that instance is now terminated uh let's go ahead and get rid of our S3 bucket if it lets us so we'll go ahead to our S3 bucket and here's my bucket so I'm going to go ahead and just delete that we'll have to empty it first of course as per usual and then we'll delete our bucket okay so now our bucket is deleted and uh we want to go over to it transfer family and I'm going to go ahead and delete that and now what I want to do is go over to um uh directory service because we really don't want to leave that running so we'll go over to directory service and we'll go ahead and delete our directory there we go great so that is now deleting um we want to get rid of our VPC but before we do let's go ahead and get rid of these policies we're creating for the FTP just so that we don't have a mess of these going go ahead and detach sure and I'm going to go to the other one here FTP and we'll say detach that way we can actually delete them and we'll say FTP here and just clean up so that we can FTP keep things nice and tidy delete and I'm going to go have to go ahead and delete this one as well good and we'll have to do that for our rules as well say FTP uh we'll delete this one FTP user I'm just going to copy this here I'm not sure only one showed up maybe because the other one is different casing or maybe we have to give this a refresh there it is and we'll delete it and I'm going to go back to our instances and I'm going to go over to um key pairs we'll get rid of this key pair so we don't have a mess of key pairs we'll go over to Security groups and I'll get rid of this Windows one we created okay we'll go over to um elastic elastic IPS there's a lot to clean up we'll go ahead and delete or release these IPS it's the same thing as deleting them and maybe we can get rid of our VPC now so we'll go ahead and attempt to get rid of our VPC this is really dependent on whether the FTP transfer service was actually gone or uh the directory service was utilizing it we're going to go ahead and delete this VPC it's really thinking I'm not sure why but uh we'll let it think maybe just hit cancel try that again it should not take that long for it to pop up and it has a problem with these two in eyes um great what is this attached to maybe this is um the simple ad still spinning down I suppose for the directory yeah so the directory is still not done let's go over to directory Service as we have to wait for that to fully delete so we can uh get rid of our VPC so just wait for this to finish all right let's see if it is done deleting it is now done deleting excellent we'll go back to our VPC we'll see if we can go ahead and delete that now let's go ahead and try to delete that and is it working uh looks like it is yep okay we'll go ahead and delete there we go and I think we've deleted everything it's hard to say there was so much that we did but uh yeah we accomplished two big go goals which was the it was transfer family and working with directory Service uh so there you go and we'll see you in the next one okay ciao hey it's Andrew Brown and we're taking a look at Amazon open search and so this is a service that provides you a full text search service that makes it easy to deploy operate and scale open search uh a popular open source search analytics engine but in particular you actually can deploy open search or elastic search it's just depending on what you want to do um when I did the lab I was a bit confused because um I knew that this service uh could do elastic search but at the time I couldn't remember what open search was but um now I remember and this is back in 2021 was that adab us uh decided to Fork the elastic search and kubana uh open source projects because the company called elastic had changed their licensing agreement um and so this uh this move I think was specifically so that ad us would have to pay and AD us went nope we're just going to Fork him and we're not going to pay um and so that that's where open search came about elastic search is a search engine based on the Lucan Library so Lucian is something I've definitely used a lot in the past um so uh it's just an improved version of it and if you go to the elastics website they still Market this as free and open and so again the licensing was targeting uh large providers like it us to pay and they just worked around it um and you might have heard of the ALK stack before these are three um projects or uh pieces of softare created by elastic elastic search log stash and Cabana and they're commonly used together um so that you can basically have analytics and monitoring for your application uh think like log files that you could search so think of um the barebones version of data dog um that you can utilize for it so elastic search would be your full text search and analytics engine log sash would be your data processing pipeline a kibana which I think I might have spelled wrong there key k i b a no looks right uh is your visualization layer so it's basically the web UI so that you can uh quickly look at your data um but yeah there you go hey this is Andrew Brown and in this video we're going to take a look at Amazon open search and I cannot tell you how many times in my career I've had to manually set up a uh a solar or Spanx or elastic search for a startup that I worked for because they wanted to have full teex search uh in their application so if this is based off elastic search which I think it is then we're going to have I think an easy time uh working with this but um I'm going in this blind so I think that this shouldn't be too difficult so we have a few options we have create domains uh reserved instance leases I'm not looking to uh lease anything here packages um so that's kind of interesting there and I guess plugins if we wanted to bring in plugins as a lot of these fulltech search engines they'll have additional plugins that you might want to utilize but before we do anything I go take a look at the cost of this service because I'm really curious how expensive it is and if you're not comfortable with it don't spin it up but it seems like there is a free tier with 750 hours and we can run it on um a smaller smaller compute seems like we provisioned the compute so I'm going to go ahead here and create myself a domain and we'll call this my domain and we have easy create or create standard I'm going to go create standard just because I want to see all the options we have production Dev test I'm going to go to Dev test domain with standby domain without standby I'm going to say without standby uh select a uh deployment option that corresponds to the availability goals for you nodes in 1 a that are reserved nodes that are distributed across az's depending on uh depending on that I can go down to 1 a since I really again do not want to have a lot of spend here um we want to choose maybe the latest engine but it looks like we have between open search and elastic search so it looks like we can use one to the other so I'm wondering if the syntax of utilizing this is going to be different I'm more familiar with elastic search but I'm going to stick with open search because um that's what I'm going to do here today and looks like we have elastic search OSS client such as log St Etc that we can enable here we'll go down below and right away that is way too big I want this to be cheap cheap cheapap so let's see what we have here um is there anything smaller I think it's because this is memory optimized and so if we go to general purpose then we can go here and maybe choose something smaller yeah down below here we have t E3 small search that sounds good to me are suitable only for testing development purposes well that's what we're doing here I only want one node we have only EBS as our back storage that's totally fine uh gp3 is fine as well 100 EBS 100 is more than enough uh looks like the minimum is 10 so I'm going to switch this down to 10 I'm assuming this is gigabits so I'm going to put 10 gigabits here maybe 20 just in case I don't know 20 or 30 I don't know that's just too low we have our iops I'm going to keep that nice and low um dedicated Master no no snapshot configuration I don't care autogenerated input but you can also add a custom one I just want an autogenerated one here vpcs VPC access is recommended I want Public Access because I want this to be really easy here today to utilize um of course you should just do VPC and then the idea is that uh whatever your compute is I assume that we call some SDK or API calls and um that would make more sense but we're going to do public to make our lives really easy and if I just toggle this yeah find F grain controls are still here so we'll leave those alone uh I don't want to use samle I don't want to use Cognito uh do not set domain level policies I'll leave that alone uh I don't care about this lot of options here I also feel like I should just set up the server manually with all this stuff and we'll go ahead and create this and it says T2 or T3 instances are not supported for autotune I mean I didn't turn autotune on so I don't see why that's an issue but do we have any red here ah here it is um I'm going to take off find green controls I just don't want to have to configure that and there we go so we're going to wait for this to provision um and I'll do some research while we're waiting here okay also while I was reading up about this there seems to be a server list off offering but I don't see it anywhere here um which is a bit confusing so is it in preview and not out uh I'll go ahead here and just type in open search again yeah so I'm not exactly sure uh where that offering is maybe it's only available in Us East one sometimes this happens where I don't realize there's something there because I'm just in the wrong region but if we go here I'm curious ah seress so we could have uh launched a seress one which I guess is a good idea but uh and it looks like we also have one for ingestion so I guess there are those three options but um I'm going to just stick with the managed one because that's probably how I would actually end up utilizing it some of these seress Services aren't that great um uh for like if you have like relational databases or full Tech search I probably would never utilize serverless for that it's just I don't trust it um but anyway I'm going to go continue on and try to figure out what we'll have to do programmatically to work with this okay so it seems like it's suggesting that maybe we interact with it using the SDK now it has Java examples I'm not using Java no way but we'll go ahead and over to ads SDK Ruby and I'll see what they have for open search and maybe we can just look at the functions and see what there is to uh interact with it so I'm going to scroll on down here and what I'm looking for is calls um to our domain to uh query or get data or do something and I'm not seeing anything this looks like it's interact um with open search service not necessarily to query it so I'm just curious here interact so this is interact with the Open Source service how to create update delete open source domains we don't really want to do that we just want to interact with it sometimes with these uh fulltech search engines is they'll have an endpoint and you just interact with that endpoint it's all via https um but yeah I'm just looking for some kind of client sometimes if you're looking for something you just type in like open search It'll ask client maybe like Ruby and see what we get so I'm not sure if this is related the open search rute client all you interact with the open search cluster but is this the same thing highly scalable extensible open source software for search analytics is that what this is because I thought um at least at this time I thought uh open search was um I thought it was A's offering like how there's document DB but if it's this this is fine as well uh but now I'm kind of like I should have done elastic search because that's what I know really well but if that's the case it looks like we have um a c and so we just connect with the endpoint so let's go ahead and give that a go so what I'll do is go over to it examples I'm going to open this up in git pod you use whatever you want code spaces Cloud9 whatever um but I'm going to open this up and we'll wait for that to launch and then once this is launched which shouldn't take too long El just going to see how our cluster is doing it's still provisioning um I'm going to go ahead here and make a new folder I'm getting a lot of folders here so I'll just say mkd R open search uh here and we'll CD into that and then I'll make another directory for open search we'll CD into that because if we want to do elastic search we'll have to have a folder for that later on and so I'm looking for our open search open search folder wherever the O's are got to refresh this element op open search here we are and so I'm going to make a new read me file in here and I'm going to type in bundle in it to initialize um a gem file for Ruby and we will go over uh oh yeah up over here I'll bring this on down here again never used this before but this stuff is not usually that hard un we hit something that requires native extensions then we're going to have a problem um so we'll go ahead and paste that in there and we'll do gem install ax because we'll need that oh sorry it's just Gem and I would really like it if my Vim Keys would kick in here I'm just going to wait till my Vim uh my Vim extension kicks in okay it still hasn't loaded but I'm going to save this file so I have Jem open search Ruby ax and pride will go down below and say bundle install and so that's going to go ahead and install uh those three there I'm going to make a new file here call it main. RB and we'll go ahead and look at the code sample so we have to require open search oh now my Vim is here that's good I going to copy the client notice that we have an endpoint URL so that's kind of what I was expecting some kind of external endpoint uh we'll have to establish a connection to the client so that makes sense here looks like we can do a health check check um I would imagine we probably want to put this out so I'm not exactly sure what this is I'll just say inspect and that would be a way that we could look at that um I guess this is just a more of a configuration so I think that I would probably want to have logging on here and just take that out here so that looks fine to me um okay so Health would print out this stuff here I think so we go down below to connect to Amazon Open Source service oh cool so there's a very specific gem for that I did not know that we'll go over to here and Gem we'll place that one in there well that's interesting maybe this is an AA specific thing and so we'll grab these two so it looks like our implementation is now different um because this one yeah yeah it's completely different so we'll just copy all of this code to be honest if it works that's fine uh so this is should be in CA Central 1 so CA Central 1 I'm not sure why it's es I going assume that is for whatever it needs to be we don't need to supply the access key in secret as that should get picked up um locally um because in this in this uh environment I already have my credential set and then that's the host URL so I imagine we need to replace this with with whatever our domain is and so we have our signer here let's go over to here and take a look um it's still creating I'm not exactly sure what I guess the end point is not there yet so until this is created we basically can't do a whole lot here I give this a refresh here and see what we have still no no domain information that is totally fine because we have some time create an index in documents so here it says we're creating an index called Prime and and then we uh looks like what are we doing here I guess we are inserting a record into the index I think that's what's Happening Here y so I'm just fixing the formatting so it's a little bit easier to look at here and then we have search the document delete the document delete the index okay so this is a pretty simple script um I might want to include Prime here just so that we can see what's going on and then the idea is that we can just put binding prize in here so say binding pry binding pry binding pry finding pry because I don't know what this is going to return as the result or if it will return any results at all so I'm looking at this I'm going okay we're searching it but it's not showing us um what we would do with that data so what I'll do is I'll just put something like results here results and results and if it returns anything then we'll be able to see it same thing with this we'll just say results and we'll go here so I'm going to say create index create document in index because that's what's happening here say results and that looks pretty good so it looks like now we're just waiting for this to provision so I'll wait for this to finish I'll see you back here in a bit okay all right so it says it's 100% done let's go take a look and see what we can find um so we scroll on down below I'm again looking for that that endpoint um that we're going to need to utilize to connect and we made it a public endpoint so over here we have dual stacker ipv4 I'm going to go with ipv4 because that's just simpler uh um and that seems fine there's also apparently a dashboard user anomymous is not authorized to perform this okay well that's not going to help me if I can't access it what if we try the ipv4 address no so I'm not exactly sure how we get into the dashboard not too worried about that uh I would just want to be able to programmatically work with it so let's go over to here and here it says your Amazon domain so I'm I assume that we're supposed to place this in here all right and this is CA Center 1 es so clearly that is the same thing and let's go ahead and see if this works so I'll do a bundle install if we have yet to do so and we'll type in bundle exec Ruby main. RB and I have to put the E on the bundle otherwise that's not going to work and here we have an issue missing credentials provided so apparently we do have to provide them now that doesn't mean I don't have them it just means that they want this to be explicitly added so I'm not sure where that was I think it was over here let's go back to our code sample um here and we'll scroll on up and we'll choose these two and we'll go here region and what we'll do is we'll just access these via the environment variables so it's kind of weird that I have to set this explicitly that's okay it access key ID and then I have um EnV this is adabs secret access key I'm going to assume that is correct uh way to do that usually I always look these up because I always forget them but I'm pretty confident in this one so I'm going to go ahead and hit enter and there this is showing up red so I'm missing a comma here is it now happier I'm not sure bring this down on another line and it still looks like it's mad well I'll try this again and then it says here it examples is not authorized to perform esht put with an explicit deny in the resource based policy so it sounds like there's an explicit deny here and so I have to um Grant privilege so that I can do that so we'll go over to Security configuration and there is an access policy here so I'm assuming we'll have to change this access policy and allow um my very specific user uh to utilize it so we have a statement deny all for everybody but I'm going to change this to um I wonder if I can do this I'm going to try to add another statement I just don't know which order it takes place in so if I go down here and I say allow what I want to do is then put in my uh user here so I'm just going to say ads principal uh roll and I think that I can just Supply it in the ads section here yeah I think I can just place it in there I always forget every time I do it I always forget well we go over to I IM and I'm going to go over to users and I mean this user should have an r and I'm going to see if I can grab that directly and I think that I can do this I'll go ahead and just paste this in here as such uh like this and I'll go ahead and save it and I'm going to go back over to here and we'll hit enter and we'll try this again it says explicit denying a resource based policy forbidden but I think it's modifying it so we actually have to wait for the configuration to take place it's not instantaneous so we'll have to wait okay and that is updated so we'll go back over to here we'll try this again and see what happens and I still don't have permission so what I don't know is that if I remove the deny uh order of deny and allow statements in policy because this is the order I don't know um so we'll go here so also assume the falling policy is attached so we have this here allow allow deny it seems like maybe the allow would come first then the deny like I don't know I'm going to do something dangerous here again I don't know if it's dangerous but because this is open to uh to the public and on a VPC it might uh complain but what I'm going to do here is I'm just going to take out the deny I it seems really dangerous I just want to see if this works like this and I'm going to go ahead and do that it's not like I'm going to keep this up for very long so we just want to make sure we can establish connection but clearly if you're doing this for production you'd have to do a lot more work with this also you do the VPC and so that would also add an additional layer of security but we'll wait for this to update and then we'll try it again okay and so that is now updated I'm going to go back over here and try this again and now it's working excellent so I'm going to look at the results this is uh right here so we see that and it says acknowledge share acknowledge true index Prime so it clearly has been created we'll type in exit um and so now we are at this one we'll check the results to see if the document was inserted so it's turning back the information indicating that it's been created uh we'll type in exit here whoops exit and so now we are at the uh search and so if we type in results we can see we're getting results back you can see it's not the nicest thing to work with so you'd have to do a bit of work to uh integrate that into your application we'll type in exit here to get to the next step and we'll type in results now we're on the delete so it's deleted it we'll go here and we'll delete the index and we are now done so that is in a nutshell how you would uh work with this obviously this is not the most practical example but it does get the job done and we got it working I still don't know why this one's red even though our code is fine so I'll say that is good enough I wish we did a better job of the permissions but that is totally fine um what I was surprised was the fact that we uh used a a gem and we didn't necessarily just communicate directly using HTTP requests but um it's nice that there is a gem we probably could have just sent HP requests directly I just know that like using solar and other uh full Tech search Eng you don't normally have to use a library you just work with the API endpoint um but uh anyway I want to delete this I'm not sure if it is deleting we'll try this again my domain delete and I'm going to wait for this to delete just because this took so long to spin up I think I should stick around here and just confirm if we run to any problems here I'm going to go ahead and just uh save our open search code and we could go and do elastic search I'm not sure if I if we do it I might decide to do with the server lless offering but we'll see but I'll be back here in a bit okay there we go it is now deleted just took quite a while and I'll see you in the next one okay ciao hey this is Andrew Brown and we are taking a look at inabus security Hub it is a cloud security posture management system a cspm which allows you to generate out a security score to determine your security posture and the way way it works is that you'll enable these security standards which are a collection of security controls which in turn are AIS config rules if you have seen conformance packs any config rules and wondering if they're the same thing they possibly are the same thing but the key difference is that security Hub has um an additional layer of providing you a a security standard score um with a bunch of uh uh Graphics uh or what do you call it um graphs or charts like you have a pie chart there um and you can invest each finding so that's pretty much what it does um so yeah there you go hey this is angre brown in this video we're going to take a look at U security hubs security Hub is a centralized service that can do quite a bit um so what we'll do is make our way over to this service now you don't have to do this for real you can just watch me do it um for this to sufficiently work we'd actually have to have some kind of workload running so I went to an account that has stuff running in it and so I'm hoping that it will provide us some information so here you can see that U we have a security standard of 33% um pulled in here based on the CSI foundations bmark which is enabled here we can enable other things in this account um I believe that these could be tied to ad ofis config so um I don't really want to turn them on and end up having having more stuff but the point is is that you can see security standards that you can run things against now over here we have assess the most findings so here we have severity of of some issues here let's go and take a look and see what it's suggesting um which ones are critical if they actually are critical again it depends on what you're doing which says ensure Hardware MFA is enabled for the root account and so these are our standard stuff that we would normally see we go over to controls um this might show us the individual controls that we have here there we go and so my question is is this stuff in config right so if I go over tabus config because I might have turned this on before and just kind of forgot about it and if we go over to here uh do we have a conformance pack no rules yeah so I've turned this on before and that's why I have uh this information so that is where this stuff is coming from okay so if this isn't really clear um it just gives you a summary of stuff and it basically um these are basically like conformance packs but uh you can also pipe in other um information from other places uh but anyway this this is uh pretty clear what this is doing so what I'm going to do is I'm just going to log into this account I'm going to go to an account where I don't have security Hub turned on so we can see what it looks like turning it on for the first time so let me log in here okay all right great so what I'm going to do is go over to Security Hub in my other account here and we'll go ahead and see what it looks like for the first time so here it says go to the security C the security hub first 100,000 checks only cost that so you can see there is some savings and there's a 30day trial um notice that we have to enable adabas config so before you can enable security Hub you have to enable uh this stuff here I'm not sure what we'd have to download here it's a gamble file so is this what cloud information just going to open this up and visual studio code and yeah it is a um that's what it is here it is a cloud information file that it's suggesting us to launch so before we can run it we would have to configure this let's just take a look at what is going on here I'm just trying to get some of these things out of the way so what does it want to do um uhuh it wants to make a bucket and a bucket policy and then it creates an SNS topic and we have an email notification and then we have configuration recorder for A's config so we have a few things that we'd have to do and then we would enable it and we could turn it on so I think that's pretty straightforward um I think you could see what it looks like on the other side of it so I don't think it really matters for us to do this because I don't want to turn this on and then have to turn it off it's a big pain in the butt um to probably remove all those uh rules so I just wanted to show you what it looks like to actually run security Hub so you have an idea of its benefit but there you go okay ciao hey this is Andrew Brown from exam Pro and we are looking at Amazon certificate manager also known as ACM which is used to provision manage and deploy public and private SSL certificates for use within your AWS services so let's look at ACM a little bit more in detail it handles the complexity of creating and managing public SSL certificates for your adus based websites and applications it handles two types of certificates we have public those are ones that are provided by AWS and they are free and then you have private so these are certificates that you import and they cost $400 per month you generally just want to use a public certificate if you ever use let encrypt those are all public so if you're comfortable with that you're comfortable with these ones just make sure that when you're creating that uh certificate you do not make the wrong choice I myself have chosen uh incorrectly and uh but luckily I reached out to AA support and before they charged me they fixed that issue but that is a tricky one to get your money back on if you make that mistake so ACM can handle multiple subdomains in Wild Card domain so you can see that I'm entering exam Pro as the naked domain and then I have a wild card one that is the setup I always recommend that you use um because other otherwise you'll just have to create a bunch of domains or sorry certificates later and that's kind of a pain and ACM is attached to very specific a services or resources so you can attach it to elastic load balancer cloudfront API Gateway and you can apparently use it with elastic bean stock but I'm imagining that is through the elb so those are the three services you need to know that ACM attaches do so remember that uh and there you go so we're going to look at a couple of ACM examples and the point of this is to understand SSL termination so the first one here we have is we're using ACM and we're attaching that certificate to our load balancer which is application load balancer and if you see that line the idea is that this red line represents uh the traffic that is encrypted and so once it hits the ALB the certificate is going to decrypt that uh um that traffic and then everything between the ALB to the ec2 instance is now unencrypted and that's totally fine because it's within your network so it's still secure um but you know for someone to uh take advantage of that they'd have to break into your um aw's account and they'd have to be able to intercept that traffic so it's a very low risk um but a but uh the ACM can only really be attached to um uh elastic Lo bancer cloudfront or API Gateway so it's not easy to uh protect this traffic here um but the advantage of attaching uh your certificates at this level here is that you can add as many ec2 instances you want and you don't have to configure each one of them um to uh be able to handle a certificate so that makes it a lot easier to manage um certificates now the other case is terminating SSL end to endend and this is where the traffic uh from the start to the Finish is encrypted so even within your network it's going to be encrypted and the way you would do that um I don't know how to do with ACM I don't even think you can do it with ACM uh go I've only known it being able to attach to resources over here but you I guess you could use let encrypt and so you'd have to set that up on every single uh server uh and then rotate them out and that's kind of uh a bit of a hassle to maintain but a lot of people are used to doing that um and this is if you need end to end encryption this is going to be dependent on your compliancy so if you're a large corporation maybe you have like a a rule that says you have to encrypt end to end but for 99% of other use cases this is more ideal terminating SSL at the load balancer so there you go hey this is Andrew Brown in this video we're going to take a look at um Amazon certificate certificate manager so that is a ser that allows you to have SSL or TLS uh certification um this is very useful when you are trying to have htps for your application load balancer or cloudfront um so let's get to it um I'm going to go over to our GitHub over to here and I'm going to open up any examples I'm going to open this up and we can set this up using an existing load balancer so I know that we already have one um I'm going to have to just switch out um some key information here so we'll wait for this to open up um so we'll give it a moment here there we go and that's not ready just yet we'll give it just another second here there we go there's G pod and so what I'm going to do is make a new directory here this will be called ACM and I'm going to go over to ASG and I want to go ahead and copy these two files and we'll go over to here and make a new folder and we'll just say lb and we'll paste these in here okay so uh we'll try that again paste I don't think it copied them so we'll go back to the SG here I'll copy these and I'll paste it and I'm going to CD into my ACM directory um and then into my ALB directory here going to make a few changes this would be U deploy s gacm um and I don't think the region matters um if for the domain but I have domains registered here in R 53 so for you to do this you'd have to register domain if you don't want to register domain you can just watch me do this but uh domains don't usually cost that much but it's up to you based on what you are comfortable uh comfortable wanting to spend but anyway what I'm going to do is go over to here and I want to swap out a few values because um I'm not in my usual account so I'm going to go over to ec2 or sorry VPC and I I need to get the default VPC here and I'm going to switch over to ca Central 1 and we'll go into here and I'm going to go to this one here I'm going to copy this value here and paste it in and then I need two subnets here so go ahead and grab that that's really interesting because I must have done an ALB somewhere else but I remember we specified them separately it doesn't really matter you can do it either way um but we'll click into the resource map here the easiest way is just to grab it like this I'm going to avoid um one CS Central One 1 a because it or 1 D because it always gives me uh trouble so we'll just do one uh 1 a and 1 B here and I'll paste it in as such the other thing I'll need is an image ID so I'm going to make my way over to ec2 and I'm going to go ahead go to instances launch instance and we'll go ahead and copy this and we'll go ahead and paste it so that looks fine this is open we have a zoom rule this will launch a basic website and so we'll go ahead and just save this save uh we'll just say ACM um Al code we'll sync that okay and so now what what I want to do is go ahead and deploy this so hopefully this works we'll make our way over to cloud formation here it's fine if it doesn't work the first time sometimes it messes up that's totally fine but we'll go over to here and uh did that not launch in the correct location here CA Central one oh you know what it is um I have to get the credentials for this account so what I'm going to do is just load up my credentials here and I'll be back in just a moment okay all right so I swapped out my keys we'll go ahead and take a look and try to deploy this again and I think this time it should now show up here under our Stacks it does excellent and I'll go into here change sets we'll execute this and hopefully it works fingers crossed all right so I think that worked assuming my internet's not uh mess set up again so we have our load balancer and so what we'll do is go over to ec2 and we'll go over to load Bouncer and we could do this via cloud formation but I fig I figured it's just better to do this via click Ops here today um to change this configuration and oh that's interesting we don't see our load balancer what the heck's going on I could have swore created it unless it messed up it definitely created it resources oh oh this is an autoscaling group this isn't a load bouncer that explains why this isn't working okay so not a big deal um I'm going to go down to elb and yeah because like notice this one it pulls from from like that there so I'm going to go ahead I'll copy this one here I'll just call this one old rename zero and I'll paste this one in here now and then what I'll do that is embarrassing what I'll do is I'll pull up this one here on the left hand side I'll bring this on the right hand side and and I was wondering why that looked a little bit different so what we'll do is just grab this here copy and paste and we'll grab this one here and we'll paste and we'll grab the Ami here and we'll paste also kind of explains why it took so long to spin up because asgs take some time to get going and we'll past that in so what I'll do now I want this to be a T3 micro um we'll delete out our template yaml here CU that's totally useless now just say delete that permanently I'm going to go back over to our stack I'm going to delete this one and just so I'm not waiting around for a thousand years I'm just going to name this um uh well I mean it should be renamed Al ALB basic ACM we'll deploy that again and this time we won't make any mistakes so we'll wait for that to appear okay all right so I think now we have the correct thing created we'll go back over to ec2 and if we go down below to our load balcer we now have our we should have our load balcer and so what I want to do is make sure that uh before we proceed that we actually have something here so give that a moment to load there we go so there is a server so we'll make our way over to M and originally when this service first came out it was so easy it was super super easy to choose the wrong one and it cost you a lot of money but you can see that they've made this button down below here now um so we're going to request a certificate a public certificate and we'll go hit next and then this is where you enter the fully qualified domain um so I'll use it has to be rough 53 and the other thing is that I do want that but we're going to add another name to the certificate and I always recommend to do the the naked domain and then the Wild Card domain and that will cover all your bases so if you remember that you'll never have to recreate a domain um ACM is scoped per region so if we had a resource in another region we'd have to create another ACM public certificate per region but anyway we have two options DNS validation or email DNS is the easiest especially when it's in r 53 so we'll choose that I don't care about the key algorithm I don't know what it does I'll ignore it so we'll just well I mean obviously I know what a algorithm does but I'm just saying I don't see how that would make any impact to what we're doing so now let going to do the validation it should automatically do it before used to have to click a button to do that um so I'm not sure I'll go ahead and click this here and I'll add it anyway so maybe that will um satisfy it and so what we'll need to do now is just wait for this um to occur okay so it'll take a little bit of time it's usually pretty darn quick there we go now it's issued and so now we can use our certificate so if we go back over to ec2 and we go over to our load balcer here and we go into our listeners it's usually attached to the listeners so we go manage listener and edit the listener there should be an option here I'm used to doing it programmatically so I'm not 100% sure where it is but I think the idea is that we need to add a rule which is 443 like this and then it should require certificate so yeah I'm not 100% confident on this so let me think here yeah we have Port 8 I'm going to go ahead try this again so we'll say 443 uh wait SSL port it's 443 yeah just making sure I'm not putting 433 sometimes that happens we'll choose HPS yes so once you do that then you should have to choose something so authentication we're not authenticating um and I guess we would go to that Target that's fine here it is okay so now this is where we choose our our certificate so we have cloud.oracle.com this was not an option that I saw before this is actually kind of interesting import your certificate to either a certificate manager or um IM to be imported to make the default one I'm kind of worried this would cost a lot of money I don't know why but um because it says import ACM and I remember this IM IM option uh being here so I'm not sure if that's a more cost effective option or that's still expensive I probably still would not do that we'll go back to ACM here and we'll choose this and we'll go ahead and select next and so now our certificate should be attached so the idea is um that's step one but we're going to have to go to R 53 because we need to point this um over to the new um load balcer there so in here we have cloudborn name from before that I didn't get rid of we're going to go ahead and create a new record and this is for ipv4 addresses which is fine but if we uh select this here it doesn't matter if it's uh a or C name here we'll just leave it is a and we're going to choose in here our load bouncer so here we have our application load Bouncer and then we'll choose our region so this is CA Central one and then we have our load balcer here and this will just be on the naked domain so I don't actually don't want to change anything I'll just hit create and so the naked domain now should uh go there okay so now what we'll do is we will go type in cloud borgor and hopefully it will resolve no as putting the www in there I did not put the www in there it just kind of autofilled it and I really want it to not do that so I'm going to go ahead and type this in manually and so now works okay so it is now using that certificate I wonder if we could see it here um here if we click it and so we look here we see issued Cloud boorg org issuer Amazon Amazon uh certificate and there's the public key so there you go that is how you do it um so so acms as far as M does not cost anything when it's public uh ACM public certificate pricing I'm pretty sure it doesn't cost anything just going to double check here yeah it's free so there's no cost here if you keep this around it's not really a big deal I'm going to go ahead and delete it because I'm done with it and I'm going to go back to uh we can't delete it because we're using it right now so uh what I'll do is I'll just tear down our cloud formation stack and then we'll delete that then we'll get rid of the route okay so go ahead and delete this while that is delete I'm just going to save this here ACM code example but normally like you can programmatically do it but normally what I would do is I would via click Ops create the ACM certificate and I would just reference its Arn in cloud formation as opposed to um uh doing all there because you have that confirmation step and that's kind of a pain to set up in um Cloud information or I you can do it but uh it's not what I would recommend so we'll wait for that to delete and then we'll clean this up okay apparently our delete failed I didn't expect that to happen um we'll go over to our outputs and take a look and see what the problem is and it has a problem with the target group maybe it wants the instance to stop running first I'm going to go over to ec2 and maybe we just stop this here maybe that will resolve our issue yeah and we'll go back over to cloud formation and we'll go ahead and delete this again I do not want to get rid of the target group I want it to actually properly delete so I'm hoping that maybe this time it will actually delete so we'll wait here and see what happens all right so that stack is now gone and what we will do is make our way over to ACM here and we'll go ahead and delete this good and I mean I can go over to rout 53 and clean up that hosted Zone file like uh route not that it really matters but no reason to keep a dead Link in there so in here we have club.org and I'll go ahead and delete this a record and I'll see you the next one okay ciao Brown and we are taking a look at adus data sync so it is a data transfer service that simplifies data migration to and from and between cloud storage Services it works with the following protocols NFS SMB uh hdfs object storage it works with the following in Services S3 EFS FS FSX for the windows file server luster opens ZFS net app on tap Amazon snow cone Amazon S3 compatible snowball Edge it works with other cloud storage services such as a Google Cloud Storage Microsoft Azure blob storage Microsoft Azure files Wasabi cloud storage which I have no idea what that is but that's what it works with digital ocean spaces Oracle Cloud infrastructure object storage Cloud flare R2 storage back Blaze R2 cloud storage nav cloud storage object storage Alibaba Cloud object storage imb's Cloud object storage seates live with Cloud not sure how you pronounce it but the point is is that it takes data from one place and puts it to another place and it can go it can be applied for a lot of uh different applications there you go hey this is Andrew Brown in this video I want to take a look at data sync so data sync allows you to securely transfer data between two services and it's supposed to be extremely easy to use and that's what we're going to find out um so in here uh we have to let's see transfer some data and we'll create ourselves a task and we'll choose our location type and so we have some options here for this basic example I'm thinking maybe we should transfer something from I don't know S3 to S3 because that seems like the most basic thing that we could do so we'll say ca Central I'm going to go ahead and um create a couple buckets here and I do have uh this account uh over here so I'm going to just use this to do that I'm just going to make a new folder called data sync it'll CD into data sync here and I'll just touch a readme file here and what we'll do is we'll go ahead and look for data syn here yeah there we go and I want to just create two buckets I'm just going to go ahead and say ads S3 make bucket S3 and we'll just say Source data sync and put a bunch of numbers there on the end and then I'll also want one for uh destination okay so I'm going to go ahead and create these that's one and that's two they'll probably create in CA Central one because that is where I am so say create buckets we just upload a file so I'll just say touch hello.txt so say touch hello.txt and say adus S3 uh copy hello.txt to our source bucket here now I'm curious if there are any a CLI commands to use data source as I would prefer to pratically use it or data sync I should say it CI uh it may or may not be easy if it becomes too hard we'll just go back to the console which is not a big deal um not what I wanted to do here I want to click on the two here and so we probably have like a task like create a task and they don't have examples when they don't have examples it makes me think that uh nobody is using this thing okay um so because it looks like we have to create a source location or destination location R you know what I'm just going to stick with the console in this case I usually don't but uh in this case I'm going to it we'll say Source data sync and choose a folder prefix that'll be used for the data transfer so it seems like we need to specify folder so I'm going to adjust this and just say like um data over here and then just say hello.txt and so I can just run this one more time sometimes the copy paste does not copy right and sometimes you can't go to the end of the line here okay so we'll try this again and so now I've placed it into a folder called data and I'm going to go back over to here to data sync we're going to put the word data uh use to access the selected bucket let's see if we can autogenerate that that'd be really nice cool I've never seen that as a feature inas Management console before and I but I really like that I wish they' do that more if you're listening us we'll choose S3 again Central one is fine I want the destination bucket and we'll choose standard I want to place this in folder we'll autogenerate that love that just love that and we'll just call this uh S3 to S3 this seems fine transfer mode transfer all data verify only the data that's been transferred to uh sure um and this is what we want to I guess continuously do that so that's great I don't want to continuously do it I just want to run it once I guess summary report would be a good idea sure success and errors we'll go down below ahead and hit next okay if if I have to do that I'm just going to say none because we can just verify the one file um I don't care about logs we'll just skip that for now it's really straightforward what we're doing here we're not doing it complex data uh data transfer task so I don't think we do anything fancy here and we're going to start this start with the defaults and we'll see how long this takes to run okay so I'll have to refresh here a bit I'll be back in just a moment all right so it looks like our task uh might be complete or at least it says the task is available um so the way we're going to know if this actually worked is if we go over to S3 and take a look so we will go and check our destination bucket and if it's there it works um of course this is the most simplest use case of utilizing this service but it's not the most interesting one but notice that it's not there so that is not good for this simple example but also notice there's nothing in our source bucket so maybe that's our problem to begin with I could have swore we copied that file over oh it says the file doesn't exist because it's named wrong and you probably saw me make that mistake and just saying hey Andrew you're going to notice that well let's go ahead and paste that in then let's try this again okay we'll go back over to here not a big deal we'll just start it again and we'll wait again okay all right so I think this one finished we will go and take a look um in our destination bucket so go here and the data is there so yeah this is is obviously the most simplest use case that you can utilize for this um you know so again you might want to do something a little bit more complex than what we've done here but uh you know this is what I wanted to do for this one to show you that it is an easy service to use and I I think the main thing is that you can run it on a schedule so I think that is uh one of the coolest things here so I'm just emptying out these buckets so we can get rid of them okay and then we'll go ahead and delete these buckets all right and I'll get rid of this one here and I'll go back over to data sync now and we'll just delete this task not that it's going to continuously run or cause us any issues and these do not exist anymore so we'll go ahead and delete those there we go I'm going to go over here just save what whatever we have in here and I will see you in the next one okay ciao hey this is Andrew Brown we're taking a look at inabus directory service it provides multiple ways to use Microsoft ad with other inabus services in some cases it's provisioning directory services for you uh it lets you use existing Microsoft ad aware or lightweight directory access protocol aware apps in the cloud the first is simple ad um a lot of tutorials like to use Simple ad because it's super simple to set up but it definitely is not a fullblown version of active directory it is a compatible uh thing so it's something that ad us built that simulates or emulates the active directory um protocol or language or interface um and it's powered by S before Samba is an open source implementation of server message block if you don't know what it is you can read a little bit more about it somewhere but I love to show the logo because it looks really ugly it looks like something that somebody would get a tattoo in the mid90s so I like putting that there then we have um ad connector it's a proxy service to connect your existing on premise active direct active directory so it's basically a bridge to your own um active directory we have itus manag Microsoft ad so this is a full featured managed version of Microsoft Windows Server active directory I believe that you launch it through this server service so some of these things are connectors some of them are fullblown Microsoft ad and then you have Amazon Cognito so this integrates sign up and sign in into your uh your app locations cover this in a completely separate section so you probably have to create Cognito separately here and then connect it and that'd be more probably for ldap aware apps here I want to point out that with simple ad it's not available in all regions it's usually in the primary like the the main main region like the building region the main region of your geographical area so like in Canada we don't have it in any of our regions but we have to use Us East one if you're in Europe it'll probably be like uh us or sorry EU East one I think it's in Japan Tokyo 1 things like that um so there you go uh one thing that we notice like with simple ad is that you cannot use the Powershell scripts to uh automate the setup so for those who know active directory my cofounder the other Andrew knows it really well so he was telling me like when he was trying to set it up for us uh for tutorial iation here he really wanted to make Power shell scripts he just couldn't so you had to do everything through click Ops which is a pain uh so for those who know active directory just telling you that up front here okay hey this is Andrew Brown and we'll be taking a look here at adus backup which allows you to centrally manage backups across your adus services so this could be for Amazon S3 uh VMware VMS Dynamo DB FSX file system ec2 EFS CBS RDS and Aurora backend so that is a sap Hanah uh relation there uh storage Gateway that's what sgw stands for document DB Neptune time stream and more uh so like one was like cloud formation I didn't really understand how that would work U I imagine it's looking at Cloud information for resources within that to backup so idea is you need to create a backup plan this is a policy that defines the backup schedule backup window backup life cycle you have a backup Vault this is where your backups are going to be stored uh back vaults are right once read many and you set a retention period uh there are two types of vaults we have standard Vault and air gap Vault things will initially go into the default Vault the standard one if you want to move things in the air gap one you can do that afterwards I assume that is an Enterprise feature resources can be assigned a backup plan using Aus resource tags you can directly select resources but this is the preferred way of uh marking things for backup you can backup resources to other Aus regions and ab accounts you can manage backups from essentialized account across your entire Abus organization backups are incremental so you only store the difference instead of full backups to save the cost it backups can use an independent KMS encryption key uh from that of your adus resources Associated charges for adus backup appear as backup under uh cost Explorer adus backups are IM mutable to avoid them being tampered with um and then there's this other thing called the a backup audit manager which is a builtin reporting and auditing for itus backups what I didn't mention here um and we'll see that very evidently when we do the lab is that you can choose to do an on demand backup a backup immediately or you can make your backup plan which will then run on a schedule um so you know I had a few little issues there but we did get through the lab and figured it out in the end but uh yeah we'll get into that next okay hey this is Andre Brown in this video I want to take a look at adus backup so adus backup is a centrally managed place to manage your backups for many different services there's a few ways that we can get started uh utilizing the service uh we can go ahead and create a backup plan directly but if we go over to what is it like dashboard it should give us options here yeah for on demand backup and so this is one of the easier ways to work with it if you need to work within a framework because of um compliance you can go ahead over here we'll just take a look at this for this and so what we could do is go ahead and create a framework and you'll notice right away that it's showing a bunch of controls and things that you would apply for that framework but let's keep it simple here and let's go ahead and do an on demand backup so I'm going to go over to here and notice right away that we can specify the resource we want to back up and a dynb is a great idea for a backup so what I'm going to do is go over to Dynamo DB and make a table specifically for this so I'm going to go ahead and create a new Dynamo DB table so we say create my uh ddb table fun doesn't matter what you call put some numbers here on the end it'll just say PK I don't want SK here today I'm going to leave leave it with the default so we'll go ahead and create that table and I'm going to just fill in some data into that table when it's done okay all right so I think my table's created I just deleted some of my other ones they're all junk tables uh and what I'm going to do is go over into um explore table items here and we'll go ahead and just add some items and I'm almost certain that us is going to change this UI even though they just recently changed it but whatever that's fine just say uh one and then add another tribute here called uh and we'll just say apple and I'll go ahead and create that item so not a lot of data there but you know we have something to work with and so I'm going to go back over to here I'm going to give this a refresh and we'll choose our new table so we have create backup now that's a great idea move backups from warm to Cold Storage um available for cloud information Dynamo DB with Advanced features some resource types convert incremental backups full backup so I imagine that this would be a costsaving feature uh life cycle according to specify Cold Storage is not available for backups with life cycle less than 90 days I mean yeah okay by default so I'm assuming this is a costsaving feature notice we have a retention period so how long we want to hold on to for this I'm going to just say for one day okay notice it says warm storage if we go back let's say 91 days and then maybe if we checkbox this ah okay so then it shows us something here so total retention must be set between 98 and 8 Days okay well if I put eight days here uh sorry what 10 oh 98 maybe once 98 oh there we go okay great so if this was Zero then it have to be minimum 90 days what if we put one here that's be 91 days all right well hopefully that makes sense to you I don't really care about it but we'll go ahead and just change this and I'll say one day since I really don't want to keep this around for longer than that we're going to have to place it in a vault I seem to have a default Vault so you can create a new one if you want I'm going to leave it with that one I'm going to stick with the default IM rolls we'll create our uh demand backup since the IM roll does not have submission uh permissions to do that I'm not sure why because it's using the default R so what's going on here I'm going to try this again there we go so that makes no sense like of course the default has enough permissions so just have confidence in what you're doing and you can work through these minor issues so here it is uh beginning the backup so what I'm curious about is if we go over to Dynamo DB into our table here we go to backups is it creating a backup here so we don't have any backups turned on here okay and so what I want to find out is when this is done this backup will it appear in here or is it completely isolate okay so we'll just wait till then all right so our backup job is complete let's go ahead and click into it and see what we can see um so yeah I mean it says it's done we'll go back over to the jobs yeah so not much to look at there um we'll go back to our dashboard see what else we can see here nothing interesting let's go to backup vaults let's click into our vault so here we can see our database and if we wanted to we could restore it so that's kind of interesting um so that's pretty straightforward let's go over to here and see what we'll see over to here notice that we do not see backups oh nope there it is okay great so the backup will be reflected in both of them so that is great so now my question is what would happen if I deleted my table completely and I wanted to restore it so what I'm going to do here is I'm going to delete my table I'm going to say confirm and we'll go ahead and do this so now that table should be gone right we'll go back to Dynamo DB tables and so now we have no tables right um status unknown not sure why it's showing that we'll go ahead and try to delete this again I'm going to go ahead and say delete just want to make sure it's gone I don't know why it Us's console is lying to us here today but it is deleted okay so let's say we want to go ahead and restore that I'm going to go back over to our back of vaults I'm going to click into here and let's go ahead and say restore and so we'll say my new ddb table fun I put some numbers here on the end and we'll go down below I'll go ahead and hit restore back up and so now we're restoring we'll see what happens okay I guess while we're waiting for that to create we can go over here and take a look at what's happening at Dynamo DB as I imagine the table would be provisioning um so I don't see it as of yet but we'll just have to uh oh it says it's done okay was expecting that and so I'm looking for that table but I don't see it CLI into this again um how do we see our res Restorations dashboard job dashboard restores it doesn't say that anything is done here but we definitely initially uh uh initialized a restore here so is there a bit of a delay okay H interesting I'm just wondering if I should try it again because if I try it again it might end up restoring it twice and that might not make any sense so I'm just going to wait a little bit and then see what happens okay before we do anything else well you know what I went over to jobs here and I went to restore jobs and it looks like it's actually still running so um I guess it's not 100% done I just got confused by that other status so I guess we'll just keep waiting for this to uh complete okay all right so looks like that job is complete we'll go over back to Dynamo DB we'll take a look at our tables and see what we have uh so we have our new table so there we go um another thing that we could try to do is go back over to our dashboard and create a backup plan is that is another way of going ahead and doing this we have uh some templates here I don't necessarily want to retain this for 35 days so I'm going to go ahead and build a new plan I'll just say my backup plan and down below it says my backup rule so we going to go to the default Vault I want this to backup uh daily I suppose okay we'll just say backup rule uh once or once a day and then we have our window starts within eight hours I don't care completes within whatever uh we can have continuous backups I don't care about that we can enable Cold Storage I don't care about that I'm going to place this to one and then we might want to copy this to a destination if we want to back this up somewhere else um um don't really want to do that here today but we could send it to somewhere like ca Central so somewhere else and then we have some Advanced options we'll go ahead and create our plan it says so has been successfully created you can now add additional schedule rules and assign resources to your backup plan so just because we have our backup plan does not mean that it is executed so we'll go over to jobs and um where is this I would have thought we would create it here let's go into our backup plan and click into it it so yeah we have this rule okay but how do we know when this job is going to execute and maybe it's based off of when we told it to so if we go ahead and edit this here specify the time of day the backup will start so I'm right now in Toronto time it is 106 and if we're at uh this time I'm going to go here and say 13 and I'm going to put it at 08 here at the specifi time complete within two hours so I'm being very particular in terms of when I want this to start I'm going go ahead and save this okay and so the idea is that this should trigger I would think at that time because this is now all set up but it'd be nice just to queue up a job but I thought we could do that but maybe we can't and these reports do not trigger a job no so what I'll do is I'll just wait a few minutes here and see if a job gets triggered and that's what I want to find out okay all right so here's the thing is that yes we have a a schedule for that backup but notice that it says that there is a way to do on demand backups so that makes me keep thinking that that is triggered somewhere so just give me a moment to figure that out well you know what I'm thinking is that the only way we can create an on demand backup is through that option there and so I don't think there's any way around that and so basically on backups is when you want it right away backup plans is when you want it on a schedule but I would like to see that trigger now I was suggesting that there it can't do a deploy or can't do a backup when it is um within the same window as your maintenance window so I don't think that is our issue because I don't really know where a maintenance window is here what I'm going to do is just change this rule I'm going to just say to back up hourly and that's what I wanted to do okay and now this kind of doesn't really matter because we're going to be backing it up so frequently so I'm going to go ahead and save this and what I'm going to need to do is I'm going to have to basically walk away from my computer uh for an hour and at least and then see what happens okay because there's no other way around this uh for us to see if this backups works so I'll see you back here in a bit of course if you don't want to wait around you can obviously just watch this video and see what the outcome is but I'll see you back here in an hour okay all right so I noticed that this wasn't triggering and um we forgot something very obvious which is to actually assign any resources otherwise it's not going to work so I go ahead here and just say my database and the idea is that we can include specific resource types um so here I want to say Dynamo DB Dynamo DB okay and then I'm going to just choose um our tables I want to be very specific and just choose this table here uh exclude specific resource IDs from a specific resource type no and we'll go ahead and assign those resources and now the idea is that we should see a backup occur when that will occur I'm not sure as our backup window has changed just to kind of help it out I was thinking to change the time but I don't think this actually matters now so I'm going just change it to once an hour so it better reflects what it's doing here here and so hopefully now at some point we'll see this trigger so I'm going to have to wait for that to happen and I'll be back in another hour and hopefully something's going to happen okay all right so um I've waited a while I'm just going to go ahead and give this a refresh here and now we can see we have a backup job uh so definitely created one uh at whatever time that is and so that was successful so what I'm going to do is go ahead and just delete my backup plan as I did what I was able to do here which I'm happy with my backup uh plan what what was it called my backup plan there we go we'll delete that it says related to backup selection must be deleted prior to the backup plan deletion okay so I'm not exactly sure what they mean there maybe they're talking about the backup rules I really don't know what that means you must delete a backup plan only after all Associated selections of resources have been deleted oh okay that makes sense so so um we'll go back over to here and we'll just unassociate our resources so we'll say delete my database here there we go and so now we can go ahead and delete this say my backup plan there we go excellent and I'll go over to Dynamo DB and just get rid of this table it's as it's a junk table delete confirm delete um the other question is do we have any of these backups I mean they're going to persist for some time so again just looking for them and yeah I can go ahead and delete that there so just go ahead and delete that and hopefully I've gotten rid of everything but uh yeah there you go that is adab US backups hey this is Andrew Brown and we are taking a look at aba's Global accelerator which you can find opal paths for from the end user to your web servers Global accelerators are deployed within Edge locations so you send us traffic to an edge location instead of directly to your web app I like to think of it as like a reverse CDN because um you are kind of doing the same thing where you're utilizing Edge locations but it's the user connecting and not you sending it to the edge location so kind of in the reverse Direction uh there are two types of accelerators standard so this automatically routes to the nearest healthy endpoint and custom routing where you're saying I want to route to this specific ec2 instance let's take a look at the components of involved so we have listeners uh they listen for traffic on specific ports and send traffic to an endpoint group then you have your endpoint groups this is a collection of endpoints within a specific inab region uh there is a traffic D that can be used to change the percentage of the traffic between these we have your end points these represent a resource to send traffic to um at end end point can be Network load balancer application load balancer ec2 instance elastic IP there's a global acceler accelerator that has a speed comparison tool uh which not that useful because you're not utilizing against your own uh your own app it's just more like a general one to show you how fast it is compared to all the other regions we'll take a look at this this service was super easy to make a lab for um I wish we could have covered more on this just cuz I like the service but there's not much to it here um and the lab will more or less explain this very clearly okay hey this is Andrew Brown in this video I want to take a look at ads Global accelerator which allows you to accelerate uh traffic or availability and performance of your applications uh by creating a super fast route uh to the end user to your application you can almost think of it as like a reverse CDN if you will um so for this one I'm actually going to open up it examples normally when I use abis Global accelerator it's like a checkbox away for most services so when you're setting up something let's say um a application load balancer you can checkbox it on um but I think that it would be uh more interesting to set this up using cloud formation which I have yet to do but um I think that we'll learn a lot by doing it this way to understand all the components that go uh get go involved and so I looked at the cloud formation um components and it looks pretty straightforward so I don't think we were going to have too hard of a time so here in our examples repo I open this up in git pod I'm going to make a new directory I'm going to call this one Global accelerator and I'm going to CD into that directory and when I'm to do here is uh look for that folder and I'm going to start with the base with something somewhere so I'm just looking for something where I already use uh yep here we go where I use um cloud formation so in EBS I have one so I'm going to copy those and we're going to make our way over to Global accelerator I'm going to paste these in here and uh we have our deploy script so this one is going to be Global accelerator um so that is sufficient so say deploy Global accelerator and I'm going to go down to template here and we have a lot of stuff here that we don't necessarily need um but uh yeah I'm just thinking about this for a moment because this sets up an ec2 instance with an Apachi server but if we're going to utilize this we're going to want to globally accelerate something to somewhere right so this is what we need to figure out is what can we accelerate to it says a service that improves the availability and performance of your apps with local or Global users it provides static IP addresses of a fixed entry point or uh specific things so if you go into here you you have two options custom routing and standard and so for standard what we do is we' make endpoint groups and endpoints and I believe endpoints can be four Network load balancers application load bouncers but it's specific E2 instances we' have to create custom routing and so we're not going to know that until we proceed through this so I don't really want to use this to create it I just want to find out what's at this St so I'm going to just say my um uh accelerator again I'm not doing this for real I'm just going through here to see what we have we'll say Port 80 we'll choose TCP here I'm going to go next and then I'm going to say Us East one and I'm going to go next and then we have our examples here so it says add endpoints now that if you added one or more end points you can add end points to each other uh such as elbs or create ec2 instances and so I I want to add E2 instances so that's what I want to do so what I'm going to do I ignore this uh and since we already have a security group here this is actually quite excellent and I'm just going to go ahead and remove off the volumes as we don't need additional volumes here and so this will provide us a basis of our um our compute now this thing's already configured to work for um uh CS Central one so if you're copying this change out the VPC change out the uh image ID change out the subnet you can get the image ID if you go to the ec2 console I show this a lot in the ec2 section and other parts of the course but when you want to know it know it you just go here and you launch an instance okay and then you choose uh that ID right so you know replace those and go to VPC and we'll be in good shape this will launch up an Apachi web server and I believe that we could just deploy this as is um but I'm just going to proceed to create the other components here so let's go over here and take a look so we have the global accelerator cross account attachment which we're not doing uh then we have endpoints and listeners so we're probably going to need listen an endpoint and an accelerator so I'm going to go down to examples and maybe they have more for us here not necessarily but I'm going to go ahead and grab these two these seem pretty good to me and we'll go down below here and we'll paste this in and by the way if you're worried about the cost of global accelerator you can just watch me here in the video I'm not worried about costs and most things I do are not going to cost me much because I'm very uh good at shutting these things down right afterwards or checking and triple checking but uh you know just understand that there is a cost and it's something and if you're worried about that cost then you know just watch okay or investigate the the cost uh thoroughly for yourself um I don't know if we have to specify the name if we don't I'd like to leave it out so name is required okay so you have to name it and it can't have a hyphen so we going just say sample accelerator I don't think hyphens are allowed must not begin with a hyphen okay I'm going to put some numbers here I don't know if it if it's a fully qualified domain but if it is it looks like it is because it say DNS name so I'm going to put something that is going to be something that will be fully qualified and work okay so that's going to create our accelerator uh the next thing I need is a listener so we'll go here down to examples and we'll choose our listener okay and we're going to go ahead and paste this and this is going to reference our accelerator this should not be uh under outputs in fact outputs should be indented back one and we'll paste this in above and this one should be referencing that one I'm not sure why this one's upset expected a string uh just because it's not using the syntax you normally would see so we'll do that that'll fix that issue and Port 80 seems fine to me that's what I want to utilize so that is good very straightforward so far and then we'll go to our endpoint group and we'll go down below here and here we have an example of our endpoint group and we'll paste this in okay and so that looks fine we need to reference our listener which is great I'm just going to go ahead and do this we have our endpoint region um C Central one because that's where the resources are going to be y so C Central One traffic dial percentage 100 endpoint configuration endpoint ID um so that's going to be reference to whatever the end point is so I'm going to assume that we either have to Define an end point or we have to put a reference to the ec2 instance so I'm going to go up here and let's take a look at that end point ID endpoint configuration endpoint ID an attachment AR so cross account no we're not doing that an ID for the endpoint the endpoint for the NLB is the RN if the endpoint is an elastic IP this is the elastic IP address allocation ID elastic IP address so if that's the case it sounds like we have to have I mean that makes sense we should have a static IP address with this so we're going to need another thing which would be elastic IP uh CFN and we're going to have to grab one that way so I'm going to go here and I'll go down to examples and we'll have to grab this as well and I'm going to go above our E2 instance here I'm just going to say static IP and then here we're going to reference our instance we say my ec2 instance and then the idea is that we want to reference this and I'm going to go take a look at the return values here at the same time I want to go back and back again and back again so it says here the elastic IP address allocation ID so we're going to have to use get at we are in uh where is it here end point ID is where's our endpoint end point end point endpoint endpoint endpoint here it is okay so we know that we want to get this and it's going to be get at and then it'll be static IP like that that looks good I'm going to double check make sure I wrote that correctly and that looks good okay great so now we have that set up and I'm going to go ahead and deploy this my console isn't messing up on me I'm going to give this a refresh and of course I don't have to do chamod you plus X because I copied that script if it's not ex executable for you make sure you do a chamod x uh X Plus U plus X we do that in many other videos I can just show it to you here just in case some folks did not copy it the way I copied it but you do the deploy here like that okay and so I'm going to go ahead and deploy this and hopefully this is going to work um it doesn't like something for the endpoint ID so I'm going to go back to that say endpoint ID I might have some junk hanging around on this yeah have the ref in there by accident so I'll go ahead and do this take that out great I'm going to hit up and deploy we have a lot of moving Parts I almost wonder if we have to have dependson on any of these resources but hope hopefully it's going to work as expected we're going to make our way over to cloud formation I'm deploying this in CA Central 1 so we're going to have to change re regions I'm going to assume that this is available everywhere hopefully it is and uh here we are so I'm going to go into the stack I'm going to go over to change sets I'm going to go ahead and execute this change set and hopefully but not always uh I hope this is going to work the first time okay so we'll just hang tight here all right so that was very quick and we had no issues when creating uh this ec2 instance so what I want to do is go over here and take a look and see if my instance is running uh and if that static IP address is attached and can we access the website so we'll go ahead and open this up okay and this remember this is on uh Port 80 so that definitely does work here um and the IP address here is 39751 I'm sure it's using the static IP but I just want to go take a look here 397 151 230 so now the next thing I want to know is global accelerator does it show up here in the instance I don't think it does it might but uh even if it doesn't we'll go take a look at the global accelerator um the global accelerator uh console so we go here and type in global accelerator and here we have our Global accelerator we are using standard is apparently not using dual stat which is totally fine we'll click into this here and we can see some information so the question is does it work and the way we're going to find that out is by utilizing it's DNS name and so I was thinking that maybe that name that we named it was going to be incorporated in there but apparently not which is great because I hate it when you have to name stuff and figure it out uh we'll go ahead and try this out and I want this on Port 80 so I'll just say HTTP oh it worked it worked anyway so there you go was it f F I don't know um let's try that again sure okay there is a website that we can utilize to um do a test speed so I'm going to look that up here so test speed Global accelerator AWS comparison tool yeah it's it's right there I'll just click on this one here so it says if Global exol a service that improves availability and performance of your apps this tool Compares different locations and so it's not like I can say mine in particular but I going show more regions CU I'm I'm really curious about like ca Central wherever that shows up which apparently doesn't show up anywhere here but we'll go ahead and just choose something like one megabyte and hit start so files are downloaded over htps to ALB in different regions and then you can just see what happens so direct over the Internet a This Global accelerator 15% faster and now it's running in all these different regions and it's going to show us the difference why is this one slower you're not selling me on this thing if it shows slower but we'll just wait a moment here okay and so I'll just pause here and let it go through all the uh regions okay all right so uh yeah I mean we're getting stuff here I'm not sure if it shows more if I go down here and it's still running more so it's going to take time I really wanted to see uh CA Central here which is over here but it's uh far far ways out here but we get an idea that it is supposedly running faster um so yeah hopefully that is uh good enough for you but what I'm going to do is go ahead and tear this down so we'll make our way over to cloud formation and I'll go ahead and delete the stack and while we're waiting for that I'm going to go ahead and save our template here so this will be our Global accelerator and that had to be like the easiest lab I've ever done where everything just worked the first time and the thing is I had yet to do Global accelerator view cloudformation so I'm surprised that it was so darn easy but it makesense sense cuz it is just a checkbox when you're utilizing it in other services but anyway I'll be back here in just a moment when this is completely deleted all right so this is now tear down and we are in good shape I will see you in the next one okay ciao hey it's Andrew Brown we are taking a look at adus compute Optimizer this analyzes the current configuration of your adus compute resources and their utilization metrics from Amazon cloudwatch over a period of last 14 days it could make recommendations for E to es Auto scaling groups EBS volumes Lambda functions ECS services on fargate or SQL Server licenses there is supposed to be another bullet point there I don't sure what happened there but uh sometimes these slides get away from me but yeah we'll just pretend that there is a bullet point there excellent uh it will recommend specific configuration changes to save money so here's an example of an EBS volume uh that it's saying hey this is the recommended size that you should utilize you get some charts for this thing um I was hoping a lot more from this service um because I had an account that had a lot of stuff running in it it didn't really make that many suggestions other than you can see here Lambda functions um but uh we'll explore it it's a very straightforward service um yeah and there you go hey this is Andrew Brown in this video we're going to take a look at itus compute Optimizer in order to uh properly use this service you actually have to have compute that has been running for quite a while using something more than the free tier so you don't really need to replicate this you just need to watch me do it so we can learn together what's going on here I've never used a service against my compute because I don't really need to re uh right siiz uh my computer but maybe we'll find out after using the server so I'm in a different account here I'm just in the exam Pro account I'm going to go over to the adus compute Optimizer and right now it's thinking that we're in CA Central but I have stuff running in Us East one so I'm going to just switch this over to us equals US East one and we're going to see if we have anything here I'm not seeing anything here I'm just going to go back here and I mean it is showing me something here so it's saying under provision percent 83% five out of six under provision it's talking about lamba functions so I guess this isn't just for ec2 it's for EBS Lambda functions ec2 instances Auto scaling groups and ECS services on fargate um I actually surprised it's not talking about anything for ec2 but let go over here and take a look and yeah again I keep expecting us to see something here but take a look here this page list includes easy to instances of the top recommendations to optimize to explore more recommendations to understand the price performance impacting running your workload on graviton 2 instances okay so my question is why don't I see my ec2 instances because I have at least one ec2 instance running here in Us East one probably right so we go over to here and I have this very long running instance right it's right here the exam Pro production server and if I go back over to here you know why doesn't it show up because it could totally show up here so I'm going to go back over to the dashboard and uh yeah let me just read what's going on here so it might be suggesting that I need to opt into U this but I mean I can see the service right so we'll go down here to the bottom left corner it says if you haven't said any right sizing preferences computer Optimizer considers all available ec2 instances that use the default values to generate recommendations um okay we'll go ahead and hit edit any region so I mean I would assume that we're already opted in here all opted in account so everything is opted in for this to work we'll go over to General uh there's an ER getting the enrollment status of accounts refresh again okay try this again cannot get accounts account management okay you can view optin stat for individual accounts and delegator administrator so right now it seems like everything is opted in but we're not seeing anything in here well let's go take a look at the Lambda functions at least because it was suggesting that there was Lambda functions here and so we have some um very old Lambda functions in here and it is making some recommendations so it's saying these are not optimized and they're underperforming and the current cost is this but the current cost can be that and so I'd save pennies all right how would I go and optimize that if I clicked into this one how do I know exactly what to change here all right so we'll go back here just a moment not optimized okay so it's saying like I'm using 128 megabytes and it's saying to utilize 160 and this is actually kind of nice for Lambda functions I like this because uh we would have to do power tuning to find out the price so like we we have Lambda power tuner we do this in the Lambda section right and so there's this tool that we can use and power tune lambdas and then we could figure out our cost at the time of but it would be nice if we just turn on a service that doesn't cost anything and it would just tell us over time hey you should do this so that's good for the lambdas but we have nothing for EBS so we actually do have an EBS volume here and it's saying that it's optimized so there's nothing for it to do okay okay so I mean I guess it is doing stuff it's just saying that this one's totally fine uh recommended size 30 uh gbits recommended iops 3000 so I have it at almost the lowest there so this one is in good shape let's take a look if we have any Autos scaling groups we'll just give it a moment here I do believe that we have an autoscaling group here so if I go down below to asgs yep I got an auto scale group but it's not displaying anything so CU Optimizer currently only generates recommendations for Autos skill groups that are that are a single instance type and that has the same value for the desired minimum Max Capacity so does that come back to these Gravitron okay so it's for anything in the family here so generate generate recommendations for several instance types can run unsupported instance types in addition to supported types however computer optimiz only recommendations for supported instance types all instance types are available in every region so the following table lists are supported by compute Optimizer so let's take a look and see what I'm actually using for that ASG and the way we would know is we go to the launch template it says T2 medium okay so we're going to go back over here we're going to take a look we have T2 so it says that T2 is supported then we go down below here so compute Optimizer generates recommendations for asgs that run support instance types runs only a single instance type no mixed instance types we're only running one type the values of desired minimum maximum capacity are all the same uh for example an ASG with a fixed number of instances so maybe that's what's happening here I don't really want to fiddle with this but I just want to take a look at what it is so we have desired at one minimum at one maximum at two so this is where we have a difference I mean I could change it I don't think it would hurt anything because it's not like it's ever spinning up another server but if I change this to one and go back here I can imagine that this is going to change immediately but hey at least we know that if it wants to make recommendations then that ASG has to be 111 for uh for that we'll go back over to here take a look uh no scaling policy attached no overrides are configured so the next question is there a conf a scaling policy attached I don't see any because we don't have any Dynamic no predictive no scheduled stuff so there is no scaling policy um the EBS volume apparently meets the requirements the Lambda function meets the requirements what about ec2 I mean ec2 technically means the requirements but here's the thing if it doesn't have the cloudwatch metrics it can't collect it so the ec2 instance requires at least 30 hours of metric data it should have that um if you've enabled enhanced infrastructure metrics e inst require at least 30 hours so I definitely have that uh but yeah I'm again not sure why this one doesn't show up I was hoping we'd see a little bit more here but I'll go back here and just uh undo this here let's just see if it shows up maybe it will and just say it is optimized so I'll just wait here for it to load okay just give me a moment says current or Gravitron so it's maybe it's suggesting that it can't look at previous generations so I'm running a T2 medium which is technically a T2 which is a previous generation but what's considered previous generation is determined based on what will show up here when we launch an instance so maybe AB is considering T2 previous generation because I know in RDS while I was building out all the content here t2s were no longer selectable but they are here so I don't think it's a previous generation I think t2s are still in use these Zen uh Zen stuff here but yeah I mean at least we got an idea of what this does but um nothing super exciting the lamb I think makes it very clear if there was savings here it would tell us it would say to you know make a difference in the recommend instance type and the demand on price and things like that so there you go ciao