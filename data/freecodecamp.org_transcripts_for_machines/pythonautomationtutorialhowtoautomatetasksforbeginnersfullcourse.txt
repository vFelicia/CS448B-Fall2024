In this course, you will learn how to automate a bunch of different things in Python. You will learn about web scraping, automating downloads, extracting from PDFs, automated image processing, building an automated new summarizer and more. The instructor for this course is Abdul, also known as one little coder. He has been creating courses for a while and is a great teacher. Welcome to the section one of hands on with tools to automate stuff in Python. In this section, we'll build Hacker News Headlines, emailer will begin with the understanding of the basics of web scraping. Then we'll set up a system environment by installing the required Python packages. Then we'll move on to understand the project architecture. And then we'll start scraping project with Hacker News front page. And then finally, we'll complete the email section to finish the tool building so the tool can send us the Hacker News Headlines. In this video, we'll learn about project architecture of building an automated Hacker News Headlines email. This architecture starts with getting the content of the Hacker News website front page, so we'll use the Python package request to get the get request to extract the content of the website. Once we have the content of the website in place, we'll use a Python package beautifulsoup to scrape the recorded content. So the required content, I mean, the components like title, link, score, domain name, etc. So we'll use beautifulsoup. To extract these required components from the content that we had extracted from the previous step. The next step is to build the email body or the content of the email. From the scraped content. We'll arrange it in such a way that the email body looks like there is a title there is a link, there is a number so that the email body looks exactly like a news presentation. And once we have the email body ready, we'll move on to the email authentication section, where we will use your Gmail ID. to authenticate the email section, we'll use the Python package SMTP lib to set up an SMTP library authentication system where we'll use a gmail authentication once the authentication is set up. And then we have provided or email id and then other necessary information. Finally, we are going to send that email using the email body that we have set up. So ultimately, what we are doing is we are extracting content from the web page. And then we are going to take the required components and then use the component to build an email body and then use the email body in the email that we compose and then we sent it to the required users. In the next video, we'll start to learn how to set up our Python environment in such a way that we have all the required packages. In this video, we'll learn how to set up our Python environment in such a way that we have all the required packages. These are the following packages that we will be using in this particular project. Request package is used for HTTP requests. beautifulsoup is used for web scraping. SMTP lib is used for email authentication and email transaction, email dot mine is used for creating the email body. And then finally date time is used for accessing or manipulating date and time. But these packages SMTP lib email dot mine on day 10 comes by default with your Python installation. So what we have to do is there are two external libraries request package and beautifulsoup that we are supposed to install in our Python environment. So let us go ahead and see how to install those two packages that are required. First, open your terminal and then make sure that you have Python installed already. Once you have Python installed already, you can start with PIP three install requests. Once you enter it, Python is going to look up for requests and then it is going to get a request package from pi pi which is the repository where all the packages are available and then it is going to install in your machine. To check if request packages installed. Let us invoke our Python three console and then check input requests. As you can see it has successfully imported so let us exit and now move ahead with the next package. The next package that we are supposed to installers, Pip three install Beautiful Soup for now again Beautiful Soup is getting downloaded from pi pi and then Beautiful Soup is getting installed. So as we can see Beautiful Soup is installed now, let us go ahead and invoke our Python three console and then To check if beautifulsoup is installed As you can see Beautiful Soup even though is the package name while you are importing the package in your Python session, you have to use bs for as you can see bs four is successfully installed. You can even check if a particular object is getting imported bs from VS code input beautiful. So, as you can see B is capital here and s also also capital. So it has successfully imported. So let us exit our Python environment. For now we have successfully installed beautifulsoup for and then request package. Both are the external libraries that are required for this particular project. And all other packages like SMTP lib, email, dot mime and date time are inbuilt present in our Python setup. But let us make sure that those packages are available also. So let us invoke our Python console once again, Python three. Let us clear our terminal First, open Python three, and then do import SMTP lib SMTP lib is successfully imported. Let us input email.my email that mine was successfully imported. Let us input date, time, date time is also successfully imported. So this tells us that all the required packages are available in the Python environment that we have got. So we are good to go ahead with our project. In this video, they start to learn how to code the project script. For this code editing, I'm using Python Community Edition. But you can use any ID of your choice. So make sure that Python installation is proper in your machine and Python is added in the system path, then you can use any code editor to do the same thing that I'm going to show you right now. We'll start with importing all the packages that are required. And if you remember, those are the packages that we installed. In the previous section. I will start with import requests, which is for our HTTP request. And then we'll import Beautiful Soup from bs for Next, we'll move on to importing SMTP lib. And then we'll input two objects from email mime. And then finally we'll import date time. Once all the inputs are finished, we'll start with extracting the current date time, which is the system date time. The reason we are using date time to extract the current date time is to create an email subject line where it will show us the appropriate date when the email was sent. This is for us to make sure that the same email get doesn't get overwritten every day. So that we have an understanding that every day we are receiving a new email from the automated email. So the next step is to create an empty Python object with nothing, it's a string object with nothing in it, which is going to be used as an email content placeholder. Once we are ready with this thing, we can move on to start creating a new function where we will extract the Hacker News components that we are required. First, let us create a function called extract underscore news, which will take one argument which is a URL that it is required to keep the user updated will print the user message saying extracting Hacker News stories. Then we'll create another temporary placeholder, which is again an empty string. So this temporary placeholder is going to be used to assign value to this content, which is the actual email body that we want. So the first line that we want in our email body is to say that this is a 10 top stories, it N stands for Hacker News. And then we are going to display a 10 top stories and it is going to be a bold text. And then we are going to have line breaks and then we are going to show star star star star just to make it more readable once this line is defined. Now we will move on to get the content of the URL, the URL that we are going to pass to this function when we call this function. So we are going to use request packets get function to get the content of this URL and then store it in the response object. The content once we get from the get function is actually in response body HTTP response, which will contain content the actual content that is required for us which is the content of the webpage. So we are going to use the method content on the object response to store the actual content in content. Remember, this is a global object which is called And this is a local object within this particular function, which means these two are different. So the scope of this particular content lies only within this function. So do not get confused with this content and this content. Now, we are going to use this content that we extracted using the response body. And we are going to use the HTML parser to extract or make a soup out of it. from that particular soup, what we are going to be interested is about the components that we are going to require in this particular project. So, to understand what are the components that we are going to required, we have to see the website structure. So, in the next video, we will see the website structure of the Hacker News front page to see what are the components that we would be required to extract using this beautifulsoup function? In this video, we'll see what are the components that we need from the Hacker News front page website. And as you can see from this code, this is the URL that we are going to use. And this is the Hacker News front page URL. So let us go to our browser. So in this case, I'm using Mozilla Firefox. So let us go to our browser, and then open Hacker News front page. And this is how the site actually looks. This is one of the most popular websites on the internet. And this was started by Paul Graham was a very famous personality, and internet entrepreneur. And he also runs an incubator called Y Combinator. So this is a website that has been read by 1000s and 1000s of people every day. And our objective is to extract this content and automatically send an email to us so that we can see why only when there is an important or interesting content for us to see, then we can go to the website, that is the objective of this entire project. So as we can see, this website, we can see a header in this website or a navigation bar. And then we have list, which is very similar to how, you know Reddit kind of website look. So in this setup, there are some components that we could be interested in to understand what is the link? Or what is the information about First, we need this one, which is the actual title of the link, then we would be also interested in knowing the points which shows how popular that particular link is. So to know what are the contents that we should be escaping from this website, we have to first open our web inspector. To open web Inspector, you can either press F 12 in your keyboard, or you can right click in your machine, the browser using your mouse and then click inspect element. So click Inspect Element, then you will get this particular page opened, which will give you a sense of how the web page is designed to let us slightly increase the size of this. And let us pick the pick an element tool. So now we have picked up this tool to understand what are the components that we need from this particular Hacker News front page, which we will use in the code to extract that particular component from the content that we have already extracted. So pick this thing and then go here, as you can see this, as you hover, you can actually see the CSS value, which is also called the selector. As you can see, moreover, you can see a dot story link here you can see a dot h and user a dot some score value, then you can actually see a then you can see a so what we can see is we can click mouse and then see how we don't have to actually click we can just hover around and we can actually see how the chord changes and then the first area of interest for us was the title. So who are your most of them and click the button. As you can see, once you hover there what you actually get is you get an A anchor text with the class story link and which is present inside a table with class title. So, the first point that we are looking for is anchor text with story link. So let us go to our code and see what we have written. So what is our area of interest is we are trying to extract everywhere where we have got db, db is in HTML is the actual sale inside a table HTML table HTML table is created using this tag table. A table row is created using this tag BR and then the values the actual sales inside the table row is using created using the tag the D in this step what we are trying to do is we are trying to tell beautiful su to find all TD from this particular soup that we have just created. de su. So, we are using the function find all to find everything that is TD, but as you can see, we are trying to find everywhere where there is TD with class title. So, as you can see, we are trying to find everywhere there there is class title and then we are trying to extract the components of it everywhere there is class title and then we are trying to extract the components or the values inside it that is exactly what we're doing in this function soup dot find all DD and then the attributes the HTML attributes that we are looking for this class title and then we align we align is something that you can actually see, this is what we are trying to find out from this particular webpage. So, we are trying to find everything that is TD and we are trying to extract that using this attribute class title and then we are saying we align should be empty the first attribute class should have value title, the second attribute we align should have nothing in it. So, just let us see it once again. Once you click the inspector tool, and then however on the page, you click on the title that you want, and you can actually see the title The TD plus attribute holds the value title and then the we align like this one is not present yet. So, this is to eliminate the junk and then extract the component that we want. So, once we do that, we are trying to find wherever we are trying to convert the entire thing as a text. So, to see that we have to understand one more thing, what we are actually seeing is this is okay. So, we are extracting TD, which has class title, but that is only for the first element, but what we need is we need all the links, we need all the 30 links from this particular page for us to to do that, we are trying to put this entire thing in a for loop and then we are using this function enumerate just for one simple purpose because in the final email that we want, we want numbers 123456 until 30. So, for that purpose, we are trying to use enumerate which will give us the actual number the index value and also the value of this output. So we are using a for loop and we are using a numerate to say okay, I want all the values that is output of this thing, which will give us all the text all the scraped output and then we are saying enumerated so that we have the index value and we also have the actual tag that is extracted from this page. And once we do that, we are entering into the for loop trying to build the email content actual content. So as we just discussed, we are trying to create the table row number with this value I and as you can see, Python is a zero index language. So what we are trying to do is we are trying to say okay, I plus one which will give us one for the first row and then two three until 30 so on and then we are going to we are trying to actually have a nice looking format just like this, which will separate the index number from the actual title and then we are trying to convert everything. What we are trying to do is we are trying to convert everything that we just extracted using dot txt into a text. So we are trying to say okay, you have given me the tag, but I don't want the entire day, I just want that text inside the tag. So the value inside the tag. So we are using tag dot txt to do that. And then now we need a line break. So we are going to use br which is an HTML tag for the line break. And then one more thing is required here, which is if you can notice in this particular page, you have got all the title letters again open our whip inspector and you have got all the title. But at the end of the page, you actually have another title which is TD class title, we aligned nothing, but this value is more. So in order to avoid this more getting captured in our final output email body, we are trying to eliminate this saying that we want everything one to 30 except when there is a value more that is what exactly we are doing. We are saying we want everything except when the tag is not equal to more. We are saying give me everything when the tag is not equal to more. And then we are concatenating it for every row. So this for loop is executing for every row and then every row value is getting added in the CNT. And at the end of this function, we are returning this object Python object that we created, which was an empty placeholder string, as CMT. To recap, this particular function, this function is to extract the front page links or title or the components that we wanted So we are creating a function called extract underscore news, where we are passing on the URL. And then we are creating a nice title, which says agent top stories, and then we are extracting the content, we are using beautifulsoup to extract make a soup out of it. And then we are using soup dot find all to find all the teeny tags with attributes class title, and then we align nothing. And then we are trying to create rows using this thing. And while we are creating rows, we also noticed that there is one final row which has a value more which we do not want. So we are excluding that. And then finally we are returning the entire object, TNT as the result of this function. In the next section, we'll see how to call the function, and then we'll move on to composing the email. In the previous video, we learned how to build a custom function that we use for extracting the news from Hacker News friend page. In this video, we'll see how to call it function, how to finish the email content, and then how to start with email authentication. So to start with, we can see the function that we tried to build in the previous section is called extract underscore news. That takes one argument which is a URL to invoke the function or to call the function, we are going to say extract underscore news. And then we are going to pass on the URL of that Hacker News friend page as a string, once we do this thing, this function gets executed and then whatever is returned in C and D will get assigned to this particular CNT. And as we saw in the previous section, this C and T is a local object whose scope is within this function. And this C and T is part of a global object, which has scope in the entire code. Once we have this end, what we are going to do is we are going to append this end to the content placeholder that we created. So we are saying just content plus equal to which is as equal and as content is equal to content plus. So instead of this thing, we are going to simply say content CNT plus equal to c NT at the end of the email body, then we are trying to put empty lines with dashes to denote that the email is finished. And then finally, we are going to add two more lines and then say this is the end of the message. This is just for us to make the email more professional, more useful in understanding where the email starts with this thing and where the email actually ends. Once we have this finished, what we are going to now do is we are going to start with the email composing state. And then the first step of email composing is to create the parameters that is required for email authentication, as we saw in the project architecture section, so once the email composition, the email body is ready, we are going to start with the email authentication. For email authentication, there are five important parameters that we have to define first, what is the email server, the SMTP server that you are going to use? Second, what is the port number? Third, what is the from address email address where you want to send the email? What is the address where you want to send the email, and then finally, the password of the address from address from where you want to send that email. One thing that you have to keep in mind is this to address could be actually a list where you want to send this email to multiple recipients. So in this particular project, we will see how to send this email to yourself so that you can keep yourself updated with Hacker News Headlines every day. But actually, you can even send this email to multiple person provided that you give a list of email ids list. When I mean list, it's actually a Python list. So to start with, we are going to use a Gmail account for this particular step. So we are going to stay SMTP gmail.com, which is the SMTP email server for Gmail. And for Gmail, the port number is 587. And then the next thing is the from email ID, which should be in character as a string. So for the sake of this particular project, I'm going to use my gmail account from where I want to send my email. And then also, this is the same email ID for which I want to send this email to remember, the to address could be a list of email ids, where we have multiple email ids so that this one email could be sent to a lot of people. And then finally we have the password that we are going to use for this email account. So you enter the password that is required to log into this account. And then this will complete the parameters that we wanted. Once we have this in place. The next thing is we are going to create the message body. So the message body that we want is a mime multipart. So we are creating an empty object with using function mind multipad. And then we have to add the subsequent components of an email, an email is supposed to have an important thing, which is called an email subject. So to create an email subject, there are multiple things that we can actually do. The first nyovest thing that we can actually do is we can have a title, that doesn't change. But the disadvantage with that is, in an email client, like Gmail, or outlook, if you have the same title, every email that comes next day gets folded in the same conversation, instead of having a different email, the same email will be there, and then subsequent emails will be added as a conversation. So to avoid that, and also for us to understand when did we see that particular email, what we are going to do is, we are going to create a dynamic email subject and the way we are going to do that is, as you can remember from the previous videos, we had created a new object Python object that called now from the date time package, which will return the system date the current date. So what we are doing here is we are saying okay, this is my emails, which says top news stories, hm automated email, that's well and good. Next what we are trying to do is we are trying to actually create the date object, str now dot they will give you the day. The next one is str node month. The next one is str node here. So what we are trying to do is we are actually trying to create an email subject line that has the date, component, date, day, and then your year. So once we have this thing, we are going to assign it in this mime multipart that we created as subject. The next is the from address from the next store address as to and once we are done with this thing, we are going to attach the email body the email body that we created as message MSG dot attach. As you can notice, here, we are trying to make this an HTML email. If you remember, we had used Be bold as HTML tags to make our email look slightly more better than a normal text email. And that is why we are using mime text should be content HTML, and then we are attaching that content to the email. So with this, our email body is currently ready. Now we are moving on to the authentication section we are we are printing the message that initializing server. Once we have the server components in place, the next step that we are going to do is we're going to call SMTP function from the SMTP lib package. And then we are saying okay, this is my server, this is my port, I'm going to send it to server and then this function set underscore debug level. One is to say whether we want to see debug messages if the server has issue in connecting if the server has any problem. If the authentication is not successful, do you want to see the error messages or not. So if you do not want to see the error messages, you can set zero. If you want to see the error messages, you can set one which will help you in debugging. Once we have that thing, we are going to initiate this over with Hello. And then we are going to start a TLS connection which is a secured connection. And then once that is done, we are going to log in from ID using the password that we have given. Once the login is successful, then finally we are going to send the email that we have composed from this ID to this ID where the message that we have created is sent as a string using the function as underscore string. Once the message is sent successfully, we are going to print a user message email sent. And then finally, we are going to quit from the server that we just initialized. So we are going to initialize the server using the server and port detail that we just created. We are going to set the debug level one to understand the error messages, we are going to initiate the transaction with the server starting with ello and then starting with TLS server and then we are going to log into the from and the ID email id and then using the password. And then finally we are going to send the email from this ID to this ID or set of IDs that we have created here. And then finally, we are going to send it as a message using this as underscore string function. And then finally we are printing a user message and then we are going to quit from the server. In the next video we'll see how the email actually looks and then how do we execute the script. In the previous section, we completed the actual code that was required for this particular project. But before we move on to executing the code, there is one change that you have to do if you are going to use your Gmail account. If you are going to use your custom SMTP like your company email id, or you're going to have your own email server you probably would not need to know this thing. But if you are going to use your Gmail account to send an automated email, this is one mandatory step that you have to do otherwise, your email would draw authentication error. So what is that thing. So what you have to do is you have to go to your email account. So what you have to do is you have to go to my account.google.com slash security. So this is what you have to open, you have to go to my account.google.com slash security and then you could do Security tab. Once you get into the Security tab, you will see something called less secure access, you'll see less secure app access onto your scrawled on the top page looks like this. So once you go at the end, you will see less secure app access to click this button. Right now I have it on but for you It should be ideally off. So what you have to do is you have to turn it on. So read this message carefully wants to Google is trying to tell you that you are trying to enable your email sign in for less secure technology. And this project that we are doing it is calling it as less secure technology because it doesn't use two factor authentication, unlike your mobile phone or mobile app or something else. So Google is trying to let you know that you are trying to access or give access to your Gmail login for less secure app. And this is how exactly the message is for all the email automation project that you will do. So that is completely fine. But make sure if you have two factor authentication, this is not going to work properly, you have to find another way, which you can find on Google forums. But for normal login, if you have a normal login, this is what you have to do, you have to go to my account.google.com slash security, you go click this button. And then by default, it will be like this for you, which is off. And then you have to go here and then turn it on. Once you turn it on, you'll see this message. And then you will have an ELO color. Let me refresh the page, it will take slightly some time for Google to refresh your on to off logged on to probably have to wait executing this until that. So now you can actually see that this is refreshed. And then it is showing with an exclamatory Mark, which is lately a warning sign to say that you have enabled your Gmail login or less secure apps. Once you are done with this step. Now you can go ahead and then execute your code. And then let us open go to the Python Community Edition that we were using Let us open the terminal. So please note, you can even go to your system terminal and then do this thing. Or even you can use your Python terminal. For the first time we'll use your Python terminal to see what are the error messages that we are getting if you are getting some error messages. Or if you are not going to get any error messages, then we can probably you know automate this entire thing using Windows Task Scheduler, or a bash script, that would be just simply run on your terminal or shell. So to start with, what we have to do is we let us see what are all the files that we have, these are the files that we have in this thing. And this is the particular file that we are of interest, we will say Python three, and then we are pasting the file name and then executing it. As you can see, these are the error messages or user messages that we were printing. And now because we had enabled debug level one, this message has been sent. And you can see that it is showing that this is first extracting Hacker News stories composing email initiating server, all the IP address related details and starting the TLS server. And then it is saying though, okay, SMTP has started. And then from this email, you're sending it to this email. And then this is your email body starting with automated email. And then finally you're finishing the email, and then you're receiving a message that email sent and then the email connection is closed, we'll see how to do the same thing using your terminal. To open the terminal that you have taught with the terminal that you have like in my case, I'm going to open my Mac terminal. And then I'm going to first navigate to the place where I have got the quotes. Once you navigate to the folder where you have got the quotes, now check what are the files you have got. So these are the files that we have got. So copy the file name. Now open Python three, and then filename. You are executing first extracting the news composing the email. And it is the same set of messages that we have seen. So in this video, we learned how to enable the Google setting that will allow us to send automated emails through Gmail. And then we also saw how to execute our script, both in the Python and also in our terminal. So the way we execute is using this code, Python three which is to exit Python three or invoke Python three console and then the file name. In the next video, we'll actually see how the email looks like. In this video, we'll see how the email that we sent using the previous automated script actually looks like. Let us go to our email. As you can see, Google has sent me a critical security alert, which is just to notify that I had tried to enable My Account Login for less secure apps. And you know, that is completely fine for us to see and then ignore. And then the next thing is, we can see the email that we sent. As you can see, you have received two emails, because the first email was sent using the terminal inside pi charm. The second email was sent using the shell, the actual terminal that is in your voice, which is your command prompt or terminal. So let us open the email with a slightly zoom out to see how the email actually looks like. As you can see, this is the subject of the email where this is static, what we created. And then this is the date that is the current system date. And then you can see a bold email title, not the subject, actually mail title. And you can also see that email was sent from to this email. And you can actually see another important thing that this email has fallen initiate your inbox and not inside your spam box. So what happens is, if you do not have all the email companies that mime components that we set up, your email might end up in your spam not in your inbox. So make sure that you have got all the email components that we described in the code setup, right? Now you can see that you have a title, then you have all the formatting that we did, then you have the number that we used using the enumerate index i and then we have this formatting the separator, and then we have the title. And then finally, we have the domain name with the link. In fact, as you can see, we have 30. And then finally, we have this thing, and then we finally say end of message. Let us open the next email also, to see Okay, the first one is this. The second one is this. And then we have all the 30 items. And then finally, we are seeing it is in definisi latest ones, go to the Hacker News site, and then verify what is it. As you can see, bullshitters is the first we'll show this is the first out to setup setting up an ad block, putting up an ad block. There's no third probably it has changed by the time we send an email. Now how do I hide from a surveillance? How do I hide from a surveillance, this makes sure that the email that we sent is the actual Hacker News for this time during the file for 2019. At this time, 25 for 2019. So in this video, we saw how the actual email that we sent using the previous script looks like and in this section, we have learned how to build an automated Hacker News headline e mailer, which now further you can extend to probably a Windows Task Scheduler to send it every day morning, or using a bash script scheduler or a cron job to automatically send this email even without executing. So in the video, start of the video, the previous section end we learned how to execute the script. But once you automate it using a task scheduler or a bash script, or a cron job, then probably you would not have to even open or run the script every day, the script would be automatically executed. There is only one important thing that you have to keep in your mind before you know we close this section is that this email script will contain your email password. So make sure before you upload it on GitHub or before you share it with your friends the same script that you remove your password so that no one else knows your password. So in this entire project, what we learned is, how to scrape a website, how to extract the components that we want, how to build an email and how to automatically send that email from our gmail account. Hope you enjoyed this section. Let us see in the next section. In this section, too, we'll learn how to build a TED talk video downloader, we'll see how to install requests package and understand how to use request package for HTTP requests. With that, we'll build a basic script to download the video of a given a TED talk. And then we'll store the video in our local machine. Then we'll generalize the code to download any TED Talk video given the URL, and we'll ultimately package the script as a C light tool. In this video, we'll see all the packages that we'll be using in this project. The first and foremost package that we are going to use in this project is requests request is a package that will help us get the web content. And the request. The name comes from HTTP requests, which is how the communication between a server and client happens in a HTTP protocol, the client sends a request to the server, and then the server response back since the response as a result, typical HTTP request contains the actual request. And then the header lines, like authentication, and then an optional empty message body. So sometimes you can have the information that is required to be passed on in that message body, the package that we are going to use for that is requests in Python. So let us see how to install that required package inside our computer. So open your terminal. And then as you might have seen before, because we are using Python three, we should use PIP three install request. This will install request package from pi pi. So now as you can see, a course package is successfully installed. To verify that we can open our Python three console, and then import request and then see that it has been successfully installed. Thank you for listening, we'll see the next video about another package, which is Beautiful Soup. In this video, we'll see about Beautiful Soup. Beautiful Soup is another important package that we'll be using in this project. Beautiful Soup is used to extract data out of HTML and XML, primarily Beautiful Soup is used for web scraping. So the request package that we saw in the last video will give us a content from the webpage. But as you might have guessed, the content is an HTML or XML format, in which websites are designed. So Beautiful Soup is the package that will give us the formatted content, the extract of whatever we want from the HTML or XML file that we requested from the request package as a get request. Let us see how to install Beautiful Soup package inside our Python environment. Beautiful so goes by the name Beautiful Soup for so open your terminal type PIP three, install and Beautiful Soup for everything in small case, once you click enter Beautiful Soup will be collected from pi pi and then it is going to get successfully installed in your local machine. To verify whether beautifulsoup is installed. Let us open up our Python environment which is Python three, and then import bs four. As you might have seen, the package that we installed was beautiful soup, but when we are going to call it we are going to use beautiful bs for us Beautiful Soup. And especially the function that we are going to use the object that we are going to use from Beautiful Soup is cumbias for input beautiful. This is the object that we are going to use from the package Beautiful Soup. So as you can see, and you install it, you have to call it beautifulsoup for when you import it you have to call bs for and then from that this is a specific object that we are going to use inside our code. Let us get into the next video where we'll see how to build the basic code. Thank you. In this video, we'll see how to build the first version of the TED Talk video download it, as we have seen in the previous videos, we have successfully installed request package and we have successfully also install Beautiful Soup package. So let us start with the code. Let us import the required packages first in the header section. And then let us move on to the code. The first package that we are going to use this request. So let us import requests. And the second package is beautiful. So as we saw in the Beautiful Soup package video, we are going to import Beautiful Soup the object from the package bs four. And once we do that, the next package that we will be requiring is our E which is for regular expression manipulation. So regular expression, as you might have known is just to do pattern matching. And then finally, the package that we're going to use is Sis, which is for argument parsing, which is to generalize the code for using multiple URLs as a combined package. So let us start with the code. So as we have imported the required packages in our header section, we'll move on to the further section. We'll see this exception handling in the next video. Meanwhile, we'll use a URL that is hard coded. So here there is a URL of a TED talk. And that is defined in the object URL. So the first step is to use request package to send a get request to get the content of the URL. And stored in the object. So, because this is going to be a long process, it is also good to Minter message to the user who is using this package, the project that we are developing to indicate that the download is about to start, once the request packages successfully get all the content from the URL of this TED Talk, the next thing that we are going to do is we are going to use beautifulsoup. To create a soup out of the content that we have got. As you can see here, the response of the get request is stored in our which is the Python object. But here, when we are going to use it with beautifulsoup, we are going to save our content, it is because the response contains a lot of things like the status code, the result of the request. So in this entire response body, the only thing that we are interested is our dot content, which is the actual content of the website URL that we extracted. And once we are going to use beautifulsoup. To assign it to the soup object, the next step is to identify the exact location where we have got the mp4. To understand that let us actually see the source code of the doc page. When you open an Tiktok, just like this, when you press Ctrl u, you will get the actual source of the project. The page in this page, we have to see where mp4 is present. So we'll say Control F mp4. As you can see, this is the place where you have mp4 and we are interested to extract this particular URL. But before that, we have to see where the exact location, this mp4 is present in this entire page. So we'll just scroll to the first. And we'll see that this entire content is present inside a script that starts with top page in it. And that's exactly what we are going to use our beautifulsoup code to find within this entire page. So entire content is present in soup. And then we are going to say inside soup find everywhere where you have script. And inside that script using rejects regular expression we are going to find for this particular word, and then we are going to store the result inside result. And as you can see from this entire script, this script contains a lot of text. And the only part that we are interested in is a proper mp4 file. So we are building a regular expression pattern to say that it should start with the URL should start with HTTPS, and then it will contain also mp4. And then we are going to assign the result in result underscore mp4. At this point, you might have got a lot of results. So what we are going to do is we are going to split everything based on one separator and then we are going to take the first output after the split as the proper mp4 URL, because as you can see, with the mp4, you actually see medium quality, light quality, high quality, so not bothering about the quality of the videos, we are going to just take the first URL after the split. And then finally we are going to print a message that we are going to download video from the URL. And then we have to have a file name. So to get the file name dynamically, we are going to use the URL title or the file name which is also present in here. And then the final step, we are again going to use GET request to extract the content of the URL, which is currently the mp4 file. And then we are going to use that content to write using F dot write and then we are going to save in the output file. So as you might have seen, this is the mp4 file which should end with dot mp4, which we extracted from here this part and then we are going to use F dot write to write the content of the mp4 file in the output file that we wanted, which is an mp4 file and then we are going to print a message that the download process is finished. So in this video, we learned how to build the entire code, a generic first version of the code that will help us in downloading the mp4 video of a TED x Ted video from ted.com using requests and Bs for Thank you for listening. In the next video, we'll see how to generalize this code so that it can be packaged as a CI tool that anyone can pass a URL instead of hard coding it and then download the video. In this video, we'll see how to generalize the code that we built using the last video for a better CLA tool. So what do we mean by UCLA to a CLA tool is nothing but you can have a package the code that we develop Have as one line in your terminal, and then you can get the output of that without having the need of entering, you know, your PI charm or without having the need of editing the code. So as you might have seen in the last video, we actually hard coded the URL, we gave the URL as part of the code. But that is not going to help us in long term because not every time you want to open a text editor, enter the URL, and then recompile the entire or execute the entire Python code. So for that purpose, what we are going to do is we are going to generalize this code. And as we saw in the last video, that is exactly why we are going to use this particular module of Python, which is just so first, we need to check whether someone is giving a particular URL as part of the execution command. So for that, we are going to include this exception handling module in the quote, exception is nothing but unexpected error. So that's why we are calling it Exception Handling how to handle the exception. So in this section, what we are doing is we are checking sis dot arg v arg stands for argument that is passed with the code execution. So we are seeing if the length of sis.rb is more than one then we take the first argument that is passed with the code execution is we are going to pass this message sis dot exists. Exit message that says error, please enter the TED talk you are to demonstrate the output of this let us go to terminal, let us take the code that we saved using the previous section. And then we'll use Python three, and then type this and see what is the error that you are getting the error that you are getting this error, please enter the TED Talk URL. So that is exactly what is happening here. So we have written this exception handling module that will tell us error, please enter the TED Talk URL, which will show up when someone is just executing the name executing the file name without passing any argument, which is what we are checking here. So to check how the code executes properly, let us actually take a proper URL of a talk and then we'll see how to download this entire video as this is becoming a CLA tool. Now let us save this code. Let us copy this video URL that we just used hard coded in the previous video. So now we'll go to terminal we'll do the same Python three a talk downloader, and then we'll paste the video URL, the actual data URL, and let us Press Enter. And as you can see, these are the messages that we have given in the previous section download about the start and end download has started, we extracted the URL name and then storing in this particular name, and then it says download process finished. At the start of this video, you might have seen that we had only four files in this particular video or in this particular folder. But now, as we do LS you can actually see we have one extra file which is an mp4 file, which is what we have downloaded. Let us go into our finder and see we had all these four files. And now as part of this thing, we have this file also which is actually a tic toc. So what we have seen is, we actually built a first draft code first version of the code that did not have argument parsing where we hard coded the URL. In this video, we learned how to generalize this video generalize this code, which will include URL as part of an argument and that URL will be used to download the video and we also saw how to handle the exception when someone is not giving the URL as part of the execution in the terminal. Thank you for listening. In this section, we'll learn how to build a table extracted from the PDF file format. pdf is one of the most prevalent file formats that we deal with in our daily life. Anyone who works in data science would know that extracting table from PDFs is one of the most boring manual tasks than one have to deal with. In this section. We'll start with basics of PDF file format, then we'll learn how to install the required Python modules for extracting the PDF. Then we'll actually do the coding part to extract table from PDF. Then finally, we'll learn a bit about panda's data frame and then using panda's data frame to write the table that we just extracted into a CSV file. Thank you. In the next video, we'll start with basics of PDF file format. In this video, we'll learn the basics of PDF file format. pdf stands for portable document format, which is a file format developed by Adobe in the 1990s. This file format was developed to present documents that include text, graphics and images, independent of the software and hardware and operating systems as seen, let us say whether it is Apple Mac or Microsoft Windows, a document should look the same in both the operating systems and both the hardware, hence, PDF was developed. The first version 1.0 of PDF was introduced in 1993. pdf is based on the PostScript language, each PDF file encapsulates a complete description of a fixed layout flat document. The way that text and graphics are embedded on the PDF are based on the layout not with any structured format. The general structure of a PDF file is composed of four main components header body cross reference table trailer, the header contains just one line that identifies the version of the PDF, for example, percentage PDF 1.5. This indicates that this PDF belongs to the version 1.5. The trailer contains pointers to the cross reference table and two key objects contained in the trailer dictionary. It ends with percentage percentage Evo F. To identify end of file, e o f stands for end of file. The cross reference table contains pointers to all the objects included in the PDF. It identifies how many objects are in the table, where the objects begin, and its length in bytes. The body contains all the object information, for example, object informations like fonts, images, words, bookmarks, form fields, and so on. So these objects are mapped using the cross reference table and thus this forms the structure of the PDF. So far in this video, we learned the basics of a PDF file format and the general structure of a PDF file. In the next video, we'll learn how to install the required Python packages for this project. In this video, we'll learn how to install the required Python packages for this project. So we need three Python packages for this particular project. The first one is Jupiter, the second one is Camelot. And then the third one is shebang, which we are going to use for data visualization. So to install Jupiter notebook, we have to open our terminal and then use the Jupyter Notebook installation command. Before that, let us see a bit about Jupyter Notebook. Jupyter Notebook is an open source web application that allows you to create and share documents that contain live code visualization and narrative text. So Jupyter Notebook is one of the most preferred IDs are notebooks used in data science community. And the reason is Jupyter Notebook lets you write code and also the narrative text in the form of markdown in the same file format. Also Jupyter Notebook lead says upload Jupyter Notebooks rendered file which is a markdown on web. So if you are going to maintain a web blog, which is markdown based, so you can export Jupyter Notebooks markdown file and then upload it on web. Alternatively, if you want a Python file, not a Jupyter Notebook, just a Python file to share it with your peers or automation. Then Jupyter Notebook also lets you download the file that you have written the notebook file into.pi format, so let us go ahead and then install Jupiter notebook. Open your shell or terminal where you would be doing this installation if you're using Mac, open your terminal and if you are using Windows, open your command prompt. So once you open your command prompt, as we have seen in the previous videos, if you have Python three then you have to type PIP three for installation of any Python package and then type install and Jupiter. Once you type enter, This command will install Jupyter Notebook on our machine. So it seems that Jupyter Notebook has been installed. So let us validate whether Jupyter Notebook has been successfully installed. So let us type Jupyter Notebook Enter to invoke the Jupyter Notebook. As you can see, Jupyter Notebook has been successfully installed. Now for us to shut down this notebook. Let us go to the terminal and then type Ctrl C. So type Ctrl C in your keyboard to shut down this Jupyter Notebook and it asks you whether you want to shut down press why that the Jupyter Notebook has been successfully shut down so you can see the shutdown configuration. So the next package that we would like to install is Camelot. Camelot is the Python package that we will be using to extract tables from PDF. Camelot is an open source package that is available on pi pi. So the same way that we install Jupiter notebook we can use PIP to install Camelot, Camelot is the package that we have preferred in this project to extract table from PDFs. So let us go ahead and Then install Camelot package. So now once again type PIP three install Camelot but the thing with the Camelot packages instead of just typing Camelot you need to install Camelot pi The reason is there is already a Python package in the name of Camelot which is not this package. So, these package developers decided to put it in the name Camelot hyphen p y. So, the package that we should be installing is Camelot dash p y even though the package name is Camelot, we have installed it like this. So, let us type into that the package gets installed on our local machine. So, we see that the package has been installed. So, let us verify whether Camelot has been installed successfully. So, let us open our Python Client repple. Once we have Python Client, let us try to import Camelot. So, Camelot has been successfully imported without any error, which means Camelot has been successfully installed. So, let us exit the Python console. And then the next package that we are interested in is C one c one is the package that we are going to use for data visualization. So, as part of this project, once we extract the table from the PDF, we are going to visualize it so that the data science workflow is completed. So seaborne is the package that we will be installing. So, let us install c one, let us go ahead and open our terminal. Let's clear the screen and then type PIP three, install c mon. Once you type enter, c one is going to get installed. So C one ultimately requires matplotlib as a dependency. So if you have got matplotlib already on your machine, so C one wouldn't install it again. But if you have not got it, no problem not plot lib would also get installed on your machine. So let us clear the terminal and then verify whether seaborne has been successfully installed. So open Python three, input c one. As you can see c one has been successfully imported, which means c one installation is successful. So in this video, we have seen that we installed the required Python packages, we install three packages, which are Jupiter or Jupiter notebook, Camelot for extracting table from PDF and then C one for data visualization. So in the next video, we'll start with the coding of how to extract table from the PDF. Thank you for listening. In this video, we'll learn how to extract table from a PDF file. Before we start with the coding part, let us try to understand what other Python modules are available for the same purpose. The first one is tabular. tabular is one of the most widely used PDF extraction library. tabular is actually based on a Java library in the same name tabular. So, this one that we are talking about is a Python binding for the Java library. The next one is PDF lumber, then PDF tables and PDF table extract. So, all these libraries are available as an alternate for the library that we have picked for this particular project. So even though all these libraries are available, we have selected Camelot to go ahead with so the reason we selected Camelot is because of the following reasons. The first main reason is you are in control. So unlike other libraries and tools, which gives you a nice output or failed miserably, so there is no in between. So either it gives you a nice output or it fails miserably. Camelot gives you the power to tweak the table extraction with hyper parameters, which means if you do not get any output, then you can tweak your hyper parameters to get at least some output show that not everything in the PDF table extraction becomes manual. The reason is because since everything in the real world is actually fuzzy, including PDF table extraction is also fuzzy. You need to have control over the hyper parameters to tweak how you want to extract the table from the PDF. The second one is bad tables can be discarded based on the metrics like accuracy and whitespace. So Camelot gives you these metrics accuracy and whitespace so that you don't have to manually look at each table to select the good table and discard the bad table. The next reason is, the table output that you get out of Camelot is a panda's data frame. panda's data frame is one of the most widely used Python module for data analysis and data science, which means the output of the Camelot library could be seamlessly integrated into any ETL workflow or a data analysis workflow, which exhaustingly uses Camelot or Python. The last reason is because Camelot lets you export the extracted PDF table into multiple file formats, including JSON, XML and HTML. Let us say that the table that you extracted from the PDF file format you wanted it to be published online, which means you ultimately want HTML file format. So instead of sitting and hard coding, HTML file format A table HTML table Camelot lets you export the PDF table that you just extracted into a HTML file. So this way, Camelot helps you with being in control, discarding bad tables. And then using a panda's data frame, which is easily seamlessly integrated with an existing data analysis workflow. And then finally exporting the file format into a different file format. So this is the reason why we pick Camelot ahead of the other packages that we just mentioned. So let us move ahead and then learn how Camelot is going to help us in extracting tables from PDF. The PDF from which we would like to extract data is this PDF. This has been downloaded from un website, which is economic and Human Development indicators for India. So this is a fact sheet with multiple tables. As you can see, you have one table here, you have another table here, so multiple tables with multiple columns. So for this particular purpose, we are interested in extracting the values row 2021 and 22 of this table, which is literacy rate. So let us go ahead and see how we, we are going to extract this particular table and then do a little bit of data visualization with this, before we move on to the actual coding, because this is the first time we are going to use Jupyter Notebook in our course. So let us see a bit of overview about Jupyter Notebook. So open your terminal, which is Windows command prompt or Mac terminal, and type Jupiter notebook. Once you take Jupyter Notebook, it would internally evoke a server and then your Jupyter Notebook, which this interface would open. And then for you to create a new Jupyter Notebook, click here new and then type Python three, click this Python three. So once you enter here, this is how the structure of a Jupyter Notebook would look like. So this is the title which you can add to say, Okay, my first Jupyter Notebook. So once we have renamed it, this is how the Jupyter Notebook would look like. So this in the Jupyter Notebook would be called as a cell. This cell can have primarily two values. So it can have a code value where you can write your Python code. Or it can have a markdown where you write your narrative text or documentation. So let's start with the documentation and say, This is my first Jupyter Notebook. And then let's say this is a heading. So once you are done with this thing, this is how it looks like. So now let us go ahead and write a small Python code. As you all know, Python can be also used as a calculator, which means you can do basic arithmetic operation. So let us go ahead and do a little bit of arithmetic operation which says three into three. Once you are done with this code, you can press Shift Enter, like this, and then the output will be displayed. Or maybe if you do not want to use the keyboard shortcut, you can say okay, four minus three, which is probably one and then we'll go ahead and click Run button here, which will show us output one. So this way, you know that you can have documentation or narrative text and then code in the same file. And then this is the advantage of Jupyter Notebook. And one of the reasons why we prefer Jupyter Notebook for this particular project. Now, let us go ahead and start executing the actual code that we would like to write for extracting table from the UN report that we just saw. So to start with, we should name the Jupyter Notebook, which is a good practice. So in this case, we can name it extracting table from PDF. So whatever you would like to name you can name it so I've named it extracting table from PDF. The first Excel let us start with importing the Camelot package. In this case, I'm importing the Camelot package with an alias which is cm which will help us easily call that evoke that package. So let us go here and say Shift Enter. On the Jupyter Notebook successfully installed, you get this thing probably let's say if you have made a mistake instead of Camelot, we have said cam as cm. Now, you will get an error that this module is not found, because there is no such package cam in this particular Python environment. So once we successfully invoke or call that package, we will not get any error, but the package has been imported. So next step is for us to see what are all the files available in the environment. So we see these are the files that we have in that environment. So we have the PDF file that is available in the current folder. We have the CSV and XLS, which I just executed before the project and I have it in place. And then we have a bunch of other files. So now let us go ahead and then import the file. There are two ways that you can read PDF one, you can read it directly from web like from where we have downloaded the PDF, or you can read it from your local machine. So the first argument is you read you give the Python the PDF file name the second argument In this flavor, which is there are two ways Camelot can parse your PDF file one is called stream. The other one is called a dice mod. These have different variety of ways to how to parse a PDF file. And in this particular case, we will prefer lattes. And then we are explicitly telling Camelot that we have two pages one is page one, the second one is page two. So we are going to use the function named read underscore PDF from Camelot and then we are going to read the PDF file. So let us execute this shift enter. Once we execute this thing, we are writing it in the Python object input underscore PDF. So we can see that this has been successfully executed without any error. So now, let us see what is inside that input underscore PDF. So this gives you a table list object with four values inside it, which means there are four tables that has been extracted from this function from this video. And this has been put inside this into input underscore PDF as a table file. So for us to know what are the individual properties individual dimensions of this particular PDF extraction process, we will see four n in input underscore PDF, so we are iterating in through input underscore PDF to see what is inside it. So let us go ahead and execute this thing. So this shows that we have four PDF table extracted files, which is first one is the table is with the dimension four by three, which means four rows, three columns, second known as 15 by three, the third one is 14 by four, and then the fourth one is 13 by three, so our area of interest is the last part of the first page, as you saw in the previous PDF display. So we are going to say okay, I want this third one. Since Python is a zero indexed language, we will say two input underscore PDF of two. And then we are saying Do you mean it as a data frame. So once we write this thing, this is what we get. As you can see, this is our area of interest, which is the literacy rate, and which is the index value 1112, and 13. So what we are going to do now is we are going to say okay, I want input underscore PDF of two, and then from that I want the data frame. And in that give me the location leaven to 14 and then give me three columns, which is 123. So I want 123 11 1213. That is what we are specifying here. Once we execute this thing, we can see how the different looks like. So this is how the extractor table looks like. So let us do a little bit of table formatting. Before we do table formatting. We have understood that this data frame that Camelot gives us as part of a panda's data frame. So let us have a little bit of understanding of pandas. pandas is the data manipulation package that is widely used the most widely used for Python, and pandas helps you read a CSV, write a CSV, read an Excel write in Excel, do a little bit of reformatting in case if you want to, you know, do a little bit of data analysis, pandas will help you do data preparation data pre processing. So like that, what we are going to do is we are going to use a panda's function which is reset index to drop this index value 1112 and three, and then come up with our own index value, which is by default 0123. So let us go ahead and say, okay, for this data frame, dot t does reset underscore index drop is equal to true and then we'll assign it to the data frame here. So let us go ahead and execute this thing. And then let's see how the output looks like. Now as you can see, from 11 1213, it has become 012. Now as you can see, there are three columns, but the column names are 123, which is not very intuitive if you want to write a table. So what we'll do is we'll manually put the table name. So from the table, you can you can see that this is 2001 This is 2011. And these are the KPIs that we are interested in. So what we'll do is we'll say Okay, the first column is Kp, second is 2,001/3 is 2011. Let us execute this thing. And then let us see how the output looks like. So from one to three, now it has become kPa to those n one and two those 11 then the next step is for us to do any kind of data analysis with this thing, we need to convert this one which is actually a string into a number format. And the number format that we are going to use is float because this is a decimal point. So from string, we are going to save for 2001 and 2011. convert everything to float. So once we do this thing, we are reassigning it to the same old data frame, and then the output even though it looks same internally from string, it has become a character. The next step is for us to write the output as a CSV file. So because this file name is already available, we'll say Pac output dot CSV. So once we have written the CSV, we can go ahead and See using ls command to see how the current working directory looks like. So as you can see, we have packed underscore output dot CSV, which is what we just wrote using pandas function, which is to underscore CSV. Once we are done with this thing, I would like to add another information that pandas is not just letting you write it as a CSV, but it can help you write it as an Excel file. So let us go ahead and then use this function to underscore Excel on this panda's data frame to write it as an excellent, let's say, packed output Excel with the extension dot x LS x. And then once we execute this thing, let us see how the current directory looks like. So in the previous setup, you had only packed underscore output dot CSV. But now you can also see that at underscore output underscore Excel dot XLS x. So this is how the Excel file looks like. So as we write it, so now what we can do is we would like to import this in our current working directory in this current Python session, so that we can do some data analysis. So our objective in this project is to read table and then write that output table as a CSV, which we have already achieved. But as a bonus, I would like to also show you why we would be requiring such a data frame in the first place, because we want to do some data analysis, some data visualization from the PDF file, which we cannot do it directly. So we are extracting the table from the PDF as a CSV as a data frame, which is then we are converting it as a CSV. And then we are doing some data analysis with this. In this case, we'll build a bar graph with this. So let us go ahead and call our panda's data frame, which is required for us to read the CSV. So as we call the panda's data frame, we'll say PD dot read underscore CSV. And what is the name of the file that we wrote. It is packed underscore output dot CSV. So we'll replace this name with this name. And then we'll say read underscore CSV. And then we are assigning it to a Python object which is the DF two and then we'll display how it looks like. So once we execute this thing. We can see this is how it looks like with an index value because we just read it, then we'll call a data visualization library c one c one is one of the most widely used data visualization library. Seaborn is actually built on matplotlib for better visualizations. So we'll go ahead and call Seabourn with an area's SNS once we are done with that, executed, Seabourn is now imported. So, for us to build a data visualization, we have to change the format the shape of how the data frame looks like to a different shape. So, we are going to use the pandas function melt, which will convert this data frame from the wide format to a long format. So, this is called to be a wide format. Now, we are going to convert it to a long format. Now, let us execute this thing once we execute df underscore melt is available now, let us see how df underscore meant is available. So as you can see, this is a wide format. Now, this is a long format frame where 2001 and 2011 from being the column name that has become the row value and the value that we have given us here and percentage. Now the DF underscore melted is available, let us go ahead and then make a bar plot. So we are going to use acns dot bar plot for making a bar plot the x axis should contain the Kp value, the y axis should contain the percentage and the Q which is the grouping variable, which is 2011 and 2001. For us to compare how it has been different for 2011 and in 2001. Let us execute this thing. As you can see, now it has generated a plot with two bars, the blue color represent to version one, the orange color represents 2011. And then with three KPIs that we just built, literacy rate male literacy rate or female literacy rate. So this is how the overall literacy rate and as you can see the gap between 2001 and 2011 female literacy rate is huge, which means there has been a tremendous growth between 2001 and 2011 in the rate of female literacy rate. So this is what we have understood from this project that we had a raw PDF file which was unreadable as it is not a structured information. What we have done is we have used Camelot to read the PDF, which is to be technical, we parse the PDF, we extracted tables, specifically we extracted four tables. We went to the table of our interest, which is index 1112 and three, and then we did a little bit of data pre processing using pandas. Once we did the data pre processing, we went ahead and then we wrote the data frame. into a CSV file. And we also experimented with writing it as an Excel file. Once we were done with this Excel file, we finally went and did a little bit of data pre processing, which is reshaping the data from a wide format to long format. And then finally, we explored the data as a data visualization to understand some valuable insight from the PDF that we have returned show for. In this video tutorial, we learned how to build a table extracted from PDF. So we started with understanding the PDF file formats. Then we went ahead and installed Camelot and Jupyter Notebook Python packages. Then we understood how to extract PDF table. And then we saw basics of panda's data frame to write and read CSV. And then we went ahead with seaborne to do some visualization. So at the end of this project, we have a successful visualization, we have the output table as CSV and Excel. And then we also learned how to extract table from any PDF. So thank you for listening. I'll see you in the next section. In this section, we'll learn how to build an automated bulk resume parser. Going through resumes, and extracting relevant information from those resumes is one of the most essential tasks a manager has to go through before hiring new resources. In this section, we'll learn how to build an automated bulk resume a parser that can go through multiple resumes and extract relevant information from them, and convert them into a structured tabular format. with the click of a button. We'll start the section by understanding different formats of resumes, and marking relevant information that we would like to extract a brief overview of packages and the installation of those packages. Then we'll see the basics of regular expression in Python, and also the basic overview of spacey functions. And then we'll move on to build the code to extract relevant information. And then finally, we'll complete the script. To make it one click command level tool. Let us go ahead and then see the sections. In this video, we'll learn the different formats of resumes. And then we'll mark essential information that we would like to extract in this project. So as you can see, on my screen, I've got two different types of resumes. The first one is a single column, which has content one by one. And then the second one is a double column, which means in one page, they have got two columns, and then the experiences and other details are scattered across the columns. So as you can see, a resume can have me multiple types. So it is up to the creator of the resume, essentially the one who is seeking for a job to have the format that he or she likes. But it is essential for the recruiting manager to completely go through the resume to extract essential parts of it. So what do we mean by essential parts? The first one is I would say that the name, the name, to whom this resume belongs is the most essential part because if you ever want to shortlist this resume, you need to understand who is that person. And then the second thing is, if you ever want to shortlist a resume, you just do not want to know their name, but also you need to be able to contact them. And then the two key information to contact a person, one their email ID, and then the second one their phone number. So as you can see in this resume, the name is first of all mentioned in the top left, but in this regime as the name is in the center position. And in this resume, the email id is in the top right, but in this resume, it is all centrally aligned. So as you can see, we have totally listed down three elements. The first one is name, the second one is email id. And then the third one is a phone number. These are the three information significant information that we would like to extract from this resume. But more than this, what we want to do is we want to have a criteria for which we want to extract this resume, which means for example, let us say you are recruiting for a position called data scientist. And for a position called data scientists you need to have relevant resume is who have the essential skills of a data scientist and that is the most important information that we would like to see in the resume extraction project. So for that purpose, we are going to extract skills from this resume, especially to say technical skills. So the things that we are going to extract is most importantly the technical skills from the resume, then the name, phone number, email id irrespective of how or where these informations are present in a particular resume. We are going to extract this information using this particular project. Before we move on, we have to also understand one more thing. That resume itself is a file and then the file could have multiple formats. For example, a resume could be a simple image of JPEG or PNG The resume a could be a doc exe, which is Microsoft Word, or the resume could be of PDF. In this particular project, we are going to only deal with the resumes of PDF type. Because once we have written a script for PDF file format, it is not very tough for you to convert every other format into a PDF format. Let's say you can convert a JPEG to a PDF format, you can convert a docx to a PDF format. So, that is one of the reasons why we have picked PDF format as one condition where we will build this project upon so PDF is the file format that we are going to use. And also that we are going to use resumes of different types it could be single or double column. And then the information is that we are going to extract a skill name, email id and phone number. In the next video, we'll see the architecture of this project, and then the required Python modules and how to install those Python modules. In this video, we'll see the architectural overview of this project. So in this project will take three PDF files shall resume is in three PDF files. And we will store it in our local folder. So what we are going to do is we are going to take one PDF file from this folder, and then we'll convert this PDF into text. And then we'll do natural language processing and pattern matching to extract relevant information that are required the relevant information that we saw in the previous video, which are name, email id, phone number, and skills. And then we'll use these relevant information to populate a structured tabular format. And then finally, we'll write the output in a CSV format. Meanwhile, while we are doing PDF to text, we also would save those PDF files as text files for future reference. We'll iterate this process until all the files in the current directory which is the folder are completed. Sure to repeat, we are going to take PDF files, convert them into PDF to text, then do natural language processing and pattern matching. And then we are going to populate them in a structured tabular format. And then we are going to write it in a CSV. So for this purpose, these are the packages that we are going to use. The first one is PDF minor for PDF to text, then spacey for natural language processing, then our E rejects package that is for pattern matching, and pandas per output CSV saving. Meanwhile, we also would use another package called ODS. This is for operating system manipulation, which is highly required for us to iterate through multiple files and in the current working directory, and also to save the output files in the required folder. So the packages that we will be using are PDF minor, spacey, r, e, OAS, and pandas. So of all these five packages, packages, ie underwears come by default installed on your Python operating system. So of all these five packages, packages, Ari and OAS are already installed by default in your Python environment. So it is required for us to install PDF minor, spacey, and pandas. As we just saw, of the five required Python packages. Two packages come by default with your Python installation. Those are our UI, and OAS. So we'll go ahead and see how to install the rest of the packages which is PDF minor, spacey, and pandas to start with PDF minor is a package that we are going to use to convert a PDF into text. And the package that we are going to use is called PDF minor dot six for that reason is because PDF minor is the actual package name whose development stopped before python 2.7. So there was a requirement to support the latest version of Python. And that is where this fork as you can see, this is a fork of PDF minor. And this is called PDF minor dot six. So anytime you're going to deal with PDF, and then you want the PDF to be converted into a text format, and then you have got Python version which is latest, mostly three, then you have to install PDF miner dot six, not PDF miner. So as we have always seen how to install a Python package in this project also will use a terminal or shell or Command Prompt. And then we'll use PIP to install the required Python package. So let us open our terminal or in case if you have got windows mission command prompt and type PIP three, install PDF minor dot six. So as I've got already this package on my machine, this is successfully shown that it has been satisfied. Now let us open our Python terminal to see if PDF Minor has been successfully installed. So let us try import PDF minor. You can notice this difference that when we install the package, it is called PDF minor dot six. But when we import it it is as same as PDF minor. So the only reason they have got this dot six is to differentiate between the older version of PDF minor and the newer version of PDF minor. So let us type PDF, import PDF minor and then press enter and then you can see that it has been successfully imported without any error, which means PDF minor has been successfully imported. The next Python package that we would like to install is spacey. spacey is the library that we are going to use for natural language processing. In fact, spacey is one of the most popular natural language processing libraries in Python and it is widely used in the industry for natural language processing. There are a lot of features of spacey for example, the tokenization is very good, it has got a good named entity recognition, it has got a very good language support like 49 languages are supported. It also comes with pre trained models, which helps us do a lot of natural language processing without training our own model. So let us go ahead and install spacey. So to install spacey, we can see that again we will use PIP and then we'll do install spacey. And if you remember one of the things that I've told from our previous sections, we use PIP three because our Python version is Python three in case if you have got Python you have to use the so PIP three install spacey. So as you can see requirement is already satisfied because I've got this package already. So it has been successfully installed. Let us clear our terminal and then open Python three to see if spacey has been successfully installed. So Python three import spacey. spacey has been successfully imported, let us exit. But there is one more thing that we have to do with respect to spacey, which is download the language model. So a natural language processing can work better only if the language model is available, which usually has got all these words and then the part of speech the named entity recognition all this stuff. So for us to download the default language model English language model for spacey, we have to use this command, which we'll put in our terminal. And because we have got Python three, we'll do Python three spacey download. So, once this is successfully done, our language model is successfully installed. So we can see that the language model has been installed. And then it is also telling us that the way we have loaded our language models, spacey dot load the language model. So let us just check that once. Let us open our Python terminal and then say import spacey and then copy this and then say okay, my NLP is equal to Yeah, the language model has been successfully loaded, which means we have got successfully installed spacey and that language model that is required for us to do English natural language processing. And then the final library that we are going to install is pandas. pandas is the library, the go to library for any tabular data manipulation, and pandas is one of the most widely used library in data science for data manipulation. So it is much easier for us to install pandas. As the name suggests, we'll do let's clear our terminal to three installed pandas. We can see that the requirement has been satisfied. Let us open our Python terminal unsay input pandas. pandas has been successfully imported without any error, which means we have successfully installed the packages that are required for us, which is PDF minor, and then spacey, and then pandas. But we also saw that there are other two libraries that comes by default in the current Python installation that we have got. So let us just verify whether those packages are available. So the first package that we saw was our E, which is for regular Express manipulation. So we'll do import IE. Yep, that's been successfully imported. And then the next package that we saw was was for operating system manipulation, to you know, find files, pass through folders. So we'll do input was, and as we can see, these two libraries are successfully imported, which means we are all set with all the libraries that are required for us to proceed with this project. And then we'll see how to code. In the next section. We'll see basics of regular expression and natural language processing overview and then we'll move on to coding. Thank you. In this video, we'll learn basics of regular expression. Regular Expression could be also called as regex or red X or reg x. So whatever you would like to call it is a sequence of characters that define a search pattern. That search pattern is usually the combination of characters and meta characters. So the meta characters are like this cap, dollar dot pipe or curly braces. So these meta characters define the syntax around characters to create the search pattern, which is what we call as regular expression. regular expression is one of the toughest programming concepts that is not very familiar. So there are a lot of names around regular expressions on internet, you can just refer to it to see how tough it is. So just to understand regular expression, we'll go through a little bit basics, but regular expression is so old that anytime you Google for regular expression, you would find the answer for which you're searching for. So, the basics of expression is primarily to understand the meta characters, the meta character pipe, the pipe operator defines a Boolean or so, if you want to compare between two words, whether it is gi AY or GY, then you would use a pipe operator in your regular expression. So, then parenthesis, let us assume that you do not want to use individual different words, but you want to just use or for one particular character, then you would use gr, open parenthesis a pipe e close parenthesis, which is to say I want gr and then it could be either A or D and then I want to wipe So, this is how my pattern should look like. And this is what we call a grouping regular expression metacharacters also has something called quantification, which is to say, how many or how the number of elements or tokens should be present in the pattern that I would like to see. For example, the question mark indicates zero or one occurrence of the preceding element, which means let us say you have got color CEO elbow you question mark and our which means it could match both CEO elbow our our CFO elbow, you our while question mark is about one Li one occurrence as strict which is star is about multiple occurrences. Which means if you say a B star C, it could be either a, b, c, or it could be either ABC, or it could be either ABC or so on. So it doesn't strictly count only one occurrence of the preceding character. But it actually has n number of preceding characters that you could have. But the condition is that you have to have a and b and c. So this is the basic concept of regular expression. But you could actually refer more online to understand more of regular expression. But for us to proceed with this project, I think this is good enough for us to see how to build a basic regular expression, or at least to understand the regular expression that we might get from internet. So now, from the basics of regular expression. As such, we'll move forward to see how these regular expressions could be used in Python for us to do pattern matching. And as we saw in the previous video, the package that we are going to use for regular expression in Python is R E. So Ari has these major functions match search, find all split, sub and compile. So all of these you will see compile is something that would be mostly used to create a regular expression, and then compile it, and then use that compiled regular expression to find and search for everything else. And match is when you would use if you want to match the first word and search, you don't care about whether it is first or second word, but you will want to find it anywhere it isn't. And find all again it has its own purpose. So let us open our terminal and then see a little bit of regular expression, how we would like to do it. So you can open either your terminal or your Python console on pi charm or Jupyter Notebook. But for simplicity, I'm just invoking Python from my terminal, then say import IE. And let us create a sample text. Okay, my text is going to say best Python course this is my text. And now I would like to see where the word based if the word based occurs in my text, so first I'm going to say re dot match of paste, which is my search pattern here, in this case, regular expression. Comma, I'm going to save my text and then enter. So as you can see, it has replied with a real object, understand zero to four which means I've got the word best. And the match is best. But let's say we want to test it with two different words either best or let's say good match but we are going to use pipe operator to say good so it says the matches best because the text is saying We have got best Python course, let us create a second text, which is text to. And in this case, we'll say good Python course. And then we'll use the same regular expression that we used before which is best pipe good, which means either best or good and then say text to in this case, it says that it has matched with good, and then it is the same space. Now what we'll do is instead of using match in this will use good best Python course aspects too. And then instead of using match, we'll use search to see what output it gives us. As you can see, it has matched good, because we have got either based or good and then index two and the position is good based Python course, let us assume now we want to see both both of these in the given text. So now we'll say T dot find all which will result us both good and best. So, in this case, we wanted to see whether both these words are present and then because both these words are present, it is returning us both good and best that it is present in the text. And in case if the pattern that we have given the regular expression that we have given us, let's say something like this, and then it is not available and then we will not get any result which is we have got an empty list. So these are the main functions that we are going to use in this particular project. So this is a very brief introduction of regular expression, and also how to use regular expression in Python for pattern matching. In the next video, we will see very basics of natural language processing using spacey. In this video, we will learn basics of natural language processing using spacey. So let us open our Jupyter Notebook and create a new Jupyter Notebook. So you can do this by going to File and then new notebook and select Python three once you have this new notebook. Let us now see a little bit about spacey. So as we saw in the previous video, spacey is one of the most popular used natural language processing library. So we'll import the library using import spacey. Once that is done, the very first step, the next step that we are supposed to do is load our language model. So if you remember after installing spacey library, we downloaded the English language model. So if you remember, after installing spacey Python library, we actually downloaded the English language model because a language model is essential for a lot of natural language processing tasks. So, we are going to load the language model that we downloaded in the previous section. So we will use spacey dot load and then put the name of the language model that you have downloaded. For example, let us assume that you have downloaded a language model for a different language like German, so you will have a different name instead of E. And so in that case, put the language model name that you have downloaded. And then as I need to NLP. So once this is successfully executed, now what you can do is you can define the text that you have. So the text that I've got is I've got a couple of sentences from Google Wikipedia page for us to explore this package. So I'm going to use this as text and then I'm assigning it to text. So if you print text you will see okay, this is my text. So what are we going to do now is now that the entire thing is ready. So we have imported the package, we have loaded the language model and then the input text on which we have to do natural language processing is ready. So the first step in natural language processing is annotation. So what you're going to do is you're going to let the language model annotate your input text, so that the input text is created in such a way that it knows what is a part of speech tagging in it, it knows where are the named entity recognition, it tokenizers it it creates word vectors it can do a lot of other things. So the first step is use the language model because you loaded the language model and presented to NLP on the text input text that you have got. So the way you do is NLP off text and then you store the result in doc. So once you store the result in dog the first very first step that any natural language processing would require you to do is tokenization what is tokenization tokenization is nothing but splitting the input text based on token so for example, so in this case what we are going to do is we are going to split the entire text word by word so word is the token for us here. So you can do the sentence organization. You can do this even paragraph tokenization but in this case we are going to do word tokenization. So say we have got doc so let us print how the doc looks like once I print doc. I'm not getting anything other than this thing because this is what it is printing, but internally doc has been annotated using the natural language processing light library and language model that we have got. So what are we going to do is we are going to say for token and doc print token so we are going to iterate through doc and then say okay print each and every token by default, which is a word token. So we what we see is, we see one by one word by word. And then okay the word organization has been successfully then, once the word tokenisation has been successfully done, we can do a lot of things. For example, you can build a workload if you want, you can build a unigram or if you want, you can have a bi gram also combining the words. In case let's say you want to visualize the sentence, instead of doing word tokenization, you can do a sentence tokenization. So the opportunities are a lot so you can do anything you want. To move ahead with the spacey library, what we can further do is as we saw, when you annotate your input text using the language model that you have got you get out of speech also part of speeches to say okay, this this word is a noun, this word is a verb, this word is an adjective. So all these things, so what we are going to do now is we are going to say okay, for token and Doc, the same thing that we saw before, what we are saying onely if that token dot POS underscore, which is to say that this is the part of speech, which is noun, then print the token. So print all the tokens, which is known that is what we are going to do here. So you see that you have got founder Money, money, Angel, all these things. It is say instead of now let's say we want to work. So once you exude this, you get was funded. But in case if you want objective, you put a DJ, and then you get all the objectives that you have got. So this is how you extract a particular part of speech, for example, you're doing text summarization, or you're doing any other text technique, you want to extract topic from it. So you're going to only go for nouns and adjectives. So the same way for you to identify a particular part of speech. So this is what you're going to use, iterate through the document. For every token, you're going to say, give me the product speech. So the next important thing or in fact, the most important thing for this particular project is named entity recognition. So what does named entity recognition, named entity recognition is nothing but identifying entity of that particular word. For example, Google is an organization, August 1998 is a date. When you have money dollars, it is money, when you see a person's name, it is a person. So identifying, you know putting a context around the word instead of just simply saying, whether it is a grammatically noun or verb or adverb adjective. So you're trying to say okay, this is what I have identified this entity. So that is what we are calling us named entity recognition. And this is much, much easier to do and spacey, because of the language model that we have got under spacey has made it so easy for us to do it in one single function. So what we are going to do is, we are going to say dot.ns, which will apply the entity recognition on the dog. So in the previous section, you would have seen that we have done it only on dog because we were iterating through the actual words. But in this case, we want to iterate through the entities. So we are saying dog.ns, which is for entities, and then we are iterating over it with entity. And then we are saying okay, print entity dot text, the actual word, and then print entity dot label underscore, which means the label of the entity that has been recognized. So I'm printing the word, and I'm printing the label of the entity, which is organization or data or whatever it is. So once I print this thing, this is what I get. So once I print this thing, so we have entity dot txt, entity dot label underscore, I'm going to print this and once I print this thing, and this is what I get, so I get Google org, or augusztus. August 1998 is a date and Google is an organization, Jeff Bezos CEO of Amazon is a person and then Stanford University is an organization. So this is the way what we are going to do is we are going to take the text and then identify important entities that are present in this text. So this is the basic overview of natural language processing. Using spacey Of course, natural language processing is an emerging area in research and development. And of course, one of the most widely, you know anticipated areas in machine learning and artificial intelligence. So learning natural language processing with just five cells in Jupyter Notebook is completely impossible, but just wanted to give you a flavor of what we might be doing in this course, so that you have some idea of you know how to proceed further or if you want to extend this particular project to include different set of skills, or different variables that we have not captured in this resume parsing project. So you can use this understanding that we have covered in this video to extend this particular project to you know, wider objective. So in this video, we successfully learned the basics of natural language processing using spacey we saw a couple of spacey functions how to load spacey, how to learn natural language processing the library English language library, and then we also saw a little bit about tokenization and named entity recognition. In the next video we will actually start with the coding part of resume parsing project which we are interested In this particular project In this video, we'll learn how to code the actual project of resume parsing. Before we get into coding, there is a little bit of understanding of folder structure is required. So let us see our folders are organized for this particular project, we need to create two different folders that are essential for this particular project. The first one is resume his resume is is the folder where we will have all the input files all the resumes that we have got in this project. So in this project, we have taken three resumes and all those three resumes will be present inside this folder resumes, then the next folder that we would need is output folder, which will have two subfolders. The first one is txt. txt. txt will contain all the text converted format of those resumes. So once the resume a PDF is read, and then converted into a txt that is stored inside this text folder, and then once the entire parsing is done, all this content is converted into a structured tabular format, which is a CSV and then it is stored inside this folder CSV. So first, we need resume is folder where we will have all the PDFs that we want to be parsed. Second, we will have output folder inside output folder, we will have two subfolders. One is txt and then the second one is CSV. Once you are done with creating these folders, the next important file that we need for this particular project is PDF to txt dot p y. So the file that we need is PDF to txt dot p y. And how do we get this file. This is a file that is present inside the PDF miner library. So we need to get this file from the PDF miner library. So let us go ahead and then open the PDF minor PDF minor dot six to be precise PDF minor dot six GitHub repository. And then let us go inside the tool section. So open PDF, miners GitHub repository, and then get into Tools section. So from this tool section, you can see that you have PDF to text dot p y, click this. And then you get RAW file. Once you click the raw file. You have this option when you press Ctrl S, or if you're using Mac Command S or you can use your Firefox to say OK, save pages, which will tell you okay, how do I have to save the file. So once you have this file, saved in your local drive the folder where you have what this project, now we are ready to go further with the coding. So there are three essential things to folders, main folders, output and resumes. And inside output we want txt and CSV to folders. And then we need to get this PDF to text dot p y from the PDF minor GitHub repository and then store it inside the current working directory the current folder where this project is setup. Once this is done, let us go ahead and open our Jupyter Notebook. So until now, what we saw is how the folder structure is organized, and what is the essential file that we need that we source just from PDF miner GitHub repository. Once this is done, let us go ahead and open our Jupyter Notebook. Once you open the Jupyter Notebook, please create a new notebook by just going into file, new notebook Python three. Once you do this thing, your new notebook will be ready in the new notebook as we have a better coding practice in the every project that we have done. Once you create the new notebook by going to File, New notebook Python three, we will have a new notebook. And as a better coding practice, we will start with importing the library that are required and then we'll create functions that are acquired and then finally will invoke those functions and we have got the entire project set up in the Jupyter Notebook. So this is the flow that we are going to follow while creating the Jupyter Notebook. So the first one is we are going to load all the packages that are required for this project. So if you remember from the previous videos that we need five essential packages. The first one is spacey for natural language processing, PDF miner for PDF to text. re r e for rejects, works well as operating system OAS for file manipulation and then finally pandas for CSV tabular format. So pandas for CSV tabular format, once this is done, let us execute this thing. Next we are going to input the file PDF to dot txt dot p y. So next we are going to import PDF to text dot p y. Next we are going to import PDF to text dot p y that we just downloaded from the Google tab repository that we have kept it in our current working directory is a project folder. So what we are going to do is we are going to simply say import PDF to txt. And then we are going to execute this, this is successfully executed, which means we have got the file in the right Drive folder location. So the next task is the first function that we are going to create is for converting PDF to txt, and we'll call this function convert underscore PDF. So this function is going to take one argument. So this function is going to take one argument, which is the file name. So the file name is going to be given inside this function, and then we'll see what is this function is going to do. So the first thing is once we get the file name that is passed into this function, we are going to say okay, split the file name, and then had dot txt. So, imagine you have got a file name, let us say, okay, observe Majid, which is my name, and PDF, this is our typical resume might look like. So what we are trying to do is we are trying to create the output file name. So because when we are going to convert the PDF into text, we also want to save the text in the folder that we just saw. So what we want to do is we want to create an output file name using this input file name. So when we use always dot path dot split text on the input file name, what we get is we get two items just like this. So we get two items. So what we are trying to do is we are going to take the first one because Python is zero index language, we are going to take the first one like this, and then we are saying okay plus text, which uses the new output file name for the text file, and then we are assigning this name to the output file name. So understand that we got the input file name like this, a PDF file, and we are trying to remove that extension dot PDF. And then we are trying to append this new expression that is dot txt. And then this new extension dot txt is appended to this filename, and then this name is getting assigned to this output underscore file name. Once this is done, we also have to define where do we want to save this file. So as we saw just now, in the folder structure that we are going to have an output folder. And inside that output folder, we are going to have all the txt files. So what we are going to write here is we are saying Okay, give me the path, which is output slash txt slash and then this output file name that we just created. So our output file is going to be saved in this file path where it is output slash txt slash, and then the file name output file name with dot txt extension. So this one, we are going to assign it in output underscore file path. The next thing is the PDF to txt that we just imported, it has a main function, and then it takes a couple of arguments. So what we want to do is we want to save this file in that particular location. So we are going to say okay, the arguments that I'm passing is the file name, which I just received, and then the output file name, which is passed down with this argument, hyphen, hyphen, outfile, and then the output file path where it has to be saved. So, this is the function that helps us converting PDF to text and save it in the given location, which is what we create here. So, the output location is created here, output file name is created here. So the input file, which is the dot PDF file is given through this F and then the output file is saved. Once that is done, we would like to present a message to the user saying that the file has been successfully saved this is just for reference. And finally, we are going to return the output file path and then we are reading it as an input file. So what we are doing is in the same function, we are outputting, the read file so that is why we have got open the file name and then dot read which is a function to read any file in Python. So ideally, we could have done it in two lines to say okay, I'm reading the file, and then I'm passing that read object the file object as returning, but in to save space and also for simplicity, which is one of the core philosophies of Python to have simple code, what we are trying to do is we are trying to do it in same line. So the file path which we are reading after opening, so, this is what we are going to return. So, in this function convert underscore PDF, there are five things that we are doing the first thing is we are creating the output file name. Second thing is we are creating the output file path. And then the third thing is we are converting PDF to txt and saving it in the given location which is the output file path. Then we are printing a user message to say that this is successfully done and then finally we are returning the read file opened and read file with To be just saved and then with this, this function is done, let us execute this function. And this is just for sample we don't need I'll delete this cell. Once this is done, now we are going to use spacey to load the language model. So let us load the language model in this line which we just saw, load the language model. Once we have the language model, we are going to create an output file structure. So we saw that the ultimate objective of this project is to capture four important content four important components from the resume and then make it a structured document which is name, phone, email skills, and then these are going to be the four columns that we have in our output tabular format. And for that, we are creating a dictionary a Python dictionary using the curly braces. So if you remember your basics of Python, a Python dictionary is created using curly braces, a Python list is created using your square brackets. So, what we are trying to do is we are trying to create a result dictionary and then we are also creating four placeholder values names, phones, emails skills, and then we are making it a list so that when we extract these information, we can put that particular component the respective component for example, let us assume that we have extracted name from the first resume a secondary email address, so we have three names, and those three names will go into the placeholder list that we have created names, the phone numbers will go into phones, the email ids will go into emails and then the skills will go into skills. So, with this, we are ready with the placeholder the type of output that we would like to have Once this is done, we are getting into the core function which is going to extract the content from the resuming. So with the placeholder output now, we are moving forward to see how to define the function that will do the extraction the core for component extraction for this particular project. Now that we have got the output placeholder in place, which is a dictionary and then couple of lists to put the content inside it extracted content inside it, we are moving forward to do the actual core component extraction part of this particular project. And this function will call it as parse underscore content, which is to parse the content that we have, and the argument is text. So this function receives text, if you remember, what we returned from this function is the Convert PDF function is we opened the file and then we read it, which is a file object and then in this function, we are going to read a text. So that is how we are interlinking those two functions, and we'll see it in the future. So, the first thing that is required for us is to define what skill set that we are expecting to extract from this project. So extract from the resume. So, the first thing that we have to define is what are the set of skills the skill set that we would like to extract from the resuming. So, considering a data science setup, what we are trying to do is we are trying to see okay, I want Python and if you remember your rejects, you might remember that a pipe operator signifies or condition, so Python or Java or SQL or Hadoop or tableau. So, these are the five things that we are trying to extract as skills from a particular resume and we are trying to see whether this particular resume has any of these things or all of these things. The next thing is phone number. So, what we are trying to do is we are trying to create projects, that projects should be able to capture phone number and this phone number rejects has been extracted from this StackOverflow answer. So, I would like to give credit to that answer that has created a regex that can handle multiple different types of phone numbers. So, we saw we want this a skill set. So we are compiling this expression and then saving it in skill set and this is the regex to capture the phone number and then we are compiling it and saving it in for now. And then what we are trying to do is we are trying to take the text that we just got inside this function and then we are trying to do annotation. So if you remember the basics of spacey video The first thing that we have to do is annotation. Once that is done what we are trying to do is we are trying to extract two entities from the annotated the past are the natural language processing text. So the text on which the natural language processing has been done on the text is nothing but the resume content okay. So from the past content or from the extracted text content, now we are trying to do two things. The first thing is extract name. The second thing is E So, totally we want four components, which is name, email, phone number and skill set. And we just saw creating regular expressions for skill set and phone number. And now we are trying to extract name and email id and to start with the name what we are trying to do is entity any named entity recognition has one particular level which is called a person that signifies the name of a person. So, what we are trying to do is we are trying to do if the entity label is person, then give me the entity text and assign it in name. So, this in Python is called a list comprehension, which means, instead of writing a for loop in multiple steps, you can write it in a single step. So, what we are doing is we are doing a list comprehension and then we are saying okay, whenever the entity recognized entity, post label is person, then give me that entity text. And then we are saying okay, what if a resume has multiple names, for example, first, the name of the person would be on the top of the resume, probably their dad's name and mom's name is there. So for that, with that assumption that the person's name is always on top of the resume, you're saying you mean the first name that you detect. So we are trying to take the first name that is detected in the resume and then as any to name. Next, we are also printing the name for us to have some reference whose resume may have passed. Next is we are trying to do email id extraction. So what we are trying to do is we are trying to say okay, for every word, the tokenized word in the document. If the word is like email show spacey has such an attribute, like underscore email, if this word like underscore email, which is returning a boolean value, either true or false. If it is true, then give me the word. And I'm telling, okay, give me the first word, there will be multiple email ids. But I don't care about the second and third email id, at least for this particular project. So what I'm saying is okay, give me the first email id and then store it in email, and then print the email id again, for a reference. Now that we have built the regular expression for skill set and phone number, it is time for us to use the expression that we have compiled and then extract. So there is one thing that we have to notice, we are trying to convert the text enter text into a lowercase before even proceeding further with the regular expression. This is to solve the case issue. So it could be like for example, let's say this is Python, someone could have written Python as capital P y to H and someone could have written SQL as SQL capital or small letter Java could be with J capital. So to solve this, all these issues, we are normalizing all the texts or downgrading all the text to a lower case. And then we are saying okay, rejects find all where you have phone number pattern, and then the string on which you have to find this text, which is what the argument that we got. And then after converting into lowercase, and then we are assigning the output to form after converting it into a string object. So, simply we have the text, we are converting into a lower text, the text is the resume matrix, we are converting everything into lowercase. And then we are trying to find all everywhere in this resume with this regular expression. And then we are converting that result into a string and then as entering into a foreign object. And then the same thing that we are trying to do here is find all the skill set text lower, and then assign it to a skills underscore list because we will have multiple skills one resume could have Python and Java both. So that is why we are calling it as skills underscore list. Also, one thing that we have to notice in one particular resume, we could have multiple instances of Python and Java. For example, let us assume in the technical skill section, someone has mentioned Python, but also while describing the project, they might have used the word Python. So what we would end up getting here is in the skills underscore list, we would end up getting Python two times and we don't want to record Python two times for that matter, because we want to see whether Python is present or not present. So, what we are trying to do is we are trying to say okay, converted to a dictionary, which means of course, it will have only unique elements. And then we are converting that into a string and then assigning it to unique underscore skills underscore list. So, in this entire section, we have defined the regular expression for skill set and for number we have used spacey for annotation and then we have used the annotated text to extract a Persian and then as Enter to name to see whether there is anything like email then as Enter to email and then we have used phone number and skill set regular expression that we just compiled to find everywhere where we have got and then we have got the skills and unique skills. So right now, after this, what we are trying to do is we are trying to append these names in the placeholder list that we just created. So these are the four empty placeholder lists. recreated, so we are trying to append all these that we just extracted in those lists. And then finally, we are trying to print a message or message to the user to say, extraction has been completed successfully. So for a small summary, this is the function, the core function where we are extracting the four components, it takes one argument, which is the text of the resume. And then initially we are compiling the regular expression for skill set and phone number. Next, we are annotating the text document using natural language processing of spacey, and then we are extracting name, and then we are extracting email, and then we are extracting phone number and then we are extracting skills, and then making unique skills out of it. And then we are appending all those values in the placeholder list that we created in the previous cell. And then finally printing our user message to say extraction has been completed. So let us execute this thing and it has been successfully executed. In the next line, what we are trying to do is we are trying to say okay, now I've got two main functions, one to convert all the PDF to text. And also, you know, meanwhile, save that txt file in the particular folder. Second, use the text content and extract whatever I wanted the four components name, phone number, email id skills. And with this set, now what I have to do is, I have to make this project work on multiple files rather than only one PDF file. So that center objective of doing an automated bulk resume a parser, right, so you don't really want to use a code just to pass one Li one resume because a human being Of course, could be better in passing one resume, but assume that you are a manager and then you have got 100 resumes or 200 resumes or 50 resumes. And this is the case, we want to have an automated resume a parser bulk resume a parser and that is exactly what we are trying to do here. So what we are trying to do here is we are trying to list down all the files inside the folder resume. As we just saw, we have three files. Inside resume, we have three resume files, and then we are saying okay, lists down all the files and iterate through each of the file calling it has file. So we will take individual resumes, and we are calling it file. And then we are trying to just validate whether the file name ends with dot PDF. So in the same folder, you could have, let's say, forgotten to convert a docx file into PDF. So what we are trying to do is we are trying to add an extra layer of validation to say, Okay, I want only PDF, because right now this code is built only to function with PDF. And if you have got a docx, you have to manually convert a docx to a PDF or you can find a lot of script online to convert docx to PDF. So we are saying okay, if the file name ends with the dot PDF, then first print the user message that we are reading the file and then read the file. Convert underscore PDF is the function that the function that we just created, invoking that and then the path where we have all the files is this one. So the first file, the first iteration, it will be the name of the first file. In the second iteration, it will be the name of the second file. And in the third iteration, it will be the name of the third file. And as we saw in this convert underscore PDF, we are returning a file object, we opened the file using the path and also we read it. So we are assigning that output inside this object called txt. And then now we are passing this txt to the function that we just built here. So this takes one text object as argument. So we are passing that as the parameter here saying okay, parse underscore content. txt, once we run this, okay, first resume is read. Allison Parker's resume is read. And then the output is saved successfully. Allison Packers, right is the name, email id. So these are all hypothetical names that do not exist. So it's a disclaimer that this is these are all hypothetical resumes, and do not reflect to any living human being. So this is the first resume PDF is red text to successfully saved the exact same message that we printed here. And then we have the name, which we also printed here. And then we print the email ad, which is here. And then we finally say extraction has been successful. So this is first resume. Then this is second resume, reading john domnick. saving it as txt name, email id extraction completed. And then finally, actually miles reading, saving it as text extracting name, email ID, which is what we printed, we didn't print phone number and skills and then extraction complete successfully. So now that we know that we have got three resumes, all those three resumes have been successfully read, what we are going to do is we are going to use these placeholder values which is now populated with all those names. So to give you some perspective, names will have all the names phone We'll have all the phone numbers, skills, we'll have all the skills. And then of course, emails, we'll have all the emails. Now that we have got all the values of this placeholders populated. Now we are going to say okay, assign these into these. So this is a dictionary with it is a key value pair, and this is the key and then we are going to put this against the respective key as values. So what we are doing here is we are saying, okay, the value of this key should be names, the value of key, this key should be phones. and the value of this key email key should be emails. And then finally skills, then to see how does it look, let us execute this, then say result. And that's good, which is the result dictionary. So as you can see, the starts with a curly brace ends with the curly brace, this is the key, and this is all the values, this is the key, this is all the values. So this is how a typical dictionary in Python will look like. And then finally, we have to convert into a tabular format, which we will do using pandas DataFrame. So in this video, we saw how to start with importing libraries define two important functions. One is converting PDF to text. The second one is the code the main function, the it's like the engine of the entire project, which is parsing the content and extracting required components. And then finally storing the required components into a Python dictionary, which in the next video, we'll see how to convert into a tabular format using pandas, and also to save that CSV as an Excel file CSV file. In this video, we'll see how to convert the dictionary that we created into a tabular format using pandas. And we will also see how to then finally save this entire thing as a script to run on a bulk, you know, set of files in a particular folder. So to start with, this is where we left in the previous video where we had created a result underscore dict, which is a dictionary Python dictionary, with all the essential content that we just extracted from the three resumes that we had got in the race image folder. So to move ahead with, we are going to use pandas, which we just imported at the start import pandas as PD is what we used. pd is an alias. So we are going to use that alias PD dot data frame. So the beauty of pandas is that panda's data frame is nothing but a Python dictionary internally. So it is easier for us to convert result underscore date into a data frame just by invoking this function called Data Frame. From this lspd pd dot data frame. We're framus the SS capital D and therefore capital and pass result underscore dict as any to result underscore df df stands for data frame or you can give any name that you would like to and then I'm printing df, which says okay, this is my name. This is my phone number. This is my email. And these are the skills that I've got. So Alison has got it on Tableau and Java. JOHN Dominic has got How do Python and Java actually Myles has got SQL and tableau. And then the next step is for us to save this entire thing into a CSV file. As we saw in the previous video, the folder structure that we've got, where output has two folders subfolders. One is text, which stores all the text files that are converted from PDF, and in the CSV where we have got the output CSV, we are going to say okay, save the output CSV there. And then we can go ahead and open the folder and see that the output CSV is present here. So until now, what we have seen is take the dictionary convert into a tabular format, which is a panda's data frame, and then save that data frame into a CSV file. But this Jupyter Notebook is good for us to prototype. But now, the objective is, we have a folder full of resumes. And then you know, you want to give this to someone who cannot use proper Python coding, and then they should be able to convert all those PDFs into structured content. And for that purpose, we are going to convert this Jupyter Notebook into a Python script and then use the Python script to convert all those PDFs into text. So let us go ahead and then open file, and then go to download as and then do.py. This will give us the Python script. But before we do that, we have to remove those instances that are not required. For example, we have a lot of places where we have printed these things, which is quite unnecessary. So we'll delete these cells. You can use x To delete a cell, or probably you can go here and then try and delete cell. So I'll use x, delete, delete, delete, delete. Cool, and then I'm going to Delete this dictionary. So we are done with this as of now, and we can save and checkpointed just for our reference, then what we can do further is we can go download Python read poi, so we'll save the file, and then make sure that you have got the file inside the folder, the current project folder. So bring that file here, the current project folder, where we have got all the files. So now, you should have the folders that we defined with the resume is with the input files, that resume is with all the PDF regimens that we want to be parsed. And then the output folder with two subfolders, CSV and txt. txt. And then also, the PDF to txt dot p y should be inside this folder. And this is the notebook that we just created to create this entire project. And then finally, the Python script of this Jupyter Notebook. With this, we are good to go that we have got resume underscore passing with this script will use this script on command level as a CLA tool to automate this entire process of converting PDFs into a structured format of valuable content. But before we do that, let us go ahead and then delete these things because these were created when we ran this particular code in the Jupyter Notebook. But to make sure that the project is successful, we'll delete this. And also we'll delete the output file, we have got an output file, we'll delete this output file also. So right now, let us just validate, we have got three resumes, Alison Parker, Ashley Mays, Dominic, john Dominic. And in fact, you can notice that this is all three different formats. This is a double column, this is a single page resume. So we have got different resume formats. And then output does not have anything, output is empty with two subfolders, empty subfolders. And then we are all set to test our script on this the resumes folder. To see the code that we just have created using the Jupyter Notebook the Python script that we just downloaded from the Jupyter Notebook, it could be used for automation, so open the terminal. Or if you are using Windows, please open your command prompt. And then make sure that you are inside the project folder. So my project folder name is resuming parsing, I can use LS to check Okay, or if you are using Windows, check the command to see where you have got your current folder. So you can see that this is my current folder. And then it shows that I've got this file regime underscore parsing in place. They've also got the folders like output resumes, and the main PDF to txt dot p file. So with this, what we can do is we can say resume a underscore parsing dot p y but before we do that, we have to say Python three phase resume underscore parsing.py. So the same name that we have given here to say Python three ratio parsing that py. Once you press enter, we'll see all the details that we have done. So for example, this is exactly what we saw. First reading Allison Parker's the first resume output is txt saved name, email extraction completed second resume at john Dominic, john Dominic txt saved name email id successfully completed and then the third resume actually miles CSV reading PDF saving txt name email ad successfully completed now we can see okay LS nothing has changed. Let us enter into resumes folder. And then we will see okay, LS we can see that there are three files which is what we used as input. Now let us come out you see the space dot dot coming out and then doing LS just to validate where we are and then let us get into output. And then you can see LS, we have two folders, CD txt and then LS we can see that we have got three txt files. So initially before we started with this execution of the script, we did not have these txt files. Now we have this txt file which means the conversion of PDF to txt has been successful. Now let us also validate whether we have got the structured information in the form of CSV. So let us go out cd space dot.we are again inside output. We'll get into CSV and then say okay, we have got the CSV inside CSV, LS. We have got parsed underscore resumes dot CSV, which means the script that we executed has completely done what we did with the notebook which means the script is perfect, that the automation is successful, that it picks all the resumes folder resume. And it converts everything into txt and saves inside txt. And then it also finally creates one CSV file, which is the parsed resumes file. So now let us go ahead and go to output, open CSV now that we find CSV in this folder, let us right click it and open the CSV with Microsoft Excel for us to see, okay, this is Excel, the CSV has been opened, you can see that okay name is their phone number, their email it is their skill is there? And how can you use the output of this project? Let us assume that you are HR manager or you are the recruiting manager, and you want to use the output of this project and you have got, let's say maybe 100 names like that. The way you can use it is, right now you have a requirement for a SQL Developer, SQL Developer, what can you do? You can just go there, apply this filter and say, Okay, I want someone with SQL. What do you get? Let's say you go to the header, let us format this header a little bit. And then you go to the header, and then say filter, and I want someone with SQL, then you see Oh, you've got Ashley miles with SQL skillset. And additionally, Ashley miles also has tableau. So maybe let us pick up Ashley, my email ad and then male and ask if he would be interested in joining our company for an interview or let us assume that you have another requirement where you want someone with Tableau and then in this case, you will go put a filter and say one blue, then you see okay, Alison Parker right. And Ashley Mize has Tableau skill set. And then you know, now let us prioritize that these two resumes for interview and then go ahead and then call them for interview. So this is the main objective of this project. So if you have got a lot of resumes, like 50 resumes, it is nearly impossible for one human being to literally go through all the resumes. But using an automated script, a bulk resume a parser that we have built using this project, what we can do is we can extract the essential skill set that we want. And as you know, you can change the skill set like you wanted in the regex expression that we built, and then filter it using Excel to say, Okay, these are the resumes that I'm going to focus instead of just going randomly with all the 50 resumes. So in this section, we learned how to build an automated bulk resume parser using natural language processing and regular expression. We also alongside learned basics of regular expression and how to implement it in Python. And we also saw introduction to spacey, and natural language processing using spacey. And then once we successfully built the script to completely convert a resume a which is in PDF and an unstructured format into a structured tabular format. We saved it in CSV and we saw how we can use that CSV to prioritize resumes for selecting the right resume for the job requirement. Thank you for watching this video, and we'll see the next section. In this section, we'll learn how to build an image typeconverter. Converting images from one image type to another image type like PNG to JPG JPG to PNG, or BMP to PNG is one of the most wanted tools that every one of us expect to have handy. To build such a tool, we'll start learning with basic image manipulation in Python, then we'll understand what are the Python packages used for image manipulation. And then finally, we'll build a tool that helps us do image type conversion. The section contains following topics, what are the different types of an image what is an image type converter, Introduction to image manipulation in Python, and the Python packages used further. And then finally, we'll build a script project that will help us do image type conversion. In the next video, we'll learn what are the different types of image file formats, and its details. In this video, we'll learn what are the different type of image file formats, and its details. image file formats are standardized means of organizing and storing digital images. The image file format is usually identified by the image file format extension that comes with the file name, and image file format is required to store data in an uncompressed or compressed or vector format. Once rasterized, an image becomes a grid of pixels, each of which has a number of bits to designate its color equal to the color depth of the device displaying it. In general, an image file format defines how data is stored the image data is stored in that particular file format. And image compression can be of two types. lossless compression The other one is a lossy compression. In a lossless compression the image and file format is change the compression is usually lossless, which means there is no information loss in a lossy compression. The algorithm preserves a representation of the original uncompressed image that may appear to be a perfect copy but it is not so often lossy compression able to achieve smaller file sizes than lossless compression and it is highly preferred when you're going to transfer image from one place to another place where you need to compress the image. What are the various different types of image file format, the most widely used and in internet the file format that has been highly preferred to be used for Image Transfer is JPEG. jpg stands for joint photographic experts group, which is a lossy compression which means JPEG is a compression algorithm that stores image data in a compressed format. After JPEG on of the highly preferred file format is PNG. png stands for Portable Network graphics file format. png was originally created as an alternative for GIF, or GIF, however you want to share it give stands for graphics Interchange Format. In the next video, we'll see what are the different Python packages that we'll be using in this particular project. In this video will learn about the different types of Python packages that we will be using in this particular project. For image manipulation, we are going to use a package called PPI L stands for Python imaging library. Python imaging library is one of the most popular Python package which is free for image manipulation in Python. However, there was no recent support from ti l for any Python version that is greater than three, which means PL supports only Python version that is lesser than 2.7. So for this, someone has formed a friendly fork of PL repository, and that is called pillow pillow now supports any new latest Python version that is greater than three below was created by Alex Clark and its contributors to pillow is the library for image manipulation that we are going to use in this particular project below follows the same syntax as PL you have to make sure in a computer where you have installed below that you do not have bi l below can be installed using PIP which we'll see later on. The next package that we are going to use in this project is globe. globe is simply for Unix style path manipulation. So to identify the files images in our current folder, we are going to use globe. So we are going to use Glo to identify the current image files that is in our current directory. And then we are going to use pillow for converting into from one format to another format. In the next video, we'll learn how to install the required Python packages and loading them into a project. In this video, we'll learn how to install the required Python packages and loading them into our project. As we discussed earlier, the back end that we are going to use is called below. So let us open our terminal. If you are going to use Mac, open your terminal. Or if you're going to use Windows, open your command prompt. And make sure that in your computer, if you have Python three, you're going to use PIP three. So if you're going to have Python three, use PIP three or if you're going to have fight them less than 3.0 use PIP so in my computer, I've got python 3.7. So I'm going to use PIP three, install and below. Make sure that your pee in pillow isn't capital letter and then press enter the installation procedure will start now. So right now, you can see that below has been installed successfully for you to verify a pillow has been installed successfully. You can open Python console here using Python three and then import pillow. The reason that pillow is not found here is because we also saw that pillow follows the same syntax as pa l but it is just a simple folk. But in this case, if you are going to use pillow, you have to just simply input bi l make sure that you have only one version of pillow installed in your machine so that there is no clash among the packages. This is the only package that was required for us to install. The other package that we saw below is already available which also we can verify by using import glow. It has been imported, which means the package is available already by default in python 3.0. In the next video, we'll start with the coding part of creating our image typeconverter script in this video We'll learn how to code or image type converter script to begin with, open your PI charm or any other Python ID that you are going to use for this particular project. Once you open your ID or pi charm in my particular case, go to File and click New to create a new Python file. Once you click New, you'll get all these options and select Python file to create a new Python file, you might have to name the file upfront. So give a meaningful name like image conversion new.pi. For ease of process, I've already created the code and I'll take you through section by section. The first section we are going to load the library that we are going to use in this particular project. As we discussed earlier, you under the package name that we are going to use is called pillow, because it is a fork of p i e l library. So we are going to use p L. So from PL we want to import the class object image. And then we are going to import blob, which is to identify the files with a particular extension. So in this step, we are going to use print globe dot globe. And we are going to use this small regular expression pattern that tells us that anything that starts with anything, and then followed by a dot, and then finally ends with a PNG, which means we are going to tell Python that please give me the list of files that have an extension PNG. To understand this, let us see the current working directory of the current working directory offers. We have three PNG files. As you can see, the first one is Batman logo. The second one is Powerpuff Girls, and the third one is Tom and Jerry dot png. So all these PNG files will be displayed. Once we run this code. The next section, we are going to iterate through the PNG files, each and every file. And then we are going to open the image file and then assign it to a new Python variable called iron. Once we assign it to the new Python variable called Iam, we are going to use this is to apply a method called convert where we are converting into tune to its RGB file format. Our GB stands for red, green, blue, which forms the complete color that we usually see. So in this step, we're going to convert the image that we read, which is a PNG image after getting assigned into a new object, we're going to convert it into its RGB format. Please note that image conversion can also have RGB, but RGBA is not something that we are going to use here for that purpose because JPEG is a file format that cannot retain transparency. A stands for alpha alpha represents transparency. So for a JPEG image, it has only three properties, which is our G B. But for a PNG image, for example, if you're going to convert from a JPG to PNG image, you need to have RGB A, which will convert the image into a new file format along with this attribute alpha, which stands for transparency, which is not required for our current use case. So we will use RGB format and convert the input image. And then finally, while saving the image, we are going to use the same file name, which is what we read. And then we are just going to replace the extension from PNG to jpg. And then it also gives us the flexibility of setting the quality value depending upon how large the images. So if you want the image to be more compressed low quality, if you want to upload it online, then you can reduce the quality which means the size of the image will also be reduced. Because we have given it in a for loop, it is going to happen for all the files that we have got. So let us go ahead and run the code. So to run the code, we can go here and pi charm and then just execute the code. As you can see, the code has completely executed and then it has displayed all the PNG that we had. And then it also has finished with exit code zero. As you might have noticed, initially, when we had opened this finder, the Explorer where we had all the files, we had only PNG file format, but you can see now that we have also created new JPG file format. And to notice the difference when you open Batman Lego file you can actually see that there is no background in there, which means it is completely transparent, which is one of the attributes of a PNG image. But when you actually see the JPEG image, which is this one, you actually see that the entire background has been filled with the black color. That is what happens when we had done this conversion where the attribute alpha has been lost. So in this project, we learned how to import the Python image library, how to find PNG in the current working directory, and then how to convert this image from one file format to another file format with different quality level outcomes. impression level. In the next video, we'll learn how to execute our Python project that we just created using the terminal. In this video, we'll learn how to execute a Python project using terminal or Command Prompt. What we did in the previous project has been imported below package. And then we iterated through all the PNG files in our current directory. And then we converted all those PNG files into a JPEG. But the problem with that in sharing the code is that someone has to have the knowledge of Python to open the text editor, or to open pi charm and then run the code. To avoid that, we can convert this enter code into a Python executable file, which is something that we have already created in the previous project. So the.py file that we had created in the previous project is what we are going to use in our terminal or bash, or shell, a Windows command prompt, and then use that py file to execute and then convert everything within our shell itself. So this way, we can convert the entire project into a single click command line utility. So let us first get into our current working directory where we have got all the files under code. In my case, image type conversion is where I've got image underscore conversion is where I've got my code, and then project file to check that we can use their Linux command ls to see what are all the files in our current directory. So as you can see, we have three PNG files, and we have the Python file that we had created in the previous project. So to execute this file, let us first copy this file name, right click Copy. And once the file name is copied, remember whether you have got Python, or just Python three, which means if you have got Python less than 3.0, you have to use this command Python. Or if you have to, if you have Python version greater than 3.0, you have to use Python three. So in my case, I've got Python version 3.0. So I'm going to use Python three here as the first command. And then I'm going to put my file name which is image underscore conversion dot p y. This command is going to execute image underscore conversion.py, which will iterate through these PNG files and create new JPEG files. So let us see. So as we have executed, in just a microsecond, this nta code has got executed. And then we can see that the pipe PNG files have got listed, and then the code is completely successfully executed. Now let us see whether the new files are there available in the current working directory. So to check that we can use the same command ls, and then see, as you can see, we have new files, which has this extension called JPG and JPEG and jpg. So let us go to the folder where we have got all these files. As you can see, initially, we had only PNG files. And after this execution, we have all these JPG files. To just verify this, once again, we'll delete all these JPG files. And then we'll go back and then check using a list. Yes, there is no JPG files in this current directory. And then we'll run the same command again, to execute the image conversion Python file. I'm going to use Python three because my Python version is greater than 2.7. So once we executed this, we can go to the current working directory, and then see that we have got new JPG files. And as we see last time, this Batman Lego has got no background, which is one of the properties of PNG files we saw but alpha and the JPEG version of the same Batman Lego has got a black color background, which means the JPG file has replaced the transparent background with a black color. So this project, we learned how to create an image type converter using Python using the library pi L, which stands for Python image library. And then we converted that code into a shell script where we can execute in one line that the entire conversion will happen for all the PNG files in our current working directory. Thank you for listening. See you in the next section. In this section, we'll learn how to build an automated new summarizer. The reason we call it an automated new summary is because the machine learning algorithm is doing the summarization technique for us with no manual effort of going through the long text of news. News summarization is nothing but the text summarization of news. We'll start seeing an introduction to text summarizer and its techniques. we'll implement one such text summarization procedure and Python. With that, we would have extracted this summarise text of the news and thus we would have got our automated news summaries are in place. Let us go ahead and start seeing the course section. In this video we will learn about take summarization. Take summarization is the process of extracting meaningful text that is shorter in length from a huge chunk of larger text. Using algorithms powered by natural language processing and machine learning. Take summarization is actually one of the most exciting fields in machine learning and natural language processing, which is NLP. Automated text summarization allows engineers and data scientists to create software's and tools that can quickly find and extract keywords and phrases from documents thus creating a summarized takes take summarizes are implemented in a variety of web applications and mobile applications to deliver summarized content, or news. And the example is there is an app in short, which is one of the most popular news apps in India that delivers a summarized text from a larger use. Take summarization usually are of two types, the technique is of two types. The first one is extraction. And then the second one is abstraction. The extraction technique, the automated system or algorithm that we have built, extracts objects from the entire collection of text without modifying the objects, which means it extracts key phrases from the entire text that we have given and then ranks those sentences based on its importance. And then finally gives a text summarized format of text using only those most important sentences. So in this case, there is no modification of objects that are present inside the actual text that has been provided. The second technique is abstraction, under abstraction, instead of just merely copying the information from the given text. What it does is it actually pair up phrases the entire section. So it takes the entire text, paraphrases the section, and then finally identify key words and then key phrases, and then uses natural language processing to create a new text which is more meaningful, and also covers the context. And then finally, it uses the summarized text of the original text that is given. So there are two techniques. The first one is extraction, which actually returns the entire document without modifying the objects but a minimalistic version of using only important sentences. The second one is it breaks down the entire sentence, multiple objects, and then it builds meaningful sentences using natural language processing by paraphrasing the sections, and then we get the summarize text. In the next video, we'll learn about what kind of technique we are going to use in this particular project, what Python package we are going to use for that purpose, and how to install that Python package to proceed further. Thank you. In the previous video, we learned a bit about text summarization. In this video, we will see what kind of text summarization Are we going to use, and then the Python library required for that. So text summarization is of two types. As we saw in the previous video, one is extraction. The second one is abstraction. So in this particular project, we'll use extraction method for text summarization. And the Python package that we are going to use is called Jensen. So the Jensen implementation of text summarization is based on a popular algorithm called text rank. Text rank algorithm is a graph based ranking model for text processing. An important aspect of text rank is that it does not require deep linguistic knowledge, which means text rank model is highly portable to any other domain or language. Which means if you have built a model using English language, you can use the same algorithm for a different language without any further major changes. Jensen is also known as topic modeling in Python gensim is a pythonic library for topic modeling, document indexing and similarity retrieval for large text corpora. The target audience for Jensen is the one who uses natural language processing and information retrieval or information extraction. gensim is particularly one of the most popular Python libraries especially for topic modeling. gensim is been open sourced by a company called Ray technologist or a ray technologists, let us see how to install gensim on our computer, open your terminal or if you're using Windows, open your command prompt. And then type crypt three as we have seen before, if you're using Python three, you need to type PIP three install if you already have got gensim so use for upgrade Jensen, once you type this on enter gensim would start getting downloaded on your machine as Jensen is related to topic modeling and then much more. There are many pre trained models that comes with gentlemen language models also. So for that purpose gensim is quite heavy and it will take some time to get installed. So as we can see gensim has got installed successfully. So let us clear that up. mil and open our Python console to see if Jensen has been successfully installed. So import gensim Yeah, as you can see gensim has been successfully installed without any error. So we can now exit Jensen. So Jensen is the library that we are going to use for text summarization, automatic summarization. But for us to get the text itself because we are going to do new summarization, we need to extract the text from news which means the news is published on internet and we need to extract text from news and to extract text from news we are going to use something called a beautiful so as we have seen in the previous sections beautifulsoup is the library that we are going to use for extracting text from internet. As we can see, it could be from HTML or XML. So Beautiful Soup is the library that we are going to use for web scraping, which is to extract text from internet to install beautifulsoup we need to type pip install Beautiful Soup for so let us once again open our terminal or shell and clear the text that we have gotten there and then installed beautiful so PIP three install Beautiful Soup work because it is the latest version of Beautiful Soup. So we have to type Beautiful Soup for so as you can see here the terminal that I have already got Beautiful Soup installed in the previous section. So my Beautiful Soup requirement has been already satisfied. If you have not got beautiful soup, Beautiful Soup would get freshly installed on your machine. Let us open our Python console. And then try to see if we have got Beautiful Soup installed. So let us save from bs for import Beautiful Soup, which is the object of our interest. So as you can see beautifulsoup the object from the package beautiful. So for it's been successfully imported. So remember this always when we install Beautiful Soup, it should be installed as Beautiful Soup for all and small critters. But when you are importing Beautiful Soup, it is from this package bs four and then you input the object beautiful. So let us exhibit evil soup. So in this video, we learned a little bit about the kind of text summarization technique that we are going to use in this project. The package name Jensen that we are going to use in this project, how to install Jensen, and then a little bit about Jensen and we also saw that we need beautifulsoup the Python package beautifulsoup for web scraping, which is to extract text from the web, you are the news that we are going to be of our interest. In the next video, we'll learn how to extract the news text from the internet using beautifulsoup. In this video, we'll see how to use Beautiful Soup the Python package Beautiful Soup to extract text from the internet news source. So to begin with, let us import the packages that we want for text extraction. The first one is beautiful. So as we know before Beautiful Soup is a web scraping library that we are going to use to extract text from a new source. The second package that we are going to use is called request. The request package is used to extract a web content Beautiful Soup is used to parse the text content that we extracted using a request package. So after we import the packages from bs four input Beautiful Soup from request input gate, so the reason why we are importing a particular object or a function from a package instead of loading the entire package is because memory management if we have got a huge package like Jensen, and if we import the entire package, then the primary memory will be occupied with a huge memory chunk of this particular package. So it is always better to import only the packages only the functions only the objects that are our interest rather than importing the entire package. So likewise, Beautiful Soup object is of our interest and in the function request that we wanted from the request package. So once we do that thing, let us create a custom function. The purpose of this function is to extract one Li the text component, so extract only the text component from paragraph text. So in a typical website, a web URL you will see text spread across different tags. It could be in a span tag, it could be in a h1 tag h2 tag h3 tag, or it could be in a div, it could be anywhere. So the tag that is of our interest is paragraph tag, which is a HTML tag which is denoted by this symbol P. So what we are going to do is we are going to create a custom function. The first step in the function is the function is reading the URL. So the function is trying to get a sense HTTP request to get the URL using request package and then it stores it in page. Once it stores it in page we are going to use beautifulsoup to parse the content Using a XML parser. So we are using l XML parser to parse the content that we just extracted using the get function and then we are storing it in soup. So next step is we are going to identify why only the paragraph text, and then we are going to save it in text. The reason we are using lambda function is to iterate these four different tags. The reason is, because in a particular page, there could be different paragraph text. So we are using soup dot find all to find all the paragraph text and then we are using lambda function to iterate this page or text to all this paragraph function that we found out. And then we are finally using join to join all the text and store it in a text object. The next thing is, it is always good to present the title. So what we are going to do is, apart from the text that we extracted, we are going to extract the title also, the reason we are using soup, low title, dots, strings is because the title might sometimes come with escape strings like slash in the denotes end of line or slash tab that denotes a tab space. So to strip out those strings, we are using soup dot title dot strip strings. And then again, we are using join to join all the words that we have got and then store it in title. And then finally, we are returning this as title comma text, which will be read as a tupple. So finally, we are sending it as a tupple. And then the custom function that we wanted to create is done, the function name is get underscore onely underscore txt, and then the argument that we are passing is the URL that we want to extract the text from. So once this function is done, let us execute this. And then the next step is the news URL from which we are going to extract the text. This is the URL from where we are extracting the News, the news is from a very popular media publishing site called works. The news title is California is cracking down on the gig economy. And then this is what we are going to use to extract and summarize text from once we take this URL, and then we put this URL within the function that we just created. So you can see, the function that we created is called get underscore only underscore text. And then we are passing this URL as a character or string, the string argument is passed on to this URL. And let us execute this URL. As you can see, it just finished executing. And let us print the text object to see if it has successfully scraped extracted the text. So you can see that the text is available now. And let us just see how large this text is. In other words, how many number of words we have got. So once we execute this thing, the way we find it out is we take the string, and then we split the strings using str dot split as words and then we are using the length of the words to see how many words we have got. So you might doubt Why have I used text one instead of just text. So as we created the function before I mentioned that we are returning it as a tupple. A tupple is a different type of Python object from a list. So a tupple is an immutable object. So the reason we have returned it as a tupple is because we wanted to return title and takes in the same expression. So that is why we have returned both. So if you see text length of text, it will show you two which means it is a tupple with two objects in it. So you can also see it opens with an open bracket and it ends with a close bracket as opposed to a list which would open with a square bracket and close with the square bracket. And that is the reason why we are excluding this because text of zero would be the title and text of one is the actual text that we want. So text of zero would be the title and the text of one is what we actually wanted. And that is why we are using text of one to use string split and then split it by words and then calculate the number of words entered. So in this video, we learned how to use beautifulsoup and request to extract the text from the new source. And we also saw what is the length of the text the extracted text which is 1600 25. In the next video, we will learn how to do text summarization using Jensen. In this video, we'll see how to do text summarization automated text summarization of that extracted text that we did it in the previous video. So as a first step, we need to import all the required Python packages. The Python package that we are going to use here is Jensen as we know before and within gensim. We are interested of two functions one is summarize the second one is keywords. So from Jensen, we are going to import summarize and keywords. So Say do from Jensen dot summarization towards summarizer import summarize, and then from Jensen dot summarization import keywords once you have typed this in execute your Jupyter notebooks. So once this is executed, your Python package has been successfully imported. So the next step is for us to do text summarization, text summarization, as it might look very complicated gensim is offering us this in a single function. So, how we can do this is the first one is the function called summarize, and then we have to pass the text to this. So, the text that we have extracted in the previous step as we saw as a tupple. So, what we have to do is we have to do text dot one, and then the summarize function as a first argument it takes text the actual text content as a second argument it actually takes ratio. And then the third argument is word count. The thing with this is it can be either done with ratio or with the word God not with both. So, if you supply both with means ratio, let us say ratio is equal to point 01. So, the ratio is nothing but the ratio of text summarised text that you want, as opposed to the original text. So, in text summarization, we use summaries as a function, we can either use ratio or word count as argument to extract to limit the amount of text that we wanted. So, let us start with the first method, which we'll use using word count. So, using summarize function, we are passing the text which is of text one, and then we are using the word count that I want us 100 which means I do not want more than 100. So, in the previous video, we saw that we have totally 1600 25 words, and then we are going to extract only 100 words out of it, which is meaningful as summarized text. So, to make it a little bit, the cosmetic changes to the output, we are going to say okay, this is the title and then we are using text of zero to print the title and then we are saying this is somebody and then this embrace text. Once we execute this thing, we see that we have got the title of walks. And then we are getting the summarized text which says okay, the State Assembly has passed a bill which makes it harder for companies to label worker as an independent contractor instead of employees, which is what usually happens in gig economy. So, as a first step, we have extracted our text and the number of words that we have got is let us say, is 98. So we limited our word count to 100. And then we have got 98 words. So this is the first method where we have extracted the text using word count as an argument to put a threshold. In the second method, what we are going to do is we are going to say I do not want to put a threshold of word count, what I want is I'm giving a ratio, the ratio can be anything between zero and one. And then within this, it will use as an approximation as the ratio of the original text which is 1600 25. And then given this ratio, it will give us how much text that we are going to be given. So what we are going to do is we are going to use the same function, text off one because we just wanted the text and then we are going to say ratio of point one and then see how much are we getting. So you can see that you've got the title, and then you've got the text. If you want to reduce this text further, we can say instead of point 01, we can say point zero, let's say 7.07, which further reduces their takes. If you are interested in reading more text instead of point 01, we can say point two which is giving us more text point one giving us less text. So as you can see that we have extracted the summary using a different method other than specifying the word count. But still it is good for us to see how much it has reduced. So what we'll do is we'll copy this function, which is where we have got the output text. And then we'll say we are going to put it as summarized. Text is equal to this is an we'll execute this once we execute this, we have got the summary sticks. Now, we are going to use the summarize text and then put it inside our text length function to see how many words we have got we have got 217 words. In the previous muttered we got 98 words because we had set a threshold using the word count. In this method, we have set a threshold using the ratio and then we have got 217 words. So this is how we have extracted summaries text, but sometimes not just summarize text is enough but you also want to see the keywords that are more important that the algorithm has found out. So what we are going to do in this step is we are going to extract the keywords that is of more importance. And we are going to use the same method. So, we are going to say keywords, which is the function name for the first argument is the text that we are passing the original takes. The second argument is we are setting a threshold using the ratio. And then the third argument says whether you want to do lemmatization or not. So, to understand what is lemmatization So, let us first exude this function without lemmatization function. So, once you exude this thing you might see okay the critical keywords are drivers, code codes, workers, worker states state contractors contractor So, even though it gives us keywords, sometimes you might see the reputation one has quote unquote, state and states contractors and contract This is because we have not done lemmatization lemmatization is a process of taking a word and then converting it to a root word. For example, workers would be converted into worker coats would be converted into code contractors would be converted into contractor which means we would not see duplication because of that just one extra word. So, let us go ahead and then do lemmatization Lemma ties through, it's a Boolean flag. Once we execute this thing, now, we have got the new set of keywords, which is just quotes Ober, contractor, business worker, and there is no plural form and singular form because we have done lemmatization we can do this for that other method also. So in this video, we have learned how to successfully do automated text summarization using the Jensen function summarize and also we learned how to do it using two different methods. One is using ratio, the second one is using word count. Apart from this, we also learned how to do Tech's keyword extraction. So one is text summarization. The next one is keyword extraction. To find out more relevant or important keywords, for example, if you are going to run a Google AdWords campaign for this particular text, then you need to understand what are the keywords critical keywords that are presented in this particular word, text, and then we'll run the campaign we'll do the bidding accordingly. So in the next video, we'll learn how to make this a complete project and then we'll see a summary. In this video, we'll see how you can take this further forward. So, as you can see, what we have done is we have used a Jupyter Notebook to create this particular project. So Jupyter Notebook is good if we want to incorporate text, which is narrative as markdown and also the code and also the output. But Jupyter Notebook is not applicable for every purpose. So for example, let us take that you want to do this project as an automation. So what you want to do is you want to take a particular URL, and then you want to shedule this in your computer to get the summarized text from a news source every day, let us say morning. So in that case, what you can actually do is you can go to your Jupyter Notebook file, and then you can download this Jupyter Notebook as a.py file. So instead of having it as a Jupyter Notebook, you can download this as a.py file, which means you are going to get a Python code.py file, which you can use your Windows Task Scheduler or Mac Automator, to schedule it every day at a certain point of time. So this is one thing that you can do this in this project. The second thing, what you can do is, instead of a Jupyter Notebook, you can use this Jupyter Notebook to create a more generalized version of this project. And then you can convert it as a command line project. So where you can just invoke this project with an argument of URL, and then it will give you a summarized text. So in this project in this URL, if you see, we have hard coded the URL. But instead of hard coding the URL, what you can actually do is you can use this as an argument that could be paused at the command level. And then what you can do is you can use your terminal to invoke this Python project, passing the desired URL in the terminal as an argument and then you can extract the text as an output. So there are two things that you can do further with this project. One is extracting the Python code and then scheduling it so that you get the news into your inbox every day. Or instead of that you can have it as a Python project, the same.py file, but in a more generalized format instead of hard coding the URL. And then you can get the extracted summarized text whenever you supply a URL to this particular project. But if you do not wish to do any of these things, you can just keep it as this Jupyter Notebook. And then what you can do is whenever you want to change the URL, you can give a different URL here and then there under text and the text to this algorithm, this notebook would give you the desired output where you can use to read the news of a summarized format. Like just like headlines are how the in shorts app which we saw that most popular news application in India would be doing to send a card of text, which is a very summarized version of the actual news. In this video, we'll learn how to take this project further. So far. In this section, we learned how to build an automated new summarizer. We started with understanding what is the new summarizer and what kind of techniques are available. And then we learned about the Python packages required for it. And then finally, we did new summarizer using Jensen and the text extracted using beautifulsoup. I hope you enjoyed the project. Thank you very much.