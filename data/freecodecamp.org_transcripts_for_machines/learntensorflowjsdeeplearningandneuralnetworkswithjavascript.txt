what's up guys in this video we're going to introduce the concept of clientside artificial neural networks which will lead us to deploying and running models along with our full deep learning applications in the browser to implement this cool capability we'll be using tensorflow.js tensorflow's javascript library which will allow us to well build and access models in javascript i'm super excited to cover this so let's get to it whether you're a complete beginner to neural networks and deep learning or you just want to up your skills in this area you'll definitely want to check out all the resources available on the deep lizard youtube channel as well as deepblizzard.com there you'll find several complete video series and blogs where you'll learn everything from absolute machine learning basics all the way to developing and deploying your own deep learning projects from scratch here's a quick breakdown of the sections that make up this video go ahead and take a look here to get an idea of the content we'll be covering together so clientside neural networks running models in the browser to be able to appreciate the coolness factor of this we're going to need some context so that we can contrast what we've historically been able to do from a deployment perspective to what we can now do with clientside neural networks alright so what are we used to being able to do as we've seen in our previous series on deploying neural networks in order to deploy a deep learning application we need to bring two things together the model and the data to make this happen we'll normally see something like this we have a frontend application say a web app running in the browser that a user can interact with to supply data then we have a backend application a web service where our model is loaded and running when the user supplies the data to the frontend application that web app will make a call to the backend application and post the data to the model the model will then do its thing like make a prediction and then it will return its prediction back to the web application which will then be supplied to the user so that's the usual story of how we bring the model and the data together and we've already gone over all the details for how to actually implement this type of deployment so be sure to check out that series i mentioned earlier if you haven't already links in the description now with clientside neural network deployment we have no backend the model isn't sitting on a server somewhere waiting to be called by frontend apps but rather the model is embedded inside of the frontend application what does this mean well for a web app that means the model is running right inside the browser so in our example scenario that we went over a couple moments ago rather than the data being sent across the network or across the internet from the front end application to the model in the back end the data in the model are together from the start within the browser okay so this is cool and everything but what's the point for one users get to keep their data local to their machines or devices since the model is running on the local device as well if user data isn't having to travel across the internet we can say that's a plus and there's more on this concern in the previous series i mentioned additionally when we develop something that runs in a frontend application like in the browser that means that anyone can use it anyone can interact with it as long as they can browse to the link our application is running on there's no prerequisites or installs needed and in general a browser application is highly interactive and easy to use so from a user perspective it's really simple to get started and engage with the app so this is all great stuff but just as there's considerations to address with the traditional deployment implementation there are a couple caveats for the clientside deployment implementation as well but don't let that run you off there's just a few things to consider as we're talking about these i'm going to play some demos of open source projects that have been created with tensorflow.js just so you can get an idea of some of the cool things we can do with clientside neural networks links to all of them are in the description now on to the caveats for one we have to consider the size of our models since our models will be loaded and running in the browser you can imagine that loading a massive model into our web app might cause some issues tensorflow.js suggests to use models that are 30 megabytes in size or less to give some perspective the vgg 16 model is over 500 megabytes so what would happen if we tried to run that sucker in the browser we're actually going to demo that in a future section so stay tuned all right then what types of models are good to run in the browser well smaller ones ones that have been created with the intent to run on smaller or lower powered devices like phones and what types of models have we seen that are incredibly powerful for this mobile nets so we'll also be seeing how a mobilenet model holds up to being deployed in the browser as well now once we have a model up and running in the browser can we do anything we'd ordinarily otherwise do with the model using tensorflow.js pretty much but would we want to do everything with the model running in the browser that's another question and the answer is probably not having our models run in the browser is best suited for tasks like finetuning pretrained models or most popularly inference so using a model to get predictions on new data and this task is exactly what we saw with the project we worked on in the keras model deployment series using flask while building new models and training models from scratch can also be done in the browser these tasks are usually better addressed using other apis like keras or standard tensorflow in python so now that we have an idea of what it means to deploy our model to a clientside application why we'd want to do this and what types of specific things we'd likely use this for let's get set to start coding and implementing applications for this in the next sections of this video let me know in the comments if you plan to follow along and i'll see it in the next section in this section we'll continue getting acquainted with the idea of clientside neural networks and we'll kick things off by seeing how we can use tensorflow's model converter tool to convert keras models into tensorflow.js models this will allow us to take models that have already been built and trained with keras and make use of them in the browser with tensorflow.js so let's get to it tensorflow.js has what they call the layers api which is a high level neural network api inspired by keras and we'll see that what we can do with this api and how we use it is super similar to what we've historically been able to do with keras so given this it makes sense that we should be able to take a model that we built in keras or that we trained in keras and port it over to tensorflow.js and use it in the browser with the layers api right otherwise the alternative would be to build a model from scratch and train it from scratch in the browser and as we discussed in the last section that's not always going to be ideal so having the ability and the convenience to convert prebuilt or pretrained keras models to run in the browser is definitely going to come in handy alright now let's see how we can convert a keras model to a tensorflow.js model first we need to install the tensorflow.js model converter tool so from a python environment probably one where keras is already installed we run pip install tensorflow.js from the terminal and once we have this we can convert a keras model into a tensorflow.js model there are two ways to do the conversion and we'll demo both the first way is making use of the converter through the terminal or command line we'd want to use this method for keras models that we've already saved to disk as an h5 file if you've watched the d blizzard keras series you know we have multiple ways we can save a model or save parts of a model like just the weights or just the architecture to convert a keras model into a tensorflow.js model though we need to have saved the entire model with the weights the architecture everything in an h5 file currently that's done using kara's model.save function so given this i already have a sample model i've created with keras and save to disk if you don't already have access to these keras model files don't worry i've included links to keras github where you can just download these files once you have them you can follow this conversion process using those h5 files when they're needed later in this video i'm in the terminal now where we'll run the tensorflow.js converter program so we run tensorflow.js converter and specify what kind of input the converter should expect so we supply dash dash input format keras then we supply the path to the saved h5 file and the path to the output directory where we want our converted model to be placed and the output directory needs to be a directory that's solely for holding the converted model there will be multiple files so don't just specify your desktop or something like that so when we run this we get this warning regarding a deprecation but it's not hurting us for anything we're doing here and that's it for the first method we'll see in a few moments the format of the converted model but before we do that let's demo the second way to convert a keras model this is going to be done directly using python and this method is for when we're working with a keras model and we want to go ahead and convert it on the spot to a tensorflow.js model without necessarily needing to save it to an h5 file first so we're in a jupiter notebook where we're importing keras in the tensorflow.js library and i'm going to demo this with the vgg16 model because we'll be making use of this one in a future section anyway but this conversion will work for any model you build with keras so we have this vgg g16 model that's created by calling carous.applications.bgg16.vgg16 and then we call tensorflowjs.converters.save keras model and to this function we supply the model that we're converting as well as the path to the output directory where we want the converted tensorflow.js model to be placed and that's it for the second method so let's go check out what the output from these conversions look like we're going to look at the smaller model that we converted from the terminal so we're inside of this directory called simplemodel which is the output directory i specified whenever we converted the first model and we have a few files here we have this one file called model.json which contains the model architecture and metadata for the weight files and those corresponding weight files are these sharded files that contain all the weights from the model and are stored in binary format the larger and more complex the model is the more weight files there will be this model was small with only a couple dense layers and about 640 learnable parameters but the vgg16 model we converted on the other hand with over 140 million learnable parameters has 144 corresponding weight files all right so that's how we can convert our existing keras models into tensorflow.js models we'll see how these models and their corresponding weights are loaded in the browser in a future section when we start building our browser application to run these models i'll see you there in this section we'll go through the process of getting a web server setup to host deep learning web applications and serve deep learning models with express for node.js so let's get to it to build deep learning applications that run in the browser we need a way to host these applications and a way to host the models so then really we just need a way to serve static files if you followed the d blizzard youtube series on deploying keras models then you know that we already have a relatively easy way of hosting static files and that's with flask blast though is written in python and while it would work perfectly fine to host the tensorflow.js applications we'll be developing it makes sense that we might want to use a javascript based technology to host our apps since we're kind of breaking away from python and embracing javascript in this series so enter express for node.js express is a minimalist web framework very similar to flask but is for node.js not python and if you're not already familiar with node.js then you're probably wondering what it is as well node.js which we'll refer to most of the time as just node is an open source runtime environment that executes javascript on the server side see historically javascript has been used mainly for clientside applications like browser applications for example but node allows us to write serverside code using javascript we'll specifically be making use of express to host our web applications and serve our models so let's see how we can do that now first things first we need to install node.js i'm here on the downloads page of node's website so you just need to navigate to this page choose the installation for your operating system and get it installed i've installed node on a windows machine but you'll still be able to follow the demos we'll see in a few moments even if you're running another operating system alright after we've got node installed we need to create a directory that will hold all of our project files so we have this directory here i've called tensorflow.js within this directory we'll create a subdirectory called local server which is where the express code that will run our web server will reside and we'll also create a static directory which is where our web pages and eventually our models will reside within this local server we create a package.json file which is going to allow us to specify the packages that our project depends on let's go ahead and open this file i've opened this with visual studio code which is a free open source code editor developed by microsoft that can run on windows linux and mac os this is what we'll be using to write our code so you can download it and use it yourself as well or you can use any other editor that you'd like alright back to the package.json file within package.json we're going to specify a name for our project which we're calling tensorflow.js all lowercase per the requirements of this file we'll also specify the version of our project there's some specs that the format of this version has to meet but most simplistically it has to be in an x.x.x format so we're just going to go with the default of 1.0.0 all right name and version are the only two requirements for this file but there are several other optional items we can add like a description the author and a few others we're not going to worry about this stuff but we are going to add one more thing the dependencies this specifies the dependencies that our project needs to run we're specifying express here since that's what we'll be using to host our web apps and we're also specifying the version now we're going to open powershell and we have the ability to open it from right within this editor by navigating to view and then integrated terminal and you should have the ability to open the terminal of your choice that's appropriate for your operating system if you're running on linux for example and don't have powershell otherwise you can just open the terminal outside of the editor if you'd like all right so from within powershell we make sure we're inside of the local server directory where the package.json file is and we're going to run npm install npm stands for node package manager and by running npm install npm will download and install the dependencies listed in our package.json file so let's run npm install and we'll see it installs express and when this is finished you can see that we now have an added node modules directory that contains the downloaded packages and we additionally have this packagelock.js file that we didn't have before it contains information about the downloaded dependencies don't delete these things alright so at this point we have node we have express now we need to write a node program that will start the express server and will host the files that we specify let's see that makes sense to do this we'll create this file called server.js inside of server js we first import express using require express using require like this we'll import the express module and give our program access to it you can think of a module and node as being analogous to a library in javascript or python just a group of functions that we want to have access to from within our program and then we create an express application using the express module which we assigned to app an express app is essentially a series of calls to functions that we call middleware functions middleware functions have access to the http request and response objects as well as the next function in the application's request response cycle which just passes control to the next handler so within this app when a request comes in we're doing two things we're first logging information about the request to the terminal where the express server is running and we then pass control to the next handler which will respond by serving any static files that we've placed in this directory called static that's right within the root directory of our tensorflow.js project so in our case the middleware functions i mentioned are here and here note that the calls to app.use are only called once and that's when the server is started the app.use calls specify the middleware functions and calls to those middleware functions will be executed each time a request comes into the server lastly we call app.listen to specify what port express should listen on i've specified port 81 here but you can specify whichever unused port you'd like when the server starts up and starts listening on this port this function will be called which will log this message letting us know that the server is up and running all right we're all set up let's drop a sample html file into our static directory then start up the express server and see if we can browse to the page we're going to actually just place the web application called predict.html that we created in the keras deployment series into this directory as a proof of concept so we place that here you can use any html file you'd like though to test this now to start express we use powershell let's make sure we're inside of the local server directory and we run node server js we get our output message letting us know that express is serving files from our static directory on port 81. so now let's browse to localhost or whatever the ip address is that you're running express on port 81 slash predict.html which is the name of the file we put into the static directory and here we go this is indeed the web page we wanted to be served we can also check out the output from this request in powershell to view the logging that we specified so good we now have node and express set up to be able to serve our models and host our tensorflow.js apps that we'll be developing coming up give me a signal in the comments if you are able to get everything up and running and i'll see it in the next section in this section we're going to start building the ui for our very first clientside neural network application using tensorflow.js so let's get to it now that we have express setup to host a web app for us let's start building one the first app we'll build is going to be similar in nature to the predict app we built in the flask series with keras recall this was the app we built in that previous series we had a finetuned vgg g16 model running in the back end as a web service and as a user we would select an image of a cat or dog submit the image to the model and receive a prediction now the idea of the app will develop with tensorflow.js will be similar but let's discuss the differences can you see the source code that's generating the responses um yeah we can and we will but first know that our model will be running entirely in the browser our app will therefore consist only of a frontend application developed with html and javascript so here's what the new app will do the general layout will be similar to the one we just went over where a user will select an image submit it to the model and get a prediction we won't be restricted to choosing only cat and dog images though because we won't be using finetuned models this time instead we'll be using original pretrained models that were trained on imagenet so we'll have a much wider variety of images we can choose from once we submit our selected image to the model the app will give us back the top five predictions for that image from the imagenet classes so which model will we be using well remember how we discussed earlier that models best suited for running in the browser are smaller models and how tensorflow recommends using models that are 30 megabytes or less in size well we're first going to go against this recommendation and use vgg16 as our model which is over 500 megabytes in size nice priorities we'll see how that works out for us but you can imagine that it may be problematic no worries though we'll have mobilenet to the rescue coming in at only about 16 megabytes so we'll get to see how these two models compare to each other performance wise in the browser it'll be interesting all right let's get set up from within the static directory we created last time we need to create a few new resources we need to create a file called predictwithtfjs.html which will be our web app then we also need to create a file called predict js which will hold all the javascript logic for our app then we need a directory to hold our tensorflow.js models so we have this one which we're calling tfjs models navigating inside we have two subdirectories one for mobilenet and one for vgg16 since these are the two models we'll be using each of these directories will contain the model.json and the corresponding weight files for each model navigating inside of vgg 16 we can see that to get these files here i simply went through the conversion process in python of loading bg g16 and mobilenet with keras and then converting the models with the tensorflow.js converter we previously discussed so follow that earlier section to get this same output to place in your model directories alright navigating back to the static directory the last resource is this imagenet class js file this is simply a file that contains all the imagenet classes which we'll be making use of later you can also find all of these ordered imagenet classes on the tensorflow.js blogs at deeplizzer.com let's open it up and take a look at the structure so we just have this javascript object called imagenet classes that contains the key value pairs of the imagenet classes with associated ids all right now let's open the predict with tfjs.html file and jump into the code we're starting off in the head by specifying the title of our web page and importing the styling from this css file for all the styling on the page we'll be using bootstrap which is an open source library for developing html css and javascript that uses design templates to format elements on the page bootstrap is really powerful but we'll simply be using it just to make our app look a little nicer now bootstrap uses a grid layout where you can think of the webpage having containers that can be interpreted as grids and then ui elements on the page are organized into the rows and the columns that make up those grids by setting the element's class attributes that's how bootstrap knows what type of styling to apply to them so given this that's how we're organizing our ui elements embedded within the body we're putting all the ui elements within this main tag you can see that our first div is what's considered to be a container on the page and then within the container we have three rows and each row has columns the columns are where our actual ui elements reside our ui elements are the image selector the predict button the prediction list and the selected image we'll explore this grid layout interactively in just a moment but first let's finish checking out the remainder of the html all that we have left to do is import the required libraries and resources that our app needs first we import jquery then we import tensorflow.js with this line so this single line is all it takes to get tensorflow.js into our app then we import the imagenet class js file we checked out earlier and lastly we import our predict.js file which as mentioned earlier contains all the logic for what our app does when a user supplies an image to it alright so that's it for the html let's check out the page and explore the grid layout first we start up our express server which we learned how to do in the last section then in our browser we'll navigate to the ip address where our express server is running port 81 predict with tfjs.html and here's our page it's pretty empty right now because we haven't selected an image but once we write the javascript logic to handle what to do when we select an image then the name of the selected image file will be displayed here the image will be displayed in the image section and upon clicking the predict button the predictions for the image from the model will be displayed in this prediction section if we open the developer tools by right clicking on the page and then clicking inspect then from the elements tab we can explore the grid layout let's expand the body then main then this first div that acts as the container and hovering over this div you can see that the blue on the page is what's considered to be the container or the grid so now that we've expanded this div we have access to all the rows so hovering over the first row we can see what that maps to in the ui from this blue section and we can do the same for the second and third rows as well then if we expand the rows we have access to the columns that house the individual ui elements so hovering over this first column in the first row we can see that the image selector is here and the predict button is within the second column in the first row and the same idea applies for the remaining elements on the page as well so hopefully that sheds a bit of light on the grid layout that bootstrap is making use of all right in the next section we'll explore all of the javascript that handles the predictions and actually makes use of tensorflow.js i'll see you there in this section we'll continue the development of the clientside deep learning application we started last time so let's get to it in the last section we built the ui for our image classification web app now we'll focus on the javascript that handles all the logic for this app we'll also start getting acquainted with the tensorflow.js api without further ado let's get right into the code recall in the last section we created this predict.js file within the static directory but left it empty this file now contains the javascript logic that handles what will happen when a user submits an image to the application so let's look at the specifics for what's going on with this code we first specify what should happen when an image file is selected with the image selector when a new image is selected the change event will be triggered on the image selector and when this happens we first create this file reader object called reader to allow the web app to read the contents of the selected file we then set the onload handler for reader which will be triggered when reader successfully reads the contents of a file when this happens we first initialize the data url variable as reader.result which will contain the image data as a url that represents the files data as a base64 encoded string we then set the source attribute of the selected image to the value of data url lastly within the onload handler we need to get rid of any previous predictions that were being displayed for previous images and we do this by calling empty on the prediction list element next we get the selected file from the image selector and load the image by calling read as data url on reader and passing in the selected image file we then instantiate this model variable and we're going to define it directly below now this below section may look a little freaky if you're not already a javascript whiz so let's see what the deal is here we have what's called an iife or immediately invoked function expression an iife is a function that runs as soon as it's defined we can see this is structured by placing the function within parentheses and then specifying the call to the function with these parentheses that immediately follow within this function we load the model by calling the tensorflow.js function tf.loadmodel which accepts a string containing the url to the model.json file recall from the last section we showed how the model.json file and corresponding weight files should be organized within our static directory that's being served by express we're first going to be working with vgg16 as our model so i've specified the url to where the model.json file for vgg16 resides now a tf.load model returns a promise meaning that this function promises to return the model at some point in the future this await keyword pauses the execution of this wrapping function until the promise is resolved and the model is loaded this is why we use the async keyword when defining this function because if we want to use the await keyword then it has to be contained within an async function now i've added a progress bar to the ui to indicate to the user when the model is loading as soon as the promise is resolved we're then hiding the progress bar from the ui which indicates the model is loaded before moving on let's quickly jump over to the html we developed last time so i can show you where i inserted this progress bar so here we are in predict with tfjs.html and you can see that right within this first div the container i've inserted this row where the progress bar is embedded we'll see it in action within the ui at the end of this video alright jumping back over to the javascript we now need to write the logic for what happens when the predict button is clicked when a user clicks the predict button we first get the image from the selected image element then we need to transform the image into a rank 4 tensor object of floats with height and width dimensions of 224 by 224 since that's what the model expects to do this we create a tensor object from the image by calling the tensorflow.js function tf dot from pixels and passing our image to it we then resize the image to 224 by 224 cast the tensor's type to float 32 and expand the tensor's dimensions to be of rank 4. we're doing all of this because the model expects the image data to be organized in this way and note that all of these transformations are occurring with calls to functions from the tensorflow.js api all right we have the tensor object of image data that the model expects now vgg16 actually wants the image data to be further preprocessed in a specific way beyond the basics we just completed there are transformations to the underlying pixel data that need to happen for this preprocessing that bgg16 wants in other libraries like keras preprocessing functions for specific models are included in the api currently though tensorflow.js does not have these preprocessing functions included so we need to build them ourselves we're going to build a preprocessing function in the next section to handle this so for right now what we'll do is pass in the image data contained in our tensor object as is to the model the model will still accept the data as input it just won't do a great job with its predictions since the data hasn't been processed in the same way as the images that bgg16 was originally trained on so we'll go ahead and get this app functional now and then we'll circle back around to handle the preprocessing in the next section and insert it appropriately then all right so a user clicks the predict button we transform the image data into a tensor and now we can pass the image to the model to get a prediction we do that by calling predict on the model and passing our tensor to it predict returns a tensor of the output predictions for the given input we then call data on the prediction sensor which asynchronously loads the values from the tensor and returns a promise of a typed array after the computation completes notice the await and async keywords here that we discussed earlier so this predictions array is going to be made up of 1000 elements each of which corresponds to the prediction probability for an individual imagenet class each index in the array maps to a specific imagenet class now we want to get the top 5 highest predictions out of all of these since that's what we'll be displaying in the ui we'll store these top five predictions in this top five variable top five top five top five before we sort and slice the array to get the top five we need to map the prediction values to their corresponding imagenet classes for each prediction in the array we return a javascript object that contains the probability and the imagenet class name notice how we use the index of each prediction to obtain the class name from the imagenet classes array that we imported from the imagenet classes javascript file we then sort the list of javascript objects by prediction probability in descending order and obtain the first five from the sorted list using the slice function lastly we iterate over the top five predictions and store the class names and corresponding prediction probabilities in the prediction list of our ui and that's it let's now start up our express server and browse to our app all right we're here and we've got indication that our model is loading so i paused the video while this model was continuing to load and it ended up taking about 40 seconds to complete not great it may even take longer for you depending on your specific computing resources remember though i said we'd run into some less than ideal situations with running such a large model like vgg 16 in the browser i warned you now so the time it takes to load the model is the first issue we've got over 500 megabytes of files to load into the browser for this model hence the long loading time alright well our model is loaded so let's choose an image and predict on it hmm about a five second wait time to get a prediction on a single image again not great oh and yeah the display prediction isn't accurate but that doesn't have anything to do with the model size or anything like that it's just because we didn't include the preprocessing for bgg16 remember we're going to handle that in the next section there we'll get further exposure to the tensorflow.js api by exploring the tensor operations we'll need to work with to do the preprocessing all right so we've got that coming up and then afterwards we'll solve all these latency issues attributed to using a large model by substituting mobilenet in for vgg16 let me know in the comments if you were able to get your app up and running and i'll see it in the next section in this section we're going to explore several tensor operations by preprocessing image data to be passed to a neural network running in our web app so let's get to it recall that last time we developed our web app to accept an image pass it to our tensorflow.js model and obtain a prediction for the time being we're working with vgg16 as our model and in the previous section we temporarily skipped over the image preprocessing that needed to be done for vgg16 we're going to pick up with that now so we're going to get exposure to what specific preprocessing needs to be done for vgg16 yes but perhaps more importantly we'll get exposure to working with and operating on tensors we'll be further exploring the tensorflow.js library in order to do these tensor operations all right let's get into the code we're back inside of our predict js file and we're going to insert the vgg16 preprocessing code right within the handler for the click event on the predict button we're getting the image in the same way we covered last time converting it into a tensor object using tf.from pixels resizing it to the appropriate 224x224 dimensions and casting the type of the tensor to float32 no change here so far all right now let's discuss the preprocessing that needs to be done for vgg16 this paper authored by the creators of vgg16 discusses the details the architecture and the findings of this model we're interested in finding out what preprocessing they did on the image data jumping to the architecture section of the paper the authors state quote the only preprocessing we do is subtracting the mean rgb value computed on the training set from each pixel let's break this down a bit we know that imagenet was the training set for vgg16 so imagenet is the data set for which the mean rgb values are calculated to do this calculation for a single color channel say red we compute the average red value of all the pixels across every imagenet image the same goes for the other two color channels green and blue then to preprocess each image we subtract the mean red value from the original red value in each pixel we do the same for the green and blue values as well this technique is called zero centering because it forces the mean of the given data set to be zero so we're zero centering each color channel with respect to the imagenet data set now aside from zero centering the data we also have one more preprocessing step not mentioned here the authors trained vgg16 using the cafe library which uses a bgr color scheme for reading images rather than rgb so as a second preprocessing step we need to reverse the order of each pixel from rgb to bgr alright now that we know what we need to do let's jump back in the code and implement it we first define a javascript object mean imagenet rgb which contains the mean red green and blue values from imagenet we then define this list we're calling indices the name will make sense in a minute this list is made up of onedimensional tensors of integers created with tf.tensor1d the first tensor in the list contains the single value zero the second tensor contains the single value 1 and the third tensor contains the single value 2. we'll be making use of these sensors in the next step here we have this javascript object we're calling centered rgb which contains the centered red green and blue values for each pixel in our selected image let's explore how we're doing this centering recall that we have our image data organized now into a 224 by 224x3 tensor object so to get the centered red values for each pixel in our tensor we first use the tensorflow.js function tf.gather to gather all the red values from the tensor specifically tf.gather is gathering each value from the zeroth index along the tensor's second axis each element along the second axis of our 224 by 224 by 3 tensor represents a pixel containing a red green and blue value in that order so the zeroth index in each of these pixels is the red value of the pixel after gathering all the red values we need to center them by subtracting the mean imagenet red value from each red value in our tensor to do this we use the tensorflow.js sub function which will subtract the value passed to it from each red value in the tensor it will then return a new tensor with those results the value we're passing to sub is the mean red value from our mean imagenet rgb object but we're first transforming this raw value into a scalar object by using the tf.scalar function alright so now we've centered all the red values but at this point the tensor we've created that contains all of these red values is of shape 224 by 224 by 1. we want to reshape this tensor to just be a onedimensional tensor containing all 50 176 red values so we do that by specifying this shape to the reshape function great now we have a onedimensional tensor containing all the centered red values from every pixel in our original tensor we need to go through this same process now again to get the centered green and blue values at a brief glance you can see the code is almost exactly the same as what we went through for the red values the only exceptions are the indices we're passing to tf.gather and the mean imagenet values we're passing to tf.scalar at this point we now have this centered rgb object that contains a onedimensional tensor of centered red values a onedimensional tensor of centered green values and a onedimensional tensor of centered blue values we now need to create another tensor object that brings all of these individual red green and blue tensors together into a 224 by 224x3 tensor this will be the preprocessed image so we create this process tensor by stacking the centered red centered green and centered blue tensors along axis one the shape of this new tensor is going to be of 50 176 by 3. this sensor represents 50 176 pixels each containing a red green and blue value we need to reshape this sensor to be in the form that the model expects which is 224 by 224 by 3. now remember at the start we said that we need to reverse the order of the color channels of our image from rgb to bgr so we do that using the tensorflow.js function reverse to reverse our tensor along the specified axis lastly we expand the dimensions to transform the tensor from rank 3 to rank 4 since that's what the model expects okay now we have our preprocessed image data in the form of this preprocessed tensor object so we can pass this preprocessed image to our model to get a prediction before we do that though note that we handled these tensor operations in a specific way and a specific order to preprocess the image it's important to know though that this isn't the only way we could have achieved this in fact there's a much simpler way through a process called broadcasting that could achieve the same process tensor at the end you gotta be kidding me don't worry we're going to be covering broadcasting in a future section but i thought that for now doing these kind of exhaustive tensor operations would be a good opportunity for us to explore the tensorflow.js api further and get more comfortable with tensors in general checking out our app using the same image as last time we can now see that the model gives us an accurate prediction on the image since the image has now been processed appropriately now i don't know about you but tensor operations like the ones we worked with here are always a lot easier for me to grasp when i can visualize what the tensor looks like before and after the transformation so in the next section we're going to step through this code using the debugger to visualize each tensor transformation that occurs during preprocessing i'll see you there in this section we're going to continue our exploration of tensors here we'll be stepping through the code we developed last time with the debugger to see the exact transformations that are happening to our tensors in real time so let's get to it last time we went through the process of writing the code to preprocess images for vgg16 through that process we gained exposure to working with tensors transforming and manipulating them we're now going to step through these tensor operations with the debugger so that we can see these transformations occur in real time as we interact with our app if you're not already familiar with using a debugger don't worry you'll still be able to follow we'll first go through this process using the debugger and visual studio code then we'll demo the same process using the debugger built into the chrome browser we're here within our predict.js file within the click event for the predict button where all the preprocessing code is written we're placing a breakpoint in our code where our first sensor is defined remember this is where we're getting the selected image and transforming it into a tensor using tf.from pixels the expectation around this breakpoint is when we browse to our app the model will load we'll select an image and click the predict button once we click predict this click event will be triggered and will hit this breakpoint when this happens the code execution will be paused until we tell it to continue to the next step this means that while we're paused we can inspect the tensors we're working with and see how they look before and after any given operation let's see we'll start our debugger in the top left of the window which will launch our app in chrome alright we can see our model is loading okay the model is loaded let's select an image now let's click the predict button and when we do this we'll see our break point will get hit and the app will pause and here we go our code execution is now paused we'll minimize the browser and expand our code window now since this is where we'll be debugging we're currently paused at this line where we define our tensor object we're going to click this step over icon which will execute this code where we're defining tensor and we'll pause at the next step let's see all right we're now paused at the next step now that tensor has been defined let's inspect it a bit first we have this variables panel over in the left where we can check out information about the variables in our app and we can see our tensor variable is here in this list clicking tensor we can see we have all types of information about this object for example we can see the d type is float32 the tensor is of rank three the shape is 224 by 224 by three and the size is a hundred and fifty thousand and five twenty eight so we get a lot of information describing this guy additionally in the debug console we can play with this sensor further for example let's print it using the tensorflow.js print function and we'll scroll up a bit and we can see that this kind of lets us get a summary of the data contained in this tensor remember we made this tensor have shape 224 by 224x3 so looking at this output we can visualize this tensor as an object with 224 rows each of which is 224 pixels across and each of those pixels contains a red green and blue value so what's selected here represents one of those 224 rows and each one of these are one of the pixels in this row and each of these pixels contains first a red then a green then a blue value so make sure you have a good grip on this idea so you can follow all the transformations this tensor is about to go through all right our debugger is paused on defining the mean imagenet rgb object let's go ahead and step over this so that it gets defined again we can now inspect this object over in the local variables panel we're not doing any tensor operations here so let's go ahead and move on we're now paused on our list of rank one tensors called indices which we'll make use of later so let's execute this we can see indices now shows up in our local variable panel let's inspect this one a bit from the debug console if we just print out this list using console.log indices get back that this is an array with three things in it we know that each element in this array is a tensor so let's access one of them let's get the first tensor and it might help if we spell indices correctly so let's try that again we get back that this object is a tensor and we can see what it looks like just a onedimensional tensor with a single value zero and we can easily do the same thing for the second and third elements in the list too all right we're going to minimize this panel on the left now and scroll up some in our code we're now paused where we're defining the centered rgb object and from last time we know that's where the bulk of our tensor operations are occurring so if we execute this block then we'll skip over being able to inspect each of these transformations so what we'll do is we'll stay paused here but in the debugger console we'll mimic each of these individual transformations one by one so we can see the before and after version of the tensor so for example we're first going to mimic what's happening here with the creation of the tensor that contains all the centered red values within our centered rgb object in the console we'll create this variable called red and set it equal to just the first call to tf.gather and see what it looks like so we'll go ahead and copy this call and we'll create a variable red and set it equal to that before we do any other operations let's see what this looks like let's first check the shape of red okay 224 by 224 by one so similar to what we saw from the original tensor of 224 by 224 by 3 but rather than the last dimension containing all three pixel values red green and blue our new red tensor only contains the red pixel values let's print red and let's scroll up so that we can see the start of the tensor and just to hit the point home let's compare this to the original tensor so the first three values in red are 56 58 and 59 now let's scroll up and check out the original tensor to see if this lines up so 56 58 59 scrolling up to our original tensor and yep our original tensor has the red values of 56 58 and 59 in the first three zeroth indices along the second axis so red is just made up of each of these values all right let's scroll back down in our debug console and let's see what the next operation on red is this is where we're centering each red value by subtracting the mean red value from imagenet using this sub function let's make a new variable called centered red and mimic this operation so we'll define centered red equal to red and then call this sub function now let's print centered red and scroll up to the top okay so about minus 67 minus 65 and minus 64 for the first three values along the second axis let's compare this to the original red tensor now by scrolling up to look at that and these are 56 58 and 59 as the first three values along the second axis so if we do the quick math of subtracting the mean red value of 123.68 and remember we can see that by looking here 123.68 as our mean red value in the mean imagenet rgb object subtracting this number from the first three values of our original red tensor we do indeed end up with the centered red values in the new centered red tensor we just looked at now centered red still has the same shape as red which recall is 224 by 224 by 1. the next step is to reshape the sensor to be a rank one tensor of size fifty 50176 so we just want to bring all the centered red values together which are currently each residing in their own individual tensors so to mimic this reshape call we'll make a new variable called reshaped red so we'll scroll back down in our debugger console and we'll copy this reshape call and we'll define reshaped red equal to centered red and then call reshape on that all right let's check the shape on this new object to get confirmation and we see it is indeed the shape that we specified let's now look at the printout of reshaped red okay and we see all the red values are now listed out here in this onedimensional tensor all right so that's it for getting all the centered red values as mentioned last time we go through the same process to gather all the blues and greens as well so we're not going to go through that in the debugger we'll now execute this block of code to create this centered rgb object and move on to the next step this is where we're bringing our centered red green and blue values all together into a new processed tensor so from the console let's run this first stack operation by creating a variable called stacked tensor so i'll create stack tensor set that equal to this stack call remember we just saw that reshaped red ended up being a rank 1 tensor of shape 50 176 the green and blue tensors have the same shape and size so when we stack them along axis 1 we should now have a 50 176 by 3 tensor you may think the result of the stack operation would look like this where we have the centered red tensor with its 50 176 values stacked on top of the green tensor stacked on top of the blue tensor and that's how it would look if we were stacking along axis zero because we're stacking along axis one though we'll get something that looks like this where we have fifty 50 176 rows each of which is made up of a single pixel with a red green and blue value let's check the shape now in the console to be sure we get the 50 0176 by 3 we expect yep we do let's also print it to get a visual okay so we have 50 176 rows each containing a red green and blue value now we need to reshape this guy to be of shape 224 by 224 by 3 before we can pass it to the model so let's do that now with a new variable we'll call reshaped tensor so we'll copy the reshape call from over here and define reshaped tensor equal to our stacked tensor dot reshape okay let's print this reshape tensor and scroll up to the top again this shape means we have 224 rows each containing 224 pixels which each contain a red green and blue value now we need to reverse the values in this tensor along the second axis from rgb to bgr for the reasons we mentioned last time so we'll copy this reverse call here and we'll make a new object called reversed tensor and set that equal to our reshape tensor dot reverse and we need to scroll down in our debug console and let's print this one out and scroll up to the top of it okay so we see the first bgr values now let's scroll up to our last sensor to make sure this is the reverse of the rgb values we had there so minus 99 minus 87 minus 67 scrolling up we have minus 99 minus 87 minus 67. so indeed our new tensor has the reversed rgb values let's scroll back down in our debugger and our last operation is expanding the dimensions of our tensor to make it go from rank 3 to a rank 4 tensor which is what our model requires so we'll create a new tensor called expanded tensor and set that equal to reverse tensor and we'll copy the expand dim call from over here and call that on our reverse sensor all right now let's check the shape of this guy to make sure it's what we expect so we have this inserted dimension at the start now making our tensor rank four with shape one by two twenty four by two twenty four by three rather than just two twenty four by two twenty four by 3 that we had last time and if we print this out and scroll up to the start we can see this extra dimension added around our previous tensor and that sums up all the tensor operations quickly though in case you're not using visual studio code i did want to also show this same setup directly within the chrome browser so that you can do your debugging there instead if you'd prefer in chrome we can right click on our page click inspect and then go to the sources tab here we have access to the source code for our app predict.js is currently being shown in the open window so now we have access to the exact code we were displaying in visual studio code and we can insert breakpoints here in the same way as well let's go ahead and put a breakpoint in the same place as we did earlier now let's select an image and click the predict button we see that our app is paused at our breakpoint and then we can step over the code just as we saw earlier and we have our variables all showing in this panel here and we also have our console down here so i can do indices zero dot print for example to get that same output that we got in visual studio debugger and from this console i can run all the same code that we ran in visual studio code as well alright hopefully now you have a decent grasp on tensors and tensor operations let me know what you thought of going through this practice in the debugger to see how the tensor's changed over time with each operation and i'll see it in the next section in this section we'll learn about broadcasting and illustrate its importance and major convenience when it comes to tensor operations so let's get to it over the last couple of sections we've immersed ourselves in tensors and hopefully now we have a good understanding of how to work with transform and operate on them if you recall a couple sections back i mentioned the term broadcasting and said that we would later make use of it to vastly simplify our vgg g16 preprocessing code before we get into the details about what broadcasting is though let's get a sneak peek of what our transform code will look like once we've introduced broadcasting because i'm using git for source management i can see the diff between our original predict.js file and the modified version of this file that uses broadcasting on the left we have our original predict.js file within the click event recall this is where we transformed our image into a tensor then the rest of this code was all created to do the appropriate preprocessing for vgg16 where we centered and reversed the rgb values now on the right this is our new and improved predict.js file that makes use of broadcasting in place of all the explicit one by one tensor operations on the left so look all of this code in red has now been replaced with what's shown in green that's a pretty massive reduction of code before we show how this happened we need to understand what broadcasting is broadcasting describes how tensors with different shapes are treated during arithmetic operations for example it might be relatively easy to look at these two rank two tensors and figure out what the sum of them would be they have the same shape so we just take the element y sum of the two tensors where we calculate the sum element by element and here we go we have our resulting tensor now since these two tensors have the same shape one by three no broadcasting is happening here remember broadcasting comes into play when we have tensors with different shapes alright so what would happen if our two rank two tensors instead looked like this and we wanted to sum them we have one with shape one by three and the other was shaped three by one well here's where broadcasting will come into play before we cover how this is done go ahead and pause the video and just see intuitively what comes to mind as the resulting tensor from adding these two together give it a go write it down and keep what you write handy because we'll circle back around to what you wrote later in the video all right we're first going to look at the result and then we'll go over how we arrive there our result from summing these two tensors is a three by three tensor so here's how broadcasting works we have two tensors with different shapes the goal of broadcasting is to make the tensors have the same shape so we can perform elementwise operations on them first we have to see if the operation we're trying to do is even possible between the given tensors based on the tensor's original shapes there may not be a way to reshape them to force them to be compatible and if we can't do that then we can't use broadcasting the rule to see if broadcasting can be used is this we compare the shapes of the two tensors starting at their last dimensions and working backwards our goal is to determine whether or not each dimension between the two tensor shapes is compatible in our example we have shapes three by one and one by three so we first compare the last dimensions the dimensions are compatible when either a they're equal to each other or b one of them is one comparing the last dimensions of the two shapes we have a one and a three are these compatible well let's check the rule are they equal no one doesn't equal three is one of them one yes great the last dimensions are compatible working our way to the front for the next dimension we have a three and a one similar story just switched order right so are these compatible yes okay that's the first step we've confirmed each dimension between the two shapes is compatible if however while comparing the dimensions we confirmed that at least one dimension wasn't compatible then we would cease our efforts there because the arithmetic would not be possible between the two now since we've confirmed that our two tensors are compatible we can sum them and use broadcasting to do it when we sum two tensors the result of this sum will be a new tensor our next step is to find out the shape of this resulting tensor we do that by again comparing the shapes of the original tensors let's see exactly how this is done comparing the shape of one by three to three by one we first calculate the max of the last dimension the max of three and one is three three will be the last dimension of the shape of the resulting tensor moving on to the next dimension again the max of one and three is three so three will be the next dimension of the shape of the resulting tensor we've now stepped through each dimension of the shapes of the original tensors and we can conclude that the resulting tensor will have shape three by three the original tensors of shape one by three and three by one will now be expanded to shape three by three also in order to do the elementwise operation broadcasting can be thought of as copying the existing values within the original tensor and expanding that tensor with these copies until it reaches the required shape the values in our 1x3 tensor will now be broadcast to this 3x3 tensor and the values in our 3x1 tensor will now be broadcast to this 3x3 tensor we can now easily take the elementwise sum of these two to get this resulting three by three tensor let's do another example what if we wanted to multiply this rank two tensor of shape one by three with this rank zero tensor better known as a scalar we can do this since there's nothing in the broadcasting rules preventing us from operating on two tensors of different ranks let's see we first compare the last dimensions of the two shapes when we're in a situation where the ranks of the two tensors aren't the same like what we have here then we simply substitute a one in for the missing dimensions of the lower ranked tensor in our example we substitute a one here then we ask are these two dimensions compatible and the answer will always be a yes in this type of situation since one of them will always be a one all right all the dimensions are compatible so what will the resulting tensor look like from multiplying these two together again go ahead and pause here and try yourself before getting the answer well the max of three and one is three and the max of one and one is one so our resulting tensor will be of shape one by three our first sensor is already this shape so it gets left alone our second tensor is now expanded to this shape by broadcasting its value like this now we can do our elementwise multiplication to get this resulting one by three tensor let's do one more example what if we wanted to sum this rank 3 tensor of shape 1 by 2 by 3 and this rank 2 tensor of shape 3x3 before covering any of the incremental steps go ahead and give it a shot yourself and see what you find out alright assuming you've now paused and resumed the video the deal with these two tensors is that we can't operate on them why well comparing the second to last dimensions of the shapes they're not equal to each other and neither one of them is one so we stop there all right and now we should have a good grip on broadcasting let's go see how we're able to make use of it in our vgg g16 preprocessing code first we can see we're changing our mean imagenet rgb object into a rank 1 tensor which makes sense right because we're going to be making use of broadcasting which is going to require us to work with tensors not arbitrary javascript objects alright now get a load of this remaining code all of this code was written to handle the centering of the rgb values this has now all been replaced with this single line which is simply the result of subtracting the mean imagenet rgb tensor from the original tensor okay so why does this work and where is the broadcasting let's see our original tensor is a rank 3 tensor of shape 224 by 224x3 our mean imagenet rgb tensor is a rank 1 tensor of shape 3. our objective is to subtract each mean rgb value from each rgb value along the second axis of the original tensor from what we've learned about broadcasting we can do this really easily we compare the dimensions of the shapes from each tensor and confirm they're compatible the last dimensions are compatible because they're equal to each other the next two dimensions are compatible because we substitute a one in for the missing dimensions in our rank one tensor taking the max across each dimension our resulting tensor will be of shape 224 by 224 by 3. our original tensor already has that shape so we leave it alone our rank 1 tensor will be expanded to the shape of 224 by 224 by 3 by copying its three values along the second axis so now we can easily do the elementwise subtraction between these two tensors exiting out of this diff and looking at the modified predict js file alone we have this so the reversing and the expanding of the dims at the end is still occurring in the same way after the centering now actually if we wanted to make this code even more concise rather than creating two tensor objects our original one and our preprocessed one we can chain all these calls together to condense these two separate tensors into one we would first need to bring our mean imagenet rgb definition above our tensor definition then we need to move our sub reverse and expand dim calls up and change them to the original tensor lastly we replace this reference to process tensor with just tensor and that's it so if you took the time to truly understand the tensor operations we went through step by step in the last couple of sections then you should now be pretty blown away by how much easier broadcasting can make our lives and our code given this do you see the value in broadcasting let me know in the comments oh also remember all those times i asked you to pause the video and record your answers to the examples we were going through let me know what you got and don't be embarrassed if you were wrong i was wrong when i tried to figure out examples like these when i first started learning broadcasting so no shame let me know and i'll see you in the next section in this section we'll be adding new functionality to our deep learning web application to increase its speed and performance specifically we'll see how we can do this by switching models so let's get to it we currently have a web app that allows users to select and submit an image and subsequently receive a prediction for the given image up to this point we've been using vgg16 as our model bgg16 gets the job done when it comes to giving accurate predictions on the submitted images however as we've previously discussed a model of its size over 500 megabytes is not ideal for running in the browser because of this we've seen a decent time delay in both loading the model as well as obtaining predictions from the model well we're in luck because we'll now make use of a much smaller model mobilenet which is pretty ideal size wise for running in the browser coming in at around 16 megabytes with mobilenet we'll see a vast decrease in time for both loading the model and obtaining predictions let's go ahead and get into the code to see what modifications we need to make all right we're here in our predict with tfjs.html file and we're going to make a model selector where the user has the ability to choose which model to use for now we'll have vgg16 and mobilenet as available options currently the call to load the model occurs immediately when the web page is requested but now we'll change that functionality so that the model will be loaded once a user selects which model they'd like to use our model selector will take on the form of an html select element so the first thing we need to do is add this element to our html within the same row as the image selector and the predict button we're adding this new select element within a column to the left of both of the previously mentioned elements when a user shows up to the page the model selector will be set to the option that states select model and they'll have the option to choose either mobilenet or vgg16 now also recall how we mentioned that until now the model was being loaded immediately when a user arrived at the page and during that time the progress bar would show to indicate the loading since we'll be changing the functionality so that the model isn't loaded until a user chooses which model they want to use we won't need the progress bar to show until that model is selected so navigating to the progress bar element we're going to set the display style attribute to none which will hide the progress bar until we explicitly instruct it to be shown in the javascript code alright that's it for the changes to our html jumping to predict.js will now specify what should happen once a user selects a model when a model is selected this will trigger a change event on the model selector we're handling this event by calling a new function which we'll discuss in a moment called load model load model essentially does what it sounds like it does we pass this function the value from model selector which is either going to be mobilenet or vgg16 do you remember how previously we were loading the model using an immediately invoked function expression or iife well now that we don't want to load the model until we explicitly call load model like we just specified we no longer want this loading to happen within an iife the code for load model is actually super similar to the iife we had before just with some minor adjustments load model accepts the name of the model to be loaded once called the progress bar will be shown to indicate the model is loading we initially set the model to undefined so that in case we're in a situation where we're switching from one model to another the previous model can be cleared from memory afterwards we set model to the result of calling the tensorflow.js function tf.loadmodel remember this function accepts the url to the given model's model.json file the models reside in folders that were given the names of the actual models themselves for example the vgg16 files reside within a directory called vgg16 and the mobilenet files reside within a directory called mobilenet so when we give the url to the model.json we use the name of the selected model to point to the correct location for where the corresponding json file resides once the model is loaded we then hide the progress bar all right now let's navigate to the click event for the predict button previously within this handler function we would get the selected image and then we would do all of the preprocessing for vgg16 and get a prediction well now since we have two different models that preprocess images differently we're putting the preprocessing code into its own function called preprocess image so now once a user clicks the predict button we get the selected image we get the model name from the value of the model selector and then we create a tensor which is set to the result of our new preprocess image function we pass the function both the image and the model name let's go check out this function all right as just touched on preprocess image accepts an image and the model name it then creates a tensor using tf.from pixels passing the given image to it resizes this tensor to have height and width dimensions of 224x224 and cast the tensor's type to float all of this should look really familiar because we had this exact same code within the predict buttons click event before this code won't change regardless of whether we're using vgg16 or mobilenet now in case later we want to add another model and say we only want the base generic preprocessing that we just covered then in that case we won't pass a model name and we'll catch that case with this if statement that just returns the tensor with expanded dimensions if vgg16 is the selected model then we need to do the remaining preprocessing that we went over together in earlier sections so we have our mean imagenet rgb tensor that we defined last time here and we subtract the mean imagenet rgb tensor from the original tensor reverse the rgb values and expand the dimensions of the tensor we then return this final tensor as the result of this function if mobilenet is selected on the other hand then our preprocessing will be a bit different unlike vgg16 the images that mobilenet was originally trained on were preprocessed so that the rgb values were scaled down from a scale of 0 to 255 to a scale of minus 1 to 1. we do this by first creating this scalar value of 127.5 which is exactly one half of 255 we then subtract the scalar from the original tensor and divide that result by the scalar this will put all the values on a scale of minus one to one but notice the use of broadcasting that's going on with these operations behind the scenes lastly we again expand the dimensions and then return this resulting tensor also in this last case if a model name is passed to this function that isn't one of the available ones already here then we'll throw this exception alright we've made all the necessary code changes let's now browse to our app and see the results we've arrived at our application and we now have the new model selector we added in clicking on the selector we can select either mobilenet or vgg16 let's go ahead and select mobilenet and you can see that loaded pretty fast remember when we loaded vgg16 in previous sections i had to pause and resume the video since it took so long to load but mobilenet was speedy alright cool now we'll select an image click predict and again mobilenet was super fast relative to vgg16 and returning a prediction to us so hopefully this exercise has illustrated the practicality of using mobile nets in situations like these and that's it congratulations for making it all the way to the end give yourself a pat on the back if you're looking to learn more be sure to check out deep blizzard on youtube and the available resources on deeplister.com thanks for watching and i hope to see you later you