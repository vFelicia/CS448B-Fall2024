if you are going to use sql server at any sort of scale you're going to have to think about performance in this course you will learn the essentials of sql server performance you will see how to diagnose what is happening with a slow running sql statement and what strategies are available to make these statements run faster or john aurora teaches this course to follow along with the course you should have some development experience and some experience with sql server this course is divided into the six modules you see on the screen now you're going to learn everything from analyzing sql statements for performance to applying common best practices to ensure the best possible performance i encourage you to leave a comment with the most interesting thing you learn for the benefit of other campers who are watching this course you are watching the video of module 1 getting started part 1 course introduction it is very annoying for every developer to get the result very slow from the queries that we are executed on the database and the slow result leads to the slow application performance which leads to unsatisfied customers in this course i am going to show you what you need to know as a developer about sql server to make sure that your data access code is performing well as a developer i am assuming that you are already comfortable in analyzing and debugging code in languages like c sharp and java but you may be uncomfortable with your level of knowledge of sql server and how things works in the database in this course i will help you feel more comfortable when you are working with sql server especially for the times that you are diagnosting and solving the performance issue and this is the most important part because in almost all system that we build our database is a critical component of the whole system so it is very important to understand that a database like sql server works so that we can make sure that we are getting the most out of it in this course we are going to focus on the performance aspect that we as a developer have control over that is making sql statement run as efficiently as possible and making sure that our application avoids the use of some wellknown data access antipatterns that are common cause for the performance problem this will give you the number of benefits for the application that we build the first benefit is efficient sql statements means that our statements will run faster making our application more responsive to our valuable users second benefit is efficient data access means that our application will be more scalable so we will be able to serve more number of users and transactions on the same hardware and finally we know that many applications are deployed on the cloud either as sql server deployed on a virtual machine hosted in the cloud or by using a platform as a service solutions like sql azure when you are hosting in the cloud you have to pay for the resources that you use so in that case if you have an inefficient sql it doesn't just make your application slow but it actually cost more to run since you are using more resources so you can see there are pretty much reasons why we want to understand how we can access our data in sql server as efficient as possible now there are some arguments that sql server performance is solely the domain of the database administrators but i believe it is important equally for application developers to have a strong grip of sql server performance concepts so let's discuss it in the next video why you as a application developer should learn about sql server performance you are watching the video of module 1 getting started part 2 why developers should understand sql server performance there are many reasons why you as a developer should understand sql server performance one of the biggest reason is that generally the responsibility of the overall performance of the app is on the shoulders of the application development team and we all know that how much application performance is important for the user experience in today's time if our application is slow and unresponsive or can't scale to meet the number of users it need to be then the user is not satisfied with the app and looks for the alternative the second reason is as an application developer we have the view or scope for the whole system along with the knowledge of the application code we also have an idea of the use cases for the application and the data being used stored accessed by the application most important we have a view of how all the pieces of the applications fit together into different feature of our application and how these features are used by our users at the time of tuning sql statements or creating indexes in our database it is always good if we understand the larger context of our application such as what the application is trying to do and how it is trying to do so our knowledge of how the application works is greatly benefited in this case finally there will be no doubt at the times when you need to work closely with the dba team at your company this might be to search an issue from an application or do some database design work or any other reason by better knowing how sql server works you will be able to engage or help your dbas more efficiently you will be able to work more effectively analyzing problems and you will better understand what tools are available and how they can help solve these problems in the next video we are going to see what tools you required to follow along with me you are watching the video of module 1 getting started part 3 tools you need in this course you will see me working in the local copy of sql server 2019 developer edition however the lessons and techniques in this course will also be applicable in sql server 2008 2012 2014 2016 and also in 2017. if there is difference in how you perform something i will point that also if you are using azure sql as your database then there are no worries the tools and techniques you are going to see will also be applicable in that case also if there is difference in how you perform something i will point out that also there is a misconception among many peoples that to really understand sql server performance you need fancy expensive tools but it is not always true in fact all the tools that we are going to use in this course have already been installed on your workstation as they are a part of the sql server client installation if you have already installed sql server management studio you are ready for this course the features that i am going to show you in this course are already present in management studio and you might be unaware of those features i will also show you how we can query performance data out of sql server dynamic management views from management studio and how we can use management studio to set up event traces so by having basic sql server client tool installed and knowing how to use them you are done to accomplish almost all the performance tuning that you need to do you have to download sql server developer edition so that you can install the sample database and follow along i suggest to download from the link shown on screen and i will provide this link in the description box for your convenience press download and run the setup file and follow the instructions after installing sql server you have to download and install sql server management studio which is also called ssms you can download it from the link shown on the screen or i will provide the link in the description box for your convenience on this link click download sql server management studio and run the setup file and follow the instruction now speaking of demos i have a sample database for this course that based on a fictional university that all of the demos are based so in the next video i will show you how you can download a copy of the database and get that imported in your instance of sql server you are watching the video of module 1 getting started part 4 restore the sample database we are going to use the sample database throughout this course and this database contains enough data so that we can do some performance tuning on sql statements and get a feel of how things are going on in the database so first you have to download the simple data and you can download it from the link shown on the screen and i will provide this link in the description box also for your convenience download the appropriate file for the version of sql server that you have installed this is going to download a sql server backup file called students.b we will take this file and restore this in sql server to create a sample database to do this you have to go into sql server management studio and then in the database folder we just going to rightly and we select restore database that's going to bring up the dialog box here and we want to choose the second radio button which is device and then we will click on these three dots here to find our file then we will click add and this will bring up the file explorer here and i have my file out here in the d drive under the folder sql backups select the dot bak file i will say ok everything looks good here so i will say ok again and then this is going to go ahead and start the restore process so now you can see that i have this database and i can access this i have my tables in here i can use this just like any other database on my system so now that we have the sample database installed in the next video i will cover some basic sql server concepts you need to understand around tables and indexes before we dive into analyzing sql statements in the next module so we will go ahead and do that next you are watching the video of module 1 getting started part 5 table concepts i would like to go through a couple of sql server concepts that will come up over and over again throughout the course to make sure that you are familiar with them we have all worked with tables of sql server but many of us may not have given much thought to how sql server actually stores the data inside the table typically table data in sql server is stored in the form of cluster index structure this means the data in the table will be stored in a tree like structure like you can see on the screen at the bottom of this tree we have the leaf nodes of the tree and this is where the actual data rows for the table are stored each of these leaf nodes is what is known as a page in sql server and the page is 8 kb in size depending on the size of the rows a table contains each page will generally store somewhere between 50 to a few hundred rows of data inside of it what is important to know is that the data in these bottom nodes of the tree is actually stored in sorted order and that sorted order is determined what is called the cluster key of the table by default the primary key of the table will be cluster key by having the data in sorted order in this bottom nodes sql server can maintain the tree structure like you see over the top of the data the nodes at the top of the page allow sql server to quickly traverse the tree and find the data it's looking for as we will see in example on the slide in the sample database i have a table named students and the primary key of the student table is the student id column so student id would be our cluster key in this case what that means is that down here in my leaf nodes where the data is stored my student data would be in sorted order by the student id so in this example the first leave node is going to be storing the data rows for students 1 to 100 the second node will be storing the rows for students 101 to 200 and so on in this top two levels of the tree we have a series of pointers that will help direct us to the data so let's say now that we want to find the row for student number 327 we start up here at the root node and it will tell us that we need to go to the intermediate node that is leaf node that is in the second from the left and it is in the leaf node where we will find the data that we are looking for so you see you are able to traverse this tree in just three operations even for tables that have hundreds of thousands or even millions of rows in them by using a tree structure like this sql server is able to find the data it is looking for in a just few operations this tree structure that sql server uses for storing data is very efficient as long as we are looking up data by how the data is stored in the table in this case by the primary key which is a student id column however we often need to be able to look up the data by some other criteria say in this example we want to search for a student by their first name or last name and this creates a problem our data down here in the leaf nodes is sorted by student id but there is no correlation between a student's id number and their name students names will effectively have a random distribution throughout all the data pages at the bottom of the table so we have no idea where a particular name may be so what sql server will do in this case is perform a scan of the structure meaning it will read each and every one of these data pages here at the bottom of the cluster index structure looking for the data that matches your criteria for a table of any significant size though you will have hundreds or even thousands of these data pages in the bottom of the tree most likely many of this would need to be read of the disk so there is a significant i o cost and then sql server will have to use lot of cpu in order to read through each and every row what we can do though is we can create an index on our table though to help sql server finding that data more efficiently so let's discuss the index in the next video you are watching the video of module 1 getting started part 6 index concepts a database index help us quickly find data in a table when we are searching for data by some other criteria other than the cluster key that our table is organized by for the example that we have just talked about searching for students by name we would want to create an index on the students table over the last name and the first name column as you see an index uses a tree structure just like our table but in this case the entries in the index are sorted by the index key which is in this case is the combination of the last name and the first name column and the index does not contain the actual data rows instead in the bottom nodes of the index that are represented and marked with the rp the index will contain row pointers back to the table which indicates where the corresponding data in the table is located if we look in the detail onto one of the bottom nodes in the index it would look something like this we see here that we have the key of the index the last name and the first name of the students and those are in sorted order and then we have the student id value for that student so for example if we have a sql statement with the where clause that you see on your screen where we are searching for this student by name sql server will first traverse the tree structure of the index to find the student id number and then it will traverse the tree structure of the table to locate the data for the student so there are actually two operations that must occur but each of this operation is very efficient and is usually turn out to be much faster to do these two operations traversing the tree structure then to scan through all the rows of a table we also can and often do have multiple indexes on a table in order to cover the different ways our application might search for data in that table so if we think about the students table we might want to find a student by the name as we just talk about or perhaps by their email addresses so in this case we would create two separate indexes on the students table each index being targeted in a way that our application searches for the data in the table what all of this means is that if your sql statement is using some criteria to locate data other than the primary key we are going to want that statement to use an index this way we can take the advantage of the tree structure of the index to quickly find the pointer to the data that we need and this is much more efficient and therefore much faster than having to scan through all the rows in the table from the next video we will start our next module which is module 2 analyzing sql statements for performance in that we will see what our sql statements are doing in the sql server you are watching the video of module 1 getting started part 7 summary of this module in this introductory module we talked about why it is important for you as a application developer to understand sql server performance almost every application written is going to use a database backend in some way and the database is almost always an important component of overall application performance since we as application developers understand what our application is trying to do when it runs a statement against sql server we can be very effective at database performance tuning once we know what to look for because we understand the larger context that all of our sql is running on we also covered the tools that you need to analyze sql server performance then we discuss how to get the sample database downloaded and installed so you can follow along throughout the course and finally we covered some basic concepts around how sql server stores data in tables and how indexes works so that we have a solid understanding of these fundamentals for the future modules in this course here we have successfully finished module 1 getting started in the next module we are going to start looking at how to analyze the performance of individual sql statements you are watching the video of module 2 analyzing sql statements for performance part 1 module introduction so you have a sql statement that is running slow or maybe you just have a statement that you aren't sure is optimized correctly and you need to know what to do next that is what this module is all about understanding how to analyze any sql statement to tell what they are doing and how they are doing to perform in a very quantitative way the first step in this process is to have sql statement give us an execution plan which is the detail steps of how sql server will run our statement when we submitted it to the database after that we will understand how to read the execution plan that sql server gives back to us and how we make sense of what sql server is telling us from a performance point of view we will also see how sql server calculates a cost value for each statement and in fact for each operation within a statement and by looking at these cost values we will tell what statements and operations are relatively more expensive than others we also see how sql server can give us execution statistics on our statements and this means the amount of cpu and io that the statement is consuming what all of this allows us to do is to understand in a very quantitative way how expensive a sql statement is and then when we started tuning that statement we have some very good metrics in place to judge just how much we have improved the performance of this statement we are also going to see little bit about how it is sometimes possible to improve the performance of a sql query just by changing the way that you write your sql and i will show an example of this and finally one of the keys to nla sql statements for performance is to understand how sql server is processing your query so i will briefly talk about the most common operations that you will see in the execution plan and highlight the ones that you especially want to watch out for so let's start right into the example of a sql query that isn't performing quite the way that it should be and see how we work through analysing and fixing this statement in the next video you are watching the video of module 2 analysing sql statements for performance part 2 understanding how sql server will execute a sql statement i have a sql select statement here in my editor window that we are going to use and see the concepts around execution plan and how to analyze a sql statement you can see the output of this statement in the result pane and what this statement is doing is selecting all of the courses that a particular student has taken with a title of the course the number of the credits and the grade that the student received to get this information we need to join three tables we need to query the course enrollments table to get what courses the student has taken and what grade they received and then we will need to join through the course offerings table to get to the courses table so we can get the course number and course title and credits so you might imagine a query like this being run on a web page that produce a grade report for an individual student so i am going to go ahead and run this query and we do see that it takes second to run so maybe you have identified this query as something you need to take a look at and we want to figure out how we can make this query run faster so the first step in that process is to have sql server give us the execution plan that it's using to process the query and to do that we are going to go up to this button on our toolbar with the three boxes and a little icon in it and this is labeled display estimated execution plan and so as you can see if i mouse over over that and the tool tips comes up you can also use a keyboard shortcut of control plus l now clicking this button is not going to actually execute the statement that is in the window it just going to send the statement to sql server to our sql server to return back to us the plan that it would use to run the statement so let's go ahead and do that and so now down here we see in our result pane instead of the data coming back for this query we see a graphical representation of the execution plan what sql server has done is it taken our statement and it broken it down into the individual steps that sql server is going to have to perform when it executes this statement and each of this step is represented by one of this icon that you can see down here in the bottom now you might have a question that how did sql server arrive at this plan well it looked at the tables our statement is accessing and any index that are on those tables and what we are trying to do in our statement and it very quickly compared all of the different options for how our statement could be processed and it determined that this set of operation in this order was the most efficient one when we have a statement that we don't think is performing the way that it should the first thing that we want to do is we want to pull up the execution plan like this and understand what step sql server is performing and if any of these steps are inefficient and time consuming if there is anything we can do about those steps to affect them or affect the overall plan and get them to be more efficient this is just like if you are troubleshooting a slow code in a language like c sharp or java you would go and find out what steps are executed in that program and figure out if any of those steps was bottlenecked or could be executed faster and what you could do speed things up and really the same thing that we are going to do here so in the next video we are going to understand how to read and understand the execution plan that sql server has given us and that's given us some clue about how we can make this query run faster you are watching the video of module 2 analyzing sql statements for performance part 3 reading and interpreting an execution plan for an sql statement let me zoom in on the execution plan to make this plan easier for us to read to read an execution plan we want to work from right to left and from top to bottom you can think of this as though the operations in the right are getting executed first and then as we move to the left this intermediate operations will build upon the already completed operations and eventually we will get to this upper hand corner which represents our complete statement actually this isn't quite hundred percent accurate since sequel server will parallelize parts of our statement for us such that things aren't strictly sequential but for analysis purpose it's easiest if you think of things happening serially starting from the right and moving to the left so we will start out on the right with this cluster index scan operation and we can see that on the second line this operation is occurring against the course enrollment table we can also see on the third line what percentage of the total cost of the query this particular operation accounts for and we see that this is 91 percent in this case so that immediately signals to us that when we get to the point of tuning the statement this is an operation that we really going to want to take a look at in this case i will go through each operations in the execution plan so you can get a feel for how to read these plans that sql server gives you but a lot of times what you will do is you will bring up execution plan and look for the operations that are accounting for the highest percentage of the overall statement cost and immediately focus it on those so this cost percentage can really serve as a quick filter of what you are looking for if i mouse over any operation i will get a popup tool tip that going to give me some additional details information on that particular operation so we see at the top of the tooltip we have the name of the operation again below that we have a little bit of description of the operation that sql server give us in this case we want to remember that a cluster index structure is a typical way that data in a table is stored in sql server so the word scan indicates that sql server is reading the entire cluster index so what that means is that in this operation sql server is reading each and every row in the course enrollments table and that's a table that has 37 lakh rows in it when you are reading execution plans this is actually something that you want to look for is if you have any cluster index scan operations on tables that you know contains a lot of data because what a cluster index scan really means is that you are reading all of the rows in that table and for large tables that of course is going to be very expensive and takes a lot of time so knowing this that we are processing 37 lakh rows in this operation we have a good idea already of why this particular operation is accounting for 91 percent of the cost of this statement now you might ask what rows are we trying to find in this table and that can be answered by looking at the predicate data on this popup which is down here at the bottom so you see our where clause for the statement and what is happening is that we are trying to find out the rows for this particular student but as we just talked about we are having to look through all of the rows in the table to do that we can also get an estimate of how many rows sql server expect to find in this operation and that's given by estimate number of rows value which is shown up here the way that sql server calculate this is because sql server keeps some statistics on each table in the database and these are statistics like how many rows are there in the table and what the rough distribution of data in that table is sql server uses that information in order to form its execution plan and that's also helpful to us here because it tells us that sql server is only expecting to find 45 rows even though we know that it reading a very large number of rows from this table the final item i want to talk about in this popup are these items that are labeled with the word cost and you can see that there are four of them here cost is just a value that sql server used to express how expensive a particular operation or entire statements is compared to another statements or operation as you see here there is a cpu cost which reflects how much cpu this operation is going to use and an i o cost reflecting how expensive the io is for this operation so basically what sql server has done is it's taken the amount of cpu it thinks this operation will consume and the amount of io it's taken those value and its normalized those amount against a scale such that it can add each of those values together and get a single value which represents how expensive this operation is and that's what we see on this estimated operator cost line now we also get a cost for an entire statement which if we go over here and we look at the select in the upper hand left corner we get we see that the estimate subtree cost is 12 here and we can use this value so that we can compare the relative expense of running this statement versus other statement that we have and that helps guide us about what are our most expensive sql statement and which statements that we should tune first so let's go ahead and continue to read this execution plan and understand what is going on in it so we just read the course enrollments table and we got out the records for a particular students in there and then if we come over here to the left we see this step of parallelism if you are unaware of the concept parallelism then i would like to give you a brief explanation about it parallelism is a concept in which the big task is divided into the small task that runs simultaneously and create desired result and finally that partial result of each small task combined into the final big task this concept of parallelism helped to run queries much faster ok now we proceed further in the left and now we can see the nested loop joint and we can mouse over this and from the description it says that we are going to take each row in the upper data set and we are going to prop or investigate the lower data set in order to get that information so this is what is doing our join between these two tables if we go back over here we see that this is the course offering table which is down here on the bottom now this is a cluster index seek operation so it means what sql server is doing is it's actually using the tree structure of the cluster index in order to find the data and as you see here the cost is just one percent and if we mouse over this both of our cost values are very very low for this operation but basically what these three operations here are doing together is finding the data it needs in course enrollments and then in joining that over to course offerings so now we have an intermediate data set of those two table combined in the next step here in nested loops sql server will join the previous data set of course enrollment and course offerings and the result data set after fetching data from the course table and in that it again perform parallelism to generate result so finally we get up here to the select box and this represents our entire statement we already looked at this and this has our entire cost inside of it and also gives us an estimate number of rows now we know that we actually are getting more rows back again the statistic data is just an estimate the sql server has but this is basically the procedure that you use to read an execution plan what you are going to want to do is move from right to left and then when you encounter a join operation you want to read those from top to bottom you can see what each operation is doing and how it fits it with the operations around it and if you want to see more details then you have to mouse over one of the operations so you can get that popup box this is my suggestion to you that you don't have to analyze each and every operation but instead you can just focus on the high cost operations and that helps you understand where you want to focus your tuning efforts in the next video i will demonstrate one additional technique for analyzing a statement and that is how we get the amount of cpu and io that a statement uses when you actually run the statement so we will do that next you are watching the video of module 2 analyzing sql statements for performance part 4 getting execution statistics for a sql statement technique that you will find useful is to use the set statistics commands in management studio so that you can get detailed i o and cpu statistics about the statements being run to do this we are just going to run two commands in our sql editor window set statistics io on and set statistics time on and then i am going to go ahead and execute both of these commands so now when i run a statement i am going to get some detailed information back in the messages tab down here in the bottom in the result pane so just to be clear to get this detailed cpu and io data back you do need to actually execute your statement so unlike in the last segment where we looked at where getting an estimate execution plan didn't actually run the statement this is actually going to run the statement again sql server so you want to be careful with any dml statements like update or deletes because those of course would actually be modified the data in your database in this case i am just running a select statement so i can run that like a normal one so i will go ahead and do that now i will make sure that we get the entire statement here and i will execute this statement so now you see that here is our results and that actually did run a little bit quicker and so the reason that ran quicker is because sql server actually has cached the data that this query needs in memory but as we are going to see if we click over here on the message tab this statement is still pretty inefficient so there are some tuning opportunities present there so let's take a look at that so the first set of items which is right here tells us how long sql server took to create an execution plan for our statement remember the last part where we talk about that when sql server gets a sql statement submitted it has to break the statements up into the individual operations that is going to run and this is the amount of time that it took in this case sql server already has an execution plan cached in memory that it would use so this actually didn't take any time at all in this case next we have some io information for the database object sql server has to read in order to process our statement as we see there were three tables in this case the data needed to be read from and the important number that we want to pay attention to here are these logical reads a logical read is when sql server has to read a page either from memory or disk in order to process the statement so it really helps to indicate how much data is having to be processed in order to run the statement we see here that the course enrollment stable that was over 12 000 logical reads that had to occur and this is because as we saw in the last segment sql server is reading the entire table to find the data that we need each logical read is a page of sql server and remember a page is 8 kb in size so 12 000 times 8 kb we can see that's quite a bit of data that sql server is having to process through here if we move down to the last section it gives us the amount of cpu it took to actually execute the statement and how long that process took now sometimes you will see in this section an amount of cpu that is larger than the elapsed time and what's happening in those cases is that sql server has used some parallel execution for at least part of your statement as such since your cpu on multiple cores we can use more cpu time than what it actually took for the statement to run overall these commands do gives us a good picture of the amount of resources up on the database server that are being used by this statement of course what we are aiming for is that for each one of our statement we want to minimize this number and so that i myself do is i really focus on this number of logical reads a high number of logical reads often indicates a statement is inefficient because it's reading a bunch of data that ultimately it's going to filter out and throw away in some other operation in the statement i also find that when you are reading a lot of data in terms of a lot of logical reads that tends to consume a lot of cpu and ultimately those statements take longer to process so between the execution plan we saw in the last part and this execution statistic we have a pretty good idea of what's going on in this statement so in the next video we are going to actually start working on this and tuning this statement to make it run more efficiently you are watching the video of module 2 analyzing sql statements for performance part 5 improving statement performance by adding an index i bought the execution plan for our sql statement back and by now we realized it this operation here the cluster index scan that is responsible for most of the cost of the sql statement as we discussed this operation is reading all 37 lakh rows which is 12 000 pages from the table just to find out the handful of rows that we need for one student so there is a lot of waste here so what we can do now well actually you have probably noticed that sql server has given us a hint and it's telling us that we really need to create an index on this table so i am going to right click on this recommendation and go to the missing index details so that we can see exactly what is telling us in this case the suggestion is to create an index on the student id column in the course enrollments table and that exactly that we want to do if we look back at our statement we are using the student id column as a filter so we can find just one particular student and if we think about this table in the larger context of our application and how it might be used it is going to be pretty typical use case that we have a student id and we want to find out of all the courses that a student is enrolled in so creating an index on the student id column of the course enrollments table will allow us to quickly search for course enrollments by student id and that's make a lot of logical sense so let's go ahead and get this index created so i just need to uncomment this and then go up here and i will have to give this index a name and this is just how i like to name indexes you actually can give it valid name that you want to and then i am going to go ahead and create this index and so that will take a few seconds here and there we go and so now let's head back to our sql statement and we will get a new execution plan and see how things have changed okay we can see out here that the cluster index scan operation has been replaced with an index seek and then a key lookup so if we go and we look at this here we see the seek predicate now is on the student id so we are using the index to look up the students and notice the cost of this operation it's down there around 0.003 it looks like so this is much much cheaper than the 10.92 value that we saw before now as we talk about in the module 1 part 6 at this course all this does here it lookups just the information in the index and then we have to go to the table to get the actual data for those rows so that's what this key lookup is here and it's doing that with a nested loop join and so this is what actually going up and looking up the data in the table so that we can get the data to come out here so we can see that the rest of the plan looks basically the same if we do look at the overall query though we see that our estimate cost now is around 0.207 right in here remember before it was way up over 12.01 so we have seen a significant saving here so i am going to go ahead and i am going to run this query again and i still have the statistics turned on so we can see the number of i o and the amount of cpu that it's using so i will run this and look at the message tab and you can see here first of all on logical rates before from the course enrollments table we had over 12 000 logical reads and now we are down to 134 only so by using that index we are traversing the tree structure of the index we are doing much much less work before and that's accounting for our statement being more efficient you see down here at the bottom of the sql server execution times we are actually showing zero both for cpu and ellipse time in reality it's probably isn't zero cpu time or zero ellipse time it just below the measurement threshold so again now we have a very efficient statement so i am going to take this result and put them on a slide so we can discuss them i have summarized the result on this slide and as you can see the improvement is dramatic now you might be thinking that the elapsed time only improved by approximately 100 milliseconds but remember i am the only user on my pc running sql server on an actual production system you may have hundreds of user running statement at a time and you may also be dealing with the data sets that are much much larger than what i have in my sample database so on an actual production system the efficiency of the first statement would certainly result in a less scalable system and would probably also result in the longer wait times for users as this inefficient query used up all the resources causing contention on the server i have actually seen this before where a single query that was doing a full table scan brought an entire system down because the query was run often enough to consume all the cpu on the database server which then slow down every other sql statement in the application because those statements were having to wait to get on the cpu so it is really a key to keep all the sql statements in our application running as efficiently as possible especially for statement that are frequently run in our application with that in mind let's look at another sql statement that we need to tune in the next video but in this case the answer is not to add an index you are watching the video of module 2 analyzing sql statement for performance part 6 rewriting sql statements for improved performance i have another statement in my editor window and this statement is designed to get all of the courses section that no students are sign up for so you might imagine a university running a query like this at the start of a semester to find out any courses section that need to be cancelled to accomplish this this query is doing a left join and then looking for any course enrollment records that are null because that indicates that no one signed up for the class we can see the result in the lower pane and we see that we do have a couple of sections then no one has signed up for what we are really interested is the execution plan for this query so let's take a look at that if i mouse over the overall query here we will see that there is an overall cost of 8 and if we look at the execution plan it looks like that cost is being driven by the index seek lookups occurring against the index in the course environments table down here at the bottom so what is happening in these three operations here is that sql server is reading all of the offerings for the courses in a current semester and then through this nested loop join it is probing an index on the course enrollments table ix course enrollments course offering id and looking up how many enrollments there are for that course the issue here is that sql server is looking for all the enrollments for each course when we really only need to know if there is any student enroll in the course since we are looking up all the environments these operations turns out to be pretty expensive making our whole query pretty expensive now in this case the answer is not an index but instead for us to change how we write this query so let's take an alternate approach this version of the query uses the not exist clause to look for any code offerings that don't have any records in the course environments table so let's go ahead and run this version of query and we see we get the same answer as before so now let's take a look at the execution plan and we see if we mouse over this query that our overall cost for this version of this statement is quite a bit lower down around 0.85 and if we look at the entire plan we notice this top operation here and if we mouse over this we see the description tell us that we are just going to return the first few rows from the previous operation and looking down at the bottom we see that the expression is top one so it is actually just getting the first row from the index seek operation which is over here to the right so what this means is that sql server is reading all of the course offerings just like it was doing before but then in the nested loop join what it is able to do is able to prop the index looking for any single row that matches the course offering id number so sql server doesn't have to read the data for every student enrolled in the course it is just looking to see if at least one student is in the course and that's more efficient than getting the data for each and every student android so think of this like you are searching an array of values if you can jump out of this search when you find any value that matches what you are searching for that's more efficient than having to find every value that matches because once you find the first matching value of course you can jump out of the search functions and that's essential what's happening here now i also want to point out that sql server is giving me a create index recommendation here but in this case i am actually choosing to ignore this suggestion the recommendation here is basically sql server wants me to create an index on the course offerings table that contains every column of the table so effectively i will be duplicating all of the data in the table we will talk in the upcoming module about why this is not a good idea but i want to point out this out so that you understand when sql server gives you this recommendation you want to be sure and analyze what sql server is telling you and that it makes sense and don't simply create an index because sequels are made in a recommendation the main point of the two queries we just looked at though is that something the way to improve the performance of sql statement is to rewrite the statement in a different way a sub query can be written as a join or you might change a join to a sub query and as we just showed sometimes using the where exist or where not exist clause can improve a plan because sql server can satisfy the condition with any record that matches the condition instead of having to read all matching records from a table so keep these options in mind when you tune your sql statements in many cases in the modern version of sql server the query optimizer is smart enough to find the best execution plan regardless of how you write your statement but there are few cases where writing your statement a different way will help the query optimizer find a more efficient plan so keep this option in mind so in the next video i am going to summarize the common execution plan operations for better understanding so let's jump ahead you are watching the video of module 2 analyzing sql statements for performance part 7 common execution plan operations we have looked through a couple of execution plan so i want to summarize some of the common operations that you are going to see when you are accessing the data either in a table or in an index these are the typical data access operations you are going to see i am not going to read through each one of these but i do want to point out there are really two different types of operations that are listed here and those are scan operations and seek operations what you need to remember is that a scan operation means that sql server is reading the entire data structure that is acting on so either an entire table or an index and that is of course going to be expensive for any table or index that has a large amount of data in it seek operations on the other hand use the tree structure that the data is laid out in so they can be very efficient to find the data that they are looking for the takeaway is when we are looking at an execution plan and we see scan operations on large table we want to ask why a scan operation is being used and how we can get sql server to change over to using a seek operation this slide summarize the join operations that you will see sql server perform we have seen the nested loop joined quite a bit and the implementation is exactly like the name implies mud join tends to be used more infrequently because the data for both data sets need to be in a sorted order for sql server to use the join if the data for one of the data sets is not already sorted correctly then sql server will have to perform a sort operation first and then perform the join if the data is already sorted mud join tends to be very efficient but if sql server has to first sort the data to use this operation then one of the outer join operations tends to be more efficient the last type of join is the hash match which is sometimes referred to a hash join this is used when sql server needs to join two large data sets and there isn't already some sort of key like an index key that can be used to join the data sets together so what sql server does is it builds a hash table on a smaller data sets on a join key and then iterates through the larger data sets probing the hash table for matching values typically when you see a hash match join it is relatively expensive operation build a hash table in memory takes time and it can take a lot of memory plus as we said sql server will only use a hash match when large data sets are involved so when you see a hash match join you want to ask yourself why a hash match join is required first is that any way to narrow down the data set that are being joined together so that not as much as data need to be joined for example you might look to see if you can include a more restrictive where clause in your statement to cut down on the size of the data that need to be joined second a hash match join indicates there is no existing key like an index key that can be used for join many times this indicates that you are missing an index on one of the foreign relationship between your tables so adding an index may be appropriate again in the next module we will be talking quite a bit about creating good indexes including what columns in a table you want to ensure to index and this usually includes any foreign key columns that your table has there are many operations an execution plan can contain but covering each and every one would take a very long time and lead to some very boring video for you to watch if you do encounter an operation that you need to understand more about that what is provided in the description that sql server management studio gives to you then i would suggest you to take a look at this book on sql server execution plans you can use the url shown on the screen and i will include this url in the description box for your convenience and as you see the ebook can be downloaded for free and you can actually don't ever have to register it this is a direct download link here the book contains a lot of material on how to read execution plan as well as detail information on most of the operation that can be contained within an execution plan as a developer you are probably not going to sit down and read this book from cover to cover but it is a useful reference to have around for the times that you don't quite understand why something is working the way you think that it should and need a little bit more indepth explanation of how an operation work so with that let's go ahead and summarize everything we have learned in this module and wrap things up you are watching the video of module 2 analyzing sql statements for performance part 8 module summary in this video let's summarize what we have discussed in this module about analyzing sql statements for performance when you have a statement you need to analyze the first step you are going to take is to get execution plan of that statement getting the execution plan doesn't actually run the statement but just tell us how sql server will process the statement so we can analyze the plan for any performance bottlenecks once you get the plan you want to read the plan from right to left and when you encounter a join operation you have to read from top to bottom so you analyze the operation at the top of the join first and then the operation on the bottom the statement in each individual operation is going to have a cost value assigned to it and cost is simply a single value that combines together both the amount of cpu and are you required by the statement or the operation with any statement we want to focus our attention on the operation that have relatively high cost because a high cost means that those operations are consuming lot of resources and consequently that will take longer time to complete at a statement level we can use the cost value to compare the relative expense of executing different statements within our application and again we want to focus our attention on the high cost statement our application runs sql server can also give us a detailed execution statistics about each statement we run if we use the set statistics command in management studio what you want to pay special attention is the number of logical reads as each logical read represent an 8 kb page of data a high number of logical reads often represents an inefficient operation that is having to read a lot of data to find the piece of data that is really needed once we understand what operations are causing the statement to be slow we can start to tune the statement to improve its performance we looked at one case where the solution was to add an index as this allowed sql server to use the tree structure of the index to find the data it needed rather than scanning the entire table we also look at the example where we change our sql statement and this is another solution you may want to consider sometimes changing a sub query to a join or may be using the exist keyword helps sql server find a more efficient execution plan you can also do a general evaluation of your statement and see if there is an opportunity to include more selective where clause or possibly change your statement to where it's asking the question you want answer in a slightly different way what getting an execution plan allows you to do to see the individual steps that have to be executed to run your statement and to find which statements are most costly then you can focus on this operation to see what you can do to make these operations more efficient one of the most important tool we have is to create a good index on our table that the statement in our application will use so we will dive into the topic of how to create effective indexes in the next module you are watching the video of module 3 building effective index part 1 module introduction probably the most important step you can take to make sure the sql in your application performs well for which we have to create effective indexes on the table in your database and that is exactly what we are going to cover in this module we are going to start off with a revision on index terminology just to make sure that some definitions are fresh in your mind as we go through this module second we will discuss what column in your database you want to create index and why then we are going to turn our attention to two characteristics you need to pay attention in order to make sure that your indexes are effective and those are the order of the columns in the index and the selectivity of your indexes we will move on and talk about covering indexes and include columns and when those are useful and then we will discuss the effect of using a function in your where clause and how these affect if sql server can use an index for your statement after that we will see the concept of over indexing which is when too many indexes are created on a table which slows down dml statements like insert and updates against the table and then we will wrap up with the discussion about index recommendation provided to you by sql server and some advice on how to interpret those we have a lot of material to cover so let's go ahead and review some index terminology in the next video you are watching the video of module 3 building effective indexes part 2 index terminology refresher before we go ahead i want to take your just few minutes and refresh on some index terminology so these definitions are refresh in your mind as we go through this material when you look at the sql server documentation or any writings on sql server you are going to see the terms cluster index and noncluster index a cluster index is the structure that sql server stores the data in the table typically a cluster index is built on the primary key of the table so the data in the table is arranged in the tree structure organized by the primary key so when we are talking about cluster index we are really talking about the structure that contains the data for the table and how the data is physically stored in a table on disk any other index you create on a table is going to be non cluster index a noncluster index is built over one or more columns of the table which define the index key and then the index also stored a row pointer to where in the table the matching rows for that index key are located in this module when i talk about indexes i am really talking about these non cluster indexes because you are going to need to query the data in your tables by columns other than the primary key and creative effective noncluster index is really the key to achieving good performance when you are doing so there are two types of operations that can be performed against the key a scan operation or a c corporation a scan operation is going to read the entire index to find the matching values it is looking for whereas a seek operation is going to traverse the tree structure of the index in order to find it matches when we say a sql statement is using an index what we really mean is that the statement is conducting a seek operation against the index because this is much more efficient than a scan operation and is really using the index and as it is designed to be used so when you examine operations against indexes in your execution plans you really want to be looking for the seek operations if instead you are seeing scan operations while sql server is accessing the index it isn't really what you want from a performance point of view so you want to examine why you are getting a scan operation and not a seek operation now that we have refreshed on some fundamentals so let's talk about what columns we should be thinking about indexing in our database to deliver good application performance you are watching the video of module 3 building effective indexes part 3 what should i index in my database we saw an example in the last module where we dramatically improved the performance of a sql statement by adding an index to a table by adding the correct index to our database we are going a long way to assure that our application's data access layer will perform well so what we want to do is to understand what tables and columns we want to be indexing in our database from the start so we can avoid any performance issues in our applications there are really two categories of columns we want to make sure to index the first set of columns we want to make sure and index are those that are being used in where clauses of our sql statements whether we are running a select update or delete statement the where clause is how sql server locates the rows we are interested in for our statement having an index on the columns used in the where clause means that sql server will be able to find the rows needed by the statement by using an index seek operation which is very fast and efficient now it is very possible that different statements in our application will contain different where clauses when accessing the same table like we see here in this case we would want to create an additional index to support each statement so in this case we would create a second index over email address you can imagine a search form in your application that allows the user to search for students by their name or by their email address and these two indexes would correlate to those two different use cases now you might also want to search for a student by the student id but in this case student id is the primary key of the students table and sql server automatically creates an index on the primary key columns of the data so we don't need to do that by ourself but for any other way that we are going to be looking up the data on a table we are going to have an index on those column in the where clause that we are using to look the data up the second category of columns that we want to make sure and index are any foreign key columns in our table there are two reasons for this first when we join two tables together sql server will look up the rows in one table and then find the corresponding rows in the second table and to do this sql server is going to be searching for those matching rows in the second table by their foreign key values second when we query data in our application we are often traversing this foreign key relationship to load the data that our application needs we saw an example of this in the last module where we had a query that load the grades for a particular student by their student id if we think about this query in the larger context of the application we probably had some page in our application that the student used a login and once logged in our application kept track of the student's id number during the login session then as we navigate the various pages in our application the student id number will be feed into various queries like the one that we see on our screen to get the data related to the student if you have written any data access code you realize it it is quite common to query across the foreign keys in the table like this is what we are really doing is squaring across parent child relationships so now that we know that what columns we should be looking at indexing let's understand in the next video what makes an index effective so that sql server can actually use it in our sql statements you are watching the video of module 3 building effective indexes part 4 by index column order matters in any database some of our indexes will have multiple columns and the order of the columns in the index determine if sql server will be able to use the index and how sql server can use the index so let's look at an example to understand this i have this index that you see on your screen already created on my applicants table and you see the query i want to run in the editor window as well just below the query is trying to find an applicant by their last name with their state so you might imagine a query like this being run from a sort of search form in one of your application to let an admission counselor finds an applicant what i want to do is to point out that in the index the column order is first name last name and then state and in the where clause of the query i have just the last name and the state specified which is the second and third column of the index so let's get an execution plan and see how sql server will process this statement what i want to focus your attention on is this operation here this is the scan operation against our index and notice that i said scan operation a scan operation like this is actually going to read the entire index so this is not going to use the tree structure of the index to find the matching keys but instead reach each and every key of the index in order to find the matching values so this is like a big linear search through all of the keys of the index what we want to see here is a seek operation because that would mean that sql server is using the tree structure that the index is organized by to find the matching keys and this would be much more efficient the reason why sequel server is using a scan operation rather than a c corporation in this case is because our where clause does not include the first column of the index which in this case is the first name column if you don't include this leading column of the index in your where clause sql server will either not be able to use the index at all or it may have to scan the entire index like we see here which is not very efficient and i will show you some stats in this case to prove it so if we mouse over the query we will be able to see the cost for the entire query and that's about 4.3 and now i have already actually set the set statistics io and time on so if run this query we will get additional execution stats and we can see that this query is taking over 2600 logical reads to process and that includes read both against the table and the indexes on the table so this really isn't very efficient so let's think about what we can do to fix this we could modify our query and make the user include a first name value but this is not really very realistic the reason our user didn't include a first name value is probably because they don't know it so we can't really require them to provide a value that they don't know what we can do though is we can change the order of the columns in our index so let's do that so what i am going to do is i am going to drop the current index and there we go and now i am going to recreate an index and in this case i am going to have the last name column in the first position of this index and the first name column in the second position so i am going to go ahead and create this and now i will take this query and i am going to get the execution plan again we see now that the operation here has changed to a index seek operation and that is what we want because it means that sql server is traversing the tree structure of the index to find the matching index keys so this is going to be a lot more efficient and we can prove that by looking at some of the stats so first we are going to look at the cost here of the overall query and we see now that's down at 1.4 much more efficient than before and if we run this query again we see that now we are only doing 188 logical grids whereas before we are doing over 2600 so again this is much more efficient and the reason why is because when we have those columns in the index in the right order such that sql server can use the index then sql server is having to process a lot less data because it can directly find the data that it needs this has important implications for when we create multicolumn indexes in terms of what order we should put the columns in what we want to do is to think about how our user query the data in the table we are creating the index on and what different combinations of columns are used in the where clauses of these queries then with each combinations of column we want to put the most frequently included columns as the first column of the index the second most frequently used column in the second column of the index and so on so in the first example of this slide since the last name column is always included when doing and search by name it is in the first position in the index first name is frequently included so it is in the second position and only sometimes the state is included so that is in the last position if the user only includes the last name in the query or the last name and the first name that's okay sql server can still use this index because we have the leading column of the index and these columns are selective enough which we will talk about in a moment now our user might also need to search this table for data in a complete different fashion as well in this case by the state and the city that an applicant is from so in this case we need to create a second index on the table because we need the state column to be in the lead position so sql server can use this second index so what you will end up with on your table is an index that is target at supporting each of the different ways that your user of your application access data on this table so it's pretty typical that on the key tables of your application you will probably have from two to four different indexes to support these different query combinations the next step in the process though is to make sure that the index are selective enough so that sql server can use them and we will look at this in the next video you are watching this video of module 3 building effective indexes part 5 index selectivity the second factor that governs how effective an index will be is the selectivity of the index and selectivity is simply a way of saying how many or how few rows there are in the table for each key value in the index for our indexes to be used by sql server and to be effective at speeding up the performance we want our indexes to be as selective as possible that is each value of the index key should only correspond to a few rows in a table or perhaps even only one row let's see why this matters we know that when sql server uses an index it traverses the tree structure of the index to finding the matching keys in the index when it finds the matching key or keys it will read from the index the value of the row identifiers for the rows that matches those index keys in a typical table this row identifiers are just the primary key values of the matching rows then sql server takes this row identifier and look up the actual rows in the table which as we have talked about is usually another tree structure called a cluster index so if the index is selective we will only find a few matching values in the index that we have to look up in the table so we are doing a small number of i o operations overall but what if our index isn't very selective what if for the index key we look up we get back several thousand matches well then we are going to have come over to the table and look up each and every one of those rows so that is several thousand times we are going to have to look up data in this table and since our data is probably randomly distributed throughout the table we are going to end up reading most if not all of the pages in the table anyway so in this case it is actually more efficient for sql server not to use the index and just to read the entire table anyway this way sql server doesn't have to incur the i o of reading the index because the index isn't really helpful in terms of narrowing down what data sql server needs to find when we say that we want our indexes to be selective what we are really saying is that we want them to really help sql server target exactly where the data is that we need to find and to do that we want our indexes to have a high number of unique key values compared to the total number of rows in the table let's jump into management studio and look at some example of this i have created the index at the top of the screen on the students table and i am going to run the query that's at the bottom of the text editor but before i do i am going to give you some statistics about the students table there are about 1 lakh 20 000 rows in the students table and in terms of unique first and last name combinations there are about 1 lakh 7 000 of those so we can tell from those numbers the combination of last and first name that is index is built on is a pretty selective criteria while we do have multiple students that have the same first and last name overall this criteria is pretty unique the calculation that sql server does internally in its statistics to determine selectivity is more complex than a simple ratio but this ratio gives us a good idea of how unique the values in this index are and i find that for everyday use this simple ratio works pretty well so we would expect that sql server would use this index when it executes the query that we see we will get the execution plan to see that indeed that is the case and so we see here that sql server is doing an index seek operation against that index so indeed our index is being used if i mouse over the index seek operation you see that sql server is expecting about 25 values in the index would match this criteria and again sql server is calculating this value based on the statistics it has on the table and on the index so its calculation is more sophisticated than our simple ratio but this is what sql server is expecting to happen then it's going to come down to this key lookup operation and look up each one of those rows and if we mouse over the key lookup operation we again see that value that's about 25 in terms on the number of estimated executions so at this threshold sql survey has calculated that using the index is the right decision because performing the estimated 25 lookups on the table is going to be much faster than reading the entire table now if we run this query we did actually get back 4 rows of the data so this lookup operation actually would be executed 4 times not 25 times the point is though that the lookup operation is only executed a handful of times and we only need to read a few data pages from the table so we are being very targeted in the data that we read which is what an index is supposed to give us let's look at the different situation though when we don't have a selective index and see what sql server does in that situation if i go over to this other tab that i have here we see again that we have another index on the students table that i have created and this index is just over us states run the query that you see here in the bottom of the text editor and what that's going to do is it's going to look up all the students that live in appleton wisconsin remember we have about 1 lakh 20 000 rows of students in the table and in terms of states abbreviation there are actually 52 distinct value in our students table so we have the 50 states the district of columbia and maybe we have some students from puerto rico and another u.s territories if we divide 1 lakh 20 000 rows by 50 we get about 2400 matches per index in this case now this max isn't perfect because of course there are some states like california where proportionately we are going to have many many more students than in the small states like wyoming but for a rough estimated to start with what this is telling us is that for having an index just on the state column it is not going to be very selective because we are actually going to get about 2400 students back so let's go ahead and get the execution plan of this query and see what sql server does with this statement what we see here is that sql server actually isn't using this index on the state column in the query and that is because when sql server look at its statistics we would be getting a lot of matches back from that index and when you read all of those matches out of the index and then go and look up all of those values in the table that actually was going to be more expensive than just reading the entire table direct so sql server not using our index is really telling us that in sql server's eyes the index isn't that helpful because it's not selective enough now you might say that i am not sure i believe that i think that the index would still be more efficient so what i am going to do is i am going to force sql server to use the index on this table and i am going to demonstrate to you that actually the sql server optimizer has made the right choice here so to do this i am going to modify our query to use a database hint and what this hint is going to do is going to tell sql server that i want to use that index and normally i strongly recommend that you avoid using hints in your application as today the sql optimizer included with sql server is very very good and it's almost always come with the correct answer in the most efficient way to execute any statement in 99.9 percent of the cases today the sql optimizer comes with the right answer so the only way i could use a hint in a production application is if it was instructed to do so by microsoft support or i want to demonstrate something like i am going to do now so we have our hint in there and now once again i am going to get our estimated execution plan and we see now a sql server using the index but if we mouse over this index we see that sql server thinks that it is going to get back from the index over 2300 matching rows and if we go down to the key lookup operation once again we are going to have to look up 2300 different values in the table and this is going to turn out to be very expensive in fact then just scanning the entire student table i did run both version of this query prior to recording one without the hint and one with the hint so i could summarize the stats and show them to you which i am doing here what you can see here is that by forcing sql server to use the index that indeed is more expensive and what this is driven by is the fact that by just having the index on the state column that is not selective enough so if we force sql server to use the index it's actually having to read and process more data then if it just read the entire table directly this is an important point many times someone will create the index and they think that the index they created is going to speed up the things but when they look sql server isn't even using the index as we are saying in this case sql server is making the right decision not to use the index because using the index would be more expensive and the reason why is because of selectivity because sql server has to read a large part of the index and then conduct a large number of lookup operations that's accounting for roughly 3500 extra logical ios that have to be read and processed by sql server so the answer here is we need to make our index more selective so let's do that we are going to go ahead and drop this index and then we are going to create a more selective index which is both over state and city in terms of unique combinations for city and state together there are about 13 thousands of rows so we are in a much better place in terms of selectivity for this index so let's get this created now we will go ahead and get the execution plan again for our query and actually we have to get rid of our hint in order to make this work because that index does not exist anymore and we see now with this more selective index sql server is able to use the index because we see this index seek operation here if we mouse over this we see now sql server is expecting to get back about 20 rows in reality it's actually going to get a few more than that but because this index is more selective it's something that can be used by sql server because we are doing a better job of targeting the data that we want one of the takeaways from this is that if we have a column that's not very selective by itself that doesn't mean that we can't use the column in the index it means that we probably need to be using that column in conjunction with other columns in our index and if we need to make sure that the combinations of column together is selected selectivity is really important concept when we start looking at our indexes and the where clauses of our statements so let's look in the next video at some other ways in which selectivity matters you are watching the video of module 3 building effective indexes part 6 like clauses and index selectivity one special case i want to talk about with regards to indexes and selectivity is when you use a like clause in your sql statement first you need to know that if you use this percentage sign which is the wildcard character at the front of the value in the like clause then sql server will not be able to use an index for that column remember the key in index are in sorted order and by having a leading wildcard character in your search value sql server isn't able to use the sorted order of the data to take its advantage so in this case sql server will have to resort to some sort of scan operation either of the entire index or of the entire table when the wildcard character is used somewhere else in the like value of your statement as we see here this has an impact on the selectivity of your index basically only the characters to the left of the percentage sign counts in terms of selectivity for the statement and the index so in this case maybe i am not sure if the last name of the applicant i am looking for is harris or harrison so i am using the wildcard character to search and try to find out the right applicant and i am doing the same thing for the first name but in this case i am only know that the first name starts with the letter t if we get the execution plan for this statement we see that we will still able to use the index because we have specified enough characters in the value that we are searching for but this is still selective enough but let's look at what happened if i trim down the value that we are searching for in the last name to just a couple of characters and i will run the execution plan again and now we see that sql server is doing a scan operation of the table it is not able to use the index and the reason why is because we haven't provided enough information here that we can use the index so really our query just isn't selective enough here the reason i bring this up is because many times i see search pages in applications that don't enforce any sort of minimum requirements on the user in terms of the data that they must supply now sql server will happily run this statement for you they are just going to take a lot of resources and they are going to take a lot of time to finish so selectivity is really a twoway street first we want to make sure that our indexes are selective second we want to make sure that we are including specific enough criteria for our where clause of our sql statements such that our statements are selective and those statement will be able to use the indexes that we have created in our databases this is where we have to use our knowledge of how the user is searching for and accessing data to design good indexes and enforces reasonable constraint on how the user is going to search for data in our application as developers we have a good understanding of how our applications are trying to access our data and we have a good understanding of what the data in our database is so it is really just a matter of combining this knowledge together so we have good indexes in our databases and we are writing sql statements in our application that can take advantage of those indexes in the next video we are going to talk about how using functions in the where clauses of your statement affect sql servers ability to use an index you are watching the video of module 3 building effective indexes part 7 how functions in the where clause affect indexes something you want to be aware of is that if you have a function on the left side or column side in your where clause sql server will not be able to use an index on the table so you see here that i have an index on a table over email address however looking at my query on the left hand side of the where clause where we have the column definition you can see that in this case i am using a couple of different functions to compute the value of just the local part of the email address meaning everything left of the address sign in the email address if you have any sort of function over here on the left hand side of the where clause processing the value in the column sql server is not going to be able to use a regular index created over the column like we see has been created up here if we look down at the execution plan we can see that is the case because we see that sql server is doing a scan of the index not a seek operation so this query isn't going to perform nearly as well the reason why this is happening is because what is sorted in the index are the actual email address values not the computed values so what sql server has to do is to run this function in real time while the query is executing to create those computed values and then it can do the comparison and so to do that you are going to incur some processing cost and not have a very fast query so you want to keep this in mind whenever you see a function like this operating on a column value in the where clause of one of your queries if this is something that you really need to do in your application though there is a technique we can use to address this situation what we are going to do is we are going to add a computed column to the table and then we are going to create an index over that computed column and in doing so sql server will be able to use that index over the computed column so let's do that so first i am going to use alter table statement in order to create the computed column so you can see in this statement i have that same formula with those two functions i could use any functions i want to here even as user defined function the only thing that i need to remember is in order to create an index over this computed column the expression that i put in here has to be deterministic that is it always has to return the same value for the same inputs so let's go ahead and get this computed column created and now that the column is created i actually can query the value back out and i will show those to you if we scroll over here to the right we can see here in our computed column over here on the right and at this point what sql server is doing is it's just calculating the value on the fly when we request this rows out of the table in our query hence the name computed column what we can do now though is we can create an index over this column so let's do that so you see the create index syntax looks just like any other index that we might create what will happen though is when we run this create index statement is that sql server is going to compute all of the values of this computed column for each row of the table and it's going to store those computed values in the index so let's go ahead and get this index created and there we go so now we are going to go back up to our original query and we are going to get the execution plan for that again if i can select the right thing and we see now sql server is able to use this new index that we have created over the computed column and i will mouse over this so you can see that it is using indeed that index and the reason why is because the formula in the left hand side of our where clause matches the formula of the computed column value that the index was created over so what do you want to remember is to be on the lookout for any type of function or expression in your where clauses of your sql statements on that left hand side of the where clause expression because those won't be able to use regular indexes on your table however if you do need to be able to search for data using a computed value then you can use the technique that i have just shown you here where you first create a computed column on the table and then you create an index over that computed column and that will allow your statement to use an index and consequently perform much faster we have covered the fundamental aspects of indexes but there is more to learn about so we will continue our discussion by talking about including columns and covering index next you are watching the video of module 3 building effective indexes part 8 include columns and covering indexes if you take a look at the sql server documentation or some of the index recommendation you get back from management studio you are going to see this index with the keyword include so let's understand what is going on here the include keyword lets you specify that one or more columns values should be stored in the index with the index key but they are not part of the index key so this include columns can't be used by sql server when it searches the index and they don't affect how sql server will lay the index data out in the tree structure they will just have their values stored with the index the reason why this can be useful is because we can create what is called a covering index by using these values normally when sql server uses an index it looks up the index keys in the index to get an row pointer to where the data is in the table and then it has to perform a key lookup operation to get the actual raw data out of the table and we have seen several example of this a covering index is a term used when sql server can get all of the data it needs for a query from an index itself and it doesn't need to perform the key lookup operation there are two factors at play here one notice that we are not using a select star in our query but i am spelling out exactly what columns i want and two notice that all of these columns are in the index either as an index key or as an include column so sql server does not need to go find the actual row in the table to read the data because it has everything that it needs right here in the index and we can see that down here in our execution plan because we noticed that we have the index seek operation but no corresponding key look of operation like we have seen before because in this case we have already all the data that we need if there is just one column that we don't have the data for in the index sql server would have to perform the lookup operation on the table and i can demonstrate that for you if i add the telephone column to this query and now i will get the execution plan for the query again now since telephone is needed by the query and it's not part of the index the index is not a covering index for this query and sql server has to go to the table to get that value if we take out telephone and run our execution plan again we see again we just need the index to fulfill the needs of this query sometimes you will have a query like this where you only need one or maybe two columns that are not in the index and that you are having to go to the table to look up the values for in this case it can make sense to use an include column or 2 so that your index can cover the query and avoid the key lookup operation if you start adding 3 or 4 or more columns as include columns in your indexes though then that is probably a warning that you are going a little too far with include columns in creating covering indexes what you are essentially doing is making another skinned down version of your table but what that mean is that when any of these columns get updated sql server will also have to update the entries in the index to keep it up to date so you are going to incur higher maintenance cost for your index by doing this if you have a query that only needs one or two columns including columns and a covering index can provide a nice additional performance boost just be careful not to go overboard and include too many columns such that you basically now have another copy of your table sometimes to improve performance we need to create numerous indexes on the table which unknowingly leads to over indexing in the next video we are going to discuss about over indexing you are watching the video module 3 building effective indexes part 9 over indexing we have seen how indexes can speed up our sql statements and reduce the number of resources it takes for sql server to run our statements so you might be tempted to think that we should create indexes for every column on every index in our database even if those columns are rarely used in any where clauses or join clauses for our statements unfortunately this isn't quite the case while you want to create indexes that will be used by the sql statements in your application you also want to be careful not to over index your database and by over index i mean create indexes that aren't going to be used by any statements in your or any other application that are running against your database the reason why is that index has a maintenance cost of them and if these indexes aren't helping speed things up in your queries then you are paying this maintenance cost without getting any value from this indexes so let's understand this more when we create an index sql server creates a separate physical structure that contains the data for the index namely the values of the index keys and any include columns that are stored in the tree structure of the index when any sort of dml operation is performed on the table sequel server has to keep all of the indexes on the table in sync with the data in the table so this means that when an insert or delete statement is performed against the table sql server will also have to add or remove an entry from all the indexes on the table when an update statement is running if the values of any of the column in the index are modified then the index has to be updated as well all of this has to be happened when the dms statement executes against the table so sql server can keep all of the data in a consistent state so you want to regard your indexes as investments if an index is being used by statement in your application and is helping speeding up those statements then the cost of the index is well worth it because for most applications the queries that the index helps speed up are run far more frequently than any dms statements against the table so the index is good investment if however you have an index that is never used or is used only rarely you want to investigate why the index is not being used and possibly consider dropping it so how can you tell this well sql server contains a number of views called dynamic management views that gives you access to all types of performance statistics and diagnostics information within the database engine itself we will talk much more about these views in the next module but among these views is one that gives us index usage statistics so we can use a query like the one you see on the screen to look at the values in the user c column and compare it to the user updates column and as long as we have more seeks than updates we are in a good shape otherwise if all we are seeing are values in the user update column this indicates we are paying the cost of the index but not getting any value out of it so it may be appropriate to drop the index there is one more topic i want to discuss and that is the index recommendation that are made by sql server that we have seen popup throughout the module in management studio so we will discuss it in the next video you are watching the video of module 3 building effective indexes part 10 interpreting sql server index recommendations no doubt as you have been watching this module you have been seen a number of index recommendations popup in management studio while i have been demonstrating different concepts and as you experiment with different statements that your application runs no doubt you will see this recommendation pop up then as well it is very useful to have these recommendations from sql server but i would remind you these are just suggestions and you should not automatically create an index because sql server makes a suggestion to do so in my opinion sql server can be overly aggressive with some of its recommendations and if you follow each and every recommendation you would probably end up with the database that was over indexed and therefore had sub optimal performance when it comes to dml statements let's look at an example of this that we have saw in this module earlier i executed this query with selected applicants based on their last name and state i already had an index over the last name first name and state columns which sql server was able to use for this query and you can see though sql server is giving me a recommendation to create a second index over just the last name and state column it is true that this second index would result in somewhat better performance for this particular variation of the query however since there is already an index that sql server is able to use and use pretty effectively i would personally pass on creating this second index the recommendations that sql servers give you are based on the individual query that you are running at the moment so sql server will try to optimize an index for that exact query but what you really want to do is to look at the different variations of your statement and their where clauses and come up with the set of indexes that will work across all the different combinations that you have this means a couple of variations of the statement may not be 100 optimized but again we have to consider the tradeoff around creating multiple extra indexes versus the cost that we will have to pay to maintain each of those indexes but it is to say that you should take this recommendation just as suggestion and then it is up to you to combine your knowledge of your application the data in the database and the suggestion to come up with the right set of indexes for your tables let's summarize what we have learned and wrap up this module in the next video you are watching the video module 3 building effective indexes part 11 module summary having effective database indexes is probably the most important factor in determining the performance of your application data access layer so in this module we talked about how you build effective indexes in your database we lead off by talking about what columns you want to be creating indexes in your database first among this are any columns that you are using in the where clauses of your sql statements as this is the way that your application locates the data that it needs additionally we talk about the need to index the foreign key columns of your table to support any join operation we do as well as the fact that often our application will query data across these foreign key relationships once we know the columns we need to create index on we need to make sure that sql server will actually be able to use the index we create the first thing we need to make sure of that the column in our index are in the right order as sql server will not be able to use the index if the first column in the index is not in the where clause or join condition of the sql statement so that means when we create our indexes we want to put the column that is most frequently used as part of our where criteria at the front of the index followed by the next most frequently used column second and so on we may need to create multiple indexes to support multiple use cases but this is where we can use our knowledge of the application and how our user query the data to determine the best column order for our indexes secondly we want to make sure that our indexes are selective enough the idea of an index is that it helps sql server quickly target a particular piece of data so we want indexes that have relatively few rows per index key because this really helps sql server to target on the right data if we have a column that by itself is not very selective we can still use it so long as we have other column in the index that makes the entire index selective and we make sure that the where clause of our sql statement is also selected oftentimes we need to use the like clause in our statements and in doing so we need to remember that if we use the wildcard character at the front of the like value sql server will not be able to use an index on the column if we have the percentage sign somewhere else in our value sql server will be able to use the index so long as we make sure that we have enough information in the supplied value that it is still selective we also saw that if we have a function in the where clause of a sql statement this will also cause sql server not to use an index and it is this function here on the left hand side or column side of the where clause that is the problem because those computed values are not stored anywhere so sql server has to read all the data for the column and compute these values on the fly if we do have a use case in our application where we need to do something like this and this example of using the sound x function to do a phonetic search might be a good one then what we can do is to create a computed column or column on the table and then create an index over those computed columns and then the sql statement we see here would be able to use the index over those computed columns values we then talk about include columns and covering indexes a covering index is when sql server doesn't have to perform a table lookup to fulfill a query but instead can get all the data it needs directly from the index so if we already have all but one or two of the columns we need for a query in an index we could use an include column to make that index and covering index which provides a little bit of an additional performance boost because sql server can avoid the additional lookup operation the last major topic we discussed was over indexing indexes do a great deal to speed up our statements especially our queries but they do require more maintenance as sql server has to keep all of the index on a table up to date whenever a dms statement is executed against that table so we want to make sure that we only create indexes that are really being used by the statement in our application so we don't have an adverse impact on any dml operation that we may have that's wrap up our discussion of indexes in the next module i am going to show you how to find performance bottleneck in sql server using sql survey built in dynamic management views these views contain all sorts of useful performance information like what statement is taking the longest to execute and if sql server thinks that we are missing an index that would improve performance knowing what information is available in this views and how to access them can really speed up the performance troubleshooting process so i hope you will join me in the next module you are watching the video of module 4 finding performance bottlenecks in sql server part 1 module introduction one of the best feature about sql server is that internally it is always taking data about sql statements that are being run in the database and other events that are happening and we can access this data to give us an overall picture of what is happening inside of sql server with regards to our application for example sql server will give us statistics on all of the sql statements that have been executed in our application database over the last several hours including how many times a statement has been executed how long the statement took on average to run how much cpu would the statements consumed and how much average io needed to be performed with data like this we can quickly identify what statements in our applications are running well and which one we might need to do some performance tuning on now you might think that you need some fancy expensive tools to do all of this and indeed there are lot of tools out there on the market that exposes this information to you in a nice user interface however these tools are just looking at data that is already inside of sql server and all you have to do is to know where to look to find this data and then you can just write queries to get all of the information that you need the source of this data is a series of views in sql server known as a dynamic management views often referred to as the dmvs for short if you go to this page on the microsoft website you can navigate through all of the different dmvs that are available and the data they contain what we are going to do in this module is just concentrate on a few use cases that are the most important to us as developer and the dmvs that we need to solve this use cases we are going to start off in this module by talking about the permission that you need to query the dmvs then we are going to talk about how to use the dmvs to see information like who is connected to sql server and how many resources each session is using then we are going to talk about how we can find what sql statements are currently executing against our server then we will talk about how to find the most expensive statements that are running against our database next we will talk about how we can query sql server to see if it has any recommendation about indexes that might be missing in our database that we need to create and finally we will discuss how we can find if our current index are being used and how often although this gives us a good database level summary of how our application is interacting with sql server and it makes it easy for us to spot any performance bottlenecks that may exist one last item i do want to mention is that all of the queries you are going to see in this module are located on my blog so if you go to the link that you see here on the screen i will also mention this link in the description for your convenience all this query will be there so you don't need to worry about pausing the video and trying to type this query in on your own so with that let's start by talking about the permission we need in sql server in order to access the dmvs and run the queries that we are going to see in this module in order to query the dynamic management views your sql server user needs to have the view server state permission granted to it and unless you are a dba you are unlikely to have this permission by default traditionally sql server's dynamic management views has been thought of as a tool for the dba because they contain information not just related to performance but also about system health and configuration the view server state permission gives you access to all of this information and for some dbs this creates some security concern to give away that level of access it may be possible to get this level of access in your development and test environments but it completely depends on the rules of the company whether this level of access will be available to you as a nondba user in a production database system if you are at a company that is not comfortable granting this access in production to developers you still have a couple of options though first you could have your dba execute the query shown in this module and forward those result to you the second option is that you could work with your dba team to create some views over the top of the sql server dmvs that only exposes a specific subset of the dmv data to you which would relax any security concerns for this module i am going to assume that you do have the view server state permission available to you as i show you these queries so with that being said let's start taking a look at some of the information sql server can provide to us around connections and sessions in the database you are watching the video of module 4 finding performance bottlenecks in sql server part 2 getting information about sql server sessions and resource usage the first query we are going to look at is a query that uses the dm exec sessions dmv in sql server to get a list of all the clients that are connected to our sql server database on my laptop i will get some results from another process that i have running so let's go ahead and run this query right now so we see we get back some information like the session id and the database that the session is currently connected to we also get a status if this session is doing anything at the moment either running or sleeping and we also get when the session was initiated then we get some performance stats about each session the cpu time here is in millisecond memory usage is in 8 kb block and we also have read write and logical leads so if we had a session that are consuming a lot of resources and maybe slowing everyone else down that are connected to sql server we did be able to tell that from this data next we see that we have some information about the client process that owns this session including the host name the program name and the host process id so if we need to know where a session was coming from this would be enough information to help us figure that out now a lot of times what you are going to see in this program name column is something like we see here net sql client data provider and not the actual name of the program that's making the connection for any dotnet program it is this string that is put in by default what you can do though is in the connection string of your program you can include this property application name and then the value that you specify here in the application name property will be used to populate the program name in the column in sql server when you have multiple applications that are using the same database this can be really useful because then you can very easily tell which session is coming from which application the result of this query are useful in two situations one like we said when there is a session that is consuming a lot of resources on sql server second we are also getting a lot of connectivity information here in terms of what clients are connected and how many connection each client is using so for example if i have 4 servers in web cluster i would expect to see about the same number of connections from each one of those machines if i saw that one of those web server didn't have any connection to sql server i would want to go and check the connection string on that machine and also make sure that server was actually receiving in processing web request so there is actually quite a bit that you can tell just by looking at the session information now another question you might want to answer is what sql statement are currently executing in sql server right now so we will look at how to answer that question in the next video you are watching the video of module 4 finding performance bottlenecks in sql server part 3 finding what sql statements are currently executing sometimes we have the need to see what statements are running right now in sql server imagine you have a situation where your application is running very slowly or maybe even appears to be unresponsive and you need to quickly be able to treat that situation so you may be trying to figure out if this is a problem in the application code maybe a web service you are calling is down or maybe you have a query in sql server that takes very long time so what you want to be able to do is look at sql server and quickly figure out what your app is doing in the database and if the problem you are seeing is in the database tire and so we can answer this question with a query like we see on our screen right now this query seems quite long but we will walk through it and we will see that actually it's pretty straightforward so we will scroll down to where we can look and see what dmvs this query is using first we see that what this query does is it joined together the dm exec sessions and dm ex ec request view so that we can get what statements are currently running for each session then we need to bring in the text of this statement and we do that with the dm exec sql text view and the execution plan of the statement is brought in with the dm exec query plan view what this will allow us to do is that if we see a statement that we want to know more about we will be able to just click on that execution plan in the result set and bring that execution plan up so that is pretty useful also in the dm exec request view there is a column called blocking session id and if there is another session that has a sql statement that is blocking this sql statement that column will have a nonzero value so that is what these three views here are about if a sql statement is being blocked by another session we will get all of the information on the blocking statement and blocking session right here in the same query looking at the where clause these lines tells sql server not to return any statement from our current session so basically we are not going to get our dmv query that we are looking back here in our result set also we have this line here which is going to limit the result to the current database if you want to see all of the activities on the server you would just remove this line or command it out let's jump back up to the top to where we can see what data we are getting back we have the session id of the sql server session and we also have the sql statement here these three line here that are labeled sql statement what these are doing is if the statement that running is part of a stored procedure these are getting the individual sql statement so we can look at if we have just a plain old sql statement then the sql statement field and the parent statement field which is down here those are going to be the same but if we are inside of a stored procedure we will get the individual statement here and then parent statement will be the name of the stored procedure you can see we also have some information about the client that is running the statement and this includes the host name the program and the process id of that client so for example if an asp.net web app was running a statement the process id field here would contain the process id of the worker process over on the iis web server then we have some statistics about the statement itself including what time the statement started how long it has been running how much cpu it has consumed and how much io it has performed in most situation these numbers are going to be quite low because most statements run for a couple of hundred of milliseconds if you have a statement though that's taking a long time you would see that reflected in these values and if you can run this query over and over again you would see these numbers continue to increase with each subsequent run so that it could indicate that the statement is not just running a long time but also consuming a lot of resources as it's running on the database server we can see that we are also getting the execution plan back in the query plan field so if we do have a statement running we can look and see what is doing inside of the sql server and then we also have the blocking session information but this will only be populated if indeed our statement is being blocked so let's go ahead and run this query and see what we get so here we see that on my system at the moment that i run this query we have few statements coming back on a real production system you would probably see more statements in more variety than what we are seeing here by just running locally on my laptop we see all of the fields that we talk about so if we want the sql it's right there and then we also see some statistics what i want to show you though is if i move over here to the right we have the execution plan labeled in this query plan field and i can just click on this so management studio popups the execution plan that's being used by the statement and that's pretty useful because we can see the actual execution plan that the statement is running especially we have a performance problem in a production environment because this can clue us in about what's actually happening up on our database server it's useful to know what's running right now but other times we want to be able to get a bigger picture view of performance and what statements are performing well and which ones aren't so we will talk about the query that will do that for us in the next video you are watching the video of module 4 finding performance bottlenecks in sql server part 4 finding the slowest and most expensive sql statement one of the most useful thing you can do with the sql server dms is to have sql server gives you a list of all the statements that your application has run along with the execution statistics for those statement if you are looking for some quick bins for performance in your application this query is a great place to start because this immediately tells you what statements are performing well and what statements you need to target for performance tuning so how does sql server know this every time sql server processes a statement it keeps tracks of the execution plan that it uses and some execution statistics about the statement and store this data in an inmemory structure inside of sql server since this data is in memory what data inside the structure can and does change as new statements are run and old statements are cached out but generally you will have the last several hours worth of data available in the dmv that shows you this view and this is plenty of data to get a strong pulse about what is happening with your application so let's look at this query this query is based around the dm exec query stats view and you can see that we are also pulling in the text of the sql statement and the execution plan for the statement this is handy to pull in the execution plan because if you do find a poorly performing statement we can look at the plan being used and use the skills we have learned in the last couple of modules to analyze the plan for the statement if we look back up towards the top of the editor window we see the data that we are returning we see that we have the sql text like before so we know what statements we are talking about and then we have a number of stats about this statement like the number of times that it's been executed the average amount of cpu per execution and the total amount of cpu across all executions and we have similar measures for both logical i o and alph stamp one thing i do want to point out is that the total rows column here this was added in sql server 2008 r2 so if you are still on sql server 2008 you will want to comment this line out what is useful to do though with this query is that we can take the result of the query and we can sort those result by various parameters with this query right now at the moment i am actually sorting by the average alerts time so that's going to be statements that taking the longest however i could also sort this by the average amount of cpu time per statement execution or the average amount of io to find the most resource hungry statements if you can also be really interested to sort the results by the number of execution so you can see what statements from your applications are getting run most frequently if you see a statement that has a very high number of executions maybe that statement is being executed inside of a loop and you want to rethink how you are doing that data access or maybe the data returned by that statement is essentially static so you might want to consider if you could possibly catch that data so like all the queries that i am showing you in this module take a moment and experiment with these queries in your database because that's really the best way to learn how to use this query and what they can tell you so let's go ahead and run this query so this is what your data is going to look like when it come back and this is pretty straightforward to interpret this data one thing that i want to point out is our dime based measurements which are the cpu time and the ellipse time column this units are in microseconds so we move the decimal places over 6 places to the left we can see that i have got a number of statements here that are taking over 5 seconds and it look like those statements are also are using a lot of cpu so if this was a real database i did be pulling the sql out of here and then i will be scrolling over here to the right where i can see the execution plan for the statements to analyze what is going on so you can see this query is very useful because it can help you immediately pinpoint any statement that are really slowing down your application so you can get those fixed even after i have tuned all of these statements in my application i still like to run this query every week or so just to make sure that nothing has changed or snug up on me in terms of a performance issue next we are going to take a look at how sql server can give us some recommendation on if any indexes are missing in our database you are watching the video of module 4 finding performance bottlenecks in sql server part 5 getting sql servers recommendation on missing indexes we have seen in this course that when we run a sql statement in management studio if sql server thinks the statement would benefit from an index it will give us that recommendation right in management studio sql server doesn't just make this recommendation in management studio but anything a sql statement is run these recommendations are logged to a series of dmvs known as the missing index dmes we don't want to automatically create every index that is recommended in this series of views but looking at this result can help us finding an index that we may have otherwise overlooked the query you see on your screen will join these views together and give us some statistics about the recommendations so let's go ahead and run this query to see what we get the first column in the result set is the table name and this is the table that the index recommendation is for then we have three columns contain information about what columns a potential index would be created across and we see the column names are given to us in a comma separated list within each of these columns mainly you are going to be looking at the column called equality columns because most where clauses and join clauses are based on the equality relationship we also see that we have some stats about why sql server thinks this index would be a good idea this two column here user scans and user seeks represent the number of times that sql server could have used this index had this index exist so for this top row this means that sql server could have performed an index seek operation against this index over 12 000 times had the index existed we also see the average cost of the statements that could have used the index was and this is the cost of the statements without the index existing the next column avg user imp tells us the percentage that sql server thinks that this cost would have reduced if we have had this index finally the last two columns are some simple calculations the first column give us the average cost saving per statement we would have by having this index and the last column give us the total saving we would have across all statements that could benefit having this index so this last value is useful to sort these results to get an idea what the most impactful index to create might be now when you run this query and pull this list up what you don't want to do is just start working your way down the list and creating an index for each row in this result set this is because sql server has a tendency to recommend an index to optimize every individual statement and if you did that you would be in a over index situation which would slow down all your dml statements against your table instead what you want to do is to scan through the result for each table you will able to pick out some pattern of index recommendation across similar columns and then you can combine the information with your knowledge of how your application works to come up with the right sets of index for each table so for example this last 5 recommendations that are in the list are all about the student table and by scanning through the columns that are recommended we see that all of the recommendation includes the last name column so we know that we need to have an index that start with last name we also see that first name is used in three of the recommendation so that is good choice for the second column in our index and then maybe we will choose state as a third column because that's used two of the recommendation as well the point is that you want to look at the different recommendations for a table and extract out the common columns such that you are creating the fewest number of possible indexes on the table in a lot of cases you will have a number of similar recommendations that all have a common subset of columns so you want to create just one index that contains those common columns this way you are creating an index that can benefit multiple statements rather than trying to create individual index for each statement which would be inefficient so when you look at the results of this query keep in mind that these are just recommendations and it's up to you to analyze and combine this recommendation together into the actual indexes that you are going to want to create on your tables now the flip side of missing an index is having index on table that aren't being used so we will look at a dmv query that can help us finding those situation in the next video you are watching the video of module 4 finding performance button lag in sql server part 6 finding indexes that are not being used in the last module we talked about how sql server has to maintain any index you create on your tables by updating an index every time a dml statement is executed against the table so while an index will greatly speed up a query there is a slight performance penalty in terms of executing a dms statements against the table therefore we want to make sure our application is using all of the indexes we have created in our database because otherwise we are paying the cost of maintaining an index which is not being used or not giving any benefit and we can do this with a query that looks like what you are seeing on your screen which uses the dm db index usage stats dmv in conjunction with the sys indexes system views i will run this query and then we can discuss how it works so we see in the first few columns we have information like the table name the index name and the index type remember that an index type of cluster really means a table data as this is how sql server normally stores the data for a table in a cluster index structure what is really of interest to us though are these statistics that are located out at the right of the result set and i am going to expand this column just a little bit so we can read this a little bit better and there we go so user seeks scan and lookups each represent the number of times respectively that the index has been used of those types of operations since sql server was last restarted so for a standard noncluster index that we would evaluate we would be most interested in the value of user 6 as this is the number of times that sql server is using the index as intended by searching the tree structure of the index for matching keys we also see that we have a column named user updates and this value reflects the number of times that sql server has had to update the index because of a dml statement being run against the table now when you run these statements on your database you aren't going to see all zeroes in this columns like i am but you will see actual values what you are looking for here is indexes that have high number of updates and very low usages generally meaning a low number of user seeks operation this means that we are having to update the index more often than its being used so if we find one of these situations we want to investigate why this is happening maybe the index has a wrong column order or it isn't selective enough or maybe something has changed in our application such that we are not querying a table in the same way that we are used to whatever the case you want to understand the situation and see if you can possibly modify that index so it can be used or if you determine that the index really doesn't have a purpose then you want to drop the index so it isn't slowing down your dms statements i do want to give you one word of caution though sometimes you have an index that isn't used by your website or other interactive applications but it is critical to a night batch process in this case you will have only a handful of usages of the index because of course a nightly batch process usually runs just once per night but you don't want to drop these indexes because otherwise there will be serious performance consequences for those batch processes so just make sure that when you are getting ready to drop an index you have really thought through all of the processes that may be using the underlying table and make sure you have not missed an important use case like a nightly or weekly batch process let's go ahead and wrap up our discussion of dmvs and this module in the next video you are watching the video of module 4 finding performance bottlenecks in sql server part 7 module summary in this module we have seen that sql server collects all types of diagnosis and performance information and makes it available to us via its dynamic management views by queering these views we can get an overall picture of what is happening with our application inside of sql server so we can quickly identify problem areas and address them we looked at five different queries that we could use and these queries gave us information on what client had sessions connected to sql server what sql statements are currently executing what statements are taking the longest to run and using the most resources inside the sql server what indexes might be missing in our database and what the current usage of our existing indexes is there are many more queries that could be written and of course the queries we have shown here would all be modified really the best way to learn about dmvs is to try them out on your system and you will be sure to learn something about your application data access layer in the next module we are going to continue talking a more global view of how our application interacts with sql server and demonstrate how you can trace all the sql statements that your application sends to sql server and at the same time collect detailed performance data about them so let's start it from the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 1 module introduction in this module we are going to talk about the ways you can trace all your data access statements inside of sql server and by trace i mean capture each statement your application run inside of sql server along with some performance statistics about that statement why would we want to do this well there are couple of common reasons one as a developer we might want to know exactly what a process inside one of our applications is doing inside of sql server maybe we are trying to debug a process that doesn't seem to be working correctly or maybe we have taken over support of an existing application what tracing allows us to do is to see the exact sql statements the application is running with their values and the order in which they are being run in so we can tell exactly what table are being hit and how they are being used which is really useful for debugging second when we use the tracing capabilities that are built into sql server we can get detailed performance information for each statement that is being executed from our application in the last module we saw how we can get aggregate statistics like average cpu time or average logical reads by querying sql servers dynamic management views as we will see when we trace our data access in sql server we will be able to get statement level statistics for each statement that is run so we can really understand in detail how each statement is performing so we might use tracing in one of our development environments to understand how one of our applications is working or we might use it in a test environment during a load test of our application together detailed information about what parts of our data access are performing and which parts needs to be tuned and finally we could even use a trace in our production environment if we wanted to capture detailed information about every statement that took over a certain amount of time let's say 5 second or accelerate some other performance threshold in sql server there are two different ways which you can trace what is happening inside of the database and which one you choose depends on the version of sql server that you are running if you are running sql server 2008 r2 or earlier you should use a tool called sql profiler and this is provided as a separate gui in the sql server client tools if you are running sql server 2012 or later or using sql azure then you are going to use what are known as sql server extended events to perform your trace going forward sql server extended events are going to be standard and in fact the only way to perform a trace in sql server as the sql profiler tool was depreciated in sql server 2012 you can check out this on microsoft site with the link shown on screen but you will still find sql profiler widely used so we will cover it but known that the trend is moving towards extended events so if you are on sql server 2012 or later you want to be investing your time in learning how to use extended events and probably skip learning profiler regardless of which approach you end up taking running a trace on sql server requires some pretty high level permission in the database which we will discuss when we demonstrate each approach this is because a trace can capture every sql statements and it value that are being executed against the database and clearly there are some security concern in letting someone do this if you have sql server installed on your workstation you will be able to run a trace for any local testing that you do against that local instance for your development and test environments if you will be able to run a trace is going to be vary by company in production you will almost certainly not have permission to run a trace by yourself and will need to engage someone from your dba team to help you run a trace however don't be discouraged by this quite the opposite being able to trace all of your applications sql is a powerful capability and it is important that you have a developer be aware of its capability and what it can do so that when you need to do it you know that the capability exists and you can ask for it so what we are going to do in this module is guide you through using both sql profiler and sql server extended events to take a trace for each method i will show how to set up a trace including what events you can log how you can filter the data you capture and how you log this data to a file for later review i will also discuss what you need to do in order to analyze this data when we are finished you will have a good overview of how to set up a trace in both tools and a good idea of what data you can capture within sql server this will help you understand what tracing capabilities exist in sql server so you know when a trace might be useful in your application development process so let's get started by looking at the sql profiler tool in the next video you are watching the video module 5 capturing what your application is doing inside sql server part 2 setting up of sql profiler trace to run sql profiler you have to go to your microsoft sql server tools in your start menu here you can see on must install it is labeled sql server profiler 18 and you just click on this icon you can also start sql server profiler from sql server management studio for that click on tools menu then the first menu is sql server profiler once you click on the sql server profiler menu the app will open and this is what the screen will look like you want to go up here to the left upper corner and click on the icon that is all the way on the left to start a new dress then as we see here we have a login dialog box popup and what we need to do here is to login to the sql server instance where we want to run the sql trace on also we need to log into sql server as a user that has the altered trace permission otherwise when we click connect here we are going to get an error in this case i am just running a trace on my local sql server instance so i will have a permission to do this but as we talked about in the intro of the module you may or may not have rights to run a trace against different sql server at your company so i will click connect here and then we will presented with this dialog box where we can specify the various options for the trace that we are going to perform so first we will specify a name and then we don't have to specify an output file where we want this data to go but it's a generally a good idea so you can review the data later so we are going to check this box to save to a file and we will specify a file name here and then we get this option to specify a maximum file size which i am going to set 25 mb then i also want to make sure that file rollover is set so as this files fill up profiler with roll them over to a new file now what i want to do is to set the events i want to capture and some filter criteria about what to capture and i do that on this second tab called event selection you can see we already have some default selected and from a developer perspective wanting to trace some sql statements and get their performance information these are actually some pretty good defaults these names are pretty selfexplanatory but if you mouse over one of them you do get some additional description down here in the lower part of the dialog the rpc completed event will fire when a stored procedure completes and the sql batch complete event fires when individual statements completes so you want to make sure to have both of this event checked so you can capture all the sql from your application this is important even if you are not using stored procedure explicitly because the way that some of the database drivers work they actually in some cases will wrap your sql inside of the sp execute sql build in procedure so you just want to make sure that both of these boxes are checked also you want to make sure to check the box labeled text data here on rpc completed because this is what is going to let us see the sql that's been executed as part of that event if you can see that we have a number of other parameters here like cpu and logical reads and i suggest you to leave all of this checked so you get this performance information now there are many other events that you can capture and to see all of these events you check this box here named show all events as you can see this list is quite extensive though most of the items in this list are of more interest to dbs than to us as developers i am not going to go through each and every one of these obviously but there is one that i want to point out and that is in the error and warnings section and the event name is user error messages if you have a sequel statement in your application that throws an error when it runs that error will be captured by this event so this can be useful debugging tool if that error isn't otherwise getting bubbled up to your application logs this event will also log some informational messages that sql server generates but for our purposes we can disregard those informational messages because what we are really interested in for this event is the ability to capture errors from our sql statements i am going to uncheck the show all events check box and you see this gets up back to just the event that we are going to capture and now we want to talk about this other button here column filters the way we have things right now we are going to capture every login logout and sql statements that executed across the entire sql server instance if you are running a trace against your local install of sql server that will probably work fine because you are probably the only user but on a busy server you are going to capture a lot of events that you don't care about this not only makes it harder to sort through all of the data and find what is important to you but it can also have a negative impact on the performance of sql server because you are capturing so many events so we want to filter the data so that we just capturing the data that's of interest to us one of the things you probably want to do is to limit the data you collect to just your application you can do this using the application name filter or by the login name filter assuming that your application uses a unique login for sql server if you go to the application name root remember you need to set this application name in the connection string your application uses in dotnet this is done by including the application name parameter in the connection string and something similar can be done in other languages as well you can also limit your trace to statements that takes a longer time or use a lot of resources with columns like duration cpu or writes to set one of these just click on it and then go over here to the criteria you want to set and in this case i want queries that takes longer than 5 seconds so that's 5000 milliseconds and set the value and now this trace would capture just this long running queries if you want to get rid of this criteria which i do for the demo that i am going to run in just a second here then you just double click on the values so that you can edit the values and delete it and now that criteria will not be used anymore so now we have our trace setup and in the next video we will continue with this and go ahead and run this trace and collect some data you are watching the video module 5 capturing what your application is doing inside sql server part 3 running a sequel profile address so we got our sql profiler trace setup in the last segment and now all we have to do to run the trace is to click this button here that says run now in the background you should know i do have a program running that's generating some synthetic load against my sql server instance so we will be able to see something when we actually run this trace so i will go ahead and click this button and there we go we see this window popup and we can see the events that are being captured by the trace in this window now running the trace interactively like this is fine if you are on a local machine like i am or otherwise maybe a dev server that has a very very light load on it but you don't want to run a trace interactively like i am doing against a busy server because otherwise you could really cause some performance degradation on that server there is a way to convert our setup here to run what is called a server side trace that runs just up on the server itself and captures its data to a file and that's more efficient and i will show you how to do that in just a moment but for now know that this interactive mode is something that you only want to be doing locally or maybe you do it for just a very brief instance of time on a server to make sure you are capturing the right data and then you turn it off that way you can avoid any performance impact i am going to actually go ahead and stop this trace because i think we have captured enough data to review in the time that i have been talking and so to stop the trace we just hit this stop button here in the main pane we see the event that are captured by the stress in the order they occurred if i click one of these i will see the text of the command down here in the lower pane with the values that were submitting for this query i can also see up here in the main grid some of the performance stats about this statement like cpu reads rights and duration so what i can do is i can scroll through here and i can look for statements that were the most expensive ones and then inspect the sql for that statement and any parameters that were used in that statement what is lacking about this user interface is that there is no way to sort or filter the data once it's been taken so you can see i can go up here and click on the column header and nothing happens i am not getting the data to sort here so if you want to be able to slice and dice this data what you are going to do is to save this data to a database table and then you can query it with just some normal sql like any other table so to do this you are going to go up here to the file menu and then down to save as and then select trace table this will prompt you for a database login and what you are going to want is a place and a login that profiler is going to be able to create a table to store this trace data in if that table doesn't already exist and so what i have done on my machine here is i have created a separate trace database called trace data on my local sql server instance and that's where i am going to put this data so i will go ahead and login and then i am going to find trace data in this list for my databases i will leave the schema as dbo and finally i am going to give this table a name and now i will click ok and so that's going to do is to take this data and create that table and insert all of the data into the table and so now if i pop up over to management studio i can go and i can find the database and i will open the tables and there is our table and i will just right click so that i can grab the first thousand rows and there those rows are so obviously i could write sql queries against this table like any other table and i could filter and sort this data however i wanted to now that i have it in a table we mentioned that you could also run a trace as a serverside trace which is more efficient and therefore more appropriate if we are trying to trace access in a server environment rather than on our local machine so let's see how to do that in the next video you are watching the video module 5 capturing what your application is doing inside sql server part 4 running a trace as a serverside trace when you use sql profiler to trace your sql statements what you really want to do is to run your trace as a serverside trace rather than a interactive mode because a serverside trace will consume fewer resources in sql server so what we do is use the sql profiler tool to setup the trace meaning we select the events we want and add in any filters and then we can instruct profiler to give us a sql file of the commands that need to be run in order to run this trace on the server side so let's see how to do this i have the same trace here that we have been using throughout and we have this all set up and then all we need to do is go up here to file export and script trace definition then we will choose from sql server 2005 to 2019 and we are just going to save the sql file somewhere on our local machine so now what we need to do is we need to open that sql file in management studio and i will jump over to management studio and i actually have a version of this file that i saved earlier already opened up here in the window that we can see we see that all the sql file is a series of commands that we are going to run on sql server the only thing that we need to do is to put in a file name of where the data is going to be logged to and we see that we need to do that right here and there is nice long command telling us exactly what we need to do the file path is going to be the path that is going to be up on a sql server so that need to be a directory that exists on a machine that hosting sql server in my case i am running sql server locally so the client and server are one of the same but just remember that part is up on the server so i will go ahead and set this and then we will look through the rest of the file here just to see what else is in here and what all these commands do is they set up the events that we want to capture and any filters so sql profiler has generated all of the correct sp trace set event commands for us so we don't have to look up all of these different codes and ids so all we have to do to start the trace now is to run this script again you will need to be logged in as a user with the altered trace permission in order to run this trace otherwise you are going to get an error the user i am logged in as does have this permission so i am going to go ahead and get this trace started and so now this trace will be running up on the server and collecting data and logging that data to the output file that we specified one important thing to note here is the id number for the stress that you get back down here in the result pane because this is what we are going to need to stop the trace once we have captured all of the data that we need so let's talk next about how we manage this trace so first of all you might want a status on what traces are running on sql server or maybe you forgot that number of the trace that we have just started so let's see how we can find that information out i will go over here to this other window where i have a couple of queries and what you want to do is you want to query the system function fn trace get info and by passing a 0 to this function you will get back information on all traces that currently exist in sql server so i am going to go ahead and run this statement and you see we get this information back but this isn't a very user friendly format trace number one is the system default trace that's always running and then we see the information we have on your trace and we can convert it out our trace id the file and status information from this but it's not very user friendly so if we run this second query here this will give us a little bit friendlier view of the data so let's go ahead and run that and you see that's the same information it is just in a little bit better format for us to consume so now we have our trace that's been running for a while we have collected the information that we need and we want to stop this trace and so how do we do that well what we do is we run a stored procedure name sp trace set status and i have the various use cases of that stored procedure over in this third window so as you can see here you pass in your trace id and a value of 0 if you want to stop the trace if after some time you want to restart the trace you would pass in the trace id and a value of 1 to restart that trace so then after you have stopped the trace if you want to remove the definition of the trace from sql server you would call this procedure again with the value of 2 and what that does is it just remove the definition of the trace in sql server it doesn't go out and delete any trace file that was generated whenever sql server restarts all these definitions get removed this is just a way that we can clean up things when we are finished and keep our sql server tidy so i am going to go ahead and i am going to stop this trace and so at this point we have covered what you need to know about how to run a server side trace now once one of these traces has been taken either by you or maybe somebody on your dba team and you have this dot trc file of the trace data how do you view the data that's in this file that's pretty easy you just open them up in sql profiler so i am going to go back to sql profiler and i will select file open and then trace file and then i just find the trace file i want to open i select and say open in the dialog box and now that file will open up in profiler from here you can review the file in profiler or use profiler to load the data into a table in sql server whatever meets your needs this wraps up our discussion of sql profiler so now we are going to move on and talk about sql server extended events which for our purpose do much the same thing but is really the preferred tool on new versions of sql server for taking traces like this and also gives us the capability to trace sql statements on sql azure also so we will take a look at extended events in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 5 introduction to using extended events for sql tracing if you are using sql server 2012 or later or using sql azure then extended events is the way to go in terms of tracing what sql is running inside of your database sql server extended events use the event tracing for windows or etw framework to trace data and this framework is newer and more efficient in terms of server resources than the older tracing framework using sql profiler extended events also gives you access to many more events within sql server that can be traced and logged and many more options in terms of filtering those events down to only the ones that you want so you want to make sure that you are familiar with using extended events for tracing because this is the direction that microsoft has chosen and will be the only option available on future version of sql server to run an extended event trace the user running the trace is going to need some permission in order to setup and start the trace extended events was actually first introduced in sql server 2008 r2 and if you are on sql server 2008 r2 then your user will need the control server permission granted to it on sql server 2012 or later you will need the alter any event session permission in order to define and run an extended event trace so this is a much more targeted permission however having only alter any event session will only allow the user to create an extended event trace through sql if you want to be able to use the gui within sql server management studio in order to define your extended events capture session as we are going to do here you also need the view server state permission for your user if you will probably have this permission on your local instance of sql server that's running on your workstation and it may be possible to get these access rights in your dev and potentially your test environments in production at a lot of companies you will probably be asking your dba group to define and run these stresses for you because of security concerns around production data in servers but again these tools can be very useful in your development and application support process so we will introduce these tools here so you are familiar with what capabilities exist and you are aware of all the tools that can help you in diagnosting and solving problems within sql server in terms of setting up extended event session to capture events for the onpremises version of sql server and for sql azure about 90 percent of the steps that you need to perform are common between the two platforms there are some small differences though so what i am going to do over the next few clips show you how you would set up extended events to trace your sql on an onpremises version of sql server so let's jump in and create new extended event session to capture some of the sql inside of sql server in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 6 setting up an extended event trace session to set up an extended event session we use sql server management studio what you want to do is in your object explorer go down and open up the management folder then you will see extended events under it so expand that and finally expand sessions and you will see the existing extended event trace sessions defined on your sql server to define a new session right click on the session folder and you see you have two choices here new session wizard and new session and i suggest you select the second choice new session because this is pretty straight forward to do and there is not really a need for a wizard once we do that we will get this window to pop up and if we look on the left we have this four pages of configuration that we will go through to define this capture session so we will start out here on this general page first we want to give our session a name and it is good to make this name descriptive because this is what's going to show up in the list of management studio and it will help identify this configuration in the future so i will put in a name and then we have this drop down of template we can choose from and we can see those template and in this case i am going to choose the one named query batch tracking once you select a template you will see that sql server does gives us a good description of each one of this template all these templates do is preselect some of the events that we can choose from on the next page so they make setting up your trace a little bit faster for us as developers this query batch tracking is a really good starting point and then if we wish we can add more events on the next page finally on this screen we have some options that we can start the trace at server startup or as soon as we create this trace and i am going to leave each one of these boxes unchecked as we will just start our session manually when we are ready to the next phase that we want to look at is the event page so i will click on that and on this page the events that are currently selected to be captured are over here on the right in this list and then this main section in the middle of the page is where we can search for and select any other events of interest you can search for event with the event search box so i am actually going to do that for an event and we see the list is now filtered by my search criteria i am going to click on this event query post compilation show plan and when i do notice that these two controls on the bottom populated given me a description of what this event is and the data fields that this event will capture for this particular event if i scroll through the description i see that sql server is warning me that collecting this event can be very expensive so if i do choose to turn this event on i want to be mindful of that and only collect data for a brief period of time as not to impact the rest of my sql server so that's where the description field can be really useful because sql server lets us know if we are collecting a high resource usage event if we did want to add this or any other event to be collected as part of this trace session we just use these two buttons here to manage what events were selected once we have the list of events then we have to capture we can configure what data we want collected for each of these events and to do that we click on this configuration button that is right here that button take us to this screen where we have our selected events on the left side now and the configuration for each event is over here on the right in terms of the data that you collect for each event that is divided into two sections fields that are globally available to all events which is on this first tab here and event specific fields which are out here on the third tab so for the rpc complete events we see we are always going to collect performance data like the amount of cpu time duration and logical reads and that is good because those are fields of interest to us and also on the rpc completed event you just wanted to make sure that the statement field is checked because this will allow you to see first of all the statement and also the parameter that were passed either to the stored procedure or the parameterized query over on the global field tab there is one field you want to make sure and select and that is the sql text field the reason why you want to make sure this field is selected is because this field gives you just the sql text of the sql statement without the parameter values and we will see in a moment that allow us to group all of the various executions of a particular statement together when we start analyzing our data so you want to make sure that the sql text field is selected and we want to do this both for the rpc completed event and then also for the sql batch completed event and then you can just look through the list of the rest of the global fields and see if there are any that make sense for the trace that you are putting together for example you might want to capture fields like client host name or username to know what application ran a particular statement the last bit of configuration to do on this screen is to configure the filters for the event and we want to do this so we can focus on the data that we are really interested in and just collected the data otherwise we are going to have a lot of extra data that we are going to have to sort through and by focusing in on just the data we are interested in we can lighten the footprint of any additional load that we are putting on sql server to do this we use the filter tab and again we are going to need to do this for each event type that we are capturing we see that we already have a couple of default filters in here and what this filter will do is keep our session from capturing any internal system activity run internally by sql server since that activity is probably not of interest to us what we do want to do though is to limit our session to only capture data from our application so there are a couple of ways that we can do that you just click here to add a clause and now i will click on this middle box and you see all of the criteria that i can filter by so if i am on a shared sql server box that has many many databases i could come down here to sql server database name and now i will enter the value of students in the value field and i will only capture this event sql batch completed in the students database so that will effectively limit me to only sql statements that are run in the database there are many criterias in this list as you see so you could only capture statements that took a certain amount of time or longer or statements that took a certain amount of cpu and this could be useful if you are trying to identify what the real long running statement in your application were you could also put a filter on the sql text field and then there is like operator available so you could use this to capture all of the statements then run against a certain table in your database i will set up the filter for the other two event types offline so we can move on to the last item of configuration we are going to do and that is where we are going to store the data that we capture as part of this extended event session trace we configure that on the data storage page so i will click over there and you can see by default the storage is of type ring buffer and what this is as in memory data structure that acts like a queue so in this case you see we will keep just the last 1000 items we could use this but what you probably really want to do is to process this data you capture to a file so let's do that so i will remove this ring buffer by clicking on the remove button now i will click add and i will choose a type and what i want is an event file and now i will go down here and i will select the name and location for a file and now i am going to say i want a max file of 250 mb for my file and now this data will be captured in the file which is going to be more useful for us the last page of configuration is the advanced page but we will be in a good shape just to accept the defaults there now to create this extended event session that i have it all configured all i need to do is to hit ok button and this session will be created so in the next video what we are going to do is we are going to run this session and we are going to capture some data on our sql server you are watching the video of module 5 capturing what your application is doing inside sql server part 7 running and configuring the display settings for an extended event trace now that we have our trace defined we just right click on the extended event session and say start session and so now this trace session is running and collecting data to the file that we set up for the session to log data to if we want to stop taking data i just right click again and i would say stop session but i am not going to do that just yet we can see here on our menu that we can also have another option watch live data that we can use while the session is running and we can use this to peek at any data that the session is capturing so if we click on this this is going to bring up this window and now as the session captures data in real time those events are going to get displayed in this window we are still capturing and logging all of these events to a file we set up this just gives us a live view of things up here in the top part of the window we see a list of the captured events and if we click on one of these then we get the details of that event down here in the bottom part of the window now having to click on each and every event to inspect its results and to find the data you are looking for in not real user friendly and you are going to quickly tire off clicking through each one of these events trying to find what you are looking for so lets look next at how we can adjust this view of our data into something that is more useful so let's see how we can make our view of the data a little bit more useful here we can add columns to the upper part of this view and turn it into more of a data grid view in a couple of ways first if we have an event selected we can come down here to the data of the event and just right click on an item and select show column in table and then as we see that column now shows up in our table above the other thing we can do is right click on the table header and say choose columns and now we get this dialog where we can choose the columns that we want to see and the order that we want to see them in so i am going to choose a few columns here and i will change this order up a little bit and now when i click ok we can see that i have more of a grid view here in the top part of the pin and that makes things a lot more useful if you want to turn off this bottom part you just go up here and you click on the button here and that will toggle the bottom part of the display on and off and then once you have a column setup that you like what you want to do is you want to save that setup so you can pull it up again anytime you are viewing extended events either a file of extended events or when you are capturing live events so to do that you come up here to display settings and click save as and we will choose a file name and now i will click save and so now when i go back into management studio and i want to bring up this view up all i have to do is go up here to the display settings say open recent and there is our configuration file and we would just click on this so let's move into analyzing some of the data that we have captured in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 8 analyzing extended event trace data when an extended event session produces an output file the file produced is an xel file and you can open this file right in the management studio so you just go to the file open file again and then just find the xcl file on your hard drive that you want to open and click open and so there we go we can view our data now i am also going to get my column definitions so i am going to go up and i am going to do that real quick and now we have got a better view of our data if you watch the clips on sql profiler or you view sql profiler you know that profiler was pretty limited in terms of what we could do with the data but for this extended event files we actually can do quite a bit in management studio with this data first of all i can sort the data just by clicking on the column header i can also filter the data right here in the ui by clicking on the filter button that's going to bring up this dialog where we can add filter clauses so you just click to add a clause and then you can select the criteria the operator and the values if i use the arrow key here to cycle through some of these values you can see all the fields that we are collecting for this event we can now filter on so i am going to select cpu time as a filter then i can select greater than equal to and finally i give this filter a value of 2 lakhs because this value is in microseconds so that will be 2 seconds so this will now show us in the ui all the statements that took two seconds or more of cpu time so filter like this can be very useful in order to narrow down the data set and look at just what you need in addition to a filter on something like duration or cpu time we could also create a filter that was only for a certain time frame statements from a certain host or a filter that would look for a certain table name in the statement text for our event so we could see just the statement that run against a single table so this is pretty useful what we can do with filtering right here in management studio now and we don't have to go through the extra step of exporting our data and getting it into excel or something like that we can do all of that right here in addition if we do filter the data down and we want to share just that filter subset of data with some of our colleagues we can go up here to the extended event menu item then down to export to and you can see that we can export this filter view of the data to a separate xcl file a table in sql server or a csv file so that's really useful when you just want to share a subset of data with someone to analyze one of the other features that management studio gives us is that rather than viewing just a list of raw events like we are now we actually can group our events together and generate aggregate statistics on them so let's do that first i am going to aggregate my data together and i am going to do that by the sql text of this statement which is this column sql text remember this was the global field for our events that we made sure to capture when we set up our trace and we did that because this field contains just the sql of the statement or the stored procedure name and not the parameters so now that we can group our statements by that sql so we see that view of the data here and this basically tells us how many times of sql statement or a stored procedure executed while we are running our trace if we want to see the individual statement we just click on the little plus sign and then we see the individual statements under that grouping we can even sort these statements within the group by double clicking the column headers so of course one of the things you want to look here is if you have a statement that's being executed thousands of times more than any other statements and then ask yourself what that is for example maybe a statement is being executed inside of a loop so your application is going to the database over and over and again and that could be an indication that you want to take a look at that piece of code and refactor it the next thing that we can do is we can get some aggregate statistics for each of this statement and we do that by clicking on the aggregation button and now for all of these other fields that we are displaying we see we can get some aggregate statistics like average or sum so for this example i am going to sum up the cpu and the duration and i am going to click ok and now i see i have the summary statistics displayed along with the statements so what this allows you to do is to run a trace which is going to capture data about a specific process and then you could filter down just to that data either at capture time or after the fact in this video using the filter button as we just saw and then for that process that you have the data for you could figure out not only which statements had the highest average cpu usage or the highest duration which would indicate the longest average weight but you can also see that for all the statements that were running by this process or in this window of time that you have captured which statement has the most total cpu or resulted in the most total duration that the application was waiting so you can really zero in on what a process is doing and where the performance bottlenecks are in that process in terms of the data access so we have seen how to use extended events to capture address with the onpremises version of sql server taking the trace in azure is very similar there are just a few differences so we will cover those in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 9 using extended events in sql azure if you are using sql azure you can still capture what your application is doing inside of sql azure with extended events but there are couple of small differences that you need to account for first of all you are going to want a version of sql server management studio that fully supports sql azure what this means is that you want to get the latest version of management studio and use that to set up your extended event sessions earlier versions of management studio would allow you to connect to a sql azure database and run queries but if you try to create an extended event session you would get an error with the latest version of management studio though the ability is setup and extended event session has been extended to sql azure database as well so you will be able to use the same user interface that we have seen over the last few clips to setup your event capture session in sql azure the second thing you need to do is to set up a storage location of where you can write your extended event trace files to with sql azure being a platform as a service offering you don't have a traditional file system available where you can write files to but what you can do is to write your files to azure storage and this article on microsoft.com tells you exactly how to do that if you read through this article you will see that there are two steps step one is that you run a powershell script to create azure storage container and this part also create the storage access policy that will be used to allow your sql azure instance to be able to write to that container when you run this powershell script you will get an output like i am showing here and it is this information that you will need to feed into the second step of the process step 2 evolves running some transact sql in your sql azure database if you look at the tsql on the microsoft.com page as i am here you see that the sql creates a sample data at the top of the script and then a sample trace at the bottom what is really important though is step number two because this is what gives sql azure the credential it needs to write to the storage location so what is happening here is that you are creating some azure storage putting a security policy on that storage and then giving the credential needed to write to that storage to your sql azure instance the script covered the details of the exact command to do this but big picture that's what's going on once you have the storage setup now we can go into management studio and define an extended event session when you are looking at an azure database you will see that the extended event is under the database itself not out under a management folder like in the on premises version but otherwise we just right click on the sessions folder and say new session just like we did before the selection of the events to capture what fields you want to capture in those events and the setup of any filter is going to be like before the only real difference is that when we get the data storage tab you are going to put the url of your azure storage container in the storage url field with the file name and now this is where your extended event file is going to be created so really the process of creating a trace using extended events on sql azure is very similar to the onpremises version of sql server just make sure that you have the latest version of management studio and get a storage location setup and then you will be able to capture what your application is doing inside of a azure database just like you would be able to do with the onpremises version of sql server let's go ahead and wrap up this module in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 10 module summary there are many circumstances where we would like to be able to tell exactly what our application are doing inside of sql server and by making use of the tracing capabilities built into sql server we can do that this can really gives us a lot of insight into what our application is doing because we can capture the exact sequence that our sql is being executed in so this is really useful for debugging we can also get detailed statistics on the execution of each statement that is run so we can quickly identify which statements are using the most resources or taking the longest finally we get a good idea of how often our application is running different statements against sql server and sometimes simply knowing how often a statement is run tells you something important about how your application is executing or maybe that there's an opportunity to combine multiple sql statements into single statement if we are on sql server 2008 r2 or earlier we will use the sql profiler tool to set up this trace and then most likely we will export the definition to a trace definition in sql so we can run the trace up on the server since that consume fewer resources for sql server 2012 and beyond and for sql azure we will use sql server extended events and we can create an extended event session right in management studio in both cases we are going to select the types of events that we want to capture there are a wide variety of events that you can capture in the database but for us developers usually we are interested in tracing our sql so the sql batch complete and rpc complete events are usually the starting point of us this means that every time our application runs a sql statement or stored procedure we will be able to capture the text of what was run any parameter used for the execution and some detailed performance information like how long the statement took the number of rows written the amount of cpu and how many reads the statement performed we could also supplement these events with other events of interest to us for example we see here events around the start and completion of individual sql statements and around transactions finally we will set up some appropriate filters so we can capture only the data that we are interested in we might filter by the database name or sql server user to look at just the activity from our application but we might even filters by the text in the sql statement just to identify statements that are running against a certain table having good filters like this means there is less data to sort through and it also helps limit the load that we are putting on sql server once we run our trace we will usually end up with an output file of all the events in it that we captured and we can analyze if you are using sql profiler you can just scan through that file or you can export that data into a table in sql server to do detailed analysis if the output is from an extended event session we saw that you could do a lot of your analysis right in management studio itself what all of this gives us is the capability to see step by step what our application is doing inside of sql server so this really helps us understand what our data access layer is doing and where we can mix improvements in the next video we are going to see our last module of this course that is how we can apply common performance practices so let's get start next you are watching the video of module 6 applying common performance practices part 1 module introduction in the other modules of this course we have concentrated on what is happening inside of sql server in this module our focus is a little bit different in that we are going to go through some common performance practices that you want to apply in your application code first we will talk about why you want to make sure to use parameterize sql when you write your data access code then we will talk about if it really is faster to use stored procedures instead of including your sql directly inside of your application next we will talk about commit frequency and how that can affect the performance of your data access layer then we will talk a little bit about object relational mappers or orms first we will talk about how using an orm can make what is happening inside of sql server a little less visible and what you can do about it and finally we will wrap up by talking about the n plus 1 select problem which is commonly encountered performance problem when using orms but fortunately is easy to resolve so let's get started by talking about why it is important to parameterize the sql in your application you are watching the video of module 6 applying common performance practices part 2 use parameterize sql one practice you want to make sure to adopt to use parameterized sql statements in your data access layer what do you mean by this consider the sql statement you see on your screen the statement is using simple string concatenation to dynamically build the sql statement to be run against the database what this means is that every time this data access method is called with different values a different sql string is going to be generated and then sent to sql server to be executed contrast that without data access method that is using parameters like the one you see now on your screen these two values with the address sign in your statement signify sql parameters to sql server and then down here we attach the values that we want to use for those parameters when we submit the statement to the database so every time this method is run the same sql is sent to the sql server and it's just the parameter values that are attached outside of the sql tags that will vary in sql server we will then be able to substitute those parameters in up on the server itself there are big two advantages to this approach of using parameters first this helps prevent sql injection attacks because any value passed in this way are automatically escaped by the database so it doesn't matter if a malicious user has included some code characters or other special characters in here to try and do something inferior those characters are going to be automatically escaped in our statement and that goes a long way into feeding sql injection attacks like this the automatic escaping of these strings also means that you don't have to do anything special in your code when a user wants to legitimately include a single code character in the value they are entering like when they are typing the name o corner or orale secondly using a parameterized sql statement like this will also perform faster and use fewer resources on sql server why is this remember that every time a sql statement is sent to sql server sql server has to determine an execution plan of how to execute that statement what actually happens is that sql server looks in an area of memory known as the plan cache to see if it has already executed the same statement before and therefore already has an execution plan that can be used if yes then sql server can just use this cache execution plan if not sql server will have to do additional work to come up with an execution plan either through a process known as a simple parameterization or through generating a complete new execution plan if you are using parameterized sql statements then once that statement is run the first time every subsequent execution of that statement is going to be able to use the same cached execution plan even when different values are included with a new statement execution if you are dynamically generating your sql statements like we saw in the string concatenation example the actual sql is different each time so sql server has to perform this additional work to get an execution plan each time and that additional work will slow down your application but maybe more importantly make it less scalable the code that you saw earlier in this clip was part of the simple test application that i wrote that run the same sql statement several thousand times with different values and you see the result of the test on your screen the test took 8.5 seconds of elapsed time to run almost 7 seconds of cpu time and we use 79 mb up in sql to store all those different execution plans using parameterized sql statements all of those same sql statement executed in under 1 second and we used almost no cpu time on the database server since we are using the same plan over and over again and as you can see we just use 104 kb of memory in the database to cache our execution plan so we see that using parameterize sql not just makes our statement execute faster but also consumes significant fewer resources on our database server and this is a big deal because most database servers are pretty busy supporting many many concurrent users so being efficient about resource consumption is important if you are using an orm like entity framework entity framework core or hibernate these tools should already be parameterize your sql for you but if you are writing your data access layer using something like ado.net or jdbc or even using a micro orm where you are still writing the sql statements directly make sure that you are parameterizing your sql statements this will provide a performance boost for your application as well as help to keep you safe from sql injection attacks after watching this video you might ask that are store procedure faster than sql in application code so we are going to see that in the next video you are watching the video module 6 applying common performance practices part 3 r stored procedures faster than sql in application code if you are being doing database development for any period of time you have probably heard someone say that you should use stored procedure for your data access layer because stored procedures offers better performance than having the sql in your application code this is one of those statements that's been around a long time and it is partially correct however these statements alone does not tells the full story so let's explain what is going on here in the last section we compared the performance of dynamically generated sql statements to that of parameterize sql and we saw that parameterized sql was not just faster but also used less cpu and memory on the database server the main reason for this is that when using dynamic sql sql server has to do additional work to parse each unique sql statement and come up with an execution plan whereas in the parameterized sql case we don't need to parse the sql text each time and can use the same plan over and over again when people say using stored procedure offers a performance advantage what they are referring to is that using stored procedure will outperform dynamically generated sql from a performance standpoint stored procedure are very similar to parameterized sql statements you pass in some values to a stored procedure these are attached to a statement or statements within the stored procedure that needed them but effectively sql server has already passed the procedure and has an execution plan so you are getting the same benefit as in the parameter sql case when you compare the performance of using stored procedure to using parameterized sql statements the performance is essentially the same there is effectively no difference between the two now this doesn't mean that there aren't good reasons to use stored procedures there are especially when it comes to security and restricting excess of data but from a performance point of view parameterized sql statements and stored procedure perform about the same so the next time you hear the statement that stored procedures offer better performance you will understand this statement comes with a qualifier stored procedures do perform better than dynamic sql but as we see in the last clip we don't want to be writing dynamic sql anyway otherwise stored procession and parameter sequence statements offer the same benefits in terms of performance so as long as you choose one of these strategies you will be fine from a performance perspective in the next video we are going to take a look at commit behavior and how this can affect the performance of your data access code you are watching the video of module 6 applying common performance practices part 4 commit behavior and performance when you run an insert update or delete statement from your application against sql server an implicit commit is issued for each statement because by default the sql server driver is set with it auto commit behavior turned on this is true not just for the dotnet data provider but also for jdbc oledb and odbc as well so consider the code sample that you see on your screen what i am doing is adding all of the courses a student wants to enroll in for a semester and these environments are in a list and the typical student will have five or six courses that they want to enroll in so some pretty simple data access code we just iterate over each time in the list and run our insert command however because of the default auto commit behavior of the sql server driver when this code is run there will be an implicit data comment done each time one of this statement is executed what this means is that up on the sql server sql server has to write an entry to the transaction log to make sure there is a record of the dml operation being performed so there is a write to the transaction log incurred each time one of this statement is executed what we really intended though with this code is probably to insert all of the records at once for the student or none of the records at all so this work should be performed inside of a transaction this is a pretty common pattern to use a transaction like this another example being an ecommerce site inserting items with an order and we want all of those inserted to succeed or none of them so our database is in consistent state so to accomplish this we are going to create a transaction here then we will pass the transaction to the constructor of the sql command to attach it to the command and finally when all the items are inserted we commit to the transaction so this makes much more business sense but as it turns out it also performs better because now we are committing once per student rather than once per enrollment record so sql server doesn't have to write to the transaction log as often i have some results of when i run this program before i started recording that i can show you and you can see here that using a first method when we had to the default autonomic behavior on it took a little bit more than 30 seconds to insert one leg 65 000 rows when i used a transaction which is also more correct from a business sense it took a little more than 23 seconds so about 7 seconds faster on those same 1 lakh 65 000 rows now these numbers of course aren't absolutes and you will likely get somewhat different results on your system but the point is that we want to pay attention to our commit behavior because if we are committing after every dml statement we execute and we really don't need to we are imposing more of a load on sql server and that's going to slow things down so how often should you commit as often as your business transaction dictates and in some cases that may very well be after every statement if not though just make sure that you are explicitly using a transaction so you are controlling the commit behavior and not using the default auto commit behavior of the database driver if you are using an orm most of the time your orm should be handling this property for you however there are some other issues you want to watch out when you're using an orm so we will talk about those in the next video you are watching the video module 6 applying common performance practices part 5 object relational mapper just generates sql over the last few years object relational mappers have become very popular in the development community and it's easy to see why orms allow us to work at higher level of abstraction and they also eliminate the need to write a lot of repetitive data access code to get data into and out of our database thereby helping to increase our productivity as developers on the downside removing us from having to work directly with the database gives us less visibility and less control into how our data access code is executed and so one of the side effects of this is that if you just leave your data access up to the orm and don't think about how that data access is performed you can get very poor performance let's see what we mean by this i have a very simple application here that is using entity framework and the intention here is to search all of the current application who have a gpa over 3.5 and the math sat score of over 700 the code here is perfectly valid and will return the desired results the issue is that in this case there is not an index on any of these columns so this query will end up performing as full table scan which in this case will not result in very good performance having an iq variable we have a fluent interface that is very flexible and it becomes easy to just think about attaching the necessary expression here in code and not to think what needs to happen when the expression is converted to sql and sent to the database so we just need to keep in mind as we use our orm ultimately this expression will turn into sql and all of the rules we have talked about in this course like being properly indexed and having a query with a selective where clause still apply if you have any question about exactly what is happening you can get the actual sequel that is being generated by the orm by tracing your application as we showed in the last module or by using some of the client clientside tracing techniques that are present in most orms then once you have the sql you can generate an execution plan for the statement and apply all other techniques we have learned throughout the course to make sure the statement performs the way you need to it then once you have the sql you can generate an execution plan for the statement and apply all the other techniques we have learned throughout the course to make sure that statement performs the way you need it to in the next video we will look at another issue that frequently occurs with orm and also occurs in other scenarios and that is the n plus 1 query issue you are watching the video module 6 applying common performance practices part 6 solving the n plus 1 selects problem another performance pitfall you want to be on the lookout for especially when using an orm is the n plus 1 selects problem the problem occurs when you have a parent object loading its child data and in loading that child data the data access layer issues a separate sequence statement for each child object that needs to be loaded let's look at an example of this we have a piece of code here and what this code is trying to do is to get the cumulative gpa for a student so it needs to get all of the courses taken by the student their grades in each course and the number of credits each course is worth in order to perform that calculation so this seems pretty straightforward we load a student object and then through the navigation properties on the object we can get to all of the other data that we need to do this calculation what happens though is that initially our students object doesn't have all of the data that it needs so the data for this navigations property are lazy loaded i have an extended event session tracing all of my sql right now and i am going to actually run this program so we can see what's happening and so we see that the program just prints the name and gpa but now i am going to switch over to the trace results so we can see what sql was run i have my trace file open here and i have done a little bit of cleanup to filter out all of the sp reset connections messages that the sql server driver issues when it retrieves a connection from the connection pool so you see right away there are quite a few statements here that are executing and if we group these statements together we actually get a better view of what's going on there are some queries in here that are over headed for entity framework but we are not worried about those what we care about are these two statements that are executed 42 times each and if you expanded out the sequel that in each of those statements you would see that these two queries that are grabbing the course offerings and course records from the database what is happening is that entity framework is able to get all of the courses this student has taken with a single query from a course enrollment table but then in iterates through each of those course enrollment records and to get over to the course records where the number of credits for the class is stored it first has to get each course offering record and then each course record and so one by one it is iterating through this lazy loading each of those objects for however many courses that this student has taken and we can see here this student has taken 42 courses so that means we have 42 unique courses offering records we have issued queries for and then again 42 unique courses records that we also had to load we also had to load some grade records so we could see what each grade is worth but there is only two unique values for those so that's just two more statement in this case but you see the problem to calculate the student's gpa entity framework is having to run 87 individual queries by my count so that's going to be very chatty and not very efficient the issue here is the lazy loading pattern which fetches data just as it needed while this is convenient for many scenarios it's not the right pattern for this scenario because we are having to go back and forth to the database to get all of this database piece by piece what we can do instead is use eager loading to instruct entity framework to load all of these data upfront and then entity framework will create a single query that gets all the data we need rather than issuing all of these little queries so to do that we just use the include method up here when we fetch our student object so i will put that in now and this first line is going to tell entity framework to get all of the courses offering as well as courses for the student and the second line will get all of the grades attached to any of the course enrollments data so when we get our student object we will have all the data we need to do this calculation so i am going to save this file and rerun this program so we can get a second trace of what's happening here and now we will look at that race we can see that this time we have only one statement and in fact if i take this statement here and i look at it and i have copied that in a notepad again so we can do just that we see that this one statement is getting all of the data that we need so in this case this is going to be much more efficient because we are not having to go back and forth to the database many many times this can happen when you are lazy loading objects in an orm as we have just seen but i have also seen this problem in traditional data access code where the code was written in such a way that it was retrieving all the child data for an object in an one by one fashion the point is when you are loading child data like this you want to think about the internals of how that is happening and make sure that you are not iterating over some sort of collection and issuing a sequel statement for each object in that collection because that's going to be very chatty and that's not going to perform very well so think about this whenever you are loading data that's child data of another object and if needed we don't hesitate to use one of the various tracing facilities to make sure that you are not issuing a whole bunch of individual sql statements in order to get the data in the next video let's wrap up this last module of this course you are watching the video module 6 applying common performance practices part 7 module summary in this module we looked at some application practices that you want to make sure and implement for your application to have the best performance possible first we talk about making sure that all of the sql in your application is parameterized sql this not only protects your sql injection attacks but has a performance benefit as well as sql server can reuse the same execution plan over and over making your code run faster and saving resources within sql server we also talked about stored procedure and showed that from a performance standpoint parameters sql and stored procedures are equivalent so you can be comfortable choosing whichever approach best fits for your situation then we talk about commit frequency and how auto commit is turned on in the sql server driver by default in some cases it makes sense to commit after each dml statement but otherwise using transaction and committing however often makes logical business sense not only in more correct for your application it will also perform better as well finally we talked about orms orms are a great tool that enhance our productivity and save us from writing the same data access code over and over again but this additional level of abstraction sometimes makes it hard to understand what is happening inside of sql server so just remember that ultimately everything does have to get converted to a sql statement and then all of the normal rules about selectivity and indexing apply finally if you are loading any child object especially through lazy loading be on the lookout for the n plus 1 select problem basically each child object will load one by one from the database and this chattiness has a tendency to slow down things so be on a lookout for this and use eager loading in those situations where it makes more sense this brings us to the end of the course and i hope you have found your time well spent in learning how sql server works from a performance standpoint and what you can do to make sure your application have optimal performance we have introduced a wide variety of concepts and tools that can help you understand what sql server is doing and what you need to do in order to performance tuning your application the best course of action now is to get some handson experience with these tools by looking at an application and database that you are familiar with because this is how you really learn what to look for and what the data is telling you thank you very much for your valuable time if you have any feedback please feel free to contact me on twitter at icode mechanic and you can also send me the mail on icodemechanic gmail.com you can also comment in the comment section down here at youtube thank you very much again and good luck for your future