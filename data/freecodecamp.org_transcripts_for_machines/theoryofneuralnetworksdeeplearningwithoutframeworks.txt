my talk is called theory of neuro networks and the alternate title would be deep learning without frameworks because i'm going to be um talking i'm going to be talking about the basics of deep learning and neural networks but i'm not going to be using any framework so i'm not going to be using keras or tensorflow and the whole point of this talk is to hopefully help you guys understand how deep learning and neural networks work kind of under the hood because usually when you're working with these things you're going to use a framework that kind of hides a lot of the implementation so hopefully after this talk even if you're you're completely new to deep learning and neural networks you'll have a better understanding of how how those things work and so the first part of my talk i'm going to be going over some of the basic concepts and showing how things work and then i have a second part after lunch where i'm going to bring it all together and do some live coding and show you how to solve an actual problem using just just straight python and showing you how to create a neural network from scratch so first a little bit about me let me get my there we go so my name is beau carnes like like you heard and i am from the united states i'm from the state of michigan and i currently work at freecodecamp.org it's a nonprofit with the goal of providing a free curriculum for people learning software development and so anybody can go on and learn for free so i like a new thing that we're doing over at free code camp is we've translated the curriculum into five different world languages and we just use like this google automatic translate google automatic translator but we are an open source community so we're trying to get people to go in their native speakers and fix the automatic translations and russian is one of the languages so if anybody's interested in trying to help out this open source community and you speak russian you can go in and help with the translation besides that i also have made some courses for this book publishing company called manning publications i saw on the the booth out there i saw some manning publications books and i've created some video courses so i have one called algorithms in motion and then grocking deep learning in motion so the the second one is kind of the grocking deep learning emotion that's what this talk is based off of and the whole course um i mean the whole video course is actually based on this great book by another author named andrew trask so so this i would highly recommend this book for anybody trying to learn deep learning so my my whole my video course is based on the book and this talk is based on the course so let's kind of get into it here um oh first of all i wanted to tell you that i do so this is my personal website karns.cc and then i made a site a page called moscow so all my my slides are on there and all the code that i'm going to be going over during this part of the talk and the second part of the talk so you don't necessarily need to look at it now but if you ever want to refer to that you can and so i'm going to be going into more detail into code than some of the previous speakers so sometimes it's good to see the actual code so deep learning verse versus machine learning so machine deep learning is actually a subset of machine learning so if we go here um you can see that we have machine learning and the sub says deep learning and then the subset of that is artificial intelligence so um machine learning is kind of like what it sounds like machines or computers are are trying to learn something that they were not explicitly programmed for so machines observe a pattern and attempt to imitate it in some way so machine learning is often attempting to take an input data set and transform it into an output data set so let me show you some examples with an input dataset and an output data set so these are all examples that you would use machine learning or even deep learning to try to figure out so if you have pictures of a cat you have the input is pixels and then the output would be a presence or absence of a cat so you input the pixels and the output of the algorithm would be whether or not there's a cat there and the next one would be you could input movies you liked into the algorithm and then on output movies you may like so these are just some different possible use cases you would input words and then would output whether those words indicated that the person was happy or sad and then you would input weather sensor data and the output would be the probability of rain so these are all supervised machine learning tasks and the machine learning algorithm is attempting to imitate the pattern between two data sets in such a way that it can take use one data set to predict the other data set so i'm going to give you a summary of how machine learnings would do that and how a deep learning algorithm would do that let's say you have this data set of pictures and a lot of the pictures are pictures of cats but then some of the pictures are not cats so how would work is you would input all these pictures into into your algorithm and you would also tell it which pictures were cats and which pictures are not cats so um the supervised learning algorithm is going to extract patterns from that data set after it's learned the patterns we can show it a picture and algorithm will hopefully tell us whether the picture is of a cat or not so it takes all the information you give it and then hopefully later you can get a picture of a cat that it hasn't seen before and will be able to figure out whether it's a cat or not so deep learning is a subset of methods in the machine learning toolbox that uses artificial neural networks so let's get into more details about deep learning oh here would be an example is this a cat yes it's a cat so all deep learning and machine algorithm learning algorithms are going to be classified as either supervised or unsupervised learning uh so we take what we know and we transform into what we want to know so that would be a supervised learning we have something that we know to be true and we transform it into what we want to know so like an example would be we know that we know that this picture we know that this is this is a picture of a cat or maybe not a cat and we put into the algorithm to find out what we want to know like whether really it is a picture of a cat and so the um the goal of a supervised learning is is to learn a function that given a sample of data and the desired outputs best approximates the relationship between the input and the output in the data supervised learning is done using prior knowledge of what the output values for a sample should be like in the cat example we know that the that the output is either going to be cat or not cat or maybe you're trying to identify handwriting and you know it's going to be one of the letters of the alphabet you know you know the alphabet you know the the you already know what the output could be for supervised learning so for unsupervised machine learning we don't know what the output could be we're just import and putting a list of data points and we put into the algorithm and then out comes a list of cluster labels so we don't know for sure so the algorithm is trying to figure out and sort the data but it doesn't know what the categories that it's trying to sort into so unsupervised learning groups your data the goal is to infer a natural structure present within a set of data points and we put in a we put in a list of data points and we get the cluster labels out so basically an unsupervised learning algorithm says find patterns in this data and tell me about them so it's just trying to figure out some patterns let's see that was slightly out of order we'll get back to that slide so in this so this would be kind of an example of unsupervised data where you have all this all these data points and your algorithm groups them into these different groups and it basically so the algorithm basically says hey hey data scientist i found some structure in your data it looks like here are some groups in your data here are the groups and then it's up to the data scientist or the programmer to figure out what the groups are it doesn't tell you what the groups are you just kind of have to figure that out after you see the groups so the next thing i want to talk about is parametric versus nonparametric learning and you know pretty soon we'll get to some actual code examples so this will start to make more sense but for for this um you can just kind of imagine a cloud so with uh so this is our machine learning cloud there's two knobs we have supervised an unsupervised knob and a parametric and nonparametric so there's you can kind of switch your algorithm to be either supervised or unsupervised parametric or nonparametric supervision is about the type of pattern being learned and parametricism is the way the learning is stored so the two types parametric and nonparametric parametric is with a fixed number of parameters and nonparametric is possibly an infinite number of parameters so here's kind of an oversimplification a parametric is trial and error and nonparametric is about counting and probability so you could have you're just kind of counting and you don't know how many parameters there's going to be a nonparametric so let's do an example i'm going to go back to this slide so let's say you're trying to figure out you have this square and you're trying to figure out where it should go in this little toy here now again this is a pretty simple example but a some people like a baby may may just jam it into every hole until they find out where it fits so that's kind of like parametric learning now nonparametric learning would be you're counting the sides of the square and counting the sides of the the hole and then you kind of figure out where it'll go on that one so a parametric models tend to use trial and error like just trying everything a nonparametric tends to count to figure out where it should go but most algorithms are supervised parametric learning so from the rest of this talk and the next talk i'm gonna be talking about supervised parametric learning and giving you some examples to go along with that so this is basically trial and error using knobs and so there are three steps uh step one will be predict so um the three steps to supervise parametric learning is the first step is predict so for the predict step you take all your data in this case we're trying to predict if a sports team is going to win a game or not so we input this data like number of toes on the team number of players number of fans we put it into our algorithm which is our machine and then out comes a prediction so in this case our prediction is saying that there's a 98 chance given this data that our team is going to win so step two is going to be compared to the truth pattern so we have our prediction which is 98 chance that the team's gonna win and our truth pattern is we found out that in real life they lost the game so zero percent chance of that they're gonna win so we thought we our original prediction was 98 chance they're gonna win but actually they lost uh so we after and so then after we compare to the truth pattern this is where we are going to learn the pattern so learning um when we're going to just um take the information about how much we were wrong so we look at how much we missed by which is 98 and we also look at the input data at the time of the prediction and then we're going to turn the knobs so these knobs would be weights so with the knob of the wind loss the home away number of toes number of fans so based on all the information we're going to adjust the weights so hopefully the next time the next iteration of the algorithm we can uh be a little closer instead of 98 percent chance we're going to win that's going to that number is going to be lower because we know that in reality they didn't win with that information now um so now that you kind of know the basic steps i know it's kind of hard to understand all that how all that fits together but now i'm going to get into some actual code so hopefully it starts to make sense what i was talking about with the predict compare learn so this is a simple neural network and a neural network is one or more weights which we can multiply by our input data to make a prediction so it's all about trying to make a prediction on on the correct answer based on those weights so in the example that i gave the prediction will be whether or not the team is going to win a game based on the the input debt and the weights so here are our first few lines of code here we i start by just hard coding a weight number now generally your weights are going to start as random numbers which we'll see in the live coding section later but um here it's just a hardcoded so you can kind of see how it works better and we have our neural network function so we can see that our neural network function we just pass in the input and the weights and then it's going to set the prediction to input times weights and it returns the prediction so it's all about just multiplying inputs times weights so this diagram would kind of show to you how it works you input the information here multiplies by the weight of 0.1 and the prediction is the percent chance that the team is going to win so this next section of code we just talked about this part right here but here is our data so we have this array of number of toes this would be like average number of toes for each of the first four games in in the season so it's kind of a silly example but just kind of bear with me here our input is just going to be the first number because we can only run on one number at a time so we're inputting just as 8.5 and then our prediction we just call the neural network function which multiplies the input times the weight and then we get out the predictions so it inputs so we have the 8.5 times the weight so our prediction is that it's going there's an 85 chance that our team is going to win so we can see that the prediction is the input times the weight here so let's talk more about what the input data is so an input data is a number recorded in the real world somewhere so it's usually something that's easily knowable like in the example average number of toes it could be today's temperature it could be yesterday's stock price um so it's just some we're going to input some data that we can see in the real world and then we are going to make a prediction so a prediction is what the neural network tells us given our input data so like given today's temperature we i think the network thinks there's a zero percent chance that people are going to wear swimsuits today or maybe given yesterday's stock price there's uh today's stock pro stock price is going to be 101 0.3 or something like that or given this many number of toes this team is going to win the game so that would be the the prediction and then the network learns through trial and error so in that example it was just it only went through at one time so in a real neural network which we will get to and i'll show you a network with a lot of iterations every time it runs through the code it updates the weight and updates the error and it keeps running through the code over and over to to learn so it first it's going to try to make a prediction then it sees whether it was too high or too low and then finally it changes the weight up or down to predict more accurately the next time it sees the same input so so far we've seen um we've seen the network making a prediction so remember the three steps were prediction and then comparing and then learning so we still need to i still need to show you how you would compare it to the truth pattern and learn and i'll get to that in a minute but i'm going to show you uh one another example before i get to the compare and learn so here i'm going to show you a neural network with multiple inputs and outputs so the first example there was just one input and one output in this example we're going to have three inputs so be like the number of toes per player the win loss record number of fans would be the inputs we're inputting all those things into the network and we're outputting the percentage of hurt players whether or not the team is going to win and whether or not the team is sad so this is just kind of to show that i started with showing a very simple network but they can get more complex you can have any number of inputs you want and you can be trying to predict multiple things you don't just have to predict one thing for the inputs you can predict multiple things so here's all the code now i'm going to kind of zoom in so you can see the different sections so first we're going to talk about the weights so before we just inputted one number for the weight but now that we have multiple inputs and multiple predictions we have to have a matrix so a matrix is just an array of arrays basically so you can see the first line is the no is the the weights for number of people hurt second line number of people that if we won or lost and then percentage of sad and then for kind of each column we have the weights for the number of toes what the win ratio is and the number of fans so then our neural network looks almost exactly the same as before remember before we just multiply the input times the weights and that's what we're doing now except we have this fancy function of vector matrix multiplication before we are just multiplying one number by another number now we're multiplying a a vector which is an array by a matrix which is an array of arrays and so we need an extra some extra code to make it so you can multiply a vector by a matrix and we'll see that in a little bit here so this is our information we're passing our this is our input data for the first four games but for our input we're just going to take the first number of each array so we're just going to do kind of one game at a time and so here's an example of what it would look like for we're passing in our input data and then we have all these predictions that kind of come out of the network so i talked to talk about the vector matrix multiplication function i'm not going to go into this and great detail it's just a way to multiply a vector by a matrix but in real and this function kind of uses this weighted sum function but in real life you're probably never going to write these functions yourself because most people are going to use numpy so numpy gives you just extra extra functions to do math like multiplying matrices and func matrices and vectors so if you this line right here from our neural network and numpy is going to be like this and when i do my live coding i'm actually going to use numpy so you'll see how that works so this dot dot here that's a numpy function and this just means multiply the input by weights and then we get the answer into our prediction here okay so at the end of our network we just set our prediction to equal the the return value of our neural network and we can print our predictions so you can see here here are predictions that we printed out based on this network here so if we go over here you can kind of see everything on the screen at once and then down here what you can see the prediction so that first and you can see how the math is calculated and so our her prediction was 0.55 so we think there's a probably 55 percent of players may be hurt a 98 chance of winning and a 97 chance that people are sad on the team so we can predict three things at once so the next thing i want to talk about is error and gradient descent so this green descent is used while training a machine learning model and it's an optimization algorithm that tweaks its parameters iteratively to minimize a given function to its local minimum so in other words it allows us to calculate both the direction and the amount that we should change our weights so we reduce our error so the whole point of our neural network is trying to get our error to zero so our gradient descent which we'll see the code later it's pretty simple it helps us figure out which direction we should move our weights and how much we should move our weight so when we calculate the error it goes down it's just a way to every time we run the the code every iteration we're going to use our great descent function to get our weights to the right spot so our error goes to zero so we talked about supervised parametric learning in the three steps we talked about so but so far you've only seen predict but now i'm going to show you how to do the second two steps the compare to truth pattern and learning the pattern so for this step two is compared to the truth pattern to measure the error we determine how far our prediction is from our goal prediction and then step three learn the pattern it takes our air and tells each weight how how it can change to reduce it so the whole point is trying to figure out which weights are impacting our air so we can reduce our error to zero and by time when our air is zero that shows us that our weights are accurate because there's no error so now i'm going to show you pressing this button here hopefully i'll change slides just a second oh wait okay so learning is adjusting our weight to reduce the error to zero so that's what we're trying to do to actually that's what our learning is actually doing so here is our one iteration of gradient descent now in a real example you could have like hundreds of iterations or thousands of iterations and when i get to the live coding i'll show you one with a lot of iterations but let's just take it one iteration at a time so first we get our weight and our alpha and our alpha is the simplest way to prevent over correcting our weight updates so sometimes the goal is to get our air to zero but sometimes instead of our air going down it will start going up so alpha is something that's going to help it to always go down and go in the right direction and you'll see later how that fits into our code it's kind of trial and error for what alpha you're going to use but it's usually a multiple of ten so you would start with maybe point zero one then you could try point one and then maybe try one and just try to see which one is gonna make your air go down instead of up so uh let's see and then our neural network is the same as before which is input times weight and here's the diagram so our input the data goes here goes through a weight and then we outcomes our prediction of our wind prediction so now we are going to um take we we're going to do our prediction step so we pass in our information you can see there's only a few things different here which are goal prediction and error so we didn't talk about air in our last code but that's very important because we're trying to get our air to zero so right here we see that we have our number of toes is 8.5 just like before our input is our number of toes now our goal prediction is our winner loss binary which is a one means that the number one means that they won the game so our goal prediction is one so we think that with an input of 8.5 the goal prediction is one that we they won the game so we do our prediction just like before with our neural network we multiply the input times the weight and then our error is going to equal prediction minus goal prediction square so this is prediction minus gold prediction is the raw error it's it's how much we predicted was going to happen minus what we thought was going to happen and then the reason why we square it it forces the raw error to be positive by multiplying by itself so a a negative prediction a negative error when it makes sense so when we square it's always positive and also has added benefit of making large areas larger and small errors smaller which actually helps our neural network to go quicker to the correct uh to the correct thing by squaring it like that so if we run our neural network we do the 8.5 times 0.1 we get the 0.85 but then our error ends up being 0.023 okay so then we have our delta so this is the compare step we're going to calculate our node delta and the delta is how much this node missed so that's the raw error prediction minus goal prediction so so we need to know how much the node missed so we can calculate we can kind of put it into our output so we know uh we know we missed by negative 0.15 because our prediction was 8.85 and our goal prediction was 1. so 0.85 minus 1 is negative 0.15 so weight delta this is kind of where this is where the gradient descent happens so you can see it's actually a pretty simple line of code it's how much this weight caused the network to miss so we want to find out we know that are we missed but how much do we miss we just use the weight delta which is this the input times delta gives us the weight delta and we're trying to figure out how much to update the weight because we're trying to change the weight on every iteration now this actually does three good things for us it helps us with stopping negative reversal and scaling so stopping it's the first effect on our pure error caused by multiplying it by our input so just imagine you're listening to music or you're trying you're playing music on your computer or you're trying to play music and you have speakers so you turn up the speakers all the way but music still isn't coming out because you forgot to hit play on your computer so this would kind of be an example of stopping where it's kind of addresses in our neural network so if our input is zero like you've got to explain your computer it will force the weight delta to also be zero so we don't learn when our input is zero because there's nothing there's nothing to learn and so moving it makes no difference then negative reversal um when our input is positive moving our weight upward makes the prediction move upward but if our input is negative then all of a sudden our weight changes directions now we only want our weight to go in one direction so when our input is negative then moving our way up makes the prediction go goes down we don't want that so multiplying our delta by our input will reverse the sign of our weight delta in the event that our input is negative so this is the negative reversal this ensures that our weight moves in the correct direction um scaling is just when we mol anytime you multiply things together it's just it's either going to get a lot bigger or a lot smaller if the number is less than zero so this is this is good because we want our we want our weight delta to we want big errors to be really big and small to be really small and alpha is going to help it so it doesn't go out of control so we don't want it to get too big that's why we have the alpha which we'll just about to talk about here but first we can see how the weight delta it kind of gets applied to the weight here of negative 1.25 okay so now we're getting toward the end of this we are going to update the weight so when it says weight minus equals that just means weight equals weight minus weight delta times alpha and so this this allows we're going to multiply the weight delta times the weight or we're going to do weight minus the weight delta and we multiply it by the alpha to control how fast the network learns because it can like i said can it can update weights too aggressively but the alpha is going to remember the alpha in this case was .01 so it makes it so the weight doesn't get updated as quick quickly and so it doesn't get too out of control so in this case the new weight is 0.11275 so we're actually getting toward the end of our the first part of the talk i know this is kind of a lot of information all at once especially if you're kind of new to deep learning um so my hope is that in this far part of your the talk i gave you some like good foundations and background knowledge so when we get to the second part of the talk um it will you'll be able to understand how it all goes together so in the second part of the talk i'm basically we're going to use everything we've learned but we're going to use a a real problem and i'm going to live code um a full neural network complete with iterations so you'll be able to see how the weight updates you'll be able to see how the air updates you'll you'll be able to see in the output of the the neural network how the air is going to zero and hopefully all these things we've kind of talked about that you've seen for the first time will start to make more sense when you see how it all works in an actual neural network so we'll also in the second part of the talk talk more about the deep learning part of this so so far i haven't even talked about why it's called deep learning but that's something else we'll be talking about i'm glad to see so many people decide to come back to part two of my talk so this is where i'll be pulling it all together what we were talking about in the first talk and hopefully a lot of things will start to make a lot of sense once you see a full neural network based on trying to solve a problem let's um let's get into it first of all i'm going to explain the problem so this is a problem we're going to try to solve with a neural network and it's kind of um it's it's a real world problem but it's a simplified real world problem so we have enough time to get through everything and so you can understand all the components of it so the problem so we you have to imagine the scenario so imagine the situation where you go to a country you've never been to before and you see a stoplight and you don't know there's three lights but you don't know which configurative configuration of lights means to walk and which configuration of lights mean to stop so um since you're since you're a programmer and a data scientist you decide you want to collect some data so you just sit there and watch and you observe whether people stop or walk so after a little while you you collect this information so you can see this is the first stop light in that configuration of lights with the two on the side on and the middle one off people stopped and then you see this people walked in the the second configuration of lights and then the third was just this one on a person was stopping so you're trying to figure out which configuration means walk and stop but this isn't quite enough information so you keep looking and you collect all this information here and so we're trying to um we collected the information of what we know and what we want to know so what we know is the configuration of the stoplights what we want to know is whether they mean to walk or stop so we're going to use this data and see if our neural network that we're going to develop can figure out which configuration of lights means to walk or stop now this is a simple example so hopefully you can kind of already see a pattern that the the middle light is perfectly correlated to whether you should walk or stop and actually the left and right light don't have anything to do with whether you can walk or stop so when we develop our neural network we're not going to tell it that information we're going to see if our neural network can learn on its own that the middle light is perfectly correlated to the walk and stop data so to make a neural network we cannot just put in these notes or this picture here so we have to convert it to numbers so this is how we're going to convert our street light data to numbers we're going to make if the light is on it's a one if the light is on or if it's off it's a zero so you can kind of see how these correspond to each other so we've got this new data set and we're going to use it to create a neural network to solve it so i'm going to go over to my code editor here now this is something called google colab and google colab a lot it's basically like an online code editor for jupyter notebook so i if you got i don't know if you're familiar with jupiter notebook but it's used a lot for deep learning and machine learning and this is an online version so you don't have to have anything installed on your computer and so it's make things makes things a lot simpler when you're testing things out here so i'll zoom in a little bit more so this is where i'm going to start typing code if you want to follow along you can or you can just look up at the screen but the first thing we're going to do is import numpy as np now let me build okay let's can you guys can see that okay so before we didn't use numpy but now we're going to use numpy and that's going to help with our multiplication remember before i showed you that vector matrix multiplication function we're not going to need that because numpy has that built in and that's the only thing we're going to import the rest of it's just going to be straight python so now we're going to have our weights array and i'm going to do mp dot array and the mp.array means it's a it's a numpy array and this allows us to do um special operations with it like the vector matrix multiplication and stuff like that so a lot of neural networks their weights are going to start off randomly and so i thought this would be a cool chance for some audience participation and i'm gonna see if you guys can give me some numbers they have to be between negative one and one so give me a decimal point between negative one and one so does anybody want to call out a number for our first weight zero seven okay how about 0.7 and then we need another it has to be between negative 1 and 1. anybody else have another number 0.2 okay and then our final number okay so this is just a fun way to get random numbers so you can see later that it kind of doesn't matter what you start your weights out as your network's going to be able to learn and get to the correct weight with just random weights so the next line is the alpha and we talked about that a little bit before the alpha helps make sure your network doesn't get out of control and the whole point of the network is to get our air to zero and but sometimes the air can start going up instead of down to zero and the alpha will make sure it doesn't go go out of control like that so now we're going to import our data that we collected so it's going to do mp.array now i'm going to be just this is going to be a vector or a matrix so that's an array of arrays and i'm just going to be typing in this information i kind of have some notes here but this is directly from the other slide of what this is going to look like so this is kind of the sometimes the tedious part of machine learning and data science is like dealing with the data and inputting data but it's just a part of it is actually just trying to put everything into the network and usually you're not going to be having to type in the data you're usually going to get it from some other source so it may not be you typing in every single number in the data set let's see one and you at some point you may see me type something in wrong that lets it's obvious that's wrong so feel free to let me know like if i spell like a variable wrong or something like that okay so if we go back to our data this should look just like what we typed in right here and now our next piece of information is our walk verse stop array so this is again information we collected in the real world in this example and we're going to do another mp.array and then pass in the array which is going to be 0 1 zero one one zero so again that's the the walk versus if we actually have to go back a slide so this is anytime it's zero or stop is zero one is walk okay there we go and then we can start with our iteration so in the last example before the break i showed you a single iteration but in this example we're going to do an iteration of 40. so i'm going to do four iteration and range 40. now like i had said previously and sometimes you could have hundreds or thousands of iterations and the whole point is that every time we go through an iteration the error gets closer and closer to zero and the way you figure out how many iterations to do is just through trial and error so from like from preparing for the talk and other experiences kind of trying out different things i found out that in 40 iterations this network should be able to get the air to zero but there's no there's not really any magic number you just have to try different things and see the results and see how long it takes to get to zero so now we are going to have a variable called error for all lights and it's going to be set to zero so we we need to figure out the air because the whole point is to get the air to zero so we have to something to collect the air so we're going to be adding to that air later and now i'm going to have another iteration for row index and range and then it's going to just be the length of the stop verse walk array so we're going to do something for every element in this walk versus stop array and the number of elements in the walk for stop array is also the number of arrays or vectors in the streetlights array so we're going to do something for each one that you'll see in a second here so in our last example just like in our exam last example we need an input and a goal prediction so this is going to be pretty similar i'm going to the input is going to be from the streetlights array and i'm going to put the the row index and then the goal prediction is going to be from the walk verstappen and that's going to be the row index so you'll see we're going to go through we're going to iterate through each one because this array corresponds to this number the second array corresponds to the second number the third rate corresponds to the third number okay what part the space after what oh stop verse walk i should definitely oh like is this what we're talking about oh yeah okay thank you i don't know i was looking at it for some reason i was thinking there's a spelling here so it's like what's spelled wrong so yeah thanks for pointing that out so this is a this is an example of what i was talking about i'm gonna make some mistakes but luckily i have the audience here too to help me fix these mistakes here okay so now we have our input and our goal prediction we can make our prediction so let me see if i can can you guys still see down at the bottom of the screen there okay so our prediction now if you remember before when we had our prediction in the first part of the talk i showed that we called our neural network function and our neural network function just multiplied the input times the weight so instead of creating a neural network function we're just going to do the input times the weight but we're going to use numpy so this is this dot function just multiplies input times weights and it can multiply a vector times a matrix so we're getting the the prediction the same ways from before and now we have to calculate the error let's see okay there we can see a little better the error is going to equal prediction minus goal prediction and that's the same as before so that's how we figure how far away our prediction was from what we thought it was going to be is the error and then also oh yeah one last thing we have to take that to the power of two so remember the point of that is to make sure the error is positive if you square it it's going to be a positive number and then also it makes big errors even bigger and smaller areas even smaller and that's actually something we want because it can it will get to the correct answer even faster so now we are just going to add to our error for all lights let's see error for all lights and i'm just going to plus equals error so we're just we're just collecting all the errors into the air for all lights so we're getting an air for each one of the walk for stop array and we're putting them all together because we want the collective error to get as close to zero as possible and then we're going to calculate the delta and that's going to be prediction minus goal prediction this is just how far away our prediction is from our goal prediction and it's considered the raw error this is the same as before and now we are going to calculate the weights now in our previous example the weights equaled um weights minus weight delta times alpha and we're going to basically have the same same thing here but we're going to instead of creating the weight delta variable if you remember from before we're going to kind of put it in line and it's going to be weights minus alpha oh and we need a parenthesis here times input so before the weight delta was input times delta and instead of making that weight delta variable we're going to put it right in line so weights equals weights minus alpha times input times delta so that's the same as before and now we're going to just print out some data so this is actually the whole code and so i'm going to print out some data here and then we'll see what happens so we're going to do we're going to print the prediction so we want to see the prediction variable and i'll just do the string of prediction now i'm going to use some copying and pasting here the prediction will do for each of that iteration but the next two will put in to this next loop let's see if that lines up right oh right here okay thanks i appreciate that okay so now we have um weights and weights and then we'll print out the error and the error okay so we're about to run this code and see what happens but before we run the code i'm going to go back to the slides okay so this diagram demonstrates what our network looks like so we have three inputs and then we pass them through these weights and then we're going to get an output which is the walk versus opt whether the the output is going to show whether those inputs mean to walk or stop so that's the whole point of our network we're trying to learn from do these inputs of the light mean to walk or stop so that's what we're going to do here we go i'll just run this code and it's going to take a little bit but we should see if i didn't make any spelling mistakes we should see some stuff appearing at the bottom here which is the data okay so you'll see at the fir first the the prediction doesn't really correspond to anything at first and the weights are going to be pretty similar to the weights we passed in at the very beginning so you can see here here are the weights so we have 0.6 0.4 negative 0.2 that's pretty close to the weights we started with right here but if we keep going down remember we did 40 iterations so every time it gets the air we're to the next iteration so the air starts at point three we're trying to get as close as possible to zero so if we just keep going down we're not going to look at every single iteration we're going to just kind of go down to the bottom and see what we've gotten to okay so here's the the last the last thing that we want to look at and if you look at the prediction um this number is very close to zero then the second one is close to one then we have one close to zero one one zero so if you remember one this actually is the same as our stop first walk so we have the stop is zero one zero one one zero and that's what the prediction ended up being close to zero one zero one one zero um now that's not necessarily super exciting because we actually gave gave the network that information so of course it's going to be able to figure that out that information because we put it right in the code uh another thing i just kind of showed you first was the error this is a number very close to zero it starts with a three point but then when you see we this is just like a an exponent that makes it really it's a really small number very close to zero so and that's the whole point of our our algorithm is trying to get the air to zero so what's really important to look at is the weights so if you see the weights these weights correspond to each light so the first light the first weight is close to zero the second light is close to one and the third white is close to zero so one means it's perfectly correlated and zero means it's not correlated at all at all and if you remember from our diagram the middle light was perfectly correlated on whether you should walk or stop and the the right and left lights were not correlated at all so there was no point in the code that we gave the program that information but somehow it was able to learn that the middle light was perfectly correlated and the outer lights were not correlated at all now in this example it was something that you can easily figure out just by looking at the data which one's correlated but you can just kind of imagine in a more complex data set you may not be able to figure out how the data is correlated just by looking at it so you use the exact same type of ideas to make an even more complex neural network and so you can actually learn things that that's not that aren't as easy to just learn just by looking at the data so um our network like i said our network correctly identified the middle light by analyzing the final weight positions of the network so to like think about how it identified the correlation it was in the process of gradient descent each training example either is going to assert upward pressure or downward pressure on the weights so on average there was more upward pressure for the middle weight and more more downward pressure on the outer weights so they got to the correct numbers so this is pretty cool and now i'm going to show you something even a little a little more complex so the next part of the talk instead of live coding i'm going to show you some code that's already written that's pretty similar to the code we already had but i'm going to introduce a few more concepts first of all i'm going to tell you that some of the concepts could even be a whole talk to themselves so i'm going to do my best to like quickly overview the concepts but some of the things you'll just kind of get a taste of so you can learn more about on your own later but the reason why i'm going to show you one more program is to talk about the deep part of deep learning so nothing i've showed you so far is actually deep so let me deep refers to one or more hidden layers between the input and the output layers of a neural network so if i go to my code here so our input layer is right here our output layer is right there there's nothing in between but if i look we look at this example this is a true deep neural network because we have our input layer that looks the same there's three on the input and one on the output for layer two but now there's this middle layer which is a hidden layer and since there's one or more hidden layers this is a deep neural network so you may be wondering why would you even need a hidden layer if our other network was able to correctly predict the correlation between the lights because we're going to use the same example from before so if you remember the data the middle light was perfectly correlated with whether you should walk or stop but if the middle light wasn't so perfectly correlated our network would have had a much harder time figuring out how the how the input was related to the output so like for instance if our training data looked like this our other network wouldn't have worked so well if you look at this so the the numbers in red are the the light information whether the lights are on or off and then the numbers in black are the stop or walk information so you can see this first the first three up here means to walk and we have walk stop stop and one thing you'll notice is that the output data is not perfectly correlated with any of the rows so this is where you need to have a hidden layer um what is because since our input data set does not correlate with our correlate with our output data set we're going to use our input data set to create an intermediate data set that does have correlation with our output and this intermediate data set is our hidden layer it's almost going to be like two neural networks it's going to be from layer 0 to layer 1 is going to be one network that runs just like before and then the so the output to the layer 0 to layer 1 network will become the input to the layer 1 to layer 2 network so let me show you okay so can you guys see this all right even in the back okay so a lot of this is going to look pretty similar here we have we're importing numpy as mp and then this line right here just make sure we're going to be using random numbers in this code and that line just makes sure the rand.seed makes sure every time we get random numbers it's the same random numbers so a lot of times when you're dealing with neural networks you're using random numbers for the weights but to make sure the output is always the same even though you're using random numbers you can make sure that every time the code runs the random numbers were the same random numbers that you used last time you ran the code and it helps for like comparing uh to make sure the code the results are exactly the same so this is called a this is an activation function it's the relu function we're going to pass our data into it it's going to return x or return it's going to return what the data we passed in if x is more than zero or return zero otherwise now this is a it's called an activation function because it activates some input by returning the input and deactivates other input by returning zero this code is going to show a three layer neural network and by turning any middle network off whenever it would be negative we allow the network to sometimes have the correlation from various inputs and sometimes not have the correlation so it's impossible for a two layer network to sometimes allow this input to correlate and sometimes not allow it to imp correlate so it adds more power to three layer networks so this sometimes correlation is important for things to work correctly and this is one of the areas that like i was saying could get a whole talk to itself as activation functions and how they work and why but i'm just going to kind of leave it at that for now and that the activation is functions important to make it sometimes correlate and sometimes not correlate the the relu two derive function is next and uh this returns one when the output is more than zero or zero otherwise and that's the slope of a reload function or its derivative and you'll see later how that's important so the next two this is just inputting the data just like we did before we do have the dot t that's a numpy thing it's the transpose operator and it just um changes so you have a matrix which is going to have like uh almost like an x and y coordinates like the so it's a if it's a three by four matrix the transpose operator would make it a four by three or if it's a one by four it'll become a four by one and when it's when you're multiplying a matrix and a vector it matters um which kind of direction the the matrix is if that makes sense and so the alpha is same before again sometimes you have to kind of try out different things to figure out what it's going to work and the hidden size is four so if we go back to our diagram you can see that our middle layer has four nodes so that's what this hidden size equals four is there's four nodes and then here is our weights so okay let me get back to that code here i accidentally did the keyboard the mouse shortcut for going back so if we go to our weights here before this is where we had the audience participation and i had people shout out different weights but now we're going to get truly random weights because we have a lot more weights this is going to be a matrix of weights and the size of the matrix like the matrix is similar to this this is a matrix right here up top a 3 by 4 matrix and this is going to be a three by four matrix and this is going to be a four by one matrix and each number in that matrix is going to be between negative one and zero and the reason why we have three by four is because you can see there's three nodes by four nodes and then we have four nodes by one node so the size of the weights has to correspond to the input and output layers because you can kind of you can see that this has there's a weight between layer one and layer zero and there's weights between layer one and layer two and the size of the weights have to correspond to those layers so now we're gonna you can see we're gonna iterate 60 times and then we're gonna keep track of the air just like before we were keeping track of the error it's just called something different so in this code there's gonna be a lot of things that are pretty much just like the code that i just did for the live coding but the names are slightly different so layer 0 here this is the same as the input from before so we're just getting the first street light which would be this matrix right here this vector right here and this is just a kind of fancy way of saying that we're going to get a nested mate a nested array so instead of an instead of an array of one zero one it will be an array of an array of one zero one so they'll just be like and if you output it there'd be an extra array on the sides um there you go so the re so that that just makes the kind of math work to have an array of arrays but it's just getting the first array from the streetlights and then layer one here is going to be the same as the input and the same as the prediction from before remember the other network we had one input and that input had one prediction but in this neural network it's basically two neural networks uh there's two two networks there's layer one to layer there's layer zero to layer one and there's layer one to layer two so the prediction or output from layer 0 to layer 1 becomes the input from layer 1 to layer 2. so this layer 1 is the input and the prediction so just like before we're always just mult to get a prediction you always multiply the input times weights so that's what this is it's another numpy function to just multiply the things that you pass into that function layer zero and weight zero one that's what you're always doing for your prediction multiplying inputs times weights then we run it by the relu function which will selectively activate like we talked about before layer two is our output layer and which is the same as our prediction from before and again we're just multiplying the input times the weight so the input is layer one and the weight is weights one two so this is the same thing we've been doing all along to get the prediction from the first stock from the very first code example this was the neural network we had a neural network function that just multiplied weights times input and that's exactly what we're doing here so now we have to get our error so our error is going to we have our layer 2 error here and we're going to calculate that pretty much just like before but we have different names so before it was the air was prediction minus goal prediction squared uh but now the prediction is that layer two variable the goal prediction is the getting something from the walk versus stop array and then we square it so air is always prediction my school prediction squared and then we're going to do a plus equal so we're going to add up all the errors together into our layer 2 error variable okay so the next thing is our delta this is basically exactly like the the other example um so this we have the layer two so this is just input or wait a second yeah input minus goal prediction prediction minus goal prediction so layer two minus walk for stop is the same as prediction mice goal prediction and remember the goal here is um air attribution it's all about figuring out how much each weight contributed to the final error so in our first two layer neural network we calculated a a delta value a delta variable which told us how much higher or lower the output prediction is supposed to be so we're calculating that the same way so now we have how much that we that though we want the final prediction to move up or down that's the delta and now we need to figure out how much we want each middle layer layer one node to move up or down so one thing that's interesting about this is if we go to our diagram we can see there's an order layer 0 layer 1 layer 2. but in our code we're gonna calculate layer two first and then layer one so we're not calculating it in the same order as you would normally think going through the going through the layers this is another like really big concept called back propagation let me it's a let me zoom out so you can see all the notes i have here so in back propagation the layer 2 delta is back propagated to the layer one delta and this is going to give us a weighting of how much each weight contributed to that error that's supposed to be in the word error oh it is it's just off the side of the screen okay so we need some way for our layer two weights to impact layer one because we need to update everything so that's what back propagation is and so this is another pretty big concept i mean back propagation could be a whole talk of its own but it's all about taking uh information from later on in the the network and passing it back to uh to the earlier in the network so it can use that information when it's making the calculations and let's see if there's one more thing we need to talk about so this is for the layer one delta here we're just using this dot function to multiply layer two delta by the weights and we have the transpose operator but then the final thing is this relu to derive function so if the relu set the output to layer one to the layer one node to be zero then it didn't contribute to the air at all so when this was true we should also set the delta of that node to be zero and multiplying each layer one node by the relu two derived function accomplishes this the relu two derived is either a one or a zero depending on whether the layer one value was more than zero or not so now we're going to update the weights and we update the weights just like we did before so for each weight we are going to multiply its input value by its output delta and so it's just weights one two equals weights one two uh minus wait i'm thinking yeah yeah weights one two equals weights one two minus alpha times and then so this is just how we calculate the weight before where you multiply the input by the delta and then you multiply that by the alpha and then that's that's just how you update the weights so again this code is all all online so you can kind of review it later like some of this stuff it kind of takes going through a few times to really see how it all goes together but you'll see that we're updating the weights in the exact same way as we were in the other live coding sessions that we did so down here this is going to just make sure that we're only going to um print this information uh just every once in a while so every few iterations so when we run when we run this it's only going to give the information every few iterations so i'm just going to run this right now and i think it has that from last time i ran it but if the main thing to look at here is that if we go to the bottom our error gets very close to zero so that's the whole point of learning the learning is trying to get our air as close as possible to zero now if you remember before that that meant that our weights were correctly we're making a correct prediction in this case it's it's the same now i'm kind of getting close to to the the conclusion of kind of why all this matters so what i mean there's probably we could probably talk about this code for a few more hours to explain it every little part like the back propagation and and the weights but i kind of want to do kind of step back a little bit and give kind of an overview of the point of why we create intermediate data sets that have correlation so i go back over here okay so consider this image of this handwritten four that's kind of blown up it's supposed to be supposed to look like the number four and uh if you if we had a data set of a bunch of images of handwritten digits and they were all labeled like zero to nine um if we wanted to train a neural network to take the pixel values from this image and predict that this was a four our two layer neural network would have never been able to do it so they're just there's no individual pixel in this and this here so there's no individual pixel that perfectly correlates with whether it's a four or not just like in our second example that i just went over there was no light combination that perfectly correlated with walk or stop so there's there's only different configurations of pixels that correlate with whether or not this is a four so this is the essence of deep learning if i go to the next slide you'll see how this would work with kind of the multiple layers where deep learning is all about creating intermediate data sets or layers where each node in an intermediate layer represents the presence or absence of a different configuration of inputs so in this way no pixel has to perfectly correlate with whether or not it's a four or not instead we have these middle layers that they attempt to identify different configurations of pixels that may or may not correlate with the four so we go through a lot of different layers and finally it finds which layer correlates if it finds a configuration of pixels that correlates with whether it's a four not and then it outputs that this is a four so the presence of many four like configurations would then give the final layer the information or correlation it needs to correctly predict whether the digit is a four so we can take our three layer network and continue to stack many many more layers some networks even have hundreds of layers and each neuron plays its part in detecting different configurations of input data so you could start with like we started with these uh input of three layers but the hidden layers could get bigger and bigger and bigger until it finally has what it needs to correctly predict the output layer so i'm hoping that just kind of seeing this for example helps you kind of see how how adding additional layers can help you get the final prediction where you don't need just like our stop light example you don't need the perfect correlation to figure out the final answer you can make other layers and other layers after that that finally get the correlation to get the final answer so i'm getting kind of i'm cl to the end of the talk here so i'm hoping you just by this talk you've got a good introduction to deep learning and neural networks and you saw by the examples how we were able to write a neural network they actually learned what we're trying to learn and then i also hope you unders you start to understand how hidden layers can can give your neural network even extra power to figure out things it would not have normally figured out so yeah thank you for coming to the talk and i'm open for questions right now