hey hello everyone how is it going good what about the party yesterday did you stay long nice that's a good feeling i was about to stay long enough was like nah maybe i need to work on my slide a bit more and maybe i should have stayed longer so we'll see okay so as you have seen um you know this uh title of my talk is about tensorflow and uh i kind of like had a bit of like hard time to see um how deep we are in a topic so um if you can put your hands up how many of you do some machine learning like i don't know from time to time oh that's amazing okay how many of you did some neural networks nice and deep learning cool and tensorflow i like your people okay so since you know depending on the city sometimes background is like a bit harder right uh for instance in munich um i also organize machine learning meetup when we have like a bunch of talks about i don't know all kind of deep learning right and um you know sometimes you know that the city is like a bit more prepared so like a general audience is like a bit um aware of things here i wanted to kind of start with the small things and after i go like deeper and more high level so hopefully even if you i don't know android developer or um like web developer you're not going to be feeling lost it's more like we will get there so we'll start slowly and after i get somewhere else um as you also noticed uh those slides didn't look like super amazing like for instance like pictures like super small you know why because it's in jupiter and there is a real gs that rendering that that also means that uh all the codes that they're going to be showing today and they're going to be a bunch of it you can just download jupiter notebook and just like run through it so um it's kind of excuse for myself like why it doesn't look that amazing yes um i mean uh one of the parts as well that you know people start talking like all this machine learning stuff right and it's all amazing right google changed from mobile first to ai first like and uh sometimes it feels like high pipe hype right and i didn't want to bring like many examples like how you can do like image classification is in this part so since we are talking about hitchhiker guys to the galaxy i brought this like babel fish like who's a wire what is that yes at least a couple of people travel the galaxy so in the original movie right they had this uh small babble fish right that um one of the actors kind of like putting inside of the ear right and this fish is translating to no matter what language you are listening to right into your own language basically right so it sounds pretty cool right you don't need to learn a land which you can go to whatever country and it just like does it for you um turns out uh it's not like you know futuristic and super crazy if you have seen uh you know announcement event from google i don't know maybe a couple of weeks ago they also announced like google pixel um buds right so those like headphones that you can tap on them and they're gonna be doing translation right um and it's kind of like pretty close what battlefish basically does right but you can basically buy it more or less right now and it's already going to be working um what is interesting about that that i don't i don't know i think like a huge percentage of that is basically based on the machine learning and deep learning right um on this case on this picture you can see just like a nice animation how google does uh one shot learning right so they do um like corp they have a corpus like of different languages right and uh essentially they are able to you know like build embeddings that means that uh you know you can uh um if you have different words right and based like how words are used right there is some common sense what they call the distributed presentations right and based on this information if you um give your network like a huge amount of text right you can learn that uh i don't know stage is uh somehow similar uh to the speaker right or clause in the um embedding space and what they also using like for uh translation like as a part of it right um another hand like right now even uh you know uh generating these audio waves is uh also in many cases like way better if you use like deep learning there so there was like an article from apple and this uh one from deepmind and they basically switch it more or less completely right and the sounds uh of you know what google home saying what is uh uh other part of google is saying and most likely this google buzz they sounds also like way better like way realistic right because you don't do concatenation of like smaller pieces but the network does is like endtoend basically it's a wavenet yes and uh you know when you're starting to talk about all those uh um i don't know like machine learning and deep learning stuff usually you see this picture like how many of you have seen that yeah it's quite many especially at google i o and other place and you know i mean it's cool like it's showing you some information there are some layers right and i know like kind of a structure right and it's magically kind of works right but the problem right that uh sometimes it's kind of like missing all the details right you're like okay i got data i put it inside pom pom and it already works right it always remind me of this like oh how you do that you do circles pompom and here we go like he's already oh and what is even more funny and like the same time is like helpful right that many people that i was like meeting like lately right they were like so what they do i do some i don't know data science machine learning is all like oh yeah i can do data science and machine learning like i did a course like on udacity you put data inside bum bum and it already works right and i mean on one hand it's fine right because people feel motivated on the other hand like it's not that easy right otherwise we're not gonna be get paid for that and uh i wanted to go like a bit more in details right but is it gonna be like all this crazy about formulas and uh basically this is a formula uh i think that was for gans right how you can do that um no like i i think it's it doesn't make sense right because i mean we don't have like time for that and uh maybe you know you need like more your own time to digest it and like feel good about that so we kind of like have like a different approach we kind of go through a bunch of like cod samples um trying to you know like show some pain points and from there explain okay why did you do that or like what was happening there um yeah and the some first part is basically um it's just how we do kind of logistic regression just to give you a feeling like what is that about and after we will you know go like deeper a bit um yes if any moment you feel like what's happening just put your hands up and uh i will react on that and uh i don't know we will slow down and maybe do something crazy um okay so i think if you were doing i don't know computer science or um other uh related subjects uh most likely you have heard about logistic regression right and you know already how is it working and uh like have some ideas um it's a very basic and uh super simplified approach that you have some nice kitten right because everybody loves them and you have some you know vectors that you transfer it afterwards right and uh theoretically you can you know learn some kind of like representation that uh you'll be able to you know have some uh um activations or like hot points like on the pictures that will say okay because i'm getting you know positive values here right and negative like somewhere else right i can uh basically detect that it looks like cat like in that direction right and uh because we don't want to do it uh super simple we can also add some nonlinearity right that in future we can use it for something more complicated right and i mean there's like a bunch of formulas how you can uh you know in terms of like mathematics define it right and um yeah so you can uh define your functions that is uh based on uh your weights right your input vector or some bias you can uh uh add some activation on top of it and uh you can have some loss function right so you can understand how bad you are right and on top of it you can have like a cost functions that you're trying to minimize right next one um yeah but you know if you have this part you still need to learn like what weights are coming from right and i'm not really going like that much deep in details because i think um it might be like a good topic to talk about but you can also like easily google how exactly back propagation works but ideas that um you know you have the way how you compute your cost function right and you can understand uh how to take like derivative of that so you know okay what smaller pieces you're taking from there and uh um if you will have like a small uh implementation in i don't know like a numpy for instance um it's not going to be like super complicated right in that direction so uh the formulas that you have seen before like it just goes like in the presentation of a numpy right and um once you analytically compute like derivatives you can also like measure how much they're coming from right and um when you do like in direction of optimization right what you do is basically um you compute your gradients right you know that okay you have some learning rate how much we kind of like going deeper and uh like how fast we are um but overall it's about you know step by step trying to adjust like your weights and the like biases in the direction and i mean okay it's not that hard right you can also um you got this working you got like your weights and after you basically compute the same functions that you had even before right and uh yeah sounds good in that direction right and i mean i'm not focusing that much on those slides because they still gonna be like available online and i better spend a bit more time like this uh tensorflow it's just like to show you how that you know it's not like totally crazy um yeah and uh essentially your model is basically um what it does like it initializes those like waste and biases right you do some optimization steps that you showed before and um when it's uh get like into proximity of your good parameters you say like okay we get from there um you know it's kind of like taking some time and uh depending like on uh structure because we had just logistic regression right it's pretty straightforward but if you go to like a deeper um like i don't know you could do like a multilayer perceptron even um it's getting more complicated because you still need to um do some gradient checking now like other techniques that help you to understand that your implementation is still working right and i mean um especially right now there's like a bunch of companies that uh um do all those like frameworks for neural networks and i mean sometimes i have a feeling that um you know everybody wants to invent their own right there's like amazon there's uh google there's uh facebook with cafe too and stuff like that but today we're gonna be talking more about tensorflow right so tensorflow is kind of uh like a big animal right um it has many things that not really super visible for us like for instance all this uh super useful but uh not super you know applicable by like big percentage of people is uh execution engine right that knows like okay how do i work like with your symbolic graphs like how do i schedule them how like what to do with that right and plus some like uh optimizations as well that can run your vector operations like even faster on the all kind of hardware right so you don't need to think like okay i implemented like a numpy now i need to use like uh i think it's called like scipy there's a python package that you can not like scifi but to cooper that can run it like on a good architecture but uh in this case uh um tensorflow is kind of like taking care of all those things and even if it's like tpu you don't need to like you know implement the stuff like on your own um on top of it there is like a front end right like that it gives you ability to write some python stuff and um even on top of it there is also like layers that give some you know a structure that you need to reimplement it uh lately they also release like data sets that help you to you know like since you already have um gpus right for like fast learning but in many cases like preprocessing the data is like getting like a bottleneck and say trying to build like a good uh wrapper on that to make it even faster um and on top of it there is like a keras that is also going to cover in the talks that gives like even higher level obstructions there and what is kind of relatively new is like kent estimators if you use a skylarkit or psychic learn it's kind of in a very similar format you can just create like dnn and say fit and it's like kind of magically works so um theoretically you can also use that but practically it feels like more that you know you're working on that if you're like defining something like customize and architecturewise is also more helpful um if you give like a bit of concepts introduction right um so there is like tensorflow as a word right like it has like two different parts like one is like tensor and another one is flow right quite obvious obviously but uh so if you're talking about tensors right um so it's like two diff like different pieces right in mathematics there is a different definition of tensors um but in tensorflow world is basically you can define um you know different uh ways of i don't know in this case like constant right but multiplication is also going to be like a part of this operation or like in this case if you print it right first is going to be tensor oh sorry but the result is also tensor right so you can define your graph basically in those like tensors what is also interesting right that you know if you do like this multiplication here and we print result at the end of the day it doesn't really show us result right like why is it happening because all the concepts that uh you know tensorflow and this like scheduling engine it does it later so it's like lazy uh evaluation and um you know like it still can do some crazy stuff like how to optimize it because i mean for this case there is nothing to optimize but in general case there is something more advanced for that if you have like uh deep network but what uh is kind of essential for that that uh once you have tensorflow session right you can say okay there's a um you know this operation set or tensors that uh i would like to execute right and when you do session run it's kind of okay does some scheduling some magic in the background and only after it does evaluation and execution of this lazy formula that you had right and what you can do is basically it's kind of like a hacky way and not super visible right now but wait a second so once you define it like in the current scope of your script or jupyter notebook right you can also use a jupyter uh sorry with a tensorboard you can try to visualize it and since since this one isn't like sliding betting not really good we have this one that basically shows that uh for the operation that we were showing right it's going to have the graph like that right and um if you were surprised that this like multiplication doesn't look very you know comfortable for you you can still use like a different sign like depending like on what kind of multiplication you have um yeah but basically you have session session basically based on what um backend you're using and uh like where to execute those things yes um what is kind of essential for that is also being able to you know pass some variables so like in this case we are saying that okay i'm creating like a placeholder right for x with um yeah some information that i'm going to be using afterwards and you can afterwards pass it inside of uh your session so that's how you're also going to be um like learning your models um yeah if we uh you know like go in direction uh of initializing parameters right because it's what we usually do in the beginning um it's where tensorflow is kind of like getting verbose right because every time when you okay i'm defining like a variable for my ways right and it's not like how you would do in production system but it's more like a visible like what's happening there right and uh when you're defining that you will also say okay like what is the shape of this part right and um in many cases like you need to okay sit and like understand what you're going to be doing there um but you know there's like some complexity with this part is happening and i will mention it like a bit later like why is it important um yeah and there's also like some fancy way like how you can do like proper initialization like this uh like uh javier utilization for instance and stuff like that but uh what i would like you to just like see that uh okay we initializing parameters and uh you need to know like uh shapes of the layers what you're going to be using right and when you were talking about uh you know um there's like a loss function like and how we evaluate all those things right before we were like implementing like our own right and uh depending like okay is it um what kind of classification is it like multilabel right you still need to do like your own uh jobs there right um with that case like you know because tensorflow is already like a framework right they already have all things like in place um and it's also kind of like just going like a bit ahead uh sometimes in keras you like missing some parts that uh um what they call like or um i can curse it's like a low level but uh even though for multilevel classification we have some issues that we need to like implement stuff um yeah but back to um this part so you can basically just use uh crossentropy with sludges and in your code is basically gonna be pretty straightforward right you're getting um you know your labels your lodges and after you can just uh pass it like as a convention um yeah so it doesn't really go like super crazy right um we still you know like in original uh uh part with logistic regression we had some forward propagation and uh um that's basically what we have here right um we have like a separate uh uh you know the parts that has like our parameters right that you wanna pass on like if you do like inside of the batch but basically um what you do is pretty similar what you did like in numpy beforehand right just like it's different prefix um yeah and they already have like all kind of like activations like relu or leaky will lose that kind of like a part of the story for you um yeah if you feel like that you know you don't really know like what directions we should be like is it relu like or sigmoid or why are we talking about those things just like catch me afterwards and we can have like a long discussion like how i know things are helpful like for um you know gradient vanishing like an explosion like afterwards but i'm afraid that here on the stage you just like take it as a given and after you can always go deeper and see like why i don't know i'm using relu and uh um not uh like proper sigmoid here um but uh i wouldn't go like that much in details yes and after if you you know like um defining kind of uh the model like in tensorflow way right um besides like having the bullet plates that we kind of do some reshaping part right um we have like a placeholder like parameters that we showed before right but essentially it's pretty similar right you have like a forward propagation uh what you did before right you were like uh analytically computing like your gradient right and after trying to like implement it um what tensorflow gives you is they have like a set of like optimizers right that depending like okay is it going to be uh granny and descend or stochastic gradient descent or the one is momentum right um just like to you know for those of you who are not familiar with that just depending like how you you know you have a function that you're trying to find uh i don't know like maximum or minimum basically what you're optimizing for there's like a different uh um numerical way to do that and uh there are like a different behavioral things right so for instance um like adam might be do some adaptive stuff that this more sophisticated than just using like stochastic gradient descent um but there's somebody who just want to play around with that you just can you know like use like item optimizers like one of default ones and uh just get it from there and what you can do afterwards is okay you're providing like a learning rate and you can say like minimize my cost to define beforehand right and even if you remember like cost function was not that complicated to define right so it's just like a sigmoid cross entropy right now it's like a bit complicated but after we will like switch into keras when it's going to be like way simpler just like to show you also like a part of complexity um yeah and what you could do afterwards you have your sessions that you are running right um here they kind of like do some um like mini batch part but basically ideas that you have like a session that you're running with your uh optimizer and like cost right and uh um yeah essentially it just uh uh if you run this model it's going to be showing like okay like after every epoch and how is your loss is basically changing so it goes pretty far pretty good right and again um i'm gonna share uh jupyter notebook afterwards so you can like click those things and try to change the parameters and make them available yes um okay so we can write those things right but one of the problems why the things was not taking off is like neural networks right i mean they're like multiple theoretical things right that you are not able to train them deep enough right but another problem resources right i remember like back in i don't know 2007 i believe like when you're like using some neural networks for um like as a kind of a student project thing for uh i i think like detecting some faces and like classifying like other things right we had like a separate machine that was running like for quite some days and it was like training on like on its own right and uh you know it was like machined with cpu right so it's like gonna be like super slow but the problem is that as you know human individuals right or like your personal project hobby one it's like take some time right to train those things um hopefully for us like there are like a bunch of uh cool companies that invest in like a good amount of resources that make it available and they're also working like on building like a cool architecture so like in this case it's inception model from google and uh um you can like download already pretrained models like you think like okay we say train for some cases right um but turns out that if you haven't just like like the players right um after having like a bunch of you know you have like a bunch of classes right and in this case i think they use like uh um in some cases like it's google net uh coolness sorry imagenet uh like data set with like hundred thousands of images google has their own data set but basically they learning you know to um uh understand like to classify different classes right and meanwhile network uh also learns different representations right so if before you were coming from computer vision background and you're gonna use like uh sift or serve feature descriptors right now you can just use uh one of those like networks and get depending like what uh you know like level of presentation already for you and you can do like crazy stuff that i will show in a second um and i mean there are like a bunch of like a different uh you know network architectures and they already pretrained one of more famous one is like vgg16 and it's like also super old uh what i want to show like on this uh graph is basically uh especially if you want to port it like for mobile or like some lowlevel uh hardware and if you want to you know optimize for inference um you will see that you know like on one access you will see like number of operations so like how much you know time it takes and also uh resources and another one like accuracy right and uh turns out not the most you know like number of operations are getting like the highest accuracy right so uh sometimes you can pick up like us inception v3 that uh relatively or like resnet from microsoft that uh like in terms of operations it's like pretty low right but in terms of accuracy is pretty high so just like keep in mind that uh you know there is like a good uh comparisons of things and uh i mean which g16 is cool but there's like other free train network that you can be using and there's also like a nice paper on archive basically that you can read on those things yes um okay so imagine that google trains this network for you right can we make it easier and like use it for something from us right so in many cases they uh sharing their models like in the protobuffer definition um and you can just like load it as as we do like in this case right so uh you can define like a graph definition and you can like read a file but basically essentially um yeah if you load it again like it's tensorboard visualizations that shows you um how simplified it looks like if you click on those blocks they're actually going to be extending so it's going to be as original image inception model um but basically you get the thing is running right so we got the model right like and uh what we can do afterwards is just basically um if you know like you define uh your network right usually uh you will have a um okay just like different layers right if you're familiar with topic there's some convolution layers that also um take into account not only the value of the point right but also um you know proximity so you're basically having like a bigger high level features um but usually if it's you know like multiple classes and zen zones a day you're gonna use like soft max right as a like last layer and what's happening here right that before soft max you're gonna have some i don't know fully connected layer just to like learn some things there um and maybe some pulling like on top of it um and what you can do is basically use this information like as a feature descriptor right and in this case like i'm taking uh i don't know i think like one of these intro pictures actually um and uh passing like inside and i will get like a vector of uh i think like 2048 or something if i'm not wrong but basically this um a bunch of digits here is going to be like a really good feature descriptor for you so you can easily uh implement uh you know like if you again like go to tensorboard and you show some uh i think in this case it's like pca but you can also do like a disney it's just like a way how you can put like a multidimensional data like in three dimension in this case but basically you will see that you know images that uh kind of look alike right will have like a similar image vectors right so you can easily implement the image similarity without i don't know spending much time so i've been at this like hackathon in zurich like months ago right and we had like one of projects that we wanted to add like image similarity and it took us like there was like one girl who was doing uh i think like data science at yandex and she was like are going to be using like surf and shift and i was like okay and meanwhile i will try to use some feature description from inception and uh it was like way faster on my side because you know like with all this like opencv stuff that takes some time it didn't really help that much yeah and if you go like more in production direction with all this like image similarity there's a library called uh like annoy like very nice name but it's basically built by spotify and they do like approximate nearest neighbor oh yeah like cisco how they call library but the basic idea is that once you have those you know um feature representations of your images right you can have some um approximate like neighborhoods right and it's super fast basically to say like okay i have this image that like belongs somewhere here right give me like a neighborhood of images that look look alike right so you don't need to do like some k n um but you can just like extract it from this index so if you do it in production it's like super cool project all those are like a couple of tricks so if you're going to be using this just like ask me or i don't know like ping me i will tell you that um yeah so you know you have seen that it's getting easier right you don't need to implement stuff but it's still pretty robust right there's like much things happening uh fortunately for us uh yeah there was like this library keras right um and uh they make it even better so they like wrap up things like together and um they also like add some um you know like proper way to like handle you know the ways that you need to think that you need to implement tensorflow on your own but basically if you're building like a simple model is as simple as it is right you have like a layers um yeah that like could be convolutions like or max pooling or whatever else like you can you know i can uh tell you afterwards but basically what you have afterwards you have a model and you adding like all the layers like in between so you do need to think like okay what is uh a shape of the layer before like how do i connect them together it's like you're focusing on like more important parts than just like thinking about shapes so it's pretty good right and uh yeah you can also do like a pretty second learn approach of uh you know model fit with like all your data and uh it's pretty good yeah and after if you want to go more in details you can also do like a model summary that shows you okay what layers you're having like number of parameters if you want to like decrease number of parameters because you wanna i don't know move uh to like smaller device and you wanna replace some fully connected layers with convolution and stuff like that yeah and if you do like evaluation it's basically also pretty simple afterwards right um what you can do afterwards is also like saving this stuff so you know like once you train it you can get it yet in different format for keras here but basically um yeah it will generate you like uh one is uh like a graph definition and another one ways basically that you can easily load afterwards um yeah and it's like a way how you can load the stuff um and uh there was examples that i trained like on a news data set and uh yeah it was like at the end of the day if you um you have like original image and you can uh yeah model predict that gets your like image and you can reshape to the proper shape but basically it will show you that okay it's like fours basically class that has it so it's basically look like four um yeah and uh if it was not simple right you can make it even simpler so like keras in this case they have already like applications and like in this case it's deception victory right and from there you can just uh define it and uh you know like you can say like weights are coming from image snare drive and uh um yeah and you can have like a graph already like working here right and uh yeah if you have the graph and you can load it you can just use a prediction from there um and this example that we had this uh image and uh depending like on you know uh um classes that the imagenet already having it's gonna be good but what you can do is basically um freeze like entire architecture besides the last layer and it will um like retrain not entire network because it can screw up the ways but only like your parts that you know it's using like a feature representations and like mapping a feature representation to your own classes you can go like even as far as uh using like one shot learning as uh you know like having like very few images that you can train on top of it um yes and uh you know it's already given like a pretty good idea that if you really want to use it you don't need to you know like know all the details it's like a similar direction of you know as i was saying if you want to drive a car right you don't need to know how exactly your engine works right you can just be like studying the wheel and like making it working and it's basically what's happening with like many deep learning models right if you know like a couple concepts right how you get like image features right you can always like build on top of it so it's kind of a lego game right and one of examples is like image captioning i didn't really bring the code inside but uh i can share with you if you interested but ideas there is that you also have image right you have like some convolutional parts that might be based like or i know like one of wellknown networks right and after you do some like uh um iron and part right that can also maps the part of the words that you are saying right and uh if you have like once a data set right you can say okay this image as a data set has some descriptions that i don't know like uh um this like fly or broad display um and after a network you will learn it right so it's pretty good and you can go like that crazy as you know like even combining like more advanced right you know you're getting like uh questions that you know goes through some lstm right um yeah and uh after you have the same like feature descriptors and after um if you don't use like sequential model but basically functional approach for keras you can just join it in the same model so it's more about data that you're having but uh your tool set is basically staying the same yeah and in terms of deployment since uh you know it was pretty fast and i think like many of you is like what's happening i need to check jupiter afterwards um there's like many ways to do that right like one is a way is your own kind of way and uh you have like tensorflow servings that optimize for you have your model and can run it like in the background and uh it has you can use like nvidia docker and some kubernetes that you already most likely see and talk about that but basically you will have your own infrastructure like on gpus that shows those things yeah um and for android people of you and even android things of you um there is also like a version of tensorflow right that you can train on your um like a separate cluster for instance and after just saves the weights um but essentially what you would do is you have like a grading dependency right and you will you know basically uh define like uh um yes it's a part like where your protobuffer model is lies right so you can put it into your resources and um oops um you can also define where you're fitting it so like all those network architecture right they have like a specific names right so you can say okay feed it in direction of like input right and give me results like of output right in this case we don't really use like a feature presentation but we use it like original glasses and uh um yeah and it will get you pretty much working um and uh yeah in this case you can run the same one for android or like a small android things device uh the only thing that i also don't cover here is like how to optimize it right um because if you optimize for inference on mobile phones there is like a bunch of like other steps it might be doing like reducing um um like you know like how many bits you are storing like per weights right because sometimes it's like representation is pretty high sometimes you can also be more efficient on uh um archiving those models so um you can easily you know like decrease the weight of the models that you saving like by a factor of like 20 i believe um so if you um like interested about this part being me on twitter like i should have like a slide from munich when i was talking about uh doing deployments like for android but overall it's basically um it's pretty straightforward um so what i wanted basically to say here is uh you know there are like many different pieces right you can go like pretty low level you can go like pretty high level right um but it's like your own choice right if you want you can go pretty high level with keras with some free training models and get results like in a matter of hours so don't be afraid of like trying that and you can always go deeper right then be you know like scared of that okay i need to do like you know all those like derivatives like partial derivatives and uh no like bunch of mathematics it's not that much of rocket science like you can always go on that level but for the beginning you can just do like clicky clicky stuff like combining models and it still looks pretty cool like if it's really working for you so um yeah like there was lots of pieces kind of missing and it's like was pretty fast but i hope it's kind of like give you like a road map right and um like a guide basically right what things are there and uh if you have particular questions on like similar topics just like catch me up and we can talk about all cool for cool cans like for one megapixel and stuff like that yeah with that i finished my talk please thank you