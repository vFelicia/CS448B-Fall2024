okay everybody good to go everybody sit awake good let's see if we say manage to stay that way so I'm Philip I want to talk about some machine learning some hyping some making fun of stuff so let's see what we can take this so I work for elastic the company behind elastic Serge dogfish Cubana like previously called the elk stick now we call it the elastic stick I'm part of our infrastructure team I always say in the middle that is a UNIX pipe a kind of piped it out into developer advocacy so I try to speak a lot about the good stuff that we do and why am i talking about machine learning well every company needs to have some machine learning in their stack you can see we have kind of hitting it on the righthand side at the bottom there kind of like we have something about cloud we have something about machine learning like all the usual stuff that you need to have we'll get back to that later on but at first I want to talk a bit about machine learning in general and what it is and what it is not and what it could do for you so some months ago there was this like machine learning is going viral and everybody was going oh we need machine learning we need to do more about that and one thing about going viral was also that who remembers that iOS park when suddenly the eyes were exchanged for a different character that was actually machine learning as well because that was kind of some Apple added some machine learning for the keyboard that it would replace specific characters or words and that was kind of as soon as you received that bad eye which was replaced by some other character that was already machine learning at work because then your system would learn that other way to to write the replace the term and only once you receive such a change diet then you would be or would do the same when you device would do the same thing because your device had learned that so that was kind of the power of machine learning which was kind of a bark and like kind of a bit of a virus since it was spreading like that but yeah machine learning is kind of going viral in that regard yeah and then there are always the people who are saying like oh we're doing some very fancy technology and even though we probably would have just needed a few if statements which would have been much easier to write in debug which probably looks something like this like have this big digger and then it's just using that little shovel there and to get going so that is some people we are wielding kind of the powers of machine learning just to do something very small and confined and read like easy to get going with sometimes you might need it sometimes you might not need it I guess some of you are probably familiar with that that is the Gartner hype cycle that is probably the only thing that is true that Gartner is always bringing out and because everything else is kind of like just making stuff up or at least it was always my impression that whatever the Gartner is predicting for the next five years is probably not going to be the way it will be but their hype cycle is kind of like very very true like you have some technology trigger and then you have like this peak where everybody is saying like oh this will be so awesome and this will do all the things I can still remember when everybody said that about no sequel and then people were kind of like oh maybe a relational databases were not all that bad and I have the feeling that kind of machine learning and AI and whatever you want to call it this kind of falling into the same perception every now and then and so you have some peak and then you get kind of like a bit of a disillusionment and at some point you may be reached at 2 or 3 activity will machine learning or AI solve all your problems um unfortunately it probably will not but that's probably no surprise so anybody who has been in the field for some years knows ok all the promises like everything will be solved by this technology unfortunately it's not that easy or fortunately it's not that easy otherwise nobody would need us anymore so I want to talk about like a bit machine learning in general and like putting it in with AI and whatever then I will pick a domain of what we can do since our use case is always about machine learning for ops data I will use some op stater for that and then I'll just play around with some data set for a quick finish let's see where we can go so machine learning there's always this misconception of this discussion like what is artificial intelligence what is machine learning what is deep learning how do they belong together isn't all the same thing because some people always say oh it's basically the same thing it's just like depending on when people started with something they had to come up with a new fancy term and that fancy term could be either AI or if you came a bit later it would be machine learning or if you're very uptodate and it would be deep learning I kind of find this very nice this graphics so you can see it all kind of started in the 50s already back in the days I don't assume that most of you were around an IT back then but maybe somewhere or maybe a little later so in the beginning there was machine learning and there were high hopes and after some initial breakthroughs people assumed well general artificial intelligence will be sooner things so general artificial intelligence will be like machines will be as good as humans at almost everything and they will just learn one field by the other and and initially it was just it started as a very narrow field like you would do something where machines would be good in one specific field and then the idea was well we will just generalize that and this will be applicable to any other thing afterwards and it will just keep generalizing that until you're kind of on human level unfortunately that didn't play out that well and then in the 80s we had like with machine learning we had another wave of trying debt and then like the 2010s we had another wave of that with deep learning yeah so the general purpose of generally AI is always let's get something that is human level this is something we have not reached yet and we don't even have a proper timeline like in the 50s 60s and 70s it was assumed like oh we will get there but it didn't turn out that way because in the 70s there was the socalled AI winter and this was also one of the reasons why you had to find a new fancy name because a lot of people got burnt by that they assumed Oh artificial intelligence will solve everything and a lot of money was pumped into that and people wrote papers and plans and everything and at some point it didn't really plan will work out the way people planned so there was kind of like the first wave of ai ai then came the air I winter and afterwards people came back with better algorithms more computing power it's kind of like coming back in waves and you're getting closer to your target but with generally AI that's something we have not really achieved yet and we don't really know when that will happen what we are very good at right now is something like narrow AI where you have one specific task and the goal is to have something where you're as good or nearly as good as a human for example to identify an image so I was two weeks ago I was in San Francisco and I uploaded this picture to Facebook and then Facebook does the classification automatically for you so for example if you look in the meta tags what Facebook does is it adds the alt tag and it says like image may contain and then it's just guessing what could be in that image and it's actually pretty good because yeah they have a lot of data they have trained on a lot of data and they're pretty good at just detecting what might be in your image even though I haven't told anything like it didn't tag and say okay this is the Golden Gate Bridge I've just uploaded the image without other any other meta information and the narrow eye in that case like the image classification that just kind of found or detected what might be in that image and it's actually pretty good another thing you might have seen which was pretty recent is that cylinder is cutting some marketing jobs where they're automating the marketing now because previously they had a lot of handcrafted content and now they kind of said well we can automate a lot of that with machine learning we can hyper localize the content where we know okay you're interested in these products and you're living in that specific city and previously somebody would probably hand curate the set of data that or the products that might be interesting for you or would write some content that is tailored for someone in a specific region but they came to the conclusion with machine learning this is much easier to do or will scale much better and they don't need all those humans anymore so they now start to kick out some of their marketing people's some will move to a different department but a lot of them will be just kicked out which sucks and people are very afraid that all machine learning will take over all my jobs probably some but probably also not all of them because you might have seen this example as well where some people used Google Translate to translate sorry for the German if you don't speak German some economic topic and this sentence here is actually the very first sentence of Wikipedia if you look up folks with shirts really which is ya know some German sentence and this is what people got when they use Google Translate actually I tried it yesterday evening by now Google got kind of clever I don't know if somebody went in there to fix that because the people tweeted about that and it made the rounds but by now Google Translate got a bit smarter again and the same thing is still not great what you're getting now but it's still got a bit better and it's not just economics of economics and yeah the same word over and over again but yeah having the fear that professional translators will go out of business like next week it's probably a bit premature we're not there yet like it's kind of general idea yes you can do nice stuff and if you don't understand some language you can use Google Translate to get the idea but it's probably not going to replace proper writing so for example our support team when we do a support in some localized language we always say for example people in Japan don't want to speak English and for them it's super important to get support in Japanese and we always say yes you will get support in Japanese during Japanese business hours because then our people are online but if you want to get help after hours either you can write you text in English in English and somebody will help you or we will use Google Translate and we're actually handling support cases with Google Translate and whatever comes out of Google Translate we will try to help you with and if it's no good unfortunately we will need to wait until somebody wakes up who speaks Japanese yeah and then there's always this chat pod thing I know that Vienna has this kind of I always said the chat pod bubble because a lot of people are working in in chat pods I hope not too many are here and come with the pitchforks afterwards but I have no or I had this thing once or twice where somebody came to me and I was or I was talking to somebody and they were saying I'm doing AI and I Wow that sounds super advanced what are you doing and then they people say well we're using this Facebook API and we're sending some data they are in it senses something back and so we're doing AI and I'm always like you doing api's it's just like the pee is probably silent in your AI yeah and I like to say yeah AI API is kind of it's very close some people don't make the distinction we can't have a discussion about that but at least for me when you say I'm doing AI is not just calling some Facebook or whatever API and get something back and use that this is one of my favorite examples I think this is the Pancho one of the chat BOTS which you can ask like what is the word I like and then you can see yeah it's working well or not that well and I get the idea that is trying to be cute but like cute is not the main thing I want if I want some information like just because it's trying to be to speak French or saying it dozed off for a few seconds and that doesn't make something very smart or intelligent it might be cute at first but if I want to get some information this is not going to make me very happy with something else I recently had I'm using a one Telecom and I recently called their hotline and normally you know you dial in and then you need to press five for that service and press three for whatever and they didn't have that anymore they had the whole thing voice automated so you would need to speak to the system and it didn't work at all like my internet was not working and I was trying to in various ways to describe like what my problem was and even when I said internet it would say oh you want to know something about your bill and I like no that's not where I went and then you do like five circles or so and in the end you could get back to that dial like five to get support for your internet or whatever so yeah the future might be very bright about all these automated systems with voice control and chat pots and whatever um there was sometimes there is super annoying or maybe I'm just too old and grumpy by now anyway um and this is other thing maybe you've once seen a stateless jet pod which makes for great conversations because it just knows the current sentence and it doesn't have any context context about what which is another kind of common problem this is solved you can totally do that but like simple chat pod will not be able to do that because yeah life is hard so what is machine learning after all kind of together so you basically have an algorithm than cat that can work with some data you can kind of learn from that and then you make some determination or prediction out of that data so it's basically a trained machine you haven't hand coded the rules so for example one common example could be like classification of spam emails initially people would write rules like if there's something like I'm from Nigeria and I will send you 100 million if you detect something like that in an email it there's a very good chance that this might be spam but of course people are clever and they will reword stuff and then it's kind of like it's something point five billion it's not Nigeria but some other country but so people will found various ways around but with machine learning you can actually not kind of write these rules explicitly but can just learn like what does the spam email look like and probably can find out these are the different kinds of spam emails like these are the ones who want to sell you viagra did this one wants to get your money and this one is just some plain annoying thing yeah and this is the proper definition what machine learning is but you will not get any venture capital with if you just write this because it sounds very dry you learn from some experience with some some class of tasks and then you perform some measure and then the performance at that task improves with the experience the more experience you have the better the machine learning should get this is kind of like what happens this is kind of the algorithmic view but again this is not getting you any venture capital or you cannot build a startup just based on this one sentence what machine learning is generally doing is something like it's one indie of these four categories commonly so you could have some classification so you could have an either like binary classification guess no or you could have a classification which has kind of like more outputs for example you could have a picture of somebody and then you could try to do a sentiment analysis like is this person happy said angry whatever and then your algorithm tries to classify what kind of facial expression does this person have so that could be a classification then you have regressions where you basically try to know or calculate what a specific value will be in the future which could be like a temperature or a stock price where you do the prediction of all the stock prices will go up or go down you can do ranking ranking is pretty much like you search for something and then you don't just have the hits but the system learns over time like what is it good hit and you kind of like feedback information of what is a good hit into that search system so that your ranking will improve over time and then clustering is just putting stuff that belongs together or isn't kind of some groups that make sense together you just try to classify similar things and then say okay these are all the things that are in that classification or clustering yeah this is what machine learning might look like that one machine is teaching the other little machines of what it knows or at least that's how we humans do it pretty much would be nice like if it was like that so the most common thing is when people say oh we were leveraging machine learning is that they really doing linear regression so basically you have some some information these would be like the little dots and then you want to find the linear regression which is basically this red line and you try to optimize that line and try to keep the distance of all these dots as close as possible to the red line and the more data you have the better that line can be and the more you kind of get to the value of what makes most sense for your data set so if you have like very sparse data and you only have three or five points you will have a very coarse linear regression if you get very or have lots of data the linear regression can kind of optimize much better for all the data points you have and if people are just saying oh we're doing a machine learning oftentimes it just means yes we have this linear regression which is not all that fancy but it can still do meaningful stuff for you yeah whenever people say that assume if there is no other information it's probably a linear regression not something more fancy and then you can still make the distinction between supervised and unsupervised machine learning supervised machine learning is and you have labeled data so you have some input data and then you have some output and you know this is for example with spam emails if enough people have said like this is spam this is not spam you have a huge data set which Google probably has and then oh this is a spam email and this is not a spam email and they can turn on that data because they have some supervised data await and no these are the inputs these are the outputs and then you can take a lot of that data to for your training data and then you can keep some of the remaining data for your test data you could do the same for unsupervised machine learning where you don't have any annotated data you just try to find some hidden relationship in your data so you don't know what is actually this good this is bad and it's dissonant normally you just have data and you try to find some relationship out of that the finding relationship in data can be kind of tricky there's this very nice xkcd comic which I've broken up for better readability here where somebody says like Oh jelly beans cause acne and then the scientists have to investigate even though they just want to play Minecraft so you can see that was kind of like one year ago whenever minecraft was very popular so that first to say like know what like with the 95 percent confidence we can say like no acne is not caused by jelly beans and then somebody says well but it I have heard it it's just a specific color and then they do the test for all the different colors and it's a bit hard to see but all of them like no we found no link and then there is the one where it says whoa and there it says oh there is like a link the problem is if you have like a 95% confidence interval and you make and you run the trial on 20 different data sets just by the mathematics one of them will then be kind of true and then you can write the big news article the green jellybeans obviously daling a workhorse or we're causing acne with a 95 percent confidence and only 5% chance is in there but that's kind of like if you run the same thing 20 times there is a chance that well with 95% confidence one of them will fire if you run it just 20 times and that's pretty much what is happening here then you can do reinforcement learning where you basically have a feedback loop so whatever your outputs are you're kind of continuously optimizing that on the feedback you're getting so for example if people are saying this is helpful this is not helpful and you can then keep optimizing whatever your machine learning algorithm is doing and then there is deep learning which is you have the neural network which is also very fancy nowadays because now finally we have enough computing power or mainly good graphics cards to calculate those to basically calculate a probability vector and you have lots of training and then parallel sted that is if you get enough graphics card to actually do that and not and they haven't been sold out to mine bitcoins before which is kind of sad that people are using all that computing power for mining some cryptocurrency but yeah that's a totally different topic so what basically happens here this is a very simple example and we were having you have slept X hours the night before a test and you have studied Y hours before a test and then you try to find like what does this mean for your test score like the green thing on the right hand side that is basically the outcome of the test score and then you have with these two inputs like they're weighted like how important is sleep how important is how much time you have spent studying and then you can have one or more layers the more layers you have the deeper it gets or as soon as it's multiple once then you will have deep learning and they're kind of a hidden input so you you can see they say it's as mysterious you don't really know how the system behaves in there you just have some weighted inputs and then at the end some value plops out and it says okay if you yep X hours and you studied Y hours your test score will probably be I don't know you will pass you will not pass so you will probably get that great so that is what you can do with neural networks which is also very common for example for image classification or the standard example that a lot of people have is like you have this detection of handwriting so people write the numbers from 0 to 10 to 9 and then you try to detect each image or each number and try to defer like ok which number could that be and then you can extract some characteristics of each number and that is what makes up your neural network and then we will say ok this was the number 5 for example whatever is what is really most important there is to have some big data set because you will always need something to Train there so companies like Google Facebook Amazon they have a huge advantage because they just have a shitload of data to work with so for example when people are now surprised oh how can Facebook be so good at detect stuff um well you have helped with their training data for years and years so this is if you have been on Facebook for a long time or a long time ago probably this was what it looked like 10 years ago so it didn't do any machine learning back then but what you could do is you could upload a photo and you could tag people and you could just say ok this is that person and that gives Facebook actually a huge amount of data because it knows now ok this there is the face of a person and that's probably somebody who is male and from the profile of that person they know like the age the gender the ethnicity whatever so it knows a lot of information and can use all of that information later on to then train its machine learning and that's an advantage that you have given kind of to Facebook and all these other big vendors who can build cool stuff and it's very hard to replicate that for you because you don't have this huge amount of data to work with so yes you can make data great again but only if you have huge data that's the one thing you really need yeah and that's what big companies are doing and then people are like Oh a IML will it just use them interchangeably and it really depends like for example when you're raising money then you want to call it AI when you're hiring then normally people say machine learning that's also some distinction I have seen or some people have framed differently they say when you're fundraising then it's AI when you hire in its machine learning when you're implementing it then it's just a linear regression and as soon as you debugging it it's back to the good old printf so like all the fanciness you have kind of at the start is going kind of like down down down until you back it's basics so while it sounds very fancy and it can do some cool stuff some basics are not going away or there are these cattle contests where you have some problem and you try to solve the problem and at the end at the end you might win I don't know a million dollars depending on the carrot contest so what people are doing is we're just starting a competition we are using some machine learning on it and at the end we will profit but normally what you won't really get the profit you will mainly get some disappointment because what is still totally relevant is that you will need to know the domain and need to know what it is about to optimize that it's not that you can just throw some generic machine learning on it and then it will magically find out what it's just the right thing to do and will make you rich unfortunately it's not that easy okay so enough making fun of machine learning let's do something more productive so since I have an office background mmhmm and our product does work pretty well with that I've picked something so you have some trend over time and we try to determine some pattern in our data so for example this could be people visiting your website this could be traffic this could be error messages you have or return HTTP return codes you have all of these you can't try to find some pattern in data like that so the first one is you have some trend which is just like it's growing linearly or it's kind of like falling linearly or you have exponential growth or whatever anything that is not stationary so if you just have some stable value you're not going to find out any trend out of that because well no change then oftentimes you have some cycles in the data cyclists would be the days of the week so Monday to Friday you have kind of the same traffic patterns for example I said they Sunday will have totally different traffic patterns at least on the average website and you have some seasonality so for example you know if you have some ecommerce shop like Christmas will be much higher I think like Amazon is making or amazon.com for shipping stuff and it's I don't know making fifty percent or so of its revenue around Christmas or at least some crazy number so they just have a seasonal trend and they know ok November and December will be different than all the other months because everybody is doing their online shopping there well the one thing that is not great if you have machine learning is you if you have like some irregular pattern where there is no real pattern it's just irregular because then you will not be able to properly find anything unfortunately we cannot help with that and then you often try to find anomalies in that data so you have for example a point anomaly a point anomaly would be assumed your your bank knows how much money you get out of the ATM machine every time so whenever you go to the ATM machine you get I don't know let's say 50 euro and you normally do that and at some point you get a thousandyear out of the ATM machine let's assume that's possible then that would be a point anomaly because normally it's always like you get money you get money you get money always the same amount but that one time is a totally different amount so that would be a point normally and you could have a contextual anomaly where kind of the context doesn't fit in with the rest the contextual anomaly could be like and you have more requests and you have more CPU and memory usage in your system but the number of requests is actually lower than before all the traffic is lower than before so you have some kind of metrics are pointing in one direction but another one is kind of like in a different one then you have this contextual anomaly we're kind of in the context of all of them together it's kind of going the wrong way and then you have collective anomalies where you have assume that everything is going in one direction but something is kind of like an outlier out of how the collective kind of direction is going then you can have breakouts these are kind of like changes in your data which are not really anomalies they're just changes like you sometimes have ramp up for example you make an ad campaign and more and more people come to your website then you probably see some shift we're just more and more people are coming this is probably not an anomaly but the result of your ad campaign or you can have a mean shift which basically you're deploying a new version of your software and suddenly you only need half of the CPU anymore because you fix some bugs or optimized something there would be a mean shift where you have like just like you deploy the software it's just dropping to some other value and that's kind of like the new baseline which will always follow whereas the ramp up is just something that develops over time yeah if you want to detect anomalies with machine learning you basically have two options you have a supervised or unsupervised machine learning the good thing is in supervised machine learning is you know okay these are the inputs and these are than the outputs the bad thing about supervised machine learning is you will need to have that annotated data so somebody will have to kind of have gone through all of the data and say like okay with this input like this is an anomaly now and this is not an anomaly and oftentimes you don't have that data so very commonly you don't have supervise but instead unsupervised machine learning where you say okay this is the input this is the output and we assume this isn't normally because this is kind of different of what we had before um yeah examples out of the ITER you get suddenly a spike of five hundreds because somebody made a bad deployment and you know okay something is wrong more 500 something changed in our system we need to fix it again and you could have something like security events for example you have some unusual DNS activity where you could do DNS exfiltration for example if an attacker wants to get out information out of your network in a very sneaky way they could just call subdomains under there control with that specific information they want to get out in as a subdomain and most systems don't detect that but if you kind of like look at the DNS traffic you can spot that even though it can be pretty well hidden or you could have some business analytics where you know okay Jenny we have these log entries or log events suddenly we have something totally different and then your machine learning will probably or hopefully picked it up and say okay here we have something that doesn't follow the regular patterns or events that we had before so something must be off here you could totally do visual inspections and we always say they are not enough interns in the world to monitor all the dashboards that you could build with sorry for the interns but the Sproles are not what you want to spend your summer on especially if you have like some complex or fastmoving data yes you can stare at the graphs the entire time and after one or two weeks you probably know okay these are the patterns that I'm expecting but hey you can easily miss something and B that's probably not where you want to spend your time at what this time is kind of best used for so for example if you have this data here this would be unique IP addresses which have connected to my system worse the anomaly we don't really see one we'll get back to the data that is these are actually locks from our own website where we have counted or it's basically it's an engine X proxy and we're just using the engine X X s log to see what is going on on our site and then to find anomalies of what is going on and what is not really going well on our site so you could totally define some static rules then the problem is to define rules somebody needs to know the system well enough to define the rules you could have false positives and negatives depending on how kind of like narrow the band of alerts would be and as soon as your system starts changing over time you will need to adjust it so if you have more visitors to your site you probably will need to change your alerts on a kind of a frequent basis or adjusting to whatever your system looks like today so if you want to set a threshold here like what is the number of unique Pietrus is connecting to your system which values would you pick like yes you could say maybe here at the bottom to below 2,000 might be a problem you could say that and then you could say oh maybe more than 14,000 would also be a good threshold but there's a huge gap in between and you probably don't really see like is there anything going on in my system that is okay or not okay where's the anomaly like yes you can't set the thresholds but there will be super coarsegrained yeah so we need some kind of machine learning let's see what we can do there and yeah normally machine learning is pretty CPU intensive or if it can run on a GPU you can also run it on a GPU since all my demos are on my laptop it's like running slack or zoom or whatever which will also eat up all my CPU so you can totally build it yourself on some very common frameworks for that are tensorflow is especially well known from Google Kara's psych it and loads of others are where you can just build your own machine learning algorithms or you have some data and basically you want to extract some value out of that and find your own am√©lie's so these are just very widely used tools you can use how do you build that pipeline kind of like the machine learning component that is the fun part and people who are data scientists will always say like yeah we're doing data science the entire day but what they basically do is they probably spent five or ten percent of their time on data science and the rest is like 70% is probably of the rest of the time is like getting the data and cleaning up the data and 30% is complaining about not having having clean data yeah because it's kind of a very common thing you need to find the right data then you need to prepare it in the right way so you can load it and then you need to clean it up you need a proper data storage system where you can keep all of the data that you want to keep around and then you probably want to have some optimization algorithm to make stuff scale and make it faster yeah and who have takes the expectation that your data will be clean and will work the right way it's always kind of cute so yeah finally data scientists ask them and they wait we'll probably make an very unhappy face about data cleaning but it happens and then people always ask like which one is the fastest machine learning algorithm and for all these performance things this is my favorite comic well you have yeah and the similar conditions you're testing to systems we're testing the squid and the house cat and we want to know which one of them is more intelligent or faster or more performant or whatever and you just need to find the right scenario because then your system will always win and this is really the bond number one thing whenever some company benchmarks their tool against some competitors they will probably find one use case that is very good for them and very bad for their competitors that's why I would not trust any vendor based benchmarks I would not normally call them bench marketing and not proper benchmarks because well yeah people do something like this so if you ask me like which is the best machine learning algorithm um this is the answer so what you generally want to do is when you want to find some anomalies you want to find out like what is normal like what is the baseline of stuff that happens here for example you could see we have the black line is some I don't know I think it's that the data transfer the visitors on your website or something like that and you can see there is this pattern where probably the first two a week days then the next two are probably Saturday Sunday and then you have five more days which are probably a weekday again and then you try to get that baseline of kind of what is normal in your system so you normally calculate within some confidence interval you calculate what is the upper into lower bound so the upper bound is here at the lower bound is blue and you're just trying to find the system that is getting closer and closer to the actual kind of X curve or the actual value that you have to get kind of like this dynamic baseline of how close are you getting to what you're expecting and yeah most of the data is normally distributed sometimes you're unlucky and it's kind of like paranormally distributed and that will totally throw off any machine learning so if you don't have like a good regular distribution of your data and no machine learning for that will save you yeah so this would be another example so you can see here we had the first three directions we didn't really know what the system was up to so and here the upper and lower bound is just like this light blue background and that the darker blue lines kind of like this is the actual gate that is coming in but then after about three durations this system here has learned like what is the baseline and what is kind of expected of the values and then you can see the band that the light blue band around the actual data is getting narrower and narrower so it's learning what is normal in that system and at some point you have these colored dots the more it's in the red the more of an anomaly it is so you can for example see pretty much at the right hand side there is this where it's falling down there is kind of like this is an anomaly which would probably be pretty hard to spot otherwise in in that system here and the one thing that you need to keep in mind is that whatever your baseline is your baseline will probably evolve unless you have a system that is super stable which is not that common is that depending on how much more visitors you have or whatever you change in your system the baseline will change and you should have a system that keeps learning the new stuff and ages out the other information so that is not stuck on any other information so here yeah I think it's even the same data you can see three iterations and then it kind of knows what what is happening and then you can see we have since I shouldn't move out of the way I'm not trying to head over all the way so here you can see here we have this anomaly and you can see here the system has also learned the anomaly so it's kind of like expecting there might be something going on here so it doesn't really since it's not a supervised machine learning it's just learning what kind of is happening in the system and it's then continuing that trend and kind of feeding that normally into the model for the future and will keep up to date with whatever happen in your system yeah that's another example where you can see stuff is happening and at some point there you have an anomaly and then it keeps learning like it's aging normally again because here you have the anomaly here it's expecting Myanmar normally since it doesn't happen here the anomaly is getting smaller smaller and smaller so the anomalies are kind of being aged out over time is getting again as well yeah you can have a single time series for example unusual traffic where you can see okay this is a perfect pattern I'm expecting this is what might happen you could also break that up into multiple time series so you could have not one metric but multiple metrics you would be interested in and then each of those would be modeled as an independent baseline so you have a machine learning model for each one of them so for example instead of having two global traffic you could say I want to have traffic by country and then you can see you can break it down into different country countries and then you can see sometimes you just don't have traffic for example here in France probably they had either a strike or a national holiday you never know with the French but but something happened and nobody was online anymore there and so it can make sense to break it up into different intervals to see how that they tourism morning okay so as I said I'm just using for my example I'm using from our own website some some visitors so it's just I think one month so or no it's two weeks of data or something like that it's pretty much the energy engine xaxis lock that you're expecting so you can see you have some bytes that you have transferred with you the geoip look up so we know okay this visitor here probably came from India this is the approximate longitude and latitude where they have come from and then you get like the other information that you want so you see then was the remote IP address they did a get and got a 200 back though we couldn't extract any proper browser information from that okay and since I always try to do at least something like this what this move might look like so for example here you can see I have just taken January to okay one and a half month march website visitors which were like around 15 million website hits and this is the general distribution so you can already from this data you get a rough idea what is going on in the system so you can see here this was probably weekend these were weekdays weekend again so you get kind of the general pattern then you could see this is just some some default dashboard that we have you can see where are people coming from like where from where is your website being accessed so you can see for example Germany and France are doing pretty well if we zoom in a bit more here we could probably see Austria as well but since I'm not online my map data would need to load online so here somewhere would be Austria and then you can see ok these are the return codes of my data like this is pretty much all the data you can extract from an engine xaxis log so this would be just the return codes you have you could extract like these where the top URLs you hit you see since you have to buy it sent you see how much traffic your website was sending and then you could also do like a breakdown of okay this was the these were the operating systems that were accessing our website and these were I don't know the browsers and their specific versions so that is what you get in the engine xaxis lock there is no machine learning necessary for that that's just block parsing so no no major magic here then you could write your own visualization so for example this was the number of unique remote IP addresses I've shown you before and this was the the bytes that we were transferring from our website which they kind of correlate slightly but not that wrongly for especially here kind of there is no strong correlation to that one and does anybody see the anomaly in the top chart here or should I let you stare at that dashboard for a week and maybe then you can see it I think if I'm not mistaken the anomaly is this one here so this like this downward thing though it's very hard to see we can actually leverage the machine learning stuff now our to see what is going on here so I've quickly built two visualizations for that the first one is about having a single metric that is the same information that I've just shown you that is the unique IP address that we had in our system and you can see our system needs approximately three directions to learn so you can see here these are the first three days this was Wednesday Thursday Friday and then the algorithm thought well I know what to expect the problem was then we have Saturday Sunday and the traffic pattern is totally different so you're kind of throwing off the machine learning algorithm here a bit because that is here you're having your five days of the week days again and then you have your weekend again and you can see the pattern continues that you need about three durations to learn a pattern because we can one weekend to week and three about here it got tweaked days pretty well already and here it also got the week end then and from then onwards it knows okay this is kind of the weekday pattern that I'm expecting this is the weekend is the weekday pattern so system kind of learned where how stuff works and this is here where we have an anomaly so let's quickly see normally and you can see here something was happening like generally we would expect traffic in this area here but somehow it was falling down and each of these points is actually an anomaly and it actually has an anomaly score between zero and hundred this is how we modeled that but you could totally build something similar the closer it gets to 100 the worse the anomaly is and then you could even have like the list which says like Oh normally I'm expecting that value here 1,400 something I got only 86 so this is 17 times lower than our than I was expecting and this was on the 27th of February last year anybody remembers what happened then no okay let's jump back to my slides I think we've seen most of those already and that was when there was the first ever bigger as3 outage from aw yes and half of the internet basically relied on that so yeah three went down and that affected a lot of stuff including Amazon stuff as well because there were also themselves relying on s3 to be available like everybody else so unfortunately we had the dependents in our website which basically killed our website and our downloads and lot of other stuff we then thought about should we make some multi cloud strategy or whatever but in the end nobody complained because half of the internet was down anyway and if the internet or if your own site is down and everybody everything is on fire nobody complains that your downloads are not working so we kind of gave up on making stuff unnecessarily complex because there was just no point of that but we you could very nicely see that normally there yeah we are nearly out of time okay then I then I will stick to slides then you can build the same thing with multiple models here I have broken my data down into the response codes that my website is sending out and then you can find further anomalies so for example here I have an over the normal is code that is like the top one is kind of like this is anomalies over all the red things are where the anomalies are and then you can break the entire thing down into the response code so you can see for the 404s we have a band of trouble over there this was for example our on our block and we suddenly had a spike of 404 this is when we published a bad link on our own block and suddenly the 404 started to spike and this was when we had in our CMS since we're using a CMS and CMS are mostly crap when we kind of broke some links internally unintentionally with the CMS change you could totally see that here that we suddenly had this pipe of anomalies where yeah suddenly stuff is not going well you can have combined multiple models and see what they're doing so here I'm combining this was the remote IP addresses the example which I've shown before where you can see this was when our website went down because of Amazon s3 so this is here then was the a3 outage because nobody could access our data anymore and these are the 404 and these two totally look correlated right so something is happening here and then suddenly the 404 spiked unfortunately these two events were totally unrelated because this was at the AWS outage and this was our own change on the CMS and that will screw off or throw off any machine running because it tries to learn and correlate some data but if there is no real correlation it will just be kind of like try to infer one which is not there so yeah correlation doesn't mean it's actually a causation everybody knows that but always keep that in mind and yeah it's kcd and extract the best once you take a statistics class does it help to make the distinction maybe yeah and then they're always like what is the cause and what is the reason or like what is the cause and what is the effect um for example for cancer waste it's very interesting study that in the US and the cancer rate has spiked and cell phone usage spiked afterwards so here the theory could be that cancer is causing cell phones which is also not the right thing but if you just look at the data and misinterpreted that is what you might get um yeah any correlated features would mess up your system there's unfortunately no way around that yeah finally thing here what you can also do is future predictions so the yellow stuff here these are future predictions and you can see the further it goes into the future the less certain they are because the more coarsegrained the entire system gets but this is very helpful if you like I'm expecting this number of visitors or this is the disk space I'm expecting in the future or the resource usage and this can predict for the future like this is what you will probably have a new system given that there are no other anomalies yeah then you could start doing like categorized users but let's keep that for today so to wrap up we've seen machine learning domain where I picked like mainly ops data and then some nginx data set where I've played around with if you want to know more about machine learning one a very nice paper I've seen is this one here best practices for machine learning engineering which has some very nice rules so for example the rule number one is don't be afraid to launch a product without machine learning because you might not need it and then yeah if you can interpret the model properly and debugging will be much easier if it's not just a black box spits out random data and plant launch and iterate on stuff but it has 43 rules all about machine learning which probably makes sense if you build anything in that space and yeah and don't forget maybe at some point machine learning is not the hot new thing anymore but something else comes around so yeah Silicon Valley is probably heading somewhere else already again anyway that's it I think we're pretty much out of time if you have questions come to me find me afterwards talk to me if you want to have stickers I have loads of stickers here so if anybody wants stickers yeah very nice take them with that I think we're done thanks very much wait I'm always taking a picture because my colleagues don't know where I am and this is my way to prove that I've been working today smile everybody yeah you can always wave thank you enjoy the break