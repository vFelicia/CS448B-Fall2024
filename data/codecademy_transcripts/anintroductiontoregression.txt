00:05 - We will now study the topic of linear regression.
00:08 - Where we will seek to understand, what exactly is meant by regression,
00:12 - what is the kind of problem it is meant to solve, and
00:14 - how exactly regression is applied.
00:17 - Previously in the learning path,
00:19 - we have covered that regression is a technique which is used in
00:22 - order to make a prediction on a continuous value.
00:25 - So given a set of input features, for example, the features describing
00:29 - a particular house, a value which can be predicted is the cost of the house.
00:34 - To understand exactly how this can work,
00:36 - let us consider an example where we have a single feature.
00:40 - Which is the amount of exercise performed by some individuals, and
00:43 - a single output value which needs to be predicted,
00:46 - which is the weight of the individuals.
00:49 - In order to understand the relationship between exercise and
00:52 - weight, it is useful to plot the data in a two-dimensional plane,
00:56 - just as we have over here.
00:57 - And a quick glance confirms to us that there does seem to be
01:00 - some kind of relationship between the two.
01:03 - So how exactly can we express this relationship?
01:06 - Well, we could draw some kind of curve, for
01:09 - example, this squiggly one over here.
01:11 - Which will go through many of the points and
01:13 - will be very close to the others in a plot.
01:16 - On the other hand,
01:17 - the relationship could be modeled using a simple straight line.
01:20 - So for example, there could be these two different straight lines which go
01:23 - through many of the points.
01:24 - And we could consider either of them to be representative of
01:27 - the relationship between exercise and weight.
01:32 - When any relationship is modeled using a straight line or
01:35 - a plane, rather than some kind of squiggly or
01:37 - complex curve, we say that the relationship between the variables is linear.
01:42 - We have already seen that there can be multiple curves or
01:45 - lines which can represent the relationship between two variables.
01:48 - But which of these happens to be the best fit line or curve?
01:53 - Well, simply put, it is that line or
01:55 - curve which happens to be closest to the points in your data.
01:59 - So once you have drawn a line or
02:00 - a curve, you can drop vertical lines from each of your points to that curve.
02:05 - And for each point, the length of the line determines the difference between
02:09 - the weight which will be predicted by the curve, and
02:11 - the actual weight of the individual.
02:14 - As you can imagine, the curve which minimizes the distance between each
02:17 - point and the curve itself is the one which can be considered to represent
02:21 - the best fit for your data.
02:24 - However, there is one caveat when using very complex
02:27 - curves to represent your data.
02:29 - Specifically, your curve may represent an overfitted model.
02:33 - So if all of the green points over here represent your training data, and
02:36 - you come up with this complex curve to model the relationship between
02:40 - exercise and weight.
02:41 - There is the very high likelihood that your model is very specific
02:44 - to your training data, including its own biases.
02:48 - So when it is time to make predictions on the real data, such as this
02:51 - orange point over here, then the model may not perform particularly well.
02:56 - Which is why it is often best to come up with a more general solution,
02:59 - in order to model the relationships between your data.
03:02 - And this is exactly why a best fit straight Line
03:05 - is often preferred over a complex curve.
03:08 - This is precisely what a linear regression accomplishes,
03:11 - where a best fit straight line is found to
03:13 - represent the relationship between your inputs and your outputs.
03:18 - So just to summarize, when we need to create a machine learning model
03:21 - to predict a continuous value,
03:23 - such as the weight of an individual, using a collection of input features,
03:28 - then the solution is to perform a linear regression.
03:30 - That is, you find the best fit straight line or plane
03:33 - which is able to model the relationships between the inputs and the outputs.
03:38 - But how exactly do we determine this best fit straight line?
03:42 - Well, there is a mathematical way to solve this problem.
03:45 - And for that, consider two different straight lines which
03:47 - are represented by the equations y = A1 + B1x,
03:51 - which is the first straight line you see on the top over here.
03:55 - And then there is a second straight line,
03:56 - which is the horizontal one at the bottom.
03:59 - Intuitively, you will know that it is the first straight line which
04:02 - represents a better fit than the second one.
04:04 - But how exactly can we quantify this?
04:07 - Well, as we touched upon previously, we will drop
04:10 - these vertical lines from each of the data points to each of the two lines.
04:14 - And for each point in a dataset, this distance will represent the error for
04:18 - that point.
04:18 - We will then square up the errors.
04:20 - And then for each line, we will sum up the square of the errors.
04:25 - The best fit regression line is that where the sum of the square
04:28 - of the errors is minimized.
04:30 - So once we have done that, we have quantified the fact
04:33 - that line number 1 is a better fit for our data than line 2.
04:37 - So the sum of the squares of the errors is one way to quantify
04:41 - the quality of a regression line.
04:43 - And similarly, the mean of the square of the errors, where you divide the sum
04:47 - of the squares by the number of data points, is another metric.
04:51 - There is one more evaluation metric for regression lines.
04:54 - And this is something which, once again,
04:55 - measures the quality of the fit of the data.
04:58 - And this is known as the R-square metric.
05:01 - So higher the quality of the fit,
05:03 - the higher is the value of the R-square for the data.
05:06 - So considering these data points,
05:09 - if we have a straight line which represents the relationship.
05:12 - And all of the points are very close to this regression line,
05:15 - the value of R-square will be very high.
05:18 - And one thing to note about R-square values is that this range is from
05:21 - a minimum of 0 to a maximum of 1.
05:25 - You can consider the R-square value to represent
05:27 - how well your regression line captures the variance in the underlying data.
05:32 - So while this particular straight line captures most of the variance in
05:35 - the data, a line such as this one
05:38 - which happens to be very far away from most of the points.
05:41 - Does not capture a lot of the variance, and
05:43 - will thus have a low R-square number.
05:45 - So once we have gone through a training phase in order to find the best fit
05:49 - linear regression line,
05:50 - we are then ready to use it in order to make predictions on real data.
05:54 - So if you have come up with a straight line,
05:57 - which happens to be the model representing the relationship between
06:00 - the amount of exercise performed by individuals and their weight.
06:04 - If we need to predict the weight of an individual,
06:06 - and we're only given the amount of exercise they get,
06:09 - we can simply plot the amount of exercise on the x-axis.
06:12 - And this is represented by the lower orange dot.
06:15 - We will then extend a vertical line from this point on the x-axis,
06:19 - over to our regression line.
06:20 - And then, produce a horizontal line all the way to the y-axis,
06:24 - in order to predict the weight of that individual.
06:26 - Since any regression line can be represented by the equation,
06:29 - y is equal to a plus bx, given an input, x,
06:33 - we can calculate the prediction y using that equation.