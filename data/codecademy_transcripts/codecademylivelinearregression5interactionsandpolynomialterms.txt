00:00 - um i'll read it out loud it says hi
00:03 - um what i'm wondering most is if the
00:05 - discussion here will have differences
00:06 - with the lessons on linear regression in
00:08 - codecademy
00:10 - that is a good question um there will be
00:14 - some differences just in that this is
00:17 - more of an open discussion and so if
00:18 - there's questions in the chat or there
00:20 - are specific things
00:21 - um that anybody wants to talk about uh
00:24 - we might
00:24 - go on some tangents and um and cover
00:27 - some things that are not in the lesson
00:29 - but for the most part we're gonna start
00:31 - by uh
00:32 - going through the the same content
00:36 - that's in the lesson
00:37 - yeah cool cool we are back in the office
00:41 - with uh
00:42 - our fancy streaming setup so we're
00:44 - trying out like new mics
00:45 - and cameras all that kind of stuff so
00:48 - let us know in the chat if
00:50 - anything is screwed up if you can't hear
00:52 - us if the mic sounds bad
00:53 - um but uh hopefully everything is uh
00:57 - is working fine yeah all right
01:01 - shall we get started let's do it all
01:02 - right so
01:04 - um the topic for today is going to be
01:08 - um interaction terms and polynomial
01:12 - terms which basically are ways that we
01:15 - can
01:16 - alter a linear regression a simple
01:18 - linear regression model or a multiple
01:20 - linear regression model
01:22 - to make it a little bit more flexible
01:23 - and handle some different things
01:26 - that we might see in some data
01:29 - so um i actually think this is exactly
01:32 - the same
01:32 - to the question that was asked earlier
01:34 - this is exactly the same data set
01:36 - that we give you in the lesson on
01:38 - codecademy
01:39 - and i believe it is simulated i didn't
01:42 - write this lesson
01:43 - um but the idea
01:46 - behind this this data is that we're
01:48 - imagining
01:49 - um some sort of study that measures
01:54 - self-reported levels of happiness self
01:57 - reported or maybe like measured levels
02:00 - of stress
02:01 - um whether or not a person exercises
02:05 - um i think sleep is probably
02:08 - hours of sleep make sense 13.8 it's a
02:10 - little extreme
02:13 - maybe and then maybe hours of free time
02:16 - per day i think that's the intention of
02:19 - this data set it's
02:20 - i'm pretty sure simulated though so not
02:22 - not real
02:24 - cool so um what we're gonna get started
02:28 - with is uh visualizing some of this data
02:31 - um you'll notice actually in the
02:35 - lesson on codecademy and i just grabbed
02:38 - this code directly from there
02:41 - you'll notice that we actually use this
02:43 - lm plot function
02:45 - to do a lot of the plotting in this
02:49 - in this lesson the reason for that is
02:52 - partially because
02:54 - uh we haven't updated uh
02:57 - matplotlib and seaborn in a little while
02:59 - so
03:00 - this is an older function in seabourn
03:04 - that does the same thing as the newer
03:05 - scatter plot function
03:08 - um but also it has the option to fit a
03:11 - regression line
03:12 - directly onto the plot if you wanted to
03:15 - um
03:16 - and so i'll just demo that really
03:18 - quickly
03:19 - um here actually i'll demo it in a
03:22 - separate plot
03:25 - so if i say let's look at the
03:29 - relationship
03:30 - between stress and happiness so
03:35 - is there a relationship between how
03:36 - stress someone appears to be and how
03:38 - happy they report being
03:40 - um and i say fit reg
03:43 - equals true and i think that's also the
03:46 - default
03:47 - and i run this i get
03:51 - here are the points and then it'll
03:54 - automatically draw this regression line
03:56 - for me
03:57 - which is kind of cool and it allows you
03:59 - to
04:00 - do this a little do this plotting a
04:02 - little bit more quickly
04:04 - um which is which is always fun there
04:06 - you'll notice that
04:08 - something interesting happens actually
04:10 - if i leave this is fit rug
04:11 - equals true and add q
04:15 - equals exercise right so now it's going
04:18 - to even be a little bit hard to
04:20 - visualize right because there's uh
04:24 - two variables there so three variables
04:25 - right there stress happiness
04:27 - and exercise yeah um
04:30 - it it's pretty smart actually it will do
04:32 - something here
04:35 - actually draw me two regression lines
04:38 - one for exercise equals one and
04:42 - the other for exercise equals zero
04:46 - alex do you notice anything weird about
04:48 - this plot though that's like different
04:49 - from
04:50 - what we might expect based on everything
04:53 - we've covered so far like
04:55 - do you remember i think maybe it was the
04:57 - last stream you joined me on
04:59 - where we started talking about um
05:02 - using this using a categorical variable
05:05 - in our regression
05:06 - and we we plotted um some regression
05:10 - lines
05:11 - when we included a categorical variable
05:15 - in the regression right and the the two
05:18 - lines had something
05:20 - different from what we're seeing here um
05:23 - you remember
05:24 - no i don't recall okay so
05:28 - um so actually the two lines were
05:30 - parallel
05:31 - so um there was no way previously
05:35 - or we didn't have a way previously to
05:38 - draw um to draw regression lines that
05:42 - were like
05:42 - totally just fitting the orange points
05:45 - and the blue points separately we could
05:47 - only change
05:48 - the intercept so if you go back you
05:51 - watch the
05:52 - older video you'll see that in action
05:55 - um and today we're gonna actually learn
05:57 - how to
05:58 - fit these lines um so that
06:01 - they can have different slopes yeah and
06:04 - in that graph
06:05 - is are these lines so say you took out
06:08 - all of the blue circles
06:09 - um and just fit it to the orange axes is
06:12 - that the orange line and similar
06:14 - to the other way around so they're
06:16 - completely independent of each other
06:17 - they're not um they're basically
06:19 - ignoring the other set of points
06:23 - um if you fit yeah if you fit
06:27 - just a regression for the orange line or
06:29 - orange points and just regression for
06:31 - the blue points you'd get these lines
06:33 - okay cool um so
06:36 - i'm gonna now just show you this plot uh
06:39 - without the line i
06:40 - lines i think i kind of gave away via
06:43 - the end goal before we even got started
06:45 - so we'll start back at the beginning
06:47 - and we'll motivate this by saying
06:49 - sometimes
06:50 - when we look at a plot
06:54 - of two quantitative variables and then
06:57 - we color by categorical variable
07:00 - we'll see that if we were to draw a line
07:03 - through just the blue points and if we
07:04 - were to draw a separate line through
07:06 - just the orange points
07:07 - those points would or those lines would
07:09 - probably have different slopes
07:11 - and different intercepts whereas up to
07:13 - this point
07:14 - if we just added um so if we just fit a
07:17 - model where we did like happiness
07:19 - as a function of stress and exercise
07:23 - we'd end up with a model that allows us
07:26 - to draw separate lines for these two
07:29 - sets of points but they would have to
07:30 - have the same slope they could just have
07:32 - different intercepts
07:34 - and we'll i'll show you on the ipad in
07:36 - just a second mathematically
07:38 - why that happened and then i'll show you
07:40 - how we can
07:41 - now fit a regression that visually would
07:45 - look like this
07:46 - cool cool so um
07:50 - what i'm gonna do is i'm going to
07:54 - here let's add a um
07:58 - let's add a chunk and let's fit this
08:00 - regression
08:01 - right off the bat um i'm gonna grab this
08:05 - code from over here
08:07 - and we're gonna talk through it for a
08:09 - second and then
08:11 - we'll come back to this in a minute okay
08:15 - so i'm fitting this model i believe i've
08:17 - already
08:18 - loaded yes um so i'm fitting this model
08:23 - and it's happiness as a function of
08:26 - stress
08:27 - plus exercise plus stress
08:30 - colon exercise now for a second
08:34 - let me remove this this is what we've
08:36 - done so far
08:38 - in previous um live streams so if i run
08:41 - this
08:43 - this is kind of what we've seen before
08:46 - we get an intercept
08:47 - we get a slope for stress and we get a
08:50 - slope
08:51 - for exercise same intercept so these
08:54 - lines aren't parallel
08:56 - it's same intercept and different slopes
08:59 - so this these lines actually are
09:00 - parallel so
09:02 - actually this is a good a good place to
09:04 - kind of like come back to this
09:06 - i hadn't initially planned on it but i
09:08 - think this is like a good
09:09 - reminder so um let me
09:13 - actually you can flip me over but
09:17 - um can we leave that up yes okay
09:20 - great so this is for the um
09:23 - the other one that had the stress colon
09:25 - exercise but i'm just gonna write down
09:27 - the um the values for the other
09:30 - regression for a second
09:31 - so for this one we had an intercept
09:35 - of 10 about we had
09:38 - a slope on stress
09:42 - that was equal to negative 0.7
09:46 - and then we had a slope on exercise
09:51 - that was equal to about negative 0.9
09:56 - and we fit this model it was happy
10:01 - as a function of stress
10:04 - plus exercise and what that meant
10:08 - was that we were fitting this model we
10:10 - fit something that looked like
10:12 - happy equals
10:16 - intercept which we called b0 but we'll
10:19 - replace that
10:21 - plus b1 times stress
10:26 - plus b2 times exercise
10:29 - it's gonna abbreviate
10:33 - and so plugging in b0 b1 and b2
10:37 - we got something like happy
10:40 - equals 10 plus
10:43 - well it's really uh plus negative so
10:47 - minus 0.7 times stress
10:52 - minus 0.9 times
10:55 - exercise but remember that there
10:59 - are only two possible values of exercise
11:01 - someone can exercise or not
11:03 - exercise got it so we ended up with two
11:06 - separate
11:07 - equations one when exercise
11:10 - equals zero we get happy
11:14 - equals 10 minus 0.7
11:18 - times stress minus 0.9 times 0 but 0.9
11:22 - times 0 is 0.
11:23 - so that kind of goes away and then the
11:26 - other one we got right down here
11:30 - when exercise equals 1 we get happy
11:35 - equals 10 minus 0.7
11:38 - times stress minus
11:42 - 0.9 times 1 which is just 0.9
11:45 - and so then we end up with happy equals
11:49 - um we can take the 10 and subtract the
11:52 - 0.9
11:53 - right because order of operations we can
11:55 - all of these
11:57 - addition and subtraction things can
11:59 - happen in any order so we end up with
12:01 - like
12:02 - 9.1 minus 0.7
12:06 - times stress got it so same slope
12:09 - different intercepts and so
12:10 - they both have the same slope on stress
12:13 - but the intercept is 10
12:17 - when exercise equals zero and the
12:19 - intercept is 9.1
12:21 - when exercise equals one and so that
12:23 - slope on
12:24 - exercise is really the difference in
12:26 - intercepts
12:28 - for the two lines that we would draw oh
12:30 - i'm now realizing that this is
12:31 - that we are covering the uh
12:36 - oh can't
12:39 - darn it sorry guys oh alex i don't even
12:42 - know
12:43 - it's there all right
12:47 - we'll figure it we're figuring out the
12:48 - green screen sorry guys
12:51 - um hopefully that was clear though uh
12:54 - okay
12:54 - so now let's go back to
12:58 - that second um let's can we switch back
13:01 - to the uh
13:03 - that's just okay yes um
13:06 - okay so that was when we just fit this
13:09 - where we have
13:09 - happy as a function of stress plus
13:11 - exercise
13:13 - but now we're gonna add an interaction
13:16 - term
13:16 - and the way we can do this in stats
13:19 - models
13:20 - is we can just say i want an interaction
13:23 - between
13:23 - stress and exercise
13:27 - by putting a colon between them
13:30 - so now when we run this model run it
13:34 - um now we get an intercept
13:38 - we get a slope on stress
13:41 - and then we also get a slope on exercise
13:44 - just as before
13:45 - but we get now a third slope
13:48 - slope on stress colon exercise
13:53 - so now we're going to take this
13:56 - and we'll go back to the ipad in just a
13:59 - second
14:01 - and talk through what those
14:06 - sorry i'm erasing really quickly um
14:10 - talk through what those translate to
14:13 - in terms of our model okay
14:17 - so now
14:21 - oops well that's okay um so now we see
14:24 - okay
14:25 - we had intercept stress
14:28 - exercise stress colon exercise all an
14:32 - interaction term is is an extra
14:35 - it's almost like having an extra column
14:37 - in your data that is the product
14:40 - of two other columns so
14:43 - it's almost like an extra feature that
14:45 - is just the product of two others
14:48 - so the way this looks like in terms of
14:51 - the model
14:52 - is we're fitting now happy
14:58 - screwed up let's see
15:04 - so now we're fitting happy
15:08 - equals b0
15:11 - plus b1 times stress
15:16 - plus b2 times exercise
15:21 - plus b3
15:24 - times stress times exercise
15:28 - times sign okay okay
15:32 - so in like for example
15:36 - in um scleren you have to create
15:41 - a um an interaction term yourself in
15:44 - your data frame and literally what you
15:46 - would do and we could demonstrate this
15:47 - after
15:48 - is you would just multiply everything in
15:51 - the stress column
15:52 - by everything in the exercise column and
15:54 - create a new column
15:56 - of your data frame called stress
15:58 - exercise
15:59 - and then you would just add that into
16:01 - your model
16:02 - as a predictor i think i'm a little bit
16:05 - confused of like what the overall goal
16:07 - is here
16:08 - because so in in this situation isn't
16:10 - this gonna still result in two
16:12 - parallel lines because that times one
16:15 - the time
16:15 - zero is still going to be um i can't
16:18 - even
16:18 - uh that's not pointing at anything it's
16:20 - still gonna be right when when
16:22 - exercises is equal to 0 the
16:25 - b2 times exercise and b3 times stress
16:28 - times exercise all of that is just going
16:29 - to
16:30 - go to zero so isn't this really similar
16:32 - to the last thing that we just looked at
16:34 - it's very similar but let's actually
16:36 - walk through exactly that
16:38 - so let me plug in all of these numbers
16:40 - so we've got
16:41 - happy equals
16:45 - our b0 is 12 that's still our intercept
16:49 - um then we've got b1
16:52 - it's negative so we're gonna do b1 is
16:55 - negative 0.97 so minus 0.97
16:59 - times stress and then
17:02 - again the negative sign minus 3.1
17:06 - times exercise and then we've got
17:11 - plus .36
17:15 - times stress times
17:18 - exercise okay
17:21 - now let's break this up into two
17:24 - separate
17:25 - um
17:28 - two separate equations
17:31 - feel like this eraser never works the
17:34 - way i wanted to
17:35 - um okay so let's break it up into two
17:38 - separate ones
17:39 - one where exercise equals zero and one
17:42 - where exercise equals one
17:43 - so i think you alex were doing the
17:45 - exercise equals zero
17:46 - one in your head first so um it's a
17:50 - little simpler so
17:51 - let's do that first so
17:55 - when exercise equals zero we've got
17:59 - happy equals
18:02 - 12 minus .97
18:05 - times stress and then we've got
18:08 - minus 3.1 times zero
18:13 - plus .36 times stress
18:18 - times zero now
18:21 - like you were saying right anything
18:24 - times
18:24 - zero is zero so 3.1 times 0 is 0
18:29 - but also 0.36 times stress times 0
18:33 - even though we're multiplying stress in
18:34 - there it's still like a bunch of things
18:36 - multiplied by zero so that's also zero
18:40 - okay so that's zero that's zero
18:44 - so our equation is really just
18:48 - happy equals
18:52 - 12 minus point nine
18:55 - seven times stress
19:00 - okay so we've got our line again
19:03 - for exercise equals zero now let's try
19:07 - exercise equals one
19:11 - this is where it gets a little bit more
19:12 - complicated yeah
19:14 - so we've got um and also if anybody has
19:17 - questions as
19:18 - we go i know this is like again a little
19:20 - bit of math
19:21 - um so ask us questions if they come up
19:26 - but for exercise equals 1 we've got
19:29 - happy
19:30 - equals 12 minus 0.97
19:33 - times stress just like before
19:36 - then we've got um minus
19:41 - 3.1 times 1
19:44 - and then we've got plus
19:49 - 0.36 times stress
19:54 - times one got it okay
19:57 - so multiplying by one doesn't change the
20:00 - value of anything
20:02 - right so we can do the same thing as
20:04 - before with our
20:06 - with our intercept rate so like this
20:09 - minus 3.1
20:10 - times 1 we can just add to
20:14 - the 12 again order of multiple
20:17 - order of operations since this is just
20:19 - like multiplication
20:21 - and addition we can order we can add or
20:23 - subtract these terms in any order so we
20:25 - can
20:26 - combine these two together
20:29 - so i'm going to do that first and then
20:31 - i'll write out the other set
20:33 - but some of you might already kind of
20:35 - see what that's going to be
20:37 - so we've got happy equals and then 12
20:40 - minus 3.1
20:41 - is like 8.9 right
20:46 - and then we've got minus .97 times
20:49 - stress and then we also have
20:52 - plus .36
20:55 - times stress and then i'm gonna just
20:58 - drop the one because
20:59 - anything times one is just equal to
21:01 - itself
21:03 - and now we've got we've got minus 0.97
21:06 - times stress
21:07 - plus 0.36 times stress and so
21:11 - both of these things are multiplying
21:13 - stress
21:14 - so we can actually factor stress out
21:18 - of this like piece right here
21:22 - so stress is multiplying both things
21:26 - we can factor it out and add the 0.9
21:30 - minus 0.97 and 0.36 together
21:33 - and so what we get is something that
21:35 - looks like this
21:36 - we get happy
21:40 - equals 8.9
21:43 - and then we've got um
21:48 - i guess i'll put like plus stress
21:54 - times and then we've got
21:57 - the minus 0.97
22:01 - plus 0.36 oops
22:05 - 0.36 and that
22:08 - um again like thinking about how you
22:11 - would
22:12 - calculate this or like how you would
22:14 - expand this right you would multiply
22:16 - stress by both things inside the
22:18 - parentheses so you'd get like
22:20 - you'd get back to what you had above
22:21 - you'd have the stress times minus 0.97
22:24 - that's this term from before
22:27 - and then you'd get the stress times .36
22:30 - which is this term
22:31 - so we just factored it out but we
22:33 - rewrote that same thing
22:35 - as before and then minus 0.97
22:39 - plus 0.36 uh
22:43 - 61.
22:46 - yeah um is
22:49 - yeah so we got happy equals 8.9 plus
22:56 - 0.61 or sorry minus 0.61
23:06 - i guess i shouldn't have switched the
23:07 - order but you can multiply in
23:10 - either order so um okay
23:13 - so that's our equation for
23:16 - exercise equals one that's our equation
23:19 - for exercise equals zero
23:21 - and you'll see that they have different
23:23 - intercepts
23:24 - 8.9 versus 12 and they have different
23:27 - slopes minus 0.61 versus minus 0.97
23:33 - cool cool and so right the change in
23:36 - slope
23:37 - came from this term that included both
23:40 - stress and exercise that was the thing
23:42 - that ultimately affected
23:44 - the slope when exercise was one
23:48 - exactly um
23:52 - can you briefly describe again how the
23:54 - 0.36 was calculated from the original
23:56 - data right so these are these numbers
23:57 - that are up in the top left corner
23:59 - where do where do those even come from
24:01 - oh so
24:02 - these numbers came from fitting the
24:05 - model
24:06 - um so we'll go back to the jupiter
24:07 - notebook in a second um
24:09 - but basically when we fit the model with
24:11 - that interaction term it calculated a
24:13 - slope
24:14 - on the the interaction between
24:18 - stress and exercise and that is
24:21 - basically like a multiplier right of
24:24 - stress times exercise
24:26 - um so then we just followed that out in
24:28 - the model
24:29 - i also see a really good question in
24:32 - here
24:32 - about it says would we consider the
24:35 - third variable stress times exercise
24:38 - dependent on the values of stress and
24:40 - exercise
24:41 - that you get more accurate models when
24:43 - all your feature variables were
24:44 - independent so that's a really good
24:46 - question
24:48 - i'm not sure how great of an answer i
24:52 - can give but
24:53 - um all to say so yes
24:57 - you you don't want to have
25:00 - uh co-linear very or you don't
25:03 - want to have highly correlated variables
25:06 - in
25:06 - um in a model together because
25:10 - they're both if they're both
25:13 - explaining your outcome variable
25:16 - then the model is going to have some
25:18 - trouble
25:20 - estimating the coefficients for both of
25:22 - them
25:23 - because they're both like they both have
25:25 - the same explanatory power
25:27 - in terms of your outcome variable um and
25:30 - it also
25:30 - right if they're highly dependent like
25:33 - if you just take
25:34 - um if you take one of the columns and
25:37 - multiply it by a number
25:38 - theoretically you shouldn't be able to
25:40 - fit that model at all because
25:42 - it's perfectly your new variable is
25:44 - perfectly correlated with your old
25:46 - variable
25:47 - in this case it's not perfect it's not
25:50 - like this
25:50 - new variable that we created by
25:52 - multiplying stress and exercise
25:55 - it's not perfectly correlated with
25:57 - either stress or
25:58 - exercise because we're multiplying by a
26:00 - different
26:01 - value for each so like if you think just
26:04 - about
26:04 - stress whether you multiply it by zero
26:08 - or multiply by one depends on the value
26:11 - of exercise
26:12 - so there's no way you could just take
26:16 - that column without seeing the exercise
26:18 - column
26:19 - and and get the new one right it's not
26:21 - like if you were
26:22 - just doubling stress that would be
26:24 - perfectly correlated with stress but
26:25 - there's
26:26 - um yeah it's either you're sometimes
26:29 - multiplying it and sometimes not
26:30 - depending on if exercise is zero or one
26:32 - yeah but on the flip side there are
26:36 - reasons why you might not want to
26:39 - just create like the most complicated
26:42 - model that you can
26:43 - with as many extra predictors as like
26:46 - this as
26:47 - you possibly could create because you
26:48 - could imagine like if you have
26:51 - i don't know 10 columns of a data set
26:53 - which is even in real life that's like
26:55 - pretty small um you could imagine
26:58 - you could create an interaction term for
27:00 - every possible
27:02 - pair of pair of variables basically
27:06 - pair of features and that would be a
27:09 - very large number
27:10 - that i i don't know what it is but
27:13 - yeah but that would be a very large
27:15 - number and you could create a very
27:16 - complicated model to model like every
27:18 - possible interaction
27:20 - um but that's not
27:24 - always a good idea because you end up
27:26 - what ends up happening is you end up
27:28 - over fitting and that's kind of
27:31 - actually a very important topic in a
27:33 - newer course that we're building right
27:35 - now
27:35 - on feature engineering um and like
27:38 - regularization
27:39 - um but basically if you if you fit your
27:43 - data
27:43 - too well then you're not your model is
27:45 - not going to be very good at
27:47 - predicting outcomes for new
27:50 - data so like this this model if i make
27:52 - it super complicated
27:54 - might perfectly fit my my data that i
27:57 - collected on my like 100 patients or
27:59 - whatever
27:59 - but if i go out and i find another
28:01 - patient like this might not be a good
28:04 - way of explaining the relationship
28:05 - between these things
28:06 - right because you were like so dependent
28:07 - on those original 100 patients
28:09 - exactly um can you
28:13 - describe just like the total takeaway or
28:16 - upshot of of this whole process because
28:18 - what i'm walking away from this is
28:20 - before we were able to draw two lines
28:22 - that were parallel
28:24 - that kind of stinks for some reason and
28:26 - now we can draw two lines that aren't
28:28 - parallel like
28:28 - what is what is like the thing that i
28:30 - should be taking away from
28:32 - creating this uh this new variable will
28:34 - you go back to the
28:35 - the picture or the um yeah the browser
28:38 - the browser
28:39 - yeah so that's a really good question i
28:41 - think so
28:42 - a couple of things first of all um
28:47 - it's just you know if you were to plot
28:50 - your data and you see this picture maybe
28:52 - you see something even
28:54 - more obvious like maybe you see these
28:57 - lines look completely different like
28:58 - maybe you even see
29:00 - that for um people who do
29:03 - exercise the relationship between
29:05 - happiness and stress
29:07 - is this like negative relationship like
29:10 - people who are
29:11 - less happy are more stressed but for
29:13 - people who don't exercise maybe it's
29:15 - like a positive
29:17 - relationship and so what you actually
29:19 - see sometimes and this is
29:21 - sometimes i think referred to as
29:23 - simpsons paradox and we actually have
29:25 - here i can go and then um show you
29:28 - in the lesson as a note so we're using
29:33 - new uh streaming software and your mouse
29:35 - pointer isn't showing up on the uh
29:37 - on the software so if you're ever
29:38 - gesticulating with the mouse uh that's
29:40 - not getting backed up good to know
29:42 - something we can uh play around with
29:45 - is it oh it might not be in this lesson
29:48 - which lesson is it in um
29:55 - let me see if i can find it quickly i
29:57 - think it's
29:58 - in
30:02 - this lesson
30:07 - um yes
30:12 - so this is like
30:15 - a more complicated example here let me
30:17 - like
30:18 - pull this up um but you'll see
30:22 - right like in this in this example we've
30:25 - got
30:26 - um like a positive if we were to just
30:29 - like
30:30 - ignore this um categorical variable
30:33 - and draw a line through all the points
30:35 - we would draw this black line
30:36 - which is like this positive relationship
30:39 - but then
30:40 - if you add this um
30:43 - this like categorical variable to the
30:46 - model
30:47 - and you're trying to draw the lines now
30:50 - based off of
30:51 - um you're basically trying to draw the
30:53 - lines
30:54 - through each set of points in this
30:57 - example they're parallel so like
30:59 - we're not constructing but you'll see
31:00 - right that like
31:02 - the um the lines have negative slopes
31:06 - even though the initial relation like
31:08 - the relationship
31:09 - between these two variables in among all
31:12 - the points is positive
31:14 - if you zoom in you now are getting these
31:16 - like negative relationships
31:18 - and so you can see stuff like that where
31:20 - like
31:21 - maybe right maybe one of these
31:24 - relationships is
31:25 - negative and the other one is positive
31:27 - and if you
31:28 - didn't fit anything like you didn't fit
31:31 - um
31:33 - any sort of exercise term
31:36 - like you'd end up with a flat line you'd
31:40 - think there's like no relationship
31:41 - between happiness and stress but like
31:43 - once you add this the model you suddenly
31:45 - start seeing them
31:46 - but now if you are restricting this so
31:49 - that those lines have to be
31:50 - parallel then you're like ending up with
31:54 - you're gonna end up with maybe like two
31:56 - parallel lines that are flat
31:57 - and that's like not gonna model your
32:00 - data super well so
32:01 - there's definitely situations where
32:02 - adding an interaction term
32:04 - means that you significantly improve
32:06 - your model from where like you basically
32:08 - can't
32:09 - explain anything to um oh it looks like
32:12 - i didn't we didn't switch over maybe
32:14 - um that's possible oh sorry yeah you're
32:16 - good um okay
32:17 - uh but yeah so sometimes it
32:20 - significantly improves your model
32:22 - i think like the flip side or the other
32:24 - thing
32:25 - that maybe is like the other half of the
32:27 - answer to that question
32:28 - is and maybe this gets a little bit more
32:31 - into like when we do this
32:33 - um is we do this when we think
32:36 - that the relationship between to the
32:39 - outcome variable
32:40 - and one of our features is
32:44 - moderated by or like influenced by some
32:47 - other
32:48 - factor um so in this example
32:52 - uh it looks like
32:55 - right the relationship between stress
32:58 - and happiness
32:59 - is maybe like less
33:02 - steep here i'll do it with the lines um
33:06 - is less steep for people who
33:09 - do exercise so like maybe people who are
33:12 - exercising
33:14 - are more like immune
33:17 - to stress in general and um
33:20 - and so like being more stressed
33:23 - has less of an effect on their happiness
33:26 - than
33:26 - people among people who don't exercise
33:28 - it's a steeper line
33:30 - and so we're saying that like we think
33:31 - that you know
33:34 - more stress has like more of a effect
33:37 - i say in quotes because like we're not
33:39 - really depending on how the studies run
33:41 - we don't know if it's like a causal
33:42 - relationship but
33:43 - the effect of stress on happiness is
33:46 - more significant um among people who
33:49 - don't exercise and so we think like
33:51 - there's some
33:53 - exercises moderating this relationship
33:55 - and that's why we might think like we
33:57 - want
33:57 - to we want to be able to model that in
34:00 - some way
34:01 - this is maybe a tricky question or uh
34:06 - this may be a tricky question but like
34:07 - how do you do this
34:09 - in the real world of like so you just
34:12 - described oh we have this situation
34:13 - where
34:14 - one set of data points has like a
34:16 - negative slope one has like a huge
34:18 - positive slope we want to be able to
34:19 - draw these two lines with different
34:20 - slopes
34:21 - in order to capture that and so we want
34:23 - to do something like this
34:25 - but do you know that by plotting the
34:27 - data and looking at it
34:28 - do you like just give both things a shot
34:32 - and
34:32 - like see and see what the models look
34:34 - like after
34:36 - after trying to add it or trying not to
34:38 - add it like how do you actually do this
34:39 - with
34:40 - a data set that you don't know anything
34:42 - about
34:43 - that is a great question um so you can
34:48 - certainly and i
34:49 - highly recommend in fact i'm working
34:51 - right now on some content on
34:53 - exploratory data analysis and it's
34:55 - definitely a good idea
34:57 - to before you fit a model take a look at
35:00 - some of these relationships and try to
35:02 - see
35:02 - whether maybe you have hypotheses like
35:04 - maybe you
35:05 - know something you have some prior
35:08 - research about the relationship between
35:10 - these
35:10 - all of these features and you have some
35:13 - hypotheses
35:14 - about what might be moderating what
35:16 - relationships
35:17 - and then maybe you get some data
35:20 - and you want to take a look at it before
35:22 - you try to fit the model
35:24 - and so that's one thing you would do um
35:26 - the other thing you can do which is more
35:28 - like post talk
35:29 - is you can um you can compare different
35:33 - models so you could fit it both ways
35:35 - and then you could see um you know like
35:38 - the simplest thing is you might
35:39 - calculate like a mean squared error
35:41 - like basically um or a or a
35:45 - total squared error or whatever um
35:48 - basically like how far off are all these
35:50 - points
35:51 - from their respective lines and you
35:54 - could calculate that for
35:56 - both models the one where you don't
35:58 - include the interaction and the one
35:59 - where you do
36:00 - um and then you could say like okay
36:03 - i see that when i add this term to the
36:05 - model um
36:06 - it significantly improves its ability to
36:08 - explain
36:09 - this data um and you'll actually see if
36:12 - you're following along in this linear
36:14 - regression course
36:15 - um there is a
36:19 - a section here on choosing a linear
36:21 - regression model
36:22 - that starts to cover some of those
36:24 - methods that you could use to compare
36:26 - two different models once you fit them
36:28 - um
36:29 - to get a sense for whether whether both
36:32 - of them make sense
36:33 - or whether one makes sense over the
36:35 - other then there's also additional
36:38 - additional things you can do like
36:39 - regularization which nithya is working
36:41 - on right now
36:42 - um that can even further
36:45 - like help you basically like fit the
36:48 - model at the same time as estimating
36:50 - whether like those parameters are useful
36:54 - um so cool that was
36:57 - a bit of a tangent but hopefully um
37:00 - i see a question how do you do feature
37:03 - selection
37:04 - in linear regression i've heard a
37:06 - forward and backward elimination methods
37:09 - but don't know what they do um again
37:11 - this is like a plug for
37:12 - all the content that nitia is currently
37:14 - working on
37:15 - um and that will be i think going live
37:18 - at the end of the summer early fall
37:20 - um but essentially so forward and
37:23 - backward elimination
37:25 - are essentially methods for
37:28 - like sequentially adding um terms to
37:31 - your model so like maybe you want to
37:33 - sequentially add different interaction
37:35 - terms and
37:37 - re-evaluate the model every time and
37:39 - then if
37:40 - adding the term improves the model you
37:42 - keep it if adding it
37:44 - doesn't improve the model then you don't
37:46 - keep it and you try something else
37:48 - or you start with like all of them in
37:50 - the model and you sequentially like
37:52 - delete things that are not helping you
37:55 - um that would be backwards elimination
37:58 - and then regularization is
38:00 - essentially a way that you can fit this
38:01 - model um
38:03 - with like an extra term so that
38:06 - you allow some of the coefficients to
38:08 - get really small or even go to zero
38:11 - um and that's kind of a way of like
38:15 - eliminating eliminating those uh from
38:18 - your regression
38:20 - um okay and then
38:24 - suppose i have data can i add two extra
38:26 - decimals on point five
38:30 - six
38:32 - oh you can add as many decimals as you
38:34 - want to run the linear regression
38:36 - um yeah as much i mean
38:40 - i don't know if that's that is that's
38:41 - not the question uh
38:44 - yeah the question is saying like oh if i
38:46 - have this data can i
38:48 - add can i add or the way that i'm
38:50 - interpreting this question is can i add
38:52 - points between the my existing points in
38:55 - like regular steps
38:56 - of what can i add 0.5611.56
39:01 - like can i add data that falls along
39:03 - that line i think that that's what the
39:04 - question is asking but
39:06 - if you want to clarify the question um
39:11 - so i guess like basically
39:14 - i think you can't add data if you're
39:16 - trying to answer a question
39:18 - about some data or like fit a model to
39:21 - some data then you probably don't want
39:22 - to alter your data
39:23 - before you fit the model um but
39:29 - yeah i mean if it's like something that
39:30 - you could collect
39:32 - so like if these are values of stress
39:37 - and you don't have anyone in your data
39:39 - set who reported like a value of 0.5611
39:42 - for stress but you can go out and find
39:44 - someone and you can record their
39:45 - happiness score
39:47 - then you could add that to the model if
39:49 - you don't
39:51 - have or you could add that to your data
39:52 - to fit the model if you don't
39:54 - if you can't find someone with that
39:55 - value you shouldn't just like add that
39:57 - to your data set before you hit the
39:58 - model um
40:01 - i got it and the clarification in that
40:04 - question of if
40:05 - 0.56 is occurring multiple times can i
40:07 - add extra decimals to it
40:11 - so i still don't fully understand but
40:15 - yeah if you see multiple people with
40:17 - this value of 0.56 and you are able to
40:20 - measure
40:21 - more specific values um
40:25 - then yeah like if you are able to
40:27 - measure to the fourth decimal place but
40:28 - most like measurement
40:30 - tools have some limits um
40:33 - i also see a question about
40:34 - heteroscedasticity
40:36 - um which is a fun word i think um that
40:40 - is coming in the next live stream
40:45 - so yeah we'll come back to that next
40:48 - week um
40:49 - cool so i think uh
40:53 - we've got like 20 minutes left i want to
40:54 - cover two more things if we
40:57 - if we can fit them in um so one thing
41:00 - i'll kind of go
41:01 - over a little bit more quickly but
41:04 - i will i will pull this over
41:08 - and re-run in our other notebook
41:11 - um so
41:16 - i just want to demonstrate so up to this
41:18 - point we
41:20 - have shown that we can fit this
41:23 - interaction term with a categorical
41:25 - variable
41:26 - um as one of the terms that we're
41:29 - interacting
41:30 - so in that case we ended up with just
41:33 - two lines
41:34 - because there are only two values of
41:36 - exercise so you basically get like a
41:37 - separate line for each value of exercise
41:41 - um but we can also fit interaction terms
41:44 - with two quantitative variables so
41:48 - here's another plot it shows the
41:50 - relationship between
41:52 - happy and stress and then we've colored
41:55 - by this value or this other variable
41:59 - free time which is like the number of
42:01 - hours of free time that you have
42:03 - um and this is also a quantitative
42:06 - variable
42:07 - we're just showing values uh like
42:10 - integer values
42:11 - for the the key but you could imagine
42:13 - this is like
42:14 - a scale where people that have more free
42:17 - time are darker colored dots
42:19 - and people that have less free time are
42:22 - lighter colored dots
42:23 - in this plot and you could imagine like
42:27 - i i can't do this super easily in my
42:29 - head but you can imagine if you just
42:31 - isolated all the people with
42:32 - five hours of free time you might draw
42:35 - oh i
42:36 - am remembering now that you can't see my
42:39 - my pointer but right
42:40 - so if you uh just isolated all people
42:43 - all this like the darkest purple dots
42:45 - the line that you draw through those
42:47 - might be different from the line that
42:50 - you draw through
42:51 - the lightest color dots to people with
42:52 - zero hours of free time
42:54 - and so um so we can fit this for
42:59 - uh this setup as well
43:02 - so i'm gonna go ahead and grab
43:06 - this model again and we'll
43:09 - edit it a little bit
43:14 - so this time let's edit this let's do
43:16 - stress
43:17 - uh plus free time
43:24 - plus stress interacted
43:28 - with free time and then let's fit that
43:33 - and we can fit something again we see
43:36 - we get our intercept slope on stress
43:40 - slope on free time and also a slope on
43:44 - free time colon strap or stress colon
43:47 - free time
43:48 - um and i'll just write this out super
43:51 - quick on
43:53 - our ipad again
43:56 - um
44:01 - okay cool so here we've got those
44:03 - written out
44:05 - for ourselves and let's write out what
44:06 - this model is super fast
44:08 - so the model is exactly the same as what
44:10 - we had before we've got like
44:12 - happy
44:17 - we've got happy equals b0 which is eight
44:23 - plus um and again with the minuses sorry
44:27 - minus point five five
44:30 - times stress
44:33 - plus point 0.12 times free time
44:41 - and then plus 0.04
44:45 - times stress
44:49 - times free time i'll abbreviate ft
44:53 - okay and then you can see for different
44:56 - values of free time we get different
44:58 - equations so
45:00 - for example when free time equals zero
45:04 - our equation is just like happy equals
45:07 - eight minus point five five times stress
45:11 - and these two other terms go to zero
45:14 - because point one two times zero
45:16 - is zero and .04 times stress times zero
45:19 - so zero
45:21 - and then when free time equals one
45:24 - we get happy equals eight
45:28 - minus point five five times stress
45:32 - and then plus 0.12 times 1
45:36 - which is just 0.12 and then plus
45:39 - 0.04 times stress
45:43 - times 1 which is just 0.04 times stress
45:46 - and so if we simplify this oops
45:50 - i don't know how to go back
45:55 - did that change yeah you can see it oh
45:57 - well um
46:00 - oh oh no
46:05 - this back button i'm just gonna like
46:09 - reopen it cool um
46:12 - so we've got happy
46:19 - happy equals 0.8 um
46:22 - and then we've got the minus 0.55 plus
46:25 - 0.04 so it's going to be like
46:27 - minus 0.51
46:30 - times stress and sorry i should have
46:34 - added it's 0.8 plus 0.12
46:38 - so this is going to be like 0.92 over
46:41 - here
46:46 - so it's going to be 0.92 minus 0.51
46:49 - times stress
46:50 - and then we can keep doing this for
46:52 - other values of free time so we can even
46:54 - do
46:54 - for free time equals two
46:58 - we're basically going to end up adding
47:00 - another
47:01 - .12 to this um
47:05 - to this intercept because we're going to
47:07 - end up with
47:08 - right one two times two instead of point
47:11 - one two times one in this equation so
47:15 - for free time equals two our equation is
47:17 - going to be happy
47:19 - equals point nine two plus another point
47:21 - one two which is going to be
47:24 - 1.04 and then we're going to
47:26 - also add another 0.04 onto the slope so
47:29 - it's going to be like
47:31 - minus 0.47
47:36 - right
47:40 - times stress and so we can do this for
47:43 - every value of free time
47:45 - we get a new equation every new equation
47:48 - has its own intercept and its own
47:52 - um sorry our its own
47:56 - intercept and its own slope
47:59 - and we can
48:02 - try to visualize it it's a little bit
48:06 - harder to visualize because
48:08 - now we have you know i only did integer
48:10 - values of this but like you could
48:12 - imagine
48:13 - there's a different line for every
48:14 - possible value of free time so you could
48:16 - also have like free time equals
48:17 - 1.5 theoretically you could also have
48:20 - free time equals negative
48:21 - something um but obviously that's
48:25 - not realistic um
48:28 - but here sorry i can't see you so
48:31 - sorry i could have you yeah back um
48:34 - okay so uh so we've got like
48:38 - infinitely many lines in here but we can
48:41 - visualize what some of them would look
48:43 - like i think i
48:45 - grabbed this and i will
48:48 - um i will really quickly talk through
48:51 - what this
48:52 - code is doing
48:58 - oh um i think
49:02 - in my initial
49:05 - i call this model q okay
49:08 - so here's three lines um
49:11 - those are the three lines for happiness
49:14 - or sorry for free time
49:16 - equals i think zero
49:19 - um three and six exactly um and so you
49:23 - can see what i'm doing is i'm modeling
49:25 - so for each line i'm taking the stress
49:28 - values and then i'm well i'm taking
49:31 - the intercept plus
49:34 - the um the coefficient on stress
49:38 - that's all i need for the the first line
49:42 - when um free time equals zero because
49:44 - remember those other two terms kind of
49:46 - go away
49:47 - but then uh for my line when
49:51 - uh free time equals three i'm basically
49:54 - like
49:54 - writing out this whole equation it's
49:56 - like this is my b0
49:59 - this is my b1 times the stress
50:02 - values and then this is my b2
50:07 - times three times um
50:11 - oh sorry this is my b2 times 3 which is
50:14 - the value of free time and then this is
50:15 - my
50:16 - b3 times stress times 3.
50:19 - so i'm basically plugging in a value for
50:21 - of 3 for free time
50:22 - into that equation in order to draw this
50:25 - line
50:26 - um and so that's like the second line
50:28 - and then um
50:30 - the third line for uh
50:33 - for free time equals six is this dark
50:36 - purple line and you see that they all
50:38 - have different intercepts
50:40 - and they all have different slopes
50:44 - cool um okay so that's
50:48 - uh that's it that we're going to cover
50:51 - for
50:52 - um interaction terms for right now
50:56 - um were there any questions that you
50:58 - wanted to cover
50:59 - yes i've been chatting with people in
51:00 - the chat so if you've seen me typing
51:03 - that's what i was doing uh let's see um
51:08 - uh there was one where did it go um
51:13 - what are the aic and bic criteria
51:17 - cool yeah so those we're gonna cover i
51:19 - think two live streams from now
51:21 - and like i said that that's all in this
51:24 - um
51:25 - in this course menu that's in the
51:28 - um choosing a linear regression
51:32 - model lesson you'll find
51:35 - lots of stuff about aic and bic
51:38 - uh here we go here's a exercise on it
51:41 - um so those will those will come up
51:44 - later
51:45 - i also see a question about fixed effect
51:49 - and random effect models um
51:52 - that is uh something so
51:55 - what we're doing right now is fitting
51:59 - fixed effect models um
52:02 - random effects are useful for
52:06 - different kinds of problems like i think
52:09 - the classic one is when
52:11 - you have data where the observations are
52:14 - not independent so like
52:16 - students within a school are not
52:18 - independent and so if like
52:20 - you have school as a predictor in your
52:22 - model um
52:23 - and also like individual students within
52:27 - each school
52:28 - um you want to account for those like
52:31 - that
52:32 - co-linearity or that correlation um
52:36 - i i don't think we currently have any
52:38 - content that covers that
52:39 - but um but yeah like a random effect is
52:43 - basically like
52:44 - a it's like an extra term in your model
52:47 - that can that is basically like random
52:50 - error
52:51 - um that's like us but the random errors
52:55 - are correlated within
52:57 - like say a school or something
53:01 - um lots of questions about office hours
53:03 - do you want to plug what we're doing
53:04 - there
53:04 - oh yeah so office hours we we did our
53:07 - first test run last week
53:09 - um and then we'll we'll have another
53:11 - office hours
53:12 - this week um we're doing them on discord
53:16 - and it's basically just a place where
53:19 - you can come and ask questions
53:21 - and we'll we have voice capabilities so
53:24 - we can like answer them
53:26 - speaking um you can also share our
53:27 - screen and show you stuff if
53:29 - if we need and um but it's also we also
53:32 - have a
53:33 - um like a chat in the
53:36 - on discord uh where you can post
53:39 - questions
53:40 - at any point and if you post your
53:42 - questions ahead of time
53:44 - um i think it's called yeah questions
53:45 - bank curriculum
53:47 - now um but we'll be checking that and
53:51 - keeping an eye on it
53:52 - we're not gonna respond like right away
53:54 - probably but if you have questions
53:56 - we'll look there before office hours so
53:58 - that we can make sure we get to anything
54:00 - that
54:01 - is asked ahead of time um
54:04 - cool um yeah we also have
54:07 - someone was saying in chat if we could
54:08 - do office hours directly after this
54:10 - stream that's something we weren't
54:11 - planning
54:12 - uh we weren't planning for this week but
54:14 - if folks would prefer that if you're
54:16 - here anyways watching the stream and and
54:18 - want like more time to chat and ask
54:20 - questions right away
54:21 - um we could potentially make that happen
54:23 - yeah for sure
54:24 - so that's good feedback um similarly it
54:27 - sounds like there's a cool python
54:29 - package that will let us like render
54:30 - math
54:32 - math equations quickly um so yeah more
54:34 - more feedback if uh
54:36 - like i like what sophie is doing with
54:37 - the ipad i think it's helpful to be able
54:39 - to like draw graphs and stuff but
54:41 - um if there are ideas that you have that
54:43 - uh
54:44 - would make this better um yeah
54:46 - definitely let us know cool
54:48 - all right i've got one more thing that i
54:49 - can show you guys
54:51 - since we have time um
54:55 - so last we
54:59 - another kind of flexible model that we
55:02 - can create
55:04 - is um using polynomial terms
55:07 - so i'll show this graph right here
55:11 - again a little bit made up but this is
55:14 - sleep
55:14 - versus happiness and so we see um i
55:18 - guess
55:18 - the um we'll do lm plots
55:35 - hold on i got this we got
55:38 - sleep happy
55:43 - and then what was the data called
55:47 - happiness
55:55 - all right let's try that okay so
55:58 - it's gonna fit this line
56:02 - um if we just leave it as fit rug equals
56:04 - true
56:05 - but you'll notice actually i'll add the
56:08 - like
56:08 - fit rug equals false
56:14 - re-run right you notice that if you were
56:17 - to try to like
56:18 - draw a line through these points
56:21 - you would say okay like as you get more
56:23 - sleep your happiness
56:25 - increases up to a point if you have more
56:28 - than 10 hours of sleep your happiness is
56:30 - starting to like decrease again
56:32 - and so we almost want to be able to fit
56:35 - like a curve to this
56:36 - instead of a straight line and with
56:39 - regular linear regression we don't have
56:41 - a way to do that
56:42 - so we can do it using polynomial terms
56:46 - and this is again a little bit of like a
56:49 - trick
56:49 - of almost adding another um
56:53 - another term into our model or another
56:56 - like column into our data
56:58 - that's based off of another um
57:03 - another column that already exists and
57:06 - uh and it basically allows us to
57:10 - have a model where we have like a
57:14 - squared term
57:15 - in the model or so like a higher order
57:18 - polynomial
57:19 - so i'm going to demonstrate this really
57:21 - quickly with
57:23 - this data um
57:29 - and we want so we want
57:33 - happy as a function of
57:36 - sleep this time and then
57:40 - um we're gonna add
57:43 - a i think it's like
57:49 - like this basically this is taking sleep
57:52 - to the second power let me just make
57:55 - sure
57:55 - that that's oh yeah that's right okay
57:59 - so we're just saying okay i want to add
58:01 - sleep squared
58:03 - into my model and then fit this
58:06 - and then print this out let me get
58:10 - model p for polynomial and then
58:14 - oh model yeah
58:17 - um and then you'll see that we get a
58:21 - intercept we get a slope on sleep and
58:24 - then we get a slope on
58:25 - sleep squared and i'm not going to
58:28 - switch
58:29 - to the ipad now because we're we're kind
58:31 - of running out of time but i'll i'll
58:32 - actually do this like
58:34 - mathematically i'll write it out in
58:35 - python
58:37 - so um so basically what this is doing
58:42 - is it's creating a new equation where we
58:46 - have sleep squared
58:48 - in our model um so let's like
58:51 - i guess save these as b0 b1 and b2
58:54 - so model.params
58:59 - zero so like the first thing is the
59:02 - intercept that's our b0
59:04 - b1 is the second
59:08 - thing in this model so that's at index
59:10 - one
59:12 - and then b2
59:15 - is at index 2.
59:19 - so i'm just saving i'm just saving these
59:21 - numbers as b0
59:23 - b1 and b2
59:26 - i'm gonna also just save the um the
59:29 - values of sleep
59:30 - as sleep instead of like happiness
59:34 - dot sleep because it's a column in my
59:36 - happiness data set
59:38 - i'll just save it separately as sleep
59:41 - and then i'm gonna say like predicted
59:45 - happiness based on this model
59:48 - is now equal to b zero
59:53 - plus b one times sleep
59:58 - plus b2 times
60:01 - sleep squared um so i can do that as
60:04 - like sleep
60:06 - um star star 2 that will square
60:10 - all the values and sleep i could also do
60:12 - like numpy dot power
60:15 - sleep comma two um but basically right
60:18 - this is like all the values in this
60:19 - column
60:20 - square and if i plot this
60:24 - now and the result there is going to be
60:26 - a column right because sleep is a full
60:28 - column so
60:29 - you're yeah you're doing that for every
60:31 - value in this sleep column you're doing
60:33 - that equation
60:34 - exactly in fact i can also just show
60:37 - like what this looks like
60:41 - i'm so run this so i have them saved and
60:43 - then yeah
60:44 - basically like this is every value
60:47 - in in
60:50 - sleep squared and then multiplied by b2
60:54 - i guess we can just do that
60:56 - um this is every value in the sleep
60:57 - column squared
61:00 - so okay so i'm just creating these
61:03 - predicted happiness scores
61:05 - and then what i can do is i can add it
61:08 - to this picture
61:10 - so we can see what it looks like
61:14 - and i'm going to um
61:18 - see that plot sleep and
61:21 - predicted happiness based off of this
61:23 - model
61:25 - and i'll show it
61:29 - [Music]
61:32 - i guess actually
61:38 - what is a better way to do this um
61:41 - i guess now i'm realizing this is not
61:44 - pretty because
61:45 - uh it's like drawing the line between
61:47 - all the points when we're doing it on a
61:48 - line
61:49 - all of these were like still on top of
61:51 - the line
61:52 - um so let's see we can just do like lin
61:55 - space
61:57 - um or we can do like
62:00 - sleep equals so sleep is roughly between
62:05 - 0 and 14. so let's instead of
62:09 - grabbing this actual column we can do
62:11 - like
62:14 - is it in numpy so you just want like a
62:18 - list of values from
62:20 - zero to 14. yeah
62:24 - um by 5.5
62:29 - 4.1 here let's just
62:32 - test this really fast
62:45 - thank you here actually i can
62:51 - demo this even though we're inside that
62:54 - linspace
62:55 - and then start stop
62:58 - and then
63:04 - oh so then it's just like the third
63:08 - parameter is the number of values so
63:10 - we'll do like 100 values
63:12 - and that should give me yeah okay sorry
63:15 - guys
63:16 - but hopefully this is helping you see
63:18 - like how we do this in real life
63:20 - and so here we're not using the actual
63:22 - sleep data we're making up a bunch of
63:24 - fake points just to draw what this line
63:26 - looks like
63:27 - right and then yeah
63:30 - so that's a little nicer now we see that
63:34 - that polynomial term allowed us to
63:36 - create this like kind of
63:38 - curve um in our line cool
63:41 - all right well quick question was so
63:43 - when we did um
63:44 - sleep times exercise that was the
63:46 - interaction term and if we're doing this
63:48 - sleep squared is that called the
63:49 - polynomial term it is a polynomial term
63:52 - cool yes and we could even add sleep
63:55 - cubed
63:56 - and sleep to the fourth we can add more
63:58 - and more polynomial terms but
64:00 - again like we probably don't want to if
64:03 - we added
64:04 - another if we added like a cube term
64:07 - that would allow us to get like a
64:09 - another curve in the data or in the line
64:12 - basically
64:13 - um but then i think there's
64:16 - there's some really good examples of
64:18 - this if you just like add lots and lots
64:20 - of polynomial terms you could have like
64:21 - a very squiggly line which is a good
64:24 - example um
64:26 - of overfitting in action
64:29 - cool cool um sophie you're going to be
64:32 - gone for office hours
64:34 - this week yeah i might sign on though
64:36 - okay
64:37 - if i can um i might be i might be gone
64:40 - for office hours though
64:41 - but i'll be back next week and i'll keep
64:43 - an eye on this yeah i'll i'll be in the
64:45 - office hours i
64:46 - am not the stats person that sophie is
64:48 - so i don't know if i'll be able to
64:49 - answer your
64:50 - stats questions but i could probably get
64:53 - a niche
64:54 - yeah we'll find someone yeah that knows
64:56 - what they're talking about andrea
64:57 - actually wrote
64:58 - all this this lesson okay maybe i can
65:00 - get hurt
65:01 - cool all right cool well
65:04 - awesome i'm glad that our first uh
65:07 - stream back in the office
65:10 - i love this green screen and the fact
65:12 - that we're just like
65:14 - yeah yeah i don't know if you noticed
65:15 - but i shrunk us uh in the middle of the
65:17 - stream to uh that's good yeah i like it
65:20 - like tiny people awesome
65:23 - well thank you everyone for coming and
65:25 - if you have questions ask us on discord
65:27 - or
65:27 - ask in the comments on the youtube video