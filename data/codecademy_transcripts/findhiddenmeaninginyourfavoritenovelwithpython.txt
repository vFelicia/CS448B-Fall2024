00:02 - hi they're making sure we're alive
00:05 - awesome thank you everyone for joining
00:09 - us today we're here to do another
00:13 - livestream here at code Academy HQ where
00:16 - we'll be doing a natural language
00:19 - parsing analysis on a novel of your
00:23 - choosing we'll give everyone a little
00:27 - bit of time to log in and get settled at
00:30 - your computers my name is Ian freed I'm
00:33 - a curriculum developer here at code kata
00:36 - me working on some of their data science
00:39 - content that we have here and also
00:42 - delving into natural language processing
00:44 - which is why we're here today
00:47 - so if you are coming from a background
00:50 - where you've done some naturally
00:52 - entrusting before that's amazing
00:54 - hopefully you'll get to learn something
00:55 - new in NLP today if you're not coming
00:58 - from that background it's totally
00:59 - alright I hope that today you will get
01:02 - to learn a little bit about what natural
01:05 - language processing is and some of the
01:07 - things that we can do with natural
01:08 - language processing and along the way
01:11 - we'll also get to use a really useful
01:14 - tool known as regular expressions and
01:16 - we'll utilize that in order to do our
01:18 - analysis and thank you Mario for the
01:24 - happy birthday wishes I appreciate it
01:27 - and I will have some colleagues in the
01:29 - chats also answer some questions that
01:31 - anyone has along the way feel free to to
01:34 - be active in the chat I want this to be
01:35 - a conversation I don't want to just be
01:38 - talking here for an hour but have it be
01:41 - a conversation a two-way street where we
01:44 - can work together to hopefully perform a
01:46 - really great analysis together so
01:50 - natural language processing is a area of
01:55 - data science where we can look at text
01:57 - data and do really cool analyses on it
02:01 - and so today what our plan is going to
02:03 - be is to look at a whole novel and we
02:07 - have a few different novels that we can
02:08 - consider working with and maybe at the
02:10 - end you'll have the chance to choose any
02:11 - novel of your own
02:12 - choosing and do an analysis on the word
02:16 - choices that the author has made and
02:19 - trying to find some meaning that we can
02:21 - quickly discern from the novel without
02:23 - having to read the whole thing so using
02:25 - our programming skills and specifically
02:27 - Python in order to find out some of the
02:29 - insights that might be hidden in a piece
02:33 - of text and just to give you kind of an
02:35 - idea of some of the really cool things
02:37 - that we can do with naturally into
02:41 - processing and specifically parsing
02:43 - natural language I just want to
02:45 - highlight some really cool examples that
02:47 - are out there that people have done um
02:49 - one was an analysis on the Harry Potter
02:52 - novels and someone went and did this
02:57 - kind of parsing analysis on the Harry
03:02 - Potter novels and looked at how the
03:04 - author and the characters in Harry
03:07 - Potter used different words so what
03:10 - nouns or what what nouns are common so
03:13 - names potentially or what additives are
03:15 - used to describe those nouns in the
03:16 - novel or how action occurs to different
03:21 - characters in the novel so you can see
03:22 - here in this analysis they were looking
03:24 - for certain combinations of verbs and
03:27 - nouns or nouns and adverbs or
03:29 - potentially adjectives as well and
03:31 - seeing what patterns arose and how these
03:33 - different kinds of parts of speech are
03:35 - put together and this person who did
03:40 - this analysis was able to find a bit of
03:42 - a gender bias in how different
03:44 - characters have action happened to them
03:46 - or action is described by characters so
03:49 - that was a really cool thing that maybe
03:52 - just by reading the novels you wouldn't
03:53 - get to understand but this person was
03:55 - able to discover using parsing with
03:58 - natural language processing another
04:00 - really cool analysis that was done by
04:03 - someone was posted here on Twitter and
04:06 - they were looking at declassified CIA
04:08 - documents and they were able to use this
04:11 - kind of analysis to find a code word
04:15 - that hadn't been with analyzed before I
04:18 - discovered that this code word has some
04:20 - importance and so they found this
04:23 - code word to be really relevant just
04:26 - because it came up so often in the
04:28 - documents and these were hundreds and
04:30 - hundreds of declassified CIA documents
04:31 - that you couldn't really read through by
04:33 - hand but by using this kind of analysis
04:34 - they were able to find this this code
04:40 - word and so yeah today I'm meant to
04:44 - mention this we'd love for you to follow
04:46 - along with us by clicking on the link on
04:50 - the bottom of in the YouTube description
04:52 - which will take you to code Academy
04:55 - where you'll have a workspace set up
04:56 - where you can complete this kind of
04:58 - analysis with us so hopefully you can
05:02 - follow along step-by-step and get the
05:05 - same results as weekend so for today's
05:09 - analysis we are going to work with one
05:12 - piece of text and I have two texts that
05:15 - I've been thinking about using one is
05:19 - the wonderful what sort of oz which many
05:21 - of us are familiar with has characters
05:24 - that are you know from our childhoods
05:26 - and we've all probably seen the movie at
05:28 - one point so we can do a fun analysis
05:30 - with that or I have another novel anthem
05:33 - by ein Rand that I've also gotten the
05:37 - text for so I'm trying to decide between
05:40 - what which texts to use today so if
05:43 - someone wants to maybe shout something
05:45 - out in the chat about what you prefer we
05:47 - can choose one of those two texts and
05:50 - move forward with our analysis I like
05:55 - both books you know Wizard of Oz it's a
05:57 - classic that I think we all have fun
06:01 - memories of but anthem is also another
06:04 - interesting novel that I think bring out
06:10 - some interesting insights by doing this
06:11 - kind of parsing analysis on the text
06:14 - anthem is kind of about a dystopian
06:16 - future and a character who's coming to
06:19 - this sense of individuality in his
06:22 - society yeah I'm glad I some of the
06:27 - other Rafal think that this is awesome
06:30 - natural language processing is a really
06:31 - really powerful tool
06:34 - and it's really crazy what can be done
06:37 - with utilizing this tool I'm kind of
06:39 - insights that we can find especially
06:42 - going through these sometimes lengthy
06:44 - pieces of text and finding insights that
06:46 - would maybe take longer if we were to
06:48 - fully read them or things that you might
06:50 - not even pick up in a casual reading
06:55 - okay so what we'll do is we'll probably
06:58 - go ahead with The Wonderful Wizard of Oz
07:02 - I think it's just a classic and I think
07:05 - hopefully can be relatable to a lot of
07:07 - people and knowing the characters so
07:10 - we're gonna go ahead and get started and
07:12 - what I'm gonna do is here on code kata
07:14 - me I'm just gonna minimize my left hand
07:17 - side so that we can get a better view of
07:20 - the code editor and I'm gonna expand my
07:24 - output terminal a little bit and let's
07:27 - go ahead and start our analysis so we
07:30 - can see I have two files open one's just
07:33 - a Python script I'll be writing my code
07:35 - today I've imported a few different
07:38 - libraries and functions for those
07:41 - libraries that will be useful in our
07:43 - analysis and I'll explain those as we go
07:44 - through and then I also have a text file
07:47 - that contains the wonderful wizard of
07:49 - odds and this file has the whole novel
07:52 - in here and I was able to get this novel
07:55 - from Project Gutenberg which is a really
07:57 - great resource if you like to read or
07:59 - you're really interested in digging
08:00 - international language processing and
08:02 - Project Gutenberg is this project that
08:06 - contains lots of open source of public
08:08 - domain pieces of literature that you are
08:11 - able to use you know that are open and
08:15 - can be reproduced so this website as you
08:18 - can see right here has a huge variety of
08:21 - these public domain novels and you can
08:23 - feel free to go find any novel from here
08:26 - and do an analysis on it later which you
08:28 - will have an opportunity to but I've got
08:31 - the text for The Wonderful Wizard of Oz
08:34 - from here and I just pasted that text
08:36 - file into this text file that I created
08:39 - over here in
08:42 - the code editor so now that we have this
08:46 - text file let's go ahead and open it in
08:49 - Python so I'm going to create a variable
08:51 - named text and to open the text file I'm
08:54 - just going to use the open function and
08:57 - I'm going to pass it as an argument the
08:59 - name of the text file which is the
09:02 - Wizard of Oz text dot txt and I could
09:11 - just try and then add a dot read here at
09:14 - the end to read that file in and if I
09:17 - try and run this right now I'm going to
09:20 - run into an issue and say ok no such
09:24 - file directory found because I repeated
09:26 - off so that was not the error I was
09:28 - expecting but another error nonetheless
09:31 - but so I'm going to delete that extra of
09:33 - The Wizard of Oz here we go miss the Z
09:39 - and I'll run this one more time and The
09:43 - Wizard of Oz no text file names are
09:48 - important guys you will see that if you
09:50 - don't get the exact filename right it's
09:52 - not going to load so this time I want to
09:54 - check it word for word The Wizard of Oz
09:55 - dot txt matching my text file there at
10:00 - the top no I will run this again and
10:03 - we're going to see you okay well so I'm
10:08 - not printing out anything so we won't
10:09 - see any response but if I go and try and
10:11 - print text right now we're still gonna
10:13 - not have our full text arrow we do great
10:19 - so we can see we have on the right hand
10:21 - side our full text that we opened if
10:26 - you're working with certain text files
10:27 - sometimes there's an encoding that might
10:29 - be included on that file so another
10:32 - argument you can add is encoding to our
10:35 - open function and just add utf-8 utf-16
10:45 - m8 encoded but it doesn't appear to be
10:47 - so so you can include that just to be
10:49 - careful when you're loading in that text
10:52 - file and one more thing I'm going to do
10:55 - in addition
10:56 - reading the file in is to make every
10:58 - single word in the text lowercase and
11:02 - the reason for doing this is when we're
11:05 - trying to see how frequent maybe certain
11:08 - words occur in the text we want maybe
11:12 - words that are capitalized at the
11:14 - beginning of the sentence to be
11:16 - considered the same as a word that maybe
11:18 - appears in the middle of a sentence so
11:21 - just because it has a capital letter we
11:22 - don't want there to be a difference so
11:25 - I'm going to add lower to the end of
11:28 - this importing of the text and now if I
11:32 - rerun my file will see that now every
11:36 - single word is now in lowercase
11:37 - depending on your analysis you might
11:40 - care that certain words are uppercase or
11:42 - lowercase so you can choose to you know
11:46 - make that decision if you feel the need
11:48 - to do so so I see oh we have a question
11:51 - here I want to carry out analysis on
11:52 - Fight Club not sure where to stand with
11:55 - the copyright so when going into natural
11:59 - image processing it's good to keep in
12:01 - mind you know the copyright of the text
12:04 - that you are working with or who has
12:07 - ownership of the piece whether it's an
12:09 - article from a newspaper or it's even
12:12 - let's say you're grabbing a bunch of
12:13 - tweets and doing your analysis a bunch
12:15 - of tweets so it depends what you plan to
12:17 - do with your analysis if you're
12:19 - distributing it out to the public and or
12:23 - if you're doing it personally so you do
12:25 - want to be careful in that realm you
12:28 - should look at the copyright that is
12:30 - given on the piece of text that you're
12:32 - working with the beauty of Project
12:34 - Gutenberg tax is that they are in this
12:36 - public domain I'm allowing us to to kind
12:39 - of use it without a lot of worry so I
12:42 - would say to just you know proceed with
12:44 - caution if you're going to be sharing
12:45 - any any of your work just so you're not
12:49 - violating any sort of copyright law
12:52 - great question okay so we've gone ahead
12:58 - and imported our text but to perform our
13:05 - analysis we need to break down our text
13:08 - from this giant
13:09 - novel that we have into a smaller pieces
13:13 - that we are able to analyze and so one
13:15 - smaller piece that we can break down our
13:17 - checks into is from the whole novel to
13:20 - individual sentences and when we break
13:23 - Tom check to individual sentences we can
13:24 - then perform a sentence by sentence
13:26 - analysis and and this makes things a
13:29 - little easier to digest in this process
13:33 - of breaking down checks into smaller
13:34 - components as a process notice
13:36 - tokenization so let's go ahead and
13:38 - perform some tokenization on our text
13:41 - the first step being substance
13:42 - organization so to do this we are going
13:45 - to use the natural language toolkit
13:47 - which is a really great tool and package
13:52 - and Python so natural language tool kit
13:54 - or NLT kay it has a lot of great NLP
13:59 - functionality that we can use and one of
14:02 - these is the punked sentence tokenizer
14:05 - and this is a tool that will go ahead
14:07 - and look through our entire text and
14:09 - split it up into individual sentences so
14:13 - I'm going to create a sentence tokenizer
14:16 - right here and I'm gonna set equal to
14:19 - punked sentence tokenizer
14:22 - and if you look at the top of the file
14:24 - we have seen that imported at the top
14:27 - and I'm going to give as an argument to
14:32 - that Punk census tokenizer my entire
14:34 - text and what this is doing is it's
14:38 - almost training this sentence organizer
14:41 - on the piece of text that we are giving
14:43 - to it and it's using some machine
14:44 - learning in the background to do this
14:47 - training and now that I have this
14:50 - sentence tokenizer I can go ahead and
14:53 - split up my text into individual
14:55 - sentences so I'm going to make a new
14:58 - variable called sentence tokenized and
15:02 - set it equal to my sentence tokenizer
15:08 - sentence tokenizer and I'm going to call
15:11 - the sentence tokenizer is tokenize
15:14 - method and once again give as an
15:18 - argument our text data so what this is
15:21 - saying is we're going to use that
15:23 - Pung sentence tokenizer that we created
15:25 - and tokenize that entire novel The
15:29 - Wonderful Wizard of Oz I'm going to go
15:32 - ahead and now print sentence tokenized
15:34 - so we can see what we are working with
15:37 - and rerun our code and so it's a little
15:47 - hard to see here but what I'm going to
15:49 - do is I'm just going to look at one
15:53 - index of this variable that we created
15:55 - and so sentence organized is now a
15:57 - Python list and so Python lists contain
16:01 - indexes which have their own individual
16:03 - pieces of information so if I let's say
16:05 - look at the tenth index of sentence
16:08 - tokenize and I print that now we can see
16:13 - at the bottom and I'm actually going to
16:15 - comment out my printing of the whole
16:17 - text to make this easier we can see that
16:20 - we're now just getting one sentence from
16:22 - the novel and if I change the index from
16:27 - ten to fifteen and I run the file again
16:31 - we're gonna see that we're now printing
16:32 - out in the console another sentence of
16:34 - the novel so we've taken that entire
16:36 - piece of text data and then split it up
16:38 - into individual sentences and if we
16:41 - wanted to see how many sentences are in
16:43 - the novel we can just go ahead and take
16:45 - the length of this list that we now have
16:51 - and we'll print in the lane and we'll
16:54 - see we have two thousand two hundred
16:56 - twenty sentences so this is now the the
17:01 - data that we're working with and it's
17:03 - easier for us to handle rather than
17:05 - having one giant piece of text okay now
17:10 - that we've broken down our text into
17:12 - individual sentences if we want to look
17:15 - at the choices that our author is making
17:18 - in terms of maybe the parts of speech
17:19 - within a sentence we need to break down
17:22 - our sentences into individual words and
17:25 - so this is the process known as we're
17:27 - tokenization so let's go ahead and do
17:32 - word tokenization on each sentence from
17:36 - our text so to access each sentence
17:39 - we're gonna do a loop through such and
17:42 - silk'n eyes so I'll say for sentence in
17:45 - sentence tokenized and right here we're
17:52 - going to use another function from an LC
17:56 - k which is where tokenized which you can
17:58 - also see imported at the top of our code
18:01 - and I'm going to say word tokenize and
18:06 - give we're tokenized as an argument each
18:09 - sentence and since we're going through
18:15 - each sentence I want to create a new
18:17 - list that's gonna store all these
18:20 - sentences that are now going to be split
18:22 - up into individual words so I'm going to
18:24 - create a new list called word tokenized
18:28 - before my for loop and I was going to
18:31 - set it equal to an empty list and then
18:34 - when I'm going and we're tokenizing each
18:36 - sentence I'm now going to append that
18:38 - result to my empty list we're tokenized
18:50 - so let's just review this what we're
18:55 - doing this loop one more time we are
18:57 - going ahead and saying let's look at
18:59 - each sentence in our novel and for each
19:02 - sentence let's break it down into
19:03 - individual words and we're gonna store
19:05 - those words in a list and so let's go
19:09 - ahead and print out a 1 1 piece of that
19:16 - word tokenized list and i'm going to
19:22 - save and run my code and will now see if
19:27 - we look at the output terminal we have
19:29 - the original sentence even the grass was
19:32 - not green for the sun had burned the
19:34 - tops of the long blades and then we see
19:37 - beneath that we have a list where each
19:41 - index in the list is an individual word
19:43 - from that sentence so we now split up
19:46 - that sentence level into an individual
19:48 - word level
19:50 - so we want to just take a second to
19:52 - pause make sure everyone's able to
19:55 - follow along and has hopefully got ahead
19:58 - and said to tokenized and we're tokenize
20:00 - on the novel check-in for any questions
20:03 - let's see
20:07 - glad you guys find NLP exciting I do too
20:11 - it's a really cool area of data science
20:14 - of a natural language study and I hope
20:22 - you got to learn something here today
20:24 - okay
20:26 - so if no other questions we'll go ahead
20:29 - and continue and just as a check I'm
20:32 - gonna go ahead and print the length of
20:35 - our word tokenized lists as well and
20:43 - you'll see that the length of this list
20:45 - is also two thousand twenty two thousand
20:48 - two hundred twenty which is good we
20:50 - still have two thousand two hundred
20:51 - twenty sentences in our novel this time
20:54 - though instead of each index just being
20:56 - the regular sentence as a string we have
20:58 - a list where each index in that list is
21:01 - an individual word in in the sentence
21:05 - and I said we have a question is a
21:06 - stream going to be staged in at all for
21:08 - later reference yes you'll be able to
21:10 - find the stream on YouTube to rewatch at
21:14 - any time so if for some reason you're
21:16 - not able to follow along don't worry
21:18 - you'll be able to go back re-watch and
21:22 - hopefully completely analysis yourself
21:31 - now that we've gone ahead and split up
21:34 - our text into individual sentences and
21:38 - individual words we need to think about
21:40 - how we're gonna go ahead and look at the
21:44 - different parts of speech of her
21:45 - sentence since you know that's a really
21:47 - helpful technique for going ahead and
21:50 - looking at the choices that an author
21:53 - has made and so let's do a little quick
21:56 - recap of what are parts of speech I
21:59 - might have been a while since you had
22:00 - been in English class but and this can
22:05 - vary between different languages but
22:06 - there's different parts of speech and in
22:08 - English there are eight or nine
22:10 - depending on us
22:11 - main parts of speech and these parts of
22:15 - speech explaining how different words
22:18 - function within the structure of a
22:20 - sentence so one part of speech are nouns
22:23 - nouns are usually the subject of a
22:27 - sentence the name of some of a person
22:31 - potentially a place or a thing or an
22:33 - idea that we are discussing and then we
22:37 - have pronouns which are words that go
22:39 - set in place of a noun we have
22:42 - determiners which determine a noun so
22:46 - kind of this taking a break here you
22:50 - know for example this is my cat and that
22:52 - sentence cat is the noun because we are
22:54 - talking about the cat in terms of
22:56 - pronouns Tyra is a student she is
22:59 - studying computer science she is
23:01 - standing in place of the noun tarot and
23:04 - then determiners are introducing nouns
23:07 - so I have two alligators it's saying
23:09 - that we have two of the noun in the
23:11 - sentence where we have some bunnies
23:15 - another common or another part of speech
23:18 - or verbs verbs describes what we do you
23:21 - remember that Nickelodeon I think it was
23:23 - like verbs it's what you do kind of
23:25 - jingle so always good to review these
23:28 - I'm sure many of you are familiar with
23:29 - them but it's you know easy to forget
23:32 - sometimes adjectives will modify or
23:37 - describe a noun so you're describing
23:41 - let's say your pirate your power
23:42 - it is colorful or your part is big
23:44 - colorful and big are words that are
23:47 - describing that noun then we have
23:50 - adverbs which are modifying how
23:53 - modifying a verb so are you running
23:57 - quickly are you eating quickly quickly
23:58 - is modifying that verb and then just a
24:04 - few more prepositions which are placed
24:07 - before noun so I went to work I went to
24:09 - the store
24:10 - - is that preposition conjunctions we'll
24:13 - combine things together and or but so I
24:18 - like Tigers and I like lions and then
24:21 - interjections like ouch Wow
24:23 - exclamations things that are inserted
24:27 - into sentences and often are evocative
24:30 - so by looking through each of our
24:34 - sentences in our novel what we can do is
24:37 - we can assign the part of speech to the
24:39 - words in those sentences and then go
24:42 - ahead and try and find patterns in how
24:44 - authors or writers are using these
24:47 - different parts of speech so how are
24:49 - they choosing their nouns or how are
24:52 - they using to describe the nouns with
24:54 - adjectives or how are they choosing to
24:57 - describe the action or the verb that
24:59 - maybe happens to their subjects which
25:02 - are nouns and this can bring some
25:04 - interesting insights so we could go
25:08 - ahead and manually look at every single
25:12 - sentence of ours and assign the part of
25:15 - speech but that would take a long time
25:20 - but thankfully with an ltk and natural
25:25 - language processing we can go ahead and
25:27 - automate that process and part of speech
25:29 - tag sentences automatically and that's
25:33 - what we're gonna do here today and we're
25:37 - going to use an NL CK function
25:38 - POS tag in order to do this part of
25:42 - speech tagging so what we want to do is
25:45 - we want to do this for every single
25:47 - sentence in our text so we're gonna do
25:50 - another for loop so I'll say for
25:51 - sentence in word
25:56 - tokenized and maybe let me stop for
26:00 - looking at questions before I jump into
26:02 - this any questions
26:03 - someone asked what is the advantage of
26:05 - the tokenizer function over using the
26:08 - dot split function so tokenize will go
26:15 - ahead and kind of use a little bit of a
26:18 - more advanced analysis than then dot
26:19 - split so dot split you can provide as an
26:21 - argument what you want to split on so
26:23 - maybe you want to split on spaces but
26:27 - the the sentence organizer from and the
26:31 - word tokenize are from NLT k just have a
26:34 - little more behind the hood to make sure
26:36 - that we're splitting as best as possible
26:38 - that's a great question and as I'll I
26:43 - was we're saying we can capture
26:45 - potentially punk punctuation or or
26:48 - whitespace better with I don't see J's
26:50 - tools okay so now we're going to loop
26:56 - through each sentence and our word
26:58 - tokenize text and we're going to go
27:01 - ahead and part of speech tag that
27:03 - sentence using the POS tag function and
27:07 - as an argument we'll pass each sentence
27:10 - and once again we want to store all of
27:14 - these part of speech tag sentences that
27:16 - we are creating so we're gonna create a
27:18 - list right before hand and I was gonna
27:20 - call it POS tag text and we'll
27:26 - initialize that as an empty list and
27:30 - also it's just because my code is
27:33 - probably running off the screen I'm
27:34 - gonna move it up a little bit and I'm
27:40 - going to append this part of speech tag
27:43 - sentence to my POS tagged text list
27:53 - just like so and once again I'm just
28:00 - going to print one value from the part
28:04 - of speech tag sentence that I have
28:07 - created so we can take a look at it so
28:10 - I'm going to say a POS tagged text and
28:18 - I'm gonna say at let's say index 10 so
28:24 - I'm going to run and oh no I'm getting
28:29 - an error so let's go and investigate and
28:31 - it's saying that NLT kay is trying to do
28:35 - this part of speech tagging but we are
28:38 - missing essentially a resource which is
28:41 - a data set that ltk uses in or it's a
28:44 - part of speech tag the sentence
28:46 - um and so if you are kind of doing this
28:50 - analysis on your own computer at another
28:52 - time there are different packages or a
28:54 - different data sets is part about lck
28:57 - that aren't always downloaded so you
28:58 - might need to download some of this
28:59 - extra data so we'll go ahead and
29:01 - download this data by running the
29:04 - command that's listed here in the ER
29:05 - says NLT kay download average perceptron
29:08 - tagger and this is going to give
29:10 - basically a data set of part of speech
29:13 - 10th words for adult ek to use as a
29:15 - reference so I'm gonna go ahead and
29:17 - above where I use POS tag I'm going to
29:20 - run that one line and I'll take a
29:22 - download average perception tracker
29:25 - I'll click Save to run my code and there
29:32 - we go we can see this confirmation
29:34 - saying that we've downloaded the
29:39 - resource and then we can see here we
29:43 - have this part of speech tag sentence
29:44 - and what I'm gonna do just to make it
29:46 - easier to see what's going on I'm just
29:47 - gonna comment out some of my previous
29:49 - print statements just that we can focus
29:52 - on the part of speech tag sentence and
29:54 - I'm also gonna comment out our download
29:57 - we all need to do the download one time
29:59 - once you download the data there's no
30:01 - need to download it again so well
30:03 - comment
30:04 - and let's run our code again and here we
30:14 - can see this sentence where the split up
30:18 - into a list and it's a list of tuples
30:20 - which are defined by the pair of
30:23 - parentheses and the first index in the
30:26 - parenthesis is a word and the sentence
30:28 - is the part of speech tag so we can see
30:33 - we have was which is tagged vbd
30:36 - and you're thinking okay you were
30:38 - talking about nouns verbs adjectives
30:40 - pronouns what
30:43 - what is vbd what does this mean it's a
30:45 - great question if you're thinking that
30:46 - and basically there are these
30:49 - abbreviations for the different parts of
30:51 - speech and they get even more specific
30:54 - than the 8 or 9 main parts of speech in
30:57 - English and we'll look at a list of what
31:00 - some of these abbreviations are here and
31:04 - we can also I can post this into the
31:08 - chat as well for you guys to see on your
31:11 - computer but basically this is a list
31:13 - that shows what the abbreviations are
31:19 - for different parts of speech and the
31:21 - part of speech name so we saw vbd before
31:24 - back in our code so that was for was so
31:30 - if we look up vbd
31:31 - we can see vbg is a past tense verb so
31:36 - this gets more specific than just a
31:37 - general verb it's going into the tense
31:39 - of the verb or the different forms
31:43 - potentially for adverbs or for
31:46 - adjectives so for adjectives we have
31:47 - regular adjectives which was indicated
31:49 - by JJ but we also have comparative
31:52 - adjectives or superlative adjectives so
31:55 - it gets pretty specific which can be
31:58 - really useful for finding specific
32:00 - sequences of parts of speech in our text
32:03 - and this is just a great reference to
32:05 - have when you're doing this kind of
32:06 - analysis if you're ever confused about
32:08 - what part of speech is being tagged you
32:10 - can come here and get an explanation and
32:13 - if you're still unsure about what the
32:15 - the part of speech means may be
32:16 - something you're not from
32:17 - with their you know definitely some
32:19 - things on here that I'm not familiar
32:21 - with I'm no expert linguist you know
32:26 - don't be don't be afraid to go google it
32:28 - dig into the word a little bit and get a
32:30 - better understanding of what that part
32:32 - of speech is it's all part of the the
32:34 - process of learning of digging into NLP
32:37 - which is such a huge area of research so
32:43 - so don't don't ever feel like
32:44 - intimidated or I think you don't know
32:46 - it's okay you can always you know fill
32:50 - in your knowledge by doing a little bit
32:52 - of research so we can see here that this
32:57 - sentence has now been tagged with the
33:00 - different parts of speech so let's just
33:01 - zoom in a little bit and we can see we
33:04 - have an adjective small so small JJ it's
33:08 - describing a noun a small hole hole is
33:12 - the noun and we also have determiner DT
33:17 - for the word dumb and let's see some
33:22 - other things that we can point out crush
33:27 - crush is a verb so this is a really
33:33 - powerful tool that we have available to
33:35 - us and so we've gone ahead and now part
33:38 - of speech tagged our whole piece our
33:41 - whole text what is the next step how do
33:43 - we go ahead and tell our program hey I
33:47 - have these different parts of speech I
33:49 - want to look for now certain patterns a
33:52 - part of speech that might give me some
33:54 - sort of insight into what my text is
33:56 - about or how my characters you're acting
33:59 - or how my author is describing things
34:01 - and this is where we get to use the
34:04 - power of regular expressions regular
34:09 - expressions are a hugely useful tool in
34:13 - computer science and are used to
34:17 - essentially describe patterns of
34:19 - characters that need to be found in a
34:21 - piece of text so if you've ever you know
34:25 - control after in your computer to look
34:27 - for
34:28 - word in the background there could be
34:30 - some regular expressions running to to
34:32 - find that search term on your on your
34:36 - page they're often used for validation
34:41 - of information so if you're ever typing
34:43 - in something on a form there could be
34:45 - some sort of regular expression running
34:47 - in the background that's validating what
34:51 - you're entering is this a valid phone
34:52 - number is this a valid email address by
34:58 - looking for essentially the @ symbol so
35:02 - it's a really really useful tool and if
35:05 - you're not familiar with regular
35:06 - expressions that's okay we're just going
35:08 - to touch on a few concepts for them here
35:10 - today and if you want to learn more
35:12 - there you can feel free to come on code
35:14 - Academy we have a great course on
35:15 - regular expressions that you can check
35:17 - out and against them further
35:19 - but what we're gonna do here today is
35:21 - we're going to use regular expressions
35:23 - to not match the specific characters but
35:27 - to match specific sequences of parts of
35:29 - speech and this is a really really
35:32 - useful thing that we can do to find
35:36 - these patterns in our text and the way
35:40 - that we can do that is through through
35:44 - parsing our text through a means known
35:47 - as chunking and chunking basically says
35:50 - looking at these part of speech tag
35:52 - sentences that we have I want to find a
35:54 - certain sequence of words and I want to
35:58 - pull out that sequence and we do this by
36:01 - defining what's called a piece of chunk
36:04 - grammar so I'm going to find that right
36:07 - here I'm gonna say chunk grammar and
36:12 - what my chunk grammar is is essentially
36:15 - a explanation of this pattern of parts
36:20 - of speech that we want to
36:23 - that we want to find and so the first
36:28 - part of the song grammar is the name of
36:30 - the chunk that you're looking for and
36:32 - this is something that you can decide
36:33 - yourself so I'm gonna start off by just
36:35 - naming it chunk and you put a colon and
36:38 - then what you do is you put a pair of
36:40 - braces and inside the braces you put the
36:44 - pattern of parts of speech that we want
36:47 - to match so to do this you put in the
36:55 - parts of speech with brackets so I will
36:59 - do an open and close and I know you
37:03 - inside inside these brackets you put the
37:07 - name of the part of speech that you're
37:09 - looking to find so I wanted to just find
37:11 - nouns inside my braces I then have the
37:15 - this NN and that means I want to just
37:18 - chunk mounts and and this would just
37:23 - find nouns for us in our text once we
37:27 - use the chunk grammar but we don't want
37:29 - to just look for nouns the the idea here
37:31 - is to look for a pattern a different
37:33 - parts of speech that will give us
37:34 - insight and one part of speech that we
37:37 - can chunk together is what's known as a
37:39 - noun phrase and this is essentially any
37:42 - phrase that contains a noun and other
37:44 - parts of speech that relate to that now
37:47 - and I'm going to write out here the
37:49 - chunk grammar for a popular form of noun
37:51 - phrase that we're gonna search through
37:52 - search for in our text and then we'll go
37:56 - ahead piece by piece and try to
37:58 - understand what is going on in this
38:00 - piece of chalk grammar
38:10 - okay I'm gonna zoom in here so we can
38:14 - see and before I dive into this I want
38:18 - to just look at some questions that
38:20 - people have someone's having this is
38:24 - what saying on my computer attribute our
38:25 - list object has no attribute opened so I
38:30 - would see that open dispelled with
38:32 - double P so maybe I look at your
38:33 - spelling there to see exactly what what
38:37 - are you're running into and yeah okay
38:42 - and yes I know it's hard sometimes to
38:47 - find room to learn and and your busy day
38:49 - whether you're in school you're in work
38:52 - or there's other things going on in life
38:54 - my advice to you in that case is just
38:56 - try and set aside maybe 10 minutes every
39:00 - day 30 minutes every day depending on
39:03 - what's available to you just some time
39:04 - to code jump on your computer log in to
39:07 - code kata me or just you know open up a
39:11 - code editor and get a little practice in
39:13 - that day consistent practice I think is
39:16 - the is the most useful thing when I'm
39:18 - trying to learn something new so you're
39:20 - not able to devote a huge chunk of time
39:22 - every day if you are able to do it a
39:25 - little bit of time and keep it as
39:27 - consistent as possible I think you'll
39:29 - you'll find a lot of success in learning
39:31 - whatever it is you're trying to learn
39:37 - okay
39:40 - and someone else does any question about
39:42 - the the download of the average
39:44 - perception tracker you should just be
39:45 - able to type in that exact piece of code
39:47 - and run it and it should download that
39:50 - data set for you and then you can come
39:52 - to it out after that okay looking now at
39:57 - our chunk grammar I've put in three
39:59 - different parts of speech here with a
40:01 - few different symbols from regular
40:03 - expressions so let's tackle it from left
40:06 - to right so once again we have the name
40:07 - of our chunk which I'm naming chunk and
40:09 - I'm actually gonna update this now to NP
40:11 - since I said we're gonna be looking for
40:13 - noun phrases noun phrases being these
40:16 - phrases that contain a noun and a few
40:18 - other parts of speech and the powder
40:21 - we're looking for is going to start with
40:23 - a determiner
40:24 - et and once again these are words like
40:27 - the and I'm putting a question mark next
40:31 - to the determiner and this means I'm
40:33 - looking for either 0 or 1 determiners
40:36 - I'm making it optional so this noun
40:39 - phrase might have a determiner and might
40:41 - not I don't care I'm willing to take
40:44 - either either form moving over to the
40:48 - right we then have this JJ JJ if you
40:53 - recall stands for adjective some saying
40:56 - after that determiner that we may or may
40:58 - not have I want to find an adjective
41:01 - right afterwards and I have a star after
41:05 - that adjective if you're familiar with
41:08 - regular expressions you'll know that the
41:09 - star is the clean star and this
41:11 - basically means that we want to match
41:12 - this preceding adjective 0 or more times
41:17 - so I can have one adjective in my noun
41:21 - phrase I can have 0 adjectives I can
41:23 - have 7 adjectives so if I have a whole
41:25 - string of adjectives describing my noun
41:27 - that's great I'll take them all
41:29 - if there's none it's ok then at the end
41:33 - of my noun phrase the one thing I need
41:35 - to have the one thing I want to have is
41:37 - a noun
41:39 - just one noun but it's required
41:42 - I need this noun to be in my noun phrase
41:44 - otherwise I don't have it I don't have a
41:46 - noun phrase so so this is the regular
41:49 - expression pattern that we're looking
41:50 - for an optional determiner followed by
41:53 - an adjective or no adjectives or five
41:58 - adjectives any number and then ending
42:00 - with a noun and this is the chunk
42:02 - grammar that I can use to go ahead and
42:05 - look through my sentences and pull out
42:08 - the noun phrases that I'm looking for so
42:12 - now that we've gone ahead and defined
42:14 - this chunk grammar we can go ahead and
42:17 - and get it ready for use on our
42:20 - sentences so what we're gonna do is we
42:23 - need to create a chunk parser to go
42:24 - ahead and do this for us and this is
42:27 - basically something that will go ahead
42:29 - and
42:31 - download that or sorry not download that
42:35 - we'll go ahead and look through a
42:38 - sentences and pull out the noun phrases
42:40 - so I'm going to say chunk parser is
42:44 - equal to and we're going to use another
42:47 - on lck tool if we you scroll up to the
42:50 - top called regex parser and what I want
42:57 - to do is I want to create a reg X parser
42:59 - object and give it an argument my chunk
43:02 - grammar so this is saying that our what
43:05 - we're going to use to look through each
43:06 - of our sentences I wanted to chunk out
43:09 - these noun phrases using the chunk
43:11 - grammar that we have defined right of
43:13 - both and then what I'm going to do is
43:21 - use this chunk parser to parse a piece
43:24 - of text so we will say chunk parser dot
43:29 - parse and we're using the chunk parsers
43:32 - parse method and what we will do is we
43:34 - will pass this chunk parser a sub chance
43:38 - from our mobile so I'm going to go ahead
43:41 - and use this same sentence that we use
43:43 - up here just so we can take a look at it
43:45 - and I'm gonna save this also a variable
43:49 - I'm gonna say chunked sentence it will
43:53 - chunk so strong sentence will be able to
43:55 - chunk parser dot parse this one sentence
43:58 - from our text I'm gonna go ahead and
43:59 - print this sentence chunk sentence and
44:05 - I'm also gonna comment out
44:08 - I'll leave I'll leave our previous
44:10 - sentence so we can take a look at it and
44:12 - I'm gonna click Save and let's see I'm
44:19 - getting a ern so let's see to do invalid
44:25 - syntax so I'm gonna go review what I did
44:30 - just see if I can find the error so I've
44:34 - defined my chunk grammar here and then I
44:38 - go ahead and I create my chunk parser
44:41 - object my records parser object with the
44:44 - grammar oh okay so I did not have a eco
44:49 - stumble right here so I'm saying that my
44:52 - chunk sentence is equal to chunk parser
44:54 - dot parse posx text at the next time so
44:59 - we'll go ahead and rerun our code and
45:05 - I'll scroll to the top so we have our
45:08 - original sentence here that is split up
45:12 - into the different parts of speech and
45:14 - then we scroll down we see our sentence
45:18 - now written in this new format and so
45:21 - the sentence reads there was and then we
45:24 - see this break NP no Garret at all and
45:29 - NP no seller so NP is indicating every
45:33 - place in our sentence where we are
45:35 - finding a noun phrase so it's gone ahead
45:39 - and looked at this sentence and said
45:41 - okay where are my patterns of determiner
45:44 - option determiner followed by adjective
45:46 - followed by noun and it pulls those out
45:49 - into individual chunks which are listed
45:51 - with his NP so you can see multiple end
45:54 - piece here so we have any building we
45:57 - have path and we have case it's another
46:00 - noun and the family is another noun
46:03 - phrase so we're finding all those noun
46:05 - phrases from within our sentence so now
46:11 - that we've done this on one sentence we
46:13 - can go ahead and do this on every single
46:14 - sentence from our novel and to do this
46:17 - once again we're just going to run a
46:19 - loop through our part of speech tag text
46:23 - so I'll say for sentence once again in
46:26 - POS tagged text and POS SEC text and I'm
46:36 - just going to call chunk parsers parse
46:38 - method once again on each sentence and
46:43 - I'll once again need to create a list to
46:46 - store all of our sentences that have
46:49 - been chunked for these noun phrases so
46:52 - I'm going to call it a MUX variable NP
46:56 - Chun
46:57 - sentences and it'll start off as an
47:00 - empty list and then as I go ahead and
47:03 - parse each sentence to pull out these
47:05 - chunks I'm going to append them to NP
47:09 - chunked sentences
47:20 - and then just to confirm that
47:21 - everything's been done I'm going to
47:24 - print one of these chunk sentences so
47:27 - I'm gonna say let's look at NP Chun
47:29 - sentences at index we'll do
47:33 - 222 I feel like that's gonna be a good
47:35 - sentence and I'm going to comment out my
47:38 - previous sentences or my previous print
47:40 - statements just so that we can focus on
47:44 - this one sentence so I'm going to go
47:48 - ahead and run this code oh and we're
47:51 - gonna see since I haven't run my quoted
47:54 - in a little bit sometimes it might reset
47:57 - your your workspace a little bit so I'm
48:00 - gonna do this download once again so I
48:02 - want to uncomment my download let's see
48:12 - running into this error again so I'm
48:17 - going to try and rerun my code hmm okay
48:28 - I'm going to try troubleshooting this to
48:32 - see what's happening I'm gonna refresh
48:38 - I'm gonna copy my code and refresh
48:41 - refreshing often helps solve issues when
48:44 - they arise so let's go ahead and rerun
48:57 - I know TK average perceptrons I'm gonna
49:04 - go ahead and we're gonna copy my code I
49:10 - just reset my project maybe that won't
49:12 - solve my problem I'm just gonna go ahead
49:18 - and we paste my code in and let's give
49:24 - this a shot
49:25 - troubleshooting code running into errors
49:27 - is a common thing that we run into while
49:31 - coding it's a part of the process so
49:35 - don't ever get too upset and you know
49:40 - don't ever get to worrying if you're if
49:42 - you're running into errors it's a
49:43 - totally natural part of the process so
49:46 - it seems like I'm not having any issues
49:48 - now with downloading that data but I do
49:52 - have a issue chunk parse is not defined
49:55 - so I'm missing the R in my chunk parser
50:00 - so I'm gonna go ahead add that are in
50:02 - and rerun my code Gregg expressor object
50:10 - is not callable okay let's see on line
50:16 - 39 so chunk parser so what I need to do
50:20 - is I need to use trunk parser stop parse
50:22 - method here's my error again so you
50:24 - can't just use that chunk parser you
50:26 - need to call its dot parse method so
50:30 - I'll say chunk parser dot parse and I'm
50:33 - gonna comment out my download so we
50:35 - don't see the pop-up happening and I'll
50:38 - save my code give it a run and there we
50:44 - go
50:45 - working through our errors as they arise
50:48 - thank you and the chat for helping me
50:51 - out with some of the spelling and we can
50:58 - now see that for the sentence which is a
51:02 - long one that we have these different
51:06 - noun phrases indicated by NP so once in
51:09 - noun phrase a while
51:11 - she would pass noun phrase a house and
51:14 - the people came out to look at her and
51:16 - bow low as she went by for noun phrase
51:20 - everyone knew she had been non phrase
51:23 - the means of destroying a noun phrase
51:26 - the wicked witch and setting them free
51:30 - from bondage noun phrase so we can see
51:34 - that our noun phrases are picking up
51:36 - these sequences of determiner adjective
51:39 - noun and sometimes it's picking up these
51:42 - items of importance so if you're
51:44 - familiar with the Wizard of Oz the
51:45 - Wicked Witch is a character who comes up
51:48 - often she gets crushed by the house at
51:52 - the beginning when Dorothy comes to Oz
51:55 - so we're seeing we're picking up these
51:57 - phrases that might have some importance
52:00 - Patrick I'm sorry for some spoiler
52:03 - spoiler warning it's for the warning
52:05 - for anyone who maybe hasn't read so
52:14 - you'll see there's any other questions
52:16 - in the chat is there a list of chichi
52:20 - where I can find the explanation of all
52:22 - the functions use so there is
52:26 - documentation that exists online
52:29 - regarding a lot of these functions which
52:32 - are mostly coming from NLT kay also we
52:36 - have a course on parsing with like your
52:38 - expressions here in code kata me where
52:40 - you explain a lot of the functions that
52:42 - were using here today ahead of a form is
52:44 - such an analysis but you can also always
52:47 - come back here to reference this video
52:50 - and see how we did it this work so now
52:56 - that we've gone ahead and chunked these
52:58 - noun phrases we want to go ahead and
53:02 - perform some sort of analysis on the
53:05 - chunks and one way that we can easily do
53:08 - this is with a frequency analysis and
53:10 - this analysis we'll go and look at each
53:12 - sentence that we've chunked from our
53:14 - text and count how how often these
53:18 - chunks appear and then we can see in a
53:20 - novel what are the most common noun
53:22 - phrase chunks
53:25 - so we'll go ahead and do that here today
53:28 - to get an idea of maybe what are the
53:31 - important topics in The Wizard of Oz and
53:39 - to do this I'm going to use a function
53:42 - that I have built myself that will go
53:46 - ahead and easily count these chunks for
53:51 - you and if you go to the top of the
53:54 - workspace you'll see that we are
53:57 - importing these functions for you they
54:00 - are included in another file that is
54:02 - given to you and if you click on the top
54:04 - left our file navigator you can see that
54:07 - there is a file called chunk counters
54:10 - and I'll open up that file and we can
54:12 - see the function written in here and so
54:16 - I'm not gonna go into in-depth detail
54:19 - here today on the chat of how this
54:23 - function works since it gets a little a
54:28 - little entailed but basically what it's
54:30 - doing is it's looking through each of
54:31 - our chunk sentences and it's finding all
54:34 - of those n P chunks that we had and it's
54:37 - using a really useful tool in or package
54:41 - in Python the collections package to go
54:44 - ahead and count how often these chunks
54:47 - appear and so you can look at this
54:51 - function and yourself here in the
54:52 - workspace if it seems like something
54:54 - that people want to dig into further
54:55 - maybe at the end I can jump back in and
54:57 - you know explain more deeply but we're
54:59 - just going to use it here today in order
55:01 - to go ahead and get that frequency
55:04 - account for us and so what we're gonna
55:08 - use is this NP chunk counter function so
55:13 - I'm gonna go back to the bottom of my
55:15 - script and I'm going to create a new
55:19 - variable called most common and P chunks
55:23 - and I'm going to set it equal to and
55:28 - we're going to call this function and
55:29 - Peach on counter right so NP chunk
55:34 - counter and I'm going to give this phone
55:38 - my list of cheong sentences so what this
55:41 - will do is it will take as input this
55:44 - list of chunk sentences and go count all
55:47 - of the noun phrase chunks in them and
55:49 - return to me the 30 most common chunks
55:52 - ranked from most common to the least
55:55 - common out of the top 30 and I can go
55:59 - ahead and then print that most common
56:03 - chunks and I'll run this and we'll see
56:16 - most common chunks is not defined most
56:18 - common NP chunks so that's the name that
56:19 - I named my variable and I'll say most
56:22 - common and P chunks and we'll see here
56:30 - now that we have this list of different
56:33 - chunks that appear in the novel with
56:37 - their count so the most common NP chunk
56:39 - is the noun I so lots of characters are
56:42 - referring to themselves as I and the
56:44 - novel not too much inside there but then
56:46 - we get this list of Dorothy with 222
56:50 - counts the Scarecrow with 213 counts the
56:53 - lion with 148 the tin so we're maybe
56:57 - missing part of this name here but
56:59 - alluding to the tin the Tin Man or the
57:03 - tin one men as he is referred to in this
57:06 - original of The Wonderful Wizard of Oz
57:08 - novel and we also see toto at 73 so
57:11 - we're getting an idea from this analysis
57:13 - of who are the important characters in
57:15 - our novel without reading the text
57:18 - ourselves clearly a lot of us coming
57:21 - into this have an understanding of you
57:23 - know who is important in The Wonderful
57:26 - Wizard of Oz we know the characters well
57:29 - but but but this is kind of giving us
57:32 - really quickly an insight into who are
57:35 - who are the main characters we also see
57:38 - the Wicked Witch come up the Emerald
57:40 - City the Emerald City seems to be a
57:42 - place of importance so now we're not
57:44 - only are getting an understanding of who
57:46 - is important to this text but a location
57:49 - of importance as well and and the nice
57:52 - thing about using the whole noun phrase
57:55 - rather than maybe just looking at the
57:56 - frequency of individual words as we pick
57:58 - up these these names essentially that
58:01 - are made up of multiple parts of speech
58:03 - so if we were just picking up which and
58:06 - the count of which in the novel it gives
58:08 - us less insight then the occurrence of
58:10 - the Wicked Witch which as we know is you
58:14 - know a character who is very prevalent
58:18 - so it's interesting to see what comes up
58:23 - in this list also a heart we know that
58:27 - on on this journey you know to do Wizard
58:31 - of Oz we have a character who may be
58:34 - looking for a heart they're looking for
58:36 - something so we get these ideas that
58:38 - maybe are important and our novel
58:42 - another thing that we can do that can
58:45 - give insight is not just looking at noun
58:47 - phrases but is looking at verb phrases
58:51 - and verb phrases give some insight into
58:55 - action that is occurring and our piece
58:59 - of text that we're analyzing and also
59:00 - sometimes shows how an author talks
59:05 - about the characters or how the author
59:07 - describes action occurring two
59:09 - characters I'm noticing gifts sometimes
59:10 - insight into like for example with the
59:13 - Harry Potter novels maybe a bias that
59:15 - the author is writing with or give us
59:18 - insight into maybe how they offer
59:21 - whether the author thinks about certain
59:22 - characters and so to go ahead and do
59:26 - this we can do what's called verb phrase
59:30 - chunking and this is essentially
59:31 - changing our chunk grammar to instead of
59:34 - finding noun phrases to finding these
59:37 - verb phrases and verb phrases similar to
59:39 - noun phrases will contain a verb that is
59:44 - describing an action that's occurring
59:45 - and then they will often just also
59:47 - include a noun phrase as a part of that
59:52 - verb phrase so let's go ahead and go
59:55 - back and add some new trunk grammar that
59:58 - will chunk verb phrases so I'm going to
60:02 - create a new variable
60:03 - called VP Chong grammar and I'm going to
60:12 - create this new piece of junk ramp trunk
60:14 - grammar I'm gonna say VP as the name of
60:16 - the chunk and then I'm gonna give within
60:19 - the curly braces this pattern of parts
60:21 - of speech tags that we are going to
60:24 - search for within our our texts and so
60:30 - I'll write out this pattern and then
60:32 - we'll walk through how how that pattern
60:34 - or where the powder is searching for so
60:38 - it's gonna start out with a verb VB and
60:40 - I'm going to add a little bit more
60:41 - regular expression syntax here so if
60:45 - you're you know don't get overwhelmed or
60:47 - don't get scared by it we'll talk it
60:48 - through and I'll explain what what it's
60:50 - doing so maybe dot star so we're gonna
60:59 - find that verb and then after the verb
61:00 - we're gonna copy this noun phrase over
61:03 - and then we're gonna end with a
61:24 - okay so let's dig into this verb phrase
61:27 - on grammar piece by piece so we're
61:31 - starting out with VP the name of the
61:33 - chunk that we're looking for so VP in
61:35 - this case will stand for verb phrase and
61:37 - then we are going to look for a verb and
61:41 - so verb is indicated by BB but I've also
61:44 - included this dot star here so if you
61:48 - are familiar with regular expressions
61:49 - the dot is the wild character wild card
61:52 - character and this could represent any
61:54 - any character that we'd like so as you
61:58 - saw before when we were looking at those
62:00 - different parts of speech tags for verbs
62:02 - we can have a past tense verb vbd or we
62:07 - can have a past part of all verb VPN or
62:10 - [Music]
62:11 - gerund vbg and so we want to try and
62:15 - catch all these fun kinds of verbs in
62:17 - our verb phrase and to give this
62:19 - flexibility we can use the power of
62:22 - regular expressions to catch all these
62:24 - different types of verbs which is why
62:26 - we're using this dot wildcard to say
62:29 - okay I want to find a look for VB for
62:34 - verbs and my Sciences but I might want
62:37 - there to be a potentially a D after the
62:40 - VB to represent a past tense verb or I
62:43 - might want there to be an N to represent
62:45 - I believe it is a verb past participle
62:49 - so we're using that dot as the wild-card
62:52 - and then I'm using the clean star to say
62:54 - okay
62:55 - I can I can match that dot I can find
62:57 - that extra letter in in what I'm looking
63:01 - to match or I don't need it I could just
63:03 - find a simple verb VB and so that's all
63:06 - that we're doing there then we're using
63:08 - the same piece of chunk grammar to find
63:11 - a noun phrase so we were looking for
63:14 - that optional determiner we're looking
63:16 - for any number of adjectives and we're
63:18 - looking for finally unknown but then at
63:21 - the end we're also saying okay we might
63:24 - also be looking for a adverb RB and once
63:32 - again we're using this wildcard
63:34 - to say okay
63:36 - if we go back we look at our B or B here
63:40 - our B is an adverb but we want to also
63:43 - try and catch the two other forms of
63:44 - adverb comparative and superlative and
63:48 - to do that we're going to use the
63:49 - wildcard dot and then we're going to say
63:55 - use this question mark to say you we can
63:57 - match that additional character we can
64:00 - get that comparative or the superlative
64:02 - adverb or we can just match the regular
64:05 - adverb RV and then we're adding one more
64:11 - optional quantifier right at the end to
64:13 - say okay we don't even need any adverbs
64:15 - as we've won all we care about is having
64:17 - a verb and then having a noun which is
64:22 - either this performing the action or the
64:26 - action is happening to them if you're
64:31 - not familiar with are your expressions
64:32 - once again don't get don't get scared by
64:34 - them it's one of those languages that at
64:37 - first sight can be a little overwhelming
64:40 - can be a little bit scary but as you
64:42 - work with them you get more control with
64:44 - them and you can see how powerful they
64:46 - can be and how useful they can be and so
64:49 - so if you are unfamiliar my
64:51 - recommendation is get some practice
64:52 - either using your course on our platform
64:55 - or going out there and just practicing
64:57 - on your own and in time you'll come to
64:59 - understand how to work with them how to
65:01 - use them too and they're really useful
65:05 - on any project you might be working on
65:06 - but definitely useful also here in
65:10 - syntax parsing and it can get a little
65:14 - complicated sometimes since we're within
65:16 - the carrots where we're kind of matching
65:20 - individual characters but then I'm also
65:22 - saying okay use this quantifier to match
65:26 - just one adverb which is like a whole
65:30 - whole word so it is a little more
65:32 - complex thanks for hanging in there and
65:36 - and hopefully maybe if it's not clicking
65:38 - for you immediately or right now on
65:41 - maybe a second pass or a second learn
65:43 - things can be a little bit clearer but
65:47 - now we've gone ahead and defined this
65:49 - verb phrase chunk grammar we can go
65:52 - ahead and once again chunk our text and
65:59 - this time instead of looking for noun
66:01 - phrases looking for for phrases so what
66:07 - we're gonna do is we're gonna just copy
66:08 - a lot of the similar code that we did
66:10 - before and this time we're gonna
66:14 - recreate a new chunk parser but we'll
66:16 - name it VB chunk parser and this time
66:21 - we'll create another reg X parser object
66:25 - but with our very own grammar instead of
66:29 - our regular Tron grammar and we're very
66:35 - similarly going to create a list called
66:39 - VP Chun senses that will hold all of our
66:45 - verb phrase Chun sentences and we're
66:50 - gonna then loop through each of the part
66:52 - of speech tag sentences and you know
66:56 - what I don't even need to do this loop
66:57 - again let's reuse the loop that I used
66:58 - before so there's no need to duplicate
67:01 - code so I'm gonna move actually that
67:03 - list above my for loop and I'm going to
67:09 - add within that same for loop that I
67:11 - wrote before I'm going to use my VB Chun
67:16 - parser that we initialized right up here
67:18 - at the top and say VB chunk parser and
67:26 - use its dot parse method dot parse and
67:31 - give dot parse as an argument the
67:33 - sentence that we are chunking and we
67:37 - want to go ahead and then append that
67:38 - sentence to our vp chunk
67:43 - sentence
67:51 - make sure all my parentheses line up
67:54 - make sure that all my underscores our
67:56 - other scores and our pluses and then I
68:03 - want to go ahead and then similarly
68:06 - let's count up our verb phrase chunk
68:08 - sciences and so this time instead of
68:12 - using the n piece on counter function
68:14 - that we have created for you we're gonna
68:16 - use the VP chunk counter function so
68:19 - same EP chunk counter and we're gonna
68:25 - give this function as an argument our VP
68:29 - chunk sentences and we'll assign this to
68:35 - a variable most common VP chunks so
68:44 - let's go ahead and just review
68:48 - everything that we just did here because
68:50 - we just did a lot of like steps over
68:52 - again so we went ahead and we defined a
68:55 - new chunk grammar our very own grammar
68:57 - which is going to find for us a pattern
68:59 - of a verb followed by a noun phrase
69:03 - followed by an optional adverb also
69:06 - we're doing that here on this line and
69:08 - then we're gonna go and create a verb
69:11 - phrase chunk parser another buy gekks
69:13 - parser object with our very own grammar
69:15 - did here then we're going and
69:20 - initializing an empty list that will
69:23 - hold our verb phrase chunk sentences and
69:26 - then we go ahead and go through each
69:29 - part of speech tag sentence in our text
69:32 - and we use that chunk parser to parse
69:35 - the sentence to pull out those verb
69:38 - phrase chunks and we save we saved those
69:43 - chunk sentences to our vp chunks
69:46 - sentences and then we're using the VP
69:49 - ton counter which is that function that
69:51 - we created for you that we're importing
69:54 - from our chunk counter file
69:57 - here and we're using it to find the top
70:00 - 30 most common verb phrase chunks and
70:05 - just so we can see the split in between
70:07 - I'm going to add one print statement
70:09 - right here this is going to say VP
70:12 - chunks and I'll add another print
70:15 - statement right above my NP chunks that
70:17 - says add P chunks just so we have some
70:21 - way to more clearly see in the console
70:26 - what is happening and I'm gonna go ahead
70:33 - and I just need to add one more print
70:34 - statement at the bottom to print the
70:37 - most common VP chunks I will run that
70:45 - code give it a second and VB chug
70:50 - grammar is not defined so I misspelled
70:53 - VP chunk grammar somewhere in my code
70:56 - mixing up my keys in my B's so I'm gonna
70:59 - change that to a P and we'll rerun our
71:03 - code here great and now if we look we'll
71:13 - see we have oh well I'm printing my VP
71:17 - chunk too early so I'm gonna move that
71:19 - down
71:21 - so I'll reset from there and then move
71:24 - it to right above my VP chunks and this
71:27 - will now separate for us my NP chunks
71:30 - and IVP chunks and while while we wait
71:36 - for to load
71:39 - George is asking what languages do you
71:42 - know
71:42 - so personally I learned to code
71:47 - originally in Java never really went too
71:50 - far in it and then really got my
71:53 - experience learning to code deeply in
71:55 - Python and which is what we're using
71:57 - here today using some common packages in
72:00 - Python for performing natural language
72:01 - processing and I'm here in in the US
72:07 - here in New York and code kata me HQ
72:10 - anyone else in the chat wants to share
72:12 - where they're from we're happy to see
72:14 - where everyone is maybe you're coming
72:15 - from around the world in the u.s.
72:18 - wherever you're from we're happy to have
72:20 - you here okay so we now have not only
72:27 - our MP chunks which we looked at before
72:29 - but if we scroll down we'll see we have
72:31 - our VB chunks and so here the the most
72:39 - common VP chunk of this form with a
72:42 - count of 33 times in our novel is said
72:44 - the Scarecrow said Dorothy asked Dorothy
72:48 - so we see once again characters who are
72:52 - relatively important in our novel maybe
72:56 - we have some insight into who's asking
72:57 - questions so Dorothy is asking lots of
73:00 - questions maybe she is confused about
73:03 - where she is or or what is happening to
73:06 - her if you are familiar with with the
73:09 - novel this might rain true for you or
73:12 - you know might make sense that Dorothy's
73:13 - asking lots of questions so reading a
73:15 - little bit of more insight now into not
73:17 - just who is important in the text but
73:19 - some descriptors about how this
73:21 - character is acting or what they're
73:23 - experiencing we can also see more Carter
73:31 - said the lion said the girl asked the
73:35 - Scarecrow said the Cowardly Lion so so
73:40 - we're just seeing more and more more
73:42 - discourse that's happening with the
73:44 - characters and we see another thing the
73:51 - Scarecrow let's see the Scarecrow
73:53 - answered the Scarecrow so that's one
73:56 - that's popped out to me answered the
73:57 - Scarecrow and so that's one of the most
74:00 - common verb phrase chunks that we're
74:01 - seeing in in the novel so you know it's
74:07 - not it's not a huge amount we see I see
74:09 - a times but if you are familiar with the
74:12 - novel the Scarecrow is sometimes looking
74:14 - for more knowledge he's kind of seeking
74:19 - to to have a brain and overseeing that
74:22 - the Scarecrow is answering questions
74:24 - so maybe this is giving us some sort of
74:26 - insight into how the author really
74:29 - thinks of the Scarecrow maybe maybe he
74:32 - does have more knowledge and where then
74:34 - we think so by looking at these verb
74:38 - phrases we can see maybe not just the
74:41 - action that's happening to characters in
74:43 - the novel but also maybe how the author
74:47 - thinks about the characters so this is
74:52 - the analysis that we're able to do one
74:54 - of the fun things I think to do with
74:55 - syntax parsing is to get creative I gave
75:00 - you the chunk grammar here for very
75:03 - common forms of chunking that people
75:05 - perform noun phrase chunking and verb
75:07 - phrase chunking
75:08 - but you're not limited to these patterns
75:11 - you can choose to put in a pattern okay
75:15 - excuse me
75:16 - any pattern of parts of speech tags that
75:19 - you like that you enjoy and
75:28 - [Music]
75:34 - [Applause]
75:39 - [Music]
75:45 - everyone watching these are my amazing
75:47 - colleagues here at code Academy jumping
75:52 - into the picture it's my birthday today
75:54 - and they so graciously surprised me with
75:59 - these cupcakes which I'm super thankful
76:01 - for thank you all so much for for this I
76:07 - really appreciate it we are we are a
76:10 - team yes we're a team here at code
76:12 - Academy I don't work on my own so so all
76:15 - these people are here behind the scenes
76:17 - helping out - thank you all so much
76:27 - yes okay I will take a cupcake I will
76:30 - bite into it in just a second
76:32 - you guys can see thank you everyone for
76:36 - all the happy birthday wishes and the
76:38 - chat I appreciate it before I dig into
76:41 - this lovely cupcake one thing I wanted
76:44 - to say was you can't get creative so you
76:46 - can change this chunk grammar that you
76:48 - have thank you guys
76:51 - change the Strunk grammar that you have
76:53 - listed to do any pattern of parts of
76:56 - speech that you'd like and you can go
76:59 - ahead and use any text that you'd like
77:01 - whether it's a novel that you've read
77:04 - maybe you keep your own journals and you
77:06 - want to analyze your own writing or get
77:09 - your text message data and I don't like
77:11 - your text messages you might be able to
77:12 - find some interesting things and to go
77:14 - ahead and do that if you go to the file
77:16 - navigator on the top left you can click
77:19 - that and you'll see there is a file
77:21 - called my text dot txt if you clicked
77:24 - that oh man - just single click that
77:27 - you'll see that there is a empty text
77:30 - file here and you can put in any text
77:32 - that you like into this text file and
77:37 - then go back to your Python script here
77:40 - and change the name of that file that
77:43 - we're opening from The Wizard of Oz into
77:46 - my text dot txt and that will allow you
77:51 - to go ahead and run this analysis
77:54 - and any piece of tax that you'd like I'm
77:57 - so I really recommend you go ahead and
77:59 - do it you'll see that in the file
78:01 - navigator there are some other novels
78:03 - we've included in there for you so you
78:05 - can run those analyses but I say go get
78:07 - creative I think it's one of the best
78:10 - parts of coding as you get to kind of
78:12 - work off of what other people have and
78:14 - add on your own insights so building off
78:17 - the knowledge of others so go and put in
78:20 - whatever text you'd like find some
78:22 - interesting patterns of parts of speech
78:23 - that you want to search for and look at
78:27 - the frequencies of the of the chunks
78:30 - that appear and see what foot in stage
78:34 - you can find maybe you'll see that you
78:36 - you know talk about a certain topic more
78:38 - than you thought or maybe you see that
78:40 - you use certain kinds of adjectives or
78:42 - some kinds of verbs more often and maybe
78:45 - that gives some sort of insight into who
78:47 - you are or who your friends are if
78:49 - you're looking at something that your
78:50 - friend wrote so get creative if you do
78:53 - any of this please share it with us on
78:55 - Twitter you can feel free to add us at
78:59 - cook Adam e or hashtag code Academy we
79:03 - want to see your results we want to see
79:04 - what you find there's a lot out there to
79:07 - be analyzed there's a lot of mysteries
79:09 - to uncover lots of insights that can be
79:12 - found so go out there share with us your
79:16 - knowledge what you find and thanks so
79:19 - much for joining us here today I'm gonna
79:20 - dig into my cupcakes I don't know if I
79:23 - should do this one or this beautiful
79:25 - like unicorn one there's so many options
79:29 - so thank you all for joining we also
79:33 - have a a feedback form that we would
79:36 - love for you to fill out just so you can
79:39 - give us you know what you liked about
79:41 - the live stream what you would like to
79:45 - see in the future it's posted for you in
79:47 - the chat thanks to our wonderful Alex
79:51 - and yeah so that's it if there's any
79:56 - questions I you know feel free to reach
79:58 - out to us on Twitter or in the comments
80:01 - here and I really recommend you go check
80:04 - out all of our natural language
80:05 - processing content
80:07 - me and my colleague Mario we just put
80:11 - out two courses this week on this topic
80:13 - we discuss here today and also the bag
80:14 - of words language model there's also a
80:17 - great introduction to natural language
80:18 - processing of course that we have that
80:20 - Muriel also worked on that really gives
80:22 - you a high-level overview of this whole
80:25 - field and what you can do with it
80:27 - it's amazing give it a watch and enjoy
80:30 - thank you so much for joining me here
80:31 - today thanks for the birthday wishes and
80:34 - enjoy the rest your day Cheers
80:37 - I'm gonna take one bite and then I'm
80:39 - gonna then I'll cut off mmm yummy
80:46 - very good