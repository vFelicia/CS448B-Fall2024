00:01 - i know it says we're live but
00:03 - i don't believe it i don't believe it
00:06 - until i see it
00:11 - okay we're good to go
00:14 - all right hi everyone uh
00:17 - i hope that everyone is having a good
00:20 - tuesday
00:21 - and i'm excited to get into uh
00:24 - our week six topics um
00:28 - i think we had a little snafu with the
00:30 - timing on the youtube page so
00:32 - it's possible uh that people will be
00:34 - filtering in
00:35 - slowly uh today but if you're here and
00:38 - you're live
00:40 - ask us any questions you have on the
00:42 - youtube chat and we will
00:44 - make sure to address them uh
00:47 - cool so i think we can get started
00:50 - pretty quickly this
00:51 - this time around um i have something
00:53 - that i want to address
00:55 - from last week uh a small correction so
00:59 - i'm gonna start with that and then we're
01:00 - gonna get into the material for this
01:02 - week
01:03 - so with that i will share my screen
01:06 - yeah i saw you posted a comment on the
01:08 - last video so this is also in the last
01:09 - video but
01:10 - uh something that we were going over
01:13 - that
01:13 - um uh i was either said incorrectly or
01:17 - you just want to go
01:19 - oops sorry i shared the wrong screen
01:23 - yes exactly
01:27 - all right sure
01:30 - all right um yeah so this was a really
01:32 - quick
01:33 - edit from last week and this was related
01:36 - to a
01:36 - really good question that alex asked so
01:40 - um it comes back to our discussion of
01:43 - how the binomial distribution
01:44 - is pretty similar to the normal
01:47 - distribution
01:48 - except for um except that it's discrete
01:52 - uh which means that like you can only
01:55 - have
01:57 - the numbers one two you can only have
02:00 - integer values
02:01 - um along your x-axis and i kind of walk
02:04 - through the
02:04 - exam or i talk through how the
02:08 - central limit theorem sort of applies
02:10 - and um at one point alex asked if we
02:12 - could
02:13 - see how the the width of the
02:15 - distribution changes for different
02:16 - sample sizes
02:18 - and we didn't get to it but uh off
02:21 - as i was thinking about it later i
02:23 - realized that actually
02:25 - what we were looking at last time and i
02:27 - think we still have it
02:28 - was we were looking at a histogram of um
02:32 - in this case the number a number of
02:34 - purchases or earlier we were looking at
02:36 - a histogram
02:38 - of coin flip results where each number
02:41 - on the x-axis represented
02:42 - a number of coin flips that resulted in
02:44 - heads um
02:46 - but this example
02:50 - of a normal or this example of a
02:51 - binomial distribution if you think about
02:54 - whether you flip
02:55 - 500 coins versus 10 coins um
02:58 - the 10.1 is going to be less
03:02 - the distribution for 10 coin flips
03:05 - the distribution of the number of heads
03:06 - for 10 coin flips is going to be
03:09 - um thinner than this distribution just
03:12 - because the number
03:13 - there are less numbers between 0 and 10
03:16 - than there are between
03:17 - 0 and 100 so what i
03:20 - everything i was saying only applies if
03:22 - you
03:23 - divide each of these numbers by the
03:25 - sample size
03:26 - so just demoing it down here and the
03:29 - intuition
03:30 - like makes a lot of sense and i feel
03:32 - like people probably intuited this at
03:34 - the time
03:34 - but if you flip 10 coins and you record
03:38 - the proportion of heads
03:39 - instead of the number of heads and plot
03:42 - that distribution
03:44 - you could get anywhere from zero to a
03:46 - hundred percent heads
03:49 - in infinitely many coin flips
03:52 - whereas if you put if you flip a hundred
03:55 - coins
03:55 - um you're more likely or 500 coins i
03:59 - think in this example
04:00 - you're more likely to get closer to 50
04:04 - heads you could still potentially get
04:06 - zero percent heads but it's a lot like
04:08 - it's like
04:09 - so much more minuscule it's not even
04:11 - gonna happen in our
04:12 - in this simulation um and so
04:16 - so so this example the blue histogram is
04:19 - uh 10 um what's the terminology 10
04:23 - attempt 10 coin flips
04:24 - 10 like attempts or something like that
04:27 - yeah so
04:28 - this is we repeated this process a bunch
04:30 - of times
04:31 - each time we flipped 10 coins and we
04:34 - recorded the proportion of heads in each
04:35 - of those
04:36 - 10 flips and we did that and then the
04:40 - orange is 100
04:41 - 500 or something like that i think the
04:42 - orange i i made it 500 but yeah you can
04:44 - see
04:45 - and the histograms look a little funny
04:47 - because of how the x-axis
04:49 - shows up but um but you get the idea
04:51 - that it's basically
04:52 - you can be more confident about the
04:56 - proportion
04:56 - of heads that will come up the more
04:59 - times you flip the coin
05:01 - okay cool and yeah and and again the the
05:04 - correction from last week is we weren't
05:05 - talking last week we didn't specify
05:07 - that's the proportion
05:08 - if you scroll back up to that other
05:09 - graph we were talking about like the
05:10 - overall number
05:12 - and so of course when you're flipping
05:15 - a hundred coins the number of heads can
05:17 - range from zero to a hundred
05:18 - versus ten coins it can only be zero to
05:20 - ten so like by definition
05:22 - that range is going to be smaller but
05:23 - you know that's not what we're actually
05:24 - interested we're interested in the
05:26 - percentage or proportion exactly cool
05:29 - makes sense
05:29 - so with that um i'm going to move to
05:33 - today's topic which i think is a really
05:35 - fun one
05:36 - and we're going gonna use the same data
05:38 - in the same framework as we did last
05:40 - time just to keep things simple
05:42 - um but we're going to talk
05:45 - about the issue of multiple hypothesis
05:49 - tests and we're going to talk a little
05:50 - bit more so
05:52 - last week we we introduced the idea of a
05:54 - p-value
05:56 - as being like an outcome of a hypothesis
05:58 - test
05:59 - um and we'll we'll review a little bit
06:02 - what that
06:02 - definition of a p-value was but then
06:05 - there's another question about
06:07 - what to do with that p-value value
06:10 - and in order to answer that question
06:13 - we're going to need to
06:15 - think a little bit more deeply about
06:19 - about what it means to make a decision
06:22 - based off of a probability
06:24 - so that's the that's the topic for today
06:28 - and this is really important because you
06:30 - may have heard
06:31 - of the reproducibility crisis
06:35 - in statistics um which is the idea that
06:38 - a lot of published research can't be
06:40 - reproduced like people aren't getting
06:43 - the same findings when they try to
06:45 - do the same study over again um
06:48 - and that issue is related to
06:52 - what a lot of people call p hacking um
06:56 - which is going to be a topic of today
07:01 - so with that uh let's jump
07:04 - in so a quick reminder from last week
07:08 - i'll actually
07:09 - pull up um pull up our code from
07:13 - last week and so as a quick recap
07:16 - we were looking at this situation where
07:18 - we had
07:19 - some data about uh people making
07:22 - purchases
07:23 - on a website and we had a sample of 500
07:26 - people
07:27 - and we expected the purchase rate to be
07:30 - 10
07:31 - and we wanted to know if the purchase
07:34 - rate was
07:35 - significantly less than that because we
07:37 - thought there was some sort of bug or
07:39 - we had changed something where we
07:41 - thought the purchase rate
07:43 - might go down we want to check that
07:45 - assumption so we ran this test
07:47 - where we collected our sample we have
07:49 - 500 visitors to a web
07:51 - website who saw this thing and we saw
07:55 - that
07:55 - i think it was like uh like
07:59 - eight percent made a purchase i forget
08:01 - like 41 out of 500 whatever that was
08:03 - yeah so 41 out of 500 so less than our
08:06 - expectation made a purchase
08:08 - um right if it was 10 it would be 50 and
08:11 - we saw only 41
08:12 - made a purchase and then we talked about
08:14 - how
08:15 - um the outcome of this hypothesis test
08:18 - is
08:18 - we could kind of say well if the
08:20 - purchase rate was what we expected
08:22 - then we would expect 50 purchases but if
08:25 - we repeat this process
08:26 - randomly a bunch of times and give
08:29 - everyone a 10
08:31 - chance of making a purchase we still see
08:33 - a range in the number of purchases
08:36 - that happen among 500 people in
08:38 - simulated samples
08:40 - and so based off of that we estimated
08:42 - this p value which was
08:44 - the probability that
08:47 - a randomly sampled group of people a
08:50 - group of 500 people
08:52 - made 41 or fewer purchases given that
08:55 - the true
08:56 - probability of a purchase was 10
09:00 - um and that was that's our
09:04 - p value it came out i think to like 0.1
09:06 - um
09:07 - we did it using simulation we also did
09:09 - it with a built-in function and
09:11 - both times it came out to around 0.1 and
09:13 - so that's
09:14 - that's a probability um and it can be
09:16 - interpreted right if we
09:19 - if we got a really low probability so
09:21 - like let's say
09:22 - we observed only 30 purchases in our
09:26 - sample
09:27 - then the probability would just be like
09:30 - the area of this tiny little block to
09:32 - the left of 30
09:34 - divided by the total area of the whole
09:36 - thing so it'd be a really small number
09:39 - and right and intuitively right if we
09:42 - say the probability of making
09:44 - 30 or fewer purchases given that the
09:47 - true probability of a purchase is 10
09:51 - if that probability is really small then
09:53 - it's unlikely
09:54 - that um it's unlikely that the purchase
09:58 - rate really was
09:59 - 10 in that case so we're making some
10:02 - estimate of
10:03 - how likely is it that the null
10:05 - hypothesis
10:06 - is true given what we observed
10:10 - right and i think uh you might go into
10:12 - this in this lesson sophie but i think
10:14 - a lot of times or the thing that i know
10:16 - about p-value is that like
10:18 - 0.05 p-o value 0.1 p-value right like
10:23 - we're saying that okay at this level we
10:25 - can reject the null hypothesis
10:27 - because um you know the probability of
10:30 - purchase is
10:31 - ten percent or you know in the other
10:33 - case it'll be like two percent if we saw
10:35 - um if we saw 30 purchases instead of
10:39 - um instead of 41. so why
10:42 - why those values why 0.5 and
10:45 - 0.1 as like the values where you say
10:48 - like this is
10:48 - when we can reject the null hypothesis
10:50 - yeah so we're actually gonna just
10:52 - jump right into that right now so um
10:55 - so what alex is alluding to is that
10:58 - um i think we all if we're all okay
11:02 - with the idea of a probability or
11:05 - hopefully we feel comfortable with the
11:06 - idea of the probability
11:08 - but sometimes you need to use that
11:09 - probability to actually make a decision
11:12 - so
11:13 - in this case it's maybe maybe the
11:15 - example that i came up with
11:17 - isn't super conducive to this but let's
11:19 - say that
11:20 - um let's say that what we were doing was
11:23 - we tried out some new version of our
11:26 - website that's
11:27 - cheaper for us to build and um
11:31 - and so we expected the purchase rate
11:33 - might go down
11:34 - we wanted to know if it really did go
11:36 - down or not
11:38 - and if the purchase rate didn't go down
11:41 - significantly with our new
11:44 - cheaper version of the website then
11:47 - we're going to
11:48 - implement the cheaper version of the
11:49 - website because it will save us money
11:51 - but if the purchase rate went down
11:53 - significantly then
11:54 - we don't want to implement it and so we
11:56 - have to make that decision
11:58 - implement don't implement based off of
12:01 - this number and
12:04 - if we're going to do that we don't
12:06 - really want to like
12:08 - i think in general we often want to
12:12 - have some cut off for ourselves like
12:15 - this is a probabilistic
12:18 - number the p-value is a probability um
12:22 - but we're not necessarily comfortable
12:25 - with the idea that like
12:26 - there's just a 10 chance that
12:30 - the purchase rate really did drop at a
12:32 - 90
12:33 - chance that it didn't or whatever we
12:35 - want to
12:36 - um or whatever we want to
12:39 - ascribe to this we want to actually use
12:42 - that probability to decide like is it
12:44 - worth making this decision or not
12:46 - and so a way that people do that
12:49 - is they choose a threshold and they say
12:53 - okay any p-values below
12:56 - this threshold i'm gonna deem them
12:59 - significant what that means it or
13:03 - i'm gonna decide to reject the null
13:06 - hypothesis which is the
13:07 - um the wording that alex used and the
13:10 - idea here is that
13:12 - the null hypothesis in our example was
13:15 - that the probability of a purchase is 10
13:18 - if we get data
13:21 - that challenges that null hypothesis
13:24 - that makes it
13:25 - unlikely that that null hypothesis is
13:27 - true then we're gonna reject it
13:30 - and say okay it's unlikely that the null
13:33 - hypothesis
13:34 - is true therefore i'm gonna say that
13:36 - it's more likely or therefore i'm gonna
13:39 - accept
13:40 - or i don't know people use different
13:42 - language
13:43 - uh usually we don't say like accept the
13:47 - alternative but you say i reject the
13:49 - null hypothesis in favor of the
13:51 - alternative which is to say that we
13:53 - haven't proved
13:54 - that we haven't calculated a probability
13:57 - of the alternative hypothesis we've only
13:59 - calculated a probability of the null
14:01 - hypothesis
14:02 - and then said that it's unlikely um and
14:05 - so
14:05 - a common choice for that threshold at
14:08 - which we say
14:09 - will reject the null hypothesis is .05
14:14 - um but lots of people use .01
14:17 - or um or a different threshold or 0.1
14:21 - um so there's some choice in that and we
14:25 - need to decide it ahead of time
14:27 - um in this case yep i hear i see someone
14:30 - wrote fail to reject so
14:32 - in this case if we got a p-value of
14:34 - about 0.1
14:35 - and if we set a significance threshold
14:38 - of 0.05
14:39 - we would fail to reject the null
14:41 - hypothesis because
14:42 - 0.1 is not smaller than .05
14:46 - um and so the probability that the null
14:49 - hypothesis is
14:51 - untrue given our data data is not
14:54 - small enough for us to reject the null
14:58 - and so uh interesting thing with the
15:00 - language here is that yes is that
15:02 - then saying that we think that the
15:05 - probability of purchase is indeed 10
15:07 - or that we just don't think that it's
15:10 - less than 10 percent
15:13 - um no it's saying that we so if we
15:17 - oh you mean if we don't if we fail to
15:19 - reject the null if we if we fail to
15:21 - reject the null is that us saying that
15:23 - we think that it is
15:24 - that it remains uh 10 probability of
15:27 - purchase
15:28 - um no i mean
15:33 - depending on if you want to be technical
15:36 - about how you should interpret it
15:37 - but if you fail to reject the null
15:39 - you're basically just saying that
15:41 - there is not enough evidence
15:44 - that the purchase rate is not 10
15:48 - so um you can't say
15:51 - basically you can't say whether the null
15:54 - hypothesis
15:54 - is true or not right just that you can't
15:57 - that you can't accept this alternative
15:59 - hypothesis
16:00 - or you just say you can't reject the
16:02 - null
16:04 - i know it's like it goes in circles
16:06 - because
16:07 - uh and the language is complicated and i
16:10 - feel like it's been ingrained in me to
16:12 - be really
16:13 - careful about what language i use really
16:15 - precise
16:16 - um but yeah the whole framework of a
16:19 - hypothesis test is really about
16:22 - setting up the null hypothesis testing
16:24 - the null hypothesis
16:26 - and then saying whether you have enough
16:28 - evidence to reject the null hypothesis
16:30 - or not reject and
16:33 - that's like that's as far as you can go
16:36 - if you reject you're saying
16:38 - it's unlikely that the null hypothesis
16:40 - is true
16:41 - if you fail to reject then you're saying
16:43 - there's not enough evidence
16:45 - for me to say the null hypothesis is not
16:48 - true
16:49 - okay and that's a lot of words that i
16:51 - just said um
16:53 - and i know it's confusing so uh bear
16:55 - with me
16:56 - i think as you're learning this stuff
16:58 - you have to like
17:00 - do it a bunch of times try practice
17:02 - problems
17:03 - decide reject or fail to reject
17:06 - interpret
17:07 - and practice interpreting so that you
17:09 - feel comfortable with that idea
17:11 - yeah lots of double negatives like fail
17:13 - to reject is like
17:14 - a double negative of like oh we're
17:16 - failing to turn this thing down right
17:17 - it's uh i can see how it gets uh it gets
17:19 - confusing
17:20 - yeah okay so with that i'm gonna jump
17:24 - right in
17:25 - so um
17:29 - first thing i wanna do is
17:32 - i want to think through um
17:35 - in the same way that when we talked
17:37 - about the central limit theorem we kind
17:39 - of
17:40 - had this like all-powerful mode
17:43 - and in that all-powerful mode we got to
17:46 - know what the truth was
17:48 - and then um and then we kind of went
17:50 - into researcher mode where we didn't
17:51 - know where the what the truth was and we
17:53 - were trying to
17:54 - analyze something as a researcher we're
17:56 - gonna do the same thing
17:57 - for right now so remember that
18:00 - um that last time we looked at this data
18:04 - this was like a simulated data set
18:06 - 500 visitors to a website um and
18:09 - basically the important thing here is
18:11 - whether or not the
18:12 - each visitor made a purchase and
18:16 - in this data set we have um
18:20 - 41 purchases but we can recalculate that
18:23 - so
18:24 - i think we discuss such disrespect the
18:27 - importance of our
18:28 - uh items that we came up with uh i mean
18:31 - no disrespect at all i love this date i
18:34 - love this data set so much
18:36 - i just watch it here even though it's
18:38 - totally fake
18:39 - i watched love actually for the first
18:40 - time over the weekend so now i could uh
18:42 - be this purchaser of the cue card i wish
18:44 - i had watched it this weekend i totally
18:47 - forgot about that
18:49 - um did you enjoy it it was great
18:52 - very pleasant yeah
18:56 - it was just nice yeah it is nice
19:00 - okay so we'll just confirm there's 40
19:02 - oops
19:04 - i didn't
19:08 - yeah okay so confirming that there's 41
19:11 - purchases
19:12 - but now we're let's go back to what we
19:15 - were doing
19:16 - in the previous week so
19:19 - remember that last week we started doing
19:22 - this
19:24 - these kind of simulations where we said
19:26 - okay but what if
19:28 - the true probability of a purchase was
19:30 - 0.1
19:32 - i'm going to copy over this code so we
19:34 - don't have to rewrite it
19:37 - but for those that haven't seen that
19:41 - if you're just visiting for the first
19:44 - time
19:44 - um what we're doing here right is we're
19:46 - saying okay but what if
19:48 - in this data set we saw 41 purchases but
19:51 - what if the truth
19:52 - is that the probability of a purchase is
19:57 - 0.1
19:58 - and we simulate 500 new visitors so this
20:02 - data set has 500 visitors what if we
20:04 - simulate 500 new visitors and see how
20:06 - many purchases we get
20:08 - and remember that we expect 0.1 times
20:11 - 500 or 50 purchases but we're going to
20:14 - get
20:15 - some variation around 50 so this time we
20:17 - got 55
20:19 - um and every time we run this we're
20:21 - going to get a different number and it's
20:23 - going to be totally random
20:26 - okay so next thing i want to do is
20:30 - think about what would happen if we
20:33 - repeated
20:34 - this a bunch of times and instead of
20:37 - every time collecting the number of
20:39 - purchases what if
20:40 - every time we run this we
20:43 - run a binomial test and calculate
20:47 - a p p value
20:50 - okay so um
20:53 - let's let's try writing this out
20:56 - here i'll make a new
21:00 - so alex do you want to like walk me
21:02 - through
21:03 - yeah say that sentence again i think i i
21:05 - think i got lost so
21:07 - every time we run one of these um tests
21:10 - or run one of these simulations we're
21:12 - going to run a
21:14 - um p-test on it or we're gonna get a
21:16 - p-value for it
21:18 - yeah so we're gonna run a binomial test
21:22 - okay yeah so
21:25 - [Music]
21:28 - right
21:31 - this is this is gonna get like a little
21:33 - bit meta so
21:34 - um if if anyone is confused in the chat
21:38 - um please please please let me know
21:41 - um okay so each time i run
21:46 - this loop i'm going to simulate
21:50 - 500 visitors each with a 0.1 probability
21:54 - or 10
21:54 - probability of making a purchase
21:58 - i'm going to calculate the number of
21:59 - purchases
22:01 - but then what i'm going to do is i'm
22:03 - going to grab this code
22:04 - and i'm actually going to just grab the
22:08 - the binom test code so
22:12 - right because because last week what we
22:14 - did is we ran that a thousand times
22:16 - um and then made a distribution of like
22:18 - oh this time we saw 50 purchases this
22:20 - time we saw 45 and we made that
22:22 - distribution
22:23 - um now we're not making that
22:25 - distribution anymore we're just
22:27 - running the uh binomial test on it
22:30 - yeah actually you know what i feel like
22:32 - i skipped a step
22:33 - let me let me go back before we do this
22:36 - for loop
22:37 - let me quickly just demonstrate what i'm
22:39 - going to do
22:40 - inside the for loop but demonstrate it
22:43 - outside the for loop
22:44 - so right we have this process where
22:48 - here every time we're calculating the
22:51 - number of purchases
22:52 - in our simulated sample but what if
22:54 - instead of
22:56 - that we import this binome test function
23:00 - and then we run the binomial test
23:05 - the exact same test that we simulated
23:07 - last week
23:14 - and we'll do with alternative
23:18 - is less and instead of putting 41
23:22 - in here let's put in the number of
23:24 - purchases
23:25 - in our simulated test
23:29 - our simulated data set so the originally
23:31 - the 41 came from our real data of 41
23:33 - people
23:34 - um bought the item but let's see what
23:37 - happens if we do it
23:38 - with a random a random number
23:41 - yeah and actually each time i'm going to
23:44 - print the number of purchases
23:45 - and then i'm going to print the p value
23:47 - so you can see both of them
23:49 - so here we go
23:54 - all right this time we got 55 purchases
23:57 - and we got a p-value
23:59 - of 0.795 alex do you want to
24:02 - talk through like how would you
24:05 - conceptualize this
24:07 - p-value or how would you explain that
24:10 - p-value
24:11 - right so i'm visualizing the
24:14 - um the distribution right and so
24:18 - um we expect the um
24:22 - number of purchases to be 50. we saw 55
24:25 - and so we're saying if um
24:29 - if the actual number of
24:32 - or if the actual rate of purchase was 10
24:34 - which would give us our 50
24:35 - and we saw 55 percent there's a and we
24:38 - saw 55 purchases rather than 50.
24:40 - there's a uh 79.5
24:44 - chance that um
24:48 - that the uh the the true value is
24:52 - uh ten percent right
24:55 - or not sort of so we again it's like the
24:58 - double negative of like we can't rej
25:00 - it's not that it's 10 it's that it's um
25:03 - like can you write out the um null
25:05 - hypothesis for me
25:07 - yeah so the null hypothesis here i'll do
25:10 - it and mark down
25:11 - so the null hypothesis
25:15 - is that the purchase rate is 10
25:19 - and then the alternative hypothesis
25:24 - is that because we did alternative
25:26 - equals less the alternative
25:28 - hypothesis is that the purchase rate
25:31 - is less than 10
25:35 - right and so um
25:40 - like what i want to say is there's a
25:41 - 79.5 chance that the null hypothesis is
25:44 - true but it's not exactly that right
25:46 - so i would say there's a 79.5
25:50 - chance that um
25:53 - or sorry let me back up
25:57 - there's a 79.5 chance of observing
26:00 - 55 or fewer purchases
26:03 - given that the purchase rate is 10
26:07 - so because we had alternative equals
26:10 - less
26:10 - this is going to be it's going to be a
26:12 - probability of observing 55 or fewer
26:15 - purchases
26:16 - if the true purchase rate were 10
26:22 - um so let's do another one let's run it
26:24 - let's see if we can get something that's
26:26 - like
26:26 - less than 50. okay so here's 39.
26:30 - actually this
26:31 - this p value is pretty close to 0.0.05
26:35 - this is pretty close to our threshold
26:37 - even though
26:38 - right in our simulation the probability
26:40 - of a purchase was
26:42 - 0.1 no it was yeah 0.1 but we got a
26:45 - randomly really small number
26:47 - so this is saying that there's a 5.5
26:50 - chance of seeing 39 purchases
26:52 - given that the true purchase rate is 10
26:55 - yeah 39 or fewer purchases because yeah
26:59 - because the alternative was less than
27:01 - yeah right
27:02 - yeah something that's interesting so we
27:04 - uh in the last one that you ran
27:06 - i think the value the total number was
27:08 - like 62
27:09 - and so the p value was like 0.98 um
27:13 - which means that there's like a 98
27:17 - chance of seeing 63 or fewer
27:20 - purchases given that the total purchase
27:22 - rate is 10 which is like it's a little
27:24 - bit confusing because normally
27:26 - you know you're used to seeing like very
27:28 - small p values
27:30 - so i always come back to it has to do
27:33 - with the fact that we have
27:34 - that alternative equals less
27:37 - um contingency if we had um
27:41 - if we had changed the alternative or to
27:44 - uh both i think is how it's coded here
27:48 - or
27:48 - i think it's also the default then you
27:50 - get the um double-sided p-value it would
27:52 - have been really small
27:54 - for that value but um i think the i like
27:56 - to conceptualize it
27:58 - and i think i have some pictures
28:00 - somewhere i can
28:01 - like pull them up but i like to
28:03 - conceptualize it with this
28:04 - this image it this image so i think
28:07 - about
28:08 - that p-value as a proportion of the area
28:11 - of this blue
28:13 - histogram and so for when we had
28:16 - 41 the p-value was the proportion of the
28:20 - histogram that was
28:21 - left of 41. so that was about 0.1
28:25 - when we got 69 it was all the way over
28:28 - here
28:29 - and it was the proportion of the
28:31 - histogram that's less than
28:33 - 69 and so we got 0.98
28:37 - and could we could we see can we go back
28:40 - to our code and hard code in that
28:41 - 69 and see that it's 98 percent
28:45 - rather than purchases and then maybe
28:47 - flip from
28:49 - let's first see what it looks like with
28:50 - alternative equals less and then
28:53 - um here
28:56 - uh actually
28:59 - let me just create a new yeah
29:04 - right i'm sorry if this is a tangent no
29:06 - no no this is great
29:08 - this is a really helpful discussion
29:09 - because you feel like this is the kind
29:11 - of thing that helps
29:13 - um conceptualize the next steps i'm glad
29:15 - we're taking the time to do it
29:19 - okay so 99
29:23 - chance of seeing 69 or fewer purchases
29:26 - given a 10
29:27 - purchase rate which again if you're
29:28 - picturing that histogram that's like
29:30 - basically all of the area under the
29:31 - histogram but now if we switch to not
29:33 - less but the default which is like
29:36 - two-sided right
29:40 - so this is now saying there's a point
29:42 - seven percent that we're
29:44 - either above 69 purchases
29:47 - or below some very small value
29:50 - right yeah so i'm gonna just actually
29:54 - we're going to go to codecademy.com
29:57 - outlooker and
30:01 - let's pull up
30:06 - there's
30:14 - um thanks for hanging with me guys
30:18 - um i think it's in this lesson
30:29 - oh yeah right
30:33 - so i find this picture kind of helpful
30:36 - so um right
30:40 - the two-sided p-value which is the um
30:42 - which is the default
30:44 - is looking just at the probability of
30:47 - observing
30:48 - something as extreme or more extreme
30:51 - than
30:52 - um than what was observed in the data so
30:55 - in the case where we had a value of
31:01 - 69 we're like drawing we're drawing a
31:04 - line at 69
31:06 - shading everything to the right of 69
31:09 - red and then we're drawing we're saying
31:10 - okay 50
31:12 - 69 minus 50 is 19 so 69 is
31:16 - 19 units away from 50. so let's go 19
31:20 - units away from 50 in the other
31:22 - direction
31:23 - to 31 and let's also add in
31:26 - everything to the left of 31 right
31:31 - so um right so it's like distance away
31:34 - from the expected
31:35 - mean right versus what we did
31:39 - what we're doing before is we're just
31:40 - coloring everything with the alternative
31:43 - being less we're coloring everything to
31:45 - the left
31:46 - and then um if we get a high number
31:50 - then you're going to get a really large
31:51 - p value yeah and so going back to our
31:54 - code now if we
31:54 - i assume we can put in a parameter for
31:56 - the third value where we've done less
31:58 - and then
31:59 - two-sided can we put in greater than and
32:01 - see
32:02 - they'll like point the like one percent
32:05 - of
32:05 - uh seeing something greater than 69 um
32:08 - given the 10
32:10 - yeah so it's going to be even it's gonna
32:11 - be half as big as this
32:13 - right so if we do alternative
32:17 - i think it's just greater let's try it
32:21 - yeah yeah right and so again that's
32:24 - picturing that histogram
32:26 - finding uh your value and saying okay
32:28 - what's the area greater than that
32:30 - that area and it's um you know 0.5 of
32:32 - the total histogram
32:34 - yeah exactly cool
32:38 - um all right i'm glad we went on that
32:40 - tangent i feel like hopefully that
32:42 - clarified some things for people um
32:46 - and now we'll go and try to do this a
32:48 - bunch of times so
32:51 - in this case right if we and
32:54 - let's just like take it back to the um
32:58 - to the original goal of this
33:01 - right in this case if we had observed 69
33:05 - purchases and let's say we used our
33:08 - alternative
33:09 - hypothesis was that the purchase rate
33:12 - was greater than 10
33:14 - and then we observed 69 purchases we'd
33:17 - be
33:18 - pretty certain or we could be pretty
33:21 - certain that
33:22 - the null hypothesis is not true
33:26 - or in other words i should say we could
33:29 - be
33:30 - there's a very low probability that the
33:32 - null hypothesis is true
33:33 - in that case because 69 is so much
33:35 - different from our expectation
33:39 - and so if we were using a
33:44 - threshold of 0.05 to decide whether
33:46 - something
33:47 - whether we should reject the null
33:49 - hypothesis then in this case we would
33:52 - and the key that we're going to talk
33:55 - about now is that
33:56 - sometimes when we reject the null
33:59 - hypothesis
34:00 - we're going to be wrong right like we
34:03 - randomly simulated
34:05 - 69 visitors we did that earlier right
34:08 - completely randomly where the true
34:10 - purchase rate was 0.1
34:12 - um and so sometimes by random chance
34:14 - extreme things happen right like
34:16 - sometimes by random chance you um flip
34:20 - 100 coins and 80 of them come up heads
34:23 - or whatever it is so sometimes we'll
34:26 - reject the null and we shouldn't have
34:28 - rejected the null so that's where we're
34:29 - going
34:30 - but let's let's go through this so um
34:33 - here we've got
34:34 - all of this now i'm putting it inside a
34:36 - for loop so i'm just saying i want to
34:38 - repeat this a bunch of times
34:39 - each time i'm going to simulate my
34:41 - visitors with a
34:43 - 10 probability of a purchase
34:46 - then i'm going to calculate the number
34:48 - of purchases and then i'm going to put
34:51 - that into my
34:54 - um binomial test and i'm going to get
34:56 - rid of this alternative equals less
34:58 - but for some reason
35:02 - my computer doesn't like to
35:06 - i think it has something to do with zoom
35:09 - um okay
35:11 - and then what i want to do actually
35:12 - before this is i want to collect those p
35:15 - values from for further inspection
35:19 - so i'm going to say
35:22 - p vowels equals
35:26 - empty list and then each time i'm going
35:28 - to say p
35:29 - vowels call this a thing
35:34 - simpler like p val u
35:38 - and then we'll say p vowels equals p val
35:41 - stop append um p
35:44 - value thank you
35:49 - do you do the overwrite or can you just
35:51 - do p values dot append
35:53 - um what do you mean do you need to set p
35:56 - values equal to the new thing or can you
35:58 - just do
35:58 - p files oh right you're right yes
36:02 - okay so um so we've appended the new p
36:05 - value
36:06 - onto our list so that we're saving them
36:08 - every time
36:09 - um so i'm gonna run this and then
36:13 - what i'm gonna do is um
36:16 - i'm gonna plot a histogram of the p
36:19 - values
36:20 - so here's where i feel like
36:24 - when i first saw this like i had a
36:26 - professor once that did this in
36:28 - class and before before plotting this
36:31 - they asked the whole class what they
36:32 - thought this histogram would look like
36:34 - and like the one person that had seen it
36:37 - before
36:38 - got it right and all of the rest of us
36:41 - voted
36:41 - for something that was wrong so um so
36:44 - now i'm going to pose this question to
36:46 - you alex
36:48 - that's a hard question the people in the
36:50 - chat also answered this question
36:51 - oh yeah people in the chat also please
36:53 - try to answer this question so
36:55 - i'm going to repeat as i'm going to give
36:57 - you a minute to think and while you're
36:59 - thinking i'm going to repeat what we're
37:00 - doing
37:01 - so basically we've repeated this a
37:04 - thousand times
37:05 - each time we're going to simulate 500
37:07 - visitors
37:09 - where each visitor has a 10 chance of
37:11 - making a purchase
37:12 - we've set the purchase rate to 10 but by
37:14 - random chance there's going to be
37:15 - different numbers each time
37:17 - for each simulated sample of 500 we're
37:20 - going to calculate the number of
37:21 - purchases
37:23 - then we're going to run a binomial test
37:26 - to see whether
37:28 - the number of purchases is significantly
37:30 - different from
37:32 - 50 which is our expectation if the
37:34 - purchase rate is 10
37:36 - and so and given also a sample size of
37:40 - 500
37:41 - right and this is our null hypothesis
37:43 - and then we're going to collect the p
37:44 - values from we're going to collect the p
37:48 - values in a list and then we're going to
37:49 - plot a histogram of them
37:51 - we want to know like so remember when we
37:53 - were doing this example before
37:55 - we were printing out the p values each
37:56 - time and we said like
37:58 - you know when it's 39 we're gonna get a
38:02 - p-value of 0.05
38:04 - um or whatever
38:07 - so each time we're just collecting the
38:09 - p-values
38:11 - um yeah dr monkey uk says no pressure
38:15 - alex nobody else
38:16 - nobody else has been brave enough to
38:18 - venture a guest so
38:19 - the bad thing is that i like reviewed
38:21 - your content uh and so i've
38:23 - i've already oh this is i actually i
38:26 - don't think i put this in the content
38:27 - because
38:28 - i feel like i
38:31 - don't remember i didn't want to this is
38:34 - a bonus
38:35 - by watching this okay so i think the
38:39 - so i i don't think this is the actual
38:42 - answer because you've like prefaced this
38:43 - with so much like anticipation and
38:45 - uh trickery maybe you're just smarter
38:48 - than
38:48 - my entire class i mean so i would just
38:50 - guess that it looks like
38:52 - you know the normal curve where it's um
38:55 - we see a lot of
38:58 - so we're not going to see a lot of
39:02 - um things that are
39:06 - um oh god i don't even know now sorry
39:09 - i'm trying to think through it because
39:10 - we ran this a couple of times and we saw
39:11 - like
39:12 - we occasionally saw things that are tiny
39:14 - like this like .004
39:17 - um and i guess we would also
39:20 - occasionally see something that's huge
39:22 - like 0.99 but more often than not we're
39:25 - gonna see something that's like
39:26 - right in the middle so i would guess
39:29 - that it's like the normal curve
39:30 - with the center going from zero to one
39:33 - with the center at
39:34 - 0.5 okay and i see some message and
39:38 - messages in the chat um kristin in the
39:41 - croissants at
39:42 - croissant says total shot in the dark a
39:44 - normal distribution shape so
39:46 - agrees with you um dr monkey uk says i
39:49 - thought that too
39:50 - you guys are all following the same trap
39:52 - that i fall
39:53 - i fell into so that makes me feel better
39:56 - about myself
39:57 - um i'm gonna plot it now the way it is
40:00 - over
40:03 - i don't know if you want to drumroll
40:04 - this now i actually need sound
40:06 - i need sound effects on this thing i
40:09 - know
40:11 - whoa what is it it's basically
40:15 - what we call a uniform distribution
40:20 - which is to say that in fact
40:23 - if the null hypothesis is true
40:26 - you're equally likely to get any p-value
40:29 - between zero and one
40:32 - nice kristen and the croissant actually
40:34 - got it there right before we yeah
40:37 - just we we know that there's a delay on
40:40 - chat so
40:40 - we got it before i showed the picture
40:44 - yeah exactly um yeah
40:47 - so actually your intuition when you were
40:49 - first talking it through alex
40:50 - was right right on right like we saw
40:53 - some
40:54 - we saw just as many large and small
40:57 - numbers
40:58 - as we did in the middle so um
41:01 - so basically this is this is a super
41:04 - important
41:04 - like thing to understand for the context
41:08 - of um when you
41:11 - when you make mistakes in hypothesis
41:14 - testing
41:15 - which is that so again i'm just
41:18 - repeating what we did
41:19 - but um with like a new lens so
41:22 - we simulated a sample where the true
41:26 - probability of a purchase was 0.1
41:29 - then we ran a binomial test where the
41:31 - null hypothesis was that the purchase
41:33 - rate was 0.1
41:35 - so we when we ran this
41:38 - this hypothesis test in this line
41:42 - every single time we ran this hypothesis
41:44 - test
41:45 - then the null hypothesis was actually
41:48 - true
41:49 - because we simulated the sample like the
41:52 - way that we got this number
41:54 - was that we simulated a sample where the
41:57 - null hypothesis was true where the where
41:59 - the purchase rate really was 0.1 so
42:02 - because
42:02 - we simulated our sample knowing
42:06 - like saying the purchase rate is 0.1
42:10 - then when we run this test with the null
42:11 - hypothesis that the purchase rate is 0.1
42:15 - we're running this test where
42:18 - the null hypothesis is true
42:21 - does that that makes sense but why does
42:24 - that
42:24 - why does that make this uh you know
42:28 - essentially a straight line across so
42:30 - okay
42:31 - so let's let's again
42:34 - now like think through some potential
42:36 - errors that we can make so
42:38 - um so if the null hypothesis is true
42:42 - then um there's there's two things that
42:45 - could happen
42:46 - so if the null hypothesis is true then
42:50 - we don't want the p-value to be
42:52 - significant right we
42:54 - if the null hypothesis is true we should
42:57 - not reject
42:58 - the null um and maybe instead of not
43:01 - significant
43:02 - p-value above
43:06 - threshold right and then p-value
43:11 - below threshold
43:14 - um
43:20 - make this a little bit easier right so
43:23 - if the null hypothesis is true we want
43:26 - our p-value to be above
43:27 - the threshold that if the p-value is
43:30 - above the threshold then we are correct
43:32 - um if the p-value is below the threshold
43:35 - like if we simulate
43:37 - a sample where the null hypothesis is
43:38 - true we run our binomial test and we get
43:41 - a p-value less than 0.05
43:43 - then we've made what's called a type 1
43:45 - error
43:46 - which means that we rejected the null
43:49 - hypothesis
43:50 - even though it's true and
43:54 - it it turns out right when we said so
43:57 - think back to
43:58 - our um our picture
44:01 - and our interpretation of that p-value
44:04 - remember that
44:06 - in this example right the interpretation
44:08 - of the p-value
44:10 - was the probability of observing 41
44:13 - or fewer purchases given that the null
44:16 - hypothesis is true
44:17 - which is to say that our p-value is
44:19 - exactly
44:20 - the probability of observing one of
44:23 - these values by random chance
44:25 - even when the null hypothesis is true
44:28 - so right like
44:31 - if we um right
44:34 - we got a couple of those examples of
44:36 - like when we when we randomly
44:38 - got 30 true and the p-value
44:41 - was like 0.02 or something like that
44:44 - um then that was just
44:48 - that was a time when we would have
44:50 - rejected the null hypothesis even though
44:52 - we know that we were randomly picked
44:54 - those values with a 10
44:55 - purchase rate yeah exactly
44:59 - and whoop right the p
45:02 - values that we collected here are just
45:05 - the probability
45:06 - of observing something more extreme
45:10 - than this given that the null hypothesis
45:13 - is true
45:15 - there is a it's a probabilistic process
45:19 - so every time we run this
45:22 - right every time we we simulate a new
45:26 - set of visitors we're simulating it
45:30 - so that we there is a
45:33 - a probability of getting something that
45:36 - is extreme that we don't
45:38 - expect and then every time we're just
45:42 - looking at the probability of observing
45:44 - something
45:45 - that extreme
45:48 - or more extreme wouldn't we more often
45:51 - get things that aren't as extreme right
45:53 - aren't we going to more often get
45:56 - exactly
45:56 - 50 people purchasing because the true
45:59 - probability is 10
46:01 - so more often than not we're going to
46:02 - get 50 people purchasing less we're less
46:04 - often going to see 10 people purchasing
46:06 - um and so wouldn't that make it because
46:09 - we more often see
46:10 - 50 people purchasing wouldn't we more
46:12 - often um
46:15 - like see a different value here like
46:18 - what what's the so if we go back up to
46:20 - the the code um
46:21 - and we enter in let's the not in the
46:24 - loop let's just do one example
46:27 - uh if you scroll back and scroll up like
46:30 - if we hard code in rather than 69 there
46:31 - if we hard code in 50
46:33 - right right what's the
46:36 - what's the um upsides 40
46:39 - not not 50. oops sorry what's the
46:42 - p-value going to be alex
46:44 - right so i think this is going to be
46:45 - like 0.5 or wait it's one
46:47 - oh because it's uh because it's
46:49 - two-sided in this case
46:51 - um gotcha okay
46:56 - so wouldn't we see a lot more ones
46:59 - because we're
47:00 - most often going to get 50 there as that
47:03 - number
47:05 - that is a really well phrased question
47:08 - and
47:09 - i'm gonna be honest that my brain is
47:11 - like i
47:12 - when i really like putting you on the
47:14 - spot no no i this is really good and i
47:17 - feel like
47:18 - um i i this is one of those things where
47:21 - i go in and out of
47:23 - being able to explain it clearly and
47:24 - sometimes i
47:26 - have it in my head and sometimes my
47:28 - brain is tired and i'm like
47:30 - that's a good point alex uh let me see
47:33 - if i can explain that to you
47:34 - um let me let me like talk it through
47:38 - and maybe we can figure it out together
47:40 - and then if not i want to
47:41 - leave a little time to do one other
47:43 - thing totally um
47:44 - also in the chat if you have like a good
47:46 - way of conceptualizing this
47:48 - definitely share it with us
47:52 - and i guess one thing that we could do
47:53 - is can we run the the
47:55 - loop of a thousand over again and see
47:58 - another example of this graph because
47:59 - i'm curious
48:00 - because right now we do see one more
48:02 - often than not one right the the highest
48:04 - bar in the histogram is one
48:06 - um and i don't know if that's a
48:07 - coincidence or not no it's just a
48:08 - coincidence
48:09 - actually what i'd like to do to help
48:11 - answer our question
48:13 - is just i mean we know what this is
48:15 - going to look like but
48:19 - the so the part if we were to collect
48:21 - the purchases when we do this
48:24 - right if we like purchase
48:29 - [Laughter]
48:32 - and we do
48:45 - this and then
48:48 - below this we'll also
48:51 - do pld.hist
48:55 - um yeah purchase files here
49:02 - this should be a i guess a binomial
49:04 - distribution right
49:06 - right let's read
49:10 - it's funny that you're right it does
49:11 - like
49:13 - look like that every time but i
49:16 - promise you and if we crank it up to
49:18 - like 10 000 which might take
49:20 - more time to run i'm curious what
49:21 - happens right
49:24 - so okay so more frequently we're getting
49:27 - around 50 purchases
49:30 - um and now
49:34 - yeah let's see 10 000.
49:44 - it's running you can see a little good
49:46 - jupiter notebook trick is that you can
49:47 - see the
49:48 - okay there we go interesting
49:51 - why is this i'm actually confused why it
49:55 - looks like this
49:56 - every time
49:59 - like it it keeps seeming to like have
50:02 - the shape where it goes like
50:04 - up to two percent like point two and
50:07 - then
50:08 - up to point to one point oh so i hope
50:10 - i'm not like misleading you guys
50:12 - again like i did last time
50:16 - at the start of the next session though
50:18 - no no no but
50:19 - it should be that i i know this is like
50:22 - making me sad that my brain is
50:26 - not upset well it is confusing
50:29 - um um but basically
50:32 - this time it looks a little different
50:34 - but it still has that same shape i'm
50:35 - like
50:36 - i'm not sure about that but yeah we are
50:39 - getting
50:40 - this distribution which we expect um
50:48 - let's uh let's get to the the rest of
50:52 - the
50:52 - exercise and next week we can come back
50:54 - with further thoughts about this because
50:56 - i'm also curious about
50:57 - what's happening here okay yeah i have
51:00 - i definitely um i definitely
51:04 - can't explain this well when uh when
51:06 - i've like
51:09 - go through yeah sorry um
51:12 - no no it's i'm so glad that you do um
51:15 - okay so
51:16 - the last thing i wanted to kind of
51:18 - demonstrate is that
51:20 - um so remember
51:23 - so we went over these different kinds of
51:25 - errors
51:27 - where if the probability if the p-value
51:29 - is below
51:31 - our threshold when the null hypothesis
51:33 - is true
51:35 - then we've made a type 1 error and
51:40 - so the question is like if the null
51:42 - hypothesis
51:43 - is true how often are we going to make
51:46 - a type 1 error and so let's pick a
51:50 - threshold let's pick
51:52 - 0.05 i guess okay
51:55 - and let's let's run this with a
51:58 - threshold of 0.05
52:00 - um so i'm gonna
52:03 - copy and paste this below so that we can
52:07 - make a couple of edits to it
52:09 - and this time i'm gonna start a counter
52:14 - um and i'm gonna i'm gonna count the
52:16 - number
52:17 - of times that we make an error so i'm
52:19 - going to count
52:21 - the type 1 errors
52:26 - so i'm going to start my counter at zero
52:29 - and every single time
52:31 - i run this
52:34 - i'm going to um
52:38 - i'm going to say if
52:42 - actually we don't need to keep the
52:45 - p-values this time
52:49 - so now i'm going to say if
52:52 - the p-value
52:56 - is less than our threshold so less than
52:58 - 0.05
53:01 - arbitrarily chosen threshold then i'm
53:04 - going to say
53:06 - add 1 to type 1 error so i'm going to
53:09 - say
53:09 - type 1 errors plus equals 1. which just
53:12 - means
53:13 - so and so this is a case where it's like
53:15 - we from our
53:17 - randomized sample we got a really let's
53:19 - say tiny value this time
53:20 - like 30 purchases when
53:24 - uh when we know that the the actual
53:27 - purchase rate is 10
53:28 - and that's going to happen a certain
53:29 - number of times where we get this like
53:30 - really tiny value
53:32 - randomly yeah or i guess really large
53:34 - value randomly too since this is
53:36 - two-sided right
53:37 - yeah so i'm going to do print type 1
53:40 - errors
53:40 - over i'm going to do the this is going
53:43 - to be the type 1 error rate so we
53:45 - we ran this experiment 10 000 times
53:49 - and then i'm going to print how many of
53:51 - those 10
53:52 - 000 times resulted in a type 1 error
53:58 - okay let's try that
54:02 - it's gonna run it's a little slow
54:06 - quick question from chat briefly uh
54:08 - there's no uh
54:09 - you haven't seated the random numbers
54:11 - have you
54:12 - no i haven't seeded them um
54:15 - one thing that gets a little bit funky
54:18 - with um with the binomial distribution
54:23 - is also if our probability is too
54:26 - close to zero we start getting like
54:29 - um like some weird skew
54:33 - so we can try again but basically
54:37 - the point that i was trying to make and
54:39 - now i'm going to like go back and
54:40 - try to talk through your um your
54:43 - confusion alex
54:45 - but as you'll see here this number
54:48 - is pretty close to 0.05 right and
54:52 - because this should be
54:55 - uniformly distributed um
54:58 - basically exactly five percent
55:02 - of the time if you use a
55:06 - a threshold of 0.05 then exactly
55:09 - 5 of the time you're going to get
55:12 - a p-value that is less than 0.05
55:17 - because right like any p-value between 0
55:21 - 1 and 1 is equally likely and so
55:24 - if we say what's the
55:28 - what's the probability of getting a
55:29 - p-value less than 0.05 it's really just
55:33 - like that proportion of this area
55:36 - right and if that was a perfect
55:37 - rectangle then that would be five
55:39 - percent of the rectangle
55:40 - right um which is to say that
55:44 - basically whatever significance
55:46 - threshold you set
55:48 - becomes the type 1 error rate
55:51 - for your test and so um so if you say
55:54 - i'm going to set
55:55 - a significance threshold of 0.05 i'm
55:57 - going to reject the null hypothesis
55:59 - when um a p-value
56:02 - is less than .05 then
56:06 - you're saying that i'm okay with the
56:07 - idea that five percent of the time
56:10 - the null hypothesis is going to be true
56:12 - and
56:13 - i'll still reject the null
56:16 - and so putting that in context of like
56:18 - back to the very start of like we've
56:19 - made this change to the website we want
56:21 - to see if
56:22 - purchases have uh decreased because of
56:25 - this change
56:26 - um or i guess are different because of
56:29 - this change because it's two-sided
56:30 - right right then
56:34 - then this is saying like what it what is
56:36 - this
56:38 - can you put it in terms of like the
56:39 - actual experiment
56:41 - so right so and i think actually this
56:44 - like
56:45 - somehow helps with the confusion from
56:47 - before so
56:48 - remember that our experiment was
56:51 - essentially
56:52 - we're gonna look at a sample of 500
56:56 - visitors
56:56 - and see whether the purchase rate in
56:59 - those visitors
57:00 - was less than
57:03 - our expectation so our expectation was
57:06 - that it would be 10
57:08 - and we're gonna see if
57:12 - it's less than that or we're going to
57:13 - test this hypothesis that it's
57:15 - equal to 10 or less than that is our
57:18 - alternative
57:19 - and so now we're coming at it from
57:22 - another
57:22 - perspective we're saying let's say like
57:24 - the true purchase rate like if we could
57:26 - show
57:27 - this website to this version of the
57:30 - website to
57:31 - every visitor who could ever visit our
57:33 - website
57:34 - if we could show it to all of those
57:36 - people
57:38 - the true purchase rate the two
57:40 - probability of a purchase for each
57:41 - visitor would be 10
57:44 - let's say let's suppose that that's true
57:46 - but we by random chance like
57:48 - we got 500 we got a sample of
57:51 - 500 uh visitors where
57:54 - among those 500 visitors the purchase
57:58 - rate just happened to be low by random
58:00 - chance
58:01 - we're saying this like
58:05 - five we're we're okay with the idea that
58:09 - five percent of the time if the true
58:12 - purchase rate was
58:15 - ten percent we're good five percent of
58:17 - the time
58:18 - we're still going to get a p value less
58:20 - than .05
58:22 - right so if we did that this experiment
58:24 - of getting 500 different people
58:25 - 10 000 times like we're doing here then
58:27 - five percent of those times
58:29 - we're going to reject the null
58:31 - hypothesis even though it was true
58:34 - okay yes um okay
58:38 - one thing i want to see with respect to
58:42 - this picture is really quickly is if we
58:45 - change this
58:46 - to like 0.5 and 0.5
58:52 - and 0.5 whether
58:56 - this shape is still persisting
59:05 - so now it looks more like i would expect
59:10 - um although it's still really jagged
59:12 - right i
59:14 - guess that's that jagged with 10 000 uh
59:19 - uh runs yeah i don't know
59:22 - this honestly might be yeah i'm
59:24 - surprised
59:25 - about it too i feel like there's
59:27 - something going on with
59:29 - how random seeds are being interesting
59:32 - or like how the random function is
59:35 - is working yeah um that's interesting
59:38 - but yeah that is really interesting it
59:40 - should be if the null hypothesis
59:42 - is true then this should be a a uniform
59:45 - distribution
59:46 - i'm going to um after this because we're
59:49 - running out of time
59:51 - but once i'm not on the spot i'm going
59:53 - to write out
59:54 - a clear explanation of this and why this
59:57 - is the case
59:57 - and then i will add it to the video
60:00 - but um but i want to leave
60:04 - you with kind of like the the main
60:07 - takeaway from all of this
60:09 - which is that when we run
60:13 - an experiment and we use a significance
60:16 - threshold so we say
60:17 - we're going to reject the null
60:18 - hypothesis if the p-value is below some
60:20 - threshold
60:22 - the p-value is is probabilistic
60:25 - remember that p-value is the probability
60:28 - of observing something
60:29 - or more extreme given that the null
60:32 - hypothesis is true
60:33 - but that doesn't
60:36 - even if you get a p-value of 0.001
60:42 - that doesn't mean that what you observed
60:46 - is not possible it just means that it's
60:49 - unlikely but you can still observe that
60:53 - and so you have
60:54 - even if the null hypothesis is true so
60:56 - you have to have some threshold for
61:00 - error and what this means is
61:03 - because many people are using 0.05 as a
61:06 - significance threshold
61:08 - it means that five percent
61:11 - of all the tests that have been run with
61:15 - a significance threshold of 0.05
61:19 - are wrong um if we're using that
61:22 - threshold
61:23 - right and so what ends up happening and
61:25 - we didn't get into this as much as i
61:27 - hoped but like
61:28 - if you let's say you run
61:32 - a hundred experiments at your company
61:35 - so i know my boyfriend for example
61:38 - his company is caught constantly running
61:41 - experiments
61:42 - let's say you run and they're probably
61:44 - running like thousands
61:46 - a day i would wouldn't be surprised
61:49 - so let's say but let's say you ran a
61:51 - hundred different tests
61:53 - what that means is that even if the null
61:55 - hypothesis was true for
61:57 - all of those 100 tests five of them
62:00 - would result in a significant p-value at
62:03 - a 0.05 significance level
62:05 - and if all you do is report and act upon
62:08 - the ones that are significant
62:10 - then you're acting upon being a mistake
62:15 - and then that
62:19 - experiment might not be reproducible it
62:21 - was a fluke that you got
62:23 - something that your sample was so
62:25 - different from expectation
62:27 - but and how do you how do you counteract
62:29 - that though because you're no matter
62:30 - what you're always going to have
62:31 - whether it's five percent or one percent
62:32 - or point five percent
62:34 - right there's always going to be uh it's
62:37 - always probabilistic so
62:39 - like what's the what's the upshot of
62:41 - that how do you protect it yeah there's
62:43 - lots of different ways that people
62:45 - do this um sometimes people just use a
62:48 - smaller
62:49 - threshold so 0.01 instead so
62:52 - you have a lower chance of making a
62:55 - mistake
62:56 - um there's also like ways that you can
62:59 - basically uh what's the word
63:03 - um like adjust all of your p-values
63:06 - based on the number of
63:09 - uh based on the number of tests that
63:11 - you've run so like a bonferroni
63:14 - adjustment is like one that's common um
63:18 - so there's different there are different
63:21 - uh
63:21 - approaches but the truth of the matter
63:24 - is you never are gonna fully
63:26 - you're never gonna fully escape the
63:29 - probability
63:30 - of some probability of making a mistake
63:33 - and the only antidote to that is being
63:37 - is one setting all of this like setting
63:41 - your threshold and setting
63:43 - and figuring out how you're going to
63:44 - adjust p values or what p what threshold
63:47 - you're going to use ahead of time
63:48 - figuring out how many tests you're going
63:50 - to run ahead of time and then if you're
63:52 - publishing a paper
63:53 - and you ran or your pub you're making a
63:56 - decision based on some results and
63:57 - you're like presenting it to your
63:58 - company
63:59 - you should show data for every single
64:02 - test that you run
64:03 - you ran not just the ones where you got
64:06 - a significant result
64:08 - right um so yeah
64:11 - the the antidote is clear communication
64:15 - of what you did and how you decided to
64:18 - do it
64:18 - um cool yeah
64:22 - cool all right um good work sophie
64:26 - uh sorry for putting on the spot i feel
64:28 - bad oh no no
64:29 - don't feel bad this is like what that's
64:31 - what this is all about
64:32 - i feel bad i feel like we've been doing
64:35 - these streams on tuesdays
64:36 - aft after the work day and so sometimes
64:39 - i
64:40 - you know i come in a little bit frazzled
64:43 - but i want to make sure that everybody's
64:45 - getting as much as possible out of these
64:47 - and i hope that they're super helpful
64:48 - and i will be sure and we're kind of
64:51 - pretending that this is like a
64:53 - this is a real class and so in a real
64:56 - class
64:56 - it's like we would come back next week
64:58 - and talk about this which is
65:00 - why we talked about last week this week
65:01 - so um so yeah i will i will make sure to
65:04 - post
65:05 - a more full explanation um
65:08 - so that everybody can benefit from that
65:11 - cool
65:12 - all right well thank you everyone we
65:13 - will uh yeah see you next week
65:16 - all right good one