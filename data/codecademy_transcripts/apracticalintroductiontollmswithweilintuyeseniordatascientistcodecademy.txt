00:06 - all right everybody thank you so much
00:08 - for stopping by I'm fet your host for
00:11 - today and I'm here with wailing you and
00:16 - we are super excited to bring you a
00:18 - practical introduction to
00:20 - llms we are you know trying to be in
00:23 - Trend with this there's a lot of talk
00:25 - about Ai and gen Ai and we thought that
00:28 - will bring you a little bit of something
00:30 - here to learn alongside you so hello way
00:33 - how you doing today hey B good how are
00:36 - you I'm doing fantastic are you ready to
00:39 - go yep ready to go all right take it
00:41 - away all right hello everyone my name is
00:45 - Wayan tuya and I'm a senior data
00:47 - scientist at code academy and today
00:49 - we'll be talking about a practical
00:51 - introduction to
00:54 - llms so um we're going to start this off
00:58 - pretty simple um with what is artificial
01:02 - intelligence so you know we've heard
01:05 - artificial intelligence as a word thrown
01:07 - around um and I just kind of want to
01:09 - clear this up but it's basically a
01:11 - machine's ability to perform the
01:13 - cognitive functions that we associate
01:14 - with human Minds um and artificial
01:18 - intelligence as a practice uh
01:20 - encompasses machine learning and deep
01:21 - learning uh within it so it's been
01:23 - around since the 1950s although recently
01:26 - as we all know this has been a huge
01:27 - Resurgence of it um and most of the
01:30 - breakthroughs that we've seen today is
01:32 - actually thanks to deep learning So
01:35 - within this very uh smaller scope within
01:37 - artificial intelligence that's really
01:38 - pushing the field
01:41 - forward um so going back to you know
01:45 - artificial intelligence AI I want to
01:47 - talk about a bit about machine learning
01:49 - I'm just going to assume um no prior
01:52 - knowledge of data and machine learning
01:55 - and whatsoever so just kind of give you
01:57 - a brief overview of what machine
01:59 - learning is
02:00 - um and in this diagram on the right it's
02:02 - kind of uh showing you what machine
02:06 - learning in action so it's a machine
02:08 - learning is as simple as the linear
02:09 - regression we might have learned in
02:10 - school and essentially it needs some
02:13 - data points uh some computing power and
02:16 - a mathmatical mathematical formula that
02:19 - you're trying to optimize so on in this
02:22 - GIF to the right you can see that we
02:23 - have our set of data points and uh using
02:27 - um the Computing you know our computers
02:29 - and and uh our form our formula which is
02:32 - basically trying to minimize the
02:33 - distance between data points and the
02:35 - line which is the model we can with many
02:37 - iterations start approximating um that
02:42 - those little dots with our model which
02:43 - is a line and you know you do this long
02:46 - enough and you get uh pretty good
02:48 - impress pretty impressive results and
02:50 - you can you have a model that can
02:51 - predict the future essentially or
02:53 - predict
02:56 - things um and so going back to that
02:59 - diagram with within machine learning
03:00 - there is deep learning which is um
03:02 - basically a subset of machine learning
03:04 - and it's to do with neuron networks so
03:06 - what is Neuron networks what is deep
03:07 - learning it's a discipline within AI
03:10 - that teaches computers to process data
03:12 - in a way that is inspired by the human
03:14 - brain so on the right here you can see
03:17 - you know tons of layers um and that is
03:20 - essentially a NE Network and this uh you
03:23 - can see the the hidden layers within
03:25 - them those are U what makes a neuron
03:27 - Network deep if there are a lot of
03:29 - hidden layers it's a deep neon network
03:31 - if there aren't that many then it's a a
03:33 - shallow neon Network and anytime you
03:35 - hear about modern you know Advanced
03:38 - capabilities of AI it's because we have
03:40 - uh a lot of the a lot of research and
03:43 - compute power that we can run these deep
03:44 - learning uh deep neural networks and um
03:48 - essentially that's what's driving AI
03:50 - forward right
03:52 - now um so now that we know you know what
03:56 - neuro networks are um I want to kind of
03:59 - touch briefly on how we can teach neon
04:01 - networks how to learn English so English
04:04 - I'm going to call it uh so that's
04:06 - natural language um it's basically you
04:08 - know a natural language is any language
04:10 - that we speak as humans but how do we
04:12 - teach an a neuron Network to learn a
04:14 - natural language uh computers are can
04:17 - only process numbers and they're very
04:18 - good at them but um you know that
04:21 - numbers are not the same as natural
04:23 - language so uh we have to find a way to
04:26 - transcribe or uh to decod encode and
04:29 - decode code numbers into language and
04:31 - vice versa so going back um a vector uh
04:35 - so computers are very good at working
04:37 - with vectors and the vectors can be uh
04:39 - in this example you know 0.2 Nega 5.36 a
04:43 - series of numbers right and in order to
04:46 - for them to understand English we have
04:47 - to turn them um into um basically turn
04:51 - English into numbers so what if we say
04:54 - one is hello and 001 is goodbye that
04:57 - essentially could work if we have
05:00 - um a way to to store that information
05:03 - and that mapping right so that is a very
05:06 - basic example of uh an embedding uh so
05:09 - anytime anytime you hear the word
05:11 - embedding within the NLP the natural
05:13 - language processing and uh llm field
05:17 - then just think of transcri um turning a
05:20 - set of uh numbers into English language
05:23 - or natural language or vice
05:26 - versa so this example that I gave was
05:29 - actually kind of simple we can actually
05:30 - do a lot better um one to and 001 is
05:35 - very like arbitrary I just pick those
05:37 - numbers on a whim but uh on this example
05:39 - to the right we can see there is word
05:42 - Toc which is one of the most famous
05:44 - techniques um which was developed I
05:46 - think in mid 20 maybe 2013 14 um but
05:51 - essentially you can see that uh in this
05:53 - we have this space and we have uh map uh
05:56 - man woman king and queen right in this
05:59 - space
06:00 - and you see that there is some structure
06:01 - to that data it's not arbitrarily spaced
06:03 - out um if we take the distance from uh
06:06 - man and woman to king and queen is
06:08 - actually the same distance so it's
06:11 - actually helpful to structure your
06:14 - embeddings in a certain way that makes
06:16 - actual sense to you know someone looking
06:17 - at it and Al and also to the machine
06:19 - because essentially um that structure
06:22 - get does help a machine understand uh
06:25 - language
06:28 - better
06:30 - so now that we've talked about a little
06:31 - bit about embeddings and you know
06:33 - General um uh deep learning we can talk
06:37 - about the rise of large language models
06:39 - uh so since before 2017 there were large
06:42 - language models but they did not exist
06:44 - as we know them today um really this all
06:47 - started with a paper uh called attention
06:49 - is all you need uh that is a paper by
06:51 - Google in 2017 where they describe a
06:54 - transform architecture and essentially
06:56 - what a transform architecture is is I
06:58 - was talking about new networks and how
07:00 - there are many layers in between them a
07:02 - Transformer is basically one of those
07:04 - layers or um I guess a set it could be a
07:07 - set of layers within that neuron Network
07:09 - and the architecture this was pretty
07:12 - novel back in the day allows llms um to
07:15 - kind of pay attention to the entire
07:17 - question at hand instead of um maybe to
07:20 - certain parts um I guess and allows
07:24 - basically this attention mechanism kind
07:26 - of like what we uh know as attention to
07:30 - um for so allows the llm to basically
07:33 - look at each part of the question that
07:35 - you provided and then kind of really
07:36 - understand what it's talking about so
07:38 - this kicked off uh family of models gbt
07:41 - B um now more recently llama 2 llama
07:44 - llama 2 um and fast forward to today we
07:48 - have even more models so um Claude Bloom
07:51 - Falcon all these models that are open
07:54 - source uh so it's super cool to see um
07:56 - this explosion of models in this domain
08:00 - so now I want to kind of talk about how
08:03 - you know given all these models and all
08:05 - this um all these new exciting uh tools
08:08 - and apps and models out there what can
08:11 - we do Beyond chat gbt we've um uh you
08:16 - know we've kind of messaged uh so we've
08:18 - kind of touched upon uh Chad gbt I'm
08:20 - sure a lot of you have used it and you
08:23 - know just go on type a message and get a
08:25 - response but kind of keeps you in the
08:27 - box right like what if we want to do
08:29 - more with with the API um what more can
08:32 - we
08:35 - do so I want to talk about um basically
08:40 - Chad gbt in this case is a closed Source
08:43 - model um and you know of course it's
08:46 - owned by open AI um and on the other
08:49 - side we have this open source um models
08:52 - the the llamas the uh the blooms or
08:55 - sorry the clouds the Falcons that we can
08:58 - U actually tweak a lot and actually um
09:03 - you know do use it and host it ourselves
09:06 - right so what are the strengths of
09:07 - having you and using these open source
09:09 - models well first of all you don't need
09:11 - to send data anywhere outside of your
09:12 - organization you can um actually use all
09:16 - that data within the machine you're
09:18 - hosting it on uh you're not dependent on
09:21 - third- party uh changes so if opening ey
09:23 - changes something llama 2 for example if
09:26 - you're using it you know if you have
09:28 - your own setup it doesn't change
09:29 - they can be more customizable so there
09:32 - are tons of tools out there um by the
09:35 - open source communities that you can
09:36 - leverage to you uh these days so it's
09:38 - that's kind of cool and you get more FX
09:41 - uh freedom and flexibility when you use
09:43 - um Lama Tu for example um so one one use
09:48 - case is if you want to use um an open
09:51 - source llm model to write like a horror
09:54 - film script right so if you use LL 2 you
09:56 - might get um it's easier to write
09:59 - uh to get the details versus if you use
10:01 - open AI they might say oh this is too
10:03 - graphic like we can't generate a script
10:06 - like this because it's against their
10:08 - policy so um you know if you're a
10:10 - filmmaker maybe you can get some
10:12 - benefits of using a open source
10:15 - model so now on the other side what are
10:18 - the strengths of openi and closed Source
10:21 - models so if we one good thing is that
10:24 - opena manages a lot of the
10:26 - infrastructure in backend so um it's
10:28 - very easy to use and GPT
10:31 - 3.54 would actually perform much better
10:34 - than your open source models this is
10:36 - because they're running it in the back
10:38 - end in huge data centers and versus if
10:40 - you're running an open source model
10:42 - you're probably running on a smaller
10:43 - computer um You probably don't have as
10:45 - much budget and for what it's worth uh
10:48 - using GPT it's very cheap compared to um
10:52 - the amount of power and uh that you're
10:55 - getting for uh basically a lot of bang
10:57 - for your buck because you're getting a
10:58 - better model and a lot of times it can
11:00 - be cheaper than open
11:01 - source um and you know they say they
11:05 - don't train on your API data which is
11:06 - it's you know it's it's good that
11:08 - they're um they claim that um and
11:12 - however of course if you you're working
11:14 - with really secure data you you don't
11:16 - even want to take that chance of sending
11:17 - your data to someone else so uh pros and
11:19 - cons if you're but essentially if you're
11:22 - you know if your use case is um pretty
11:26 - General and and basic I would say stick
11:28 - to um open AI gbt because of its ease of
11:31 - use its cheap prices and the popup
11:33 - performance but if you have something
11:35 - more um do you want to keep private or
11:38 - you want more customizable then um and
11:42 - open source llm like llama 2 might be
11:44 - the way to
11:47 - go so you know I've been talking about
11:49 - llama 2 and that's because it's one of
11:51 - the best open source models out there
11:54 - that's commercially available uh to the
11:56 - public so basically a compan most
11:59 - companies um Can leverage something like
12:01 - that and use it for themselves um uh it
12:05 - has a bunch of sizes of uh so it comes
12:08 - with a bunch of parameters so from all
12:10 - the way down from 7 billion parameters
12:12 - to 70 70 billion parameters um however
12:15 - it's still not as good and as big as gbt
12:18 - 3.5 which is 175 billion parameters um
12:22 - but it is one of the best options you
12:23 - have if you want to host something
12:24 - locally and you can see um in the open
12:27 - source and closed source marks to the
12:29 - right um that llama 2 performs probably
12:33 - at the top at the of the open source uh
12:35 - models but still has some ways to go uh
12:38 - when compared to GB3 gbt 3.5 and gbt
12:47 - 4 another tool that um I would highly
12:52 - encourage uh y'all to check out if
12:54 - you're interested in llms is hug and
12:57 - face so it's basically a GitHub of
12:59 - models and it allows users to save
13:01 - models on there to share ideas and
13:03 - collaborate better and it's heavily used
13:06 - by the open source people so it's a
13:08 - great way to um you know if you have a
13:12 - model you want to try it out uh it's a
13:13 - great way to just go and hug in face
13:15 - download it um try it out um they have a
13:18 - lot of other features out there um and
13:20 - they have tons of models that you can
13:21 - play around
13:22 - with so that's just another if you're
13:25 - get into this
13:27 - um something worth
13:29 - in and techniques for customizing so now
13:32 - that we've talked about kind of what is
13:34 - AI how it generally works and um the
13:37 - landscape out there I kind of want to
13:39 - touch upon um how we can customize these
13:42 - models to to do our bidding better right
13:46 - um so why would we even want to
13:48 - customize LMS in the first place so chat
13:51 - TBT is great but it's training data ends
13:53 - around September 2021 so it doesn't know
13:56 - anything beyond that point right and so
13:59 - um if you ask it a question about recent
14:02 - event it's not going to know and so
14:04 - you're G to you you want to find some
14:06 - way around that right the good thing is
14:09 - the hard work is done for us uh training
14:11 - gb4 costs a lot of money and with
14:15 - comparatively small incremental changes
14:16 - we can actually um tweak gb4 or any of
14:20 - these open source L models to um do our
14:24 - bidding so uh there are two main ways
14:27 - the first one is called retrial
14:29 - augmented generation the second one is
14:31 - fine tuning um retrieval augmented
14:34 - generation also known as rag is probably
14:36 - the first way uh you always want to
14:38 - consider because it's a simpler
14:41 - approach here we can see kind of um a
14:46 - diagram on the right of what uh retrieve
14:49 - augmented generation is and essentially
14:52 - um when a user asks a question uh you
14:55 - know on the right it's the query arrow
14:57 - pointing uh
14:59 - um the the qu the query first comes to a
15:03 - system where it's basically routed to um
15:07 - a set a data set right so basically you
15:10 - have an llm imagine an llm on the side
15:12 - and you have uh your own data set your
15:15 - own propr proprietary data set domain um
15:18 - on you know kind of on the side right
15:21 - and whenever a user asks a question
15:22 - instead of asking the llm directly so
15:25 - say I have a billing question right and
15:27 - and I'm a working in a customer service
15:29 - department if I have a user ask ask a
15:31 - question it can go to Bas LM but the Bas
15:34 - LM will be like well I don't know what
15:36 - our policies are um and you know it can
15:39 - give a very generic answer so instead we
15:41 - can have that question get routed uh to
15:45 - a specific uh domain a data set first
15:48 - and we can compare that question so if
15:51 - it's a question about billing we can get
15:52 - hey uh the most you know top five most
15:55 - recent most similar questions about
15:57 - billing in in our data set so these are
16:00 - the problems that other users have dealt
16:02 - with and like these these are how
16:04 - they've been solved and then we can get
16:06 - because uh so we do a similarity
16:08 - comparison and we can get the top five
16:10 - and we can feed then feed the the query
16:12 - and the relevant documents to the Bas LM
16:15 - so we're basically saying Hey gbt or
16:18 - llama 2 um based on these you know five
16:21 - similar queries where the user had a
16:24 - problem with their billing how would you
16:26 - solve this uh new problem uh and then
16:29 - that's the query of the user and then
16:30 - it'll generate a much better response
16:32 - because uh then it can it has a context
16:34 - to say oh this is what happened in the
16:36 - past and this is probably what should
16:38 - happen uh now and then that response
16:40 - gets forwarded back to the user so
16:43 - that's retrieval augment generation
16:44 - you're
16:45 - augmenting um you're basically
16:48 - retrieving data documents and you're
16:50 - using those documents to augment uh the
16:52 - generation of text uh using your
16:57 - llm
16:59 - um so now we can talk about Vector
17:02 - databases um going back to um what we're
17:06 - to our example here where we are talking
17:09 - about a domain specific data set that's
17:11 - actually um a vector database and so
17:14 - when we talk about rag basically um we
17:17 - need somewhere to store all our data
17:19 - right and that's why um Vector databases
17:22 - exists so if you're familiar with
17:24 - relational databases which is SQL uh
17:27 - they store tables relational uh Vector
17:29 - databases store vectors and they make it
17:32 - very easy for us to compare certain
17:35 - vectors because they're all um it's
17:38 - optimized to do that right and so in
17:40 - this uh picture on the right we can see
17:43 - you
17:44 - know we can see kind of a word cloud and
17:47 - we can see certain words that are
17:49 - clustered together so this is
17:50 - essentially what we're envisioning um is
17:52 - an embedding and so you can see that you
17:55 - know you look for software uh and then
17:57 - you can see uh Excel Oracle web
18:00 - Microsoft all in that cloud right
18:02 - they're all very close together because
18:03 - they're all related to this software um
18:07 - realm I guess and so um using Vector
18:10 - databases we can really easily find
18:12 - vectors that are similar uh and these
18:14 - vectors can be you know just single
18:16 - words or entire queries um and so uh
18:19 - some popular ones that we can use if
18:21 - you're interested are chroma DB and
18:23 - pinec con which are one of the most
18:24 - those two I feel like are the most
18:26 - popular ones but there are tons out
18:27 - there uh that um are available to try
18:30 - out as
18:34 - well um now we can talk about supervised
18:37 - fine tuning so we talked about uh
18:40 - retrieval argumented generation which is
18:42 - you don't touch the model you just
18:43 - basically feed it additional context
18:45 - based on some other data um but what if
18:48 - we want to do something more what if you
18:50 - want to incorporate your actual data
18:52 - into the model itself so that's you know
18:55 - in some cases that's better because the
18:57 - model then actually you know internalize
18:59 - all that data and like learn from that
19:01 - better so um it's you have one uh you
19:06 - know smarter model than rather than like
19:08 - a pieces of um you know a whole system
19:12 - right and so it's very Sim to find tune
19:15 - something it's very similar uh to uh how
19:18 - you you train a machine learning model
19:20 - you're basically giving it data set so a
19:23 - data set so in this um example we have
19:26 - um an instruction on the right and an
19:27 - output on the left and given a set of
19:30 - instructions you're going to tell it to
19:32 - Output a certain um I guess data in a
19:35 - certain way right so you have you feeded
19:38 - this data and um over time you know
19:41 - given a certain number of iterations and
19:44 - and after process all that data it will
19:47 - have um kind of learned if you did it
19:50 - properly it will it will slowly learn um
19:53 - and be fine-tuned towards your use case
19:56 - and so libraries on hugging space which
19:58 - I bring up again because they have a lot
20:01 - of libraries um that are useful for um
20:04 - for this and so they're great ways to to
20:07 - um play to find two models and you can
20:10 - um play around with ways that are I I
20:13 - haven't even talked about here but like
20:15 - um it's they're both they're all like uh
20:17 - it's it's super useful and you should
20:19 - check them out
20:20 - basically um and when we talk about fine
20:24 - tuning there's there's uh full fine
20:26 - tuning and there's Laura fine tuning
20:29 - um as well as a bunch of other ways um
20:32 - full fine tuning
20:33 - essentially is um you're like I said
20:37 - before you're training you're training
20:39 - the entire model um again with your data
20:43 - I guess enhancing it with your data but
20:46 - it is expensive because you have to
20:48 - train a lot of parameters and it's
20:50 - infeasible for most people um because
20:53 - that's it gets expensive and it also
20:56 - there risks associated with it like cat
20:57 - just profic forgetting where you're you
21:00 - train a bunch of things but it'll forget
21:01 - some other things if you don't do it
21:03 - properly so here we're going to touch
21:05 - upon luras uh low rank adapters um
21:09 - essentially what they are is it uses a
21:11 - trick so uh let me just talk about this
21:14 - trick first and if you're familiar with
21:16 - linear algebra you might have seen this
21:18 - trick done before but it uses
21:19 - decomposition which is basically if you
21:22 - have a huge Matrix of say A Million
21:24 - numbers a thousand by a thousand Matrix
21:27 - right so a thousand rows th000 columns
21:29 - you multiply those two uh uh numbers
21:32 - together you get a million numbers so
21:34 - that's a Million numbers you have to
21:35 - keep in memory to um remember right uh
21:39 - Laura basically is saying uh is uses a
21:42 - trick where you can actually approximate
21:45 - those Million numbers with two smaller
21:47 - skinny matrices so if you take two a
21:50 - th000 by5 matrices um you can actually
21:53 - you know I say a th000 by five in this
21:56 - case but you can actually make that five
21:58 - 10 or 20 but and that just kind of
22:00 - tweaks how much you're approximating
22:03 - your um your original Matrix but if you
22:06 - take these a th by five matrices and you
22:07 - multiply them together you can actually
22:09 - get uh a million um numbers but you're
22:14 - actually storing them as 10,000 numbers
22:16 - and so this is a cool trick because
22:18 - you're storing 10,000 numbers now versus
22:21 - um you're storing you were storing a
22:23 - Million numbers before so that's an
22:25 - improvement of like a comparison of uh
22:27 - like guess you're now only storing 1% of
22:30 - that data versus a million uh which 100%
22:33 - of that data um so Lowa uses this trick
22:37 - um it freezes whenever you do low you're
22:39 - basically freezing your original llm
22:41 - model so let's say we have let's say we
22:43 - have llama 2 we're taking all our
22:47 - parameters of llama 2 and we're freezing
22:49 - those um so they can't change and we
22:51 - can't mess them up right and then we
22:53 - extract approximations of uh those
22:56 - pre-trained weights in a separate
22:58 - uh on the side right and then what we do
23:01 - with that is kind of what I said before
23:03 - you you have this huge table of Weights
23:05 - but then you can actually decompose that
23:07 - to two smaller tables and then you can
23:10 - uh basically you can find you in these
23:11 - two smaller uh matrices and essentially
23:15 - what you're doing is you're cutting the
23:16 - computational load um by uh
23:19 - significantly and um you can get pretty
23:22 - good results that way uh and you can
23:25 - train it yourself um you know on our on
23:28 - consumer Hardware which is a pretty
23:30 - great thing uh and essentially what
23:32 - happens is that you can actually train
23:34 - multiple luras so now you can see that
23:37 - um if you're familiar there there are
23:39 - websites out there that allows you to
23:40 - post your luras up there upload your
23:42 - luras and then anyone else um can
23:44 - actually download them and they have the
23:46 - same model they can kind of slot them in
23:48 - and use that Laura to tweak their
23:51 - results um so essentially you get a lot
23:55 - of modularity with this um if you have
23:58 - for example an angry Laura and a happy
24:00 - Laura you can get your base llama 2
24:03 - model and then you can slot each one in
24:05 - and if you slot in your happy Laura it
24:06 - can maybe produce uh text with uh you
24:10 - know in a very cheerful note but if you
24:13 - have an angry Laura you can slot it and
24:15 - maybe it'll make your um LM produce text
24:18 - with an angry manner right um so this is
24:20 - kind of cool to to who is to personalize
24:23 - um what your model can
24:26 - do another thing I want to uh touch upon
24:29 - is quantization so um if you're not
24:34 - familiar with this so basically is a
24:36 - models are usually trained in fp32
24:38 - meaning it takes 32 bits to encode a
24:41 - number right so 30 just 32 pieces
24:45 - information zeros and ones right to
24:46 - represent a number um however it was
24:49 - discovered that we don't actually need
24:50 - to work with all 32 bits all the time
24:52 - and that there are little tricks that we
24:54 - can um work with to save in terms of
24:58 - memory um so we can actually get by with
25:01 - storing our weights in 8 Bits or even
25:03 - four bits and then what we do is we can
25:05 - change them back to 32 bits uh during
25:07 - calculation time um and essentially uh
25:11 - if you uh if this doesn't make sense to
25:13 - you essentially what um what we're doing
25:15 - is taking 32 bits of information and
25:18 - replacing that with eight bits right and
25:20 - we're kind of summarizing that data in
25:22 - eight bits um and so 32 divid 8 is four
25:25 - so you get a four times reduction in
25:27 - memory footprint but you get kind of the
25:30 - gist of your information still saved in
25:32 - there so an example I have um is the
25:35 - cars on that you see on the right you
25:38 - know if we can save the full resolution
25:40 - car um and you know you see the car you
25:42 - see all the pieces of information but if
25:45 - we want to kind of compress it down uh
25:47 - we can actually you know use a blurrier
25:49 - version of image um imagine this
25:51 - blurriness is actually saves us um uh
25:54 - memory so um we can still tell if if you
25:57 - look at the picture on the on the left
25:59 - that it's still a car we just can't tell
26:01 - um you know maybe the details but you
26:03 - can in general make out that it's still
26:04 - a car the gist gets across right it gets
26:06 - across right so um quantization is
26:09 - similar in the way that like it'll get
26:11 - you it summarizes and there's some loss
26:14 - of information but um if you do it
26:16 - properly um it's still very useful and
26:19 - you can uh it can still kind of do your
26:23 - bidding as you want
26:26 - to and lastly I want to talk about Cura
26:30 - which is a combination of Laura and
26:32 - quantization so it's taking that these
26:35 - um little tricks that we've learned and
26:37 - then put them all together which is uh
26:40 - very neat uh it comes well together it
26:42 - comes together very uh nicely and um
26:46 - there is a whole paper out there talking
26:48 - talking about Kora um but uh so if
26:50 - you're interested feel free to go online
26:53 - and search that up but essentially I
26:54 - just kind of want to showcase uh Kora
26:57 - here here how cool it is um we can find
27:00 - TOA 65 bilon parameter model in one
27:03 - single GPU um and that reduces basically
27:08 - this memory footprint from 780 gigabytes
27:10 - to 48 gigabytes so 16 times smaller
27:13 - right um if you're familiar with gpus
27:15 - you know that 70 780 gigabytes of RAM
27:19 - like you can't fit that into any
27:20 - consumer GPU so because of horaa we can
27:24 - actually uh train things ourselves um if
27:27 - we have you know a 48 gab GPU which is
27:29 - still pretty expensive um and the the
27:32 - creators of guanako um which is another
27:35 - chat LM out there um actually used Kora
27:38 - to train the 33 billion parameter llama
27:41 - model and it got to 97 so 98% of chat
27:44 - gbt's performance you know I I feel like
27:48 - this this case um kind of show this
27:51 - example kind of showcases the power of
27:53 - uh lores and quantization and granted
27:56 - you know this is probably the best case
27:58 - um I'm sure there the cases out there
28:00 - where um K actually you know you lose
28:03 - some information and won't uh it won't
28:05 - perform as well as um you know a fully
28:08 - trained fine tune model um but there are
28:12 - cases where um customizing it this way
28:15 - can be very useful and it's evident here
28:17 - is another fun fact here is that the the
28:20 - mod actually beat CPT 3.5 at chess so
28:24 - you know not always not always super
28:27 - helpful but when it does work it can be
28:30 - um uh pretty useful and it can run on
28:33 - consumer Hardware which is what's
28:35 - important and that brings us to the end
28:38 - of our discussion today I hop I hope
28:40 - that everyone learned something uh about
28:43 - llms and I'm happy to take questions
28:47 - now let me take a look at the awesome
28:51 - thank you so much that was uh a lot of
28:53 - information I feel like people are going
28:54 - to be processing that for a little bit
28:57 - um we did have a question earlier
29:01 - somebody said hello with all this Chad
29:05 - GPT Gro llama is it worth it to learn
29:09 - machine learning from the beginning to
29:11 - build AI
29:13 - models um I
29:15 - think when we so again when we talk
29:18 - about machine learning um it's a very
29:20 - big field uh and we're Chad gbt Gro Lama
29:24 - that's specifically that's deep learning
29:26 - um if we're talking about machine
29:27 - learning there is still a lot of use
29:28 - cases of machine learning Beyond simpler
29:31 - cases right so if you want to um you
29:34 - know uh predict uh you know if you want
29:38 - to do linear regression or um for simple
29:41 - use cases machine learning is definitely
29:42 - still useful I would say with all these
29:45 - free tools out there it um if you want
29:49 - to build another free tool such as llama
29:52 - that's going to be very hard unless
29:53 - you're learning machine learning to go
29:55 - work at a big tech company to train um a
29:59 - model there but if you want to train a
30:01 - model yourself that's going to be very
30:02 - hard because you're going to need a lot
30:04 - of funding uh to run big gpus um in the
30:08 - cloud and you're going to need a lot of
30:10 - data and you're going to need to know
30:12 - you know how to do all of that right so
30:14 - one single person it's very hard for
30:16 - them to do that and uh so I would say
30:19 - you should still learn machine learning
30:20 - too you know if you want to join teams
30:23 - that develop this like these tools are
30:25 - developed by entire teams um um and and
30:28 - understanding machine learning is
30:29 - definitely useful but now that these
30:31 - tools are available it definitely lowers
30:33 - the barriers to entry democratizes AI
30:36 - for everyone so people without knowledge
30:39 - of it can use them um you know and to to
30:44 - to be productive do you think that this
30:46 - is a side effect of the way that people
30:48 - learn this sort of uh like the
30:51 - introduction to this kind of field I
30:53 - feel I feel like um when you're a total
30:55 - novice and you're just getting into Tech
30:58 - you might just hear hey go learn machine
31:00 - learning or go learn data science like
31:03 - it's just a very generalistic approach
31:04 - to the field yeah so that might be
31:06 - creating some confusion when people say
31:08 - well is it worth learning the entire
31:10 - field to get into it and I think you are
31:13 - you're saying that it's it's more about
31:15 - learning all the basics so that you can
31:17 - then dig deeper into the part of the
31:19 - field that you're interested in yes yes
31:22 - I would say that um it's a huge field no
31:25 - one is um you know an expert in
31:27 - everything but there are people who are
31:29 - experts in certain segments of it right
31:31 - and as someone who's getting into coding
31:34 - um you might hear about machine learning
31:36 - but um It's actually kind of a mixed
31:39 - discipline right you're to learn machine
31:41 - learning you you have to know coding and
31:42 - how to program but you also have to know
31:45 - uh statistics and math and linear
31:47 - algebra it's very multi-is
31:50 - multi-disciplinary so um I would say you
31:54 - learn coding first and then on the side
31:56 - you kind of Tinker with machine learning
31:58 - to if you're interested and then um
32:01 - that's like the best way to um go to
32:04 - kind of progress in both ways if you're
32:06 - straight going into machine learning
32:07 - there is a lot of things that you might
32:09 - not know about General programming uh
32:11 - and you might miss out on that um and in
32:14 - a lot of ways machine learning jobs are
32:16 - hard to come by for um self-taught
32:19 - people so um if you're talking about if
32:21 - you're looking at um all these you know
32:24 - research machine learning jobs engineers
32:28 - umot oftentimes they'll require a
32:30 - master's or a PhD uh you can get it with
32:34 - a bachelor's degree as well but like um
32:36 - it's much easier to you know do a boot
32:39 - camp or go through code academy and get
32:40 - a programming job first and then jump to
32:43 - machine learning later on just because
32:46 - it encompasses so many fields that um
32:50 - it's yeah it's it's it's very
32:52 - complicated uh field I I mean what do
32:54 - you think that is if it feels very
32:56 - obscure like it's really hard to just
32:58 - jump in it it sounds like it's more
33:00 - adjacent like you get a job in Tech in
33:03 - somewhere that is somewhat related or
33:06 - it's or you either are already on track
33:08 - from like your college degree like you
33:10 - you know you you already picked a a
33:13 - degree an undergraduate that you knew
33:14 - was going to drive you into a masters
33:16 - which going to drive you into a PhD that
33:17 - was gonna it just sounds like it's a
33:19 - it's um maybe not a straight forward has
33:21 - people that decide to be web Developers
33:23 - for example where it's a very clear
33:25 - here's the stack here's the things you
33:27 - learn that's going to get you a a junior
33:31 - uh you know an entry level job and I
33:32 - think this is connected to one of the
33:33 - questions we had in the chat that said
33:36 - uh there's a barrier to entry-level jobs
33:39 - in deep learning jobs are llms a thing
33:42 - that can get me into the
33:44 - door um I would say LMS are not a thing
33:49 - that can get you into deep learning jobs
33:51 - because you're trying to build llms in a
33:55 - deep learning job so you can't I guess
33:58 - you can't just use lmms to
34:01 - um do your job for you I mean hopefully
34:04 - at some point in the future we can but
34:06 - for now llms um are not that smart yet
34:10 - and so you can maybe use LMS to learn
34:12 - some Concepts um you know if you have
34:14 - any questions ask chat gbt it's great
34:16 - for learning but it'll it won't do your
34:19 - job for you especially if you're um
34:21 - trying to get a deep learning job right
34:24 - um and I think it's going back to your
34:25 - question of why it's kind obscure I
34:27 - think in some ways a field is
34:29 - progressing so rapidly that um schools
34:32 - haven't caught up with it yet schools
34:33 - just caught up with the software
34:35 - engineering wave and like education this
34:38 - I guess the entire education industry
34:40 - right so now we have boot camps um and
34:43 - programs for software engineers and
34:45 - computer science right um but you know
34:49 - when we look there's nothing for deep
34:50 - learning there's nothing for machine
34:51 - learning just yet and that's because
34:53 - like I said it's a is very multi-is
34:56 - disciplinary it's it's a lot of that
34:58 - work is research focused and um when
35:01 - you're you know when you're re when
35:04 - you're hiring someone from research you
35:05 - want someone who's experienced with
35:06 - research and um those people tend to be
35:10 - the ones with phds who have um worked a
35:13 - lot with uh you know professors
35:15 - researching whatever field they are in
35:17 - right so they they're they're familiar
35:19 - with that aspect that's a they are um I
35:23 - guess
35:24 - more uh machine learning engineers and
35:26 - and scientists um that are more on the
35:29 - product side or you know more work more
35:31 - with the business and so those kind of
35:34 - are closer related to software
35:36 - engineering jobs um but you still there
35:39 - still um you still have to know these
35:42 - you know the models and Frameworks and
35:44 - you have to learn SQL as long as Python
35:46 - and and all these you know bunch of
35:48 - other things stack entire stack right
35:50 - and so those are we're seeing more um in
35:54 - boot camp pro programs and we're seeing
35:56 - more in
35:57 - uh Master's programs but um as a
36:00 - researcher I think it requires a lot of
36:02 - Education to get to that point w yeah
36:05 - that's a really good point that it's
36:06 - moving so fast that it's not even clear
36:09 - what is it that you should be learning
36:10 - like I guess even as a professional I
36:12 - guess it's probably a bit of a race to
36:15 - to be on top of everything I mean you
36:17 - know open AI just did like a
36:18 - presentation a couple days ago a lot of
36:19 - new things coming out um we had here a
36:22 - question from earlier from Maria asking
36:26 - the deeper the neural network does that
36:29 - make it a bigger blackbox and how could
36:32 - we mitigate it or find ways to control
36:34 - it right uh yes I would say the deeper
36:38 - neuron Network the bigger the blackbox
36:41 - essentially blackboxes you don't know
36:43 - what's happening right and when you're
36:44 - adding more layers uh into the neuron
36:47 - Network um you're making that very
36:49 - complicated uh it's not you know you can
36:52 - go decipher it but most people will not
36:55 - be able to decipher what's going on or
36:57 - it will take a lot of work to the cipher
36:58 - R and so uh the deeper you make it
37:02 - you're adding complexity it's making a
37:03 - more black boxed approach um actually if
37:07 - you're if you're just learning about
37:08 - neuron networks if you have a very
37:10 - shallow neuron neuro Network it's
37:12 - basically is uh the same as a simple
37:15 - linear regression but just um kind of um
37:20 - visualize in a different way so a very
37:22 - shallow neuron network is the same as a
37:23 - regression and it performs very
37:25 - similarly it can perform um you know
37:28 - predictions on like linear tasks um but
37:31 - when you add more complexity to it that
37:34 - uh line that I kind of showcased at the
37:35 - beginning of like you know you're kind
37:37 - of fitting the line to data that line
37:39 - instead of a straight line can become
37:42 - curvy wavy like it becomes a function
37:43 - right and so um it get can get very
37:47 - complicated but it can also means it can
37:49 - get very good
37:51 - approximating um you know data of
37:53 - different
37:54 - shapes it's uh it almost makes me think
37:58 - of linear regressions vers versus
38:00 - polinomial regressions I guess yes um I
38:04 - guess I think I don't know if I'm wrong
38:06 - with this but I think I
38:08 - remember uh testing out shallow models
38:11 - with matlb it was like a really easy way
38:14 - like a really good environment to like
38:15 - see it in action how you would just have
38:17 - it would be very shallow few nodes and
38:19 - you could actually like see how it was
38:21 - changing the output like very very cool
38:23 - um yeah m one of those tools do you you
38:27 - know you were talking before about data
38:29 - in different you were talking about
38:30 - different parts of data science um it
38:32 - sounds like people that want to go into
38:36 - this field might find themselves first
38:39 - going into more of data analytics jobs
38:42 - or data wrangling jobs where you're sort
38:44 - of like working with data but you're not
38:46 - working in the model that is using the
38:48 - data you're just it's I don't know if
38:51 - you can explain to the people that are
38:52 - watching uh sort of like the faces I
38:54 - think you touched into that in your
38:55 - presentation how you know you ingest the
38:58 - data then you process the data which is
39:00 - like you know the fancy models and stuff
39:02 - and then there's the output which is
39:03 - what you get from chgb at the end right
39:05 - um it sounds like most people are really
39:08 - interested in the middle part like the
39:09 - modeling and the mathematics of it but
39:12 - Mo but realistically most people will
39:14 - end up in the front end of that process
39:17 - right right uh yeah I think so I think
39:20 - in general even it sounds the coolest
39:22 - it's like the coolest part to work on
39:24 - the middle part with all the math but
39:25 - it's also
39:27 - um actually in some ways the most
39:29 - scalable so you need uh you know a few
39:33 - people uh a few smart people uh can make
39:36 - a very good model and then they can just
39:38 - send that out you know basically chat
39:40 - gbt right like uh a company has made a
39:43 - model and they can send it out and
39:44 - everyone else can use it and it doesn't
39:46 - require that many uh other humans to
39:51 - kind of maintain it and or I mean it
39:52 - does but like not in the scale that
39:54 - we're talking about right versus in the
39:57 - beginning end of that cleaning data
39:59 - analyzing data um giving insights that
40:02 - at least llms you know they're starting
40:04 - to be able to do some of those things
40:05 - but um they still can do it very uh fast
40:10 - and very efficiently and like very
40:12 - insightfully right and so that's why I
40:14 - would say yes most people will end up
40:16 - that first uh part as data analyst
40:18 - you're cleaning data you're um
40:21 - visualizing it you're kind of maybe
40:23 - doing experiments on it and like playing
40:25 - with that data and so uh it's still very
40:28 - incredibly useful for a business to to
40:31 - have people do that but um yeah and then
40:34 - most people will not be able to U work
40:37 - with the advanced math concepts directly
40:40 - off the bat and even if you do I think
40:43 - uh there is you know as much demand as
40:45 - there is for that kind of um Talent
40:48 - there comparatively I think there's a
40:50 - lot more work to be done um on the the
40:54 - front side of that or the data cleaning
40:56 - for example example um there's and data
40:58 - visualization and Analysis like there's
41:00 - a lot more demand for you know if you if
41:02 - you're talking to a product manager
41:04 - they're going to be like oh what's uh
41:06 - what's going on with my data like um how
41:08 - are how is onboarding going on right how
41:10 - do you what are the pieces of data
41:12 - you're looking at like all these
41:13 - questions um you want to be able to
41:16 - analyze and give answers to that and um
41:21 - there are a lot of these questions
41:22 - around right uh versus um the other side
41:25 - is like oh can you build me a machine
41:27 - learning model to predict XYZ right if
41:30 - unless you're working in a big tech
41:31 - company that has a budget and wants to
41:33 - develop spend that kind of money into uh
41:36 - researching all these things it's hard
41:38 - to um dive into it straight away because
41:42 - also um this ma the more um mathematical
41:48 - and machine learning part there is
41:51 - you're making something but it might not
41:53 - actually work out as well as you want it
41:54 - to right so um there is more of a risk
41:57 - in that
41:59 - sense awesome great answer uh here going
42:02 - back to the chat somebody was asking
42:04 - about open Ai and saying that they they
42:07 - heard that open AI is able to use
42:09 - specific versions of the llm just to
42:11 - answer specific questions so I guess
42:13 - they just kind of like optimizing you
42:15 - know you don't want to kill flies with a
42:17 - cannon so to speak yeah so uh they were
42:20 - just wondering if you have any any
42:21 - thoughts on that or like how I guess how
42:23 - they're doing that or like what what why
42:25 - is that useful
42:26 - yeah so uh is I'm assuming that's
42:29 - related to the
42:31 - gbts uh that they just released a few a
42:34 - few days ago or yesterday what um but
42:37 - essentially that goes back I haven't
42:39 - read too much into it yet but I'm
42:41 - assuming in some in a lot of ways that
42:43 - goes back to the retrieve augmented
42:45 - generation in which they are taking um
42:49 - essentially um well you can do this
42:51 - yourself and there are tons of companies
42:52 - that actually do this right if you're um
42:55 - you know you know say you want to make a
42:58 - llm for uh a law business right you can
43:01 - take uh your your you know law books and
43:05 - like kind of get all that data and um
43:07 - have um your own database and have llms
43:11 - kind of interact with it and respond
43:13 - based on uh this data that they uh
43:15 - you're returning so I'm assuming the
43:18 - gbts that um open are releasing are uh
43:23 - could be based on on that sort of
43:25 - architecture where there
43:26 - um there's a lot of like expert um
43:29 - knowledge in one place and they're kind
43:30 - of referenc in that or um yeah I guess
43:34 - that that's if you ask me like what's my
43:36 - intuition that's kind of my hunch that
43:38 - that's where they're getting at and I
43:39 - and I know there are tons of startups
43:42 - that we doing that because they're like
43:44 - oh it's so cool can we just use GPT and
43:46 - like plug in our data and then we have a
43:48 - product we can make our own startup
43:51 - right I know a lot of them are uh
43:53 - getting disrupted by by this because now
43:56 - now it's like oh wait people would
43:58 - probably rather just go to gbt open the
44:00 - ey to for that functionality they build
44:02 - an entire business model out of a rapper
44:04 - of chpt and then open eyes said well we
44:06 - can just do that in house too exactly
44:09 - yeah um somebody here okay so there's a
44:12 - question here for Michael asking about
44:15 - uh is there are any methods to assure
44:17 - the responses for llms meet standards or
44:20 - there's ways to track when you change an
44:23 - llm if that's going to affect the way
44:26 - that he huc hallucinates or I guess it
44:29 - puts outputs out yeah I think uh people
44:32 - usually use benchmarks a lot of this LM
44:35 - stuff is um actually so a lot of it
44:39 - requires human feedback for example when
44:42 - you training LM GPT all your um like
44:45 - basically the base GPT model all it does
44:48 - is output and it's a very good com
44:49 - sentence completion right so output
44:52 - words are similar to um to what you kind
44:57 - of like the topic that you asked but it
44:58 - might give you just giberish and to make
45:01 - gbt chat gbt there takes a lot of humans
45:04 - to actually create a data set um that
45:06 - makes sense and uh they use re re
45:09 - reinforcement learning um and human
45:11 - feedback to uh kind of accomplish that
45:14 - so um I'm not the most first in it but I
45:17 - would say I think there it takes a lot
45:19 - of humans to kind of actually um uh I
45:23 - guess bet that and they've actually
45:25 - created Ben marks to to score um you
45:28 - know basically these these sets of tests
45:30 - that models can go through and and take
45:33 - them and like kind of like they get a
45:34 - score at the end to see like oh how good
45:36 - and how helpful you were as a model
45:38 - that's the the the main ways that um
45:41 - I've seen these models being compared as
45:43 - as to how good they are sounds like a
45:45 - lot of M your work it reminds me of
45:47 - Amazon Turks and classifying a thousand
45:50 - cat photos a day just to try and train
45:53 - something I I guess yeah um you you have
45:56 - to do you have to do what you got to do
45:57 - to train these models um I know that
45:59 - some people were talking about uh using
46:02 - AI agents to train the AI but I guess
46:06 - that goes into the whole idea of like
46:07 - well you're just putting a lot of trust
46:10 - in that AI agent doing the right thing
46:12 - right like otherwise they're just going
46:13 - to spiral totally out of control um not
46:16 - knowing really what's gonna like I guess
46:18 - you will have to have a a really high
46:20 - level of confidence and predictability
46:23 - in that AI agents to let it train all
46:26 - AIS right right exactly and I mean it's
46:29 - it's been done for example in the sense
46:32 - of like um you can use in responses to
46:36 - train such as the example I gave about
46:38 - guanako training to J gbt data and you
46:41 - know a lot of models can you can train
46:43 - to chat jbt outputs and inputs and that
46:46 - actually is um that's pretty feasible
46:50 - but if you're talking about like
46:52 - automating that entire process and like
46:54 - doing it large scale I would say um you
46:58 - know the you can still do it but the
47:00 - quality of the data the outputs are not
47:02 - vetted right and so unless you have
47:03 - someone they're ensuring uh that the
47:06 - quality is good you might get
47:08 - predictions and get outputs that are not
47:10 - as good as you expected and yeah I think
47:13 - I think essentially um yeah basically
47:17 - you want um they found that these data
47:20 - sets like a good way to make your model
47:22 - better is by having very high quality
47:24 - data sets and if you're just outputting
47:26 - inputting outputting gibberish are like
47:28 - uh not very helpful very generic uh
47:31 - answers you might not get the answers
47:33 - that you want in your own model I can
47:35 - only wait to see what Gro comes out with
47:37 - uh training on on X and all those tweets
47:40 - uh I don't know if you call that a high
47:42 - quality data set uh I guess we'll see um
47:46 - okay so we can close off with one last
47:48 - question I guess this that be more
47:50 - open-ended so just feel free to answer
47:55 - however you want somebody was
47:57 - wondering how you believe that the end
47:59 - point of AI or like what's the top reach
48:01 - of AI so maybe if you can imagine the
48:04 - way that AI is progressing right now ji
48:06 - and all these tools maybe looking 10 15
48:09 - 20 years down the road how far as you
48:11 - can imagine uh what do you see AI going
48:14 - towards yeah um I honestly I think there
48:20 - it's as much of a guess as you know my
48:23 - guess as anyone else's um I would say uh
48:26 - there's certainly uh a lot of room for
48:29 - um for progress and like we've come a
48:32 - long way but there still so far to go
48:34 - and there uh we will probably see AI
48:37 - incorporate in all these um other tools
48:40 - and other use cases that we don't see as
48:42 - of now so is it a game changer I
48:44 - definitely think it is um but at the
48:47 - same time I think there are problems
48:49 - that we're um going to run into uh so
48:53 - for
48:53 - example uh one thing right now is like
48:56 - you know our models are huge right and
48:57 - there's this thing about how do we carry
49:01 - this model this huge model and like uh
49:03 - have it inferenced on like Edge devices
49:05 - right so basically your cell phone how
49:07 - does cell phone um how will it be able
49:10 - to process these large models and so um
49:13 - I know there's a this rise of analog
49:15 - Computing um that they talk about which
49:18 - basically it's almost like oh we've you
49:20 - know we've done all this in the digital
49:22 - realm and we want to kind of carry that
49:24 - to um the analog like kind of like real
49:27 - life and like use analog computers which
49:29 - are these like it's it's it's a
49:32 - different way of computing but um I I
49:35 - want to say that we'll see that more so
49:37 - it's like we we've Advanced a lot but
49:39 - now we might have to back track a little
49:41 - bit to say like oh you know these are
49:43 - actually some of the downsides that we
49:44 - see with these models and these are um
49:47 - alternatives to get around it and so we
49:49 - might kind of pull back in some ways and
49:51 - Advance uh in those areas and then all
49:54 - in all though I think we we will see um
49:58 - you know I don't think it'll be it'll
50:00 - spell out Doom or you know I'm not kind
50:02 - of the doom and boom kind of guy but I
50:04 - do think it'll be very uh useful for
50:07 - day-to-day tasks in the future nice
50:10 - awesome I think it's like um it's like
50:13 - the way that UI has changed right with
50:15 - like web you look at websites from 2000
50:17 - and you look websites now how much
50:19 - interacted they are how much media there
50:20 - is in it like and and the you know
50:22 - websites from 20 years ago look Prett
50:24 - historic like it's just text
50:26 - um I I know I I remember seeing somebody
50:28 - online saying in a conversation that AI
50:31 - is going to change the way that we
50:33 - understand how things just work like
50:34 - what we expect things functionally wise
50:37 - to be and it's just going to integrate
50:39 - in those small ways where your app might
50:41 - do things that they couldn't do before
50:43 - your services might answer questions
50:45 - like be do things and then people just
50:47 - get used to it and you don't realize
50:49 - that you know this whole wave of GNA is
50:51 - what enabled uh those little things to
50:54 - to just make your life easier I guess um
50:56 - yeah even if it doesn't change the world
50:58 - in a big way right yeah agree agree yeah
51:01 - all right well thank you so much willing
51:03 - to you for stopping by sharing what you
51:05 - know with the community and with
51:07 - everybody on YouTube If you enjoyed
51:09 - today's programming Please Subscribe and
51:12 - keep up with Co Academy's social media
51:15 - WE Post over there every time that we
51:17 - have a live event every time we have
51:18 - guest speakers and every time that we
51:20 - have new curriculum content going out
51:23 - like uh lately we have upgrade or react
51:26 - courses to V8 for example so we'll see
51:29 - you next time thank you for stopping by
51:31 - and having have a great week
51:34 - everybody
51:38 - bye-bye and that's